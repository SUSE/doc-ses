<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha.ses.cifs">

 <title>Exportar o CephFS via Samba</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Esta seção descreve como exportar o CephFS por meio de um compartilhamento Samba/CIFS. É possível usar compartilhamentos Samba com clientes Windows*.
 </para>
 <warning>
  <title>Technology Preview</title>
  <para>
   A partir do SUSE Enterprise Storage 5, a exportação de compartilhamentos Samba é considerada uma versão Technology Preview e não é suportada.
  </para>
 </warning>
 <sect1 xml:id="sec.ses.cifs.example">
  <title>Exemplo de instalação</title>

  <para>
   A exportação do CephFS é uma versão Technology Preview e não é suportada. Para exportar um compartilhamento Samba, você precisa instalar manualmente o Samba em um nó de cluster e configurá-lo. É possível fornecer a funcionalidade de failover com o CTDB e a SUSE Linux Enterprise High Availability Extension.
  </para>

  <procedure>
   <step>
    <para>
     Verifique se já existe um CephFS em execução no cluster. Para obter os detalhes, consulte o <xref linkend="cha.ceph.as.cephfs"/>.
    </para>
   </step>
   <step>
    <para>
     Crie um chaveiro específico do Gateway do Samba no master Salt e copie-o para o nó do gateway do Samba:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     Substitua <replaceable>SAMBA_NODE</replaceable> pelo nome do nó do gateway do Samba.
    </para>
   </step>
   <step>
    <para>
     As etapas a seguir são executadas no nó do gateway do Samba. Instale o daemon do Samba no nó do gateway do Samba:
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     Edite o <filename>/etc/samba/smb.conf</filename> e adicione a seguinte seção:
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     Inicie e habilite o daemon do Samba:
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ses.cifs.ha">
  <title>Configuração de alta disponibilidade</title>

  <para>
   Esta seção apresenta um exemplo de como definir uma configuração de dois nós de alta disponibilidade dos servidores Samba. A configuração requer a SUSE Linux Enterprise High Availability Extension. Os dois nós são denominados <systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>) e <systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>).
  </para>

  <para>
   Para obter detalhes sobre a SUSE Linux Enterprise High Availability Extension, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <para>
   Além disso, dois endereços IP virtuais flutuantes permitem aos clientes se conectarem ao serviço independentemente do nó físico no qual está sendo executado. <systemitem class="ipaddress">192.168.1.10</systemitem> é usado para administração do cluster com Hawk2, e <systemitem class="ipaddress">192.168.2.1</systemitem> é usado exclusivamente para exportações CIFS. Isso facilita aplicar as restrições de segurança mais tarde.
  </para>

  <para>
   O procedimento a seguir descreve a instalação de exemplo. Mais detalhes podem ser encontrados em <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
  </para>

  <procedure xml:id="proc.sec.ses.cifs.ha">
   <step>
    <para>
     Crie um chaveiro específico do Gateway do Samba no master Salt e copie-o para os dois nós:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     Prepare o <systemitem class="domainname">earth</systemitem> e o <systemitem class="domainname">mars</systemitem> para hospedar o serviço do Samba:
    </para>
    <substeps>
     <step>
      <para>
       Verifique se os seguintes pacotes estão instalados antes de continuar:
       <package>ctdb</package>, <package>tdb-tools</package> e
       <package>samba</package> (necessário para os recursos smb e nmb).
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       Verifique se os serviços <literal>ctdb</literal>, <literal>smb</literal> e <literal>nmb</literal> estão parados e desabilitados:
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       Abra a porta <literal>4379</literal> do seu firewall em todos os nós. Isso é necessário para o CTDB se comunicar com outros nós do cluster.
      </para>
     </step>
     <step>
      <para>
       Crie um diretório para o bloqueio do CTDB no sistema de arquivos compartilhado:
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     No <systemitem class="domainname">earth</systemitem>, crie os arquivos de configuração para o Samba. Mais tarde, eles serão sincronizados automaticamente com o <systemitem class="domainname">mars</systemitem>.
    </para>
    <substeps>
     <step>
      <para>
       Em <filename>/etc/ctdb/nodes</filename>, insira todos os nós que contêm todos os endereços IP privados de cada nó no cluster:
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       Configure o Samba. Adicione as seguintes linhas à seção <literal>[global]</literal> do <filename>/etc/samba/smb.conf</filename>. Use o nome de host de sua escolha em vez de "CTDB-SERVER" (todos os nós no cluster aparecerão como um nó grande com esse nome, efetivamente):
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       Para obter detalhes sobre o <command>csync2</command>, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Instale e inicialize o cluster da SUSE Linux Enterprise High Availability.
    </para>
    <substeps>
     <step>
      <para>
       Registre a SUSE Linux Enterprise High Availability Extension no <systemitem class="domainname">earth</systemitem> e no <systemitem class="domainname">mars</systemitem>.
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       Instale o <package>ha-cluster-bootstrap</package> nos dois nós:
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       Inicialize o cluster no <systemitem class="domainname">earth</systemitem>:
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       Permita que o <systemitem class="domainname">mars</systemitem> ingresse no cluster:
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Verifique o status do cluster. Você deve ver dois nós adicionados ao cluster:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     Execute os seguintes comandos no <systemitem class="domainname">earth</systemitem> para configurar o recurso CTDB:
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     O binário <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> na opção de configuração <literal>ctdb_recovery_lock</literal> tem os parâmetros <replaceable>CLUSTER_NAME</replaceable> <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable> <replaceable>CEPHX_USER</replaceable>, nessa ordem.
    </para>
   </step>
   <step>
    <para>
     Adicione um endereço IP em cluster:
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     Se <literal>unique_clone_address</literal> for definido como <literal>true</literal>, o agente de recurso IPaddr2 adicionará um ID de clone ao endereço especificado, resultando em três endereços IP diferentes. Geralmente, eles não são necessários, mas ajudam no equilíbrio de carga. Para obter mais informações sobre este tópico, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>.
    </para>
   </step>
   <step>
    <para>
     Verifique o resultado:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     Faça o teste de uma máquina cliente. Em um cliente Linux, execute o seguinte comando para ver se você pode copiar arquivos do sistema e para o sistema:
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
