<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_tiered_storage.xml" version="5.0" xml:id="cha-ceph-tiered">

 <title>Camadas de cache</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editando</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes (sim)</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A <emphasis>camada de cache</emphasis> é uma camada de armazenamento adicional implementada entre o armazenamento de cliente e padrão. Ela foi criada para acelerar o acesso aos pools armazenados em discos rígidos lentos e pools com codificação de eliminação.
 </para>
 <para>
  Em geral, as camadas de cache envolvem a criação de um pool de dispositivos de armazenamento relativamente rápidos (por exemplo, unidades SSD), configurado para agir como uma camada de cache, e um pool de suporte de dispositivos mais lentos e baratos, configurado para agir como uma camada de armazenamento. Normalmente, o tamanho do pool de cache é 10-20% do pool de armazenamento.
 </para>
 <sect1>
  <title>Terminologia de armazenamento em camadas</title>

  <para>
   As camadas de cache reconhecem dois tipos de pools: <emphasis>pool de cache</emphasis> e <emphasis>pool de armazenamento</emphasis>.
  </para>

  <tip>
   <para>
    Para obter informações gerais sobre pools, consulte o <xref linkend="ceph-pools"/>.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>pool de armazenamento</term>
    <listitem>
     <para>
      Um pool replicado padrão que armazena várias cópias de um objeto no cluster de armazenamento do Ceph ou um pool com codificação de eliminação (consulte o <xref linkend="cha-ceph-erasure"/>).
     </para>
     <para>
      O pool de armazenamento algumas vezes é chamado de “suporte” ou de armazenamento “a frio”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>pool de cache</term>
    <listitem>
     <para>
      Um pool replicado padrão armazenado em um dispositivo relativamente pequeno, porém rápido, com seu próprio conjunto de regras em um Mapa CRUSH.
     </para>
     <para>
      O pool de cache também é chamado de armazenamento “a quente”.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-caution">
  <title>Pontos a serem considerados</title>

  <para>
   As camadas de cache podem <emphasis>prejudicar</emphasis> o desempenho do cluster para cargas de trabalho específicas. Os pontos a seguir mostram alguns dos aspectos que você precisa levar em consideração:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Dependente da carga de trabalho</emphasis>: A melhoria no desempenho dependerá da carga de trabalho. Como existe um custo associado à adição ou remoção de objetos do cache, talvez seja melhor que a maioria das solicitações acesse o menor número de objetos. O pool de cache deve ser grande o suficiente para capturar o conjunto ativo de sua carga de trabalho para evitar sobrecarga.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Benchmark difícil</emphasis>: A maioria dos benchmarks apresenta baixo desempenho com camadas de cache. O motivo é que elas solicitam um conjunto grande de objetos e leva muito tempo para “aquecer” o cache.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Desempenho possivelmente baixo</emphasis>: Para cargas de trabalho não adequadas às camadas de cache, o desempenho costuma ser menor do que um pool replicado comum sem as camadas de cache habilitadas.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Enumeração de objetos do <systemitem>librados</systemitem></emphasis>: Se a replicação usa o <systemitem>librados</systemitem> diretamente e executa a enumeração de objetos, as camadas de cache talvez não funcionem conforme esperado. (Isso não é um problema para o Object Gateway, RBD ou CephFS.)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1>
  <title>Quando usar as camadas de cache</title>

  <para>
   Considere usar as camadas de cache nos seguintes casos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Os pools com codificação de eliminação são armazenados no FileStore, e você precisa acessá-los por meio do Dispositivo de Blocos RADOS. Para obter mais informações sobre o RBD, consulte o <xref linkend="ceph-rbd"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Os pools com codificação de eliminação são armazenados no FileStore, e você precisa acessá-los por meio do iSCSI. Para obter mais informações sobre o iSCSI, consulte o <xref linkend="cha-ceph-iscsi"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Você tem um número limitado de armazenamento de alto desempenho e uma grande coleção de armazenamento de baixo desempenho e precisa acessar os dados armazenados mais rapidamente.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-cachemodes">
  <title>Modos de cache</title>

  <para>
   O agente de camadas de cache processa a migração de dados entre a camada de cache e a camada de armazenamento de suporte. Os administradores podem configurar o modo de execução dessa migração. Há dois cenários principais:
  </para>

  <variablelist>
   <varlistentry>
    <term>modo write-back</term>
    <listitem>
     <para>
      No modo write-back, os clientes do Ceph gravam dados na camada de cache e recebem uma confirmação (ACK) da camada de cache. No momento certo, os dados gravados na camada de cache são migrados para a camada de armazenamento e eliminados da camada de cache. Conceitualmente, a camada de cache é sobreposta “na frente” da camada de armazenamento de suporte. Quando um cliente do Ceph precisa de dados que residem na camada de armazenamento, o agente de camadas de cache os migra para a camada de cache na leitura e, em seguida, eles são enviados ao cliente do Ceph. Na sequência, o cliente do Ceph poderá executar a E/S usando a camada de cache, até que os dados se tornem inativos. Esse procedimento é ideal para dados mutáveis, como edição de fotos ou vídeo, ou dados transacionais.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>modo apenas leitura</term>
    <listitem>
     <para>
      No modo apenas leitura, os clientes do Ceph gravam dados diretamente na camada de suporte. Na leitura, o Ceph copia os objetos solicitados da camada de suporte para a camada de cache. Os objetos obsoletos são removidos da camada de cache de acordo com a política definida. Essa abordagem é ideal para dados imutáveis, como apresentação de fotos ou vídeos em uma rede social, dados de DNA ou imagens de raio X, porque a leitura de dados de um pool de cache que possa conter dados desatualizados oferece pouca consistência. Não use o modo apenas leitura para dados mutáveis.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>Pool com codificação de eliminação e camada de cache</title>

  <para>
   Os pools com codificação de eliminação exigem mais recursos do que os pools replicados. Para compensar essas limitações, é recomendado definir uma camada de cache antes do pool com codificação de eliminação. Esse é um requisito para quando se usa o FileStore.
  </para>

  <para>
   Por exemplo, se o pool <quote>hot-storage</quote> for constituído de armazenamento rápido, o <quote>ecpool</quote> criado na <xref linkend="cha-ceph-erasure-erasure-profiles"/> poderá ser acelerado com:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add ecpool hot-storage
<prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   Isso colocará o pool <quote>hot-storage</quote> como uma camada do ecpool no modo write-back para que cada gravação e leitura no ecpool realmente usem o armazenamento a quente e aproveitem os benefícios da flexibilidade e velocidade.
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   Para obter mais informações sobre camadas de cache, consulte o <xref linkend="cha-ceph-tiered"/>.
  </para>
 </sect1>
 <sect1 xml:id="ses-tiered-storage">
  <title>Configurando um armazenamento em camadas de exemplo</title>

  <para>
   Esta seção mostra como configurar uma camada de cache de SSD rápida (armazenamento a quente) na frente de um disco rígido padrão (armazenamento a frio).
  </para>

  <tip>
   <para>
    O exemplo a seguir é apenas para fins de ilustração e inclui uma configuração com uma raiz e uma regra para a parte da SSD que reside em um único nó do Ceph.
   </para>
   <para>
    No ambiente de produção, as configurações do cluster normalmente incluem mais entradas raiz e de regra para o armazenamento a quente e também nós mistos, com SSDs e discos SATA.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Crie duas regras CRUSH adicionais: “replicated_ssd” para a classe de dispositivo de cache SSD rápida e “replicated_hdd” para a classe de dispositivo HDD mais lenta:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_ssd default host ssd
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_hdd default host hdd
</screen>
   </step>
   <step>
    <para>
     Mude todos os pools existentes para a regra "replicated_hdd". Isso impede o Ceph de armazenar dados nos dispositivos SSD recém-adicionados:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> crush_rule replicated_hdd
</screen>
   </step>
   <step>
    <para>
     Torne a máquina um nó do Ceph usando o DeepSea. Instale o software e configure a máquina de host conforme descrito na <xref linkend="salt-adding-nodes"/>. Vamos supor que o nome seja <replaceable>node-4</replaceable>. Esse nó precisa ter 4 discos OSD.
    </para>
<screen>[...]
host node-4 {
   id -5  # do not change unnecessarily
   # weight 0.012
   alg straw
   hash 0  # rjenkins1
   item osd.6 weight 0.003
   item osd.7 weight 0.003
   item osd.8 weight 0.003
   item osd.9 weight 0.003
}
[...]</screen>
   </step>
   <step>
    <para>
     Edite o mapa CRUSH do pool de armazenamento a quente mapeado para os OSDs que residem nas unidades SSD rápidas. Defina uma segunda hierarquia com um nó raiz para as SSDs (como “root ssd”). Mude também o peso e adicione uma regra CRUSH para os SSDs. Para obter mais informações sobre o Mapa CRUSH, consulte a <xref linkend="op-crush"/>.
    </para>
    <para>
     Edite o Mapa CRUSH diretamente com as ferramentas de linha de comando, como <command>getcrushmap</command> e <command>crushtool</command>:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rm-device-class osd.6 osd.7 osd.8 osd.9
<prompt>cephadm@adm &gt; </prompt>ceph osd crush set-device-class ssd osd.6 osd.7 osd.8 osd.9
</screen>
   </step>
   <step>
    <para>
     Crie o pool de armazenamento a quente que será usado nas camadas de cache. Use a nova regra “ssd” para ele:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create hot-storage 100 100 replicated ssd</screen>
   </step>
   <step>
    <para>
     Crie o pool de armazenamento a frio usando a regra “replicated_ruleset” padrão:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</screen>
   </step>
   <step>
    <para>
     Em seguida, a configuração de uma camada de cache envolverá associar um pool de armazenamento de suporte a um pool de cache. Neste caso, armazenamento a frio (= pool de armazenamento) a armazenamento a quente (= pool de cache):
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add cold-storage hot-storage</screen>
   </step>
   <step>
    <para>
     Para definir o modo de cache como “writeback”, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback</screen>
    <para>
     Para obter mais informações sobre modos de cache, consulte a <xref linkend="sec-ceph-tiered-cachemodes"/>.
    </para>
    <para>
     As camadas de cache writeback sobrepõem a camada de armazenamento de suporte, portanto, elas requerem uma etapa adicional: você deve direcionar todo o tráfego do cliente do pool de armazenamento para o pool de cache. Por exemplo, para direcionar o tráfego do cliente diretamente para o pool de cache, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay cold-storage hot-storage</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cache-tier-configure">
  <title>Configurando uma camada de cache</title>

  <para>
   Há várias opções que você pode usar para configurar camadas de cache. Use a seguinte sintaxe:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>

  <sect2 xml:id="ses-tiered-hitset">
   <title>Conjunto de acertos</title>
   <para>
    Os parâmetros de <emphasis>conjunto de acertos</emphasis> permitem o ajuste de <emphasis>pools de cache</emphasis>. Normalmente, os conjuntos de acertos no Ceph são filtros de Bloom e um método eficiente, em termos de uso de memória, para monitorar objetos que já estão no pool de cache.
   </para>
   <para>
    O conjunto de acertos é uma matriz de bits usada para armazenar o resultado de um grupo de funções de hash aplicadas a nomes de objetos. Inicialmente, todos os bits estão definidos como <literal>0</literal>. Quando um objeto é adicionado ao conjunto de acertos, um hash é executado com o nome dele, e o resultado é mapeado em diferentes posições no conjunto de acertos, em que o valor do bit é definido como <literal>1</literal>.
   </para>
   <para>
    Para saber se existe um objeto no cache, um novo hash é executado com o nome do objeto. Se houver algum bit como <literal>0</literal>, definitivamente o objeto não estará no cache, e será necessário recuperá-lo do armazenamento a frio.
   </para>
   <para>
    Os resultados de objetos diferentes podem ser armazenados no mesmo local que o conjunto de acertos. Existe a possibilidade de todos os bits serem <literal>1</literal> sem que o objeto esteja no cache. Portanto, os conjuntos de acertos que trabalham com um filtro de Bloom apenas podem informar se um objeto definitivamente não está no cache e precisa ser recuperado do armazenamento a frio.
   </para>
   <para>
    Um pool de cache pode ter mais do que um conjunto de acertos monitorando o acesso a arquivos ao longo do tempo. A configuração <literal>hit_set_count</literal> define quantos conjuntos de acertos estão em uso, e <literal>hit_set_period</literal> define por quanto tempo cada conjunto de acertos foi usado. Após a expiração do período, o próximo conjunto de acertos será utilizado. Se o número de conjuntos de acertos se esgotar, a memória do conjunto de acertos mais antigo será liberada, e um novo conjunto de acertos será criado. Os valores de <literal>hit_set_count</literal> e <literal>hit_set_period</literal> multiplicados um pelo outro definem o período geral de monitoramento do acesso aos objetos.
   </para>
   <figure xml:id="ses-tiered-hitset-overview-bloom">
    <title>Filtro de Bloom com 3 objetos armazenados</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="bloom-filter.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="bloom-filter.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Se comparado com o número de objetos com hash, um conjunto de acertos baseado em um filtro de Bloom é muito eficiente em termos de uso de memória. Menos do que 10 bits são necessários para reduzir a probabilidade de falsos positivos para menos do que 1%. A probabilidade de falsos positivos pode ser definida com <literal>hit_set_fpp</literal>. Com base no número de objetos em um grupo de posicionamento e na probabilidade de falsos positivos, o Ceph calcula automaticamente o tamanho do conjunto de acertos.
   </para>
   <para>
    O armazenamento necessário no pool de cache pode ser limitado com <literal>min_write_recency_for_promote</literal> e <literal>min_read_recency_for_promote</literal>. Se o valor for definido como <literal>0</literal>, todos os objetos serão promovidos para o pool de cache assim que forem lidos ou gravados, e esse comportamento se manterá até que eles sejam eliminados. Qualquer valor maior do que <literal>0</literal> define o número de conjuntos de acertos que são pesquisados para o objeto, ordenado por data. Se o objeto for encontrado em um conjunto de acertos, ele será promovido para o pool de cache. Lembre-se de que fazer backup dos objetos também pode fazer com que sejam promovidos para o cache. Um backup completo com o valor “0” pode fazer com que todos os dados sejam promovidos para a camada de cache enquanto os dados ativos são removidos da camada de cache. Portanto, mudar essa configuração com base na estratégia de backup pode ser útil.
   </para>
   <note>
    <para>
     Quanto maior o período e maior os valores <option>min_read_recency_for_promote</option> e <option>min_write_recency_for_promote</option>, mais RAM o daemon <systemitem class="process">ceph-osd</systemitem> consome. Especificamente, quando o agente está ativo para descarregar ou eliminar objetos do cache, todos os <option>hit_set_count</option> conjuntos de acertos são carregados na memória RAM.
    </para>
   </note>
   <sect3 xml:id="ceph-tier-gmt-hitset">
    <title>Usar GMT para conjunto de acertos</title>
    <para>
     As configurações de camada de cache têm um filtro de Bloom denominado <emphasis>conjunto de acertos</emphasis>. O filtro faz um teste para identificar se um objeto pertence a determinado conjunto de objetos a quente ou a frio. Os objetos são adicionados ao conjunto de acertos usando marcações de horário anexadas aos nomes deles.
    </para>
    <para>
     Se as máquinas de cluster forem colocadas em fusos horários diferentes, e as marcações de horário forem extraídas do horário local, os objetos em um conjunto de acertos poderão ter nomes equivocados indicando marcações de horário futuras ou passadas. Na pior das hipóteses, os objetos podem nem existir no conjunto de acertos.
    </para>
    <para>
     Para que isso não aconteça, <option>use_gmt_hitset</option> é definido como o padrão “1” nas configurações de camada de cache recém-criadas. Dessa forma, você força os OSDs a usar as marcações de horário GMT (Horário de Greenwich) durante a criação dos nomes de objetos para o conjunto de acertos.
    </para>
    <warning>
     <title>Manter o valor padrão</title>
     <para>
      Não modifique o valor padrão “1” de <option>use_gmt_hitset</option>. Se os erros relacionados a essa opção não forem causados pela configuração do cluster, nunca a modifique manualmente. Do contrário, o comportamento do cluster poderá ser imprevisível.
     </para>
    </warning>
   </sect3>
  </sect2>

  <sect2>
   <title>Tamanho do cache</title>
   <para>
    O agente de camadas de cache executa duas funções principais:
   </para>
   <variablelist>
    <varlistentry>
     <term>Descarregar</term>
     <listitem>
      <para>
       O agente identifica objetos modificados e os encaminha ao pool de armazenamento para armazenamento de longo prazo.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Eliminar</term>
     <listitem>
      <para>
       O agente identifica objetos que não foram modificados (limpos) e elimina do cache os que estão sem uso há mais tempo.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <sect3 xml:id="cache-tier-config-absizing">
    <title>Tamanho absoluto</title>
    <para>
     O agente de camadas de cache pode descarregar ou eliminar objetos com base no número total de bytes ou no número total de objetos. Para especificar um número máximo de bytes, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes <replaceable>num_of_bytes</replaceable></screen>
    <para>
     Para especificar o número máximo de objetos, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_objects <replaceable>num_of_objects</replaceable></screen>
    <note>
     <para>
      O Ceph não pode determinar o tamanho de um pool de cache automaticamente, portanto, a configuração do tamanho absoluto é necessária neste caso. Do contrário, o descarregamento e a eliminação não funcionarão. Se você especificar os dois limites, o agente de camadas de cache iniciará o descarregamento ou a eliminação quando um dos limites for acionado.
     </para>
    </note>
    <note>
     <para>
      Todas as solicitações de cliente serão bloqueadas apenas quando <option>target_max_bytes</option> ou <option>target_max_objects</option> for atingido.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="cache-tier-config-relsizing">
    <title>Tamanho relativo</title>
    <para>
     O agente de camadas de cache pode descarregar ou eliminar objetos relativos ao tamanho do pool de cache (especificado por <option>target_max_bytes</option> ou <option>target_max_objects</option> na <xref linkend="cache-tier-config-absizing"/>). Quando o pool de cache consistir em determinada porcentagem de objetos modificados, o agente de camadas de cache os descarregará para o pool de armazenamento. Para definir o valor <option>cache_target_dirty_ratio</option>, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_ratio <replaceable>0.0...1.0</replaceable></screen>
    <para>
     Por exemplo, a definição do valor como 0,4 iniciará o descarregamento dos objetos modificados quando eles atingirem 40% da capacidade do pool de cache:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</screen>
    <para>
     Quando os objetos modificados atingirem determinada porcentagem da capacidade, descarregue-os a uma velocidade mais alta. Use <option>cache_target_dirty_high_ratio</option>:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_high_ratio <replaceable>0.0..1.0</replaceable></screen>
    <para>
     Quando o pool de cache atingir determinada porcentagem da sua capacidade, o agente de camadas de cache eliminará os objetos para manter a capacidade livre. Para definir o valor <option>cache_target_full_ratio</option>, execute o seguinte:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_full_ratio <replaceable>0.0..1.0</replaceable></screen>
   </sect3>
  </sect2>

  <sect2>
   <title>Data do cache</title>
   <para>
    Você pode especificar a data mínima de um objeto recém-modificado a partir da qual o agente de camadas de cache o descarregará para o pool de armazenamento de suporte. Observe que isso apenas será aplicado se o cache realmente precisar descarregar/eliminar objetos:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_flush_age <replaceable>num_of_seconds</replaceable></screen>
   <para>
    Você pode especificar a data mínima de um objeto para ele ser eliminado da camada de cache:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_evict_age <replaceable>num_of_seconds</replaceable></screen>
  </sect2>

  <sect2 xml:id="ses-tiered-hitset-examples">
   <title>Exemplos</title>
   <sect3 xml:id="ses-tiered-hitset-examples-memory">
    <title>Pool de cache grande e pouca memória</title>
    <para>
     Em caso de muito armazenamento e pouca quantidade de RAM disponível, é possível promover todos os objetos para o pool de cache logo que são acessados. O conjunto de acertos se mantém pequeno. Veja a seguir um conjunto de valores de configuração de exemplo:
    </para>
<screen>hit_set_count = 1
hit_set_period = 3600
hit_set_fpp = 0.05
min_write_recency_for_promote = 0
min_read_recency_for_promote = 0</screen>
   </sect3>
   <sect3 xml:id="ses-tiered-hitset-examples-storage">
    <title>Pool de cache pequeno e muita memória</title>
    <para>
     Em caso de uma pequena quantidade de armazenamento, mas uma quantidade de memória disponível igualmente grande, é possível configurar a camada de cache para promover um número limitado de objetos para o pool de cache. Doze conjuntos de acertos, dos quais cada um é usado durante um período de 14.400 segundos, efetuam o monitoramento em um total de 48 horas. Se um objeto foi acessado nas últimas 8 horas, ele é promovido para o pool de cache. Portanto, o conjunto de valores de configuração de exemplo é:
    </para>
<screen>hit_set_count = 12
hit_set_period = 14400
hit_set_fpp = 0.01
min_write_recency_for_promote = 2
min_read_recency_for_promote = 2</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
