<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Instalação do NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editando</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes (sim)</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  O NFS Ganesha concede acesso de NFS ao Object Gateway ou CephFS. No SUSE Enterprise Storage 6, as versões 3 e 4 do NFS são suportadas. O NFS Ganesha é executado no espaço do usuário, em vez do kernel, e interage diretamente com o Object Gateway ou o CephFS.
 </para>
 <warning>
  <title>Acesso a Vários Protocolos</title>
  <para>
   Os clientes nativos CephFS e NFS não são restritos por bloqueios de arquivos obtidos por meio do Samba, e vice-versa. Os aplicativos que dependem do bloqueio de arquivos compatível com vários protocolos poderão ter os dados corrompidos se os caminhos de compartilhamento do Samba com suporte do CephFS forem acessados por outros meios.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparação</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Informações Gerais</title>
   <para>
    Para implantar o NFS Ganesha com êxito, você precisa adicionar uma função <literal>role-ganesha</literal> ao <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Para obter os detalhes, consulte a <xref linkend="policy-configuration"/>. O NFS Ganesha também precisa da <literal>role-rgw</literal> ou da <literal>role-mds</literal> no <filename>policy.cfg</filename>.
   </para>
   <para>
    Embora seja possível instalar e executar o servidor NFS Ganesha em um nó do Ceph existente, é recomendável executá-lo em um host dedicado com acesso ao cluster do Ceph. Normalmente, os hosts de clientes não fazem parte do cluster, mas eles precisam ter acesso via rede ao servidor NFS Ganesha.
   </para>
   <para>
    Para habilitar o servidor NFS Ganesha a qualquer momento após a instalação inicial, adicione a <literal>role-ganesha</literal> ao <filename>policy.cfg</filename> e reexecute pelo menos as fases 2 e 4 do DeepSea. Para obter os detalhes, consulte a <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    O NFS Ganesha é configurado por meio do arquivo <filename>/etc/ganesha/ganesha.conf</filename> que existe no nó do NFS Ganesha. No entanto, esse arquivo é sobregravado toda vez que a fase 4 do DeepSea é executada. Portanto, recomendamos editar o gabarito usado pelo Salt, que é o arquivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> no master Salt. Para obter detalhes sobre o arquivo de configuração, consulte o <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Resumo dos requisitos</title>
   <para>
    Os seguintes requisitos devem ser atendidos antes da execução das fases 2 e 4 do DeepSea para instalar o NFS Ganesha:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      No mínimo, um nó precisa receber a função <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Você pode definir apenas uma <literal>role-ganesha</literal> por minion.
     </para>
    </listitem>
    <listitem>
     <para>
      O NFS Ganesha precisa de um Object Gateway ou CephFS para funcionar.
     </para>
    </listitem>
    <listitem>
     <para>
      O NFS com base no kernel precisa ser desabilitado nos minions com a função <literal>role-ganesha</literal>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Exemplo de instalação</title>

  <para>
   Este procedimento fornece um exemplo de instalação que usa as FSAL (File System Abstraction Layers – Camadas de Abstração do Sistema de Arquivos) do Object Gateway e do CephFS do NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Se você não fez isto, execute as fases 0 e 1 do DeepSea antes de continuar esse procedimento.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Após executar a fase 1 do DeepSea, edite o <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> e adicione a linha
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Substitua <replaceable>NODENAME</replaceable> pelo nome de um nó no cluster.
    </para>
    <para>
     Verifique também se uma <literal>role-mds</literal> e uma <literal>role-rgw</literal> foram atribuídas.
    </para>
   </step>
   <step>
    <para>
     Execute pelo menos as fases 2 e 4 do DeepSea. É recomendável executar a fase 3 entre elas.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verifique se o NFS Ganesha está ativo confirmando se o serviço NFS Ganesha está em execução no nó do minion:
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configuração ativa-passiva de alta disponibilidade</title>

  <para>
   Esta seção apresenta um exemplo de como definir uma configuração ativa-passiva de dois nós dos servidores NFS Ganesha. A configuração requer a SUSE Linux Enterprise High Availability Extension. Os dois nós são denominados <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem>.
  </para>

  <important>
   <title>Colocation dos Serviços</title>
   <para>
    Os serviços que têm a própria tolerância a falhas e o próprio balanceamento de carga não devem ser executados em nós do cluster com limitação de serviços de failover. Portanto, não execute os serviços Ceph Monitor, Servidor de Metadados, iSCSI ou Ceph OSD em configurações de Alta Disponibilidade.
   </para>
  </important>

  <para>
   Para obter detalhes sobre a SUSE Linux Enterprise High Availability Extension, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Instalação básica</title>
   <para>
    Nessa configuração, <systemitem class="domainname">earth</systemitem> tem o endereço IP <systemitem class="ipaddress">192.168.1.1</systemitem>, e <systemitem class="domainname">mars</systemitem> tem o endereço <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Além disso, dois endereços IP virtuais flutuantes são usados, permitindo aos clientes se conectarem ao serviço independentemente do nó físico no qual está sendo executado. <systemitem class="ipaddress">192.168.1.10</systemitem> é usado para administração do cluster com Hawk2, e <systemitem class="ipaddress">192.168.2.1</systemitem> é usado exclusivamente para exportações NFS. Isso facilita aplicar as restrições de segurança mais tarde.
   </para>
   <para>
    O procedimento a seguir descreve a instalação de exemplo. Mais detalhes podem ser encontrados em <link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Prepare os nós do NFS Ganesha no master Salt:
     </para>
     <substeps>
      <step>
       <para>
        Execute as fases 0 e 1 do DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Atribua aos nós <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem> a função <literal>role-ganesha</literal> em <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Execute as fases de 2 a 4 do DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registre a SUSE Linux Enterprise High Availability Extension no <systemitem class="domainname">earth</systemitem> e no <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Instale o <package>ha-cluster-bootstrap</package> em ambos os nós:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Inicialize o cluster no <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Permita que o <systemitem class="domainname">mars</systemitem> ingresse no cluster:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Verifique o status do cluster. Você deve ver dois nós adicionados ao cluster:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Em ambos os nós, desabilite o início automático do serviço NFS Ganesha no momento da inicialização:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Inicie o shell do <command>crm</command> no <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Os próximos comandos são executados no shell do crm.
     </para>
    </step>
    <step>
     <para>
      No <systemitem class="domainname">earth</systemitem>, execute o shell do crm para executar os comandos a seguir a fim de configurar o recurso nos daemons do NFS Ganesha como clone do tipo de recurso systemd:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Crie um IPAddr2 primitivo com o shell do crm:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Para configurar um relacionamento entre o servidor NFS Ganesha e o IP Virtual flutuante, usamos colocalização e ordenação.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Use o comando <command>mount</command> do cliente para garantir que a configuração do cluster foi concluída:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Limpar recursos</title>
   <para>
    Em caso de falha em um dos nós do NFS Ganesha (por exemplo, <systemitem class="domainname">earth</systemitem>), corrija o problema e limpe o recurso. Apenas depois que o recurso estiver limpo, será possível fazer failback dele para o <systemitem class="domainname">earth</systemitem>, caso haja falha no <systemitem class="domainname">mars</systemitem> do NFS Ganesha.
   </para>
   <para>
    Para limpar o recurso:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Configurando o recurso de ping</title>
   <para>
    Pode acontecer de um servidor não conseguir acessar o cliente por causa de um problema de rede. Um recurso de ping pode detectar e minimizar esse problema. A configuração desse recurso é opcional.
   </para>
   <procedure>
    <step>
     <para>
      Defina o recurso de ping:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> é uma lista de endereços IP separados por caracteres de espaço. Será feito ping regular dos endereços IP para verificar se há interrupções de rede. Se um cliente sempre precisar de acesso ao servidor NFS, adicione-o à <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Crie um clone:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      O comando a seguir cria uma restrição para o serviço NFS Ganesha. Ele força o serviço a se mover para outro nó quando a <literal>host_list</literal> está inacessível.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>HA do NFS Ganesha e DeepSea</title>
   <para>
    O DeepSea não oferece suporte à configuração de HA do NFS Ganesha. Para evitar falha no DeepSea após a configuração de HA do NFS Ganesha, exclua a inicialização e interrupção do serviço NFS Ganesha da fase 4 do DeepSea:
   </para>
   <procedure>
    <step>
     <para>
      Copie <filename>/srv/salt/ceph/ganesha/default.sls</filename> para <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Remova a entrada <literal>.service</literal> do <filename>/srv/salt/ceph/ganesha/ha.sls</filename> de modo que fique com esta aparência:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Adicione a seguinte linha a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Configuração ativa-ativa</title>

  <para>
   Esta seção apresenta um exemplo da configuração ativa-ativa simples do NFS Ganesha. O objetivo é implantar dois servidores NFS Ganesha em camadas sobrepondo o mesmo CephFS existente. Os servidores serão dois nós do cluster do Ceph com endereços separados. Os clientes precisam ser distribuídos entre eles manualmente. O <quote>failover</quote> nessa configuração significa desmontar e remontar manualmente o outro servidor no cliente.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Pré-requisitos</title>
   <para>
    Para nossa configuração de exemplo, você precisa do seguinte:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Cluster do Ceph em execução. Consulte a <xref linkend="ceph-install-stack"/> para obter detalhes sobre como implantar e configurar o cluster do Ceph usando o DeepSea.
     </para>
    </listitem>
    <listitem>
     <para>
      Pelo menos um CephFS configurado. Consulte o <xref linkend="cha-ceph-as-cephfs"/> para obter mais detalhes sobre como implantar e configurar o CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Dois nós do cluster do Ceph com o NFS Ganesha implantado. Consulte o <xref linkend="cha-as-ganesha"/> para obter mais detalhes sobre como implantar o NFS Ganesha.
     </para>
     <tip>
      <title>Usar Servidores Dedicados</title>
      <para>
       Embora os nós do NFS Ganesha possam compartilhar recursos com outros serviços relacionados do Ceph, recomendamos o uso de servidores dedicados para melhorar o desempenho.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    Após implantar os nós do NFS Ganesha, verifique se o cluster está operante e se os pools padrão do CephFS estão lá:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Configurando o NFS Ganesha</title>
   <para>
    Verifique se ambos os nós do NFS Ganesha têm o arquivo <filename>/etc/ganesha/ganesha.conf</filename> instalado. Adicione os seguintes blocos, se eles ainda não existirem, ao arquivo de configuração para habilitar o RADOS como o back end de recuperação do NFS Ganesha.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   Você pode descobrir os valores para <replaceable>rados_pool</replaceable> e <replaceable>pool_namespace</replaceable> verificando a linha existente na configuração do formato:</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   O valor para a opção <replaceable>nodeid</replaceable> corresponde ao FQDN da máquina, e o valor das opções <replaceable>UserId</replaceable> e <replaceable>Ceph_Conf</replaceable> podem ser encontrados no bloco <replaceable>RADOS_URLS</replaceable> existente.
   </para>
   <para>
    Como as versões legadas do NFS nos impedem de aumentar o período extra com antecedência e, portanto, prolongar uma reinicialização do servidor, desabilitamos as opções para o NFS anterior à versão 4.2. Desabilitamos também a maior parte do cache do NFS Ganesha, já que as bibliotecas do Ceph já realizam um cache agressivo.
   </para>
   <para>
    O back end de recuperação “rados_cluster” armazena suas informações nos objetos RADOS. Embora não sejam muitos dados, desejamos que estejam altamente disponíveis. Usamos o pool de metadados do CephFS para essa finalidade e declaramos um novo namespace “ganesha” nele para diferenciá-lo dos objetos CephFS.
   </para>
   <note>
    <title>IDs de Nó do Cluster</title>
    <para>
     A maior parte da configuração é idêntica entre os dois hosts, no entanto, a opção <option>nodeid</option> no bloco “RADOS_KV” precisa ser uma string exclusiva para cada nó. Por padrão, o NFS Ganesha define <option>nodeid</option> como o nome de host do nó.
    </para>
    <para>
     Se você precisa usar valores fixos diferentes dos nomes de host, pode definir <option>nodeid = 'a'</option>, por exemplo, em um nó e <option>nodeid = 'b'</option> no outro nó.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Preenchendo o banco de dados extra do cluster</title>
   <para>
    Precisamos verificar se todos os nós no cluster se reconhecem. Isso é feito por meio de um objeto RADOS que é compartilhado entre os hosts. O NFS Ganesha usa esse objeto para comunicar o estado atual com relação a um período extra.
   </para>
   <para>
    O <package>pacote nfs-ganesha-rados-grace</package> contém uma ferramenta de linha de comando para consultar e manipular esse banco de dados. Se o pacote não estiver instalado em pelo menos um dos nós, instale-o com
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    Usaremos o comando para criar o BD e adicionar os dois <option>nodeid</option>s. Em nosso exemplo, os dois nós do NFS Ganesha são denominados <literal>ses6min1.example.com</literal> e <literal>ses6min2.example.com</literal>. Em um dos hosts do NFS Ganesha, execute
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    Isso cria o banco de dados extra e adiciona ambos “ses6min1.example.com” e “ses6min2.example.com” a ele. O último comando descarta o estado atual. Os hosts recém-adicionados sempre são considerados para impor o período extra, portanto, os dois têm o flag “E” definido. Os valores “cur” e “rec” mostram as épocas atual e de recuperação, que é como mantemos o controle dos hosts que podem realizar a recuperação e quando.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Reiniciando os serviços NFS Ganesha</title>
   <para>
    Em ambos os nós do NFS Ganesha, reinicie os serviços relacionados:
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    Após a reinicialização dos serviços, verifique o banco de dados extra:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>O Flag “E” foi Limpo</title>
    <para>
     Observe que os flags “E” dos dois nós foram limpos, indicando que não estão mais impondo o período extra e agora estão no modo de operação normal.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Conclusão</title>
   <para>
    Após concluir todas as etapas anteriores, você poderá montar o NFS exportado de um dos dois servidores NFS Ganesha e executar as operações normais do NFS neles.
   </para>
   <para>
    Nossa configuração de exemplo considera que, se um dos dois servidores NFS Ganesha ficar inativo, você o reiniciará manualmente em 5 minutos. Após 5 minutos, o Servidor de Metadados poderá cancelar a sessão mantida pelo cliente NFS Ganesha e todo o estado associado a ela. Se os recursos da sessão forem cancelados antes que o restante do cluster entre no período extra, os clientes do servidor talvez não possam recuperar todo o estado deles.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>Mais informações</title>

  <para>
   Mais informações podem ser encontradas no <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
