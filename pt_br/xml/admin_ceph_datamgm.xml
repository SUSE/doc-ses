<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha.storage.datamgm">
 <title>Gerenciamento de dados armazenados</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  O algoritmo CRUSH determina como armazenar e recuperar dados calculando os locais de armazenamento de dados. O CRUSH permite que os clientes do Ceph se comuniquem diretamente com os OSDs, sem a necessidade de um servidor centralizado ou um controlador. Com um método de armazenamento e recuperação de dados determinado por algoritmo, o Ceph evita um ponto único de falha, gargalo no desempenho e limite físico à escalabilidade.
 </para>
 <para>
  O CRUSH requer um mapa do cluster e usa o Mapa CRUSH para armazenar e recuperar dados de forma pseudo-aleatória nos OSDs com uma distribuição uniforme dos dados pelo cluster.
 </para>
 <para>
  Os mapas CRUSH contêm uma lista de OSDs, uma lista de “compartimentos de memória” para agregar os dispositivos em locais físicos e uma lista de regras que orientam como o CRUSH deve replicar os dados nos pools de um cluster do Ceph. Ao refletir a organização física adjacente da instalação, o CRUSH pode moldar (e, portanto, resolver) fontes potenciais de falhas de dispositivos correlacionados. As fontes comuns incluem proximidade física, fonte de energia compartilhada e rede compartilhada. Ao codificar essas informações no mapa do cluster, as políticas de posicionamento do CRUSH podem separar réplicas de objetos em diferentes domínios de falha enquanto ainda mantêm a distribuição desejada. Por exemplo, para evitar a possibilidade de falhas simultâneas, convém usar diferentes prateleiras, racks, fontes de alimentação, controladoras e/ou locais físicos para os dispositivos nos quais as réplicas de dados são armazenadas.
 </para>
 <para>
  Depois que você implantar um cluster do Ceph, um Mapa CRUSH padrão será gerado. Isso é bom para o seu ambiente de área de segurança do Ceph. No entanto, ao implantar um cluster de dados em grande escala, você deve considerar significativamente o desenvolvimento de um Mapa CRUSH personalizado, pois ele o ajudará a gerenciar o cluster do Ceph, melhorar o desempenho e garantir a segurança dos dados.
 </para>
 <para>
  Por exemplo, se um OSD ficar inativo, um Mapa CRUSH poderá ajudá-lo a localizar o data center físico, a sala, a fileira e o rack do host com o OSD que falhou, caso seja necessário usar o suporte no local ou substituir o hardware.
 </para>
 <para>
  Da mesma forma, o CRUSH pode ajudá-lo a identificar falhas mais rapidamente. Por exemplo, se todos os OSDs em determinado rack ficarem inativos ao mesmo tempo, a falha poderá estar associada ao comutador de rede ou à energia que abastece o rack, e não aos próprios OSDs.
 </para>
 <para>
  O Mapa CRUSH personalizado também pode ajudá-lo a identificar os locais físicos onde o Ceph armazena as cópias redundantes de dados, quando o(s) grupo(s) de posicionamento associado(s) ao host com falha está(ão) prejudicado(s).
 </para>
 <para>
  Há três seções principais para um Mapa CRUSH.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm.devices" xrefstyle="select: title"/> representam qualquer dispositivo de armazenamento de objetos, ou seja, o disco rígido correspondente a um daemon <systemitem>ceph-osd</systemitem>.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.buckets" xrefstyle="select: title"/> representam uma agregação hierárquica de locais de armazenamento (por exemplo, fileiras, racks, hosts, etc.) e seus pesos atribuídos.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.rules" xrefstyle="select: title"/> representam o modo de seleção dos compartimentos de memória.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm.devices">
  <title>Dispositivos</title>

  <para>
   Para mapear os grupos de posicionamento para OSDs, o Mapa CRUSH requer uma lista de dispositivos OSD (o nome do daemon OSD). A lista de dispositivos aparece primeiro no Mapa CRUSH.
  </para>

<screen>#devices
device <replaceable>num</replaceable> <replaceable>osd.name</replaceable></screen>

  <para>
   Por exemplo:
  </para>

<screen>#devices
device 0 osd.0
device 1 osd.1
device 2 osd.2
device 3 osd.3</screen>

  <para>
   Como regra geral, um daemon OSD é mapeado para um único disco.
  </para>
 </sect1>
 <sect1 xml:id="datamgm.buckets">
  <title>Compartimentos de memória</title>

  <para>
   Os mapas CRUSH contêm uma lista de OSDs, que podem ser organizados em “compartimentos de memória” para agregar os dispositivos em locais físicos.
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        OSD
       </para>
      </entry>
      <entry>
       <para>
        Um daemon OSD (osd.1, osd.2, etc.).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        Host
       </para>
      </entry>
      <entry>
       <para>
        Um nome de host que contém um ou mais OSDs.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        Chassis
       </para>
      </entry>
      <entry>
       <para>
        Chassis que compõe o rack.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        Rack
       </para>
      </entry>
      <entry>
       <para>
        Um rack de computador. O padrão é <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        Fileira
       </para>
      </entry>
      <entry>
       <para>
        Uma fileira em uma série de racks.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        Pdu
       </para>
      </entry>
      <entry>
       <para>
        Unidade de distribuição de energia.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        Pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        Sala
       </para>
      </entry>
      <entry>
       <para>
        Uma sala com racks e fileiras de hosts.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        Data center
       </para>
      </entry>
      <entry>
       <para>
        Um data center físico com salas.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        Região
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        Root
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    Você pode remover esses tipos e criar seus próprios tipos de compartimento de memória.
   </para>
  </tip>

  <para>
   As ferramentas de implantação do Ceph geram um Mapa CRUSH que contém um compartimento de memória para cada host e um pool denominado “default”, que é útil para o pool <literal>rbd</literal> padrão. Os tipos de compartimento de memória restantes oferecem um meio de armazenar informações sobre o local físico dos nós/compartimentos de memória, o que facilita bastante a administração do cluster em caso de mal funcionamento dos OSDs, dos hosts ou do hardware de rede e quando o administrador precisa acessar o hardware físico.
  </para>

  <para>
   Um compartimento de memória tem um tipo, um nome exclusivo (string), um ID único indicado por um número inteiro negativo, um peso relativo à capacidade total do(s) item(ns), o algoritmo do compartimento de memória (por padrão, <literal>straw</literal>) e o hash (por padrão, <literal>0</literal>, refletindo o Hash CRUSH <literal>rjenkins1</literal>). Um compartimento de memória pode ter um ou mais itens. Os itens podem ser constituídos de outros compartimentos de memória ou OSDs. Os itens podem ter um peso que reflete o peso relativo do item.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   O exemplo a seguir ilustra como você pode usar compartimentos de memória para agregar um pool e locais físicos, como data center, sala, rack e fileira.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw
        hash 0
        item osd.0 weight 1.00
        item osd.1 weight 1.00
}

row rack-1-row-1 {
        id -16
        alg straw
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

pool data {
        id -10
        alg straw
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm.rules">
  <title>Conjuntos de regras</title>

  <para>
   Os mapas CRUSH suportam a noção de “regras CRUSH”, que determinam o posicionamento dos dados em um pool. Para clusters grandes, convém criar muitos pools, em que cada um pode ter seu próprio conjunto de regras CRUSH e suas próprias regras. O Mapa CRUSH padrão tem uma regra para cada pool e um conjunto de regras atribuído a cada um dos pools padrão.
  </para>

  <note>
   <para>
    Na maioria dos casos, você não precisará modificar as regras padrão. Quando você cria um novo pool, o conjunto de regras padrão dele é 0.
   </para>
  </note>

  <para>
   Uma regra apresenta o seguinte formato:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      Um número inteiro. Classifica uma regra como pertencente a um conjunto de regras. Ativado quando o conjunto de regras é definido em um pool. Essa opção é obrigatória. O padrão é <literal>0</literal>.
     </para>
     <important>
      <para>
       Você precisa aumentar o número do conjunto de regras do padrão 0 continuamente; do contrário, o monitor relacionado poderá falhar.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      Uma string. Descreve uma regra para um disco rígido (replicado) ou um RAID. Essa opção é obrigatória. O padrão é <literal>replicado</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      Um número inteiro. Se um grupo de posicionamento gerar menos réplicas do que esse número, o CRUSH NÃO selecionará essa regra. Essa opção é obrigatória. O padrão é <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      Um número inteiro. Se um grupo de posicionamento gerar mais réplicas do que esse número, o CRUSH NÃO selecionará essa regra. Essa opção é obrigatória. O padrão é <literal>10</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable>
    </term>
    <listitem>
     <para>
      Usa um nome de compartimento de memória e inicia a iteração descendente na árvore. Essa opção é obrigatória. Para obter uma explicação sobre iteração na árvore, consulte a <xref linkend="datamgm.rules.step.iterate"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable>
    </term>
    <listitem>
     <para>
      <replaceable>target</replaceable> pode ser <literal>choose</literal> ou <literal>chooseleaf</literal>. Quando definido como <literal>choose</literal>, um número de compartimentos de memória é selecionado. <literal>chooseleaf</literal> seleciona diretamente os OSDs (nós folha) da subárvore de cada compartimento de memória no conjunto de compartimentos de memória.
     </para>
     <para>
      <replaceable>mode</replaceable> pode ser <literal>firstn</literal> ou <literal>indep</literal>. Consulte a <xref linkend="datamgm.rules.step.mode"/>.
     </para>
     <para>
      Seleciona o número de compartimentos de memória de determinado tipo. Em que N é o número de opções disponíveis, se <replaceable>num</replaceable> &gt; 0 &amp;&amp; &lt; N, escolha essa mesma quantidade de compartimentos de memória; se <replaceable>num</replaceable> &lt; 0, isso significa N - <replaceable>num</replaceable> e, se <replaceable>num</replaceable> == 0, escolha N compartimentos de memória (todos disponíveis). Segue <literal>step take</literal> ou <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Gera o valor atual e esvazia a pilha. Normalmente usado no fim de uma regra, mas também pode ser usado para estruturar árvores diferentes na mesma regra. Segue <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Para ativar uma ou mais regras com um número comum do conjunto de regras para um pool, defina o número do conjunto de regras para o pool.
   </para>
  </important>

  <sect2 xml:id="datamgm.rules.step.iterate">
   <title>Iteração pela árvore de nós</title>
   <para>
    É possível ver a estrutura definida com os compartimentos de memória como uma árvore de nós. Os compartimentos de memória são os nós, e os OSDs são as folhas da árvore.
   </para>
   <para>
    As regras no Mapa CRUSH definem como os OSDs são selecionados dessa árvore. Uma regra começa com um nó e, em seguida, faz a iteração descendente pela árvore para retornar um conjunto de OSDs. Não é possível definir qual ramificação precisa ser selecionada. Em vez disso, o algoritmo CRUSH garante que o conjunto de OSDs atende aos requisitos de replicação e distribui os dados igualmente.
   </para>
   <para>
    Com <literal>step take</literal> <replaceable>bucket</replaceable>, a iteração pela árvore de nós começa no compartimento de memória especificado (sem tipo de compartimento de memória). Se os OSDs de todas as ramificações na árvore tiverem que ser retornados, o compartimento de memória deverá ser a raiz. Do contrário, as etapas a seguir apenas fará a iteração na subárvore.
   </para>
   <para>
    Após <literal>step take</literal>, uma ou mais entradas <literal>step choose</literal> vêm a seguir na definição da regra. Cada <literal>step choose</literal> escolhe um número definido de nós (ou ramificações) do nó superior selecionado anteriormente.
   </para>
   <para>
    No fim, os OSDs selecionados são retornados com <literal>step emit</literal>.
   </para>
   <para>
    <literal>step chooseleaf</literal> é uma prática função que seleciona os OSDs diretamente das ramificações do compartimento de memória especificado.
   </para>
   <para>
    A <xref linkend="datamgm.rules.step.iterate.figure"/> mostra um exemplo de como o <literal>step</literal> é usado para iterar em uma árvore. As setas e os números laranjas correspondem a <literal>example1a</literal> e <literal>example1b</literal>, e os azuis correspondem a <literal>example2</literal> nas definições de regra a seguir.
   </para>
   <figure xml:id="datamgm.rules.step.iterate.figure">
    <title>Exemplo de árvore</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm.rules.step.mode">
   <title>firstn e indep</title>
   <para>
    Uma regra CRUSH define substituições para nós ou OSDs com falha (consulte a <xref linkend="datamgm.rules"/>). A palavra-chave <literal>step</literal> requer <literal>firstn</literal> ou <literal>indep</literal> como parâmetro. A <xref linkend="datamgm.rules.step.mode.indep.figure"/> apresenta um exemplo.
   </para>
   <para>
    O <literal>firstn</literal> adiciona nós de substituição ao fim da lista de nós ativos. No caso de um nó com falha, os seguintes nós saudáveis são deslocados para a esquerda para preencher a lacuna do nó com falha. Esse é o método padrão desejado para <emphasis>pools replicados</emphasis>, porque um nó secundário já tem todos os dados e, portanto, pode assumir as tarefas do nó principal imediatamente.
   </para>
   <para>
    O <literal>indep</literal> seleciona nós de substituição fixos para cada nó ativo. A substituição de um nó com falha não muda a ordem dos nós restantes. Esse é o método desejado para <emphasis>pools com codificação de eliminação</emphasis>. Nos pools com codificação de eliminação, os dados armazenados em um nó dependem da posição dele na seleção do nó. Quando a ordem dos nós muda, todos os dados nos nós afetados precisam ser realocados.
   </para>
   <note>
    <title>Pools de eliminação</title>
    <para>
     Verifique se uma regra que usa <literal>indep</literal> foi definida para cada <emphasis>pool com codificação de eliminação</emphasis>.
    </para>
   </note>
   <figure xml:id="datamgm.rules.step.mode.indep.figure">
    <title>Métodos de substituição de nó</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op.crush">
  <title>Manipulação de mapa CRUSH</title>

  <para>
   Esta seção apresenta os modos de manipulação do Mapa CRUSH básico. Por exemplo, editar um Mapa CRUSH, mudar parâmetros do Mapa CRUSH e adicionar/mover/remover um OSD.
  </para>

  <sect2>
   <title>Editando um mapa CRUSH</title>
   <para>
    Para editar um mapa CRUSH existente, faça o seguinte:
   </para>
   <procedure>
    <step>
     <para>
      Obtenha um Mapa CRUSH. Para obter o Mapa CRUSH para seu cluster, execute o seguinte:
     </para>
<screen><prompt>root # </prompt>ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      O Ceph gerará (<option>-o</option>) um Mapa CRUSH compilado com o nome de arquivo que você especificou. Como o Mapa CRUSH está em um formato compilado, você deve descompilá-lo antes que você possa editá-lo.
     </para>
    </step>
    <step>
     <para>
      Descompile um Mapa CRUSH. Para descompilar um Mapa CRUSH, execute o seguinte:
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      O Ceph descompilará (<option>-d</option>) o Mapa CRUSH compilado e o gerará (<option>-o</option>) com o nome de arquivo que você especificou.
     </para>
    </step>
    <step>
     <para>
      Edite pelo menos um dos parâmetros de Dispositivos, Compartimentos de Memória e Regras.
     </para>
    </step>
    <step>
     <para>
      Compile um Mapa CRUSH. Para compilar um Mapa CRUSH, execute o seguinte:
     </para>
<screen><prompt>cephadm &gt; </prompt>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      O Ceph armazenará um Mapa CRUSH compilado com o nome de arquivo que você especificou.
     </para>
    </step>
    <step>
     <para>
      Defina um Mapa CRUSH. Para definir o Mapa CRUSH para o cluster, execute o seguinte:
     </para>
<screen><prompt>root # </prompt>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      O Ceph inserirá o Mapa CRUSH compilado do nome de arquivo que você especificou como o Mapa CRUSH para o cluster.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="op.crush.addosd">
   <title>Adicionar/Mover um OSD</title>
   <para>
    Para adicionar ou mover um OSD no Mapa CRUSH de um cluster em execução, faça o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       Um número inteiro. O ID numérico do OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Uma string. O nome completo do OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       Um duplo. O peso do CRUSH para o OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pool</term>
     <listitem>
      <para>
       Um par de chave/valor. Por padrão, a hierarquia do CRUSH contém o pool padrão como raiz. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Pares de chave/valor. Você pode especificar o local do OSD na hierarquia do CRUSH.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    O exemplo a seguir adiciona <literal>osd.0</literal> à hierarquia ou move o OSD de um local anterior.
   </para>
<screen><prompt>root # </prompt>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op.crush.osdweight">
   <title>Ajustar o peso do CRUSH de um OSD</title>
   <para>
    Para ajustar o peso do CRUSH de um OSD no Mapa CRUSH de um cluster em execução, faça o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd crush reweight <replaceable>name</replaceable> <replaceable>weight</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Uma string. O nome completo do OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       Um duplo. O peso do CRUSH para o OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op.crush.osdremove">
   <title>Remover um OSD</title>
   <para>
    Para remover um OSD do Mapa CRUSH de um cluster em execução, faça o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd crush remove <replaceable>name</replaceable></screen>
   <variablelist>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       Uma string. O nome completo do OSD. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op.crush.movebucket">
   <title>Mover um compartimento de memória</title>
   <para>
    Para mover um compartimento de memória para outro local ou posição na hierarquia do mapa CRUSH, execute o seguinte:
   </para>
<screen><prompt>root # </prompt>ceph osd crush move <replaceable>bucket-name</replaceable> <replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable>, ...</screen>
   <variablelist>
    <varlistentry>
     <term>bucket-name</term>
     <listitem>
      <para>
       Uma string. O nome do compartimento de memória para mover/reposicionar. Essa opção é obrigatória.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Pares de chave/valor. Você pode especificar o local do compartimento de memória na hierarquia do CRUSH.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing">
  <title>Depuração</title>

  <para>
   Além de gerar várias cópias dos objetos, o Ceph garante a integridade dos dados por meio da <emphasis>depuração</emphasis> dos grupos de posicionamento. A depuração do Ceph equivale à execução do <command>fsck</command> na camada de armazenamento de objetos. Para cada grupo de posicionamento, o Ceph gera um catálogo de todos os objetos e compara cada objeto principal e suas réplicas para garantir que nenhum objeto esteja ausente ou seja incompatível. A depuração diária simples verifica o tamanho e os atributos dos objetos, enquanto a depuração semanal profunda lê os dados e usa checksums para garantir a integridade dos dados.
  </para>

  <para>
   A depuração é importante para manter a integridade dos dados, mas ela pode reduzir o desempenho. Você pode ajustar as seguintes configurações para aumentar ou diminuir as operações de depuração:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option>
    </term>
    <listitem>
     <para>
      O número máximo de operações de depuração simultâneas para o Ceph OSD. O padrão é 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>, <option>osd scrub end hour</option>
    </term>
    <listitem>
     <para>
      As horas do dia (0 a 24) que definem o intervalo para a execução da depuração. Por padrão, ela começa em 0 e termina em 24.
     </para>
     <important>
      <para>
       Se o intervalo de depuração do grupo de posicionamento exceder a configuração <option>osd scrub max interval</option>, a depuração será executada independentemente do intervalo definido para ela.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option>
    </term>
    <listitem>
     <para>
      Permite depurações durante a recuperação. Ao defini-la como “false”, a programação de novas depurações é desabilitada durante uma recuperação ativa. As depurações que já estão em execução continuam. Essa opção é útil para reduzir a carga em clusters ocupados. O padrão é “true”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option>
    </term>
    <listitem>
     <para>
      O tempo máximo em segundos antes que um thread de depuração esgote o tempo de espera. O padrão é 60.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option>
    </term>
    <listitem>
     <para>
      O tempo máximo em segundos antes que um thread de finalização da depuração esgote o tempo de espera. O padrão é 60*10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option>
    </term>
    <listitem>
     <para>
      A carga máxima normalizada. O Ceph não efetuará a depuração quando a carga do sistema (conforme definido pela proporção de <literal>getloadavg()</literal>/número de <literal>cpus online</literal>) for superior a esse número. O padrão é 0.5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option>
    </term>
    <listitem>
     <para>
      O intervalo mínimo em segundos para depuração do Ceph OSD quando a carga do cluster do Ceph está baixa. O padrão é 60*60*24 (uma vez por dia).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option>
    </term>
    <listitem>
     <para>
      O intervalo máximo em segundos para depuração do Ceph OSD independentemente da carga do cluster. 7*60*60*24 (uma vez por semana).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option>
    </term>
    <listitem>
     <para>
      O número mínimo de pacotes de armazenamento de objetos para depurar durante uma única operação. O Ceph bloqueia as gravações em um único pacote durante a depuração. O padrão é 5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option>
    </term>
    <listitem>
     <para>
      O número máximo de pacotes de armazenamento de objetos para depurar durante uma única operação. O padrão é 25.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option>
    </term>
    <listitem>
     <para>
      O tempo em modo adormecido antes da depuração do próximo grupo de pacotes. O aumento desse valor desacelera toda a operação de depuração, enquanto as operações de cliente são menos afetadas. O padrão é 0.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option>
    </term>
    <listitem>
     <para>
      O intervalo da depuração “profunda” (com leitura completa de todos os dados). A opção <option>osd scrub load threshold</option> não afeta essa configuração. O padrão é 60*60*24*7 (uma vez por semana).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option>
    </term>
    <listitem>
     <para>
      Adicione um atraso aleatório ao valor <option>osd scrub min interval</option> ao programar a próxima tarefa de depuração para um grupo de posicionamento. O atraso é um valor aleatório menor do que o resultado de <option>osd scrub min interval</option> * <option>osd scrub interval randomized ratio</option>. Portanto, a configuração padrão distribui as depurações quase aleatoriamente dentro do período permitido de [1, 1,5] * <option>osd scrub min interval</option>. O padrão é 0.5
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option>
    </term>
    <listitem>
     <para>
      Tamanho da leitura ao efetuar uma depuração profunda. O padrão é 524288 (512 KB).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="op.mixed_ssd_hdd">
  <title>SSDs e HDDs combinadas no mesmo nó</title>

  <para>
   Convém configurar um cluster do Ceph de modo que cada nó tenha uma combinação de SSDs e HDDs, com um pool de armazenamento em SSDs rápidas e outro em HDDs mais lentas. Para isso, o Mapa CRUSH precisa ser editado.
  </para>

  <para>
   O Mapa CRUSH padrão terá uma hierarquia simples, na qual a raiz padrão incluirá hosts e os hosts incluirão OSDs. Por exemplo:
  </para>

<screen><prompt>root # </prompt>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000 </screen>

  <para>
   Isso não faz distinção entre tipos de disco. Para dividir os OSDs em SSDs e HDDs, precisamos criar uma segunda hierarquia no Mapa CRUSH:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket ssd root</screen>

  <para>
   Após criar a nova raiz para SSDs, precisaremos adicionar hosts a ela. Isso significa a criação de novas entradas de host. No entanto, como o mesmo nome de host não pode aparecer mais do que uma vez no Mapa CRUSH, nomes de host falsos são usados. Esses nomes de host falsos não precisam ser resolvidos por DNS. O CRUSH não leva em consideração quais são os nomes de host, ele apenas precisa criar as hierarquias certas. A única coisa que <emphasis>realmente</emphasis> precisa ser mudada para suportar nomes de host falsos é que você deve definir
  </para>

<screen>osd crush update on start = false</screen>

  <para>
   no arquivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> e, em seguida, executar a Fase 3 do DeepSea para distribuir a mudança (consulte a <xref linkend="ds.custom.cephconf"/> para obter mais informações):
  </para>

<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>

  <para>
   Do contrário, os OSDs que você mover serão depois redefinidos ao seu local original na raiz padrão, e o cluster não terá o comportamento esperado.
  </para>

  <para>
   Depois que essa configuração for mudada, adicione os novos hosts falsos à raiz da SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add-bucket node1-ssd host
<prompt>root # </prompt>ceph osd crush move node1-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node2-ssd host
<prompt>root # </prompt>ceph osd crush move node2-ssd root=ssd
<prompt>root # </prompt>ceph osd crush add-bucket node3-ssd host
<prompt>root # </prompt>ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   Por fim, para cada OSD de SSD, mova o OSD para a raiz da SSD. Neste exemplo, consideramos que osd.0, osd.1 e osd.2 estão fisicamente hospedados em SSDs:
  </para>

<screen><prompt>root # </prompt>ceph osd crush add osd.0 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.0 1 root=ssd host=node1-ssd
<prompt>root # </prompt>ceph osd crush add osd.1 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.1 1 root=ssd host=node2-ssd
<prompt>root # </prompt>ceph osd crush add osd.2 1 root=ssd
<prompt>root # </prompt>ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   A hierarquia do CRUSH deve ter esta aparência:
  </para>

<screen><prompt>root # </prompt>ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   Agora, crie uma regra CRUSH direcionada à raiz da SSD:
  </para>

<screen><prompt>root # </prompt>ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   O <option>replicated_ruleset</option> padrão original (com ID 0) será direcionado às HDDs. O novo <option>ssd_replicated_ruleset</option> (com ID 1) será direcionado às SSDs.
  </para>

  <para>
   Quaisquer pools existentes ainda usarão as HDDs, pois elas estão na hierarquia padrão no Mapa CRUSH. Um novo pool pode ser criado para usar apenas as SSDs:
  </para>

<screen><prompt>root # </prompt>ceph osd pool create ssd-pool 64 64
<prompt>root # </prompt>ceph osd pool set ssd-pool crush_rule ssd_replicated_ruleset</screen>
 </sect1>
</chapter>
