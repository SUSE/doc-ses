<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha.storage.about">
 <title>SUSE Enterprise Storage e Ceph</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editando</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes (sim)</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  O SUSE Enterprise Storage é um sistema de armazenamento distribuído projetado para escalabilidade, confiabilidade e desempenho que usa a tecnologia Ceph. É possível executar um cluster do Ceph em servidores convencionais em uma rede comum, como Ethernet. O cluster tem capacidade de dimensionamento para milhares de servidores (posteriormente mencionados como nós) e dentro da faixa de petabytes. Ao contrário dos sistemas convencionais que têm tabelas de alocação para armazenar e buscar dados, o Ceph usa um algoritmo determinístico para alocar armazenamento de dados e não tem nenhuma estrutura centralizada de informações. Nos clusters de armazenamento, o Ceph considera a adição ou remoção de hardware a regra, e não a exceção. O cluster do Ceph automatiza as tarefas de gerenciamento, como distribuição e redistribuição de dados, replicação de dados, detecção de falhas e recuperação. O Ceph é autorreparável e autogerenciável, o que resulta na redução de overhead administrativo e orçamentário.
 </para>
 <para>
  Este capítulo apresenta uma visão geral resumida do SUSE Enterprise Storage e descreve brevemente os componentes mais importantes.
 </para>
 <tip>
  <para>
   Desde o SUSE Enterprise Storage 5, o único método de implantação de cluster é o DeepSea. Consulte o <xref linkend="ceph.install.saltstack"/> para obter detalhes sobre o processo de implantação.
  </para>
 </tip>
 <sect1 xml:id="storage.intro.features">
  <title>Recursos do Ceph</title>

  <para>
   O ambiente do Ceph tem os seguintes recursos:
  </para>

  <variablelist>
   <varlistentry>
    <term>Escalabilidade</term>
    <listitem>
     <para>
      O Ceph pode ser dimensionado para milhares de nós e pode gerenciar o armazenamento na faixa de petabytes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hardware convencional</term>
    <listitem>
     <para>
      Não há necessidade de hardware especial para executar um cluster do Ceph. Para obter os detalhes, consulte o <xref linkend="storage.bp.hwreq"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Autogerenciável</term>
    <listitem>
     <para>
      O cluster do Ceph é autogerenciável. Quando os nós são adicionados, removidos ou falham, o cluster redistribui os dados automaticamente. Ele também reconhece discos sobrecarregados.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Sem Ponto Único de Falha</term>
    <listitem>
     <para>
      Nenhum nó em um cluster armazena informações importantes separadamente. É possível configurar o número de redundâncias.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Software de código-fonte aberto</term>
    <listitem>
     <para>
      O Ceph é uma solução de software de código-fonte aberto e independente de hardware ou de fornecedores específicos.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.intro.core">
  <title>Componentes básicos</title>

  <para>
   Para aproveitar todas as vantagens do Ceph, é necessário conhecer alguns dos componentes e conceitos básicos. Esta seção apresenta algumas partes do Ceph que são mencionadas com mais frequência em outros capítulos.
  </para>

  <sect2 xml:id="storage.intro.core.rados">
   <title>RADOS</title>
   <para>
    O componente básico de Ceph é denominado <emphasis>RADOS</emphasis> <emphasis>(Reliable Autonomic Distributed Object Store)</emphasis>. Ele é responsável por gerenciar os dados armazenados no cluster. Normalmente, os dados no Ceph são armazenados como objetos. Cada objeto consiste em um identificador e nos dados.
   </para>
   <para>
    O RADOS oferece os seguintes métodos de acesso aos objetos armazenados que envolvem diversos casos de uso:
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       O Object Gateway é um gateway HTTP REST para armazenamento de objetos RADOS. Ele permite acesso direto aos objetos armazenados no cluster do Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Dispositivo de blocos RADOS</term>
     <listitem>
      <para>
       É possível acessar os Dispositivos de Blocos RADOS (RBD) como qualquer outro dispositivo de blocos. Eles podem ser usados em combinação com o <systemitem class="library">libvirt</systemitem> para fins de virtualização, por exemplo.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       O Sistema de Arquivos Ceph é compatível com POSIX.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem>
     </term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> é uma biblioteca que pode ser usada com várias linguagens de programação para criar um aplicativo capaz de interagir diretamente com o cluster de armazenamento.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    O Object Gateway e o RBD usam essa biblioteca, enquanto o CephFS estabelece interface direta com o RADOS. <xref linkend="storage.intro.core.rados.figure"/>
   </para>
   <figure xml:id="storage.intro.core.rados.figure">
    <title>Interfaces com o Armazenamento de Objetos do Ceph</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage.intro.core.crush">
   <title>CRUSH</title>
   <para>
    No centro de um cluster do Ceph está o algoritmo <emphasis>CRUSH</emphasis>. CRUSH é o acrônimo de <emphasis>Controlled Replication Under Scalable Hashing</emphasis>. Trata-se de uma função que processa a alocação de armazenamento e precisa de poucos parâmetros equivalentes. Isso significa que apenas uma pequena quantidade de informações é necessária para calcular a posição de armazenamento de um objeto. Os parâmetros são um mapa atual do cluster, incluindo o estado de saúde, algumas regras de posicionamento definidas pelo administrador e o nome do objeto que precisa ser armazenado ou recuperado. Com essas informações, todos os nós no cluster do Ceph são capazes de calcular onde um objeto e suas réplicas serão armazenados. Isso dificulta a gravação e a leitura de dados muito eficientes. O CRUSH tenta distribuir igualmente os dados a todos os nós do cluster.
   </para>
   <para>
    O <emphasis>mapa CRUSH</emphasis> contém todos os nós de armazenamento e as regras de posicionamento definidas pelo administrador para armazenar objetos no cluster. Ele define uma estrutura hierárquica que costuma corresponder à estrutura física do cluster. Por exemplo, os discos com dados estão em hosts, os hosts estão em racks, os racks estão em linhas e as linhas estão em data centers. É possível usar essa estrutura para definir <emphasis>domínios de falha</emphasis>. Em seguida, o Ceph garante que as replicações sejam armazenadas em ramificações diferentes de um domínio de falha específico.
   </para>
   <para>
    Se o domínio de falha for definido como rack, as replicações dos objetos serão distribuídas para racks diferentes. Isso pode reduzir as interrupções causadas por um switch com falha em um rack. Se uma unidade de distribuição de energia fornece uma linha de racks, o domínio de falha pode ser definido como linha. Quando a unidade de distribuição de energia falha, os dados replicados ainda ficam disponíveis em outras linhas.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.core.nodes">
   <title>Nós e daemons do Ceph</title>
   <para>
    No Ceph, os nós são servidores que trabalham para o cluster. Eles podem executar vários tipos de daemons. É recomendável executar apenas um tipo de daemon em cada nó, exceto os daemons MGR que podem ser combinados com MONs. Cada cluster requer no mínimo os daemons MON, MGR e OSD:
   </para>
   <variablelist>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       Os nós do <emphasis>Ceph Monitor</emphasis> (geralmente abreviado como <emphasis>MON</emphasis>) mantêm informações sobre o estado de saúde do cluster, um mapa de todos os nós e as regras de distribuição de dados (consulte a <xref linkend="storage.intro.core.crush"/>).
      </para>
      <para>
       Em caso de falhas ou conflitos, os nós do Ceph Monitor no cluster decidem, por maioria, quais informações estão corretas. Para formar uma maioria qualificada, é recomendável ter um número ímpar de nós do Ceph Monitor e, pelo menos, três deles.
      </para>
      <para>
       Se for usado mais de um site, os nós do Ceph Monitor deverão ser distribuídos para um número ímpar de sites. O número de nós do Ceph Monitor por site deve ser suficiente para que mais do que 50% dos nós do Ceph Monitor continuem funcionando em caso de falha de um site.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       O Ceph Manager (MGR) coleta as informações de estado do cluster inteiro. O daemon Ceph Manager é executado com os daemons do monitor. Ele fornece monitoramento adicional e estabelece interface com os sistemas externos de monitoramento e gerenciamento.
      </para>
      <para>
       O Ceph Manager não requer configuração adicional, além de garantir que esteja em execução. Você pode implantá-lo como uma função separada usando o DeepSea.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       O <emphasis>Ceph OSD</emphasis> é um daemon que processa <emphasis>Dispositivos de Armazenamento de Objetos</emphasis>, que são unidades físicas ou lógicas de armazenamento (discos rígidos ou partições). Os Dispositivos de Armazenamento de Objetos podem ser discos/partições físicos ou volumes lógicos. O daemon também se encarrega da replicação e redistribuição de dados em caso de nós adicionados ou removidos.
      </para>
      <para>
       Os daemons Ceph OSD comunicam-se com os daemons do monitor e informam a eles o estado dos outros daemons OSD.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para usar o CephFS, o Object Gateway, o NFS Ganesha ou o iSCSI Gateway, são necessários outros nós:
   </para>
   <variablelist>
    <varlistentry>
     <term>MDS (Metadata Server – Servidor de Metadados)</term>
     <listitem>
      <para>
       Os servidores de metadados armazenam metadados para o CephFS. Ao usar um MDS, você pode executar comandos básicos do sistema de arquivos, como <command>ls</command>, sem sobrecarregar o cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       O Ceph Object Gateway incluído no Object Gateway é um gateway HTTP REST para armazenamento de objetos RADOS. Ele é compatível com o OpenStack Swift e o Amazon S3 e tem seu próprio gerenciamento de usuários.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       O NFS Ganesha concede acesso de NFS ao Object Gateway ou CephFS. Ele é executado no espaço do usuário, em vez do kernel, e interage diretamente com o Object Gateway ou o CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI Gateway</term>
     <listitem>
      <para>
       iSCSI é um protocolo de rede de armazenamento que permite aos clientes enviar comandos SCSI para dispositivos de armazenamento SCSI (destinos) em servidores remotos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.intro.structure">
  <title>Estrutura de armazenamento</title>

  <sect2 xml:id="storage.intro.structure.pool">
   <title>Pool</title>
   <para>
    Os objetos armazenados em um cluster do Ceph são agrupados em <emphasis>pools</emphasis>. Os pools representam as partições lógicas do cluster para o mundo externo. É possível definir um conjunto de regras para cada pool. Por exemplo, o número necessário de replicações de cada objeto. A configuração padrão dos pools chama-se <emphasis>pool replicado</emphasis>.
   </para>
   <para>
    Normalmente, os pools contêm objetos, mas também podem ser configurados para agir como um RAID 5. Nessa configuração, os objetos são armazenados em pacotes com outros pacotes de codificação. Os pacotes de codificação contêm as informações redundantes. O número de dados e de pacotes de codificação pode ser definido pelo administrador. Nessa configuração, os pools são denominados <emphasis>pools com codificação de eliminação</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.pg">
   <title>Grupo de posicionamento</title>
   <para>
    Os PGs (<emphasis>Placement Groups – Grupos de Posicionamento</emphasis>) são usados para distribuição de dados em um pool. Ao criar um pool, determinado número de grupos de posicionamento é definido. Os grupos de posicionamento são usados internamente para agrupar objetos e são um fator importante para o desempenho de um cluster do Ceph. O PG de um objeto é determinado pelo nome do objeto.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.example">
   <title>Exemplo</title>
   <para>
    Esta seção mostra um exemplo simplificado de como o Ceph gerencia os dados (consulte a <xref linkend="storage.intro.structure.example.figure"/>). Esse exemplo não representa uma configuração recomendada para um cluster do Ceph. A configuração de hardware consiste em três nós de armazenamento ou Ceph OSDs (<literal>Host 1</literal>, <literal>Host 2</literal>, <literal>Host 3</literal>). Cada nó tem três discos rígidos que são usados como OSDs (<literal>osd.0</literal> a <literal>osd.9</literal>). Os nós do Ceph Monitor são ignorados neste exemplo.
   </para>
   <note>
    <title>Diferença entre Ceph OSD e OSD</title>
    <para>
     Enquanto <emphasis>Ceph OSD</emphasis> ou <emphasis>daemon Ceph OSD</emphasis> refere-se a um daemon executado em um nó, a palavra <emphasis>OSD</emphasis> refere-se ao disco lógico com o qual o daemon interage.
    </para>
   </note>
   <para>
    O cluster tem dois pools: <literal>Pool A</literal> e <literal>Pool B</literal>. Enquanto o Pool A replica os objetos apenas duas vezes, a resiliência do Pool B é mais importante e efetua três replicações de cada objeto.
   </para>
   <para>
    Quando um aplicativo coloca um objeto em um pool (por exemplo, pela API REST), um Grupo de Posicionamento (<literal>PG1</literal> a <literal>PG4</literal>) é selecionado com base no nome do pool e do objeto. Em seguida, o algoritmo CRUSH calcula em quais OSDs o objeto é armazenado, com base no Grupo de Posicionamento que contém o objeto.
   </para>
   <para>
    Neste exemplo, o domínio de falha foi definido como host. Isso garante que as replicações dos objetos sejam armazenadas em hosts diferentes. Dependendo do nível de replicação definido para um pool, o objeto é armazenado em dois ou três OSDs, que são usados pelo Grupo de Posicionamento.
   </para>
   <para>
    Um aplicativo que grava um objeto interage apenas com um Ceph OSD: o principal. O Ceph OSD principal efetua a replicação e confirma a conclusão do processo de gravação depois que todos os outros OSDs armazenaram o objeto.
   </para>
   <para>
    Em caso de falha no <literal>osd.5</literal>, todos os objetos no <literal>PG1</literal> ainda estarão disponíveis no <literal>osd.1</literal>. Logo que o cluster reconhece a falha em um OSD, outro OSD entra em ação. Neste exemplo, o <literal>osd.4</literal> é usado como substituto do <literal>osd.5</literal>. Os objetos armazenados no <literal>osd.1</literal> são replicados para o <literal>osd.4</literal> para restaurar o nível de replicação.
   </para>
   <figure xml:id="storage.intro.structure.example.figure">
    <title>Exemplo de Ceph de Pouco Dimensionamento</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Se um novo nó com novos OSDs for adicionado ao cluster, o mapa do cluster será modificado. Na sequência, a função CRUSH retorna locais diferentes para os objetos. Os objetos que recebem os novos locais serão realocados. Esse processo resulta no uso equilibrado de todos os OSDs.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about.bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore é um novo back-end de armazenamento padrão para Ceph, desde o SUSE Enterprise Storage 5. Ele apresenta melhor desempenho do que o FileStore, checksum completo de dados e compactação incorporada.
  </para>

  <para>
   O BlueStore gerencia um, dois ou três dispositivos de armazenamento. No caso mais simples, o BlueStore consome um único dispositivo de armazenamento principal. Normalmente, o dispositivo de armazenamento é particionado em duas partes:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Uma pequena partição denominada BlueFS que implementa funcionalidades do tipo do sistema de arquivos que o RocksDB exige.
    </para>
   </listitem>
   <listitem>
    <para>
     Normalmente, o restante do dispositivo é ocupado por uma partição grande. Ele é gerenciado diretamente pelo BlueStore e contém todos os dados reais. Normalmente, esse dispositivo principal é identificado por um link simbólico de bloco no diretório de dados.
    </para>
   </listitem>
  </orderedlist>

  <para>
   É possível também implantar o BlueStore em dois dispositivos adicionais:
  </para>

  <para>
   É possível usar o <emphasis>dispositivo WAL</emphasis> para o diário interno ou o registro write-ahead do BlueStore. Ele é identificado pelo link simbólico <literal>block.wal</literal> no diretório de dados. O uso de um dispositivo WAL separado será útil apenas se ele for mais rápido do que o dispositivo principal ou o dispositivo de BD. Por exemplo, quando:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     O dispositivo WAL é um NVMe, o dispositivo de BD é uma SSD e o dispositivo de dados é uma SSD ou HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Ambos os dispositivos WAL e de BD são SSDs separadas, e o dispositivo de dados é uma SSD ou HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   É possível usar um <emphasis>dispositivo de BD</emphasis> para armazenar metadados internos do BlueStore. O BlueStore (ou, em vez dele, o RocksDB incorporado) colocará o máximo de metadados que puder no dispositivo de BD para melhorar o desempenho. Mais uma vez, apenas será útil provisionar um dispositivo de BD compartilhado se ele for mais rápido do que o dispositivo principal.
  </para>

  <tip>
   <title>Planejar o tamanho do BD</title>
   <para>
    Planeje suficientemente o tamanho do dispositivo de BD. Se o dispositivo de BD ficar lotado, os metadados serão despejados no dispositivo principal, o que prejudica o desempenho do OSD.
   </para>
   <para>
    Você pode verificar se uma partição WAL/BD está ficando lotada e sendo despejada ao executar o comando <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command>. O valor <option>slow_used_bytes</option> mostra a quantidade de dados que está sendo despejada:
   </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage.moreinfo">
  <title>Informações adicionais</title>

  <itemizedlist>
   <listitem>
    <para>
     Como um projeto comunitário, o Ceph tem sua própria documentação online completa. Para os tópicos não encontrados neste manual, visite <link xlink:href="http://docs.ceph.com/docs/master/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     A publicação original <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> por <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> apresenta informações úteis sobre o funcionamento interno do Ceph. Principalmente na implantação de clusters em grande escala, trata-se de uma leitura recomendada. Você encontra essa publicação em <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
