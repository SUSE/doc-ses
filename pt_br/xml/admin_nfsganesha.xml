<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha: Exportar dados do Ceph pelo NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editando</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes (sim)</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha é um servidor NFS (consulte <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Sharing File Systems with NFS</link> (Compartilhando sistemas de arquivos com o NFS)) executado em um espaço de endereço de usuário, e não como parte do kernel do sistema operacional. Com o NFS Ganesha, você pode conectar seu próprio mecanismo de armazenamento, como Ceph, e acessá-lo de qualquer cliente NFS.
 </para>
 <para>
  Os compartimentos de memória do S3 são exportados para o NFS por usuário. Por exemplo, usando o caminho <filename><replaceable>NÓ_DO_GANESHA:</replaceable>/<replaceable>NOMEDEUSUÁRIO</replaceable>/<replaceable>NOMEDOCOMPARTIMENTODEMEMÓRIA</replaceable></filename>.
 </para>
 <para>
  Por padrão, um CephFS é exportado por meio do caminho <filename><replaceable>NÓ_DO_GANESHA:</replaceable>/cephfs</filename>.
 </para>
 <note>
  <title>Desempenho do NFS Ganesha</title>
  <para>
   Devido ao aumento da sobrecarga do protocolo e da latência adicional causado por saltos extras de rede entre o cliente e o armazenamento, o acesso ao Ceph por meio de um NFS Gateway pode reduzir significativamente o desempenho do aplicativo quando comparado aos clientes nativos do CephFS ou Object Gateway.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Instalação</title>

  <para>
   Para obter instruções, consulte o <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configuração</title>

  <para>
   Para obter uma lista de todos os parâmetros disponíveis no arquivo de configuração, consulte:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> para obter as opções da FSAL (File System Abstraction Layer – Camada de Abstração do Sistema de Arquivos) do CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> para obter as opções da FSAL do Object Gateway.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Esta seção inclui informações para ajudar você a configurar o servidor NFS Ganesha para exportar os dados de cluster acessíveis por meio do Object Gateway e do CephFS.
  </para>

  <para>
   A configuração do NFS Ganesha consiste em duas partes: serviço e exportações. A configuração de serviço é controlada por <filename>/etc/ganesha/ganesha.conf</filename>. Observe que as mudanças nesse arquivo serão sobregravadas quando a fase 4 do DeepSea for executada. Para mudar as configurações permanentemente, edite o arquivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> localizado no master Salt. A configuração de exportações é armazenada no cluster do Ceph como objetos RADOS.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Configuração de serviço</title>
   <para>
    A configuração de serviço é armazenada em <filename>/etc/ganesha/ganesha.conf</filename> e controla todas as configurações de daemon do NFS Ganesha, incluindo onde a configuração de exportações é armazenada no cluster do Ceph. Observe que as mudanças nesse arquivo serão sobregravadas quando a fase 4 do DeepSea for executada. Para mudar as configurações permanentemente, edite o arquivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> localizado no master Salt.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>Seção RADOS_URLS</title>
    <para>
     A seção <literal>RADOS_URLS</literal> configura o acesso do cluster do Ceph para leitura da configuração do NFS Ganesha dos objetos RADOS.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        Local do caminho do arquivo de configuração do Ceph.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        O ID de usuário cephx.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        O URL do objeto RADOS para observar notificações de recarregamento.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>Seção RGW</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Aponta para o arquivo <filename>ceph.conf</filename>. Durante a implantação com o DeepSea, não é necessário mudar esse valor.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        O nome do usuário do cliente Ceph que o NFS Ganesha utiliza.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        O nome do cluster do Ceph. Atualmente, o SUSE Enterprise Storage 6 suporta apenas um nome de cluster, que é <literal>ceph</literal> por padrão.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>URL do objeto RADOS</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     O NFS Ganesha suporta a leitura da configuração de um objeto RADOS. A diretiva <literal>%url</literal> permite especificar um URL RADOS que identifique o local do objeto RADOS.
    </para>
    <para>
     O URL RADOS pode ter dois formatos: <literal>rados://&lt;POOL&gt;/&lt;OBJETO&gt;</literal> ou <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJETO&gt;</literal>, em que <literal>POOL</literal> é o pool RADOS onde o objeto é armazenado, <literal>NAMESPACE</literal> é o namespace do pool onde o objeto é armazenado e <literal>OBJETO</literal> é o nome do objeto.
    </para>
    <para>
     Para suportar os recursos de gerenciamento do NFS Ganesha do Ceph Dashboard, você precisa seguir uma convenção de nome do objeto RADOS para cada daemon de serviço. O nome do objeto deve estar no formato <literal>conf-<replaceable>MINION_ID</replaceable></literal>, em que MINION_ID corresponde ao ID do minion Salt do nó em que o serviço está sendo executado.
    </para>
    <para>
     O DeepSea já se encarrega de gerar corretamente esse URL, e você não precisa fazer nenhuma modificação.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Mudando as portas padrão do NFS Ganesha</title>
    <para>
     Por padrão, o NFS Ganesha usa a porta 2049 para NFS e 875 para suporte a rquota. Para mudar os números de porta padrão, use as opções <option>NFS_Port</option> e <option>RQUOTA_Port</option> na seção <literal>NFS_CORE_PARAM</literal>. Por exemplo:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Configuração de exportações</title>
   <para>
    A configuração de exportações é armazenada como objetos RADOS no cluster do Ceph. Cada bloco de exportação é armazenado em seu próprio objeto RADOS denominado <literal>export-&lt;id&gt;</literal>, em que <literal>&lt;id&gt;</literal> deve corresponder ao atributo <literal>Export_ID</literal> da configuração de exportação. A associação entre as exportações e os serviços do NFS Ganesha é feita por meio dos objetos <literal>conf-MINION_ID</literal>. Cada objeto de serviço contém uma lista de URLs de RADOS para cada exportação feita por esse serviço. Um bloco de exportação tem a seguinte aparência:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    Para criar o objeto RADOS para o bloco de exportação acima, primeiro precisamos armazenar o código do bloco de exportação em um arquivo. Em seguida, podemos usar a ferramenta RADOS CLI para armazenar o conteúdo do arquivo gravado anteriormente em um objeto RADOS.
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    Após criar o objeto de exportação, poderemos associar a exportação a uma instância de serviço adicionando o URL RADOS correspondente do objeto de exportação ao objeto de serviço. As seções a seguir descrevem como configurar um bloco de exportação.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Seção de exportação principal</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Cada exportação precisa ter um “Export_Id” exclusivo (obrigatório).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Caminho de exportação no pool do CephFS relacionado (obrigatório). Isso permite que os subdiretórios sejam exportados do CephFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Caminho de exportação de destino do NFS (obrigatório para NFSv4). Ele define em qual caminho de exportação do NFS os dados exportados estarão disponíveis.
       </para>
       <para>
        Exemplo: com o valor <literal>/cephfs/</literal> e após a execução de
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        Os dados do CephFS estão disponíveis no diretório <filename>/mnt/cephfs/</filename> no cliente.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        “RO” para acesso apenas leitura, “RW” para acesso de leitura-gravação e “None” para nenhum acesso.
       </para>
       <tip>
        <title>Limitar Acesso aos Clientes</title>
        <para>
         Se você deixar <literal>Access_Type = RW</literal> na seção <literal>EXPORT</literal> e limitar o acesso a um cliente específico na seção <literal>CLIENT</literal>, outros clientes ainda poderão se conectar. Para desabilitar o acesso a todos os clientes e habilitar o acesso apenas a clientes específicos, defina <literal>Access_Type = None</literal> na seção <literal>EXPORT</literal> e, em seguida, especifique um modo de acesso menos restritivo para um ou mais clientes na seção <literal>CLIENT</literal>:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        Opção “squash” do NFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        “Camada de Abstração do Sistema de Arquivos” de exportação. Consulte a <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>Subseção FSAL</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Nome</term>
      <listitem>
       <para>
        Define o back end que o NFS Ganesha usa. Os valores permitidos são <literal>CEPH</literal> para CephFS ou <literal>RGW</literal> para Object Gateway. Dependendo da opção, uma <literal>role-mds</literal> ou <literal>role-rgw</literal> deve ser definida em <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Funções personalizadas do NFS Ganesha</title>

  <para>
   É possível definir funções personalizadas do NFS Ganesha para nós de cluster. Na sequência, essas funções são atribuídas aos nós em <filename>policy.cfg</filename>. As funções permitem o seguinte:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Nós separados do NFS Ganesha para acessar o Object Gateway e o CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     Atribuição de usuários diferentes do Object Gateway a nós do NFS Ganesha.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A existência de usuários diferentes do Object Gateway permite que os nós do NFS Ganesha acessem compartimentos de memória diferentes do S3. É possível usar os compartimentos de memória do S3 para controle de acesso. Nota: Os compartimentos de memória do S3 não devem ser confundidos com os do Ceph usados no Mapa CRUSH.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Usuários diferentes do Object Gateway para NFS Ganesha</title>
   <para>
    O seguinte procedimento de exemplo para o master Salt mostra como criar duas funções do NFS Ganesha com usuários diferentes do Object Gateway. Neste exemplo, as funções <literal>gold</literal> e <literal>silver</literal> são usadas, e o DeepSea já fornece arquivos de configuração de exemplo para elas.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Abra o arquivo <filename>/srv/pillar/ceph/stack/global.yml</filename> com o editor de sua preferência. Crie o arquivo se ele não existir.
     </para>
    </step>
    <step>
     <para>
      O arquivo precisa incluir as seguintes linhas:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      Mais tarde, essas funções poderão ser atribuídas em <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Crie um arquivo <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> e adicione o seguinte conteúdo:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Crie um arquivo <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> e adicione o seguinte conteúdo:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Agora, os gabaritos para o <filename>ganesha.conf</filename> precisam ser criados para cada função. O gabarito original do DeepSea é um bom ponto de partida. Crie duas cópias:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      As novas funções exigem chaveiros para acessar o cluster. Para conceder acesso, copie o <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copie o chaveiro para o Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      O Object Gateway também precisa da configuração para as diferentes funções:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Atribua as funções recém-criadas aos nós de cluster em <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Substitua <replaceable>NODE1</replaceable> e <replaceable>NODE2</replaceable> pelos nomes dos nós aos quais você deseja atribuir as funções.
     </para>
    </step>
    <step>
     <para>
      Execute as Fases de 0 a 4 do DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Separando a FSAL do CephFS e do Object Gateway</title>
   <para>
    O seguinte procedimento de exemplo para o master Salt mostra como criar duas novas funções diferentes que usam o CephFS e o Object Gateway:
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Abra o arquivo <filename>/srv/pillar/ceph/rgw.sls</filename> com o editor de sua preferência. Crie o arquivo se ele não existir.
     </para>
    </step>
    <step>
     <para>
      O arquivo precisa incluir as seguintes linhas:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      Mais tarde, essas funções poderão ser atribuídas em <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Agora, os gabaritos para o <filename>ganesha.conf</filename> precisam ser criados para cada função. O gabarito original do DeepSea é um bom ponto de partida. Crie duas cópias:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Edite o <filename>ganesha_rgw.conf.j2</filename> e remova a seção:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Edite o <filename>ganesha_cfs.conf.j2</filename> e remova a seção:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      As novas funções exigem chaveiros para acessar o cluster. Para conceder acesso, copie o <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      A linha <literal>caps mds = "allow *"</literal> pode ser removida do <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copie o chaveiro para o Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      O Object Gateway precisa da configuração para a nova função:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Atribua as funções recém-criadas aos nós de cluster em <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Substitua <replaceable>NODE1</replaceable> e <replaceable>NODE2</replaceable> pelos nomes dos nós aos quais você deseja atribuir as funções.
     </para>
    </step>
    <step>
     <para>
      Execute as Fases de 0 a 4 do DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Operações suportadas</title>
   <para>
    A interface RGW NFS suporta a maioria das operações em arquivos e diretórios, com as seguintes restrições:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links, incluindo links simbólicos, não são suportados.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Listas de controle de acesso (ACLs, Access Control Lists) do NFS não são suportadas.</emphasis>Propriedade e permissões de usuário e grupo do Unix <emphasis>são</emphasis> suportadas.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Diretórios não podem ser movidos ou renomeados.</emphasis>Você <emphasis>pode</emphasis> mover arquivos entre diretórios.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Apenas E/S de gravação completa e sequencial é suportada.</emphasis>Portanto, as operações de gravação são forçadas a ser uploads. Muitas operações comuns de E/S, como edição de arquivos no local, certamente falharão porque realizam armazenamentos não sequenciais. Há utilitários de arquivo que aparentemente gravam em sequência (por exemplo, algumas versões de <command>tar</command> do GNU), mas podem falhar por causa de armazenamentos não sequenciais infrequentes. Ao montar usando o NFS, a E/S sequencial de um aplicativo costuma ser forçada a realizar gravações sequenciais no servidor NFS por meio da montagem síncrona (opção <option>-o sync</option>). Os clientes NFS que não podem montar de maneira síncrona (por exemplo, Microsoft Windows*) não poderão fazer upload de arquivos.
     </para>
    </listitem>
    <listitem>
     <para>
      O NFS RGW suporta operações de leitura-gravação apenas para tamanhos de blocos menores do que 4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Iniciando ou reiniciando o NFS Ganesha</title>

  <para>
   Para habilitar e iniciar o serviço NFS Ganesha, execute:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Reinicie o NFS Ganesha com:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   Quando o NFS Ganesha é iniciado ou reiniciado, ele tem um tempo de espera extra de 90 segundos para o NFS v4. Durante o período extra, as novas solicitações dos clientes são ativamente rejeitadas. Portanto, os clientes podem enfrentar lentidão nas solicitações durante o período extra do NFS.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Definindo o nível de registro</title>

  <para>
   Mude o nível de depuração padrão <literal>NIV_EVENT</literal> editando o arquivo <filename>/etc/sysconfig/nfs-ganesha</filename>. Substitua <literal>NIV_EVENT</literal> por <literal>NIV_DEBUG</literal> ou <literal>NIV_FULL_DEBUG</literal>. O aumento do nível de detalhes do registro pode gerar grandes quantidades de dados nos arquivos de registro.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   É necessário reiniciar o serviço ao mudar o nível de registro.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verificando o compartilhamento NFS exportado</title>

  <para>
   Ao usar o NFS v3, você pode verificar se os compartilhamentos NFS foram exportados no nó do servidor NFS Ganesha:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Montando o compartilhamento NFS exportado</title>

  <para>
   Para montar o compartilhamento NFS exportado (conforme configurado na <xref linkend="ceph-nfsganesha-config"/>) em um host de cliente, execute:
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
