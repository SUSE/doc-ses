<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage.bp.hwreq">
 <title>Requisitos e recomendações de hardware</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sim</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Os requisitos de hardware do Ceph dependem bastante da carga de trabalho de E/S. Os requisitos e as recomendações de hardware a seguir devem ser considerados um ponto de partida para o planejamento detalhado.
 </para>
 <para>
  Em geral, as recomendações apresentadas nesta seção são baseadas em cada processo. Se houver vários processos localizados na mesma máquina, os requisitos de CPU, RAM, disco e rede deverão ser incrementados.
 </para>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>Nós de armazenamento de objetos</title>

  <sect2 xml:id="sysreq.osd">
   <title>Requisitos mínimos</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      No mínimo, 4 nós OSD, com 8 discos OSD cada, são necessários.
     </para>
    </listitem>
    <listitem>
     <para>
      Para OSDs que <emphasis>não</emphasis> usam BlueStore, 1 GB de RAM por terabyte de capacidade bruta do OSD é o mínimo exigido para cada nó de armazenamento OSD. É recomendável 1,5 GB de RAM por terabyte de capacidade bruta do OSD. Durante a recuperação, 2 GB de RAM por terabyte de capacidade bruta do OSD pode ser o ideal.
     </para>
     <para>
      Para OSDs que <emphasis>usam</emphasis> BlueStore, primeiro calcule o tamanho de RAM recomendado para os OSDs que não usam BlueStore, depois calcule 2 GB mais o tamanho do cache de RAM do BlueStore que é recomendado para cada processo de OSD e escolha o maior valor de RAM entre os dois resultados. Observe que o cache padrão do BlueStore é de 1 GB para HDD e de 3 GB para unidades SSD, por padrão. Em resumo, escolha o maior de:
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      ou
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      1,5 GHz de um núcleo de CPU lógico por OSD é o mínimo necessário para cada processo do daemon OSD. É recomendável 2 GHz por processo do daemon OSD. Observe que o Ceph executa um processo do daemon OSD por disco de armazenamento. Não são considerados discos reservados exclusivamente para uso como diários OSD, diários WAL, metadados omap ou qualquer combinação desses três casos.
     </para>
    </listitem>
    <listitem>
     <para>
      10 Gb Ethernet (duas interfaces de rede vinculadas a vários switches).
     </para>
    </listitem>
    <listitem>
     <para>
      Discos OSD nas configurações JBOD.
     </para>
    </listitem>
    <listitem>
     <para>
      Os discos OSD devem ser usados exclusivamente pelo SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Disco/SSD dedicado para o sistema operacional, preferencialmente em uma configuração de RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Se este host OSD hospedar parte de um pool de cache usado para criação de camada de cache, aloque pelo menos mais 4 GB de RAM.
     </para>
    </listitem>
    <listitem>
     <para>
      Por motivos de desempenho do disco, os nós OSD devem estar completamente vazios e não devem ser virtualizados.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Tamanho mínimo do disco</title>
   <para>
    Há dois tipos de espaço em disco necessários para execução no OSD: o espaço para o diário do disco (para FileStore) ou o dispositivo WAL/BD (para BlueStore) e o espaço principal para os dados armazenados. O valor mínimo (e padrão) para o diário/WAL/BD é de 6 GB. O espaço mínimo para os dados é de 5 GB, pois as partições menores do que 5 GB recebem automaticamente o peso de 0.
   </para>
   <para>
    Portanto, embora o espaço mínimo em disco para um OSD seja de 11 GB, não é recomendável um disco menor do que 20 GB, mesmo para fins de teste.
   </para>
  </sect2>

  <sect2 xml:id="rec.waldb.size">
   <title>Tamanho recomendado para o dispositivo WAL e BD do BlueStore</title>
   <tip>
    <title>Mais informações</title>
    <para>
     Consulte a <xref linkend="about.bluestore"/> para obter mais informações sobre o BlueStore.
    </para>
   </tip>
   <para>
    Veja a seguir várias regras para dimensionamento de dispositivo WAL/BD. Ao usar o DeepSea para implantar OSDs com o BlueStore, ele aplica as regras recomendadas automaticamente e notifica o administrador sobre o fato.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      10 GB do dispositivo de BD para cada Terabyte da capacidade do OSD (1/100º do OSD).
     </para>
    </listitem>
    <listitem>
     <para>
      Entre 500 MB e 2 GB para o dispositivo WAL. O tamanho do WAL depende do tráfego de dados e da carga de trabalho, não do tamanho do OSD. Se você souber que um OSD é fisicamente capaz de processar pequenas gravações e sobregravações a um throughput muito elevado, será preferível ter mais WAL do que menos WAL. Um dispositivo WAL de 1 GB é uma boa solução que atende à maioria das implementações.
     </para>
    </listitem>
    <listitem>
     <para>
      Se você pretende colocar o dispositivo WAL e BD no mesmo disco, recomendamos usar uma única partição para ambos os dispositivos, em vez de ter uma partição separada para cada um. Dessa forma, o Ceph pode usar o dispositivo BD também para operação de WAL. Portanto, o gerenciamento do espaço em disco é mais eficaz, pois o Ceph usa a partição BD para o WAL apenas quando há necessidade dela. Outra vantagem é que há pouca probabilidade de a partição WAL ficar cheia, e quando ela não for totalmente usada, seu espaço não será desperdiçado, mas usado para operação de BD.
     </para>
     <para>
      Para compartilhar o dispositivo BD com o WAL, <emphasis>não</emphasis> especifique o dispositivo WAL, mas apenas o dispositivo BD:
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      Se preferir, você poderá colocar o WAL em seu próprio dispositivo separado. Nesse caso, é recomendável o dispositivo mais rápido para a operação de WAL.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Usando SSD para diários OSD</title>
   <para>
    As SSDs (Solid-State Drives – Unidades de Estado Sólido) não têm partes móveis. Isso reduz o tempo de acesso aleatório e a latência de leitura enquanto acelera o throughput de dados. Como o preço delas por 1 MB é significativamente maior do que o preço dos discos rígidos giratórios, as SSDs são adequadas apenas para armazenamento menor.
   </para>
   <para>
    Os OSDs podem observar um aprimoramento significativo no desempenho armazenando o diário em uma SSD e os dados de objeto em um disco rígido separado.
   </para>
   <tip>
    <title>Compartilhando uma SSD para vários diários</title>
    <para>
     Como os dados do diário ocupam relativamente pouco espaço, você pode montar vários diretórios de diário em um único disco SSD. Com cada diário compartilhado, lembre-se de que o desempenho do disco SSD é reduzido. Não recomendamos compartilhar mais do que seis diários no mesmo disco SSD, e 12 nos discos NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>Número máximo recomendado de discos</title>
   <para>
    Você poderá ter quantos discos forem permitidos em um servidor. Há algumas coisas que devem ser consideradas na hora de planejar o número de discos por servidor:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Largura de banda de rede.</emphasis> Quanto mais discos você tiver em um servidor, mais dados deverão ser transferidos por placa(s) de rede para as operações de gravação em disco.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memória.</emphasis> Para atingir o melhor desempenho, reserve pelo menos 2 GB de RAM por terabyte de espaço em disco instalado.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolerância a falhas.</emphasis> Em caso de falha no servidor inteiro, quanto mais discos ele tiver, mais OSDs o cluster perderá temporariamente. Além disso, para manter as regras de replicação em execução, você precisa copiar todos os dados do servidor com falha para os outros nós no cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.mon">
  <title>Nós do monitor</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Pelo menos três nós do Ceph Monitor são necessários. O número de monitores deve ser sempre ímpar (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB de RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processador com quatro núcleos lógicos.
    </para>
   </listitem>
   <listitem>
    <para>
     Uma SSD ou outro tipo de armazenamento rápido o bastante é altamente recomendado para monitores, principalmente para o caminho <filename>/var/lib/ceph</filename> em cada nó do monitor, pois o quorum pode ficar instável com altas latências de disco. É recomendada a configuração de dois discos no RAID 1 para redundância. É recomendável o uso de discos separados ou, pelo menos, de partições de disco separadas para que os processos do monitor protejam o espaço em disco disponível do monitor contra eventos como arquivo de registro muito grande.
    </para>
   </listitem>
   <listitem>
    <para>
     Deve haver apenas um processo do monitor por nó.
    </para>
   </listitem>
   <listitem>
    <para>
     A combinação de nós OSD, de monitor ou do Object Gateway apenas será suportada se houver recursos de hardware suficientes disponíveis. Isso significa que os requisitos para todos os serviços precisam ser incrementados.
    </para>
   </listitem>
   <listitem>
    <para>
     Duas interfaces de rede vinculadas a vários switches.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>Nós do Object Gateway</title>

  <para>
   Os nós do Objeto Gateway devem ter de seis a oito núcleos de CPU e 32 GB de RAM (recomenda-se 64 GB). Quando outros processos são colocalizados na mesma máquina, os requisitos precisam ser incrementados.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.mds">
  <title>Nós do servidor de metadados</title>

  <para>
   O dimensionamento apropriado dos nós do Servidor de Metadados depende do caso de uso específico. Em geral, quanto mais arquivos abertos o Servidor de Metadados tiver que processar, mais CPU e RAM serão necessárias. Veja a seguir os requisitos mínimos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3G de RAM por cada daemon do Servidor de Metadados.
    </para>
   </listitem>
   <listitem>
    <para>
     Interface de rede vinculada.
    </para>
   </listitem>
   <listitem>
    <para>
     CPU de 2.5 GHz com, no mínimo, 2 núcleos.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.smaster">
  <title>Master Salt</title>

  <para>
   Pelo menos 4 GB de RAM e uma CPU quad-core são necessárias. Isso inclui o openATTIC em execução no master Salt. Para clusters grandes com centenas de nós, 6 GB de RAM é a sugestão.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>Nós do iSCSI</title>

  <para>
   Os nós do iSCSI devem ter de seis a oito núcleos de CPU e 16 GB de RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>Recomendações de rede</title>

  <para>
   O ideal é que o ambiente de rede em que você pretende executar o Ceph seja um conjunto vinculado de no mínimo duas interfaces de rede, que é dividido logicamente em uma parte pública e uma parte interna confiável por meio de VLANs. Se possível, o modo de vinculação recomendado é 802.3ad para fornecer largura de banda e resiliência máximas.
  </para>

  <para>
   A VLAN pública serve para fornecer o serviço aos clientes, enquanto a parte interna permite a comunicação de rede autenticada no Ceph. O principal motivo disso é que, embora o Ceph ofereça autenticação e proteção contra ataques depois que as chaves secretas estão em vigor, as mensagens usadas para configurar essas chaves poderão ser transferidas abertamente e serão vulneráveis.
  </para>

  <tip>
   <title>Nós configurados por DHCP</title>
   <para>
    Se os nós de armazenamento foram configurados por DHCP, os tempos de espera padrão talvez não sejam suficientes para que a rede seja configurada corretamente antes que os vários daemons do Ceph sejam iniciados. Se isso acontecer, os MONs e OSDs do Ceph não serão iniciados corretamente (a execução de <command>systemctl status ceph\*</command> resultará em erros "não é possível vincular"). Para evitar esse problema, é recomendável aumentar o tempo de espera do cliente DHCP para, pelo menos, 30 segundos em cada nó no seu cluster de armazenamento. Para fazer isso, mude as seguintes configurações em cada nó:
   </para>
   <para>
    Em <filename>/etc/sysconfig/network/dhcp</filename>, defina
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    Em <filename>/etc/sysconfig/network/config</filename>, defina
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage.bp.net.private">
   <title>Adicionando uma rede privada a um cluster em execução</title>
   <para>
    Se você não especificar uma rede de cluster durante a implantação do Ceph, ele assumirá um ambiente de rede pública única. Enquanto o Ceph funciona bem com uma rede pública, seu desempenho e segurança melhoram quando você define uma segunda rede privada de cluster. Para suportar duas redes, cada nó do Ceph precisa ter pelo menos duas placas de rede.
   </para>
   <para>
    Você precisará aplicar as mudanças a seguir a cada nó do Ceph. É relativamente rápido fazer isso em um cluster pequeno, mas pode ser muito demorado se você tem um cluster com centenas ou milhares de nós.
   </para>
   <procedure>
    <step>
     <para>
      Pare os serviços relacionados ao Ceph em cada nó do cluster.
     </para>
     <para>
      Adicione uma linha a <filename>/etc/ceph/ceph.conf</filename> para definir a rede de cluster. Por exemplo:
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      Se for necessário atribuir endereços IP estáticos ou anular as configurações de <option>rede de cluster</option> especificamente, você poderá fazer isso com o comando opcional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Verifique se a rede privada de cluster funciona conforme esperado no nível do OS.
     </para>
    </step>
    <step>
     <para>
      Inicie os serviços relacionados ao Ceph em cada nó do cluster.
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.net.subnets">
   <title>Nós do monitor em sub-redes diferentes</title>
   <para>
    Se os nós do monitor estiverem em várias sub-redes (por exemplo, localizados em salas distintas e atendidos por switches diferentes), você precisará ajustar o arquivo <filename>ceph.conf</filename> de acordo. Por exemplo, se os nós tiverem endereços IP 192.168.123.12, 1.2.3.4 e 242.12.33.12, adicione as seguintes linhas à seção global:
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    Além disso, se você precisar especificar um endereço ou uma rede pública por monitor, adicione uma seção <literal>[mon.<replaceable>X</replaceable>]</literal> por monitor:
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Limitações de nomeação</title>

  <para>
   Em geral, o Ceph não suporta caracteres não ASCII em arquivos de configuração, nomes de pool, nomes de usuário, etc. Ao configurar um cluster do Ceph, é recomendável usar apenas caracteres alfanuméricos simples (A-Z, a-z, 0-9) e pontuação mínima ('.', '-', '_’) em todos os nomes de objeto/configuração do Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD e monitor que compartilham um servidor</title>

  <para>
   Embora seja tecnicamente possível executar Ceph OSDs e Ceph Monitors no mesmo servidor em ambientes de teste, é altamente recomendável ter um servidor separado para cada nó do monitor em produção. O principal motivo é o desempenho: quanto mais OSDs o cluster tiver, mais operações de E/S os nós do monitor precisarão executar. E, quando um servidor for compartilhado entre um nó de monitor e OSD(s), as operações de E/S do OSD serão um fator limitador para o nó do monitor.
  </para>

  <para>
   Uma outra consideração é quando se deve compartilhar discos entre um OSD, um nó de monitor e o sistema operacional no servidor. A resposta é simples: se possível, dedique um disco separado ao OSD e um servidor separado a um nó de monitor.
  </para>

  <para>
   Embora o Ceph suporte OSDs com base em diretório, um OSD deve sempre ter um disco dedicado diferente do sistema operacional.
  </para>

  <tip>
   <para>
    Se for <emphasis>realmente</emphasis> necessário executar um nó OSD e de monitor no mesmo servidor, execute o monitor em um disco separado por meio da montagem do disco no diretório <filename>/var/lib/ceph/mon</filename> para obter um desempenho um pouco melhor.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses.bp.minimum_cluster">
  <title>Configuração mínima do cluster</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Quatro Nós de Armazenamento de Objetos
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10 Gb Ethernet (duas redes vinculadas a vários switches)
      </para>
     </listitem>
     <listitem>
      <para>
       32 OSDs por cluster de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       Diário OSD pode residir no disco OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Disco de OS dedicado para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       1 GB de RAM por TB de capacidade bruta do OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GHz por OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitors, Ceph Gateway e Servidores de Metadados podem residir em Nós de Armazenamento de Objetos
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Três nós do Ceph Monitor (requer SSD como unidade dedicada do OS)
        </para>
       </listitem>
       <listitem>
        <para>
         Nós de Ceph Monitors, Object Gateways e Servidores de Metadados requerem implantação redundante
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI Gateways, Object Gateways e Servidores de Metadados requerem mais 4 GB de RAM e quatro núcleos
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Nó de gerenciamento separado com 4 GB de RAM, quatro núcleos, capacidade de 1 TB
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses.bp.production_cluster">
  <title>Configuração recomendada do cluster de produção</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sete Nós de Armazenamento de Objetos
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nenhum nó único excede ~ 15% do total de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb Ethernet (quatro redes físicas vinculadas a vários switches)
      </para>
     </listitem>
     <listitem>
      <para>
       Mais de 56 OSDs por cluster de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       Discos de OS RAID 1 para cada nó de armazenamento OSD
      </para>
     </listitem>
     <listitem>
      <para>
       SSDs para Diário com o diário de SSD de proporção 6:1 para OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GB de RAM por TB de capacidade bruta do OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz por OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Nós dedicados de infraestrutura física
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Três nós do Ceph Monitor: 4 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
      </para>
     </listitem>
     <listitem>
      <para>
       Um nó de gerenciamento SES: 4 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
      </para>
     </listitem>
     <listitem>
      <para>
       Implantação física redundante de nós de gateway ou Servidor de Metadados:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Nós do Object Gateway: 32 GB de RAM, processador de 8 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nós do iSCSI Gateway: 16 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nós do Servidor de Metadados (um ativo/um standby ativo): 32 GB de RAM, processador de 8 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req.ses.other">
  <title>SUSE Enterprise Storage e outros produtos da SUSE</title>

  <para>
   Esta seção contém informações importantes sobre a integração do SUSE Enterprise Storage com outros produtos da SUSE.
  </para>

  <sect2 xml:id="req.ses.suma">
   <title>SUSE Manager</title>
   <para>
    O SUSE Manager e o SUSE Enterprise Storage não estão integrados, portanto, o SUSE Manager não pode gerenciar um cluster do SUSE Enterprise Storage neste momento.
   </para>
  </sect2>
 </sect1>
</chapter>
