<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>Requisitos e recomendações de hardware</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>sim</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Os requisitos de hardware do Ceph dependem bastante da carga de trabalho de E/S. Os requisitos e as recomendações de hardware a seguir devem ser considerados um ponto de partida para o planejamento detalhado.
 </para>
 <para>
  Em geral, as recomendações apresentadas nesta seção são baseadas em cada processo. Se houver vários processos localizados na mesma máquina, os requisitos de CPU, RAM, disco e rede deverão ser incrementados.
 </para>
 <sect1 xml:id="multi-architecture">
  <title>Configurações de várias arquiteturas</title>

  <para>
   O SUSE Enterprise Storage suporta as arquiteturas x86 e Arm. Ao considerar cada arquitetura, é importante notar que, de uma perspectiva de núcleo por OSD, frequência e RAM, não há diferença real entre as arquiteturas de CPU quanto ao dimensionamento.
  </para>

  <para>
   Assim como nos processadores x86 menores (sem servidor), os núcleos de desempenho mais baixo com base em Arm podem não oferecer uma experiência ideal, principalmente quando usados para pools com codificação de eliminação.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>Configuração mínima do cluster</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     No mínimo, quatro nós OSD, com oito discos OSD cada, são necessários.
    </para>
   </listitem>
   <listitem>
    <para>
     Três nós do Ceph Monitor (requer SSD como unidade dedicada do OS).
    </para>
   </listitem>
   <listitem>
    <para>
     iSCSI Gateways, Object Gateways e Servidores de Metadados requerem mais 4 GB de RAM e quatro núcleos.
    </para>
   </listitem>
   <listitem>
    <para>
     Nós de Ceph Monitors, Object Gateways e Servidores de Metadados requerem implantação redundante.
    </para>
   </listitem>
   <listitem>
    <para>
     Nó de Admin Separado com 4 GB de RAM, quatro núcleos, capacidade de 1 TB. Trata-se normalmente do nó master Salt. Os serviços e gateways do Ceph, como Ceph Monitor, Ceph Manager, Servidor de Metadados, Ceph OSD, Object Gateway ou NFS Ganesha, não são suportados no Nó de Admin.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>Nós de armazenamento de objetos</title>

  <sect2 xml:id="sysreq-osd">
   <title>Requisitos mínimos</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Recomendações da CPU:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1x Thread de CPU de 2 GHz por spinner
       </para>
      </listitem>
      <listitem>
       <para>
        2x Threads de CPU de 2 GHz por SSD
       </para>
      </listitem>
      <listitem>
       <para>
        4x Threads de CPU de 2 GHz por NVMe
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Redes separadas de 10 GbE (pública/cliente e back end), 4x 10 GbE necessário, 2x 25 GbE recomendado.
     </para>
    </listitem>
    <listitem>
     <para>
      RAM total necessária = número de OSDs x (1 GB + <option>osd_memory_target</option>) + 16 GB.
     </para>
     <para>
      Consulte o <xref linkend="config-auto-cache-sizing"/> para obter mais detalhes sobre <option>osd_memory_target</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Discos OSD nas configurações JBOD ou nas configurações individuais de RAID-0.
     </para>
    </listitem>
    <listitem>
     <para>
      Diário OSD pode residir no disco OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Os discos OSD devem ser usados exclusivamente pelo SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Disco/SSD dedicado para o sistema operacional, preferencialmente em uma configuração de RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Se este host OSD hospedar parte de um pool de cache usado para criação de camada de cache, aloque pelo menos mais 4 GB de RAM.
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph Monitors, Ceph Gateway e Servidores de Metadados podem residir em Nós de Armazenamento de Objetos.
     </para>
    </listitem>
    <listitem>
     <para>
      Por motivos de desempenho do disco, recomendamos o uso de nós OSD completamente vazios e máquinas físicas.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Tamanho mínimo do disco</title>
   <para>
    Há dois tipos de espaço em disco necessários para execução no OSD: o espaço para o diário do disco (para FileStore) ou o dispositivo WAL/BD (para BlueStore) e o espaço principal para os dados armazenados. O valor mínimo (e padrão) para o diário/WAL/BD é de 6 GB. O espaço mínimo para os dados é de 5 GB, pois as partições menores do que 5 GB recebem automaticamente o peso de 0.
   </para>
   <para>
    Portanto, embora o espaço mínimo em disco para um OSD seja de 11 GB, não é recomendável um disco menor do que 20 GB, mesmo para fins de teste.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Tamanho recomendado para o dispositivo WAL e BD do BlueStore</title>
   <tip>
    <title>Mais informações</title>
    <para>
     Consulte a <xref linkend="about-bluestore"/> para obter mais informações sobre o BlueStore.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      Recomendamos reservar 4 GB para o dispositivo WAL. O tamanho recomendado para o BD é de 64 GB para a maioria das cargas de trabalho.
     </para>
    </listitem>
    <listitem>
     <para>
      Se você pretende colocar o dispositivo WAL e BD no mesmo disco, recomendamos usar uma única partição para ambos os dispositivos, em vez de ter uma partição separada para cada um. Dessa forma, o Ceph pode usar o dispositivo BD também para operação de WAL. Portanto, o gerenciamento do espaço em disco é mais eficaz, pois o Ceph usa a partição BD para o WAL apenas quando há necessidade dela. Outra vantagem é que há pouca probabilidade de a partição WAL ficar cheia, e quando ela não é totalmente usada, o espaço dela não é desperdiçado, mas usado para operação do BD.
     </para>
     <para>
      Para compartilhar o dispositivo BD com o WAL, <emphasis>não</emphasis> especifique o dispositivo WAL, mas apenas o dispositivo BD.
     </para>
     <para>
      Encontre mais informações sobre como especificar um layout de OSD na <xref linkend="ds-drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>Usando SSD para diários OSD</title>
   <para>
    As SSDs (Solid-State Drives – Unidades de Estado Sólido) não têm partes móveis. Isso reduz o tempo de acesso aleatório e a latência de leitura enquanto acelera o throughput de dados. Como o preço delas por 1 MB é significativamente maior do que o preço dos discos rígidos giratórios, as SSDs são adequadas apenas para armazenamento menor.
   </para>
   <para>
    Os OSDs podem observar um aprimoramento significativo no desempenho armazenando o diário em uma SSD e os dados de objeto em um disco rígido separado.
   </para>
   <tip>
    <title>Compartilhando uma SSD para vários diários</title>
    <para>
     Como os dados do diário ocupam relativamente pouco espaço, você pode montar vários diretórios de diário em um único disco SSD. Com cada diário compartilhado, lembre-se de que o desempenho do disco SSD é reduzido. Não recomendamos compartilhar mais do que seis diários no mesmo disco SSD, e 12 nos discos NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Número máximo recomendado de discos</title>
   <para>
    Você poderá ter quantos discos forem permitidos em um servidor. Há algumas coisas que devem ser consideradas na hora de planejar o número de discos por servidor:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Largura de banda de rede.</emphasis> Quanto mais discos você tiver em um servidor, mais dados deverão ser transferidos por placa(s) de rede para as operações de gravação em disco.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memória.</emphasis> RAM acima de 2 GB é usada para o cache do BlueStore. Com o <option>osd_memory_target</option> padrão de 4 GB, o sistema tem um tamanho de cache inicial razoável para mídia giratória. Se você usa SSD ou NVME, considere aumentar o tamanho do cache e a alocação de RAM por OSD para maximizar o desempenho.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolerância a falhas.</emphasis> Em caso de falha no servidor inteiro, quanto mais discos ele tiver, mais OSDs o cluster perderá temporariamente. Além disso, para manter as regras de replicação em execução, você precisa copiar todos os dados do servidor com falha para os outros nós no cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Nós do monitor</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Pelo menos três nós do Ceph Monitor são necessários. O número de monitores deve ser sempre ímpar (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB de RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processador com quatro núcleos lógicos.
    </para>
   </listitem>
   <listitem>
    <para>
     Uma SSD ou outro tipo de armazenamento rápido o bastante é altamente recomendado para monitores, principalmente para o caminho <filename>/var/lib/ceph</filename> em cada nó do monitor, pois o quorum pode ficar instável com altas latências de disco. É recomendada a configuração de dois discos no RAID 1 para redundância. É recomendável o uso de discos separados ou, pelo menos, de partições de disco separadas para que os processos do monitor protejam o espaço em disco disponível do monitor contra eventos como arquivo de registro muito grande.
    </para>
   </listitem>
   <listitem>
    <para>
     Deve haver apenas um processo do monitor por nó.
    </para>
   </listitem>
   <listitem>
    <para>
     A combinação de nós OSD, de monitor ou do Object Gateway apenas será suportada se houver recursos de hardware suficientes disponíveis. Isso significa que os requisitos para todos os serviços precisam ser incrementados.
    </para>
   </listitem>
   <listitem>
    <para>
     Duas interfaces de rede vinculadas a vários switches.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Nós do Object Gateway</title>

  <para>
   Os nós do Objeto Gateway devem ter de seis a oito núcleos de CPU e 32 GB de RAM (recomenda-se 64 GB). Quando outros processos são colocalizados na mesma máquina, os requisitos precisam ser incrementados.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Nós do servidor de metadados</title>

  <para>
   O dimensionamento apropriado dos nós do Servidor de Metadados depende do caso de uso específico. Em geral, quanto mais arquivos abertos o Servidor de Metadados tiver que processar, mais CPU e RAM serão necessárias. Veja a seguir os requisitos mínimos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3 GB de RAM para cada daemon do Servidor de Metadados.
    </para>
   </listitem>
   <listitem>
    <para>
     Interface de rede vinculada.
    </para>
   </listitem>
   <listitem>
    <para>
     CPU de 2.5 GHz com, no mínimo, 2 núcleos.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Master Salt</title>

  <para>
   Pelo menos 4 GB de RAM e uma CPU quad-core são necessárias. Isso inclui a execução do Ceph Dashboard no Nó de Admin. Para clusters grandes com centenas de nós, 6 GB de RAM é a sugestão.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>Nós do iSCSI</title>

  <para>
   Os nós do iSCSI devem ter de seis a oito núcleos de CPU e 16 GB de RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>Recomendações de rede</title>

  <para>
   O ideal é que o ambiente de rede em que você pretende executar o Ceph seja um conjunto vinculado de no mínimo duas interfaces de rede, que é dividido logicamente em uma parte pública e uma parte interna confiável por meio de VLANs. Se possível, o modo de vinculação recomendado é 802.3ad para fornecer largura de banda e resiliência máximas.
  </para>

  <para>
   A VLAN pública serve para fornecer o serviço aos clientes, enquanto a parte interna permite a comunicação de rede autenticada no Ceph. O principal motivo disso é que, embora o Ceph ofereça autenticação e proteção contra ataques depois que as chaves secretas estão em vigor, as mensagens usadas para configurar essas chaves poderão ser transferidas abertamente e serão vulneráveis.
  </para>

  <tip>
   <title>Nós configurados por DHCP</title>
   <para>
    Se os nós de armazenamento foram configurados por DHCP, os tempos de espera padrão talvez não sejam suficientes para que a rede seja configurada corretamente antes que os vários daemons do Ceph sejam iniciados. Se isso acontecer, os MONs e OSDs do Ceph não serão iniciados corretamente (a execução de <command>systemctl status ceph\*</command> resultará em erros "não é possível vincular"). Para evitar esse problema, é recomendável aumentar o tempo de espera do cliente DHCP para, pelo menos, 30 segundos em cada nó no seu cluster de armazenamento. Para fazer isso, mude as seguintes configurações em cada nó:
   </para>
   <para>
    Em <filename>/etc/sysconfig/network/dhcp</filename>, defina
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    Em <filename>/etc/sysconfig/network/config</filename>, defina
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>Adicionando uma rede privada a um cluster em execução</title>
   <para>
    Se você não especificar uma rede de cluster durante a implantação do Ceph, ele assumirá um ambiente de rede pública única. Enquanto o Ceph funciona bem com uma rede pública, seu desempenho e segurança melhoram quando você define uma segunda rede privada de cluster. Para suportar duas redes, cada nó do Ceph precisa ter pelo menos duas placas de rede.
   </para>
   <para>
    Você precisará aplicar as mudanças a seguir a cada nó do Ceph. É relativamente rápido fazer isso em um cluster pequeno, mas pode ser muito demorado se você tem um cluster com centenas ou milhares de nós.
   </para>
   <procedure>
    <step>
     <para>
      Pare os serviços relacionados ao Ceph em cada nó do cluster.
     </para>
     <para>
      Adicione uma linha a <filename>/etc/ceph/ceph.conf</filename> para definir a rede de cluster. Por exemplo:
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      Se for necessário atribuir endereços IP estáticos ou anular as configurações de <option>rede de cluster</option> especificamente, você poderá fazer isso com o comando opcional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Verifique se a rede privada de cluster funciona conforme esperado no nível do OS.
     </para>
    </step>
    <step>
     <para>
      Inicie os serviços relacionados ao Ceph em cada nó do cluster.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>Nós do monitor em sub-redes diferentes</title>
   <para>
    Se os nós do monitor estiverem em várias sub-redes (por exemplo, localizados em salas distintas e atendidos por switches diferentes), você precisará ajustar o arquivo <filename>ceph.conf</filename> de acordo. Por exemplo, se os nós tiverem endereços IP 192.168.123.12, 1.2.3.4 e 242.12.33.12, adicione as seguintes linhas à seção <literal>global</literal> deles:
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    Além disso, se você precisar especificar um endereço ou uma rede pública por monitor, adicione uma seção <literal>[mon.<replaceable>X</replaceable>]</literal> para cada monitor:
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Limitações de nomeação</title>

  <para>
   Em geral, o Ceph não suporta caracteres não ASCII em arquivos de configuração, nomes de pool, nomes de usuário, etc. Ao configurar um cluster do Ceph, é recomendável usar apenas caracteres alfanuméricos simples (A-Z, a-z, 0-9) e pontuação mínima ('.', '-', '_’) em todos os nomes de objeto/configuração do Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>OSD e monitor que compartilham um servidor</title>

  <para>
   Embora seja tecnicamente possível executar Ceph OSDs e Ceph Monitors no mesmo servidor em ambientes de teste, é altamente recomendável ter um servidor separado para cada nó do monitor em produção. O principal motivo é o desempenho: quanto mais OSDs o cluster tiver, mais operações de E/S os nós do monitor precisarão executar. E, quando um servidor for compartilhado entre um nó de monitor e OSD(s), as operações de E/S do OSD serão um fator limitador para o nó do monitor.
  </para>

  <para>
   Uma outra consideração é quando se deve compartilhar discos entre um OSD, um nó de monitor e o sistema operacional no servidor. A resposta é simples: se possível, dedique um disco separado ao OSD e um servidor separado a um nó de monitor.
  </para>

  <para>
   Embora o Ceph suporte OSDs com base em diretório, um OSD deve sempre ter um disco dedicado diferente do sistema operacional.
  </para>

  <tip>
   <para>
    Se for <emphasis>realmente</emphasis> necessário executar um nó OSD e de monitor no mesmo servidor, execute o monitor em um disco separado por meio da montagem do disco no diretório <filename>/var/lib/ceph/mon</filename> para obter um desempenho um pouco melhor.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>Configuração recomendada do cluster de produção</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sete Nós de Armazenamento de Objetos
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nenhum nó único excede ~ 15% do total de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb Ethernet (quatro redes físicas vinculadas a vários switches)
      </para>
     </listitem>
     <listitem>
      <para>
       Mais de 56 OSDs por cluster de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       Discos de OS RAID 1 para cada nó de armazenamento OSD
      </para>
     </listitem>
     <listitem>
      <para>
       SSDs para Diário com o diário de SSD de proporção 6:1 para OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GB de RAM por TB de capacidade bruta do OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz por OSD para cada Nó de Armazenamento de Objetos
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Nós dedicados de infraestrutura física
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Três nós do Ceph Monitor: 4 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
      </para>
     </listitem>
     <listitem>
      <para>
       Um nó de gerenciamento SES: 4 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
      </para>
     </listitem>
     <listitem>
      <para>
       Implantação física redundante de nós de gateway ou Servidor de Metadados:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Nós do Object Gateway: 32 GB de RAM, processador de 8 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nós do iSCSI Gateway: 16 GB de RAM, processador de 4 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
       <listitem>
        <para>
         Nós do Servidor de Metadados (um ativo/um standby ativo): 32 GB de RAM, processador de 8 núcleos, SSDs RAID 1 para disco
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage 6 e outros produtos da SUSE</title>

  <para>
   Esta seção contém informações importantes sobre a integração do SUSE Enterprise Storage 6 com outros produtos da SUSE.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    O SUSE Manager e o SUSE Enterprise Storage não estão integrados, portanto, o SUSE Manager não pode gerenciar um cluster do SUSE Enterprise Storage neste momento.
   </para>
  </sect2>
 </sect1>
</chapter>
