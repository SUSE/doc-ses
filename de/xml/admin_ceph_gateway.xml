<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_gateway.xml" version="5.0" xml:id="cha.ceph.gw">

 <title>Ceph Object Gateway</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Dieses Kapitel enthält detaillierte Informationen zu Verwaltungsaufgaben, die für Object Gateway relevant sind, wie Prüfen des Servicestatus, Verwalten der Konten, Gateways an mehreren Standorten oder LDAP-Authentifizierung.
 </para>
 <sect1 xml:id="sec.ceph.rgw.limits">
  <title>Object Gateway-Beschränkungen und Benennungseinschränkungen</title>

  <para>
   Nachfolgend sehen Sie eine Liste der wichtigen Object Gateway-Beschränkungen:
  </para>

  <sect2 xml:id="ogw.limits.bucket">
   <title>Bucket-Einschränkungen</title>
   <para>
    Wenn Sie Object Gateway über die S3 API verwenden, sind Bucket-Namen auf DNS-fähige Namen mit einem Bindestrich "-" beschränkt. Wenn Sie Object Gateway über die Swift API verwenden, können Sie jede beliebige Kombination aus durch UTF-8 unterstützten Zeichen verwenden, mit Ausnahme des Schrägstrichs "/". Die maximale Länge eines Bucket-Namens beträgt 255 Zeichen. Bucket-Namen müssen eindeutig sein.
   </para>
   <tip>
    <title>DNS-fähige Bucket-Namen verwenden</title>
    <para>
     Auch wenn Sie auf UTF-8 basierende Bucket-Namen über die Swift API verwenden, empfehlen wir, Buckets unter Berücksichtigung der S3-Benennungseinschränkungen zu benennen, um Probleme beim Zugriff auf diesen Bucket über die S3 API zu vermeiden.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ogw.limits.object">
   <title>Einschränkungen für gespeicherte Objekte</title>
   <variablelist>
    <varlistentry>
     <term>Maximale Anzahl der Objekte pro Benutzer</term>
     <listitem>
      <para>
       Standardmäßig keine Einschränkung (beschränkt durch ~ 2^63).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Maximale Anzahl der Objekte pro Bucket</term>
     <listitem>
      <para>
       Standardmäßig keine Einschränkung (beschränkt durch ~ 2^63).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Maximale Größe eines Objekts zum Heraufladen/Speichern</term>
     <listitem>
      <para>
       Einzelne Uploads sind auf 5 GB beschränkt. Verwenden Sie Multipart für größere Objekte. Die maximale Anzahl der Multipart-Datenblöcke beträgt 10.000.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ogw.limits.http">
   <title>HTTP-Header-Beschränkungen</title>
   <para>
    Beschränkungen für HTTP-Header und Anforderungen hängen vom verwendeten Web-Frontend ab. Das standardmäßige CivetWeb beschränkt die Anzahl der HTTP-Header auf 64 und die Größe der HTTP-Header auf 16 KB.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw.deploy">
  <title>Bereitstellen des Object Gateways</title>

  <para>
   Wir empfehlen die Bereitstellung des Ceph Object Gateways über die DeepSea-Infrastruktur durch Hinzufügen der relevanten <literal>role-rgw [...]</literal>-Zeile(n) in der <filename>policy.cfg</filename>-Datei am Salt Master und Ausführen der erforderlichen DeepSea-Phasen.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Informationen zur Bereitstellung des Object Gateways während des Ceph Cluster-Bereitstellungsvorgangs finden Sie im <xref linkend="ceph.install.stack"/> und im <xref linkend="policy.configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Informationen zum Hinzufügen der Object Gateway-Rolle zu einem bereits bereitgestellten Cluster finden Sie in <xref linkend="salt.adding.services"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Ausführen des Object Gateway Service</title>

  <para>
   Der Object Gateway Service wird mit dem Kommando <command>systemctl</command> ausgeführt. Zur Ausführung des Object Gateway Service benötigen Sie <systemitem class="username">root</systemitem>-Berechtigungen. Beachten Sie, dass <replaceable>gateway_host</replaceable> der Hostname des Servers ist, dessen Object Gateway-Instanz ausgeführt werden muss.
  </para>

  <para>
   Für den Object Gateway Service werden die folgenden Unterkommandos unterstützt:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Gibt die Statusinformationen des Service aus.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Startet den Service, sofern er noch nicht ausgeführt wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Startet den Service neu.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stoppt den aktiven Service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Aktiviert den Service, sodass er automatisch bei Systemstart gestartet wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Deaktiviert den Service, sodass er nicht automatisch bei Systemstart gestartet wird.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.ceph.rgw.configuration">
  <title>Konfigurationsparameter</title>

  <para>
   Das Verhalten des Object Gateways lässt sich durch eine große Anzahl von Optionen in der <filename>ceph.conf</filename>-Datei beeinflussen. Nachfolgend sehen Sie eine Liste der wichtigsten Optionen: Eine vollständige Liste finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/radosgw/config-ref/"/>.
  </para>

  <variablelist>
   <varlistentry>
    <term>rgw_thread_pool_size</term>
    <listitem>
     <para>
      Anzahl der Threads für den CivetWeb Server. Erhöhen Sie den Wert, wenn Sie mehr Anforderungen verarbeiten müssen. Der Standardwert ist 100 Threads.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw_num_rados_handles</term>
    <listitem>
     <para>
      Die Anzahl der RADOS Cluster-Zugriffsnummern (weitere Informationen finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/rados/api/librados-intro/#step-2-configuring-a-cluster-handle"/>) für Object Gateway. Wenn Sie über eine konfigurierbare Anzahl von RADOS-Zugriffsnummern verfügen, wird die Leistung für alle Arten von Workloads erheblich verbessert. Jeder Object Gateway Worker-Thread würde nun eine RADOS-Zugriffsnummer für seine Gültigkeitsdauer auswählen müssen. Der Standardwert ist 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw_max_chunk_size</term>
    <listitem>
     <para>
      Die maximale Größe eines Datenblocks, der bei einer einzelnen Operation gelesen wird. Die Leistung bei der Verarbeitung von großen Objekten wird verbessert, wenn der Wert auf 4 MB (4194304) erhöht wird. Der Standardwert ist 128 KB (131072).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.ceph.rgw.configuration.notes">
   <title>Weitere Hinweise</title>
   <variablelist>
    <varlistentry>
     <term>rgw dns name</term>
     <listitem>
      <para>
       Wenn der Parameter <literal>rgw dns name</literal> zur <filename>ceph.conf</filename>-Datei hinzugefügt wird, müssen Sie sicherstellen, dass der S3 Client so konfiguriert ist, dass er Anforderungen zum Endpunkt leitet, der durch <literal>rgw dns name</literal> angegeben ist.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Verwalten des Zugriffs auf Object Gateway</title>

  <para>
   Die Kommunikation mit Object Gateway ist entweder über eine S3-fähige Schnittstelle oder eine Swift-fähige Schnittstelle möglich.  Die S3-Schnittstelle ist kompatibel mit einer großen Teilmenge der S3 RESTful API. Die Swift-Schnittstelle ist kompatibel mit einer großen Teilmenge der OpenStack Swift API.
  </para>

  <para>
   Bei beiden Schnittstellen müssen Sie einen bestimmten Benutzer erstellen und die relevante Client-Software installieren, um mit dem Gateway unter Verwendung des geheimen Schlüssels des Benutzers zu kommunizieren.
  </para>

  <sect2 xml:id="accessing.ragos.gateway">
   <title>Zugreifen auf Object Gateway</title>
   <sect3>
    <title>Zugriff auf die S3-Schnittstelle</title>
    <para>
     Für den Zugriff auf die S3-Schnittstelle benötigen Sie einen REST Client. <command>S3cmd</command> ist ein Kommandozeilen-S3 Client. Sie finden ihn im <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE Build Service</link>. Das Repository enthält Versionen für SUSE Linux Enterprise und für openSUSE-basierte Distributionen.
    </para>
    <para>
     Wenn Sie Ihren Zugriff auf die S3-Schnittstelle testen möchten, können Sie ein kleines Python-Skript schreiben. Das Skript stellt eine Verbindung zum Object Gateway her, erstellt einen neuen Bucket und listet alle Buckets auf. Die Werte für <option>aws_access_key_id</option> und <option>aws_secret_access_key</option> werden den Werten für <option>access_key</option> und <option>secret_key</option> entnommen, die vom Kommando <command>radosgw_admin</command> in <xref linkend="adding.s3.swift.users"/> zurückgegeben wurden.
    </para>
    <procedure>
     <step>
      <para>
       Installieren Sie das Paket <systemitem>phyton-boto</systemitem>:
      </para>
<screen>sudo zypper in python-boto</screen>
     </step>
     <step>
      <para>
       Erstellen Sie ein neues Python-Skript namens <filename>s3test.py</filename> mit folgendem Inhalt: <remark role="fixme">Provide script in RPM? Is it really necessary to create pool? This script is not necessary at all, remove it from documentation?</remark>
      </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
  print "{name}\t{created}".format(
  name = bucket.name,
  created = bucket.creation_date,
  )</screen>
      <para>
       Ersetzen Sie <literal>{hostname}</literal> durch den Hostnamen des Hosts, auf dem Sie den Object Gateway Service konfiguriert haben, beispielsweise <literal>gateway_host</literal>.
      </para>
     </step>
     <step>
      <para>
       Führen Sie das Skript aus:
      </para>
<screen>python s3test.py</screen>
      <para>
       Das Skript gibt in etwa Folgendes aus:
      </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Zugriff auf die Swift-Schnittstelle</title>
    <para>
     Für den Zugriff auf das Object Gateway über die Swift-Schnittstelle benötigen Sie den <command>swift</command>-Kommandozeilen-Client. In der entsprechenden Handbuchseite <command>man 1 swift</command> erfahren Sie mehr über dessen Kommandozeilenoptionen.
    </para>
    <para>
     Das Paket ist im "Public Cloud"-Modul für SUSE Linux Enterprise 12 SP3 enthalten. Vor Installation des Pakets müssen Sie das Modul aktivieren und das Software Repository aktualisieren:
    </para>
<screen>sudo SUSEConnect -p sle-module-public-cloud/12/x86_64
sudo zypper refresh</screen>
    <para>
     Führen Sie zur Installation des <command>swift</command>-Pakets folgendes Kommando aus:
    </para>
<screen>sudo zypper in python-swiftclient</screen>
    <para>
     Beim Swift-Zugang wird die folgende Syntax verwendet:
    </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
    <para>
     Ersetzen Sie <replaceable>IP_ADDRESS</replaceable> durch die IP-Adresse des Gateway Servers und <replaceable>swift_secret_key</replaceable> durch dessen Wert der Ausgabe des Kommandos <command>radosgw-admin key create</command>, das für den <systemitem>swift</systemitem>-Benutzer in <xref linkend="adding.s3.swift.users"/> ausgeführt wurde.
    </para>
    <para>
     Beispiel:
    </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
    <para>
     Die Ausgabe ist:
    </para>
<screen>my-new-bucket</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="s3.swift.accounts.managment">
   <title>Verwalten von S3- und Swift-Konten</title>
   <sect3 xml:id="adding.s3.swift.users">
    <title>Hinzufügen von S3- und Swift-Benutzern</title>
    <para>
     Sie müssen einen Benutzer, einen Zugriffsschlüssel und ein Geheimnis erstellen, um Endbenutzer für die Interaktion mit dem Gateway zu aktivieren. Es gibt zwei Arten von Benutzern: einen <emphasis>Benutzer</emphasis> und einen <emphasis>Unterbenutzer</emphasis>. <emphasis>Benutzer</emphasis> werden für die Interaktion mit der S3-Schnittstelle verwendet, während <emphasis>Unterbenutzer</emphasis> Benutzer der Swift-Schnittstelle sind. Jeder Unterbenutzer ist einem Benutzer zugeordnet.
    </para>
    <para>
     Benutzer können auch über die DeepSea-Datei <filename>rgw.sls</filename> hinzugefügt werden. Ein Beispiel finden Sie in <xref linkend="ceph.nfsganesha.customrole.rgw_multiusers"/>.
    </para>
    <para>
     Führen Sie zum Erstellen eines Swift-Benutzers folgende Schritte aus:
    </para>
    <procedure>
     <step>
      <para>
       Zum Erstellen eines Swift-Benutzers (in unserer Terminologie ein <emphasis>Unterbenutzer</emphasis>) müssen Sie zunächst den zugeordneten <emphasis>Benutzer</emphasis> erstellen.
      </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
      <para>
       Beispiel:
      </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
     </step>
     <step>
      <para>
       Zum Erstellen eines Unterbenutzers (Swift-Schnittstelle) für den Benutzer müssen Sie die Benutzer-ID (--uid=<replaceable>username</replaceable>), eine Unterbenutzer-ID und die Zugriffsstufe für den Unterbenutzer angeben.
      </para>
<screen>sudo radosgw-admin subuser create --uid=<replaceable>uid</replaceable> \
 --subuser=<replaceable>uid</replaceable> \
 --access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
      <para>
       Beispiel:
      </para>
<screen>sudo radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</screen>
     </step>
     <step>
      <para>
       Generieren Sie einen geheimen Schlüssel für den Benutzer.
      </para>
<screen>sudo radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</screen>
     </step>
     <step>
      <para>
       Beide Kommandos geben JSON-formatierte Daten mit dem Benutzerstatus aus. Notieren Sie sich die folgenden Zeilen und merken Sie sich den Wert des <literal>secret_key</literal>:
      </para>
<screen>"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
     </step>
    </procedure>
    <para/>
    <para>
     Beim Zugriff auf das Object Gateway über die S3-Schnittstelle müssen Sie einen S3-Benutzer erstellen, indem Sie folgendes Kommando ausführen:
    </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
    <para>
     Das Kommando erstellt auch den Zugriff und geheimen Schlüssel des Benutzers. Überprüfen Sie dessen Ausgabe nach den Schlüsselwörtern <literal>access_key</literal> und <literal>secret_key</literal> und deren Werte:
    </para>
<screen>[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</screen>
   </sect3>
   <sect3 xml:id="removing.s3.swift.users">
    <title>Entfernen von S3- und Swift-Benutzern</title>
    <para>
     Das Verfahren zum Löschen von Benutzern ist bei S3- und Swift-Benutzern in etwa gleich. Im Fall von Swift-Benutzern müssen Sie jedoch möglicherweise den Benutzer einschließlich dessen Unterbenutzer löschen.
    </para>
    <para>
     Geben Sie zum Entfernen eines S3- oder Swift-Benutzers (einschließlich aller Unterbenutzer) <option>user rm</option> und die Benutzer-ID im folgenden Kommando an:
    </para>
<screen>sudo radosgw-admin user rm --uid=example_user</screen>
    <para>
     Geben Sie zum Entfernen eines Unterbenutzers <option>subuser rm</option> und die Unterbenutzer-ID an.
    </para>
<screen>sudo radosgw-admin subuser rm --uid=example_user:swift</screen>
    <para>
     Die folgenden Optionen stehen Ihnen zur Verfügung:
    </para>
    <variablelist>
     <varlistentry>
      <term>--purge-data</term>
      <listitem>
       <para>
        Löscht alle Daten, die der Benutzer-ID zugeordnet sind.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>--purge-keys</term>
      <listitem>
       <para>
        Löscht alle Schlüssel, die der Benutzer-ID zugeordnet sind.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <tip>
     <title>Entfernen eines Unterbenutzers</title>
     <para>
      Wenn Sie einen Unterbenutzer entfernen, dann entfernen Sie auch den Zugriff auf die Swift-Schnittstelle. Der Benutzer bleibt im System.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="changing.s3.swift.users.password">
    <title>Ändern des Zugriffs und der geheimen Schlüssel von S3- und Swift-Benutzern</title>
    <para>
     Die Parameter <literal>access_key</literal> und <literal>secret_key</literal> kennzeichnen den Object Gateway-Benutzer für den Zugriff auf das Gateway. Ändern der bestehenden Benutzerschlüssel ist identisch mit dem Erstellen neuer Schlüssel, weil die alten Schlüssel überschrieben werden.
    </para>
    <para>
     Führen Sie für S3-Benutzer folgendes Kommando aus:
    </para>
<screen>sudo radosgw-admin key create --uid=<replaceable>example_user</replaceable> --key-type=s3 --gen-access-key --gen-secret</screen>
    <para>
     Führen Sie für Swift-Benutzer folgendes Kommando aus:
    </para>
<screen>sudo radosgw-admin key create --subuser=<replaceable>example_user</replaceable>:swift --key-type=swift --gen-secret</screen>
    <variablelist>
     <varlistentry>
      <term><option>--key-type=<replaceable>type</replaceable></option>
      </term>
      <listitem>
       <para>
        Gibt den Typ des Schlüssels an. Entweder <literal>swift</literal> oder <literal>s3</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-access-key</option>
      </term>
      <listitem>
       <para>
        Generiert einen Zugriffsschlüssel nach dem Zufallsprinzip (für S3-Benutzer standardmäßig).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-secret</option>
      </term>
      <listitem>
       <para>
        Generiert einen geheimen Schlüssel nach dem Zufallsprinzip.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--secret=<replaceable>key</replaceable></option>
      </term>
      <listitem>
       <para>
        Gibt einen geheimen Schlüssel an, beispielsweise einen manuell generierten.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="user.quota.managment">
    <title>Verwaltung des Benutzerkontingents</title>
    <para>
     Mit dem Ceph Object Gateway können Sie Kontingente für Benutzer und Buckets für Benutzer festlegen. Kontingente umfassen die maximale Anzahl der Objekte in einem Bucket sowie die maximale Speichergröße in Megabyte.
    </para>
    <para>
     Vor dem Aktivieren eines Benutzerkontingents müssen Sie zunächst dessen Parameter festlegen:
    </para>
<screen>sudo radosgw-admin quota set --quota-scope=user --uid=<replaceable>example_user</replaceable> \
 --max-objects=1024 --max-size=1024</screen>
    <variablelist>
     <varlistentry>
      <term><option>--max-objects</option>
      </term>
      <listitem>
       <para>
        Gibt die maximale Anzahl von Objekten an. Durch einen negativen Wert wird die Prüfung deaktiviert.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--max-size</option>
      </term>
      <listitem>
       <para>
        Gibt die maximale Anzahl von Bytes an. Durch einen negativen Wert wird die Prüfung deaktiviert.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--quota-scope</option>
      </term>
      <listitem>
       <para>
        Legt das Volumen für das Kontingent fest. Die Optionen sind <literal>bucket</literal> und <literal>user</literal>. Bucket Quota gelten für Buckets, die ein Benutzer besitzt. Benutzerkontingente gelten für einen Benutzer.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Sobald Sie ein Benutzerkontingent festgelegt haben, können Sie es aktivieren:
    </para>
<screen>sudo radosgw-admin quota enable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
    <para>
     So deaktivieren Sie ein Kontingent:
    </para>
<screen>sudo radosgw-admin quota disable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
    <para>
     So listen Sie Kontingenteinstellungen auf:
    </para>
<screen>sudo radosgw-admin user info --uid=<replaceable>example_user</replaceable></screen>
    <para>
     So aktualisieren Sie die Kontingentstatistik:
    </para>
<screen>sudo radosgw-admin user stats --uid=<replaceable>example_user</replaceable> --sync-stats</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.https">
  <title>Aktivieren von HTTPS/SSL für Object Gateways</title>

  <para>
   Zur Aktivierung der standardmäßigen Object Gateway-Rolle für die sichere Kommunikation über SSL benötigen Sie entweder ein von einer Zertifizierungsstelle ausgestelltes Zertifikat oder sie müssen ein eigensigniertes Zertifikat erstellen. Zum Konfigurieren eines Object Gateway mit aktiviertem HTTPs sind zwei Methoden verfügbar: die eine nutzt die Standardeinstellungen, die andere erweiterte Methode ermöglicht die Feinabstimmung der Einstellungen für HTTPS.
  </para>

  <sect2 xml:id="ogw.selfcert">
   <title>Erstellen eines eigensignierten Zertifikats</title>
   <tip>
    <para>
     Überspringen Sie diesen Abschnitt, wenn Sie bereits über ein gültiges Zertifikat verfügen, das von einer Zertifizierungsstelle signiert wurde.
    </para>
   </tip>
   <para>
    Standardmäßig erwartet DeepSea die Zertifikatsdatei am Salt Master unter<filename>/srv/salt/ceph/rgw/cert/rgw.pem</filename>. Es verteilt dann das Zertifikat an <filename>/etc/ceph/rgw.pem</filename> am Salt Minion mit der Object Gateway-Rolle, wo es von Ceph gelesen wird.
   </para>
   <para>
    Das folgende Verfahren beschreibt, wie ein eigensigniertes SSL-Zertifikat im Salt Master Node generiert wird.
   </para>
   <procedure>
    <step>
     <para>
      Fügen Sie für alle Hostnamen, durch die Ihr Object Gateway erkannt werden soll, die Option <option>subjectAltName</option> zum Abschnitt <literal>[v3_req]</literal> der Datei <filename>/etc/ssl/openssl.cnf</filename> hinzu:
     </para>
<screen>
[...]
[ v3_req ]
subjectAltName = ${ENV::SAN}
[...]
</screen>
    </step>
    <step>
     <para>
      Erstellen Sie den Schlüssel und das Zertifikat mit <command>openssl</command>. Stellen Sie vor <command>openssl</command> das Präfix <literal>env SAN=DNS:fqdn</literal>. Geben Sie alle Daten ein, die in Ihrem Zertifikat enthalten sein sollen. Wir empfehlen die Eingabe des FQDN als Eigenname. Verifizieren Sie vor dem Signieren des Zertifikats, dass "'X509v3 Subject Alternative Name:" in den angeforderten Erweiterungen enthalten ist und dass für das resultierende Zertifikat "X509v3 Subject Alternative Name:" festgelegt wurde.
     </para>
<screen>
<prompt>root@master # </prompt>env SAN=DNS:fqdn openssl req -x509 -nodes -days 1095 \
 -newkey rsa:4096 -keyout rgw.key -out /srv/salt/ceph/rgw/cert/rgw.pem
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw.ssl.simple">
   <title>Einfache HTTPS-Konfiguration</title>
   <para>
    Standardmäßig liest Ceph im Object Gateway Node das Zertifikat<filename>/etc/ceph/rgw.pem</filename> und verwendet Port 443 für die sichere SSL-Kommunikation. Wenn Sie diese Werte nicht ändern müssen, führen Sie die folgenden Schritte aus:
   </para>
   <procedure>
    <step>
     <para>
      Bearbeiten Sie <filename>/srv/pillar/ceph/stack/global.yml</filename> und fügen Sie folgende Zeile hinzu:
     </para>
<screen>
rgw_configurations: rgw-ssl
rgw_init: default-ssl
</screen>
    </step>
    <step>
     <para>
      Führen Sie die DeepSea-Phasen 2, 3 und 4 aus, um die Änderungen anzuwenden:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw.ssl.advanced">
   <title>Erweiterte HTTPS-Konfiguration</title>
   <para>
    Führen Sie folgende Schritte aus, wenn Sie die Standardwerte für SSL-Einstellungen des Object Gateways ändern müssen:
   </para>
   <procedure>
    <step>
     <para>
      Kopieren Sie die standardmäßige Object Gateway SSL-Konfiguration in das Unterverzeichnis <filename>ceph.conf.d</filename>:
     </para>
<screen>
<prompt>root@master # </prompt>cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf
</screen>
    </step>
    <step>
     <para>
      Bearbeiten Sie <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</filename> und ändern Sie die Standardoptionen, wie Portnummer oder Pfad zum SSL-Zertifikat entsprechend Ihrer Einrichtung.
     </para>
    </step>
    <step>
     <para>
      Führen Sie die DeepSea-Phasen 3 und 4 aus, um die Änderungen anzuwenden:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
    </step>
   </procedure>
   <tip xml:id="rgw.civetweb.multiport">
    <title>Bindung an mehrere Ports</title>
    <para>
     Der CivetWeb Server kann eine Bindung zu mehreren Ports herstellen. Dies ist nützlich, wenn Sie sowohl mit SSL-Verbindung als auch mit Nicht-SSL-Verbindung auf eine einzelne Object Gateway-Instanz zugreifen müssen. Trennen Sie bei Angabe der Ports die Nummern durch ein Pluszeichen ("+"). Die Zeile für eine Zwei-Port-Konfiguration entspricht dem folgenden Beispiel:
    </para>
<screen>[client.{{ client }}]
rgw_frontends = civetweb port=80+443s ssl_certificate=/etc/ceph/rgw.pem</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.sync">
  <title>Sync-Module</title>

  <para>
   Die mit Jewel eingeführte Object Gateway-Funktion für <emphasis>mehrere Standorte</emphasis> ermöglicht die Erstellung mehrerer Zonen und die Spiegelung der Daten und Metadaten zwischen diesen Zonen. <emphasis>Sync-Module</emphasis> werden auf das Framework für mehrere Standorte aufgesetzt und lassen die Weiterleitung von Daten und Metadaten zu einer anderen externen Schicht zu. Mit einem Sync-Modul kann eine Reihe von Aktionen durchgeführt werden, sobald eine Datenänderung vorgenommen wird (Metadaten-Optionen wie die Erstellung von Buckets oder Benutzern werden auch als Datenänderungen betrachtet). Da die RGW-Änderungen an mehreren Standorten letztendlich an Remote-Standorten konsistent sind, werden die Änderungen asynchron verteilt. Dies würde das Entsperren von Anwendungsfällen ermöglichen, wie Sichern des Objektspeichers in einem externen Cloud Cluster oder eine benutzerdefinierte Sicherungslösung mit Bandlaufwerken. Metadaten werden dabei u.a. in Elasticsearch indiziert.
  </para>

  <sect2 xml:id="ceph.rgw.sync.zones">
   <title>Synchronisieren von Zonen</title>
   <para>
    Eine Sync-Modul-Konfiguration wird lokal in einer Zone vorgenommen. Das Sync-Modul bestimmt, ob die Zone Daten exportiert oder in einer anderen Zone geänderte Daten nur nutzen darf. Ab Luminous sind die unterstützten Sync-Plugins <literal>elasticsearch</literal>, <literal>rgw</literal> (das standardmäßige Sync-Plugin zum Synchronisieren von Daten zwischen den Zonen) und <literal>log</literal> (ein Trivial-Sync-Plugin zum Protokollieren der Metadaten-Operation in den Remote-Zonen). In den folgenden Abschnitten verwenden wir das Beispiel einer Zone mit dem Sync-Modul <literal>elasticsearch</literal>. Bei der Konfiguration eines anderen Sync-Plugins wäre das Verfahren ähnlich.
   </para>
   <note>
    <title>Standardmäßiges Sync-Plugin</title>
    <para>
     <literal>rgw</literal> ist das standardmäßige Sync-Plugin. Es muss nicht explizit konfiguriert werden.
    </para>
   </note>
   <sect3 xml:id="ceph.rgw.sync.zones.req">
    <title>Anforderungen und Annahmen</title>
    <para>
     Nehmen wir eine einfache Konfiguration für mehrere Standorte an, wie in <xref linkend="ceph.rgw.fed"/> beschrieben. Sie besteht aus zwei Zonen: <literal>us-east</literal> und <literal>us-west</literal>. Nun fügen wir eine dritte Zone <literal>us-east-es</literal> hinzu. Diese Zone verarbeitet nur die Metadaten der anderen Standorte. Diese Zone kann sich im selben Ceph Cluster befinden wie <literal>us-east</literal> oder auch in einer anderen Zone. Diese Zone würde nur die Metadaten aus anderen Zonen nutzen. Object Gateways in dieser Zone verarbeiten Endbenutzeranforderungen nicht direkt.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.sync.zones.configure">
    <title>Konfigurieren von Sync-Modulen</title>
    <procedure>
     <step>
      <para>
       Erstellen Sie die dritte Zone so ähnlich wie die Zonen, die in <xref linkend="ceph.rgw.fed"/> beschrieben sind. Beispiel:
      </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key={system-key} --secret={secret} --endpoints=http://rgw-es:80
      </screen>
     </step>
     <step>
      <para>
       Ein Sync-Modul kann für diese Zone mit folgendem Kommando konfiguriert werden:
      </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --tier-type={tier-type} \
--tier-config={set of key=value pairs}
      </screen>
     </step>
     <step>
      <para>
       Beispiel im Sync-Modul <literal>elasticsearch</literal>:
      </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1
      </screen>
      <para>
       Die verschiedenen unterstützten "tier-config"-Optionen finden Sie in <xref linkend="ceph.rgw.sync.elastic"/>.
      </para>
     </step>
     <step>
      <para>
       Aktualisieren Sie schließlich den Zeitraum:
      </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> period update --commit
      </screen>
     </step>
     <step>
      <para>
       Starten Sie nun "radosgw" in der Zone:
      </para>
<screen>
<prompt>root # </prompt><command>systemctl</command> start ceph-radosgw@rgw.`hostname -s`
<prompt>root # </prompt><command>systemctl</command> enable ceph-radosgw@rgw.`hostname -s`
      </screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.sync.elastic">
   <title>Speichern von Metadaten in Elasticsearch</title>
   <para>
    Dieses Sync-Modul schreibt die Metadaten aus anderen Zonen zu Elasticsearch. Ab Luminous ist es die JSON-Datei der Datenfelder, die aktuell in Elasticsearch gespeichert ist.
   </para>
<screen>
{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}
   </screen>
   <sect3 xml:id="ceph.rgw.sync.elastic.config">
    <title>Parameter zur Konfiguration des Elasticsearch-Schichttyps</title>
    <variablelist>
     <varlistentry>
      <term>endpoint</term>
      <listitem>
       <para>
        Gibt den Elasticsearch Server-Endpunkt für den Zugriff an.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_shards</term>
      <listitem>
       <para>
        <emphasis>(Ganzzahl)</emphasis> Die Anzahl der Shards, mit denen Elasticsearch beim Initialisieren der Datensynchronisierung konfiguriert wird. Beachten Sie, dass dies nach der Initialisierung nicht mehr geändert werden kann. Zum Ändern muss der Elasticsearch-Index neu aufgebaut und der Datensynchronisierungsvorgang neu initialisiert werden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_replicas</term>
      <listitem>
       <para>
        <emphasis>(Ganzzahl)</emphasis> Die Anzahl der Reproduktionen, mit denen Elasticsearch beim Initialisieren der Datensynchronisierung konfiguriert wird.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>explicit_custom_meta</term>
      <listitem>
       <para>
        <emphasis>(true | false)</emphasis> Gibt an, ob alle benutzerdefinierten Metadaten der Benutzer indiziert werden oder ob Benutzer (auf Bucket-Ebene) konfigurieren müssen, welche Kundenmetadaten-Einträge indiziert werden sollten. Standardmäßig ist dies auf "false" festgelegt.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>index_buckets_list</term>
      <listitem>
       <para>
        <emphasis>(durch Komma getrennte Liste von Zeichenketten)</emphasis> Falls leer, werden alle Buckets indiziert. Andernfalls werden nur die hier genannten Buckets indiziert. Es ist möglich, Bucket-Präfixe (zum Beispiel "foo*") oder Bucket-Suffixe (zum Beispiel "*bar") anzugeben.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>approved_owners_list</term>
      <listitem>
       <para>
        <emphasis>(durch Komma getrennte Liste von Zeichenketten)</emphasis> Falls leer, werden die Buckets aller Eigentümer indiziert (falls keine anderen Einschränkungen vorhanden sind). Andernfalls werden nur die Buckets bestimmter Eigentümer indiziert. Suffixe und Präfixe können ebenfalls angegeben werden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>override_index_path</term>
      <listitem>
       <para>
        <emphasis>(Zeichenkette)</emphasis> Falls nicht leer, wird diese Zeichenkette als Elasticsearch-Indexpfad verwendet. Andernfalls wird der Indexpfad bei Initialisierung der Synchronisierung festgelegt und generiert.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.rgw.sync.elastic.query">
    <title>Metadatenabfragen</title>
    <para>
     Da der Elasticsearch Cluster nun Objektmetadaten speichert, ist es wichtig, dass der Elasticsearch-Endpunkt nicht öffentlich gemacht wird und nur für Cluster-Administratoren zugänglich ist. Die Anzeige von Metadatenabfragen für den Endbenutzer selbst ist problematisch, weil der Benutzer nur seine Metadaten abfragen soll und nicht die Metadaten anderer Benutzer. Der Elasticsearch Cluster müsste Benutzer ähnlich wie RGW authentifizieren, was ein Problem darstellt.
    </para>
    <para>
     Ab Luminous kann RGW in der Metadaten-Masterzone nun die Endbenutzeranforderungen verarbeiten. Dadurch ist es möglich, dass der Elasticsearch-Endpunkt nicht öffentlich zugänglich ist und gleichzeitig das Authentifizierungs- und Autorisierungsproblem gelöst ist, weil RGW selbst die Endbenutzeranforderungen authentifizieren kann. Zu diesem Zweck führt RGW eine neue Abfrage in den Bucket APIs ein, die Elasticsearch-Anforderungen verarbeiten können. Diese Anforderungen müssen an die Metadaten-Masterzone gesendet werden.
    </para>
    <variablelist>
     <varlistentry>
      <term>Abrufen einer Elasticsearch-Abfrage</term>
      <listitem>
<screen>
GET /<replaceable>BUCKET</replaceable>?query={query-expr}
       </screen>
       <para>
        Anforderungsparameter:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          max-keys: maximale Anzahl der Einträge, die zurückgegeben werden sollen
         </para>
        </listitem>
        <listitem>
         <para>
          marker: Kennzeichnung für die Paginierung
         </para>
        </listitem>
       </itemizedlist>
<screen>
expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]
       </screen>
       <para>
        "op" steht für eines der folgenden Zeichen: &lt;, &lt;=, ==, &gt;=, &gt;
       </para>
       <para>
        Beispiel:
       </para>
<screen>
GET /?query=name==foo
       </screen>
       <para>
        Gibt alle indizierten Schlüssel zurück, für die der Benutzer über Berechtigungen verfügt. Sie werden als "foo" bezeichnet. Die Ausgabe ist eine Liste von Schlüsseln im XML-Format, die der S3-Antwort auf die Anforderung zum Auflisten von Buckets ähnlich ist.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Konfigurieren benutzerdefinierter Metadatenfelder</term>
      <listitem>
       <para>
        Definieren Sie, welche benutzerdefinierten Metadateneinträge (unter dem angegebenen Bucket) indiziert werden und welchen Typ diese Schlüssel haben sollen. Dies ist erforderlich, damit RGW die angegebenen benutzerdefinierten Metadatenwerte indiziert, wenn die explizite Indizierung von benutzerdefinierten Metadaten konfiguriert ist. Andernfalls ist es erforderlich in Fällen, in denen die indizierten Metadatenschlüssel keine Zeichenketten sind.
       </para>
<screen>
POST /<replaceable>BUCKET</replaceable>?mdsearch
x-amz-meta-search: &lt;key [; type]&gt; [, ...]
       </screen>
       <para>
        Mehrere Metadatenfelder müssen durch Komma getrennt werden. Ein Typ kann für ein Feld mit ";" erzwungen werden. Die aktuell zulässigen Typen sind Zeichenkette (Standardeinstellung), Ganzzahl und Datum. Beispiel: Wenn Sie die benutzerdefinierten Objektmetadaten "x-amz-meta-year" als Ganzzahl, "x-amz-meta-date" als Datum und "x-amz-meta-title" als Zeichenkette indizieren möchten, müssen Sie Folgendes festlegen:
       </para>
<screen>
POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Löschen einer benutzerdefinierten Metadatenkonfiguration</term>
      <listitem>
       <para>
        Löschen Sie eine benutzerdefinierte Konfiguration eines Metadaten-Buckets.
       </para>
<screen>
DELETE /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Abrufen einer benutzerdefinierten Metadatenkonfiguration</term>
      <listitem>
       <para>
        Rufen Sie eine benutzerdefinierte Konfiguration eines Metadaten-Buckets ab.
       </para>
<screen>
GET /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.ldap">
  <title>LDAP-Authentifizierung</title>

  <para>
   Abgesehen von der standardmäßigen Authentifizierung lokaler Benutzer kann Object Gateway Benutzer auch über LDAP Server Services authentifizieren.
  </para>

  <sect2 xml:id="ceph.rgw.ldap.how_works">
   <title>Authentifizierungsverfahren</title>
   <para>
    Das Object Gateway extrahiert den Berechtigungsnachweis des Benutzers aus einem Token. Ein Suchfilter wird aus dem Benutzernamen erstellt. Das Object Gateway durchsucht anhand des konfigurierten Servicekontos das Verzeichnis nach einem passenden Eintrag. Wird ein Eintrag gefunden, versucht das Object Gateway, eine Bindung zum gefundenen eindeutigen Namen mit dem Passwort aus dem Token herzustellen. Wenn der Berechtigungsnachweis gültig ist, wird die Bindung hergestellt und das Object Gateway gewährt den Zugriff.
   </para>
   <para>
    Die Anzahl der Benutzer lässt sich begrenzen. Legen Sie dazu die Basis für die Suche auf eine bestimmte organisatorische Einheit fest oder geben Sie einen benutzerdefinierten Suchfilter an, beispielsweise eine bestimmte Gruppenmitgliedschaft, benutzerdefinierte Objektklassen oder Attribute.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.reqs">
   <title>Anforderungen</title>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>LDAP oder Active Directory</emphasis>: Eine aktive LDAP-Instanz, auf die Object Gateway zugreifen kann.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Servicekonto</emphasis>: LDAP-Berechtigungen, die vom Object Gateway mit Suchberechtigungen verwendet werden sollen.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Benutzerkonto</emphasis>: Mindestens ein Benutzerkonto im LDAP-Verzeichnis.
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <title>LDAP-Benutzer und lokale Benutzer dürfen sich nicht überlappen</title>
    <para>
     Für lokale Benutzer und für Benutzer, die mit LDAP authentifiziert werden, sollten nicht dieselben Benutzernamen verwendet werden. Das Object Gateway kann sie nicht unterscheiden und behandelt sie als ein und denselben Benutzer.
    </para>
   </important>
   <tip>
    <title>Integritätsprüfungen</title>
    <para>
     Verifizieren Sie das Servicekonto oder die LDAP-Verbindung mit dem Dienstprogramm <command>ldapsearch</command>. Beispiel:
    </para>
<screen>ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</screen>
    <para>
     Vergewissern Sie sich, dass Sie dieselben LDAP-Parameter verwenden wie in der Ceph-Konfigurationsdatei, um mögliche Probleme zu vermeiden.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.config">
   <title>Konfigurieren des Object Gateways zur Verwendung der LDAP-Authentifizierung</title>
   <para>
    Die folgenden Parameter in der Konfigurationsdatei <filename>/etc/ceph/ceph.conf</filename> beziehen sich auf die LDAP-Authentifizierung:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>rgw_ldap_uri</option>
     </term>
     <listitem>
      <para>
       Gibt den zu verwendenden LDAP-Server an. Vergewissern Sie sich, dass Sie den Parameter <literal>ldaps://<replaceable>fqdn</replaceable>:<replaceable>port</replaceable></literal> verwenden, um die offene Übertragung des Berechtigungsnachweises in Klartext zu verhindern.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_binddn</option>
     </term>
     <listitem>
      <para>
       Der vom Object Gateway verwendete eindeutige Name des Servicekontos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_secret</option>
     </term>
     <listitem>
      <para>
       Das Passwort für das Service-Konto.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw_ldap_searchdn</term>
     <listitem>
      <para>
       Gibt die Basis im Verzeichnisinformationsbaum zum Suchen von Benutzern an. Sie könnte die organisatorische Einheit Ihrer Benutzer oder eine spezifischere organisatorische Einheit sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_dnattr</option>
     </term>
     <listitem>
      <para>
       Das Attribut, das im konstruierten Suchfilter zum Abgleich eines Benutzernamens verwendet wird. Abhängig von Ihrem Verzeichnisinformationsbaum (Directory Information Tree, DIT) wäre es wahrscheinlich <literal>uid</literal> oder <literal>cn</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_search_filter</option>
     </term>
     <listitem>
      <para>
       Wenn dieser Parameter nicht angegeben ist, konstruiert das Object Gateway automatisch den Suchfilter mit der Einstellung <option>rgw_ldap_dnattr</option>. Mit diesem Parameter engen Sie die Liste der zulässigen Benutzer auf sehr flexible Weise ein. Weitere Details finden Sie in <xref linkend="ceph.rgw.ldap.filter"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.filter">
   <title>Verwenden eines benutzerdefinierten Suchfilters zur Begrenzung des Benutzerzugriffs</title>
   <para>
    Den Parameter <option>rgw_search_filter</option> können Sie auf unterschiedliche Weise verwenden.
   </para>
   <sect3>
    <title>Teilfilter zur weiteren Beschränkung des konstruierten Suchfilters</title>
    <para>
     Beispiel eines Teilfilters:
    </para>
<screen>"objectclass=inetorgperson"</screen>
    <para>
     Das Object Gateway generiert den Suchfilter wie üblich mit dem Benutzernamen aus dem Token und dem Wert von <option>rgw_ldap_dnattr</option>. Der konstruierte Filter wird dann mit dem Teilfilter aus dem Attribut <option>rgw_search_filter</option> kombiniert. Abhängig vom Benutzernamen und den Einstellungen sieht der finale Suchfilter möglicherweise folgendermaßen aus:
    </para>
<screen>"(&amp;(uid=hari)(objectclass=inetorgperson))"</screen>
    <para>
     In diesem Fall erhält der Benutzer "hari" nur dann Zugriff, wenn er im LDAP-Verzeichnis gefunden wird, über die Objektklasse "inetorgperson" verfügt und ein gültiges Passwort angegeben hat.
    </para>
   </sect3>
   <sect3>
    <title>Vollständiger Filter</title>
    <para>
     Ein vollständiger Filter muss einen Token <option>USERNAME</option> enthalten, der beim Authentifizierungsversuch durch den Benutzernamen ersetzt wird. Der Parameter <option>rgw_ldap_dnattr</option> wird in diesem Fall nicht mehr verwendet. Verwenden Sie beispielsweise folgenden Filter, um die gültigen Benutzer auf eine bestimmte Gruppe zu beschränken:
    </para>
<screen>"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</screen>
    <note>
     <title>Attribut <literal>memberOf</literal></title>
     <para>
      Für die Verwendung des Attributs <literal>memberOf</literal> in LDAP-Suchen ist die serverseitige Unterstützung Ihrer spezifischen LDAP Server-Implementierung erforderlich.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.token">
   <title>Generieren eines Zugriffstokens für die LDAP-Authentifizierung</title>
   <para>
    Das Dienstprogramm <command>radosgw-token</command> generiert den Zugriffstoken basierend auf LDAP-Benutzername und Passwort. Es gibt eine mit base-64 verschlüsselte Zeichenkette aus, die der eigentliche Zugriffstoken ist. Verwenden Sie Ihren bevorzugten S3 Client (weitere Informationen hierzu finden Sie in <xref linkend="accessing.ragos.gateway"/>), geben Sie den Token als Zugriffsschlüssel an und verwenden Sie einen leeren geheimen Schlüssel.
   </para>
<screen><prompt>root@minion &gt; </prompt>export RGW_ACCESS_KEY_ID="<replaceable>username</replaceable>"
<prompt>root@minion &gt; </prompt>export RGW_SECRET_ACCESS_KEY="<replaceable>password</replaceable>"
<prompt>root@minion &gt; </prompt>radosgw-token --encode --ttype=ldap</screen>
   <important>
    <title>Klartext-Berechtigungsnachweis</title>
    <para>
     Der Zugriffstoken ist eine mit base-64 verschlüsselte JSON-Struktur und enthält den LDAP-Berechtigungsnachweis als Klartext.
    </para>
   </important>
   <note>
    <title>Active Directory</title>
    <para>
     Verwenden Sie für Active Directory den Parameter <option>--ttype=ad</option>.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw.bucket_sharding">
  <title>Bucket-Index-Sharding</title>

  <para>
   Das Object Gateway speichert Bucket-Indexdaten in einem Index-Pool, der standardmäßig <literal>.rgw.buckets.index</literal> lautet. Wenn Sie zu viele (hundertausende) Objekte in einen einzelnen Bucket stellen und das Kontingent für die maximale Anzahl der Objekte pro Bucket (<option>rgw bucket default quota max objects</option>) nicht festgelegt ist, dann verschlechtert sich möglicherweise die Leistung des Index-Pools. Ein <emphasis>Bucket-Index-Sharding</emphasis> verhindert derartige Leistungseinbußen und ermöglicht eine hohe Anzahl von Objekten pro Bucket.
  </para>

  <sect2 xml:id="ogw.bucket_reshard">
   <title>Bucket-Index-Resharding</title>
   <para>
    Wenn ein Bucket groß geworden ist und die anfängliche Konfiguration nicht mehr ausreicht, muss für den Indexpool des Buckets ein Resharding durchgeführt werden. Sie können entweder das automatische Online-Resharding für den Bucket-Index durchführen (Informationen hierzu finden Sie in <xref linkend="ogw.bucket_sharding.dyn"/>) oder ein manuelles Offline-Resharding des Bucket-Index (Informationen hierzu finden Sie in <xref linkend="ogw.bucket_sharding.re"/>).
   </para>
   <sect3 xml:id="ogw.bucket_sharding.dyn">
    <title>Dynamisches Resharding</title>
    <para>
     Ab SUSE Enterprise Storage 5 unterstützen wird das Online-Bucket-Resharding. Es erkennt, wenn die Anzahl der Objekte pro Bucket einen bestimmten Schwellwert erreicht und erhöht automatisch die Anzahl der vom Bucket-Index verwendeten Shards. Dieser Vorgang reduziert die Anzahl der Einträge in jedem Bucket-Index-Shard.
    </para>
    <para>
     Der Erkennungsvorgang wird in folgenden Fällen ausgeführt:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Wenn neue Objekte zum Bucket hinzugefügt werden.
      </para>
     </listitem>
     <listitem>
      <para>
       In einem Hintergrundprozess, der regelmäßig alle Buckets absucht. Dies ist erforderlich, um bestehende Buckets zu verarbeiten, die nicht aktualisiert werden.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Ein Bucket, für den ein Resharding durchgeführt werden muss, wird zur Warteschlange <option>reshard_log</option> hinzugefügt und das Resharding wird für später geplant. Die Reshard-Threads werden im Hintergrund ausgeführt und führen das geplante Resharding für die einzelnen Buckets nacheinander durch.
    </para>
    <variablelist>
     <title>Konfigurieren eines dynamischen Reshardings</title>
     <varlistentry>
      <term><option>rgw_dynamic_resharding</option>
      </term>
      <listitem>
       <para>
        Aktiviert oder deaktiviert das dynamische Index-Resharding. Mögliche Werte sind "true" und "false". Die Standardeinstellung ist "true".
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_num_logs</option>
      </term>
      <listitem>
       <para>
        Anzahl der Shards für das Resharding-Protokoll. Der Standardwert ist 16.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_bucket_lock_duration</option>
      </term>
      <listitem>
       <para>
        Dauer der Sperre des Bucket-Objekts beim Resharding. Die Standardeinstellung ist 120 Sekunden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_max_objs_per_shard</option>
      </term>
      <listitem>
       <para>
        Maximale Anzahl der Objekte pro Bucket-Index-Shard. Der Standardwert ist 100.000 Objekte.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_thread_interval</option>
      </term>
      <listitem>
       <para>
        Maximale Zeit zwischen den Verarbeitungsrunden des Reshard-Threads. Die Standardeinstellung ist 600 Sekunden.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <important>
     <title>Konfigurationen für mehrere Standorte</title>
     <para>
      Das dynamische Resharding wird nicht in Umgebungen mit mehreren Standorten unterstützt. Es ist zwar ab Ceph 12.2.2 standardmäßig deaktiviert, doch wir empfehlen, die Einstellungen gegenzuprüfen.
     </para>
    </important>
    <variablelist>
     <title>Kommandos zum Verwalten des Resharding-Vorgangs</title>
     <varlistentry>
      <term>Fügen Sie ein Bucket zur Resharding-Warteschlange hinzu mit:</term>
      <listitem>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin reshard add \
 --bucket <replaceable>BUCKET_NAME</replaceable> \
 --num-shards <replaceable>NEW_NUMBER_OF_SHARDS</replaceable>
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Listen Sie die Resharding-Warteschlange auf mit:</term>
      <listitem>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin reshard list
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Verarbeiten/planen Sie ein Bucket-Resharding mit:</term>
      <listitem>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin reshard process
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Zeigen Sie den Bucket-Resharding-Status an mit:</term>
      <listitem>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin reshard status --bucket <replaceable>BUCKET_NAME</replaceable>
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Brechen Sie das ausstehende Bucket-Resharding ab mit:</term>
      <listitem>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin reshard cancel --bucket <replaceable>BUCKET_NAME</replaceable>
</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ogw.bucket_sharding.re">
    <title>Manuelles Resharding</title>
    <para>
     Das in <xref linkend="ogw.bucket_sharding.dyn"/> erwähnte dynamische Resharding wird nur für einfache Object Gateway-Konfigurationen unterstützt. Bei Konfigurationen mit mehreren Standorten müssen Sie das in diesem Abschnitt erläuterte manuelle Resharding verwenden.
    </para>
    <para>
     Führen Sie für ein manuelles Offline-Resharding des Bucket-Index folgendes Kommando aus:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin bucket reshard
</screen>
    <para>
     Das Kommando <command>bucket reshard</command> führt Folgendes aus:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Es erstellt einen neuen Satz von Bucket-Index-Objekten für das angegebene Objekt.
      </para>
     </listitem>
     <listitem>
      <para>
       Es verteilt alle Objekteinträge dieser Indexobjekte.
      </para>
     </listitem>
     <listitem>
      <para>
       Es erstellt eine neue Bucket-Instanz.
      </para>
     </listitem>
     <listitem>
      <para>
       Es verknüpft die neue Bucket-Instanz mit dem Bucket, sodass alle neuen Index-Operationen durch die neuen Bucket-Indizes gehen.
      </para>
     </listitem>
     <listitem>
      <para>
       Gibt die alte und die neue Bucket-ID an die Standardausgabe aus.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <title>Resharding des Bucket-Index-Pools</title>
     <step>
      <para>
       Vergewissern Sie sich, dass alle Operationen zum Bucket gestoppt sind.
      </para>
     </step>
     <step>
      <para>
       Sichern Sie den ursprünglichen Bucket-Index:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin bi list \
 --bucket=<replaceable>BUCKET_NAME</replaceable> \
 &gt; <replaceable>BUCKET_NAME</replaceable>.list.backup
</screen>
     </step>
     <step>
      <para>
       Führen Sie ein Resharding des Bucket-Index aus:
      </para>
<screen>
 <prompt>root@minion &gt; </prompt>radosgw-admin reshard \
 --bucket=<replaceable>BUCKET_NAME</replaceable> \
 --num-shards=<replaceable>NEW_SHARDS_NUMBER</replaceable>
</screen>
      <tip>
       <title>Alte Bucket-ID</title>
       <para>
        Als Teil seiner Ausgabe gibt dieses Kommando auch die neue und alte Bucket-ID aus. Notieren Sie sich die alte Bucket-ID. Sie benötigen Sie zum endgültigen Löschen der alten Bucket-Indexobjekte.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Verifizieren Sie, dass die Objekte korrekt aufgelistet sind. Vergleichen Sie dazu die Auflistung des alten Bucket-Index mit der neuen. Löschen Sie dann die alten Bucket-Indexobjekte:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin bi purge
 --bucket=<replaceable>BUCKET_NAME</replaceable>
 --bucket-id=<replaceable>OLD_BUCKET_ID</replaceable>
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="ogw.bucket_sharding.new">
   <title>Bucket-Index-Sharding für neue Buckets</title>
   <para>
    Für ein Bucket-Index-Sharding stehen zwei Optionen zur Verfügung:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Verwenden Sie bei einfachen Konfigurationen die Option <option>rgw_override_bucket_index_max_shards</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Verwenden Sie bei Konfigurationen mit mehreren Standorten die Option <option>bucket_index_max_shards</option>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Das Bucket-Index-Sharding wird deaktiviert, wenn die Optionen auf <literal>0</literal> festgelegt sind. Ein Wert größer <literal>0</literal> aktiviert das Bucket-Index-Sharding und legt die maximale Anzahl von Shards fest.
   </para>
   <para>
    Die folgende Formel unterstützt Sie beim Berechnen der empfohlenen Anzahl von Shards:
   </para>
<screen>
number_of_objects_expected_in_a_bucket / 100000
</screen>
   <para>
    Beachten Sie, dass maximal 7877 Shards möglich sind.
   </para>
   <sect3>
    <title>Einfache Konfigurationen</title>
    <procedure>
     <step>
      <para>
       Öffnen Sie die Ceph-Konfigurationsdatei und fügen Sie die folgende Option hinzu oder bearbeiten Sie sie:
      </para>
<screen>
rgw_override_bucket_index_max_shards = 12
</screen>
      <tip>
       <title>Alle oder eine Object Gateway-Instanz</title>
       <para>
        Fügen Sie zum Konfigurieren eines Bucket-Index-Shardings für alle Instanzen des Object Gateways <option>rgw_override_bucket_index_max_shards</option> im Abschnitt<literal>[global]</literal> ein.
       </para>
       <para>
        Fügen Sie zum Konfigurieren eines Bucket-Index-Shardings nur für eine Instanz des Object Gateways <option>rgw_override_bucket_index_max_shards</option> im Abschnitt der entsprechenden Instanz ein.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Starten Sie das Object Gateway neu. Weitere Einzelheiten finden Sie in <xref linkend="ceph.rgw.operating"/>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Konfigurationen für mehrere Standorte</title>
    <para>
     Konfigurationen für mehrere Standorte können einen anderen Index-Pool zum Verwalten von Failover haben. Legen Sie zum Konfigurieren einer konsistenten Anzahl von Shards für Zonen in einer Zonengruppe die Option <option>bucket_index_max_shards</option> in der Konfiguration der Zonengruppe fest:
    </para>
    <procedure>
     <step>
      <para>
       Exportieren Sie die Zonengruppenkonfiguration in die Datei <filename>zonegroup.json</filename>:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin zonegroup get &gt; zonegroup.json
</screen>
     </step>
     <step>
      <para>
       Bearbeiten Sie die Datei <filename>zonegroup.json</filename> und legen Sie die Option <option>bucket_index_max_shards</option> für jede benannte Zone fest.
      </para>
     </step>
     <step>
      <para>
       Setzen Sie die Zonengruppe zurück mit:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin zonegroup set &lt; zonegroup.json
</screen>
     </step>
     <step>
      <para>
       Aktualisieren Sie den Zeitraum mit:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>radosgw-admin period update --commit
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw.keystone">
  <title>Integrieren von OpenStack Keystone</title>

  <para>
   OpenStack Keystone ist ein Identitätsservice für das OpenStack-Produkt. Sie können das Object Gateway mit Keystone integrieren, um ein Gateway einzurichten, das einen Keystone-Authentifizierungstoken akzeptiert. Ein Benutzer, der durch Keystone für den Zugriff auf das Gateway autorisiert ist, wird am Ceph Object Gateway verifiziert und gegebenenfalls automatisch erstellt. Das Object Gateway fragt Keystone regelmäßig nach einer Liste der entzogenen Token ab.
  </para>

  <sect2 xml:id="ogw.keystone.ostack">
   <title>Konfigurieren von OpenStack</title>
   <para>
    Vor dem Konfigurieren des Ceph Gateways müssen Sie OpenStack Keystone konfigurieren, um den Swift Service zu aktivieren und auf das Ceph Object Gateway auszurichten:
   </para>
   <procedure>
    <step>
     <para>
      <emphasis>Festlegen des Swift Service.</emphasis> Sie müssen zunächst den Swift Service erstellen, um OpenStack zur Validierung von Swift-Benutzern zu verwenden:
     </para>
<screen>
<prompt>root # </prompt>openstack service create \
 --name=swift \
 --description="Swift Service" \
 object-store
</screen>
    </step>
    <step>
     <para>
      <emphasis>Festlegen der Endpunkte.</emphasis> Nach dem Erstellen des Swift Service müssen Sie diesen auf das Ceph Object Gateway ausrichten. Ersetzen Sie <replaceable>REGION_NAME</replaceable> durch den Namen des Zonengruppennamens oder Regionnamens des Gateways.
     </para>
<screen>
<prompt>root # </prompt>openstack endpoint create --region <replaceable>REGION_NAME</replaceable> \
 --publicurl   "http://radosgw.example.com:8080/swift/v1" \
 --adminurl    "http://radosgw.example.com:8080/swift/v1" \
 --internalurl "http://radosgw.example.com:8080/swift/v1" \
 swift
</screen>
    </step>
    <step>
     <para>
      <emphasis>Verifizieren der Einstellungen.</emphasis> Nach dem Erstellen des Swift Service und dem Festlegen der Endpunkte müssen Sie die Endpunkte anzeigen, um zu verifizieren, dass alle Einstellungen korrekt sind.
     </para>
<screen>
<prompt>root # </prompt>openstack endpoint show object-store
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw.keystone.ogw">
   <title>Konfigurieren des Ceph Object Gateways</title>
   <sect3>
    <title>Konfigurieren der SSL-Zertifikate</title>
    <para>
     Das Ceph Object Gateway fragt Keystone regelmäßig nach einer Liste der entzogenen Token ab. Diese Anforderungen sind verschlüsselt und signiert. Keystone kann ebenfalls konfiguriert werden, um eigensignierte Token bereitzustellen, die ebenfalls verschlüsselt und signiert sind. Sie müssen das Gateway so konfigurieren, dass es diese signierten Nachrichten entschlüsseln und verifizieren kann. Daher müssen die OpenSSL-Zertifikate, die Keystone zum Erstellen der Anforderungen verwendet, in das Format "nss db" konvertiert werden:
    </para>
<screen>
<prompt>root # </prompt>mkdir /var/ceph/nss
<prompt>root # </prompt>openssl x509 -in /etc/keystone/ssl/certs/ca.pem \
 -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
<systemitem class="username">root</systemitem>openssl x509 -in /etc/keystone/ssl/certs/signing_cert.pem \
 -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"
</screen>
    <para>
     OpenStack Keystone kann ein eigensigniertes SSL-Zertifikat verwenden, damit Ceph Object Gateway mit OpenStack Keystone interagieren kann. Installieren Sie entweder das SSL-Zertifikat von Keystone im Node, auf dem das Ceph Object Gateway ausgeführt wird, oder legen Sie alternativ den Wert der Option <option>rgw keystone verify ssl</option> auf "false" fest. Wenn Sie <option>rgw keystone verify ssl</option> auf "false" festlegen, versucht das Gateway nicht, das Zertifikat zu verifizieren.
    </para>
   </sect3>
   <sect3>
    <title>Konfigurieren der Optionen des Object Gateways</title>
    <para>
     Die Keystone-Integration wird mit folgenden Optionen konfiguriert:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone api version</option>
      </term>
      <listitem>
       <para>
        Version der Keystone-API. Gültige Optionen sind 2 oder 3. Der Standardwert ist 2.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone url</option>
      </term>
      <listitem>
       <para>
        Die URL und Portnummer der administrativen RESTful API am Keystone-Server. Folgt dem Schema <replaceable>SERVER_URL:PORTNUMMER</replaceable>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin token</option>
      </term>
      <listitem>
       <para>
        Der Token oder das gemeinsame Geheimnis, der/das intern in Keystone für administrative Anforderungen konfiguriert wird.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone accepted roles</option>
      </term>
      <listitem>
       <para>
        Die Rollen zur Verarbeitung der Anforderungen. Die Standardeinstellung ist "Member, admin".
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone accepted admin roles</option>
      </term>
      <listitem>
       <para>
        Die Liste der Rollen, mit denen Benutzer Verwaltungsrechte erhalten können.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone token cache size</option>
      </term>
      <listitem>
       <para>
        Die maximale Anzahl der Einträge im Keystone-Token-Cache.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone revocation interval</option>
      </term>
      <listitem>
       <para>
        Die Dauer in Sekunden, bevor entzogene Token überprüft werden. Der Standardwert ist 15 * 60.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone implicit tenants</option>
      </term>
      <listitem>
       <para>
        Neue Benutzer werden in ihren eigenen Mandanten mit demselben Namen erstellt. Die Standardeinstellung ist "false".
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw s3 auth use keystone</option>
      </term>
      <listitem>
       <para>
        Wird diese Option auf "true" festgelegt, authentifiziert das Ceph Object Gateway Benutzer mit Keystone. Die Standardeinstellung ist "false".
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>nss db path</option>
      </term>
      <listitem>
       <para>
        Der Pfad zur NSS-Datenbank.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Es ist auch möglich, den Keystone Service-Mandanten, den Benutzer und das Passwort für Keystone (für Version 2.0 der OpenStack Identity API) auf ähnliche Weise zu konfigurieren wie OpenStack Services normalerweise konfiguriert werden. Dadurch vermeiden Sie, das gemeinsame Geheimnis <option>rgw keystone admin token</option> in der Konfigurationsdatei festzulegen, das in Produktionsumgebungen deaktiviert sein sollte. Der Berechtigungsnachweis des Service-Mandanten sollte über Verwaltungsrechte verfügen. Weitere detaillierte Informationen finden Sie in der <link xlink:href="https://docs.openstack.org/keystone/latest/#setting-up-projects-users-and-roles">offiziellen Dokumentation zu OpenStack Keystone</link>. Die entsprechenden Konfigurationsoptionen sind wie folgt:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone admin user</option>
      </term>
      <listitem>
       <para>
        Der Benutzername des verwaltungsberechtigten Keystone-Benutzers.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin password</option>
      </term>
      <listitem>
       <para>
        Das Passwort des verwaltungsberechtigten Keystone-Benutzers.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin tenant</option>
      </term>
      <listitem>
       <para>
        Der Mandant des verwaltungsberechtigten Benutzers von Keystone Version 2.0.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Ein Ceph Object Gateway-Benutzer wird einem Keystone-Mandanten zugeordnet. Einem Keystone-Benutzer sind verschiedene Rollen zugewiesen, möglicherweise zu mehr als einem einzelnen Mandanten. Wenn das Ceph Object Gateway das Ticket erhält, sieht es sich den Mandanten und die Benutzerrollen an, die diesem Ticket zugewiesen sind, und akzeptiert die Anforderung oder weist sie zurück, je nach Einstellung der Option <option>rgw keystone accepted roles</option>.
    </para>
    <tip>
     <title>Zuordnung zu OpenStack-Mandanten</title>
     <para>
      Obgleich Swift-Mandanten standardmäßig zum Object Gateway-Benutzer zugeordnet werden, ist mit der Option <option>rgw keystone implicit tenants</option> auch deren Zuordnung zu OpenStack-Mandanten möglich. Dadurch verwenden Container den Mandanten-Namespace statt des S3-ähnlichen globalen Namespace, das standardmäßig für Object Gateway verwendet wird. Wir empfehlen, die Zuordnungsmethode in der Planungsphase festzulegen, um Verwirrung zu vermeiden. Wenn eine Option später gewechselt wird, betrifft dies nur neuere Anforderungen, die unter einem Mandanten zugeordnet werden. Ältere Buckets, die vorher erstellt wurden, sind weiterhin in einem globalen Namespace enthalten.
     </para>
    </tip>
    <para>
     Bei Version 3 der OpenStack Identity API sollten Sie die Option <option>rgw keystone admin tenant</option> ersetzen durch:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone admin domain</option>
      </term>
      <listitem>
       <para>
        Die Domäne des verwaltungsberechtigten Keystone-Benutzers.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin project</option>
      </term>
      <listitem>
       <para>
        Das Projekt des verwaltungsberechtigten Keystone-Benutzers.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.fed">


  <title>Object Gateways an mehreren Standorten</title>

  <variablelist>
   <varlistentry>
    <term>Zone</term>
    <listitem>
     <para>
      Eine logische Gruppierung einer oder mehrerer Object Gateway-Instanzen. Eine Zone muss als <emphasis>Master</emphasis>-Zone in einer <emphasis>Zonengruppe</emphasis> ausgewiesen sein. In ihr werden alle Buckets und Benutzer erstellt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Zonengruppe</term>
    <listitem>
     <para>
      Eine Zonengruppe besteht aus mehreren Zonen. Es sollte eine Master-Zonengruppe vorhanden sein, in der Änderungen an der Systemkonfiguration vorgenommen werden.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Zonengruppenzuordnung</term>
    <listitem>
     <para>
      Eine Konfigurationsstruktur mit den Zuordnungen des gesamten Systems, wie zum Beispiel welche Zonengruppe der Master ist, die Beziehungen zwischen verschiedenen Zonengruppen und bestimmte Konfigurationsoptionen wie die Speicherrichtlinien.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Bereich</term>
    <listitem>
     <para>
      Ein Container für Zonengruppen. Bereiche ermöglichen die Trennung von Zonengruppen zwischen Clustern. Es ist möglich, mehrere Bereiche zu erstellen. Dies erleichtert die Ausführung vollständig verschiedener Konfigurationen im selben Cluster. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Periode</term>
    <listitem>
     <para>
      Eine Periode enthält die Konfigurationsstruktur für den aktuellen Zustand des Bereichs. Jede Periode enthält eine eindeutige ID und eine Epoche. Jedem Bereich ist eine aktuelle Periode zugeordnet, die den aktuellen Zustand der Konfiguration der Zonengruppen und Speicherrichtlinien enthält. Durch jede Konfigurationsänderung für eine Nicht-Master-Zone wird die Epoche der Periode schrittweise erhöht. Die Änderung der Master-Zone in eine andere Zone löst folgende Änderungen aus:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Eine neue Periode wird generiert mit einer neuen Perioden-ID und der Epoche 1.
       </para>
      </listitem>
      <listitem>
       <para>
        Die aktuelle Periode des Bereichs wird aktualisiert, um auf die neu generierte Perioden-ID zu zeigen.
       </para>
      </listitem>
      <listitem>
       <para>
        Die Epoche des Bereichs wird schrittweise erhöht.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Sie können jedes Object Gateway so konfigurieren, dass es Teil einer Verbundarchitektur ist. Dadurch arbeitet es in einer aktiven Zonenkonfiguration und lässt zugleich Schreibvorgänge in Nicht-Master-Zonen zu.
  </para>

  <sect2 xml:id="ceph.rgw.fed.term">
   <title>Terminologie</title>
   <para>
    Nachfolgend sehen Sie eine Beschreibung der spezifischen Begriffe einer Verbundarchitektur:
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.intro">
   <title>Beispiel einer Cluster-Einrichtung</title>
   <para>
    In diesem Beispiel konzentrieren wir uns auf die Erstellung einer einzelnen Zonengruppe mit drei separaten Zonen, die aktiv deren Daten synchronisieren. Zwei Zonen gehören zum selben Cluster, die dritte Zone gehört zu einem anderen Cluster. Zum Spiegeln von Datenänderungen zwischen Object Gateways wird kein Synchronisierungsagent herangezogen. Dies ermöglicht ein viel einfacheres Konfigurationsschema und Aktiv/Aktiv-Konfigurationen. Beachten Sie, dass Metadatenoperationen wie die Erstellung eines neuen Benutzers weiterhin in der Master-Zone durchgeführt werden müssen. Datenoperationen wie die Erstellung von Buckets und Objekten können jedoch von jeder anderen Zone verarbeitet werden.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.keys">
   <title>Systemschlüssel</title>
   <para>
    Beim Konfigurieren von Zonen erwartet das Object Gateway die Erstellung eines S3-kompatiblen Systembenutzers zusammen mit dessen Zugriffsschlüssel und geheimen Schlüssel. Dadurch kann eine andere Object Gateway-Instanz die Konfiguration mit dem Zugriffsschlüssel und geheimen Schlüssel entfernt abrufen. Weitere Informationen zum Erstellen von S3-Benutzern finden Sie in <xref linkend="adding.s3.swift.users"/>.
   </para>
   <tip>
    <para>
     Es ist nützlich, vor der Zonenerstellung zunächst den Zugriffsschlüssel und den geheimen Schlüssel zu generieren, weil dies später die Skripterstellung und Verwendung der Konfigurationsverwaltungswerkzeuge erleichtert.
    </para>
   </tip>
   <para>
    Nehmen wir für dieses Beispiel an, dass der Zugriffsschlüssel und der geheime Schlüssel in den Umgebungsvariablen festgelegt werden:
   </para>
<screen># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</screen>
   <para>
    Im Allgemeinen umfassen Zugriffsschlüssel 20 alphanumerische Zeichen. Geheime Schlüssel bestehen aus 40 alphanumerischen Zeichen (sie dürfen auch die Zeichen +/= enthalten). Diese Schlüssel werden in der Kommandozeile generiert:
   </para>
<screen># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.naming">
   <title>Benennungskonventionen</title>
   <para>
    In diesem Beispiel wird die Einrichtung einer Master-Zone erläutert. Wir nehmen eine Zonengruppe namens <literal>us</literal> für die USA an. Diese Zonengruppe stellt unsere Master-Zonengruppe dar. Sie enthält zwei Zonen im Format <replaceable>Zonengruppe</replaceable>-<replaceable>Zone</replaceable>. Diese Konvention ist nur ein Vorschlag unsererseits. Sie können jedes beliebige Format wählen. Zusammenfassung:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Master-Zonengruppe: USA <literal>us</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Master-Zone: USA, Region Osten 1: <literal>us-east-1</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Sekundäre Zone: USA, Region Osten 2: <literal>us-east-2</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Sekundäre Zone: USA, Region Westen: <literal>us-west</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Dies ist Teil eines größeren Bereichs namens <literal>gold</literal>. Die Zonen <literal>us-east-1</literal> und <literal>us-east-2</literal> sind Teil des selben Ceph Clusters, <literal>us-east-1</literal> ist die primäre Zone. <literal>us-west</literal> befindet sich in einem anderen Ceph Cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.pools">
   <title>Standard-Pools</title>
   <para>
    Wenn Object Gateway mit den entsprechenden Berechtigungen konfiguriert ist, erstellt es selbständig Standard-Pools. Die Werte für <literal>pg_num</literal> und <literal>pgp_num</literal> werden der Konfigurationsdatei <filename>ceph.conf</filename> entnommen. Pools, die sich auf eine Zone beziehen, folgen der Benennungskonvention <replaceable>Zone-Name</replaceable>.<replaceable>Pool-Name</replaceable>. Für die Zone <literal>us-east-1</literal> sind es beispielsweise die folgenden Pools:
   </para>
<screen>.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</screen>
   <para>
    Diese Pools können auch in anderen Zonen erstellt werden, indem <literal>us-east-1</literal> durch den entsprechenden Zonennamen ersetzt wird.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.realm">
   <title>Erstellen eines Bereichs</title>
   <para>
    Konfigurieren Sie einen Bereich namens <literal>gold</literal> und machen Sie ihn zum Standardbereich:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <para>
    Beachten Sie, dass jeder Bereich eine ID hat, was für Flexibilität sorgt, beispielsweise beim späteren Umbenennen des Bereichs. Die <literal>current_period</literal> (aktuelle Periode) ändert sich, sobald wir eine Änderung in der Master-Zone vornehmen. Die <literal>epoch</literal> (Epoche) wird schrittweise erhöht, wenn eine Änderung an der Konfiguration der Master-Zone vorgenommen wird, was zu einer Änderung der aktuellen Periode führt.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.deldefzonegrp">
   <title>Löschen der Standard-Zonengruppe</title>
   <para>
    Die Standardinstallation von Object Gateway erstellt die Standard-Zonengruppe namens <literal>default</literal>. Entfernen Sie die Standard-Zonengruppe, da wir sie nicht länger benötigen.
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.createmasterzonegrp">
   <title>Erstellen einer Master-Zonengruppe</title>
   <para>
    Erstellen Sie eine Master-Zonengruppe namens <literal>us</literal>. Die Zonengruppe verwaltet die Zonengruppen-Zuordnungen und verteilt die Änderungen auf das restliche System. Durch Kennzeichnen der Zonengruppe als Standard lassen Sie ausdrücklich die Nennung des RGW-Zonengruppen-Schalters für spätere Kommandos zu.
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Alternativ können Sie mit folgendem Kommando eine Zonengruppe als Standard kennzeichnen:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.masterzone">
   <title>Erstellen einer Master-Zone</title>
   <para>
    Erstellen Sie nun eine Standardzone und fügen Sie diese zur Standard-Zonengruppe hinzu. Beachten Sie, dass Sie diese Zone für Metadatenoperationen wie die Benutzererstellung verwenden:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Beachten Sie, dass die Schalter <option>--rgw-zonegroup</option> und <option>--default</option> die Zone zu einer Zonengruppe hinzufügen und diese zur Standardzone machen. Alternativ ist dies auch mit folgendem Kommando möglich:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone default --rgw-zone=us-east-1
<prompt>cephadm &gt; </prompt>radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</screen>
   <sect3 xml:id="ceph.rgw.fed.masterzone.createuser">
    <title>Erstellen von Systembenutzern</title>
    <para>
     Für den Zugriff auf Zonen-Pools müssen Sie einen Systembenutzer erstellen. Beachten Sie, dass Sie diese Schlüssel auch zum Konfigurieren der sekundären Zone benötigen.
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> --system</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.updateperiod">
    <title>Aktualisieren der Periode</title>
    <para>
     Da Sie die Konfiguration der Master-Zone geändert haben, müssen Sie die Änderungen für sie übernehmen, damit sie in der Konfigurationsstruktur des Bereichs wirksam werden. Zu Beginn sieht die Periode folgendermaßen aus:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
    <para>
     Aktualisieren Sie die Periode und übernehmen Sie die Änderungen:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.startrgw">
    <title>Starten des Object Gateways</title>
    <para>
     Sie müssen vor dem Starten des Object Gateways die Zonen- und Port-Optionen für das Object Gateways in der Konfigurationsdatei angeben. Weitere Informationen zu Object Gateway und dessen Konfiguration finden Sie in <xref linkend="cha.ceph.gw"/>. Der Konfigurationsabschnitt von Object Gateway sollte in etwas wie folgt aussehen:
    </para>
<screen>[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</screen>
    <para>
     Starten Sie das Object Gateway:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-east-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.secondaryzone">
   <title>Erstellen einer sekundären Zone</title>
   <para>
    Erstellen und konfigurieren Sie die sekundäre Zone namens <literal>us-east-2</literal> im selben Cluster. Sie können alle folgenden Kommandos im Node ausführen, in dem die Master-Zone selbst gehostet wird.
   </para>
   <para>
    Erstellen Sie die sekundäre Zone mit dem selben Kommando, mit dem Sie auch die primäre Zone erstellt haben, lassen Sie dabei jedoch die Master-Flagge weg:
   </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <sect3 xml:id="ceph.rgw.fed.secondzone.updateperiod">
    <title>Aktualisieren der Periode</title>
    <para>
     Informieren Sie alle Gateways über die neue Änderung in der Systemzuordnung. Aktualisieren Sie dazu die Periode und übernehmen Sie die Änderungen:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.secondzone.startrgw">
    <title>Starten des Object Gateways</title>
    <para>
     Passen Sie die Konfiguration des Object Gateways für die sekundäre Zone an und starten Sie sie:
    </para>
<screen>[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</screen>
<screen><prompt>cephadm &gt; </prompt>sudo systemctl start ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.seccluster">
   <title>Hinzufügen von Object Gateway zum sekundären Cluster</title>
   <para>
    Der zweite Ceph Cluster gehört zur selben Zonengruppe wie der erste, kann sich jedoch geografisch an einem anderen Ort befinden.
   </para>
   <sect3 xml:id="ceph.rgw.fed.seccluster.realm">
    <title>Standard-Bereich und -Zonengruppe</title>
    <para>
     Da Sie bereits den Bereich für das erste Gateway erstellt haben, ziehen Sie den Bereich hierher und machen Sie es hier zum Standard:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
<prompt>cephadm &gt; </prompt>radosgw-admin realm default --rgw-realm=gold</screen>
    <para>
     Rufen Sie die Konfiguration von der Master-Zone ab, indem Sie die Periode heranziehen:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable></screen>
    <para>
     Legen Sie die Standard-Zonengruppe auf die bereits erstellte Zonengruppe <literal>us</literal> fest:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.seczone">
    <title>Konfiguration der sekundären Zone</title>
    <para>
     Erstellen Sie eine neue Zone namens <literal>us-west</literal> mit denselben Systemschlüsseln:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.period">
    <title>Aktualisieren der Periode</title>
    <para>
     Um die Änderungen an der Zonengruppen-Zuordnung zu verteilen, aktualisieren und übernehmen wir die Periode:
    </para>
<screen><prompt>cephadm &gt; </prompt>radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
    <para>
     Beachten Sie, dass die Nummer der Perioden-Epoche um einen Schritt erhöht wurde, was eine Änderung in der Konfiguration angibt.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.rgwstart">
    <title>Starten des Object Gateways</title>
    <para>
     Dieser Vorgang entspricht in etwa dem Starten des Object Gateways in der ersten Zone. Der einzige Unterschied besteht darin, dass die Object Gateway-Zonenkonfiguration den Zonennamen <literal>us-west</literal> enthalten sollte:
    </para>
<screen>[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</screen>
    <para>
     Starten Sie das zweite Object Gateway:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-west</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.failover">
   <title>Failover und Disaster Recovery</title>
   <para>
    Falls die Master-Zone ausfällt, führen Sie zum Zweck eines Disaster Recovery ein Failover zur sekundären Zone durch.
   </para>
   <procedure>
    <step>
     <para>
      Machen Sie die sekundäre Zone zur Master- und Standardzone. Beispiel:
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default
     </screen>
     <para>
      Standardmäßig wird das Ceph Object Gateway in einer Aktiv/Aktiv-Konfiguration ausgeführt. Wenn der Cluster zur Ausführung in einer Aktiv/Passiv-Konfiguration konfiguriert wurde, ist die sekundäre Zone eine schreibgeschützte Zone. Entfernen Sie den Status "--read-only", damit die Zone Schreiboperationen empfangen kann. Beispiel:
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default \
--read-only=False
     </screen>
    </step>
    <step>
     <para>
      Aktualisieren Sie die Periode, damit die Änderungen wirksam werden.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Starten Sie schließlich das Ceph Object Gateway neu.
     </para>
<screen>
<prompt>root # </prompt><command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
   </procedure>
   <para>
    Wenn die frühere Master-Zone wiederhergestellt ist, setzen Sie die Operation zurück.
   </para>
   <procedure>
    <step>
     <para>
      Entnehmen Sie in der wiederhergestellten Zone die Periode aus der aktuellen Master-Zone.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> period pull --url={url-to-master-zone-gateway} \
--access-key={access-key} --secret={secret}
     </screen>
    </step>
    <step>
     <para>
      Machen Sie die wiederhergestellte Zone zur Master- und Standardzone.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default
     </screen>
    </step>
    <step>
     <para>
      Aktualisieren Sie die Periode, damit die Änderungen wirksam werden.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Starten Sie dann das Ceph Object Gateway in der wiederhergestellten Zone neu.
     </para>
<screen>
<prompt>root # </prompt><command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
    <step>
     <para>
      Wenn die sekundäre Zone eine schreibgeschützte Konfiguration sein muss, aktualisieren Sie die sekundäre Zone.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --read-only
     </screen>
    </step>
    <step>
     <para>
      Aktualisieren Sie die Periode, damit die Änderungen wirksam werden.
     </para>
<screen>
<prompt>root # </prompt><command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Starten Sie schließlich das Ceph Object Gateway in der sekundären Zone neu.
     </para>
<screen>
<prompt>root # </prompt><command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw.haproxy">
  <title>Lastausgleich an den Object Gateway Servern mit HAProxy</title>

  <para>
   Mit dem HAProxy-Lastausgleich verteilen Sie alle Anforderungen auf mehrere Object Gateway Back-End-Server. Weitere detaillierte Informationen zum Konfigurieren von HAProxy finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/sec_ha_lb_haproxy.html"/>.
  </para>

  <para>
   Nachfolgend sehen Sie eine einfache Konfiguration von HAProxy zum Ausgleichen der Object Gateway Nodes mit Round-Robin als Ausgleichsalgorithmus:
  </para>

<screen>
<prompt>root # </prompt>cat /etc/haproxy/haproxy.cfg
[...]
frontend <replaceable>https_frontend</replaceable>
bind *:443 crt <replaceable>path-to-cert.pem</replaceable> [ciphers: ... ]
default_backend rgw

backend rgw
mode http
balance roundrobin
server rgw_server1 <replaceable>rgw-endpoint1</replaceable> weight 1 maxconn 100 check
server rgw_server2 <replaceable>rgw-endpoint2</replaceable> weight 1 maxconn 100 check
[...]
</screen>
 </sect1>
</chapter>
