<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha.storage.about">
 <title>SUSE Enterprise Storage und Ceph</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  SUSE Enterprise Storage ist ein dezentrales Speichersystem, das auf Skalierbarkeit, Zuverlässigkeit und Leistung ausgelegt ist und auf der Ceph-Technologie basiert. Ein Ceph Cluster kann auf Standardservern in einem gemeinsamen Netzwerk wie Ethernet ausgeführt werden. Der Cluster lässt sich gut für Tausende von Servern (im Folgenden als Nodes bezeichnet) und im Petabyte-Bereich skalieren. Im Gegensatz zu herkömmlichen Systemen, die Daten über Zuordnungstabellen speichern und abrufen, verwendet Ceph einen deterministischen Algorithmus zum Zuordnen von Speicher für Daten und hat keine zentralisierte Informationsstruktur. Ceph geht davon aus, dass in Speicher-Clustern das Hinzufügen oder Entfernen von Hardware die Regel ist und nicht die Ausnahme. Der Ceph Cluster automatisiert Verwaltungsaufgaben wie Datenverteilung und -neuverteilung, Datenreproduktion, Ausfallerkennung und Wiederherstellung. Celph kann sich selbst reparieren und selbst verwalten, was den Verwaltungsaufwand und die Gemeinkosten reduziert.
 </para>
 <para>
  In diesem Kapitel erhalten Sie einen ersten Überblick über SUSE Enterprise Storage und eine kurze Beschreibung der wichtigsten Komponenten.
 </para>
 <tip>
  <para>
   Seit SUSE Enterprise Storage 5 ist DeepSea die einzige Methode zur Cluster-Bereitstellung. In <xref linkend="ceph.install.saltstack"/> finden Sie weitere Details zum Bereitstellungsprozess.
  </para>
 </tip>
 <sect1 xml:id="storage.intro.features">
  <title>Eigenschaften von Ceph</title>

  <para>
   Die Ceph Umgebung weist die folgenden Eigenschaften auf:
  </para>

  <variablelist>
   <varlistentry>
    <term>Skalierbarkeit</term>
    <listitem>
     <para>
      Ceph ermöglicht das Skalieren auf Tausende von Nodes und kann Speicher im Petabyte-Bereich verwalten.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Kommerzielle Hardware</term>
    <listitem>
     <para>
      Zum Ausführen eines Ceph Clusters ist keine spezielle Hardware erforderlich. Weitere Informationen finden Sie in <xref linkend="storage.bp.hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Selbstverwaltung</term>
    <listitem>
     <para>
      Der Ceph Cluster verwaltet sich selbst. Wenn Nodes hinzugefügt oder entfernt werden oder ausfallen, verteilt der Cluster die Daten automatisch um. Er erkennt auch überlastete Festplatten.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Keine Single Points of Failure</term>
    <listitem>
     <para>
      Kein Node im Cluster speichert wichtige Informationen exklusiv. Die Anzahl der Redundanzen kann konfiguriert werden.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Open Source-Software</term>
    <listitem>
     <para>
      Ceph ist eine Open Source-Softwarelösung und unabhängig von spezifischer Hardware oder Anbietern.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.intro.core">
  <title>Wichtige Komponenten</title>

  <para>
   Zur vollen Nutzung aller Funktionen in Ceph ist es erforderlich, einige Basiskomponenten und Konzepte zu verstehen. In diesem Abschnitt werden einige Komponenten von Ceph eingeführt, auf die oft in anderen Kapiteln verwiesen wird.
  </para>

  <sect2 xml:id="storage.intro.core.rados">
   <title>RADOS</title>
   <para>
    Die Basiskomponente von Ceph wird <emphasis>RADOS</emphasis>
    <emphasis>(Reliable Autonomic Distributed Object Store, Zuverlässiger autonomer dezentraler Objektspeicher)</emphasis> genannt. Sie ist für die Verwaltung der im Cluster gespeicherten Daten zuständig. Daten werden in Ceph normalerweise als Objekte gespeichert. Jedes Objekt besteht aus einer Kennung und den Daten.
   </para>
   <para>
    RADOS bietet die folgenden Methoden für den Zugriff auf gespeicherte Objekte, die viele Anwendungsfälle abdecken:
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Ein Object Gateway ist ein HTTP REST Gateway für den RADOS-Objektspeicher. Es ermöglicht den Direktzugriff auf Objekte, die im Ceph Cluster gespeichert sind.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RADOS-Blockgerät</term>
     <listitem>
      <para>
       Der Zugriff auf RADOS-Blockgeräte (RADOS Block Device, RBD) erfolgt genauso wie auf andere Blockgeräte. Sie werden beispielsweise für Virtualisierungszwecke in Kombination mit <systemitem class="library">libvirt</systemitem> verwendet.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       Das Ceph File System (CephFS) ist ein POSIX-fähiges Dateisystem.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem>
     </term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> ist eine Bibliothek, die mit vielen Programmiersprachen verwendet werden kann, um eine Anwendung zu erstellen, die direkt mit dem Speicher-Cluster interagiert.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> wird vom Object Gateway und RBD verwendet, während CephFS direkt mit RADOS-<xref linkend="storage.intro.core.rados.figure"/> interagiert.
   </para>
   <figure xml:id="storage.intro.core.rados.figure">
    <title>Schnittstellen zum Ceph-Objektspeicher</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage.intro.core.crush">
   <title>CRUSH</title>
   <para>
    Der <emphasis>CRUSH</emphasis>-Algorithmus ist das zentrale Element eines Ceph Clusters. CRUSH ist das Akronym für <emphasis>Controlled Replication Under Scalable Hashing</emphasis>. CRUSH ist eine Funktion für die Speicherzuordnung und benötigt vergleichsweise wenige Parameter. Es sind also nur wenige Informationen erforderlich, um die Speicherposition eines Objekts zu berechnen. Die Parameter stellen eine aktuelle Zuordnung für den Cluster dar, einschließlich Zustand, einige vom Administrator definierte Platzierungsregeln und der Name des Objekts, das gespeichert oder abgerufen werden muss. Mit diesen Informationen können alle Nodes im Ceph Cluster berechnen, wo ein Objekt und dessen Reproduktionen gespeichert sind. Dies macht das Schreiben und Lesen von Daten sehr effizient. CRUSH versucht, Daten gleichmäßig auf alle Nodes im Cluster zu verteilen.
   </para>
   <para>
    Die <emphasis>CRUSH Map</emphasis> umfasst alle Speicher-Nodes und vom Administrator definierte Platzierungsregeln zum Speichern von Objekten im Cluster. Sie definiert eine hierarchische Struktur, die normalerweise mit der physischen Struktur des Clusters korrespondiert. Beispielsweise befinden sich Festplatten mit Daten in Hosts, Hosts in Racks, Racks in Reihen und Reihen in Rechenzentren. Diese Struktur wird zur Definition von <emphasis>Fehlerdomänen</emphasis> herangezogen. Ceph stellt dann sicher, dass Reproduktionen in verschiedenen Verzweigungen einer bestimmten Fehlerdomäne gespeichert werden.
   </para>
   <para>
    Wenn die Fehlerdomäne auf "Rack" festgelegt ist, werden Reproduktionen von Objekten auf verschiedene Racks verteilt. Dadurch werden Ausfälle entschärft, die durch einen fehlerhaften Schalter in einem Rack verursacht wurden. Wenn eine Stromversorgungseinheit eine Reihe von Racks bereitstellt, kann die Fehlerdomäne auf "Reihe" festgelegt werden. Wenn die Stromversorgungseinheit ausfällt, sind die Reproduktionsdaten noch in anderen Reihen verfügbar.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.core.nodes">
   <title>Ceph Nodes und Daemons</title>
   <para>
    In Ceph sind Nodes Server, die für den Cluster arbeiten. Sie können verschiedene Typen von Daemons ausführen. Es wird empfohlen, nur einen Typ von Daemon pro Node auszuführen, ausgenommen MGR Daemons, die gemeinsam mit MONs auf einem Node laufen. Jeder Cluster benötigt mindestens MON, MGR und OSD Daemons:
   </para>
   <variablelist>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       <emphasis>Ceph Monitor</emphasis> (oft als <emphasis>MON</emphasis> abgekürzt) Nodes pflegen Informationen zum Cluster-Zustand, eine Zuordnung aller Nodes und Datenverteilungsregeln (weitere Informationen hierzu finden Sie in <xref linkend="storage.intro.core.crush"/>).
      </para>
      <para>
       Wenn Fehler oder Konflikte auftreten, entscheiden die Ceph Monitor Nodes im Cluster nach dem Mehrheitsprinzip, welche Informationen korrekt sind. Um eine qualifizierte Mehrheit zu bilden, empfiehlt es sich, eine ungerade Anzahl von Ceph Monitor Nodes einzurichten, jedoch mindestens drei davon.
      </para>
      <para>
       Bei mehreren Standorten sollten die Ceph Monitor Nodes auf eine ungerade Anzahl von Standorten verteilt werden. Die Anzahl der Ceph Monitor Nodes pro Standort sollte so gewählt werden, dass die Hälfte der Ceph Monitor Nodes funktionsfähig bleiben, wenn ein Standort ausfällt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       Der Ceph Manager (MGR) sammelt die Statusinformationen aus dem gesamten Cluster. Der Ceph Manager Daemon wird neben den Monitor Daemons ausgeführt. Er bietet zusätzliche Überwachung und dient als Schnittstelle zwischen der externen Überwachung und den Verwaltungssystemen.
      </para>
      <para>
       Der Ceph Manager muss nicht weiter konfiguriert werden. Sie müssen nur sicherstellen, dass er ausgeführt wird. Sie können ihn mit DeepSea als separate Rolle bereitstellen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       Ein <emphasis>Ceph OSD</emphasis> ist ein Daemon, der <emphasis>Objektspeichergeräte</emphasis> steuert. Dabei handelt es sich um physische oder logische Speichereinheiten (Festplatten oder Partitionen). Objektspeichergeräte können physische Datenträger/Partitionen oder logische Volumes sein. Der Daemon kümmert sich außerdem um die Datenreproduktion und den Ausgleich, falls Nodes hinzugefügt oder entfernt wurden.
      </para>
      <para>
       Ceph OSD Daemons kommunizieren mit Monitor Daemons und informieren diese über den Zustand der anderen OSD Daemons.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Für CephFS, Object Gateway, NFS Ganesha oder iSCSI Gateway sind zusätzliche Nodes erforderlich:
   </para>
   <variablelist>
    <varlistentry>
     <term>Metadata Server (MDS)</term>
     <listitem>
      <para>
       Metadata Server speichern Metadaten für das CephFS. Über einen MDS führen Sie einfache Dateisystemkommandos wie <command>ls</command> aus, ohne den Cluster zu überlasten.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Das Ceph Object Gateway ist ein HTTP REST Gateway für den RADOS-Objektspeicher. Es ist mit OpenStack Swift und Amazon S3 kompatibel und verfügt über eine eigene Benutzerverwaltung.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       NFS Ganesha bietet NFS-Zugriff auf das Object Gateway oder auf das CephFS. Es wird im Benutzerbereich statt im Kernel-Bereich ausgeführt und interagiert direkt mit dem Object Gateway oder dem CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI Gateway</term>
     <listitem>
      <para>
       iSCSI ist ein Speichernetzwerkprotokoll, über das Clients SCSI-Kommandos an SCSI-Speichergeräte (Targets) auf Remote Servern senden können.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.intro.structure">
  <title>Speicherstruktur</title>

  <sect2 xml:id="storage.intro.structure.pool">
   <title>Pool</title>
   <para>
    In einem Ceph Cluster gespeicherte Objekte werden in <emphasis>Pools</emphasis> abgelegt. Pools stellen logische Partitionen des Clusters zur Außenwelt dar. Für jeden Pool kann ein Regelsatz definiert werden, beispielsweise wie viele Reproduktionen eines jeden Objekts vorhanden sein müssen. Die Standardkonfiguration von Pools wird als <emphasis>reproduzierter Pool</emphasis> bezeichnet.
   </para>
   <para>
    Pools enthalten normalerweise Objekte, können jedoch auch so konfiguriert werden, dass sie wie ein RAID 5 funktionieren. In dieser Konfiguration werden Objekte in Datenblöcken zusammen mit zusätzlichen Codierungs-Datenblöcken gespeichert. Die Codierungs-Datenblöcke enthalten die redundanten Informationen. Die Anzahl der Daten und Codierungs-Datenblöcke werden vom Administrator definiert. In dieser Konfiguration werden Pools als <emphasis>Erasure Coded Pools</emphasis> bezeichnet.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.pg">
   <title>Placement Group</title>
   <para>
    <emphasis>Placement Groups</emphasis> (PGs) dienen zur Verteilung von Daten in einem Pool. Beim Erstellen eines Pools wird eine bestimmte Anzahl von Placement Groups festgelegt. Die Placement Groups werden intern zur Gruppierung von Objekten verwendet. Sie sind ein wichtiger Faktor für die Leistung eines Ceph Clusters. Die PG für ein Objekt wird durch den Namen des Objekts bestimmt.
   </para>
  </sect2>

  <sect2 xml:id="storage.intro.structure.example">
   <title>Beispiel</title>
   <para>
    In diesem Abschnitt finden Sie ein vereinfachtes Beispiel dafür wie Ceph-Daten verwaltet (<xref linkend="storage.intro.structure.example.figure"/>). Dieses Beispiel stellt keine empfohlene Konfiguration für einen Ceph Cluster dar. Die Hardwareeinrichtung umfasst drei Speicher-Nodes oder Ceph OSDs (<literal>Host 1</literal>, <literal>Host 2</literal>, <literal>Host 3</literal>). Jeder Node hat drei Festplatten, die als OSDs (<literal>osd.0</literal> bis <literal>osd.9</literal>) verwendet werden. In diesem Beispiel sind keine Ceph Monitor Nodes berücksichtigt.
   </para>
   <note>
    <title>Unterschied zwischen Ceph OSD und OSD</title>
    <para>
     Die Begriffe <emphasis>Ceph OSD</emphasis> oder <emphasis>Ceph OSD Daemon</emphasis> beziehen sich auf einen Daemon, der auf einem Node ausgeführt wird. Der Begriff <emphasis>OSD</emphasis> dagegen bezieht sich auf einen logischen Datenträger, mit dem der Daemon interagiert.
    </para>
   </note>
   <para>
    Der Cluster umfasst zwei Pools, <literal>Pool A</literal> und <literal>Pool B</literal>. Während Pool A Objekte nur zweimal reproduziert, ist Ausfallsicherheit für Pool B wichtiger und es gibt daher drei Reproduktionen für jedes Objekt.
   </para>
   <para>
    Wenn eine Anwendung ein Objekt in einen Pool stellt, beispielsweise über die REST API, dann wird eine Placement Group (<literal>PG1</literal> bis <literal>PG4</literal>) auf Basis des Pools und des Objektnamens ausgewählt. Der CRUSH Algorithmus berechnet dann, auf welchen OSDs das Objekt gespeichert ist, basierend auf der Placement Group, die das Objekt enthält.
   </para>
   <para>
    In diesem Beispiel ist die Fehlerdomäne auf Host festgelegt. Dadurch wird sichergestellt, dass die Reproduktionen von Objekten auf verschiedenen Hosts gespeichert sind. Je nach der für einen Pool festgelegten Reproduktionsstufe wird das Objekt auf zwei oder drei OSDs gespeichert, die von der Placement Group verwendet werden.
   </para>
   <para>
    Eine Anwendung, die ein Objekt schreibt, interagiert nur mit einem Ceph OSD, nämlich dem primären Ceph OSD. Der primäre Ceph OSD führt die Reproduktion aus und bestätigt die Durchführung des Schreibvorgangs, nachdem alle anderen OSDs das Objekt gespeichert haben.
   </para>
   <para>
    Wenn <literal>osd.5</literal> ausfällt, sind alle Objekte in <literal>PG1</literal> immer noch auf <literal>osd.1</literal> verfügbar. Sobald der Cluster feststellt, dass ein OSD ausgefallen ist, übernimmt ein anderer OSD. In diesem Beispiel wird <literal>osd.4</literal> als Ersatz für <literal>osd.5</literal> verwendet. Die auf <literal>osd.1</literal> gespeicherten Objekte werden dann auf <literal>osd.4</literal> reproduziert, um die Reproduktionsstufe wiederherzustellen.
   </para>
   <figure xml:id="storage.intro.structure.example.figure">
    <title>Beispiel eines kleinen Ceph-Speichers</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Die Cluster-Zuordnung ändert sich, wenn ein neuer Node mit neuen OSDs zum Cluster hinzugefügt wird. Die CRUSH Funktion gibt dann verschiedene Standorte für Objekte zurück. Objekte, die neue Standorte erhalten, werden verschoben. Durch diesen Vorgang bleiben alle OSDs gleichmäßig ausgelastet.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about.bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore ist ab SUSE Enterprise Storage 5 ein neues standardmäßiges Speicher-Backend für Ceph. Seine Leistung ist besser als die von FileStore. Es umfasst vollständige Prüfsummenerstellung für Daten und weist eine integrierte Komprimierung auf.
  </para>

  <para>
   BlueStore verwaltet ein, zwei oder drei Speichergeräte. Im einfachsten Fall lastet BlueStore ein einzelnes primäres Speichergerät aus. Das Speichergerät ist normalerweise in zwei Abschnitte partitioniert:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Eine kleine Partition namens BlueFS, die Dateisystem-ähnliche Funktionalitäten implementiert, die von RocksDB benötigt werden.
    </para>
   </listitem>
   <listitem>
    <para>
     Eine große Partition belegt normalerweise das restliche Gerät. Sie wird direkt von BlueStore verwaltet und enthält alle Ist-Daten. Dieses primäre Gerät ist normalerweise durch eine Blocksymbolverknüpfung im Datenverzeichnis gekennzeichnet.
    </para>
   </listitem>
  </orderedlist>

  <para>
   BlueStore kann auch auf zwei weiteren Geräten bereitgestellt werden:
  </para>

  <para>
   Ein <emphasis>WAL-Gerät</emphasis>, das für das interne Journal oder Write Ahead-Protokoll von BlueStore verwendet wird. Es ist durch den symbolischen Link <literal>block.wal</literal> im Datenverzeichnis gekennzeichnet. Ein separates WAL-Gerät ist nur sinnvoll, wenn das Gerät schneller als das primäre Gerät oder das DB-Gerät ist. Beispiel:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Das WAL-Gerät ist ein NVMe, das DB-Gerät ist ein SSD und das Datengerät ist entweder ein SSD oder HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Das WAL-Gerät und das DB-Gerät sind separate SSDs. Das Datengerät ist ein SSD oder HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Ein <emphasis>DB-Gerät</emphasis> kann zum Speichern der internen Metadaten von BlueStore verwendet werden. BlueStore (eigentlich die eingebettete RocksDB) legt zur Leistungsoptimierung so viele Metadaten wie möglich auf dem DB-Gerät ab. Auch hier ist die Bereitstellung eines gemeinsam genutzten DB-Geräts nur sinnvoll, wenn es schneller ist als das primäre Gerät.
  </para>

  <tip>
   <title>Die DB-Größe planen</title>
   <para>
    Planen Sie sorgfältig eine ausreichende Größe für das DB-Gerät. Wenn das DB-Gerät voll ist, laufen die Metadaten an das primäre Gerät über, was die Leistung des OSDs extrem beeinträchtigt.
   </para>
   <para>
    Mit dem Kommando <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command> können Sie prüfen, ob eine WAL/DB-Partition voll wird und überläuft. Der Wert <option>slow_used_bytes</option> gibt die Anzahl der Daten an, die überlaufen:
   </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage.moreinfo">
  <title>Weitere Informationen</title>

  <itemizedlist>
   <listitem>
    <para>
     Ceph hat als Community-Projekt eine eigene Online-Dokumentation. Weitere Informationen zu Themen, die in diesem Handbuch nicht behandelt werden, finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     In der ursprünglichen Veröffentlichung mit dem Titel <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> von <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> finden Sie nützliche Informationen zu den inneren Abläufen in Ceph. Die Lektüre empfiehlt sich vor allem für die Bereitstellung von Clustern mit großem Volumen. Sie finden die Veröffentlichung unter <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
