<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph.pools">
 <title>Verwalten von Speicher-Pools</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>Ja</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph speichert Daten in Pools. Pools sind logische Gruppen für Speicherobjekte. Wenn Sie zunächst einen Cluster bereitstellen, ohne einen Pool zu erstellen, verwendet Ceph die Standard-Pools zum Speichern von Daten. Ein Pool zeichnet sich durch Folgendes aus:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Stabilität</emphasis>: Sie legen fest, wie viele OSDs ausfallen dürfen, ohne Daten zu verlieren. Bei reproduzierten Pools ist das die gewünschte Anzahl von Kopien/Reproduktionen eines Objekts. Neue Pools werden mit der Standardanzahl von 3 Reproduktionen erstellt. Da bei einer normalen Konfiguration ein Objekt und eine zusätzliche Kopie gespeichert werden, müssen Sie die Anzahl der Reproduktionen auf 2 festlegen. Bei Erasure Coded Pools ist es die Anzahl der Codierungs-Datenblöcke (d. h. <emphasis>m=2</emphasis> im Erasure Code Profil).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Placement Groups</emphasis>: sind interne Datenstrukturen zum Speichern von Daten in einem Pool mehrerer OSDs. In einer CRUSH Map wird definiert, wie Ceph Daten in PGs speichert. Die Anzahl der Placement Groups für den Pool kann festgelegt werden. Bei einer normalen Konfiguration sind etwa 100 Placement Groups pro OSD vorhanden. Dadurch wird ein optimaler Ausgleich ermöglicht, ohne zu viel Rechnerressourcen zu verbrauchen. Achten Sie bei der Einrichtung von mehreren Pools sorgfältig darauf, dass Sie eine vernünftige Anzahl von Placement Groups sowohl für den Pool als auch den Cluster insgesamt festlegen.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH-Regeln</emphasis>: Wenn Sie Daten in einem Pool speichern, findet CRUSH anhand eines Regelsatzes, der dem Pool zugeordnet ist, eine Regel für die Platzierung des Objekts und dessen Reproduktionen (oder Datenblöcken bei Erasure Coded Pools). Es ist auch möglich, eine benutzerdefinierte CRUSH-Regel für Ihren Pool zu erstellen.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshots</emphasis>: Wenn Sie einen Snapshot mit <command>ceph osd pool mksnap</command> erstellen, machen Sie im Grunde einen Snapshot von einem bestimmten Pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Eigentümerschaft festlegen</emphasis>: Legen Sie eine Benutzer-ID als Eigentümer eines Pools fest.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Zum Strukturieren der Daten in Pools können Sie Pools auflisten, erstellen und entfernen. Es ist auch möglich, die Auslastungsstatistik für jeden Pool anzuzeigen.
 </para>
 <sect1 xml:id="ceph.pools.associate">
  <title>Verknüpfen von Pools mit einer Anwendung</title>

  <para>
   Bevor Sie Pools verwenden, müssen Sie sie mit einer Anwendung verknüpfen. Mit dem CephFS verwendete Pools oder automatisch von Object Gateway erstellte Pools werden automatisch verknüpft. Pools, die zur Verwendung mit RBD vorgesehen sind, müssen mit dem Werkzeug <command>rbd</command> initialisiert werden (weitere Informationen finden Sie in <xref linkend="ceph.rbd.commands"/>).
  </para>

  <para>
   In anderen Fällen können Sie manuell einen frei formulierten Anwendungsnamen mit einem Pool verknüpfen:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>Standardanwendungsnamen</title>
   <para>
    CephFS verwendet den Anwendungsnamen <literal>cephfs</literal>, RADOS Block Device verwendet <literal>rbd</literal> und Object Gateway verwendet <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   Ein Pool kann mit mehreren Anwendungen verknüpft werden und jede Anwendung kann eigene Metadaten enthalten. Die Metadaten der Anwendung für einen angegebenen Pool werden mit dem folgenden Kommando angezeigt:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.pools.operate">
  <title>Arbeiten mit Pools</title>

  <para>
   In diesem Abschnitt erhalten Sie praktische Informationen zur Ausführung einfacher Prozesse mit Pools. Sie erfahren, wie Sie Pools auflisten, erstellen und löschen, wie Sie Pool-Statistiken anzeigen oder Snapshots eines Pools verwalten.
  </para>

  <sect2>
   <title>Auflisten von Pools</title>
   <para>
    Führen Sie zum Auflisten der Pools Ihres Clusters Folgendes aus:
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Erstellen eines Pools</title>
   <para>
    Führen Sie zum Erstellen eines reproduzierten Pools Folgendes aus:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Führen Sie zum Erstellen eines Erasure Coded Pools Folgendes aus:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    <command>ceph osd pool create</command> wird möglicherweise nicht ausgeführt, wenn Sie die zulässige Anzahl der Placement Groups pro OSD überschreiten. Der Grenzwert wird mit der Option <option>mon_max_pg_per_osd</option> festgelegt.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       Der Name des Pools. Er muss eindeutig sein. Diese Option muss aktiviert sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Die Gesamtanzahl der Placement Groups für den Pool. Diese Option muss aktiviert sein. Die Standardeinstellung ist 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Die Gesamtanzahl der Placement Groups für Platzierungszwecke. Dieser Wert sollte mit der Gesamtanzahl der Placement Groups identisch sein. Szenarios mit Aufteilung der Placement Groups sind davon ausgenommen. Diese Option muss aktiviert sein. Die Standardeinstellung ist 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       Der Pool-Typ, entweder <emphasis>replicated</emphasis> (reproduziert), um eine Wiederherstellung verlorener OSDs durch Beibehaltung mehrerer Kopien von Objekten zu ermöglich, oder <emphasis>erasure</emphasis>, um eine Art von genereller RAID5-Funktion zu erhalten. Reproduzierte Pools benötigen zwar mehr Basisspeicher, implementieren jedoch alle Ceph-Operationen. Erasure Coded Pools benötigen weniger Basisspeicher, implementieren jedoch nur eine Teilmenge der verfügbaren Operationen. Die Standardeinstellung ist "replicated".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       Der Name des CRUSH-Regelsatzes für diesen Pool. Wenn der angegebene Regelsatz nicht vorhanden ist, wird der reproduzierte Pool nicht erstellt und -ENOENT wird ausgegeben. Der reproduzierte Pool erstellt einen neuen Erasure-Regelsatz mit dem angegebenen Namen. Der Standardwert ist "erasure-code" für einen Erasure Coded Pool. Übernimmt die Ceph-Konfigurationsvariable <option>osd_pool_default_crush_replicated_ruleset</option> für den reproduzierten Pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       Nur für Erasure Coded Pools. Verwenden Sie das Erasure Code Profil. Es muss ein vorhandenes Profil sein, das durch <command>osd erasure-code-profile set</command> definiert wurde.
      </para>
      <para>
       Legen Sie beim Erstellen eines Pools die Anzahl der Placement Groups auf einen sinnvollen Wert fest (zum Beispiel 100). Denken Sie auch an die Gesamtanzahl der Placement Groups pro OSD. Placement Groups sind rechnerisch aufwendig, daher verschlechtert sich die Leistung, wenn viele Pools mit vielen Placement Groups vorhanden sind (wie zum Beispiel 50 Pools mit je 100 Placement Groups). Der Punkt, ab dem sich die Rückgaben verschlechtern hängt von der Leistung des OSD-Hosts ab.
      </para>
      <para>
       Unter <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement Groups</link> finden Sie detaillierte Informationen zur Berechnung einer angemessenen Anzahl von Placement Groups für Ihren Pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       Die erwartete Anzahl von Objekten für diesen Pool. Wenn Sie diesen Wert festlegen, wird der PG-Ordner zum Zeitpunkt der Erstellung des Pools aufgeteilt. Dadurch werden die negativen Auswirkungen der Latenz vermieden, die bei einer Aufteilung bei Laufzeit auftritt.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Festlegen von Pool-Kontingenten</title>
   <para>
    Pool-Kontingente können für die maximale Anzahl von Byte und/oder die maximale Anzahl von Objekten pro Pool festgelegt werden.
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Zum Entfernen eines Kontingents legen Sie den entsprechenden Wert auf 0 fest.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Löschen eines Pools</title>
   <warning>
    <title>Die Löschung eines Pools lässt sich nicht rückgängig machen</title>
    <para>
     Pools enthalten möglicherweise wichtige Daten. Durch Löschen eines Pools gehen alle Daten im Pool verloren und es gibt keine Möglichkeit, sie wiederherzustellen.
    </para>
   </warning>
   <para>
    Da die unbeabsichtigte Löschung von Pools eine echte Gefahr darstellt, implementiert Ceph zwei Mechanismen, durch die eine Löschung verhindert wird. Beide Mechanismen müssen deaktiviert werden, bevor ein Pool gelöscht werden kann.
   </para>
   <para>
    Der erste Mechanismus ist die Flagge <literal>NODELETE</literal>. Diese Flagge ist bei jedem Pool gesetzt und der Standardwert ist "false". Führen Sie folgendes Kommando aus, um den Wert dieser Flagge an einem Pool festzustellen:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Wenn das Kommando <literal>nodelete: true</literal> ausgibt, ist es erst möglich, den Pool zu löschen, wenn Sie die Flagge mit folgendem Kommando ändern:
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    Der zweite Mechanismus ist der Cluster-weite Konfigurationsparameter <option>mon allow pool delete</option>, der standardmäßig auf "false" festgelegt ist. Dies bedeutet, dass es standardmäßig nicht möglich ist, einen Pool zu löschen. Die angezeigte Fehlermeldung sieht folgendermaßen aus:
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Zum Löschen des Pools trotz der Sicherheitseinstellung legen Sie vorübergehend <option>mon allow pool delete</option> auf "true" fest, löschen den Pool und legen den Parameter dann wieder auf "false" fest:
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    Das Kommando <command>injectargs</command> zeigt die folgende Meldung an:
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Dadurch wird einfach nur bestätigt, dass das Kommando erfolgreich ausgeführt wurde. Dies ist kein Fehler.
   </para>
   <para>
    Wenn Sie für einen erstellten Pool eigene Regelsätze und Regeln festgelegt haben, sollten Sie diese entfernen, wenn Sie den Pool nicht länger benötigen. Wenn Sie Benutzer mit Berechtigungen genau für einen Pool erstellt haben, der nicht mehr vorhanden ist, sollten Sie diese Benutzer ebenfalls löschen.
   </para>
  </sect2>

  <sect2>
   <title>Umbenennen eines Pools</title>
   <para>
    Führen Sie zum Umbenennen eines Pools folgendes Kommando aus:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Wenn Sie einen Pool umbenennen und Capabilities pro Pool für einen authentifizierten Benutzer festgelegt haben, müssen Sie die Capabilities des Benutzer mit dem neuen Pool-Namen aktualisieren.
   </para>
  </sect2>

  <sect2>
   <title>Anzeigen der Pool-Statistik</title>
   <para>
    Führen Sie zum Anzeigen der Auslastungsstatistik eines Pools folgendes Kommando aus:
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Festlegen von Pool-Werten</title>
   <para>
    Führen Sie zum Festlegen des Werts für einen Pool folgendes Kommando aus:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    Sie können Werte für folgende Schlüssel festlegen:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Legt die Anzahl der Reproduktionen für Objekte im Pool fest. Weitere Informationen finden Sie in <xref linkend="ceph.pools.options.num_of_replicas"/>. Nur reproduzierte Pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Legt die Mindestanzahl von Reproduktionen fest, die für E/A benötigt werden. Weitere Informationen finden Sie in <xref linkend="ceph.pools.options.num_of_replicas"/>. Nur reproduzierte Pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       Der Zeitraum in Sekunden, für den Clients bestätigte, aber noch nicht zugewiesene Anforderungen erneut wiedergeben können.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Die Anzahl der Placement Groups für den Pool. Falls Sie OSDs zum Cluster hinzufügen, sollten Sie den Wert der Placement Groups erhöhen. Weitere Informationen finden Sie in <xref linkend="storage.bp.cluster_mntc.add_pgnum"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Die effektive Anzahl von Placement Groups zum Berechnen der Datenplatzierung.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       Der zu verwendende Regelsatz für die Zuordnung der Objektplatzierung im Cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Setzen Sie die Flagge HASHPSPOOL an einem angegebenen Pool (1) oder entfernen Sie sie (0). Durch Aktivieren dieser Flagge wird der Algorithmus geändert, um PGs besser auf OSDs zu verteilen. Nach Aktivierung dieser Flagge für einen Pool, dessen HASHPSPOOL-Flagge auf 0 festgelegt wurde, beginnt der Cluster einen Abgleich, um erneut eine korrekte Platzierung aller PGs zu erreichen. Denken Sie daran, dass sich dadurch möglicherweise eine erhebliche E/A-Last für einen Cluster ergibt. Daher müssen stark ausgelastete Produktions-Cluster sehr gut geplant werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Verhindert, dass der Pool entfernt wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Verhindert, dass die Werte <option>pg_num</option> und <option>pgp_num</option> des Pools geändert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Verhindert, dass die Größe des Pools geändert wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Damit setzen oder entfernen Sie die Flagge <literal>WRITE_FADVISE_DONTNEED</literal> für einen vorhandenen Pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Deaktiviert (Deep) Scrubbing der Daten für den entsprechenden Pool, um eine vorübergehend hohe E/A-Last zu vermeiden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Aktiviert die Treffersatz-Verfolgung für Cache Pools. Weitere Informationen finden Sie unter <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloomfilter</link>. Für diese Option sind die folgenden Werte möglich: <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. Der Standardwert ist <literal>bloom</literal>, die anderen Werte sind nur für Testzwecke vorgesehen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       Die Anzahl der Treffersätze, die für Cache Pools gespeichert werden sollen. Je höher die Anzahl, desto mehr RAM wird vom <systemitem>ceph-osd</systemitem>-Daemon belegt. Der Standardwert ist <literal>0</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       Die Dauer einer Treffersatz-Periode in Sekunden für Cache Pools. Je höher die Anzahl, desto mehr RAM wird vom <systemitem>ceph-osd</systemitem>-Daemon belegt. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       Die falsch positive Wahrscheinlichkeit für den Bloom-Treffersatz-Typ. Weitere Informationen finden Sie unter <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloomfilter</link>. Der gültige Bereich ist 0,0 bis 1,0. Der Standardwert ist <literal>0,05</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Erzwingen Sie bei Erstellung eines Treffersatzes für das Cache Tiering, dass OSDs die MGZ-Zeitstempel (mittlere Greenwich-Zeit) verwenden. Dadurch wird sichergestellt, dass Nodes in verschiedenen Zeitzonen dasselbe Ergebnis zurückgeben. Der Standardwert ist <literal>1</literal>. Dieser Wert darf nicht geändert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der bearbeiteten (fehlerhaften) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte in den Hintergrundspeicher-Pool verschiebt. Der Standardwert ist <literal>.4</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der bearbeiteten (fehlerhaften) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte mit höherer Geschwindigkeit in den Hintergrundspeicher-Pool verschiebt. Die Standardeinstellung ist <literal>.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der unbearbeiteten (intakten) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte aus dem Cache Pool entfernt. Der Standardwert ist <literal>.8</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph beginnt mit dem Verschieben oder Entfernen von Objekten, wenn der Grenzwert <option>max_bytes</option> ausgelöst wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph beginnt mit dem Verschieben oder Entfernen von Objekten, wenn der Grenzwert <option>max_objects</option> ausgelöst wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Temperaturzerfallsrate zwischen zwei aufeinanderfolgenden <literal>hit_set</literal>. Der Standardwert ist <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Anzahl der meisten <literal>N</literal>-Vorkommen in <literal>hit_set</literal>s für die Temperaturberechnung. Der Standardwert ist <literal>1</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       Die Zeit (in Sekunden), bevor der Cache Tiering Agent ein Objekt vom Cache Pool in den Speicher-Pool verschiebt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       Die Zeit (in Sekunden), bevor der Cache Tiering Agent ein Objekt aus dem Cache Pool entfernt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Wenn diese Flagge bei Erasure Coding Pools aktiviert wird, stellt die Leseanforderung Teil-Lesevorgänge an alle Shards aus und wartet bis sie genügend Shards zum Decodieren erhält, die für den Client verarbeitet werden. Wenn im Fall von <emphasis>jerasure</emphasis> und <emphasis>isa</emphasis> Erasure Plugins die ersten <literal>K</literal>-Antworten zurückgegeben werden, dann wird die Anforderung des Clients sofort anhand der von diesen Antworten decodierten Daten verarbeitet. Diese trägt dazu bei, einige Ressourcen für bessere Leistung freizugeben. Diese Flagge wird aktuell nur für Erasure Coded Pools unterstützt. Der Standardwert ist <literal>0</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       Das Mindestintervall in Sekunden für ein Pool Scrubbing, wenn die Cluster-Last gering ist. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_scrub_min_interval</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       Das maximale Intervall in Sekunden für ein Pool Scrubbing, unabhängig von der Cluster-Last. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_scrub_max_interval</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       Das Intervall in Sekunden für ein <emphasis>deep</emphasis> Scrubbing des Pools. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_deep_scrub</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Abrufen von Pool-Werten</title>
   <para>
    Führen Sie zum Abrufen eines Werts aus einem Pool folgendes Kommando aus:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    Sie können die Werte für Schlüssel abrufen, die in <xref linkend="ceph.pools.values"/> aufgelistet sind, und zudem folgende Schlüssel:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Die Anzahl der Placement Groups für den Pool. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Die effektive Anzahl von Placement Groups zum Berechnen der Datenplatzierung. Der gültige Bereich ist gleich oder kleiner als <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Festlegen der Anzahl der Objektreproduktionen</title>
   <para>
    Führen Sie folgendes Kommando aus, um die Anzahl der Objektreproduktionen in einem reproduzierten Pool festzulegen:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    In <replaceable>num-replicas</replaceable> ist das Objekt selbst enthalten. Geben Sie 3 an, wenn Sie beispielsweise das Objekt und zwei Kopien des Objekts für insgesamt drei Instanzen des Objekts wünschen.
   </para>
   <para>
    Wenn Sie <replaceable>num-replicas</replaceable> auf 2 festlegen, wird nur <emphasis>eine</emphasis> Kopie Ihrer Daten erstellt. Wenn Sie eine Objektinstanz verlieren, müssen Sie sich darauf verlassen, dass die andere Kopie seit dem letzten <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">Scrubbing</link> bei der Wiederherstellung nicht beschädigt wurde.
   </para>
   <para>
    Wenn der Pool auf eine Reproduktion festgelegt wird, bedeutet dies, dass genau <emphasis>eine</emphasis> Instanz des Datenobjekts im Pool vorhanden ist. Wenn der OSD ausfällt, verlieren Sie die Daten. Ein Pool mit einer Reproduktion wird normalerweise verwendet, wenn temporäre Daten für kurze Zeit gespeichert werden.
   </para>
   <para>
    Die Festlegung von mehr als drei Reproduktionen für einen Pool erhöht die Zuverlässigkeit nur geringfügig, kann jedoch in einigen seltenen Fällen sinnvoll sein. Je mehr Reproduktionen erstellt werden, desto mehr Festplattenspeicherplatz zum Speichern von Objektkopien wird benötigt. Daran sollten Sie denken. Wir empfehlen Erasure Coded Pools, wenn Sie ultimative Datensicherheit benötigen. Weitere Informationen finden Sie in <xref linkend="cha.ceph.erasure"/>.
   </para>
   <warning>
    <title>Mehr als zwei Reproduktionen empfohlen</title>
    <para>
     Wir raten Ihnen dringend, mehr als zwei Reproduktionen zu verwenden. Falls ein OSD ausfällt, ist es sehr wahrscheinlich, dass der zweite OSD aufgrund des hohen Workloads bei der Wiederherstellung ebenfalls ausfällt.
    </para>
   </warning>
   <para>
    Beispiel:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    Dieses Kommando kann für jeden Pool ausgeführt werden.
   </para>
   <note>
    <para>
     Ein Objekt akzeptiert möglicherweise E/As im eingeschränkt leistungsfähigen Modus mit weniger als <literal>pool size</literal> Reproduktionen. Sie sollten die Einstellung <literal>min_size</literal> verwenden, um eine Mindestanzahl erforderlicher Reproduktionen für E/A festzulegen. Beispiel:
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Dadurch wird sichergestellt, dass kein Objekt im Daten-Pool E/A mit weniger Reproduktionen als <literal>min_size</literal> erhält.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Abrufen der Anzahl der Objektreproduktionen</title>
   <para>
    Führen Sie folgendes Kommando aus, um die Anzahl der Objektreproduktionen abzurufen:
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph listet die Pools mit hervorgehobenem Attribut <literal>replicated size</literal> auf. Standardmäßig erstellt Ceph zwei Reproduktionen eines Objekts (insgesamt drei Kopien oder eine Größe von 3).
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pgnum">
   <title>Erhöhen der Anzahl von Placement Groups</title>
   <para>
    Beim Erstellen eines neuen Pools geben Sie die Anzahl der Placement Groups für den Pool an (weitere Informationen finden Sie in <xref linkend="ceph.pools.operate.add_pool"/>). Nachdem Sie weitere OSDs zum Cluster hinzugefügt haben, müssen Sie normalerweise auch die Anzahl der Placement Groups erhöhen, um die Leistung und Datenhaltbarkeit zu verbessern. Für jede Placement Group benötigen der OSD und die Monitor Nodes ständig Speicherplatz, Netzwerkzugang und CPU, ganz besonders während der Wiederherstellung. Daraus folgt, dass durch Minimierung der Anzahl von Placement Groups erheblich Ressourcen gespart werden.
   </para>
   <warning>
    <title>Zu hoher Wert für <option>pg_num</option></title>
    <para>
     Beim Ändern des <option>pg_num</option>-Werts für einen Pool kommt es gelegentlich vor, dass die neue Anzahl der Placement Groups den zulässigen Grenzwert überschreitet. Beispiel
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     Der Grenzwert verhindert eine extreme Aufteilung der Placement Groups. Er wird aus dem Wert <option>mon_osd_max_split_count</option> abgeleitet.
    </para>
   </warning>
   <para>
    Die Ermittlung der richtigen Anzahl von Placement Groups für einen Cluster, dessen Größe geändert wurde, ist eine komplexe Aufgabe. Ein Ansatz erhöht die Anzahl der Placement Groups so lange kontinuierlich bis die optimale Cluster-Leistung erreicht ist. Die neue inkrementierte Anzahl von Placement Groups ermitteln Sie, indem Sie den Wert des Parameters <option>mon_osd_max_split_count</option> abrufen und diesen zur aktuellen Anzahl der Placement Groups hinzuaddieren. Eine allgemeine Vorstellung davon vermittelt Ihnen das folgende Skript:
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    Nachdem Sie die nächste Anzahl von Placement Groups herausgefunden haben, erhöhen Sie sie mit
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pool">
   <title>Hinzufügen eines Pools</title>
   <para>
    Nach der ersten Bereitstellung eines Clusters speichert Ceph die Daten in den Standard-Pools. Sie können später einen neuen Pool erstellen mit
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    Weitere Informationen zur Erstellung von Cluster Pools finden Sie in <xref linkend="ceph.pools.operate.add_pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools.migration">
  <title>Pool-Migration</title>

  <para>
   Beim Erstellen eines Pools (Informationen hierzu finden Sie in <xref linkend="ceph.pools.operate.add_pool"/>) müssen Sie die ersten Parameter wie den Pool-Typ oder die Anzahl der Placement Groups angeben. Wenn Sie später einen dieser Parameter ändern möchten, nachdem Sie bereits Daten im Pool platziert haben, dann müssen Sie die Pool-Daten zu einem anderen Pool migrieren, dessen Parameter für Ihre Bereitstellung geeignet sind.
  </para>

  <para>
   Es gibt mehrere Methoden zur Pool-Migration. Wir empfehlen die Methode des <emphasis>Cache Tierings</emphasis> (Aufteilen des Cache in mehrere Schichten), weil sie transparent ist, die Ausfallzeit des Clusters reduziert und die Duplizierung aller Daten des Pools vermeidet.
  </para>

  <sect2 xml:id="pool.migrate.cache_tier">
   <title>Migrieren über Cache Tiering</title>
   <para>
    Das Prinzip ist einfach: Stellen Sie den Pool, der migriert werden soll, in umgekehrter Reihenfolge in eine Cache-Schicht. Weitere Details hierzu finden Sie in <xref linkend="cha.ceph.tiered"/>. Wenn Sie beispielsweise einen reproduzierten Pool namens "testpool" zu einem Erasure Coded Pool migrieren möchten, führen Sie die folgenden Schritte aus:
   </para>
   <procedure>
    <title>Migrieren eines reproduzierten Pools zu einem Erasure Coded Pool</title>
    <step>
     <para>
      Erstellen Sie einen neuen Erasure Coded Pool namens "newpool":
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      Nun verfügen Sie über zwei Pools: den ursprünglichen reproduzierten "testpool" mit Daten und den neuen leeren Erasure Coded "newpool":
     </para>
     <figure>
      <title>Pools vor der Migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Richten Sie die Cache-Schicht ein und konfigurieren Sie den reproduzierten Pool "testpool" als Cache Pool:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      Ab hier werden alle neuen Objekte im neuen Pool erstellt:
     </para>
     <figure>
      <title>Einrichtung der Cache-Schicht</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Erzwingen Sie, dass der Cache Pool alle Objekte in den neuen Pool verschiebt:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Leeren von Daten</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Stellen Sie alle Clients auf den neuen Pool um. Solange nicht alle Daten geleert und zum neuen Erasure Coded Pool verschoben sind, müssen Sie eine Überlagerung angeben, sodass Objekte noch im alten Pool gesucht werden:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Bei einer Überlagerung werden alle Operationen an den alten reproduzierten "testpool" weitergeleitet:
     </para>
     <figure>
      <title>Festlegen der Überlagerung</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Nun können Sie alle Clients auf den Zugriff von Objekten im neuen Pool umstellen.
     </para>
    </step>
    <step>
     <para>
      Wenn alle Daten zum Erasure Coded "newpool" migriert sind, entfernen Sie die Überlagerung und den alten Cache Pool "testpool":
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration abgeschlossen</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Pool Snapshots</title>

  <para>
   Pool Snapshots sind Snapshots vom Zustand des gesamten Ceph Pools. Mit Pool Snapshots behalten Sie den Verlauf des Pool-Zustands bei. Abhängig von der Größe des Pools wird für Pool Snapshots möglicherweise viel Speicherplatz benötigt. Prüfen Sie immer zunächst, ob im betreffenden Speicher genügend Festplattenspeicherplatz vorhanden ist, bevor Sie einen Snapshot eines Pools erstellen.
  </para>

  <sect2>
   <title>Erstellen eines Pool Snapshots</title>
   <para>
    Führen Sie folgendes Kommando aus, um einen Snapshot von einem Pool zu erstellen:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Entfernen eines Snapshots eines Pools</title>
   <para>
    Führen Sie folgendes Kommando aus, um einen Snapshot eines Pool zu entfernen:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.pool.compression">
  <title>Datenkomprimierung</title>

  <para>
   Ab SUSE Enterprise Storage 5 bietet BlueStore eine direkte Datenkomprimierung, um Festplattenspeicherplatz zu sparen.
  </para>

  <sect2 xml:id="sec.ceph.pool.compression.enable">
   <title>Aktivieren der Komprimierung</title>
   <para>
    Die Datenkomprimierung für einen Pool wird aktiviert mit:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    Ersetzen Sie <replaceable>POOL_NAME</replaceable> durch den Pool, für den die Komprimierung aktiviert werden soll.
   </para>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.compression.options">
   <title>Optionen der Pool-Komprimierung</title>
   <para>
    Eine vollständige Liste der Komprimierungseinstellungen:
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Werte: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>. Standardeinstellung: <literal>snappy</literal>.
      </para>
      <para>
       Der zu verwendende Komprimierungsalgorithmus hängt vom einzelnen Anwendungsfall ab. Hier sehen Sie einige Empfehlungen:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Verwenden Sie nicht <literal>zlib</literal>: die anderen Algorithmen sind besser.
        </para>
       </listitem>
       <listitem>
        <para>
         Verwenden Sie <literal>zstd</literal>, wenn Sie eine gute Komprimierungsrate wünschen. Beachten Sie, dass <literal>zstd</literal> nicht für BlueStore empfohlen wird, weil bei der Komprimierung kleiner Datenmengen der CPU Overhead hoch ist.
        </para>
       </listitem>
       <listitem>
        <para>
         Verwenden Sie <literal>lz4</literal> oder <literal>snappy</literal>, wenn Sie eine geringere CPU-Auslastung wünschen.
        </para>
       </listitem>
       <listitem>
        <para>
         Vergleichen Sie diese Algorithmen anhand eines Beispiels Ihrer realen Daten und beobachten Sie dabei die CPU- und Arbeitsspeicherauslastung Ihres Clusters.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       Wert: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Standardeinstellung: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: niemals komprimieren
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: komprimieren bei Hinweis auf <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: komprimieren, falls kein Hinweis auf <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: immer komprimieren
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Informationen zum Setzen der Flagge <literal>COMPRESSIBLE</literal> oder <literal>INCOMPRESSIBLE</literal> finden Sie unter <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Wert: Gleitkomma "double", Grad = SIZE_COMPRESSED / SIZE_ORIGINAL. Standardeinstellung: <literal>.875</literal>
      </para>
      <para>
       Objekte mit einem höheren Grad werden nicht komprimiert gespeichert, weil der Nutzen insgesamt sehr gering ist.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Maximale Größe der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.bluestore_compression.options">
   <title>Globale Komprimierungsoptionen</title>
   <para>
    Die folgenden Konfigurationsoptionen können in der Ceph-Konfiguration festgelegt werden und gelten für alle OSDs und nicht nur für einen einzelnen Pool. Die in <xref linkend="sec.ceph.pool.compression.options"/> aufgelistete für den Pool spezifische Konfiguration hat Vorrang.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Werte: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>, <literal>zlib</literal>. Standardeinstellung: <literal>snappy</literal>.
      </para>
      <para>
       Der zu verwendende Komprimierungsalgorithmus hängt vom einzelnen Anwendungsfall ab. Hier sehen Sie einige Empfehlungen:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Verwenden Sie nicht <literal>zlib</literal>, die anderen Algorithmen sind besser.
        </para>
       </listitem>
       <listitem>
        <para>
         Verwenden Sie <literal>zstd</literal>, wenn Sie eine gute Komprimierungsrate wünschen. Beachten Sie, dass <literal>zstd</literal> nicht für BlueStore empfohlen wird, weil bei der Komprimierung kleiner Datenmengen der CPU Overhead hoch ist.
        </para>
       </listitem>
       <listitem>
        <para>
         Verwenden Sie <literal>lz4</literal> oder <literal>snappy</literal>, wenn Sie eine geringere CPU-Auslastung wünschen.
        </para>
       </listitem>
       <listitem>
        <para>
         Vergleichen Sie diese Algorithmen anhand eines Beispiels Ihrer realen Daten und beobachten Sie dabei die CPU- und Arbeitsspeicherauslastung Ihres Clusters.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Wert: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Standardeinstellung: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: niemals komprimieren
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: komprimieren bei Hinweis auf <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: komprimieren, falls kein Hinweis auf <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: immer komprimieren
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Informationen zum Setzen der Flagge <literal>COMPRESSIBLE</literal> oder <literal>INCOMPRESSIBLE</literal> finden Sie unter <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Wert: Gleitkomma "double", Grad = SIZE_COMPRESSED / SIZE_ORIGINAL. Standardeinstellung: <literal>.875</literal>
      </para>
      <para>
       Objekte mit einem höheren Grad werden nicht komprimiert gespeichert, weil der Nutzen insgesamt sehr gering ist.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Maximale Größe der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>8K</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert und auf Festkörperlaufwerk gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>64K</literal>
      </para>
      <para>
       Maximale Größe der Objekte, die komprimiert und auf einem Festkörperlaufwerk gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>128K</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert und auf Festplatten gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>512K</literal>
      </para>
      <para>
       Maximale Größe der Objekte, die komprimiert und auf Festplatten gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
