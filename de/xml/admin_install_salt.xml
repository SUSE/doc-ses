<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>Bereitstellen mit DeepSea/Salt</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Salt bildet zusammen mit DeepSea ein <emphasis>Paket</emphasis> von Komponenten, die bei der Bereitstellung und Verwaltung von Serverinfrastruktur nützlich sind. Es ist hoch skalierbar, schnell und lässt sich relativ leicht in Betrieb nehmen. Lesen Sie die folgenden Überlegungen, bevor Sie mit der Bereitstellung des Clusters mit Salt beginnen:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>Salt Minions</emphasis> sind die Nodes, die von einem dedizierten Node namens Salt Master gesteuert werden. Salt Minions verfügen über Rollen wie zum Beispiel Ceph OSD, Ceph Monitor, Ceph Manager, Object Gateway, iSCSI Gateway oder NFS Ganesha.
   </para>
  </listitem>
  <listitem>
   <para>
    Ein Salt Master führt seinen eigenen Salt Minion aus. Er ist erforderlich zum Ausführen von privilegierten Aufgaben, beispielsweise Erstellen, Autorisieren und Kopieren von Schlüsseln für Minions, sodass Remote Minions niemals privilegierte Aufgaben ausführen müssen.
   </para>
   <tip>
    <title>Freigeben mehrerer Rollen pro Server</title>
    <para>
     Sie erreichen die optimale Leistung Ihres Ceph Clusters, wenn jede Rolle in einem separaten Node bereitgestellt wird. Manchmal ist es jedoch bei Bereitstellungen erforderlich, einen Node für mehrere Rollen freizugeben. Stellen Sie die Ceph OSD-, Metadatenserver- oder Ceph Monitor-Rolle nicht auf dem Admin Node bereit, um Probleme mit der Leistung und dem Upgrade-Vorgang zu vermeiden.
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    Salt Minions müssen den Hostnamen des Salt Master im gesamten Netzwerk korrekt auflösen. Standardmäßig suchen sie nach dem Hostnamen <systemitem>salt</systemitem>, Sie können jedoch auch in Datei <filename>/etc/salt/minion</filename> jeden vom Netzwerk erreichbaren Hostnamen angeben. Weitere Informationen hierzu finden Sie in <xref linkend="ceph-install-stack"/>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>Lesen Sie die Versionshinweise</title>

  <para>
   In den Versionshinweisen finden Sie zusätzliche Informationen zu den Änderungen, die seit der vorigen Version von SUSE Enterprise Storage vorgenommen wurden. Informieren Sie sich in den Versionshinweisen über Folgendes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Sind bei der Hardware besondere Überlegungen zu beachten?
    </para>
   </listitem>
   <listitem>
    <para>
     Wurden erhebliche Änderungen an den verwendeten Software-Paketen vorgenommen?
    </para>
   </listitem>
   <listitem>
    <para>
     Gelten besondere Vorsichtsmaßnahmen für die vorliegende Installation?
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In den Versionshinweisen finden Sie auch Informationen, die erst nach der Fertigstellung des Handbuchs bekannt wurden. Auch bekannte Probleme werden beschrieben.
  </para>

  <para>
   Nach Installation des Pakets <package>release-notes-ses</package>finden Sie die Versionshinweise lokal im Verzeichnis <filename>/usr/share/doc/release-notes</filename> oder online unter <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>Einführung zu DeepSea</title>

  <para>
   Das Ziel von DeepSea besteht darin, dem Administrator Zeit zu sparen und komplexe Operationen im Ceph Cluster zuverlässig durchzuführen.
  </para>

  <para>
   Ceph ist eine umfassend konfigurierbare Softwarelösung. Systemadministratoren gewinnen dadurch mehr Freiheit, haben aber auch mehr Verantwortung.
  </para>

  <para>
   Die minimale Einrichtung von Ceph eignet sich gut für Demonstrationszwecke, zeigt jedoch nicht die interessanten Funktionen von Ceph, die bei einer großen Anzahl von Nodes zum Tragen kommen.
  </para>

  <para>
   DeepSea erfasst und speichert Daten über individuelle Server, wie Adressen und Gerätenamen. Bei einem dezentralen Speichersystem wie Ceph müssen gegebenenfalls Hunderte solcher Elemente erfasst und gespeichert werden. Manuelles Erfassen von Informationen und Eingeben der Daten in ein Konfigurationsmanagement-Tool ist ermüdend und fehleranfällig.
  </para>

  <para>
   Die zum Vorbereiten des Servers, Erfassen der Konfiguration sowie zum Konfigurieren und Bereitstellen von Ceph erforderlichen Schritte sind weitgehend dieselben. Dies trifft jedoch nicht auf das Verwalten der unterschiedlichen Funktionen zu. Bei täglich anfallenden Vorgängen braucht man unbedingt die Möglichkeit, Hardware problemlos zu einer vorliegenden Funktion hinzuzufügen und sie wieder zu entfernen.
  </para>

  <para>
   DeepSea setzt hierfür folgende Strategie ein: DeepSea konsolidiert die Entscheidungen des Administrators in einer einzelnen Datei. Zu den Entscheidungen zählen Cluster-Zuweisung, Rollenzuweisung und Profilzuweisung. Und DeepSea fasst jede Aufgabengruppe zu einem einfachen Ziel zusammen. Jedes Ziel ist eine <emphasis>Phase</emphasis>:
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>Beschreibung von DeepSea-Phasen</title>
   <listitem>
    <para>
     <emphasis role="bold">Phase 0</emphasis> – <emphasis role="bold">Vorbereitung</emphasis> – in dieser Phase werden alle erforderlichen Updates angewendet und Ihr System wird möglicherweise neugestartet.
    </para>
    <important>
     <title>Erneute Ausführung der Phase 0 nach dem Neustart des Admin Nodes</title>
     <para>
      Wenn in Phase 0 der Admin Node neu startet, um die neue Kernel-Version zu laden, müssen Sie Phase 0 erneut ausführen, ansonsten werden die Minions nicht adressiert.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 1</emphasis> – die <emphasis role="bold">Ermittlung</emphasis> – hier wird die gesamte Hardware im Cluster erkannt und die erforderlichen Informationen für die Ceph-Konfiguration werden zusammengestellt. Ausführliche Informationen zur Konfiguration finden Sie in <xref linkend="deepsea-pillar-salt-configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 2</emphasis> – <emphasis role="bold">Konfiguration</emphasis> – Konfigurationsdaten müssen in einem bestimmten Format vorbereitet werden.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 3</emphasis> – <emphasis role="bold">Bereitstellung</emphasis> – erstellt einen einfachen Ceph Cluster mit obligatorischen Ceph Services. Eine Liste der Services finden Sie in <xref linkend="storage-intro-core-nodes"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 4</emphasis> – <emphasis role="bold">Services</emphasis>– zusätzliche Funktionen von Ceph wie iSCSI, Object Gateway und CephFS können in dieser Phase installiert werden. Jede der Funktionen ist optional.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Phase 5</emphasis> – Entfernen. Diese Phase ist nicht obligatorisch und bei der ersten Einrichtung normalerweise nicht erforderlich. In dieser Phase werden die Rollen der Minions und auch die Cluster-Konfiguration entfernt. Sie müssen diese Phase ausführen, wenn Sie einen Speicher-Node von Ihrem Cluster entfernen müssen. Weitere Informationen finden Sie im <xref linkend="salt-node-removing"/>.
    </para>
   </listitem>
  </itemizedlist>



  <sect2 xml:id="deepsea-organisation-locations">
   <title>Organisation und wichtige Standorte</title>
   <para>
    Salt wendet für Ihren Master Node einige standardmäßige Standorte und Benennungskonventionen an:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename></term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Konfigurationsdaten für Ihre Cluster Minions gespeichert. <emphasis>Pillar</emphasis> ist eine Schnittstelle zum Bereitstellen globaler Konfigurationswerte für alle Cluster Minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename></term>
     <listitem>
      <para>
       In diesem Verzeichnis speichert Salt die Zustandsdateien (auch <emphasis>sls</emphasis>-Dateien genannt). Zustandsdateien sind formatierte Beschreibungen zum Zustand, in dem sich der Cluster befinden sollte.

      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename></term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Python-Skripts gespeichert, die als Ausführungsprogramme (runner) bezeichnet werden. Die Ausführungsprogramme werden im Master Node ausgeführt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename></term>
     <listitem>
      <para>
       In diesem Verzeichnis werden Python-Skripts gespeichert, die als Module bezeichnet werden. Die Module werden auf alle Minions in Ihrem Cluster angewendet.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename></term>
     <listitem>
      <para>
       Dieses Verzeichnis wird von DeepSea verwendet. Erfasste Konfigurationsdaten werden hier gespeichert.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename></term>
     <listitem>
      <para>
       Ein von DeepSea verwendetes Verzeichnis. In ihm werden sls-Dateien gespeichert, die unterschiedliche Formate aufweisen, doch jedes Unterverzeichnis enthält sls-Dateien. Jedes Unterverzeichnis enthält nur einen Typ von sls-Datei. Beispiel: <filename>/srv/salt/ceph/stage</filename> enthält Orchestrierungsdateien, die durch <command>salt-run state.orchestrate</command> ausgeführt werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>Adressieren der Minions</title>
   <para>
    DeepSea-Kommandos werden über die Salt-Infrastruktur ausgeführt. Für das <command>salt</command>-Kommando müssen Sie eine Gruppe von Salt Minions angeben, auf die das Kommando zutreffen soll. Wir beschreiben diese Gruppe von Minions als <emphasis>target</emphasis> für das <command>salt</command>-Kommando. In den folgenden Abschnitten werden mögliche Methoden zum Adressieren der Minions beschrieben.
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>Abgleichen des Minion-Namens</title>
    <para>
     Sie können einen Minion oder eine Gruppe von Minions adressieren, indem Sie deren Namen abgleichen. Der Name eines Minions ist normalerweise der kurze Hostname des Nodes, in dem die Minions ausgeführt werden. Diese Adressierungsmethode ist für Salt spezifisch und trifft nicht auf DeepSea zu. Sie können den Bereich der Minion-Namen durch Verwenden von Platzhaltern, reguläre Ausdrücke oder Listen eingrenzen. Im Allgemeinen sieht die Syntax folgendermaßen aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>Nur im Ceph Cluster</title>
     <para>
      Wenn alle Salt Minions in Ihrer Umgebung zu Ihrem Ceph Cluster gehören, können Sie <replaceable>target</replaceable> problemlos durch <literal>"*"</literal> ersetzen, um <emphasis>alle</emphasis> registrierten Minions einzubeziehen.
     </para>
    </tip>
    <para>
     Gleichen Sie alle Minions in der Domäne "example.net" ab (unter der Annahme, dass alle Minion-Namen mit den "vollständigen" Hostnamen identisch sind):
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     Gleichen Sie die "web1"- bis "web5"-Minions ab:
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     Gleichen Sie sowohl die "web1-prod"- als auch die "web1-devel"-Minions mit einem regulären Ausdruck ab.
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     Gleichen Sie eine einfache Liste von Minions ab:
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     Gleichen Sie alle Minions im Cluster ab:
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>Adressieren mithilfe eines DeepSea Grains</title>
    <para>
     In einer heterogenen, mit Salt verwalteten Umgebung, in der SUSE Enterprise Storage 6 auf einer Untergruppe von Knoten neben anderen Cluster-Lösungen installiert ist, müssen Sie die relevanten Minions mithilfe eines „deepsea“ Grains markieren, bevor Sie die DeepSea-Phase 0 ausführen. Auf diese Weise werden DeepSea Minions leicht in Umgebungen adressiert, in denen der Abgleich anhand des Minion-Namens problematisch ist.
    </para>
    <para>
     Führen Sie zum Anwenden des „deepsea“ Grain auf eine Gruppe von Minions folgendes Kommando aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     Führen Sie zum Entfernen des „deepsea“ Grain von einer Gruppe von Minions folgendes Kommando aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     Nach Anwenden des „deepsea“ Grain auf die relevanten Minions adressieren Sie diese wie folgt:
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     Das folgende Kommando funktioniert gleichermaßen:
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title>Festlegen der Option <option>deepsea_minions</option></title>
    <para>
     Für die DeepSea-Bereitstellungen muss das Ziel der Option <option>deepsea_minions</option> festgelegt werden. DeepSea gibt den Minions während der Ausführung der Phasen darüber Anweisungen (weitere Informationen hierzu finden Sie in der <xref linkend="deepsea-stage-description"/>).
    </para>
    <para>
     Bearbeiten Sie zum Festlegen oder Ändern der Option <option>deepsea_minions</option> die Datei <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> auf dem Salt Master und fügen Sie die folgende Zeile hinzu oder ersetzen Sie sie:
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title><option>deepsea_minions</option> Target</title>
     <para>
      Als <replaceable>target</replaceable> für die Option <option>deepsea_minions</option> können Sie eine der beiden Adressierungsmethoden anwenden: <xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/> und <xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>.
     </para>
     <para>
      Gleichen Sie alle Salt Minions im Cluster ab:
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      Gleichen Sie alle Minions mit „deepsea“ Grain ab:
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>Weiterführende Informationen</title>
    <para>
     Über die Salt-Infrastruktur können Sie anspruchsvollere Methoden zur Adressierung von Minions anwenden. Auf der Handbuchseite „deepsea-minions“ finden Sie außerdem weitere Informationen zur DeepSea-Adressierung (<command>man 7 deepsea_minions</command>).
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>Cluster-Bereitstellung</title>

  <para>
   Der Cluster-Bereitstellungsprozess besteht aus mehreren Phasen. Zunächst müssen Sie alle Nodes des Clusters durch Konfigurieren von Salt vorbereiten und dann Ceph bereitstellen und konfigurieren.
  </para>

  <tip xml:id="dev-env">
   <title>Bereitstellen von Monitor Nodes ohne OSD-Profile zu definieren</title>
   <para>
    Wenn Sie die Definition der OSD-Speicherrollen gemäß <xref linkend="policy-role-assignment"/> überspringen und die Monitor Nodes vorher bereitstellen müssen, können Sie dazu die Variable <option>DEV_ENV</option> festlegen.
   </para>
   <para>
    Dies ermöglicht die Bereitstellung von Monitors, auch wenn das Verzeichnis<filename>role-storage/</filename> nicht vorhanden ist, sowie die Bereitstellung eines Ceph Clusters mit mindestens <emphasis>einer</emphasis> Speicher-, Monitor- und Manager-Rolle.
   </para>
   <para>
    Zum Festlegen der Umgebungsvariable aktivieren Sie diese entweder global, indem Sie sie in Datei <filename>/srv/pillar/ceph/stack/global.yml</filename> festlegen, oder sie legen sie nur für die aktuelle Shell-Sitzung fest:
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
   <para>
    Als Beispiel kann <filename>/srv/pillar/ceph/stack/global.yml</filename> mit dem folgenden Inhalt erstellt werden:
   </para>
<screen>DEV_ENV: <replaceable>True</replaceable></screen>
  </tip>

  <para>
   Das folgende Verfahren beschreibt die Cluster-Vorbereitung im Detail.
  </para>

  <procedure>
   <step>
    <para>
     Installieren und registrieren Sie SUSE Linux Enterprise Server 15 SP1 zusammen mit der SUSE Enterprise Storage 6-Erweiterung auf jedem Node des Clusters.
    </para>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass die entsprechenden Produkte installiert und registriert sind. Listen Sie dazu die bestehenden Software-Repositorys auf. Führen Sie <command>zypper lr -E</command> aus und vergleichen Sie die Ausgabe mit folgender Liste:
    </para>
<screen>
 SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates
</screen>
   </step>
   <step>
    <para>
     Konfigurieren Sie Netzwerkeinstellungen einschließlich der ordnungsgemäßen DNS-Namensauflösung auf jedem Node. Der Salt Master und alle Salt Minions müssen sich gegenseitig anhand ihrer Hostnamen auflösen. Weitere Informationen zum Konfigurieren eines Netzwerks finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_network_yast.html"/>. Weitere Informationen zum Konfigurieren eines DNS Servers finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_dns.html"/>.
    </para>
   </step>
   <step>
    <para>
     Wählen Sie mindestens einen Zeitserver/Pool aus und synchronisieren Sie die lokale Zeit damit. Prüfen Sie bei jedem Starten des Systems, ob der Zeitsynchronisierungsservice aktiviert ist. Mit dem Kommando <command>yast ntp-client</command> in einem Paket <package>yast2-ntp-client</package> können Sie die Zeitsynchronisierung konfigurieren.
    </para>
    <tip>
     <para>
      Virtuelle Maschinen sind keine zuverlässigen NTP-Quellen.
     </para>
    </tip>
    <para>
     Weitere Informationen zum Einrichten von NTP finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html"/>.
    </para>
   </step>
   <step>
    <para>
     Installieren Sie die Pakete <literal>salt-master</literal> und <literal>salt-minion</literal> im Salt Master Node:
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     Überprüfen Sie, ob der <systemitem>salt-master</systemitem> Service aktiviert und gestartet wurde und aktivieren und starten Sie ihn gegebenenfalls:
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     Falls Sie beabsichtigen, eine Firewall zu verwenden, verifizieren Sie, dass im Salt Master Node die Ports 4505 und 4506 für alle Salt Minion Nodes offen sind. Wenn die Ports geschlossen sind, können Sie diese mit dem Kommando <command>yast2 firewall</command> öffnen, indem Sie den <guimenu>SaltStack</guimenu> Service zulassen.
    </para>
    <warning>
     <title>DeepSea-Phasen werden bei aktiver Firewall nicht durchgeführt</title>
     <para>
      Die DeepSea-Phasen zur Bereitstellung werden nicht ausgeführt, wenn die Firewall aktiv ist (und sogar konfiguriert). Um die Phasen korrekt abzuschließen, müssen Sie entweder die Firewall durch Ausführen von
     </para>
<screen>
    <prompt>root # </prompt>systemctl stop firewalld.service
</screen>
     <para>
      ausschalten oder die Option <option>FAIL_ON_WARNING</option> in <filename>/srv/pillar/ceph/stack/global.yml</filename> auf „False“ festlegen:
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     Installieren Sie das Paket <literal>salt-minion</literal> in allen Minion Nodes.
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Vergewissern Sie sich, dass der <emphasis>vollqualifizierte Domänenname</emphasis> eines Nodes von allen anderen Nodes zur IP-Adresse des öffentlichen Netzwerks aufgelöst werden kann.
    </para>
   </step>
   <step>
    <para>
     Konfigurieren Sie alle Minions (einschließlich des Master Minion) zur Herstellung einer Verbindung zum Master. Wenn Ihr Salt Master mit dem Hostnamen <literal>salt</literal> nicht erreichbar ist, bearbeiten Sie die Datei <filename>/etc/salt/minion</filename> oder erstellen Sie eine neue Datei <filename>/etc/salt/minion.d/master.conf</filename> mit folgendem Inhalt:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     Wenn Sie an den oben genannten Konfigurationsdateien Änderungen vorgenommen haben, starten Sie den Salt Service auf allen Salt Minions neu:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Überprüfen Sie, ob der <systemitem>salt-minion</systemitem> Service in allen Nodes aktiviert und gestartet wurde. Aktivieren und starten Sie ihn, falls erforderlich:
    </para>
<screen><prompt>root # </prompt>systemctl enable salt-minion.service
<prompt>root # </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verifizieren Sie den Fingerabdruck der einzelnen Salt Minions und akzeptieren Sie alle Salt-Schlüssel am Salt Master, wenn die Fingerabdrücke übereinstimmen.
    </para>
    <note>
     <para>
      Wenn ein leerer Fingerabdruck des Salt Minions zurückgegeben wird, prüfen Sie, ob der Salt Minion eine Salt Master-Konfiguration umfasst und ob der Minion mit dem Salt Master kommunizieren kann.
     </para>
    </note>
    <para>
     Zeigen Sie den Fingerabdruck der einzelnen Minions an:
    </para>
<screen><prompt>root@master # </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Nachdem Sie die Fingerabdrücke aller Salt Minions gesammelt haben, listen Sie die Fingerabdrücke aller nicht akzeptierten Minion-Schlüssel am Salt Master auf:
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Wenn die Fingerabdrücke der Minions übereinstimmen, akzeptieren Sie sie:
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass die Schlüssel akzeptiert wurden:
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     Löschen Sie alle Datenträger vor der Bereitstellung von SUSE Enterprise Storage 6 manuell. Denken Sie daran, „X“ durch den korrekten Festplattenbuchstaben zu ersetzen:
    </para>
    <substeps>
     <step>
      <para>
       Stoppen Sie alle Prozesse, die die spezifische Festplatte verwenden.
      </para>
     </step>
     <step>
      <para>
       Verifizieren Sie, ob eine Partition auf der Festplatte eingehängt ist, und hängen Sie sie gegebenenfalls aus.
      </para>
     </step>
     <step>
      <para>
       Wenn die Festplatte von LVM verwaltet wird, deaktivieren und löschen Sie die gesamte LVM-Infrastruktur. Weitere Informationen finden Sie in <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/cha_lvm.html"/>.
      </para>
     </step>
     <step>
      <para>
       Wenn die Festplatte Teil von MD RAID ist, deaktivieren Sie RAID. Weitere Informationen finden Sie in <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/part_software_raid.html"/>.
      </para>
     </step>
     <step>
      <tip>
       <title>Server neustarten</title>
       <para>
        Wenn Sie in den folgenden Schritten Fehlermeldungen erhalten wie „Partition wird verwendet“ oder „Kernel kann nicht mit der neuen Partitionstabelle aktualisiert werden“, dann starten Sie den Server neu.
       </para>
      </tip>
      <para>
       Löschen Sie den Anfang der einzelnen Partitionen (als <systemitem class="username">root</systemitem>):
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       Löschen Sie den Anfang des Laufwerks:
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
</screen>
     </step>
     <step>
      <para>
       Löschen Sie das Ende des Laufwerks:
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
</screen>
     </step>
     <step>
      <para>
       Prüfen Sie mit folgendem Kommando, ob das Laufwerk leer ist (keine GPT-Strukturen enthält):
      </para>
<screen>
<prompt>root # </prompt>parted -s /dev/sdX print free
</screen>
      <para>
       oder
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Optional: Wenn Sie die Einstellungen des Clusters vorkonfigurieren müssen, bevor das Paket <package>deepsea</package> installiert wurde, erstellen Sie <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> manuell und legen Sie die Optionen <option>cluster_network:</option> und <option>public_network:</option> fest. Die Datei wird nach der Installation von <package>deepsea</package>nicht überschrieben.
    </para>
    <tip>
     <title>Aktivieren von IPv6</title>
     <para>
      Wenn Sie die IPv6-Netzwerkadressierung aktivieren müssen, beachten Sie <xref linkend="ds-modify-ipv6"/>
     </para>
    </tip>
   </step>
   <step>
    <para>
     Installieren Sie DeepSea im Salt Master Node:
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Der Wert für den Parameter <option>master_minion</option>wird aus der Datei <filename>/etc/salt/minion_id</filename> auf dem Salt Master abgeleitet. Wenn Sie den ermittelten Wert überschreiben müssen, bearbeiten Sie die Datei <filename>/srv/pillar/ceph/stack/global.yml</filename> und legen Sie einen relevanten Wert fest:
    </para>
<screen>
master_minion: <replaceable>MASTER_MINION_NAME</replaceable>
</screen>
    <para>
     Wenn der Salt Master über mehrere Hostnamen erreichbar ist, verwenden Sie den Salt Minion-Namen für den Speicher-Cluster, der vom Kommando <command>salt-key -L</command> zurückgegeben wurde. Wenn Sie den Standard-Hostnamen für Ihren Salt Master (<emphasis>salt</emphasis>) in der <emphasis>ses</emphasis>-Domäne verwendet haben, dann sieht die Datei folgendermaßen aus:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   Nun stellen Sie Ceph bereit und konfigurieren es. Alle Schritte sind obligatorisch, falls nicht anders angegeben.
  </para>

  <note>
   <title>Salt-Kommandokonventionen</title>
   <para>
    Sie haben zwei Möglichkeiten zum Ausführen von <command>salt-run state.orch</command>: zum einen mit „stage.<replaceable>STAGE_NUMBER</replaceable>“, zum anderen mit dem Namen der Phase. Beide Schreibweisen haben dieselbe Wirkung. Es liegt ganz bei Ihnen, welches Kommando Sie verwenden.
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>Ausführen von Bereitstellungsphasen</title>
   <step>
    <para>
     Prüfen Sie, ob die Salt Minions im Ceph Cluster ordnungsgemäß über die Option <option>deepsea_minions</option> in <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> adressiert sind. Weitere Informationen finden Sie unter <xref linkend="ds-minion-targeting-dsminions"/>.
    </para>
   </step>
   <step>
    <para>
     Standardmäßig stellt DeepSea die Ceph Cluster mit aktiven abgestimmten Profilen auf Ceph Monitor, Ceph Manager und Ceph OSD Nodes bereit. In einigen Fällen muss die Bereitstellung ohne abgestimmte Profile durchgeführt werden. Tragen Sie hierzu die folgenden Zeilen in <filename>/srv/pillar/ceph/stack/global.yml</filename> ein, bevor Sie die DeepSea-Phasen ausführen:
    </para>
<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>
   </step>
   <step>
    <para>
     <emphasis>Optional</emphasis>: Erstellen Sie Btrfs Sub-Volumes für <filename>/var/lib/ceph/</filename>. Dieser Schritt muss vor der DeepSea-Phase 0 ausgeführt werden. Weitere Informationen zum Migrieren vorhandener Verzeichnisse sowie allgemeine weitere Informationen finden Sie in <xref linkend="storage-tips-ceph-btrfs-subvol"/>.
    </para>
    <para>
     Führen Sie folgende Kommandos für jeden Salt Minion aus:
    </para>
<screen><prompt>root@master # </prompt>salt 'MONITOR_NODES' saltutil.sync_all
<prompt>root@master # </prompt>salt 'MONITOR_NODES' state.apply ceph.subvolume
</screen>
    <note>
     <para>
      Mit dem Kommando „Ceph.subvolume“ wird <filename>/var/lib/ceph</filename> als ein <filename>Btrfs-Subvolume für @/var/lib/ceph</filename> erstellt.
     </para>
    </note>
    <para>
     Das neue Subvolume wird nun eingehängt und <literal>/etc/fstab</literal> wird aktualisiert.
    </para>
   </step>
   <step>
    <para>
     Bereiten Sie Ihren Cluster vor. Weitere Informationen finden Sie in <xref linkend="deepsea-stage-description"/>.
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>Phasen mit DeepSea CLI ausführen oder überwachen</title>
     <para>
      Mit DeepSea CLI können Sie den Fortschritt der Phasenausführung in Echtzeit verfolgen. Führen Sie dazu DeepSea CLI im Überwachungsmodus aus oder führen Sie die Phase direkt über DeepSea CLI aus. Weitere Informationen finden Sie im <xref linkend="deepsea-cli"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     In der Ermittlungsphase werden Daten von allen Minions erfasst und Konfigurationsfragmente erstellt, die im Verzeichnis <filename>/srv/pillar/ceph/proposals</filename> gespeichert sind. Die Daten werden im YAML-Format in SLS- oder YML-Dateien gespeichert.
    </para>
    <para>
     Mit folgendem Kommando lösen Sie die Ermittlungsphase aus:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     Erstellen Sie nach erfolgreicher Ausführung des vorigen Kommandos eine <filename>policy.cfg</filename>-Datei in <filename>/srv/pillar/ceph/proposals</filename>. Weitere Informationen finden Sie im <xref linkend="policy-configuration"/>.
    </para>
    <tip>
     <para>
      Wenn Sie die Netzwerkeinstellungen des Clusters ändern müssen, bearbeiten Sie die Datei <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> und passen Sie die Zeilen an, die mit <literal>cluster_network:</literal> und <literal>public_network:</literal> beginnen.
     </para>
    </tip>
   </step>
   <step>
    <para>
     In der Konfigurationsphase wird die <filename>policy.cfg</filename>-Datei analysiert und die enthaltenen Dateien in der finalen Form zusammengeführt. Auf Cluster und Rolle bezogene Inhalte werden in  <filename>/srv/pillar/ceph/cluster</filename> hinzugefügt, Ceph-spezifische Inhalte dagegen in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Führen Sie folgendes Kommando aus, um die Konfigurationsphase auszulösen:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     Der Konfigurationsschritt kann einige Sekunden dauern. Nach Ausführung des Kommandos sehen Sie die Pillar-Daten für die angegebenen Minions (beispielsweise mit Namen <literal>ceph_minion1</literal>, <literal>ceph_minion2</literal> usw.), indem Sie folgendes Kommando ausführen:
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <tip>
     <title>Bearbeiten des OSD-Layouts</title>
     <para>
      Soll das standardmäßige OSD-Layout bearbeitet und die DriveGroups-Konfiguration geöffnet werden, befolgen Sie die Anweisungen in <xref linkend="ds-drive-groups"/>.
     </para>
    </tip>
    <note>
     <title>Überschreiben von Standardwerten</title>
     <para>
      Sobald das Kommando ausgeführt ist, sehen Sie die Standardkonfiguration und können sie entsprechend Ihrer Anforderungen ändern. Weitere Informationen finden Sie im <xref linkend="ceph-deploy-ds-custom"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Nun führen Sie die Bereitstellungsphase aus. In dieser Phase wird der Pillar validiert und die Ceph Monitor und Ceph OSD Daemons werden gestartet:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy</screen>
    <para>
     Die Ausführung des Kommandos kann einige Minuten dauern. Wenn es nicht ausgeführt wird, müssen Sie das Problem beheben und die vorigen Phasen erneut ausführen. Führen Sie nach erfolgreicher Ausführung des Kommandos das folgende Kommando aus, um den Status zu prüfen:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     Die letzte Phase der Ceph Cluster-Bereitstellung ist die <emphasis>Services</emphasis>-Phase. Hier instanziieren Sie die gewünschten Services, die aktuell unterstützt werden: iSCSI Gateway, CephFS, Object Gateway,  und NFS Ganesha. In dieser Phase werden die erforderlichen Pools, die autorisierenden Schlüsselbunde und Start-Services erstellt. Führen Sie folgendes Kommando aus, um die Phase zu starten:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     oder
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     Je nach Einrichtung kann die Ausführung des Kommandos einige Minuten dauern.
    </para>
   </step>
   <step>
    <para>
     Bevor Sie den Vorgang fortsetzen, wird dringend empfohlen, das Ceph-Telemetriemodul zu aktivieren. Weitere Informationen und Anweisungen finden Sie in <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSea stellt auch eine Kommandozeilenschnittstelle (Command Line Interface, CLI) zur Verfügung, mit dem Benutzer Phasen überwachen oder ausführen, während sie den Ausführungsfortschritt in Echtzeit visualisieren. Prüfen Sie, ob das Paket <package>deepsea-cli</package> installiert ist, bevor Sie die ausführbare Datei <command>deepsea</command> ausführen.
  </para>

  <para>
   Zwei Modi werden zur Visualisierung des Ausführungsfortschritts einer Phase unterstützt:
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>DeepSea CLI-Modi</title>
   <listitem>
    <para>
     <emphasis role="bold">Überwachungsmodus</emphasis>: visualisiert den Ausführungsfortschritt einer DeepSea-Phase, die vom <command>salt-run</command>-Kommando ausgelöst wurde, das wiederum in einer anderen Terminalsitzung ausgestellt wurde.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stand-Alone-Modus</emphasis>: führt eine DeepSea-Phase aus bei gleichzeitiger Echtzeit-Visualisierung der Komponentenschritte während ihrer Ausführung.
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>DeepSea CLI-Kommandos</title>
   <para>
    Die DeepSea CLI-Kommandos können nur auf dem Salt Master Node ausgeführt werden und es sind <systemitem class="username">root</systemitem>-Berechtigungen dazu erforderlich.
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI: Überwachungsmodus</title>
   <para>
    Die Fortschrittsüberwachung bietet eine detaillierte Echtzeit-Visualisierung der Vorgänge bei der Ausführung von Phasen mit <command>salt-run state.orch</command>-Kommandos in anderen Terminalsitzungen.
   </para>
   <tip>
    <title>Starten der Überwachung in einer neuen Terminalsitzung</title>
    <para>
     Sie müssen die Überwachung in einem neuen Terminalfenster starten, <emphasis>bevor</emphasis> Sie ein <command>salt-run state.orch</command>-Kommando ausführen, damit die Überwachung den Start der Phasenausführung erkennt.
    </para>
   </tip>
   <para>
    Wenn Sie die Überwachung nach Ausgabe des Kommandos <command>salt-run state.orch</command> starten, wird kein Ausführungsfortschritt angezeigt.
   </para>
   <para>
    Sie können den Überwachungsmodus durch Ausführen des folgenden Kommandos starten:
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    Weitere Informationen zu den verfügbaren Kommandozeilenoptionen des <command>deepsea monitor</command>-Kommandos finden Sie auf der entsprechenden Handbuchseite:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI: Stand-Alone-Modus</title>
   <para>
    Im Stand-Alone-Modus wird über DeepSea CLI eine DeepSea-Phase ausgeführt und die Ausführung wird in Echtzeit angezeigt.
   </para>
   <para>
    Das Kommando zur Ausführung einer DeepSea-Phase an der DeepSea CLI weist folgende Form auf:
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    <replaceable>stage-name</replaceable> stellt dabei die Methode dar, wie die Zustandsdateien der Salt-Orchestrierung referenziert werden. Beispiel: Phase <emphasis role="bold">Ermittlung</emphasis>, die dem Verzeichnis in Pfad <filename>/srv/salt/ceph/stage/deploy</filename> entspricht, wird als <emphasis role="bold">ceph.stage.deploy</emphasis> referenziert.
   </para>
   <para>
    Dieses Kommando stellt eine Alternative zu den Salt-basierten Kommandos zur Ausführung von DeepSea-Phasen (oder von Zustandsdateien der DeepSea-Orchestrierung) dar.
   </para>
   <para>
    Das Kommando <command>deepsea stage run ceph.stage.0</command> entspricht <command>salt-run state.orch ceph.stage.0</command>.
   </para>
   <para>
    Weitere Informationen zu den verfügbaren Kommandozeilenoptionen, die vom <command>deepsea stage run</command>-Kommando akzeptiert werden, finden Sie auf der entsprechenden Handbuchseite:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    Die folgende Abbildung zeigt ein Beispiel der Ausgabe der DeepSea CLI bei Ausführung von <emphasis role="underline">Phase 2</emphasis>:
   </para>
   <figure>
    <title>Ausgabe des Fortschritts der Phasenausführung an der DeepSea CLI</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>DeepSea CLI-Alias für <command>stage run</command></title>
    <para>
     Für fortgeschrittene Benutzer von Salt unterstützen wir einen Alias für die Ausführung einer DeepSea-Phase. Dieser betrachtet das Salt-Kommando, das zur Ausführung einer Phase verwendet wird (beispielsweise <command>salt-run state.orch <replaceable>stage-name</replaceable></command>) als Kommando der DeepSea CLI.
    </para>
    <para>
     Beispiel:
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>Konfiguration und Anpassung</title>

  <sect2 xml:id="policy-configuration">
   <title>Die Datei <filename>policy.cfg</filename></title>
   <para>
    Mit der Konfigurationsdatei <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> werden Rollen einzelner Cluster Nodes ermittelt. Beispiel: Welche Knoten fungieren als Ceph OSDs oder Ceph Monitors? Bearbeiten Sie <filename>policy.cfg</filename>, um die gewünschte Cluster-Einrichtung widerzuspiegeln. Die Reihenfolge der Abschnitte ist beliebig, doch der Inhalt der enthaltenen Zeilen überschreibt die passenden Schlüssel vom Inhalt der vorigen Zeilen.
   </para>
   <tip>
    <title>Beispiele für <filename>policy.cfg</filename></title>
    <para>
     Sie finden verschiedene Beispiele von fertiggestellten Richtliniendateien im Verzeichnis <filename>/usr/share/doc/packages/deepsea/examples/</filename>.
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>Cluster-Zuweisung</title>
    <para>
     Im Abschnitt <emphasis role="bold">cluster</emphasis> wählen Sie Minions für Ihren Cluster aus. Sie können alle Minions auswählen oder Minions auf eine Blacklist oder Whitelist setzen. Beispiele für einen Cluster namens <emphasis role="bold">ceph</emphasis> folgen.
    </para>
    <para>
     Fügen Sie die folgenden Zeilen hinzu, um <emphasis role="bold">alle</emphasis> Minions einzubeziehen:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     So setzen Sie einen bestimmten Minion auf eine <emphasis role="bold">weiße Liste</emphasis>:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     oder eine Gruppe von Minions, mit Shell Glob-Abgleich:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     Um Minions auf eine <emphasis role="bold">schwarze Liste</emphasis> zu setzen, legen Sie sie als <literal>unassigned</literal> (nicht zugewiesen) fest:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>Rollenzuweisung</title>
    <para>
     In diesem Abschnitt erhalten Sie detaillierte Informationen zur Zuweisung von „Rollen“ zu Ihren Cluster Nodes. Eine „Rolle“ bezeichnet in diesem Kontext einen Service, den Sie zur Ausführung im Node benötigen, z. B. Ceph Monitor, Object Gateway oder iSCSI-Gateway. Keine Rolle wird automatisch zugewiesen. Es werden nur Rollen bereitgestellt, die zu <command>policy.cfg</command> hinzugefügt wurden.
    </para>
    <para>
     Die Zuweisung erfolgt nach dem folgenden Muster:
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     Die einzelnen Elemente haben die folgende Bedeutung und Werte:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable> bezeichnet eines der folgenden Elemente: „master“, „admin“, „mon“, „mgr“, „storage“, „mds“, „igw“, „rgw“, „ganesha“, „grafana“ oder „prometheus“.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable> ist ein relativer Verzeichnispfad zu SLS- oder YML-Dateien. Bei SLS-Dateien lautet er normalerweise <filename>cluster</filename>, YML-Dateien befinden sich unter <filename>stack/default/ceph/minions</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable> bezeichnet die Salt-Zustandsdateien oder die YAML-Konfigurationsdateien. Sie bestehen normalerweise aus den Hostnamen der Salt Minions, beispielsweise <filename>ses5min2.yml</filename>. Ein weiterer spezifischen Abgleich kann über Shell Globbing erfolgen.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Nachfolgend sehen Sie ein Beispiel für jede Rolle:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - der Node hat Admin-Schlüsselbunde zu allen Ceph Clustern. Aktuell wird nur ein einzelner Ceph Cluster unterstützt. Da die <emphasis>master</emphasis>-Rolle obligatorisch ist, fügen Sie immer eine ähnliche Zeile wie die folgende hinzu:
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - der Minion verfügt über einen Admin-Schlüsselbund. Sie definieren die Rolle wie folgt:
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> – der Minion stellt den Überwachungs-Service für den Ceph Cluster zur Verfügung. Diese Rolle benötigt Adressen der zugewiesenen Minions. Ab SUSE Enterprise Storage 5 werden die öffentlichen Adressen dynamisch berechnet und sind im Salt Pillar nicht mehr erforderlich.
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       Im Beispiel wird die Überwachungsrolle einer Gruppe von Minions zugewiesen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mgr</emphasis> - der Ceph Manager Daemon, der alle Zustandsinformationen des gesamten Clusters erfasst. Stellen Sie ihn auf allen Minions bereit, auf denen Sie die Ceph Monitor-Rolle bereitstellen möchten.
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>storage</emphasis> – mit dieser Rolle geben Sie bestimmte Speicher-Nodes an.
      </para>
<screen>role-storage/cluster/data*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - der Minion stellt den Metadaten-Service zur Unterstützung von CephFS bereit.
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - der Minion fungiert als iSCSI Gateway. Diese Rolle benötigt Adressen der zugewiesenen Minions. Daher müssen Sie auch die Dateien aus dem <filename>stack</filename>-Verzeichnis hinzufügen:
      </para>
<screen>role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - der Minion fungiert als Object Gateway.
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>ganesha</emphasis> - der Minion fungiert als NFS Ganesha Server. Für die „ganesha“-Rolle ist eine „rgw“- oder „mds“-Rolle im Cluster erforderlich. Andernfalls wird die Validierung in Phase 3 nicht durchgeführt.
      </para>
<screen>role-ganesha/cluster/ganesha*.sls</screen>
      <para>
       Für die erfolgreiche Installation von NFS Ganesha ist eine weitere Konfiguration erforderlich. Wenn Sie NFS Ganesha verwenden möchten, lesen Sie <xref linkend="cha-as-ganesha"/>, bevor Sie Phase 2 und 4 ausführen. Es ist jedoch möglich, NFS Ganesha später zu installieren.
      </para>
      <para>
       In einigen Fällen kann es nützlich sein, benutzerdefinierte Rollen für NFS Ganesha Nodes zu definieren. Weitere Informationen finden Sie in <xref linkend="ceph-nfsganesha-customrole"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>grafana</emphasis>, <emphasis>prometheus</emphasis> – dieser Knoten fügt Grafana-Diagramme auf der Grundlage der Prometheus-Warnmeldungen in das Ceph Dashboard ein. Eine ausführliche Beschreibung finden Sie in <xref linkend="ceph-dashboard"/>.
      </para>
<screen>role-grafana/cluster/grafana*.sls</screen>
<screen>role-prometheus/cluster/prometheus*.sls</screen>
     </listitem>
    </itemizedlist>
    <note>
     <title>Mehrere Rollen von Cluster Nodes</title>
     <para>
      Sie können einem einzelnen Node verschiedene Rollen zuweisen. Beispielsweise können Sie die „mds“-Rollen zu Monitor Nodes zuweisen:
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>Allgemeine Konfiguration</title>
    <para>
     Im Abschnitt zur allgemeinen Konfiguration sind Konfigurationsdateien enthalten, die in der Phase der <emphasis>Ermittlung (Phase 1)</emphasis> generiert wurden. In diesen Konfigurationsdateien werden Parameter wie <literal>fsid</literal> oder <literal>public_network</literal> gespeichert. Fügen Sie die folgenden Zeilen hinzu, um die erforderliche allgemeine Ceph-Konfiguration einzubeziehen:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>Filtern von Elementen</title>
    <para>
     Manchmal ist es nicht praktisch, alle Dateien eines vorliegenden Verzeichnisses zum *.sls Globbing hinzuzufügen. Das Dateianalyseprogramm <filename>policy.cfg</filename> versteht die folgenden Filter:
    </para>
    <warning>
     <title>Erweiterte Methoden</title>
     <para>
      In diesem Abschnitt werden Filtermethoden für fortgeschrittene Benutzer beschrieben. Wenn die Filter nicht korrekt verwendet werden, können sie Probleme verursachen, beispielsweise wenn sich die Node-Nummerierung ändert.
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        Mit dem Slice-Filter beziehen Sie nur die Elemente <emphasis>start</emphasis> bis <emphasis>end-1</emphasis> ein. Beachten Sie, dass die im vorliegenden Verzeichnis vorhandenen Elemente alphanumerisch sortiert sind. Die folgende Zeile enthält die dritte bis fünfzigste Datei des Unterverzeichnisses <filename>role-mon/cluster/</filename>:
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        Mit dem regex Filter werden nur Elemente einbezogen, die den vorliegenden regulären Ausdrücken entsprechen. Beispiel:
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title>Beispiel einer <filename>policy.cfg</filename>-Datei</title>
    <para>
     Nachfolgend sehen Sie ein Beispiel einer einfachen <filename>policy.cfg</filename>-Datei:
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# STORAGE
role-storage/cluster/ses-example-[5,6,7,8].sls <co xml:id="co-policy-storage"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>
</screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       Gibt an, dass alle Minions im Ceph Cluster enthalten sind. Wenn Sie über Minions verfügen, die nicht im Ceph Cluster enthalten sein sollen, verwenden Sie:
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       Die erste Zeile kennzeichnet alle Minions als nicht zugewiesen. Die zweite Zeile überschreibt Minions, die "ses-example-*.sls" entsprechen, und weist sie dem Ceph Cluster zu.
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       Der Minion namens „examplesesadmin“ verfügt über die „master“-Rolle. Dies bedeutet übrigens, dass er Admin-Schlüssel für den Cluster erhält.
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       Alle Minions, die "sesclient*" entsprechen, erhalten ebenfalls Admin-Schlüssel.
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       Alle Minions, die "ses-example-[123]" entsprechen (vermutlich drei Minions: ses-example-1, ses-example-2 und ses-example-3), werden als MON Nodes eingerichtet.
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       Alle Minions, die "ses-example-[123]" entsprechen (alle MON Nodes in diesem Beispiel), werden als MGR-Nodes eingerichtet.
      </para>
     </callout>
     <callout arearefs="co-policy-storage">
      <para>
       Alle Minions, die „ses-example-[5,6,7,8]“ entsprechen, werden als Speicher-Nodes eingerichtet.
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       Minion "ses-example-4" erhält die MDS-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       Minion "ses-example-4" erhält die IGW-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       Minion "ses-example-4" erhält die RGW-Rolle.
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       Bedeutet, dass wir die Standardwerte für allgemeine Konfigurationsparameter wie <option>fsid</option> und <option>public_network</option> akzeptieren.
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       Bedeutet, dass wir die Standardwerte für allgemeine Konfigurationsparameter wie <option>fsid</option> und <option>public_network</option> akzeptieren.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="ds-drive-groups">
   <title>DriveGroups</title>
   <para>
    <emphasis>DriveGroups</emphasis> bestimmen das Layout der OSDs im Ceph Cluster. Sie sind in einer einzelnen Datei definiert (<filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>).
   </para>
   <para>
    Ein Administrator muss manuell eine Gruppe von OSDs erstellen, die zusammenhängen (Hybrid-OSDs, die auf Solid-State-Laufwerken und rotierenden Datenträgern implementiert sind) oder dieselben Implementierungsoptionen aufweisen (identisch, z. B. gleicher Objektspeicher, gleiche Verschlüsselungsoption, Stand-Alone-OSDs). Damit die Geräte nicht explizit aufgelistet werden müssen, werden sie in den DriveGroups anhand einer Liste von Filterelementen gefiltert, die einigen wenigen ausgewählten Feldern in den <command>ceph-volume</command>-Bestandsberichten entsprechen. Im einfachsten Fall ist dies die „rotierende“ Flagge (alle Solid-State-Laufwerke müssen db_devices sein, alle rotierenden Laufwerke dagegen Datengeräte), alternativ und etwas aufwendiger sind z. B. „Modell“-Zeichenketten oder Größen. Der Code in DeepSea wandelt diese DriveGroups in Gerätelisten um, die dann vom Benutzer geprüft werden können.
   </para>
   <para>
    Dieses einfache Verfahren veranschaulicht den grundlegenden Workflow beim Konfigurieren der DriveGroups:
   </para>
   <procedure>
    <step>
     <para>
      Prüfen Sie die Eigenschaften Ihrer Datenträger, die vom Kommando <command>ceph-volume</command> zurückgegeben wurden. DriveGroups akzeptieren ausschließlich folgende Eigenschaften:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.details
</screen>
    </step>
    <step>
     <para>
      Öffnen Sie die YAML-Datei <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> und passen Sie sie an Ihre Anforderungen an. Weitere Informationen finden Sie unter <xref linkend="ds-drive-groups-specs"/>. Denken Sie daran, Leerzeichen anstelle von Tabulatoren anzugeben. Ausführliche Beispiele finden Sie in <xref linkend="ds-drive-groups-examples"/> Das folgende Beispiel umfasst alle Datenträger, die in Ceph als OSDs verfügbar sind:
     </para>
<screen>
default_drive_group_name:
  target: '*'
  data_devices:
    all: true
</screen>
    </step>
    <step>
     <para>
      Prüfen Sie die neuen Layouts:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
     <para>
      Dieses Ausführungsprogramm gibt eine Struktur der passenden Datenträger auf der Grundlage Ihrer DriveGroups zurück. Wenn Sie mit dem Ergebnis nicht zufrieden sind, wiederholen Sie den vorherigen Schritt.
     </para>
     <tip>
      <title>Ausführlicher Bericht</title>
      <para>
       Neben dem Ausführungsprogramm <command>disks.list</command> steht das Ausführungsprogramm <command>disks.report</command> bereit, das einen detaillierten Bericht über die Abläufe beim nächsten Aufrufen der DeepSea-Phase 3 zurückgibt.
      </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
     </tip>
    </step>
    <step>
     <para>
      Imitieren Sie die OSDs. Beim nächsten Aufrufen der DeepSea-Phase 3 werden die OSD-Datenträger gemäß Ihrer DriveGroups-Spezifikation implementiert.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ds-drive-groups-specs">
    <title>Spezifikation</title>
    <para>
     <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> akzeptiert folgende Optionen:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  db_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  wal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    <para>
     Für FileStore-Einrichtungen kann <filename>drive_groups.yml</filename> wie folgt aussehen:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  journal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  format: filestore
  encryption: True
</screen>
   </sect3>
   <sect3>
    <title>Abstimmung von Datenträgergeräten</title>
    <para>
     Sie können die Spezifikation mit folgenden Filtern beschreiben:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nach Datenträgermodell:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Nach Datenträgerhersteller:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>Hersteller-Zeichenkette</title>
       <para>
        Geben Sie <replaceable>DISK_VENDOR_STRING</replaceable> stets in Kleinbuchstaben an.
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       Angabe, ob ein rotierender oder ein nicht rotierender Datenträger vorliegt. SSDs und NVME-Laufwerke sind keine rotierenden Datenträger.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Implementieren Sie einen Knoten mit <emphasis>allen</emphasis> verfügbaren Laufwerken für OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Zusätzlich mit Einschränkung der Anzahl passender Datenträger
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Filtern von Geräten nach Größe</title>
    <para>
     Sie können die Datenträgergeräte nach ihrer Größe filtern – wahlweise nach einer genauen Größenangabe oder einem Größenbereich. Der Parameter <option>size:</option> akzeptiert Argumente wie folgt:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       „10G“ – Datenträger mit einer bestimmten Größe.
      </para>
     </listitem>
     <listitem>
      <para>
       „10G:40G“ – Datenträger mit einer Größe im angegebenen Bereich.
      </para>
     </listitem>
     <listitem>
      <para>
       „:10G“ – Datenträger mit einer Größe von maximal 10 GB.
      </para>
     </listitem>
     <listitem>
      <para>
       „40G:“ – Datenträger mit einer Größe von mindestens 40 GB.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Abstimmung nach Datenträgergröße</title>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <note>
     <title>Anführungszeichen erforderlich</title>
     <para>
      Beim Begrenzer „:“ müssen Sie die Größe in Anführungszeichen setzen, da das Zeichen „:“ ansonsten als neuer Konfiguration-Hash interpretiert wird.
     </para>
    </note>
    <tip>
     <title>Abkürzungen für Einheiten</title>
     <para>
      Anstelle von (G)igabyte können Sie die Größen auch in (M)egabyte oder (T)erabyte angeben.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Beispiele</title>
    <para>
     Dieser Abschnitt enthält Beispiele verschiedener OSD-Einrichtungen.
    </para>
    <example>
     <title>Einfache Einrichtung</title>
     <para>
      Dieses Beispiel zeigt zwei Knoten mit derselben Einrichtung:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Die entsprechende Datei <filename>drive_groups.yml</filename> sieht wie folgt aus:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ
   </screen>
     <para>
      Eine solche Konfiguration ist unkompliziert und zulässig. Es stellt sich allerdings das Problem, dass ein Administrator in Zukunft eventuell Datenträger von anderen Herstellern einfügt, die dann nicht berücksichtigt werden. Zur Verbesserung geben Sie weniger Filter für die wesentlichen Eigenschaften der Laufwerke an:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
   </screen>
     <para>
      Im vorherigen Beispiel wird die Deklaration aller rotierenden Geräte als „Datengeräte“ erzwungen und alle nicht rotierenden Geräte werden als „freigegebene Geräte“ (wal, db) genutzt.
     </para>
     <para>
      Wenn Laufwerke mit mehr als 2 TB stets als langsamere Datengeräte eingesetzt werden sollen, können Sie nach Größe filtern:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <example>
     <title>Erweiterte Konfiguration</title>
     <para>
      Dieses Beispiel umfasst zwei getrennte Einrichtungen: 20 HDDs sollen gemeinsam 2 SSDs nutzen und 10 SSDs nutzen gemeinsam 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Eine solche Einrichtung kann wie folgt mit zwei Layouts definiert werden:
     </para>
<screen>
drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
</screen>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB
</screen>
    </example>
    <example>
     <title>Erweiterte Einrichtung mit nicht einheitlichen Knoten</title>
     <para>
      In den vorherigen Beispielen wurde angenommen, dass alle Knoten dieselben Laufwerke umfassen. Dies ist jedoch nicht immer der Fall:
     </para>
     <para>
      Knoten 1–5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Knoten 6–10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Mit dem „target“-Schlüssel im Layout können Sie bestimmte Knoten adressieren. Die Salt-Zielnotation trägt zur Vereinfachung bei:
     </para>
<screen>
drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
</screen>
     <para>
      gefolgt von:
     </para>
<screen>
drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Experteneinrichtung</title>
     <para>
      In allen vorherigen Fällen wird angenommen, dass die WALs und DBs dasselbe Gerät nutzen. Es ist jedoch auch möglich, WAL auf einem dedizierten Gerät zu implementieren:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Komplexe (und unwahrscheinliche) Einrichtung</title>
     <para>
      In der folgenden Einrichtung soll Folgendes definiert werden:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs mit 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs mit 1 SSD(db) und 1 NVMe(wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs mit 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 Stand-Alone-SSDs (verschlüsselt)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD fungiert als Ersatz und soll nicht bereitgestellt werden
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Zusammenfassung der verwendeten Datenträger:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Die DriveGroups-Definition lautet:
     </para>
<screen>
drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True
 </screen>
     <para>
      Eine HDD verbleibt, da die Datei von oben nach unten analysiert wird.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2>
   <title>Anpassen von <filename>ceph.conf</filename> mit benutzerdefinierten Einstellungen</title>
   <para>
    Wenn Sie benutzerdefinierte Einstellungen zur Datei <filename>ceph.conf</filename> hinzufügen müssen, finden Sie die entsprechenden Informationen dazu im <xref linkend="ds-custom-cephconf"/>.
   </para>
  </sect2>
 </sect1>
</chapter>
