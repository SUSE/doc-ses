<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Installation von NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>Bearbeiten</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>Ja</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha ermöglicht den NFS-Zugriff auf das Object Gateway oder das CephFS. In SUSE Enterprise Storage 6 werden die NFS-Versionen 3 und 4 unterstützt. NFS Ganesha wird im Benutzerbereich statt im Kernel-Bereich ausgeführt und interagiert direkt mit dem Object Gateway oder dem CephFS.
 </para>
 <warning>
  <title>Protokollübergreifender Zugriff</title>
  <para>
   Native CephFS- und NFS-Clients sind nicht durch Dateisperren in Samba eingeschränkt und umgekehrt. In Anwendungen, die auf der protokollübergreifenden Dateisperre beruhen, können dort Beschädigungen auftreten, wenn der Zugriff auf CephFS-gestützte Samba-Freigabepfade auf andere Weise erfolgt.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Vorbereitung</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Allgemeine Informationen</title>
   <para>
    Zur erfolgreichen Bereitstellung von NFS Ganesha müssen Sie eine <literal>role-ganesha</literal> zu <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> hinzufügen. Weitere Informationen finden Sie in <xref linkend="policy-configuration"/>. Für NFS Ganesha muss entweder eine <literal>role-rgw</literal> oder eine<literal>role-mds</literal> in der Datei <filename>policy.cfg</filename> vorhanden sein.
   </para>
   <para>
    Es ist zwar möglich, den NFS Ganesha-Server in einem bereits vorhandenen Ceph Node zu installieren und auszuführen, wir empfehlen jedoch, ihn auf einem dedizierten Host mit Zugriff auf den Ceph Cluster auszuführen. Die Client-Hosts sind normalerweise nicht Teil des Clusters, doch sie müssen Netzwerkzugriff auf den NFS Ganesha-Server haben.
   </para>
   <para>
    Fügen sie zur Aktivierung des NFS Ganesha-Servers zu einem beliebigen Zeitpunkt nach der ersten Installation die <literal>role-ganesha</literal> zu <filename>policy.cfg</filename> hinzu und führen Sie mindestens die DeepSea-Phasen 2 und 4 erneut aus. Weitere Informationen finden Sie in <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha wird über die Datei <filename>/etc/ganesha/ganesha.conf</filename> konfiguriert, die im NFS Ganesha Node vorhanden ist. Diese Datei wird jedoch bei jeder Ausführung der DeepSea-Phase 4 überschrieben. Wir empfehlen daher, die von Salt verwendeten Schablonen zu bearbeiten, bei der es sich um die Datei <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> am Salt Master handelt. Detaillierte Informationen zur Konfigurationsdatei finden Sie im <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Anforderungen im Überblick</title>
   <para>
    Die folgenden Anforderungen müssen erfüllt sein, bevor DeepSea-Phasen 2 und 4 zur Installation von NFS Ganesha ausgeführt werden können:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Die <literal>role-ganesha</literal> muss mindestens einem Node zugewiesen sein.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie können nur eine <literal>role-ganesha</literal> pro Minion definieren.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha benötigt entweder ein Object Gateway oder ein CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Auf Minions mit der Rolle <literal>role-ganesha</literal> muss das kernelgestützte NFS deaktiviert werden.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Installationsbeispiel</title>

  <para>
   In diesem Installationsbeispiel werden sowohl das Objekt Gateway als auch die CephFS File System Abstraction Layers (FSAL) von NFS Ganesha verwendet.
  </para>

  <procedure>
   <step>
    <para>
     Wenn Ihnen dies nicht geläufig ist, führen Sie zunächst die DeepSea-Phasen 0 und 1 aus, bevor Sie mit diesem Verfahren fortfahren.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Bearbeiten Sie nach Ausführung der Phase 1 von DeepSea die Datei<filename>/srv/pillar/ceph/proposals/policy.cfg</filename> und fügen Sie folgende Zeile hinzu:
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Ersetzen Sie <replaceable>NODENAME</replaceable> durch den Namen eines Nodes in Ihrem Cluster.
    </para>
    <para>
     Stellen Sie außerdem sicher, dass eine <literal>role-mds</literal> und eine <literal>role-rgw</literal> zugewiesen sind.
    </para>
   </step>
   <step>
    <para>
     Führen Sie mindestens die Phasen 2 und 4 von DeepSea aus. Wir empfehlen, dazwischen auch Phase 3 auszuführen.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Prüfen Sie, ob NFS Ganesha funktionsfähig ist. Stellen Sie hierzu fest, ob der NFS Ganesha-
Service auf dem Minion Node ausgeführt wird:
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Hochverfügbare Aktiv-Passiv-Konfiguration</title>

  <para>
   In diesem Abschnitt finden Sie ein Beispiel, wie eine Aktiv-Passiv-Konfiguration mit zwei Nodes des NFS Ganesha-Servers eingerichtet wird. Für die Einrichtung ist die SUSE Linux Enterprise High Availability Extension erforderlich. Die beiden Nodes werden <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem> genannt.
  </para>

  <important>
   <title>Services auf demselben Computer</title>
   <para>
    Services mit eigener Fehlertoleranz und eigenem Lastenausgleich dürfen nicht auf Cluster Nodes ausgeführt werden, die im Rahmen der Failover-Services isoliert werden. Führen Sie Ceph Monitor-, Metadatenserver-, iSCSI- oder Ceph OSD-Services daher nicht in Hochverfügbarkeits-Einrichtungen aus.
   </para>
  </important>

  <para>
   Detaillierte Informationen zu SUSE Linux Enterprise High Availability Extension finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Standardinstallation</title>
   <para>
    In dieser Einrichtung hat <systemitem class="domainname">earth</systemitem> die IP-Adresse <systemitem class="ipaddress">192.168.1.1</systemitem> und <systemitem class="domainname">mars</systemitem> die Adresse <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Außerdem können Clients über zwei virtuelle IP-Adressen nach dem Floating-IP-Prinzip eine Verbindung mit dem Service herstellen, und zwar unabhängig davon, auf welchem physischen Node er ausgeführt wird. <systemitem class="ipaddress">192.168.1.10</systemitem> wird für die Cluster-Verwaltung mit Hawk2 verwendet und <systemitem class="ipaddress">192.168.2.1</systemitem> exklusiv für die NFS-Exporte. Dies erleichtert später die Anwendung von Sicherheitsbeschränkungen.
   </para>
   <para>
    Das folgende Verfahren beschreibt das Installationsbeispiel. Weitere Informationen finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Bereiten Sie die NFS Ganesha Nodes am Salt Master vor:
     </para>
     <substeps>
      <step>
       <para>
        Führen Sie die DeepSea-Phasen 0 und 1 aus.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Weisen Sie den Nodes <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem> die <literal>role-ganesha</literal> in Datei<filename>/srv/pillar/ceph/proposals/policy.cfg</filename> zu:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Führen Sie die DeepSea-Phasen 2 bis 4 aus.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registrieren Sie die SUSE Linux Enterprise High Availability Extension in <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installieren Sie <package>ha-cluster-bootstrap</package> in beiden Nodes:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialisieren Sie den Cluster in <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Lassen Sie <systemitem class="domainname">mars</systemitem> dem Cluster beitreten:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Prüfen Sie den Status des Clusters. Sie sollten zwei Nodes sehen, die im Cluster hinzugefügt wurden:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Deaktivieren Sie in beiden Nodes den automatischen Start des NFS Ganesha Service beim Booten des Systems:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Starten Sie die <command>crm</command>-Shell im Node <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Die nächsten Kommandos werden in der crm-Shell ausgeführt.
     </para>
    </step>
    <step>
     <para>
      Führen Sie in <systemitem class="domainname">earth</systemitem> die crm-Shell aus, um die folgenden Kommandos zur Konfiguration der Ressource für NFS Ganesha Daemons als Klon des Ressourcentyps „systemd“ auszuführen:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine einfache IPAddr2 mit der crm-Shell:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Wir stellen eine Beziehung zwischen dem NFS Ganesha-Server und der Floating-IP-Adresse über Kollokation und Anordnung her.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Führen Sie das Kommando <command>mount</command> am Client aus, um sicherzustellen, dass die Cluster-Einrichtung vollständig ist:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Bereinigen der Ressourcen</title>
   <para>
    Falls ein NFS Ganesha, wie zum Beispiel <systemitem class="domainname">earth</systemitem>, in einem der Nodes ausfällt, beheben Sie das Problem und bereinigen Sie die Ressource. Erst nach Bereinigung der Ressource kann ein Failback der Ressource zu <systemitem class="domainname">earth</systemitem> ausgeführt werden, falls NFS Ganesha in <systemitem class="domainname">mars</systemitem> ausfällt.
   </para>
   <para>
    So bereinigen Sie die Ressource:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Einrichten der Ping-Ressource</title>
   <para>
    Es kann vorkommen, dass der Server den Client aufgrund eines Netzwerkproblems nicht erreicht. Eine Ping-Ressource kann dieses Problem erkennen und abschwächen. Die Konfiguration dieser Ressource ist optional.
   </para>
   <procedure>
    <step>
     <para>
      Definieren Sie die Ping-Ressource:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> ist eine Liste mit IP-Adressen, die durch Leerzeichen getrennt sind. Die IP-Adressen werden regelmäßig gepingt, um nach Netzwerkausfällen zu suchen. Wenn ein Client ständig Zugriff auf den NFS-Server haben muss, fügen Sie ihn zu <literal>host_list</literal> hinzu.
     </para>
    </step>
    <step>
     <para>
      Erstellen Sie einen Klon:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      Das folgende Kommando erstellt eine Einschränkung für den NFS Ganesha-Service. Der Node wird dadurch gezwungen, zu einem anderen Node zu wechseln, wenn <literal>host_list</literal> nicht erreichbar ist.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HA und DeepSea</title>
   <para>
    DeepSea unterstützt nicht die Konfiguration von NFS Ganesha HA. Um zu verhindern, dass DeepSea nach der Konfiguration von NFS Ganesha HA ausfällt, schließen Sie Starten und Stoppen des NFS Ganesha-Service von DeepSea-Phase 4 aus:
   </para>
   <procedure>
    <step>
     <para>
      Kopieren Sie <filename>/srv/salt/ceph/ganesha/default.sls</filename> zu <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Entfernen Sie den Eintrag <literal>.service</literal> aus <filename>/srv/salt/ceph/ganesha/ha.sls</filename>, sodass sie wie folgt aussieht:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Fügen Sie die folgende Zeile zu <filename>/srv/pillar/ceph/stack/global.yml</filename> hinzu:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Aktiv/Aktiv-Konfiguration</title>

  <para>
   Dieser Abschnitt zeigt ein Beispiel einer einfachen NFS Ganesha-Aktiv/Aktiv-Einrichtung. Es sollen zwei NFS Ganesha-Server auf demselben vorhandenen CephFS implementiert werden. Die Server bestehen aus zwei Ceph Cluster Nodes mit separaten Adressen. Die Clients müssen manuell darauf verteilt werden. Bei einem <quote>Failover</quote> in dieser Konfiguration muss der andere Server auf dem Client manuell ausgehängt und wieder eingehängt werden.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Voraussetzungen</title>
   <para>
    Für die Beispielkonfiguration ist Folgendes erforderlich:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Aktiver Ceph Cluster. Weitere Informationen zum Implementieren und Konfigurieren des Ceph Clusters mit DeepSea finden Sie in <xref linkend="ceph-install-stack"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Mindestens ein konfiguriertes CephFS. Weitere Informationen zum Implementieren und Konfigurieren von CephFS finden Sie in <xref linkend="cha-ceph-as-cephfs"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Zwei Ceph Cluster Nodes, auf denen NFS Ganesha implementiert ist. Weitere Informationen zum Implementieren und Konfigurieren von NFS Ganesha finden Sie in <xref linkend="cha-as-ganesha"/>.
     </para>
     <tip>
      <title>Verwenden dedizierter Server</title>
      <para>
       NFS Ganesha Nodes können Ressourcen gemeinsam mit anderen Ceph-spezifischen Services nutzen. Zur Leistungssteigerung werden dennoch dedizierte Server empfohlen.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    Prüfen Sie nach der Implementierung der NFS Ganesha Nodes, ob der Cluster funktionsfähig ist und die standardmäßigen CephFS-Pools vorhanden sind:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Konfigurieren von NFS Ganesha</title>
   <para>
    Prüfen Sie, ob auf beiden NFS Ganesha Nodes die Datei <filename>/etc/ganesha/ganesha.conf</filename> installiert ist. Falls die folgenden Blöcke noch nicht vorhanden sind, fügen Sie sie in die Konfigurationsdatei ein, damit RADOS als Wiederherstellungs-Back-End für NFS Ganesha aktiviert wird.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   Die Werte für <replaceable>rados_pool</replaceable> und <replaceable>pool_namespace</replaceable> finden Sie in der bereits vorhandenen Zeile in der Konfiguration mit der Form:</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   Der Wert für die Option <replaceable>nodeid</replaceable> entspricht dem FQDN des Computers und die Werte für die Optionen <replaceable>UserId</replaceable> und <replaceable>Ceph_Conf</replaceable> sind im bereits vorhandenen Block <replaceable>RADOS_URLS</replaceable> zu finden.
   </para>
   <para>
    Aufgrund von Legacy-Versionen von NFS sind wir nicht in der Lage, den Kulanzzeitraum vorzeitig aufzuheben, weshalb der Neustart des Servers verlängert wird. Aus diesem Grund sind Optionen für NFS vor Version 4.2 deaktiviert. Auch der Großteil des NFS Ganesha-Cachings ist deaktiviert, da die Ceph-Bibliotheken bereits ein aggressives Caching vornehmen.
   </para>
   <para>
    Das Wiederherstellungs-Back-End „rados_cluster“ speichert seine Daten in RADOS-Objekten. Auch wenn dies keine große Datenmenge ist, soll sie doch hochverfügbar sein. Hierzu wird der CephFS-Metadaten-Pool herangezogen, in dem ein neuer Namespace „ganesha“ erstellt wird, getrennt von den CephFS-Objekten.
   </para>
   <note>
    <title>Cluster Node-IDs</title>
    <para>
     Der Großteil der Konfiguration auf beiden Hosts ist identisch; die Option <option>nodeid</option> im Block „RADOS_KV“ muss jedoch jeweils eine eindeutige Zeichenkette für die einzelnen Knoten enthalten. Standardmäßig wird <option>nodeid</option> als Hostname des Knotens in NFS Ganesha festgelegt.
    </para>
    <para>
     Sollen andere feste Werte verwendet werden, also nicht die Hostnamen, legen Sie beispielsweise <option>nodeid = 'a'</option> auf einem Knoten fest und <option>nodeid = 'b'</option> auf dem anderen Knoten.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Befüllen der Cluster-Kulanzdatenbank</title>
   <para>
    Es muss gewährleistet sein, dass alle Knoten im Cluster gegenseitig sichtbar sind. Hierzu wird ein RADOS-Objekt zwischen den Hosts freigegeben. NFS Ganesha übermittelt anhand dieses Objekts den aktuellen Status im Hinblick auf einen Kulanzzeitraum.
   </para>
   <para>
    Das Paket <package>nfs-ganesha-rados-grace</package> enthält ein Kommandozeilenwerkzeug zum Abfragen und Bearbeiten dieser Datenbank. Wenn das Paket nicht auf mindestens einem Knoten installiert ist, installieren Sie es wie folgt:
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    Mit dem Kommando werden die Datenbank erstellt und beide <option>nodeid</option>s hinzugefügt. In diesem Beispiel erhalten die beiden NFS Ganesha Nodes die Bezeichnung <literal>ses6min1.example.com</literal> und <literal>ses6min2.example.com</literal>. Führen Sie folgendes Kommando auf einem der NFS Ganesha-Hosts aus:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    Hiermit wird die Kulanzdatenbank erstellt und sowohl „ses6min1.example.com“ als auch „ses6min2.example.com“ werden in die Datenbank aufgenommen. Das letzte Kommando gibt den aktuellen Status zurück. Bei neu hinzugefügten Hosts wird stets angenommen, dass sie den Kulanzzeitraum erzwingen; bei beiden ist daher die Flagge „E“ gesetzt. Die Werte „cur“ und „rec“ zeigen die aktuelle Epoche und die Wiederherstellungsepoche, womit nachverfolgt werden kann, welche Hosts die Wiederherstellung zu welchem Zeitpunkt durchführen dürfen.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Neustarten der NFS Ganesha-Services</title>
   <para>
    Starten Sie auf beiden NFS Ganesha Nodes die entsprechenden Services neu:
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    Prüfen Sie nach dem Neustart der Services die Kulanzdatenbank:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>Flagge „E“ gelöscht</title>
    <para>
     Beachten Sie, dass bei beiden Knoten die Flagge „E“ gelöscht ist. Dies bedeutet, dass die Knoten den Kulanzzeitraum nicht mehr erzwingen und sich nun im normalen Betriebsmodus befinden.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Schlussfolgerung</title>
   <para>
    Sobald Sie alle obigen Schritte ausgewählt haben, können Sie das exportierte NFS von einem der beiden NFS Ganesha-Server einhängen und normale NFS-Operationen für die Server ausführen.
   </para>
   <para>
    In der Beispielkonfiguration wird vorausgesetzt, dass Sie einen NFS Ganesha-Server bei einem Ausfall manuell innerhalb von 5 Minuten neu starten. Nach 5 Minuten kann der Metadatenserver die Sitzung des NFS Ganesha-Clients abbrechen und den gesamten zugehörigen Zustand stornieren. Werden die Capabilities der Sitzung storniert, bevor der restliche Cluster in den Kulanzzeitraum eintritt, können die Clients des Servers unter Umständen nicht den gesamten Zustand wiederherstellen.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>Weitere Informationen</title>

  <para>
   Weitere Informationen finden Sie im <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
