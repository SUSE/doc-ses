<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha.as.ganesha">

 <title>Installation von NFS Ganesha</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  NFS Ganesha ermöglicht den NFS-Zugriff auf das Object Gateway oder das CephFS. In SUSE Enterprise Storage 5 werden die NFS-Versionen 3 und 4 unterstützt. NFS Ganesha wird im Benutzerbereich statt im Kernel-Bereich ausgeführt und interagiert direkt mit dem Object Gateway oder dem CephFS.
 </para>
 <sect1 xml:id="sec.as.ganesha.preparation">
  <title>Vorbereitung</title>

  <sect2 xml:id="sec.as.ganesha.preparation.general">
   <title>Allgemeine Informationen</title>
   <para>
    Zur erfolgreichen Bereitstellung von NFS Ganesha müssen Sie eine <literal>role-ganesha</literal> zu <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> hinzufügen. Weitere Informationen finden Sie in <xref linkend="policy.configuration"/>. Für NFS Ganesha muss entweder eine <literal>role-rgw</literal> oder eine<literal>role-mds</literal> in der Datei <filename>policy.cfg</filename> vorhanden sein.
   </para>
   <para>
    Es ist zwar möglich, den NFS Ganesha-Server in einem bereits vorhandenen Ceph Node zu installieren und auszuführen, wir empfehlen jedoch, ihn auf einem dedizierten Host mit Zugriff auf den Ceph Cluster auszuführen. Die Client-Hosts sind normalerweise nicht Teil des Clusters, doch sie müssen Netzwerkzugriff auf den NFS Ganesha-Server haben.
   </para>
   <para>
    Fügen sie zur Aktivierung des NFS Ganesha-Servers zu einem beliebigen Zeitpunkt nach der ersten Installation die <literal>role-ganesha</literal> zu <filename>policy.cfg</filename> hinzu und führen Sie mindestens die DeepSea-Phasen 2 und 4 erneut aus. Weitere Informationen finden Sie in <xref linkend="ceph.install.stack"/>.
   </para>
   <para>
    NFS Ganesha wird über die Datei <filename>/etc/ganesha/ganesha.conf</filename> konfiguriert, die im NFS Ganesha Node vorhanden ist. Diese Datei wird jedoch bei jeder Ausführung der DeepSea-Phase 4 überschrieben. Wir empfehlen daher, die von Salt verwendeten Vorlagen zu bearbeiten, bei der es sich um die Datei <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> am Salt Master handelt. Detaillierte Informationen zur Konfigurationsdatei finden Sie im <xref linkend="ceph.nfsganesha.config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.preparation.requirements">
   <title>Anforderungen im Überblick</title>
   <para>
    Die folgenden Anforderungen müssen erfüllt sein, bevor DeepSea-Phasen 2 und 4 zur Installation von NFS Ganesha ausgeführt werden können:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Die <literal>role-ganesha</literal> muss mindestens einem Node zugewiesen sein.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie können nur eine <literal>role-ganesha</literal> pro Minion definieren.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha benötigt entweder ein Object Gateway oder ein CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Wenn NFS Ganesha das Object Gateway als Schnittstelle zum Cluster verwenden soll, dann muss die Datei <filename>/srv/pillar/ceph/rgw.sls</filename> am Salt Master entsprechend aufgefüllt sein.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.basic_example">
  <title>Installationsbeispiel</title>

  <para>
   In diesem Installationsbeispiel werden sowohl das Objekt Gateway als auch die CephFS File System Abstraction Layers (FSAL) von NFS Ganesha verwendet.
  </para>

  <procedure>
   <step>
    <para>
     Wenn Ihnen dies nicht geläufig ist, führen Sie zunächst die DeepSea-Phasen 0 und 1 aus, bevor Sie mit diesem Verfahren fortfahren.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Bearbeiten Sie nach Ausführung der Phase 1 von DeepSea die Datei<filename>/srv/pillar/ceph/proposals/policy.cfg</filename> und fügen Sie folgende Zeile hinzu:
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Ersetzen Sie <replaceable>NODENAME</replaceable> durch den Namen eines Nodes in Ihrem Cluster.
    </para>
    <para>
     Stellen Sie außerdem sicher, dass eine <literal>role-mds</literal> und eine <literal>role-rgw</literal> zugewiesen sind.
    </para>
   </step>
   <step>
    <para>
     Erstellen Sie die Datei <filename>/srv/pillar/ceph/rgw.sls</filename> und fügen Sie den folgenden Inhalt ein:
    </para>
<screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     Diese Benutzer werden später als Object Gateway-Benutzer erstellt. Außerdem werden API-Schlüssel generiert. Im Object Gateway Node können Sie später <command>radosgw-admin user list</command> ausführen, um alle erstellten Benutzer aufzulisten, sowie <command>radosgw-admin user info --uid=demo</command>, um detaillierte Informationen zu einzelnen Benutzern abzurufen.
    </para>
    <para>
     DeepSea stellt sicher, dass sowohl Object Gateway als auch NFS Ganesha die Berechtigungsnachweise aller Benutzer erhalten, die in Abschnitt <literal>rgw</literal> von <filename>rgw.sls</filename> aufgelistet sind.
    </para>
    <para>
     Das exportierte NFS verwendet diese Benutzernamen auf der ersten Ebene des Dateisystems. In diesem Beispiel würden die Pfade <filename>/demo</filename> und <filename>/demo1</filename> exportiert werden.
    </para>
   </step>
   <step>
    <para>
     Führen Sie mindestens die Phasen 2 und 4 von DeepSea aus. Wir empfehlen, dazwischen auch Phase 3 auszuführen.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass NFS Ganesha funktioniert. Hängen Sie dazu die NFS-Freigabe von einem Client Node aus ein:
    </para>
<screen><prompt>root # </prompt><command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
<prompt>root # </prompt><command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> sollte alle exportierten Pfade enthalten. Für CephFS- und Object Gateway-Benutzer sollten Verzeichnisse vorhanden sein. Für jeden Bucket, den ein Benutzer besitzt, würde ein Pfad <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename> exportiert werden.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.ha-ap">
  <title>Hochverfügbare Aktiv-Passiv-Konfiguration</title>

  <para>
   In diesem Abschnitt finden Sie ein Beispiel, wie eine Aktiv-Passiv-Konfiguration mit zwei Nodes des NFS Ganesha-Servers eingerichtet wird. Für die Einrichtung ist die SUSE Linux Enterprise High Availability Extension erforderlich. Die beiden Nodes werden <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem> genannt.
  </para>

  <para>
   Detaillierte Informationen zu SUSE Linux Enterprise High Availability Extension finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <sect2 xml:id="sec.as.ganesha.ha-ap.basic">
   <title>Standardinstallation</title>
   <para>
    In dieser Einrichtung hat <systemitem class="domainname">earth</systemitem> die IP-Adresse <systemitem class="ipaddress">192.168.1.1</systemitem> und <systemitem class="domainname">mars</systemitem> die Adresse <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Außerdem können Clients über zwei virtuelle IP-Adressen nach dem Floating-IP-Prinzip eine Verbindung mit dem Service herstellen, und zwar unabhängig davon, auf welchem physischen Node er ausgeführt wird. <systemitem class="ipaddress">192.168.1.10</systemitem> wird für die Cluster-Verwaltung mit Hawk2 verwendet und <systemitem class="ipaddress">192.168.2.1</systemitem> exklusiv für die NFS-Exporte. Dies erleichtert später die Anwendung von Sicherheitsbeschränkungen.
   </para>
   <para>
    Das folgende Verfahren beschreibt das Installationsbeispiel. Weitere Informationen finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
   </para>
   <procedure xml:id="proc.as.ganesha.ha-ap">
    <step>
     <para>
      Bereiten Sie die NFS Ganesha Nodes am Salt Master vor:
     </para>
     <substeps>
      <step>
       <para>
        Führen Sie die DeepSea-Phasen 0 und 1 am Salt Master aus.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Weisen Sie den Nodes <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem> die <literal>role-ganesha</literal> in Datei<filename>/srv/pillar/ceph/proposals/policy.cfg</filename> zu:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Führen Sie die DeepSea-Phasen 3 und 4 am Salt Master aus.
       </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registrieren Sie die SUSE Linux Enterprise High Availability Extension in <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installieren Sie <package>ha-cluster-bootstrap</package> in beiden Nodes:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialisieren Sie den Cluster in <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Lassen Sie <systemitem class="domainname">mars</systemitem> dem Cluster beitreten:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Prüfen Sie den Status des Clusters. Sie sollten zwei Nodes sehen, die im Cluster hinzugefügt wurden:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Deaktivieren Sie in beiden Nodes den automatischen Start des NFS Ganesha Service beim Booten des Systems:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Starten Sie die <command>crm</command>-Shell im Node <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Die nächsten Kommandos werden in der crm-Shell ausgeführt.
     </para>
    </step>
    <step>
     <para>
      Führen Sie in <systemitem class="domainname">earth</systemitem> die crm-Shell aus, um die folgenden Kommandos zur Konfiguration der Ressource für NFS Ganesha Daemons als Klon des Ressourcentyps "systemd" auszuführen:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine einfache IPAddr2 mit der crm-Shell:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Wir stellen eine Beziehung zwischen dem NFS Ganesha-Server und der Floating-IP-Adresse über Kollokation und Anordnung her.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Führen Sie das Kommando <command>mount</command> am Client aus, um sicherzustellen, dass die Cluster-Einrichtung vollständig ist:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.ha-ap.cleanup">
   <title>Bereinigen der Ressourcen</title>
   <para>
    Falls ein NFS Ganesha, wie zum Beispiel <systemitem class="domainname">earth</systemitem>, in einem der Nodes ausfällt, beheben Sie das Problem und bereinigen Sie die Ressource. Erst nach Bereinigung der Ressource kann ein Failback der Ressource zu <systemitem class="domainname">earth</systemitem> ausgeführt werden, falls NFS Ganesha in <systemitem class="domainname">mars</systemitem> ausfällt.
   </para>
   <para>
    So bereinigen Sie die Ressource:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.ha-ap.ping-resource">
   <title>Einrichten der Ping-Ressource</title>
   <para>
    Es kann vorkommen, dass der Server den Client aufgrund eines Netzwerkproblems nicht erreicht. Eine Ping-Ressource kann dieses Problem erkennen und abschwächen. Die Konfiguration dieser Ressource ist optional.
   </para>
   <procedure>
    <step>
     <para>
      Definieren Sie die Ping-Ressource:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> ist eine Liste mit IP-Adressen, die durch Leerzeichen getrennt sind. Die IP-Adressen werden regelmäßig gepingt, um nach Netzwerkausfällen zu suchen. Wenn ein Client ständig Zugriff auf den NFS-Server haben muss, fügen Sie ihn zu <literal>host_list</literal> hinzu.
     </para>
    </step>
    <step>
     <para>
      Erstellen Sie einen Klon:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      Das folgende Kommando erstellt eine Einschränkung für den NFS Ganesha-Service. Der Node wird dadurch gezwungen, zu einem anderen Node zu wechseln, wenn <literal>host_list</literal> nicht erreichbar ist.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha_ha_deepsea">
   <title>NFS Ganesha HA und DeepSea</title>
   <para>
    DeepSea unterstützt nicht die Konfiguration von NFS Ganesha HA. Um zu verhindern, dass DeepSea nach der Konfiguration von NFS Ganesha HA ausfällt, schließen Sie Starten und Stoppen des NFS Ganesha-Service von DeepSea Phase 4 aus:
   </para>
   <procedure>
    <step>
     <para>
      Kopieren Sie <filename>/srv/salt/ceph/ganesha/default.sls</filename> zu <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Entfernen Sie den Eintrag <literal>.service</literal> aus <filename>/srv/salt/ceph/ganesha/ha.sls</filename>, sodass sie wie folgt aussieht:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Fügen Sie die folgende Zeile zu <filename>/srv/pillar/ceph/stack/global.yml</filename> hinzu:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.info">
  <title>Weitere Informationen</title>

  <para>
   Weitere Informationen finden Sie im <xref linkend="cha.ceph.nfsganesha"/>.
  </para>
 </sect1>
</chapter>
