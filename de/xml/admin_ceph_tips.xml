<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_tips.xml" version="5.0" xml:id="storage-tips">
 <title>Hinweise und Tipps</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In diesem Kapitel finden Sie Informationen, die Ihnen dabei helfen, die Leistung Ihres Ceph Clusters zu verbessern. Außerdem erhalten Sie Tipps zum Einrichten des Clusters.
 </para>
 <sect1 xml:id="tips-orphaned-partitions">
  <title>Erkennen vom bezuglosen Partitionen</title>

  <para>
   So erkennen Sie mögliche bezuglose Journal/WAL/DB-Geräte:
  </para>

  <procedure>
   <step>
    <para>
     Wählen Sie das Gerät, auf dem sich möglicherweise bezuglose Partitionen befinden, und speichern Sie die Liste dieser Partitionen in eine Datei:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>ls /dev/sdd?* &gt; /tmp/partitions
</screen>
   </step>
   <step>
    <para>
     Führen Sie <command>readlink</command> gegen alle "block.wal"-, "block.db"- und „journal“-Geräte aus und vergleichen Sie die Ausgabe mit der vorher gespeicherten Liste der Partitionen:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
    <para>
     Die Ausgabe stellt die Liste der Partitionen dar, die <emphasis>nicht</emphasis> von Ceph genutzt werden.
    </para>
   </step>
   <step>
    <para>
     Entfernen Sie die bezuglosen Partitionen, die nicht zu Ceph gehören, mit Ihrem bevorzugten Kommando (beispielsweise <command>fdisk</command>, <command>parted</command> oder <command>sgdisk</command>).
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="tips-scrubbing">
  <title>Anpassen des Scrubbings</title>

  <para>
   Ceph führt standardmäßig täglich ein Light Scrubbing und wöchentlich ein Deep Scrubbing durch (Detailinformationen hierzu finden Sie in <xref linkend="scrubbing"/>). Beim <emphasis>Light</emphasis> Scrubbing werden die Objektgrößen und Prüfsummen geprüft, um sicherzustellen, dass Placement Groups dieselben Objektdaten speichern. Beim <emphasis>Deep</emphasis> Scrubbing wird der Inhalt eines Objekt mit dem Inhalt seiner Reproduktionen verglichen, um sicherzustellen, dass die tatsächlichen Inhalte identisch sind. Die Überprüfung der Datenintegrität führt zu einer höheren E/A-Last am Cluster während des Scrubbing-Vorgangs.
  </para>

  <para>
   Mit den Standardeinstellungen könnten Ceph OSDs ein Scrubbing zu unpassenden Zeiten durchführen, wie zum Beispiel in Zeiten mit sehr hoher Last. Kunden erfahren Latenz und schlechte Leistung, wenn zwischen den Scrubbing-Operationen und den Operationen der Kunden ein Konflikt besteht. Ceph bietet verschiedene Scrubbing-Einstellungen, die das Scrubbing auf Zeiten mit geringerer Last außerhalb der Spitzenzeiten beschränken.
  </para>

  <para>
   Wenn die Cluster-Last tagsüber hoch und nachts gering ist, sollten Sie das Scrubbing auf die Nachtstunden beschränken, wie zum Beispiel zwischen 23 Uhr und 6 Uhr:
  </para>

<screen>
[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6
</screen>

  <para>
   Wenn die Zeitbeschränkung keine effiziente Methode zum Festlegen eines Scrubbing-Zeitplans ist, sollten Sie die Option <option>osd_scrub_load_threshold</option> verwenden. Der Standardwert ist 0,5, doch der Wert könnte für Bedingungen mit geringer Last geändert werden:
  </para>

<screen>
[osd]
osd_scrub_load_threshold = 0.25
</screen>
 </sect1>
 <sect1 xml:id="tips-stopping-osd-without-rebalancing">
  <title>Stoppen von OSDs ohne erneuten Ausgleich</title>

  <para>
   Sie möchten möglicherweise die OSDS regelmäßig zur Wartung stoppen. Legen Sie den Cluster zunächst auf <literal>noout</literal> fest, wenn Sie nicht möchten, dass CRUSH automatisch den Cluster ausgleicht, um enorme Datenübertragungen zu vermeiden:
  </para>

<screen>
<prompt>root@minion &gt; </prompt>ceph osd set noout
</screen>

  <para>
   Wenn der Cluster auf <literal>noout</literal> festgelegt ist, können Sie zu Beginn die OSDs in der Fehlerdomäne stoppen, die gewartet werden muss:
  </para>

<screen>
<prompt>root@minion &gt; </prompt>systemctl stop ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   Weitere Informationen finden Sie in <xref linkend="ceph-operating-services-individual"/>.
  </para>

  <para>
   Starten Sie die OSDs nach der Wartung erneut:
  </para>

<screen>
<prompt>root@minion &gt; </prompt>systemctl start ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   Entfernen Sie die Cluster-Einstellung <literal>noout</literal> nach dem Starten der OSD Services:
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd unset noout
</screen>
 </sect1>
 <sect1 xml:id="Cluster-Time-Setting">
  <title>Zeitsynchronisierung der Nodes</title>

  <para>
   Für Ceph ist eine präzise Zeitsynchronisierung zwischen allen Nodes erforderlich.
  </para>

  <para>
   Es wird empfohlen, alle Ceph Cluster Nodes mit mindestens drei zuverlässigen Zeitquellen zu synchronisieren, die sich im internen Netzwerk befinden. Die internen Zeitquellen können auf einen öffentlichen Zeitserver verweisen oder eine eigene Zeitquelle umfassen.
  </para>

  <important>
   <title>Öffentliche Zeitserver</title>
   <para>
    Synchronisieren Sie nicht alle Ceph Cluster Nodes direkt mit öffentlichen Remote-Zeitservern. Bei einer derartigen Konfiguration hat jeder Node im Cluster einen eigenen NTP-Daemon. Diese kommunizieren kontinuierlich über das Internet mit drei oder vier Zeitservern, die unter Umständen leicht abweichende Zeitangaben zurückgeben. Bei dieser Lösung ist mit einem hohen Maß an Latenzschwankung zu rechnen, was es schwierig oder gar unmöglich macht, die Uhrenfehler unter 0,05 Sekunden zu halten (was wiederum für die Ceph Monitors erforderlich ist).
   </para>
  </important>

  <para>
   Detailinformationen zur Einrichtung der NTP-Server finden Sie im <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_ntp.html">SUSE Linux Enterprise Server-Verwaltungshandbuch</link>.
  </para>

  <para>
   Gehen Sie folgendermaßen vor, um die Zeit an Ihrem Cluster zu ändern:
  </para>

  <important>
   <title>Einstellen der Uhrzeit</title>
   <para>
    Sie kommen möglicherweise hin und wieder in eine Situation, in der Sie die Uhrzeit zurücksetzen müssen, beispielsweise wenn die Sommerzeit zur Standardzeit umgestellt wird. Es ist nicht empfehlenswert, die Uhrzeit für einen längeren Zeitraum als den Ausfall des Clusters zurückzustellen. Die Uhrzeit vorzustellen, verursacht dagegen keine Probleme.
   </para>
  </important>

  <procedure>
   <title>Zeitsynchronisierung am Cluster</title>
   <step>
    <para>
     Stoppen Sie alle Clients, die auf den Ceph Cluster zugreifen, insbesondere die Clients, die iSCSI verwenden.
    </para>
   </step>
   <step>
    <para>
     Fahren Sie Ihren Ceph Cluster herunter. Führen Sie in jedem Node Folgendes aus:
    </para>
<screen><prompt>root # </prompt>systemctl stop ceph.target</screen>
    <note>
     <para>
      Wenn Sie Ceph und SUSE OpenStack Cloud verwenden, stoppen Sie auch die SUSE OpenStack Cloud.
     </para>
    </note>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass Ihr NTP-Server korrekt eingerichtet ist. Alle <systemitem class="daemon">chronyd</systemitem>-Daemons müssen ihre Uhrzeit aus mindestens einer Quelle im lokalen Netzwerk beziehen.
    </para>
   </step>
   <step>
    <para>
     Legen Sie die korrekte Uhrzeit auf Ihrem NTP-Server fest.
    </para>
   </step>
   <step>
    <para>
     Verifizieren Sie, dass NTP ausgeführt wird und ordnungsgemäß funktioniert. Führen Sie in allen Nodes Folgendes aus:
    </para>
<screen><prompt>root # </prompt>systemctl status chronyd.service</screen>
   </step>
   <step>
    <para>
     Starten Sie alle Überwachungs-Nodes und verifizieren Sie, dass kein Taktversatz besteht:
    </para>
<screen><prompt>root # </prompt>systemctl start <replaceable>target</replaceable></screen>
   </step>
   <step>
    <para>
     Starten Sie alle OSD Nodes.
    </para>
   </step>
   <step>
    <para>
     Starten Sie sonstige Ceph-Services.
    </para>
   </step>
   <step>
    <para>
     Starten Sie die SUSE OpenStack Cloud, falls vorhanden.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage-bp-cluster-mntc-unbalanced">
  <title>Prüfung auf nicht ausgeglichene Datenschreibvorgänge</title>

  <para>
   Wenn Daten gleichmäßig verteilt an OSDs geschrieben werden, ist der Cluster ausgeglichen. Jedem OSD in einem Cluster ist ein eigenes <emphasis>Gewicht</emphasis> zugewiesen. Das Gewicht ist eine relative Zahl und informiert Ceph darüber, wie viele der Daten auf den jeweiligen OSD geschrieben werden sollten. Je höher das Gewicht, desto mehr Daten werden geschrieben. Wenn ein OSD kein Gewicht aufweist, werden keine Daten an ihn geschrieben. Wenn das Gewicht eines OSD relativ hoch ist im Vergleich zu anderen OSDs, wird ein Großteil der Daten an diesen OSD geschrieben, wodurch der Cluster unausgeglichen wird.
  </para>

  <para>
   Unausgeglichene Cluster weisen eine schlechte Leistung auf. Falls ein OSD mit einem hohen Gewicht plötzlich abstürzt, müssen sehr viele Daten zu anderen OSDs verschoben werden, was den Cluster ebenfalls verlangsamt.
  </para>

  <para>
   Um dies zu vermeiden, sollten sie regelmäßig die OSDs auf die Menge der geschriebenen Daten hin überprüfen. Wenn die Menge zwischen 30 und 50 Prozent der Kapazität einer Gruppe von OSDs, die durch einen Regelsatz angegeben ist, ausmacht, müssen Sie das Gewicht der OSDs neu festlegen. Prüfen Sie, welche der einzelnen Festplatten schneller gefüllt werden als andere (oder allgemein langsamer sind) und reduzieren Sie deren Gewicht. Das gleiche gilt für OSDs, an die nicht genug Daten geschrieben werden. Erhöhen Sie deren Gewicht, damit Ceph mehr Daten an sie schreibt. Im folgenden Beispiel ermitteln Sie das Gewicht eines OSDs mit ID 13 und ändern sein Gewicht von 3 auf 3,05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>Ändern des OSD-Gewichts entsprechend der Auslastung</title>
   <para>
    Mit dem Kommando <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> wird der Vorgang der Reduzierung des Gewichts von übermäßig ausgelasteten OSDs automatisiert. Standardmäßig wird das Gewicht von OSDs verringert, die 120 % der durchschnittlichen Auslastung erreicht haben. Wenn Sie jedoch einen Schwellwert („threshold“) hinzufügen, wird stattdessen dieser Prozentsatz verwendet.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage-tips-ceph-btrfs-subvol">
  <title>Btrfs-Subvolume für <filename>/var/lib/ceph</filename> auf Ceph Monitor Nodes</title>

  <para>
   SUSE Linux Enterprise wird standardmäßig auf einer Btrfs-Partition installiert. Ceph Monitors speichern ihren Status und ihre Datenbank im Verzeichnis <filename>/var/lib/ceph</filename>. Damit ein Ceph Monitor nicht durch ein System-Rollback eines früheren Snapshots beschädigt wird, erstellen Sie ein Btrfs-Subvolume für <filename>/var/lib/ceph</filename>. Mithilfe eines dedizierten Subvolumes werden die Monitor-Daten bei Snapshots des Root-Subvolumes ausgeschlossen.
  </para>

  <tip>
   <para>
    Erstellen Sie das Subvolume <filename>/var/lib/ceph</filename>, bevor Sie DeepSea-Phase 0 ausführen, da in Phase 0 die Ceph-spezifischen Pakete installiert werden und das Verzeichnis <filename>/var/lib/ceph</filename> erstellt wird.
   </para>
  </tip>

  <para>
   DeepSea-Phase 3 überprüft dann, ob <filename>@/var/lib/ceph</filename> ein Btrfs-Subvolume ist, und schlägt fehl, falls dies ein normales Verzeichnis ist.
  </para>

  <sect2 xml:id="btrfs-subvol-requirements">
   <title>Anforderungen</title>
   <sect3 xml:id="tips-ceph-btrfs-subvol-new">
    <title>Neue Implementierungen</title>
    <para>
     Salt und DeepSea müssen ordnungsgemäß installiert und funktionsfähig sein.
    </para>
   </sect3>
   <sect3 xml:id="storage-tips-ceph-btrfs-subvol-req-existing">
    <title>Vorhandene Implementierungen</title>
    <para>
     Falls Ihr Cluster bereits installiert ist, müssen die folgenden Voraussetzungen erfüllt werden:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Die Nodes werden auf SUSE Enterprise Storage 6 aufgerüstet und der Cluster wird von DeepSea gesteuert.
      </para>
     </listitem>
     <listitem>
      <para>
       Der Ceph Cluster ist aktiv und fehlerfrei.
      </para>
     </listitem>
     <listitem>
      <para>
       Beim Upgrade wurden die Salt- und DeepSea-Module mit allen Minion Nodes synchronisiert.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="storage-tips-ceph-btrfs-subvol-automatic">
   <title>Schritte bei einer neuen Cluster-Implementierung</title>
   <sect3 xml:id="var-lib-ceph-stage0">
    <title>Vor Ausführung der DeepSea-Phase 0</title>
    <para>
     Wenden Sie vor der Ausführung von DeepSea-Phase 0 die folgenden Kommandos auf jeden Salt Minion an, der als Ceph Monitor fungieren soll:
    </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>MONITOR_NODES</replaceable>' saltutil.sync_all
<prompt>root@master # </prompt>salt '<replaceable>MONITOR_NODES</replaceable>' state.apply ceph.subvolume
</screen>
    <para>
     Mit dem Kommando <command>ceph.subvolume</command> wird:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>/var/lib/ceph</filename> als ein Btrfs-Subvolume für <literal>@/var/lib/ceph</literal> erstellt.
      </para>
     </listitem>
     <listitem>
      <para>
       das neue Subvolume eingehängt und <filename>/etc/fstab</filename> entsprechend aktualisiert.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Validierung in DeepSea-Phase 3 schlägt fehl</title>
    <para>
     Falls Sie die in <xref linkend="var-lib-ceph-stage0"/> aufgeführten Kommandos nicht vor Ausführung der Phase 0 ausgeführt hatten, ist das Unterverzeichnis <filename>/var/lib/ceph</filename> bereits vorhanden, sodass die Validierung in DeepSea-Phase 3 fehlschlägt. So konvertieren Sie es in ein Subvolume:
    </para>
    <procedure>
     <step>
      <para>
       Wechseln Sie zum Verzeichnis in <filename>/var/lib</filename>:
      </para>
<screen><prompt>cephadm@mon &gt; </prompt>cd /var/lib</screen>
     </step>
     <step>
      <para>
       Sichern Sie den aktuellen Inhalt des Unterverzeichnisses <filename>ceph</filename>:
      </para>
<screen><prompt>cephadm@mon &gt; </prompt>sudo mv ceph ceph-</screen>
     </step>
     <step>
      <para>
       Erstellen Sie das Subvolume, hängen Sie es ein und aktualisieren Sie <filename>/etc/fstab</filename>:
      </para>
<screen><prompt>root@master # </prompt>salt 'MONITOR_NODES' state.apply ceph.subvolume</screen>
     </step>
     <step>
      <para>
       Wechseln Sie zum Sicherungs-Unterverzeichnis, synchronisieren Sie dessen Inhalt mit dem neuen Subvolume und entfernen Sie es dann:
      </para>
<screen><prompt>cephadm@mon &gt; </prompt>cd /var/lib/ceph-
<prompt>cephadm@mon &gt; </prompt>rsync -av . ../ceph
<prompt>cephadm@mon &gt; </prompt>cd ..
<prompt>cephadm@mon &gt; </prompt>rm -rf ./ceph-
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="btrfs-subvol-upgrades">
   <title>Schritte bei einem Cluster-Upgrade</title>
   <para>
    In SUSE Enterprise Storage 5.5 befindet sich das Verzeichnis <filename>/var</filename> nicht in einem Btrfs-Subvolume, sondern dessen Unterordner (beispielsweise <filename>/var/log</filename> oder <filename>/var/cache</filename>) sind Btrfs-Subvolumes unter „@“. Beim Erstellen von <filename>@/var/lib/ceph</filename>-Subvolumes müssen Sie zunächst das Subvolume „@“ einhängen (es wird nicht standardmäßig eingehängt) und das Subvolume <filename>@/var/lib/ceph</filename> darunter erstellen.
   </para>
   <para>
    Die folgenden Beispielkommandos verdeutlichen diesen Ablauf:
   </para>
<screen>
<prompt>root # </prompt>mkdir -p /mnt/btrfs
<prompt>root # </prompt>mount -o subvol=@ <replaceable>ROOT_DEVICE</replaceable> /mnt/btrfs
<prompt>root # </prompt>btrfs subvolume create /mnt/btrfs/var/lib/ceph
<prompt>root # </prompt>umount /mnt/btrfs
</screen>
   <para>
    Zu diesem Zeitpunkt wird das Subvolume <filename>@/var/lib/ceph</filename> erstellt und Sie können den Vorgang gemäß den Anweisungen in <xref linkend="storage-tips-ceph-btrfs-subvol-automatic"/> fortsetzen.
   </para>
  </sect2>

  <sect2 xml:id="var-lib-ceph-subvol-manual">
   <title>Manuelle Einrichtung</title>
   <para>
    Die automatische Einrichtung des Btrfs-Subvolumes <filename>@/var/lib/ceph</filename> auf den Ceph Monitor Nodes eignet sich nicht für alle Situationen. So migrieren Sie Ihr Verzeichnis <filename>/var/lib/ceph</filename> zu einem Subvolume <filename>@/var/lib/ceph</filename>:
   </para>
   <procedure>
    <step>
     <para>
      Beenden Sie die Ausführung der Ceph-Prozesse.
     </para>
    </step>
    <step>
     <para>
      Hängen Sie die OSDs im Node aus.
     </para>
    </step>
    <step>
     <para>
      Wechseln Sie zum Sicherungs-Unterverzeichnis, synchronisieren Sie dessen Inhalt mit dem neuen Subvolume und entfernen Sie es dann:
     </para>
<screen><prompt>cephadm@mon &gt; </prompt>cd /var/lib/ceph-
<prompt>cephadm@mon &gt; </prompt>rsync -av . ../ceph
<prompt>cephadm@mon &gt; </prompt>cd ..
<prompt>cephadm@mon &gt; </prompt>rm -rf ./ceph-
</screen>
    </step>
    <step>
     <para>
      Hängen Sie die OSDs wieder ein.
     </para>
    </step>
    <step>
     <para>
      Starten Sie die Ceph Daemons neu.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="var-lib-ceph-subvol-moreinfo">
   <title>Weiterführende Informationen</title>
   <para>
    Weitere Informationen zur manuellen Einrichtung finden Sie in der Datei <filename>/srv/salt/ceph/subvolume/README.md</filename> auf dem Salt Master Node.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-srv-maint-fds-inc">
  <title>Erhöhen der Dateideskriptoren</title>

  <para>
   Bei OSD-Daemons sind die Lese/Schreib-Operationen entscheidend für den Ausgleich im Ceph Cluster. Sie müssen oft viele Dateien gleichzeitig zum Lesen und Schreiben offen halten. Auf Betriebssystemebene wird die maximale Anzahl von gleichzeitig offenen Dateien als „maximale Anzahl der Dateideskriptoren“ bezeichnet.
  </para>

  <para>
   Um zu verhindern, dass den OSDs die Dateideskriptoren ausgehen, überschreiben Sie den Standardwert des Betriebssystems und geben Sie die Anzahl in <filename>/etc/ceph/ceph.conf</filename> an. Beispiel:
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   Nach dem Ändern von <option>max_open_files</option> müssen Sie den OSD-Service im relevanten Ceph Node neu starten.
  </para>
 </sect1>
 <sect1 xml:id="storage-admin-integration">
  <title>Integration in Virtualisierungssoftware</title>

  <sect2 xml:id="storage-bp-integration-kvm">
   <title>Speichern von KVM-Datenträgern im Ceph Cluster</title>
   <para>
    Sie können ein Datenträger-Image für KVM-betriebene virtuelle Maschinen erstellen, diese in einem Ceph Pool speichern, optional den Inhalt eines bestehenden Image zu diesem Image konvertieren und dann die virtuelle Maschine mit <command>qemu-kvm</command> ausführen und das im Cluster gespeicherte Datenträger-Image dazu verwenden. Weitere Informationen hierzu finden Sie in <xref linkend="cha-ceph-kvm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-integration-libvirt">
   <title>Speichern von <systemitem class="library">libvirt</systemitem>-Datenträgern im Ceph Cluster</title>
   <para>
    Auf ähnliche Weise wie KVM (Informationen hierzu finden Sie in <xref linkend="storage-bp-integration-kvm"/>) können Sie Ceph zum Speichern virtueller Maschinen verwenden, die über <systemitem class="library">libvirt</systemitem> betrieben werden. Der Vorteil besteht darin, dass Sie jede beliebige von <systemitem class="library">libvirt</systemitem> unterstützte Virtualisierungslösung ausführen können, wie etwa KVM, Xen oder LXC. Weitere Informationen finden Sie in <xref linkend="cha-ceph-libvirt"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-integration-xen">
   <title>Speichern von Xen-Datenträgern im Ceph Cluster</title>
   <para>
    Eine Methode zum Speichern von Xen-Datenträgern mit Ceph ist die Verwendung von <systemitem class="library">libvirt</systemitem> wie in <xref linkend="cha-ceph-libvirt"/> beschrieben.
   </para>
   <para>
    Alternativ kann Xen zur direkten Kommunikation mit dem <systemitem>rbd</systemitem>-Blockgerät eingerichtet werden:
   </para>
   <procedure>
    <step>
     <para>
      Wenn Sie kein Festplatten-Image für Xen vorbereitet haben, erstellen sie ein neues Image:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      Listen Sie die Images im Pool <literal>mypool</literal> auf und prüfen Sie, ob das neue Image vorhanden ist:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Erstellen Sie ein neues Blockgerät, indem Sie das Image <literal>myimage</literal> zum Kernel-Modul <systemitem>rbd</systemitem> zuordnen:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map --pool mypool myimage</screen>
     <tip>
      <title>Benutzername und -authentifizierung</title>
      <para>
       Geben Sie einen Benutzernamen mit <option>--id <replaceable>user-name</replaceable></option> an. Wenn Sie die <systemitem>cephx</systemitem>-Authentifizierung nutzen, müssen Sie außerdem ein Geheimnis angeben. Es kann von einem Schlüsselbund stammen oder aus einer Datei, die das Geheimnis enthält:
      </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
      <para>
       oder
      </para>
<screen><systemitem class="username">cephadm</systemitem>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      Listen Sie alle zugeordneten Geräte auf:
     </para>
<screen><command>rbd showmapped</command>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Nun können Sie Xen so konfigurieren, dass es dieses Gerät als Festplatte zum Ausführen einer virtuellen Maschine verwendet. Beispielsweise kann die folgende Zeile zur Domänenkonfigurationsdatei vom Typ <command>xl</command> hinzugefügt werden:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-net-firewall">
  <title>Firewall-Einstellungen für Ceph</title>

  <warning>
   <title>DeepSea-Phasen werden bei aktiver Firewall nicht durchgeführt</title>
   <para>
    Die DeepSea-Phasen zur Bereitstellung werden nicht ausgeführt, wenn die Firewall aktiv ist (und sogar konfiguriert). Um die Phasen korrekt abzuschließen, müssen Sie entweder die Firewall durch Ausführen von
   </para>
<screen>
<prompt>root # </prompt>systemctl stop SuSEfirewall2.service
</screen>
   <para>
    ausschalten oder die Option <option>FAIL_ON_WARNING</option> in <filename>/srv/pillar/ceph/stack/global.yml</filename> auf „False“ festlegen:
   </para>
<screen>
FAIL_ON_WARNING: False
</screen>
  </warning>

  <para>
   Wir empfehlen, die Netzwerk-Cluster-Kommunikation mit SUSE Firewall. Deren Konfiguration wird durch Auswahl von <menuchoice><guimenu>YaST</guimenu><guimenu>Sicherheit und Benutzer</guimenu><guimenu>Firewall</guimenu><guimenu>Zulässige Dienste</guimenu></menuchoice> bearbeitet.
  </para>

  <para>
   Nachfolgend sehen Sie eine Liste der für Ceph relevanten Services und Nummern der Ports, die diese normalerweise verwenden:
  </para>

  <variablelist>
   <varlistentry>
    <term>Ceph Monitor</term>
    <listitem>
     <para>
      Aktivieren Sie den <guimenu>Ceph MON</guimenu>-Service oder Port 6789 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ceph OSD oder Metadata Server</term>
    <listitem>
     <para>
      Aktivieren Sie den <guimenu>Ceph OSD/MDS</guimenu>-Service oder die Ports 6800-7300 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>iSCSI Gateway</term>
    <listitem>
     <para>
      Öffnen Sie Port 3260 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Object Gateway</term>
    <listitem>
     <para>
      Öffnen Sie den Port, an dem die Object Gateway-Kommunikation stattfindet. Er wird in <filename>/etc/ceph.conf</filename> in der Zeile beginnend mit <literal>rgw frontends =</literal> festgelegt. Die Standardeinstellung ist 80 für HTTP und 443 für HTTPS (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Ganesha</term>
    <listitem>
     <para>
      NFS Ganesha verwendet standardmäßig die Ports 2049 (NFS-Service, TCP) und 875 (rquota-Unterstützung, TCP). Weitere Informationen zum Ändern der standardmäßigen Ports für NFS Ganesha finden Sie in <xref linkend="ganesha-nfsport"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Apache-basierte Services, wie SMT oder SUSE Manager</term>
    <listitem>
     <para>
      Öffnen Sie Port 80 für HTTP und Port 443 für HTTPS (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>
      Öffnen Sie Port 22 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Öffnen Sie Port 123 (UDP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Salt</term>
    <listitem>
     <para>
      Öffnen Sie Port 4505 und Port 4506 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Grafana</term>
    <listitem>
     <para>
      Öffnen Sie Port 3000 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Prometheus</term>
    <listitem>
     <para>
      Öffnen Sie Port 9100 (TCP).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-bp-network-test">
  <title>Testen der Netzwerkleistung</title>

  <para>
   Zum Testen der Netzwerkleistung stellt das DeepSea <literal>net</literal>-Ausführungsprogramm die folgenden Kommandos zur Verfügung. 
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Ein einfaches Ping an alle Nodes:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</screen>
   </listitem>
   <listitem>
    <para>
     Ein Jumbo-Ping an alle Nodes:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</screen>
   </listitem>
   <listitem>
    <para>
     Ein Bandbreitentest:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</screen>
    <tip>
     <title>„iperf3“-Prozesse manuell anhalten</title>
     <para>
      Wenn Sie einen Test mit dem <command>net.iperf</command>-Ausführungsprogramm durchführen, werden die gestarteten „iperf3“-Serverprozesse nach Abschluss eines Tests nicht automatisch angehalten. Mit folgendem Ausführungsprogramm halten Sie die Prozesse an:
     </para>
<screen><prompt>root@master # </prompt>salt '*' multi.kill_iperf_cmd</screen>
    </tip>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="bp-flash-led-lights">
  <title>Auffinden physischer Datenträger mithilfe von LED-Leuchten</title>

  <para>
   In diesem Abschnitt wird die Anpassung der LED-Leuchten an physischen Datenträgern mit <systemitem>libstoragemgmt</systemitem> und/oder mit Drittanbieterwerkzeugen erläutert. Diese Funktion ist unter Umständen nicht für alle Hardwareplattformen verfügbar.
  </para>

  <para>
   Die Zuordnung eines OSD-Datenträgers zu einem physischen Datenträger kann mit einigen Schwierigkeiten verbunden sein, insbesondere auf Nodes mit hoher Datenträgerdichte. Einige Hardwareumgebungen arbeiten mit LED-Leuchten, die per Software so angepasst werden können, dass sie zu Erkennungszwecken blinken oder eine andere Farbe zeigen. SUSE Enterprise Storage unterstützt diese Funktion über Salt, <systemitem>libstoragemgmt</systemitem> und spezielle Drittanbieterwerkzeuge für die verwendete Hardware. Die Konfiguration für diese Funktion ist im Salt-Pillar <filename>/srv/pillar/ceph/disk_led.sls</filename> definiert:
  </para>

<screen><prompt>root # </prompt> cat /srv/pillar/ceph/disk_led.sls
# This is the default configuration for the storage enclosure LED blinking.
# The placeholder {device_file} will be replaced with the device file of
# the disk when the command is executed.
#
# Have a look into the /srv/pillar/ceph/README file to find out how to
# customize this configuration per minion/host.

disk_led:
  cmd:
    ident:
      'on': lsmcli local-disk-ident-led-on --path '{device_file}'
      'off': lsmcli local-disk-ident-led-off --path '{device_file}'
    fault:
      'on': lsmcli local-disk-fault-led-on --path '{device_file}'
      'off': lsmcli local-disk-fault-led-off --path '{device_file}'</screen>

  <para>
   Die Standardkonfiguration für <filename>disk_led.sls</filename> unterstützt die Datenträger-LEDs über die <systemitem>libstoragemgmt</systemitem>-Schicht. <systemitem>libstoragemgmt</systemitem> leistet diese Unterstützung dabei über ein hardwarespezifisches Plugin und über Drittanbieterwerkzeuge. <systemitem>libstoragemgmt</systemitem> kann die LEDs nur dann anpassen, wenn sowohl das <systemitem>libstoragemgmt</systemitem>-Plugin als auch die erforderlichen Drittanbieterwerkzeuge für die Hardware installiert sind.
  </para>

  <para>
   Unabhängig davon, ob <systemitem>libstoragemgmt</systemitem> vorliegt, müssen die LED-Leuchten ggf. mithilfe von Drittanbieterwerkzeugen angepasst werden. Diese Drittanbieterwerkzeuge werden von verschiedenen Hardwareherstellern angeboten. Einige gängige Hersteller und Werkzeuge:
  </para>

  <table>
   <title>Drittanbieter-Speicherwerkzeuge</title>
<?dbhtml table-width="50%" ?>


<?dbfo table-width="50%" ?>


   <tgroup cols="2">
    <thead>
     <row>
      <entry>Hersteller/Festplatten-Controller</entry>
      <entry>Werkzeug</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>HPE SmartArray</entry>
      <entry>hpssacli</entry>
     </row>
     <row>
      <entry>LSI MegaRAID</entry>
      <entry>storcli</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   SUSE Linux Enterprise Server umfasst außerdem das Paket <package>ledmon</package> und das Tool <command>ledctl</command>. Dieses Tool eignet sich auch für bestimmte Hardwareumgebungen mit Intel-Speichergehäusen. Die richtige Syntax für dieses Werkzeug lautet:
  </para>

<screen><prompt>root # </prompt> cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'
    fault:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'</screen>

  <para>
   Wenn Sie unterstützte Hardware nutzen und alle erforderlichen Drittanbieterwerkzeuge vorliegen, können die LEDs mit der folgenden Kommandosyntax auf dem Salt Master Node aktiviert oder deaktiviert werden:
  </para>

<screen>
<prompt>root # </prompt>salt-run disk_led.device <replaceable>NODE</replaceable> <replaceable>DISK</replaceable> <replaceable>fault|ident</replaceable> <replaceable>on|off</replaceable>
</screen>

  <para>
   Mit folgendem Kommando aktivieren oder deaktivieren Sie beispielsweise die LED-Erkennung oder Fehlerleuchten unter <filename>/dev/sdd</filename> auf dem OSD-Node <filename>srv16.ceph</filename>:
  </para>

<screen><prompt>root # </prompt>salt-run disk_led.device srv16.ceph sdd ident on
<prompt>root # </prompt>salt-run disk_led.device srv16.ceph sdd ident off
<prompt>root # </prompt>salt-run disk_led.device srv16.ceph sdd fault on
<prompt>root # </prompt>salt-run disk_led.device srv16.ceph sdd fault off</screen>

  <note>
   <title>Gerätebenennung</title>
   <para>
    Der Gerätename im Kommando <command>salt-run</command> muss mit dem von Salt erkannten Namen übereinstimmen. Mit folgendem Kommando rufen Sie diese Namen ab:
   </para>
<screen><prompt>root@master # </prompt>salt '<replaceable>minion_name</replaceable>' grains.get disks
</screen>
  </note>

  <para>
   In zahlreichen Umgebungen muss die Konfiguration <filename>/srv/pillar/ceph/disk_led.sls</filename> geändert werden, damit die LED-Leuchten für bestimmte Hardwareanforderungen angepasst werden können. Bei einfachen Änderungen reicht es, <command>lsmcli</command> durch ein anderes Werkzeug zu ersetzen oder die Kommandozeilenparameter anzupassen. Bei komplexen Änderungen kann ein externes Skript anstelle des Kommandos <filename>lsmcli</filename> aufgerufen werden. Sollen Änderungen an <filename>/srv/pillar/ceph/disk_led.sls</filename> vorgenommen werden, beachten Sie diese Schritte:
  </para>

  <procedure>
   <step>
    <para>
     Nehmen Sie die erforderlichen Änderungen an <filename>/srv/pillar/ceph/disk_led.sls</filename> auf dem Salt Master Node vor.
    </para>
   </step>
   <step>
    <para>
     Prüfen Sie, ob die Änderungen fehlerfrei in die Pillar-Daten übernommen wurden:
    </para>
<screen><prompt>root # </prompt>salt '<replaceable>SALT MASTER</replaceable>*' pillar.get disk_led</screen>
   </step>
   <step>
    <para>
     Aktualisieren Sie die Pillar-Daten auf allen Knoten mit:
    </para>
<screen><prompt>root # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
  </procedure>

  <para>
   Es ist möglich, mithilfe eines externen Skripts direkt auf Drittanbieterwerkzeuge zuzugreifen und damit die LED-Leuchten anzupassen. Die folgenden Beispiele zeigen, wie Sie <filename>/srv/pillar/ceph/disk_led.sls</filename> so anpassen, dass ein externes Skript unterstützt wird, sowie zwei Beispielskripte für HP- und LSI-Umgebungen.
  </para>

  <para>
   Bearbeitete <filename>/srv/pillar/ceph/disk_led.sls</filename>, mit der ein externes Skript aufgerufen wird:
  </para>

<screen><prompt>root # </prompt>cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off
    fault:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off</screen>

  <para>
   Beispielskript, mit dem die LED-Leuchten auf HP-Hardware mit den <systemitem>hpssacli</systemitem>-Dienstprogrammen zum Blinken gebracht werden:
  </para>

<screen><prompt>root # </prompt>cat /usr/local/bin/flash_led_hp.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

FOUND=0
MAX_CTRLS=10
MAX_DISKS=50

for i in $(seq 0 $MAX_CTRLS); do
  # Search for valid controllers
  if hpssacli ctrl slot=$i show summary &gt;/dev/null; then
    # Search all disks on the current controller
    for j in $(seq 0 $MAX_DISKS); do
      if hpssacli ctrl slot=$i ld $j show | grep -q $1; then
        FOUND=1
        echo "Found $1 on ctrl=$i, ld=$j. Turning LED $2."
        hpssacli ctrl slot=$i ld $j modify led=$2
        break;
      fi
    done
    [[ "$FOUND" = "1" ]] &amp;&amp; break
  fi
done</screen>

  <para>
   Beispielskript, mit dem die LED-Leuchten auf LSI-Hardware mit den <systemitem>storcli</systemitem>-Dienstprogrammen zum Blinken gebracht werden:
  </para>

<screen><prompt>root # </prompt>cat /usr/local/bin/flash_led_lsi.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

[[ "$2" = "on" ]] &amp;&amp; ACTION="start" || ACTION="stop"

# Determine serial number for the disk
SERIAL=$(lshw -class disk | grep -A2 $1 | grep serial | awk '{print $NF}')
if [ ! -z "$SERIAL" ]; then
  # Search for disk serial number across all controllers and enclosures
  DEVICE=$(/opt/MegaRAID/storcli/storcli64 /call/eall/sall show all | grep -B6 $SERIAL | grep Drive | awk '{print $2}')
  if [ ! -z "$DEVICE" ]; then
    echo "Found $1 on device $DEVICE. Turning LED $2."
    /opt/MegaRAID/storcli/storcli64 $DEVICE $ACTION locate
  else
    echo "Device not found!"
    exit -1
  fi
else
  echo "Disk serial number not found!"
  exit -1
fi</screen>
 </sect1>
</chapter>
