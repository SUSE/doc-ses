<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>RADOS-Blockgerät</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ein Block ist eine Folge von Byte, beispielsweise ein 4-MB-Datenblock. Blockbasierte Speicherschnittstellen werden am häufigsten zum Speichern von Daten auf rotierenden Medien wie Festplatten, CDs, Disketten verwendet. Angesichts der Omnipräsenz von Blockgeräteschnittstellen ist ein virtuelles Blockgerät für ein Massenspeichersystem wie Ceph hervorragend zur Interaktion geeignet.
 </para>
 <para>
  Ceph-Blockgeräte lassen die gemeinsame Nutzung physischer Ressourcen zu und ihre Größe kann geändert werden. Sie speichern Daten auf mehreren OSDs in einem Ceph Cluster verteilt. Ceph-Blockgeräte nutzen die RADOS-Funktionen wie Snapshotting, Reproduktion und Konsistenz. Ceph's RADOS Block Devices (RBD) interagieren mit OSDs über Kernel-Module oder die <systemitem>librbd</systemitem>-Bibliothek.
 </para>
 <figure>
  <title>RADOS-Protokoll</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Die Blockgeräte von Ceph sind sehr leistungsfähig und unbegrenzt auf Kernel-Module skalierbar. Sie unterstützen Virtualisierungslösungen wie QEMU oder Cloud-basierte Rechnersysteme wie OpenStack, die auf <systemitem class="library">libvirt</systemitem> basieren. Sie können Object Gateway, CephFS und RADOS Block Devices gleichzeitig am selben Cluster ausführen.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Kommandos für Blockgeräte</title>

  <para>
   Mit dem Kommando <command>rbd</command> werden Blockgeräte-Images erstellt, aufgelistet, intern geprüft und entfernt. Sie können es beispielsweise auch zum Klonen von Images, zum Erstellen von Snapshots, für ein Rollback eines Image zu einem Snapshot oder zum Anzeigen eines Snapshots verwenden.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Erstellen eines Blockgeräte-Images in einem reproduzierten Pool</title>
   <para>
    Bevor Sie ein Blockgerät in einen Client aufnehmen können, müssen Sie ein zugehöriges Image in einem vorhandenen Pool erstellen (siehe <xref linkend="ceph-pools"/>):
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Mit folgendem Kommando erstellen Sie beispielsweise das 1-GB-Image „myimage“, in dem Informationen aus dem Pool „mypool“ gespeichert werden:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Image-Größeneinheiten</title>
    <para>
     Wenn Sie die Abkürzung einer Größeneinheit („G“ oder „T“) auslassen, wird die Größe des Images in Megabyte angegeben. Mit „G“ oder „T“ nach der Zahl geben Sie Gigabyte oder Terabyte an.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Erstellen eines Blockgeräte-Images in einem Erasure Coded Pool</title>
   <para>
    Ab SUSE Enterprise Storage 5 ist es möglich, Daten eines Blockgeräte-Images direkt in Pools mit Löschcodierung (EC-Pools) zu speichern. Ein RADOS Block Device-Image besteht aus einem <emphasis>Data</emphasis>- und einem <emphasis>metadata</emphasis>-Teil. Sie können lediglich den „data“-Teil eines RADOS Block Device-Images in einem EC-Pool speichern. Die Flagge „overwrite“ des Pools muss auf <emphasis>true</emphasis> eingestellt sein, und dies ist nur möglich, wenn alle OSDs, auf denen der Pool gespeichert ist, mit BlueStore arbeiten.
   </para>
   <para>
    Es ist nicht möglich, den „metadata“-Teil des Images in einem EC-Pool zu speichern. Zum Speichern der Metadaten des Images müssen Sie den reproduzierten Pool mit der Option <option>--pool=</option> des Kommandos <command>rbd create</command> angeben.
   </para>
   <para>
    So erstellen Sie ein RBD-Image in einem soeben erstellten EC-Pool:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool create <replaceable>POOL_NAME</replaceable> 12 12 erasure
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> allow_ec_overwrites true

#Metadata will reside in pool "<replaceable>OTHER_POOL</replaceable>", and data in pool "<replaceable>POOL_NAME</replaceable>"
<prompt>cephadm@adm &gt; </prompt><command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable> --pool=<replaceable>OTHER_POOL</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Auflisten von Blockgeräte-Images</title>
   <para>
    Mit folgendem Kommando rufen Sie die Blockgeräte im Pool „mypool“ ab:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Abrufen von Image-Informationen</title>
   <para>
    Mit folgendem Kommando rufen Sie Informationen aus dem Image „myimage“ im Pool „mypool“ ab:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Ändern der Größe eines Blockgeräte-Image</title>
   <para>
    RADOS Block Device-Images werden schlank bereitgestellt, was bedeutet, dass Sie erst physischen Speicherplatz belegen, wenn Sie damit beginnen, Daten darin zu speichern. Ihre Kapazität ist jedoch auf den Wert beschränkt, den Sie mit der Option <option>--size</option> festlegen. Führen Sie folgendes Kommando aus, wenn Sie die maximale Größe des Image erhöhen (oder verringern) möchten:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephadm@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Entfernen eines Blockgeräte-Image</title>
   <para>
    Mit folgendem Kommando entfernen Sie ein Blockgerät, das dem Image „myimage“ im Pool „mypool“ entspricht:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Ein- und Aushängen</title>

  <para>
   Nach Erstellung eines RADOS Block Device können Sie es wie jedes andere Datenträgergerät nutzen: Sie können es formatieren, für den Dateiaustausch einhängen und danach wieder aushängen.
  </para>

  <procedure>
   <step>
    <para>
     Stellen Sie sicher, dass Ihr Ceph Cluster einen Pool mit dem Festplatten-Image enthält, das zugeordnet werden soll. Nehmen wir an, der Name des Pools lautet <literal>mypool</literal> und das Image ist <literal>myimage</literal>.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Ordnen Sie das Image einem neuen Blockgerät zu.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map --pool mypool myimage</screen>
    <tip>
     <title>Benutzername und -authentifizierung</title>
     <para>
      Geben Sie einen Benutzernamen mit <option>--id <replaceable>user-name</replaceable></option> an. Bei der <systemitem>cephx</systemitem>-Authentifizierung müssen Sie außerdem ein Geheimnis angeben. Es kann von einem Schlüsselbund stammen oder aus einer Datei, die das Geheimnis enthält:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
     <para>
      oder
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     Listen Sie alle zugeordneten Geräte auf:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    <para>
     Das Gerät, mit dem wir arbeiten möchten, heißt <filename>/dev/rbd0</filename>.
    </para>
    <tip>
     <title>RBD-Gerätepfad</title>
     <para>
      Statt <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename> können Sie <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename> als dauerhaften Gerätepfad verwenden. Beispiel:
     </para>
<screen>
/dev/rbd/mypool/myimage
</screen>
    </tip>
   </step>
   <step>
    <para>
     Erstellen Sie am Gerät namens <filename>/dev/rbd0</filename> ein XFS-Dateisystem.
    </para>
<screen><prompt>root # </prompt>mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   </step>
   <step>
    <para>
     Hängen Sie das Gerät ein und prüfen Sie, ob es korrekt eingehängt wurde. Ersetzen Sie <filename>/mnt</filename> durch Ihren Einhängepunkt.
    </para>
<screen><prompt>root # </prompt>mount /dev/rbd0 /mnt
<prompt>root # </prompt>mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
    <para>
     Nun können Sie Daten auf das und vom Gerät verschieben als wäre es ein lokales Verzeichnis.
    </para>
    <tip>
     <title>Vergrößern des RBD-Geräts</title>
     <para>
      Wenn sich herausstellt, dass die Größe des RBD-Geräts nicht mehr ausreicht, lässt es sich leicht vergrößern.
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Vergrößern Sie das RBD-Image, beispielsweise auf 10 GB
       </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
      </listitem>
      <listitem>
       <para>
        Erweitern Sie das Dateisystem, bis es die neue Größe des Geräts ausfüllt.
       </para>
<screen><prompt>root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
      </listitem>
     </orderedlist>
    </tip>
   </step>
   <step>
    <para>
     Wenn Sie Ihre Arbeit an dem Gerät beenden, können Sie dessen Zuordnung aufheben und das Gerät aushängen.
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd unmap /dev/rbd0
<prompt>root # </prompt>unmount /mnt
</screen>
   </step>
  </procedure>

  <tip>
   <title>Manuelles Einhängen und Aushängen</title>
   <para>
    Da die manuelle Zuordnung und das Einhängen von RBD-Images nach dem Start sowie das Aushängen und Aufheben der Zuordnung vor dem Herunterfahren sehr mühsam sein kann, wird ein Skript <command>rbdmap</command> und eine <systemitem class="daemon">systemd</systemitem>-Einheit zur Verfügung gestellt. Weitere Informationen finden Sie in <xref linkend="ceph-rbd-rbdmap"/>.
   </para>
  </tip>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title>rbdmap: Zuordnen von RBD-Geräten beim Booten</title>
   <para>
    Das Shell-Skript <command>rbdmap</command> automatisiert die Operationen <command>rbd map</command> und <command>rbd unmap</command> an einem oder mehreren RBD-Images. Obwohl Sie das Skript jederzeit manuell ausführen können, ist sein wichtigster Vorteil, es automatisch zuordnen und die RBD-Images beim Booten einhängen (und sie beim Herunterfahren aushängen und die Zuordnung aufheben) zu können. Dieser Vorgang wird vom Init-System ausgelöst. Zu diesem Zweck ist eine Datei für die <systemitem class="daemon">systemd</systemitem>-Einheit (<filename>rbdmap.service</filename>) im Paket <systemitem>ceph-common</systemitem> enthalten.
   </para>
   <para>
    Das Skript nimmt ein einzelnes Argument, entweder <option>map</option> oder <option>unmap</option>. In beiden Fällen analysiert das Skript eine Konfigurationsdatei. Die Standardeinstellung <filename>/etc/ceph/rbdmap</filename> kann mit der Umgebungsvariable <literal>RBDMAPFILE</literal> überschrieben werden. Jede Zeile der Konfigurationsdatei entspricht einem RBD-Image, das zugeordnet oder dessen Zuordnung aufgehoben werden soll.
   </para>
   <para>
    Die Konfigurationsdatei hat das folgende Format:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Pfad zu einem Image in einem Pool. Geben Sie diesen als <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable> an.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Eine optionale Liste der Parameter, die an das zugrundeliegende Kommando <command>rbd map</command> weitergegeben werden sollen. Diese Parameter und ihre Werte sollten als durch Komma getrennte Zeichenkette angegeben werden, wie zum Beispiel:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       Im Beispiel führt das Skript <command>rbdmap</command> folgendes Kommando aus:
      </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       Im folgenden Beispiel wird erläutert, wie Sie einen Benutzernamen und einen Schlüsselbund mit zugehörigem Geheimnis angeben:
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbdmap map mypool/myimage id=rbd_user,keyring=/etc/ceph/ceph.client.rbd.keyring
</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Wenn das Skript als <command>rbdmap map</command> ausgeführt wird, analysiert es die Konfigurationsdatei und versucht, für jedes angegebenen RBD-Image zunächst das Image zuzuordnen (mit dem Kommando <command>the rbd map</command>) und dann das Image einzuhängen.
   </para>
   <para>
    Wenn es als <command>rbdmap unmap</command> ausgeführt wird, werden die in der Konfigurationsdatei aufgelisteten Images ausgehängt und ihre Zuordnungen werden aufgehoben.
   </para>
   <para>
    <command>rbdmap unmap-all</command> versucht, alle aktuell zugeordneten RBD-Images auszuhängen und danach deren Zuordnungen aufzuheben, unabhängig davon, ob sie in der Konfigurationsdatei aufgelistet sind.
   </para>
   <para>
    Bei erfolgreicher Ausführung ordnet der Vorgang „rbd map“ das Image einem /dev/rbdX-Gerät zu. Zu diesem Zeitpunkt wird eine udev-Regel ausgelöst, um einen symbolischen Link zu einem Geräteanzeigenamen <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename> zu erstellen, der auf das reale zugeordnete Gerät zeigt.
   </para>
   <para>
    Damit das Einhängen und Aushängen erfolgreich ausgeführt wird, muss der Anzeigename des Geräts einen entsprechenden Eintrag in <filename>/etc/fstab</filename> haben. Geben Sie beim Schreiben von <filename>/etc/fstab</filename>-Einträgen für RBD-Images die Einhängeoption „noauto“ (oder „nofail“) an. Dadurch wird verhindert, dass das Init-System das Gerät zu früh einhängt, also noch bevor das betreffende Gerät überhaupt vorhanden ist, weil <filename>rbdmap.service</filename> normalerweise ziemlich spät in der Boot-Sequenz ausgelöst wird.
   </para>
   <para>
    Eine vollständige Liste der <command>rbd</command>-Optionen finden Sie  auf der <command>rbd</command>-Handbuchseite (<command>man 8 rbd</command>).
   </para>
   <para>
    Beispiele zur Anwendung von <command>rbdmap</command> finden Sie auf der <command>rbdmap</command>-Handbuchseite (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2>
   <title>Vergrößern des RBD-Geräts</title>
   <para>
    Wenn sich herausstellt, dass die Größe des RBD-Geräts nicht mehr ausreicht, lässt es sich leicht vergrößern.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Vergrößern Sie das RBD-Image, beispielsweise auf 10 GB
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Erweitern Sie das Dateisystem, bis es die neue Größe des Geräts ausfüllt.
     </para>
<screen><prompt>root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Aufnahmen</title>

  <para>
   Ein RBD-Image ist eine Snapshot eines RADOS Block Device-Image. Mit Snapshots behalten Sie den Verlauf des Zustands eines Image bei: Ceph unterstützt auch ein Snapshot Layering zum schnellen und einfachen Klonen von VM-Images. Ceph unterstützt Blockgeräte-Snapshots mit dem Kommando <command>rbd</command> sowie viele übergeordnete Schnittstellen wie QEMU, <systemitem>libvirt</systemitem>, OpenStack und CloudStack.
  </para>

  <note>
   <para>
    Halten Sie die Eingabe- und Ausgabeoperationen an und entfernen Sie alle ausstehenden Schreibvorgänge, bevor Sie einen Snapshot eines Images anfertigen. Wenn das Image ein Dateisystem enthält, muss sich das Dateisystem zum Zeitpunkt der Snapshot-Erstellung in einem konsistenten Zustand befinden.
   </para>
  </note>

  <sect2>
   <title>Hinweise zu Cephx</title>
   <para>
    Wenn <systemitem>cephx</systemitem> aktiviert ist, dann müssen Sie einen Benutzernamen oder eine ID und einen Pfad zum Schlüsselbund mit dem entsprechenden Schlüssel für den Benutzer angeben. Weitere Einzelheiten finden Sie unter <xref linkend="cha-storage-cephx"/>. Es ist auch möglich, die Umgebungsvariable <systemitem>CEPH_ARGS</systemitem> hinzuzufügen, um die erneute Eingabe der folgenden Parameter zu verhindern.
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Fügen Sie den Benutzer und das Geheimnis zur Umgebungsvariable <systemitem>CEPH_ARGS</systemitem> hinzu, damit Sie diese Informationen nicht jedes Mal neu eingeben müssen.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Allgemeine Informationen zu Snapshots</title>
   <para>
    Das folgende Verfahren zeigt, wie Snapshots mit dem Kommando <command>rbd</command> in der Kommandozeile erstellt, aufgelistet und entfernt werden.
   </para>
   <sect3>
    <title>Erstellen von Snapshots</title>
    <para>
     Geben Sie zum Erstellen eines Snapshots mit <command>rbd</command> die Option <option>snap create</option>, den Pool-Namen und den Image-Namen an.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephadm@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Auflisten von Snapshots</title>
    <para>
     Geben Sie zum Auflisten von Snapshots eines Image den Pool-Namen und den Image-Namen an.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephadm@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Snapshot Rollbacks</title>
    <para>
     Geben Sie zur Durchführung eines Rollbacks zu einem Snapshot mit <command>rbd</command> die Option <option>snap rollback</option>, den Pool-Namen, den Image-Namen und den Namen des Snapshots an.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephadm@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Bei einem Rollback eines Image zu einem Snapshot wird die aktuelle Version des Image mit den Daten aus einem Snapshot überschrieben. Ein Rollback dauert umso länger je größer das Image ist. Einen Snapshot zu <emphasis>klonen ist schneller</emphasis> als ein <emphasis>Rollback</emphasis> eines Image zu einem Snapshot durchzuführen und ist die bevorzugte Methode, wenn zu einem früheren Zustand zurückgekehrt werden soll.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Löschen eines Snapshots</title>
    <para>
     Geben Sie zum Löschen eines Snapshots mit <command>rbd</command> die Option <option>snap rm</option>, den Pool-Namen, den Image-Namen und den Benutzernamen an.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephadm@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Ceph OSDs löschen Daten asynchron. Daher wird beim Löschen eines Snapshots nicht sofort Festplattenspeicherplatz frei.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Entfernen aller Snapshots</title>
    <para>
     Geben Sie zum Entfernen aller Snapshots für eine Image mit <command>rbd</command> die Option <option>snap purge</option> und den Image-Namen an.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephadm@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Layering</title>
   <para>
    Ceph unterstützt die Möglichkeit zur Erstellung mehrerer COW(copy-on-write)-Klone eines Blockgeräte-Snapshots. Durch ein Snapshot Layering können Ceph-Blockgeräte-Clients Images sehr schnell erstellen. Beispielsweise könnten Sie ein Blockgeräte-Image mit einem darauf geschriebenen Linux VM und danach einen Snapshot von diesem Image erstellen, dann den Snapshot schützen und beliebig viele COW-Klone erstellen. Ein Snapshot ist schreibgeschützt. Daher vereinfacht ein Snapshot die Semantik und ermöglicht es, Klone schnell zu erstellen.
   </para>
   <note>
    <para>
     Die im folgenden Kommandozeilenbeispiel genannten Elemente „parent“ (übergeordnet) und „child“ (untergeordnet) beziehen sich auf einen Ceph-Blockgeräte-Snapshot (parent) und das entsprechende Image, das vom Snapshot geklont wurde (child).
    </para>
   </note>
   <para>
    Jedes geklonte Image (child) speichert einen Verweis auf das übergeordnete Image (parent), wodurch das geklonte Image den übergeordneten Snapshot (parent) öffnen und lesen kann.
   </para>
   <para>
    Ein COW-Klon eines Snapshots verhält sich exakt genauso wie jedes andere Ceph-Blockgeräte-Image. Geklonte Images können gelesen, geschrieben und geklont werden und ihre Größe lässt sich ändern. Geklonte Images haben keine besonderen Einschränkungen. Der COW-Klon eines Snapshots verweist jedoch auf den Snapshot. Daher <emphasis>müssen</emphasis> Sie den Snapshot schützen, bevor Sie ihn klonen.
   </para>
   <note>
    <title>Keine Unterstützung für <option>--image-format 1</option></title>
    <para>
     Sie können keine Snapshots von Images anfertigen, die mit der veralteten Option <command>rbd create --image-format 1</command> erstellt wurden. Ceph unterstützt lediglich das Klonen der standardmäßigen <emphasis>format 2</emphasis>-Images.
    </para>
   </note>
   <sect3>
    <title>Erste Schritte mit Layering</title>
    <para>
     Ceph-Blockgeräte-Layering ist ein einfacher Prozess. Sie benötigen ein Image. Sie müssen einen Snapshot vom Image erstellen. Sie müssen den Snapshot schützen. Nach Ausführung dieser Schritte beginnen Sie mit dem Klonen des Snapshots.
    </para>
    <para>
     Das geklonte Image verweist auf den übergeordneten Snapshot und enthält Pool-ID, Image-ID und Snapshot-ID. Durch die enthaltene Pool-ID können Snapshots von einem Pool zu Images in einem anderen Pool geklont werden.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image-Schablone</emphasis>: Bei einem üblichen Anwendungsfall für Blockgeräte-Layering wird ein Master Image erstellt und ein Snapshot, der als Schablone für Klone dient. Beispielsweise erstellt ein Benutzer ein Image für eine Linux-Distribution (zum Beispiel SUSE Linux Enterprise Server) und dann einen Snapshot dafür. Der Benutzer aktualisiert möglicherweise das Image regelmäßig und erstellt einen neuen Snapshot (zum Beispiel <command>zypper ref &amp;&amp; zypper patch</command> gefolgt von <command>rbd snap create</command>). So wie sich das Image weiterentwickelt, kann der Benutzer beliebige einzelne Snapshots klonen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Erweiterte Schablone</emphasis>: Bei einem anspruchsvolleren Anwendungsfall wird ein Schablonen-Image erweitert, das mehr Informationen als eine Basis-Image enthält. Beispielsweise könnte ein Benutzer ein Image (eine VM-Schablone) klonen und weitere Software installieren (beispielsweise eine Datenbank, ein Content Management-System oder ein Analysesystem) und dann einen Snapshot des erweiterten Image erstellen, das genauso wie das Basis-Image aktualisiert wird.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Schablonen-Pool</emphasis>: Eine Methode des Blockgeräte-Layerings ist die Erstellung eines Pools, der Master-Images enthält, die als Schablonen fungieren, sowie Snapshots dieser Schablonen. Sie könnten dann Nur-Lesen-Berechtigungen an Benutzer verteilen. Die Benutzer haben dadurch die Möglichkeit, die Snapshots zu klonen, dürfen jedoch nicht im Pool schreiben oder Vorgänge ausführen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image-Migration/Wiederherstellung</emphasis>: Eine Methode des Blockgeräte-Layerings ist die Migration oder Wiederherstellung von Daten von einem Pool in einen anderen Pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Schützen von Snapshots</title>
    <para>
     Klone greifen auf die übergeordneten Snapshots zu. Alle Klone würden zerstört werden, wenn ein Benutzer versehentlich den übergeordneten Snapshot löscht. Sie müssen den Snapshot schützen, bevor Sie ihn klonen, um Datenverlust zu verhindern.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephadm@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Geschützte Snapshots können nicht gelöscht werden.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Klonen von Snapshots</title>
    <para>
     Zum Klonen eines Snapshots müssen Sie den übergeordneten Pool, das Image, den Snapshot, den untergeordneten Pool und den Image-Namen angeben. Der Snapshot muss vor dem Klonen geschützt werden.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Ein Snapshot kann von einem Pool zu einem Image in einem anderen Pool geklont werden. Sie könnten beispielsweise schreibgeschützte Images und Snapshots als Schablonen in einem Pool beibehalten und beschreibbare Klone in einem anderen Pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Aufheben des Schutzes von Snapshots</title>
    <para>
     Vor dem Löschen eines Snapshots muss zunächst dessen Schutz aufgehoben werden. Außerdem dürfen Sie <emphasis>keine</emphasis> Snapshots löschen, die Verweise von Klonen enthalten. Sie müssen jeden Klon eines Snapshots vereinfachen, bevor Sie den Snapshot löschen.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephadm@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Auflisten der untergeordneten Klone von Snapshots</title>
    <para>
     Führen Sie zum Auflisten der untergeordneten Klone eines Snapshots folgendes Kommando aus:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephadm@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten">
    <title>Vereinfachen eines geklonten Image</title>
    <para>
     Geklonte Images behalten einen Verweis auf den übergeordneten Snapshot bei. Wenn Sie den Verweis vom untergeordneten Klon zum übergeordneten Snapshot entfernen, wird das Image tatsächlich „vereinfacht“, indem die Informationen vom Snapshot zum Klon kopiert werden. Die Vereinfachung eines Klons dauert umso länger je größer der Snapshot ist. Zum Löschen eines Snapshots müssen Sie zunächst die untergeordneten Images vereinfachen.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephadm@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Da ein vereinfachtes Image alle Informationen des Snapshots enthält, belegt ein vereinfachtes Image mehr Speicherplatz als ein Layering-Klon.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Spiegelung</title>

  <para>
   RBD-Images können asynchron zwischen zwei Ceph Clustern gespiegelt werden. Dazu wird die Funktion des RBD-Journaling-Image verwendet, um die absturzkonsistente Reproduktion zwischen Clustern sicherzustellen. Die Spiegelung wird pro Pool in Peer-Clustern konfiguriert. Sie kann so konfiguriert werden, dass alle Images in einem Pool oder nur eine bestimmte Teilmenge der Images automatisch gespiegelt werden. Die Spiegelung wird mit dem <command>rbd</command>-Kommando ausgeführt. Der <systemitem>rbd-mirror</systemitem> Daemon ist dafür zuständig, Image-Updates aus dem Remote-Peer-Cluster zu entnehmen und diese auf das Image im lokalen Cluster anzuwenden.
  </para>

  <note>
   <title>rbd-mirror Daemon</title>
   <para>
    Für die RBD-Spiegelung benötigen Sie zwei Ceph Cluster. Auf jedem von beiden muss der <systemitem>rbd-mirror</systemitem> Daemon ausgeführt werden.
   </para>
  </note>

  <important>
   <title>Export von RADOS Block Devices über iSCSI</title>
   <para>
    Sie können keine RBD-Geräte spiegeln, die über iSCSI mit einem kernelbasierten iSCSI-Gateway exportiert wurden.
   </para>
   <para>
    Weitere Informationen zu iSCSI finden Sie in <xref linkend="cha-ceph-iscsi"/>.
   </para>
  </important>

  <sect2 xml:id="rbd-mirror-daemon">
   <title>rbd-mirror Daemon</title>
   <para>
    Die beiden <systemitem>rbd-mirror</systemitem> Daemons sind dafür zuständig, Image-Protokolle am Remote-Peer-Cluster zu beobachten und die Protokollereignisse im Vergleich zum lokalen Cluster wiederzugeben. Die Journaling-Funktion für RBD-Images zeichnet alle Änderungen am Image in der Reihenfolge auf wie sie vorgenommen werden. Dadurch wird sichergestellt, dass ein absturzkonsistenter Spiegel des Remote-Image lokal verfügbar ist.
   </para>
   <para>
    Der <systemitem>rbd-mirror</systemitem>-Daemon ist im Paket
    <package>rbd-mirror</package> enthalten. Sie können das Paket auf OSD Nodes, Gateway Nodes und sogar auf dedizierten Nodes installieren. Die Installation des Pakets <package>rbd-mirror</package> auf dem Admin Node wird nicht empfohlen. Installieren, aktivieren und starten Sie <package>rbd-mirror</package>:
   </para>
<screen><prompt>root@minion &gt; </prompt>zypper install rbd-mirror
<prompt>root@minion &gt; </prompt>systemctl enable ceph-rbd-mirror@<replaceable>server_name</replaceable>.service
<prompt>root@minion &gt; </prompt>systemctl start ceph-rbd-mirror@<replaceable>server_name</replaceable>.service</screen>
   <important>
    <para>
     Jeder <systemitem>rbd-mirror</systemitem> Daemon muss gleichzeitig eine Verbindung zu beiden Clustern herstellen können.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Pool-Konfiguration</title>
   <para>
    Die folgenden Verfahren zeigen, wie einfache Verwaltungsaufgaben zum Konfigurieren der Spiegelung mit dem <command>rbd</command>-Kommando ausgeführt werden. Die Spiegelung wird pro Pool in den Ceph Clustern konfiguriert.
   </para>
   <para>
    Sie müssen die Schritte zur Pool-Konfiguration an beiden Peer Clustern ausführen. Bei diesen Verfahren wird der Einfachheit halber angenommen, dass von einem einzelnen Host aus auf zwei Cluster („local“ und „remote“) zugegriffen werden kann.
   </para>
   <para>
    Weitere detaillierte Informationen zum Herstellen einer Verbindung zu verschiedenen Ceph Clustern finden Sie auf der <command>rbd</command>-Handbuchseite (<command>man 8 rbd</command>).
   </para>
   <tip>
    <title>Mehrere Cluster</title>
    <para>
     Der Cluster-Name in den folgenden Beispielen entspricht einer Ceph-Konfigurationsdatei des selben Namens <filename>/etc/ceph/remote.conf</filename>.
    </para>
   </tip>
   <sect3>
    <title>Aktivieren der Spiegelung für einen Pool</title>
    <para>
     Geben Sie zum Aktivieren der Spiegelung in einem Pool das Unterkommando <command>mirror pool enable</command>, den Pool-Namen und den Spiegelungsmodus an. Der Spiegelungsmodus kann entweder „pool“ oder „image“ lauten:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Alle Images im Pool mit aktivierter Journaling-Funktion werden gespiegelt.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        Die Spiegelung muss auf jedem Image explizit aktiviert werden. Weitere Informationen finden Sie in <xref linkend="rbd-mirror-enable-image-mirroring"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephadm@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3>
    <title>Deaktivieren der Spiegelung</title>
    <para>
     Geben Sie zum Deaktivieren der Spiegelung in einem Pool das Unterkommando <command>mirror pool disable</command> und den Pool-Namen an. Wenn die Spiegelung auf diese Weise in einem Pool deaktiviert wird, dann wird sie auch in anderen Images (im Pool) deaktiviert, für die eine Spiegelung explizit aktiviert wurde.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephadm@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Hinzufügen des Cluster Peers</title>
    <para>
     Damit der <systemitem>rbd-mirror</systemitem> Daemon seinen Peer Cluster ermitteln kann, muss der Peer im Pool registriert sein. Geben Sie zum Hinzufügen des Peer Clusters für die Spiegelung das Unterkommando <command>mirror pool peer add</command>, den Pool-Namen und eine Cluster-Spezifikation an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool peer add <replaceable>POOL_NAME</replaceable> client.remote@remote
<prompt>cephadm@adm &gt; </prompt>rbd --cluster remote mirror pool peer add <replaceable>POOL_NAME</replaceable> client.local@local</screen>
   </sect3>
   <sect3>
    <title>Entfernen des Cluster Peers</title>
    <para>
     Geben Sie zum Entfernen eines Peer Clusters für die Spiegelung das Unterkommando <command>mirror pool peer remove</command>, den Pool-Namen und die Peer-UUID (verfügbar über das Kommando <command>rbd mirror pool info</command>) an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephadm@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Image-Konfiguration</title>
   <para>
    Im Gegensatz zur Pool-Konfiguration muss die Image-Konfiguration nur für einen einzigen für die Spiegelung vorgesehenen Peer Ceph Cluster durchgeführt werden.
   </para>
   <para>
    Gespiegelte RBD-Images werden entweder als <emphasis>primär</emphasis> oder <emphasis>nicht primär</emphasis> ausgewiesen. Dies ist eine Eigenschaft des Image und nicht des Pools. Als nicht primär ausgewiesene Images können nicht bearbeitet werden.
   </para>
   <para>
    Images werden automatisch zu primären Images hochgestuft, wenn die Spiegelung zuvor für ein Image aktiviert wird (entweder implizit mit dem Pool-Spiegelungsmodus „pool“ und durch Aktivieren der Journaling-Funktion für das Image oder explizit (Informationen hierzu finden Sie in <xref linkend="rbd-mirror-enable-image-mirroring"/>) mit dem <command>rbd</command>-Kommando).
   </para>
   <sect3>
    <title>Unterstützung für Image Journaling</title>
    <para>
     Bei der RBD-Spiegelung wird immer die RBD Journaling-Funktion verwendet, um sicherzustellen, dass das reproduzierte Image immer absturzkonsistent bleibt. Die Journaling-Funktion muss aktiviert werden, bevor ein Image zu einem Peer Cluster gespiegelt werden kann. Die Funktion kann bei der Image-Erstellung aktiviert werden, indem die Option <option>--image-feature exclusive-lock,journaling</option> für das <command>rbd</command>-Kommando angegeben wird.
    </para>
    <para>
     Alternativ kann die Journaling-Funktion für bereits vorhandene RBD-Images dynamisch aktiviert werden. Geben Sie zum Aktivieren des Journaling den Unterbefehl <command>feature enable</command>, den Pool-Namen, den Image-Namen und den Namen der Funktion an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Abhängigkeit der Option</title>
     <para>
      Die Funktion <option>journaling</option> hängt von der Funktion <option>exclusive-lock</option> ab. Wenn die Funktion <option>exclusive-lock</option> nicht bereits aktiviert ist, müssen Sie diese vor Aktivierung der Funktion <option>journaling</option> aktivieren.
     </para>
    </note>
    <warning>
     <title>Journaling an allen neuen Images</title>
     <para>
      Sie können das Journaling für alle neuen Images standardmäßig aktivieren. Ergänzen Sie hierzu die Option <option>rbd default features</option> in der Ceph-Konfigurationsdatei mit dem Wert <literal>journaling</literal>. Beispiel:
     </para>
<screen>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</screen>
     <para>
      Bevor Sie eine solche Änderung vornehmen, prüfen Sie sorgfältig, ob sich die Aktivierung des Journalings für alle neuen Images positiv auf die Implementierung auswirkt, da sie unter Umständen die Leistung beeinträchtigen kann.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Aktivieren der Image-Spiegelung</title>
    <para>
     Wenn eine Spiegelung im Modus „image“ konfiguriert ist, muss die Spiegelung für jedes Image im Pool explizit aktiviert werden. Geben Sie zum Aktivieren der Spiegelung für ein bestimmtes Image das Unterkommando <command>mirror image enable</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror image enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Deaktivieren der Image-Spiegelung</title>
    <para>
     Geben Sie zum Deaktivieren der Spiegelung für ein bestimmtes Image das Unterkommando <command>mirror image disable</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Hochstufen und Herabstufen eines Image</title>
    <para>
     In einem Failover-Szenario, in dem die primäre Bezeichnung zum Image im Peer Cluster verschoben werden muss, müssen Sie den Zugriff auf das primäre Image stoppen, das aktuelle primäre Image herabstufen, das neue primäre Image hochstufen und den Zugriff auf das Image am alternativen Cluster wieder aufnehmen.
    </para>
    <note>
     <title>Erzwungene Hochstufung</title>
     <para>
      Die Hochstufung wird mit der Option <option>--force</option> erzwungen. Die erzwungene Hochstufung ist erforderlich, wenn die Herabstufung nicht auf den Peer Cluster übertragen werden kann (beispielsweise im Fall eines Cluster-Fehlers oder Kommunikationsausfalls). Dies führt zu einem Split Brain-Szenario zwischen den beiden Peers und das Image wird nicht mehr synchronisiert bis ein Unterkommando <command>resync</command> ausgestellt wird.
     </para>
    </note>
    <para>
     Geben Sie zum Herabstufen eines bestimmten Image zu "nicht primär" das Unterkommando <command>mirror image demote</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Geben Sie zum Herabstufen aller primären Images in einem Pool zu "nicht primär" das Unterkommando <command>mirror pool demote</command> zusammen mit dem Pool-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Geben Sie zum Hochstufen eines bestimmten Image zu "primär" das Unterkommando <command>mirror image promote</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Geben Sie zum Hochstufen aller primären Images in einem Pool zu "primär" das Unterkommando <command>mirror pool promote</command> zusammen mit dem Pool-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Aufteilen der E/A-Last</title>
     <para>
      Da der Status „primär“ oder „nicht primär“ pro Image gilt, ist es möglich, die E/A-Last auf zwei Cluster aufzuteilen und dort ein Failover oder Failback durchzuführen.
     </para>
    </tip>
   </sect3>
   <sect3>
    <title>Erzwingen der erneuten Synchronisierung eines Image</title>
    <para>
     Wenn der <systemitem>rbd-mirror</systemitem>-Daemon ein Split Brain-Szenario erkennt, versucht er erst wieder, das betreffende Image zu spiegeln, wenn das Problem behoben ist. Um die Spiegelung für ein Image wieder aufzunehmen, müssen Sie zunächst das Image, das als veraltet ermittelt wurde, herabstufen und dann eine erneute Synchronisierung mit dem primären Image anfordern. Geben Sie zum Anfordern einer erneuten Synchronisierung des Image das Unterkommando <command>mirror image resync</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Spiegelstatus</title>
   <para>
    Der Reproduktionsstatus des Peer Clusters wird für jedes primäre gespiegelte Image gespeichert. Dieser Status wird mit den Unterkommandos <command>mirror image status</command> und <command>mirror pool status</command> abgerufen:
   </para>
   <para>
    Geben Sie zum Anfordern des Spiegel-Image-Status das Unterkommando <command>mirror image status</command> zusammen mit dem Pool- und Image-Namen an:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Geben Sie zum Anfordern einer Übersicht zum Spiegel-Pool-Status das Unterkommando <command>mirror pool status</command> zusammen mit dem Pool-Namen an:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Durch Hinzufügen der Option <option>--verbose</option> zum Unterkommando <command>mirror pool status</command> werden zusätzlich Statusdetails für jedes Spiegel-Image im Pool ausgegeben.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Cache-Einstellungen</title>

  <para>
   Die Implementierung des Benutzerbereichs auf dem Ceph-Blockgerät (<systemitem>librbd</systemitem>) kann den Linux-Seiten-Cache nicht nutzen. Sie umfasst daher ein eigenes Caching im Speicher. Das RBD-Caching läuft ähnlich ab wie das Festplatten-Caching. Wenn das Betriebssystem eine Sperre oder eine Verschiebungsanforderung sendet, werden alle kürzlich bearbeiteten („dirty“) Daten auf die OSDs geschrieben. Dies bedeutet, dass das Caching mit Zurückschreiben ebenso sicher ist wie eine wohldefinierte physische Festplatte mit einer VM, die ordnungsgemäß Verschiebungen sendet. Der Cache beruht auf einem <emphasis>Least Recently Used</emphasis>(LRU)-Algorithmus; im Zurückschreiben-Modus kann er nebeneinanderliegende Anforderungen zusammenführen und damit den Durchsatz erhöhen.
  </para>

  <para>
   Ceph unterstützt das Caching mit Zurückschreiben für RBD. Zum Aktivieren fügen Sie
  </para>

<screen>
[client]
...
rbd cache = true
</screen>

  <para>
   in den Abschnitt <literal>[client]</literal> der Datei <filename>ceph.conf</filename> ein. Standardmäßig nimmt <systemitem>librbd</systemitem> kein Caching vor. Schreib- und Lesevorgänge gehen direkt in den Speicher-Cluster und Schreibvorgänge werden nur dann zurückgegeben, wenn sich die Daten in allen Reproduktionen auf dem Datenträger befinden. Bei aktiviertem Caching werden Schreibvorgänge sofort zurückgegeben, außer die Menge der nicht verschobenen Daten (in Byte) ist höher als der Wert der Option <option>rbd cache max dirty</option>. Der Schreibvorgang löst in diesem Fall das Zurückschreiben aus und bleibt so lange gesperrt, bis eine ausreichende Datenmenge verschoben wurde.
  </para>

  <para>
   Ceph unterstützt das Caching mit Durchschreiben für RBD. Sie können die Größe des Caches festlegen und auch die Ziele und Einschränkungen vom Caching mit Zurückschreiben auf das Caching mit Durchschreiben umstellen. Zum Festlegen des Durchschreiben-Modus geben Sie Folgendes an:
  </para>

<screen>
rbd cache max dirty = 0
</screen>

  <para>
   Dies bedeutet, dass Schreibvorgänge nur dann zurückgegeben werden, wenn sich die Daten in allen Reproduktionen auf dem Datenträger befinden; Lesevorgänge können dagegen aus dem Cache stammen. Der Cache befindet sich im Speicher auf dem Client und jedes RBD-Image umfasst einen eigenen Cache. Da sich der Cache lokal auf dem Client befindet, ist keine Kohärenz gegeben, wenn andere auf das Image zugreifen. Bei aktiviertem Caching können GFS oder OCFS nicht zusätzlich zu RBD ausgeführt werden.
  </para>

  <para>
   Die <filename>ceph.conf</filename>-Dateieinstellungen für RBD müssen im Abschnitt <literal>[client]</literal> in der Konfigurationsdatei festgelegt werden. Einstellungen:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Caching für RADOS Block Device (RBD) aktivieren. Die Standardeinstellung ist „true“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      Größe des RBD-Caches (in Byte). Standardwert ist 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      Grenzwert für die Menge der kürzlich bearbeiteten („dirty“) Daten, bei dem der Cache das Zurückschreiben auslöst. <option>rbd cache max dirty</option> muss kleiner als <option>rbd cache size</option> sein. Beim Wert 0 wird das Caching mit Durchschreiben herangezogen. Standardwert ist 24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      Das kürzlich bearbeitete Ziel („dirty target“), bevor der Cache die ersten Daten in den Datenspeicher schreibt. Blockiert keine Schreibvorgänge in den Cache. Standardwert ist 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Zeitraum (in Sekunden), über den soeben bearbeitete Daten im Cache verbleiben, bevor das Zurückschreiben beginnt. Der Standardwert ist 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Der Vorgang beginnt im Durchschreiben-Modus und wechselt zum Zurückschreiben, sobald die erste Verschiebungsanforderung eingeht. Dies ist eine konservative, jedoch sichere Einstellung für den Fall, dass virtuelle Maschinen mit <systemitem>rbd</systemitem> zu alt sind, um Verschiebungen zu senden (z. B. der Virtio-Treiber in Linux vor Kernel 2.6.32). Die Standardeinstellung ist „true“.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS-Einstellungen</title>

  <para>
   „Quality of Service“ (QoS) bezeichnet im Allgemeinen verschiedene Methoden, mit denen der Datenverkehr priorisiert und Ressourcen reserviert werden. Dies ist insbesondere für Datenverkehr mit besonderen Anforderungen von Bedeutung.
  </para>

  <important>
   <title>Keine Unterstützung durch iSCSI</title>
   <para>
    Die folgenden QoS-Einstellungen werden ausschließlich durch die Benutzerbereichs-RBD-Implementierung <systemitem class="daemon">librbd</systemitem> verwendet, <emphasis>nicht</emphasis> jedoch von der <systemitem>kRBD</systemitem>-Implementierung. iSCSI greift auf <systemitem>kRBD</systemitem> zurück und nutzt daher nicht die QoS-Einstellungen. Für iSCSI können Sie jedoch QoS mit den standardmäßigen Kernel-Funktionen in der Kernel-Blockgeräteschicht konfigurieren.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die E/A-Operationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die E/A-Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die Leseoperationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die Schreiboperationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die gelesene Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die geschriebene Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für E/A-Operationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die E/A-Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für Leseoperationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für Schreiboperationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die gelesene Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die geschriebene Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      Minimaler Zeitplanimpuls (in Millisekunden) für QoS. Der Standardwert ist 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Read-Ahead-Einstellungen</title>

  <para>
   RADOS Block Device unterstützt das Read-Ahead/Prefetching zur Optimierung kleiner, sequenzieller Lesevorgänge. Bei einer virtuellen Maschine wird dies in der Regel durch das Gast-Betriebssystem abgewickelt, doch Bootloader geben unter Umständen keine effizienten Lesevorgänge aus. Das Read-Ahead wird automatisch deaktiviert, wenn das Caching aktiviert ist.
  </para>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Anzahl der sequenziellen Leseanforderungen, die zum Auslösen des Read-Ahead erforderlich sind. Der Standardwert ist 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Maximale Größe einer Read-Ahead-Anforderung. Beim Wert 0 ist das Read-Ahead deaktiviert. Der Standardwert ist 512 KB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Sobald die angegebene Datenmenge (in Byte) aus einem RBD-Image gelesen wurde, wird das Read-Ahead für dieses Image deaktiviert, bis es geschlossen wird. Damit kann das Gast-Betriebssystem das Read-Ahead beim Starten übernehmen. Beim Wert 0 bleibt das Read-Ahead aktiviert. Der Standardwert ist 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Erweiterte Funktionen</title>

  <para>
   RADOS Block Device unterstützt erweiterte Funktionen, die den Funktionsumfang von RBD-Images vergrößern. Sie können die Funktionen entweder in der Kommandozeile beim Erstellen eines RBD-Images angeben oder in der Ceph-Konfigurationsdatei mit der Option <option>rbd_default_features</option>.
  </para>

  <para>
   Die Werte für die Option <option>rbd_default_features</option> können auf zwei Arten angegeben werden:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     als Summe der internen Werte der Funktion. Jede Funktion besitzt einen eigenen internen Wert – „layering“ beispielsweise den Wert 1 und „fast-diff“ den Wert 16. Sollen diese beiden Funktionen standardmäßig aktiviert werden, geben Sie daher Folgendes an:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     als durch Komma getrennte Liste mit Funktionen. Das obige Beispiel sieht wie folgt aus:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Funktionen ohne Unterstützung durch iSCSI</title>
   <para>
    RBD-Images mit den folgenden Funktionen werden nicht durch iSCSI unterstützt: <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option>, <option>striping</option>
   </para>
  </note>

  <para>
   Liste der erweiterten RBD-Funktionen:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Mit dem Layering können Sie Elemente klonen.
     </para>
     <para>
      Der interne Wert ist gleich 1, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Das Striping verteilt Daten auf mehrere Objekte und bewirkt Parallelität für sequenzielle Lese-/Schreib-Workloads. Damit werden Engpässe durch einzelne Knoten bei großen oder stark ausgelasteten RADOS Block Devices verhindert.
     </para>
     <para>
      Der interne Wert ist gleich 2, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Wenn diese Option aktiviert ist, muss ein Client eine Sperre für ein Objekt erwirken, bevor ein Schreibvorgang ausgeführt werden kann. Aktivieren Sie die exklusive Sperre nur dann, wenn nur ein einzelner Client auf ein bestimmtes Image zugreift, nicht mehrere Clients gleichzeitig. Der interne Wert ist gleich 4. Die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Die Objektzuordnungsunterstützung ist abhängig von der Unterstützung der exklusiven Sperre. Blockgeräte werden per Thin Provisioning bereitgestellt; dies bedeutet, dass lediglich Daten auf diesen Geräten gespeichert werden, die tatsächlich vorhanden sind. Die Objektzuordnungsunterstützung hilft bei der Ermittlung, welche Objekte tatsächlich vorhanden sind (für welche Objekte also Daten auf einem Server gespeichert sind). Wird die Objektzuordnungsunterstützung aktiviert, beschleunigt dies die E/A-Operationen beim Klonen, Importieren und Exportieren eines kaum gefüllten Images sowie den Löschvorgang.
     </para>
     <para>
      Der interne Wert ist gleich 8, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Die Fast-diff-Unterstützung ist abhängig von der Objektzuordnungsunterstützung und der Unterstützung der exklusiven Sperre. Hiermit wird eine zusätzliche Eigenschaft in die Objektzuordnung aufgenommen, die die Erzeugung von Vergleichen zwischen Snapshots eines Images und der tatsächlichen Datennutzung eines Snapshots erheblich beschleunigt.
     </para>
     <para>
      Der interne Wert ist gleich 16, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      „deep-flatten“ sorgt dafür, dass <command>rbd flatten</command> (siehe <xref linkend="rbd-flatten"/>) für alle Snapshots eines Images und zusätzlich für das Image selbst ausgeführt werden kann. Ohne diese Option beruhen Snapshots eines Images weiterhin auf dem übergeordneten Image, sodass Sie das übergeordnete Image erst dann löschen können, wenn die Snapshots gelöscht wurden. Mit „deep-flatten“ wird die Abhängigkeit eines übergeordneten Elements von seinen Klonen aufgehoben, selbst wenn sie Snapshots umfassen.
     </para>
     <para>
      Der interne Wert ist gleich 32, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>Journal</option></term>
    <listitem>
     <para>
      Die Journaling-Unterstützung ist abhängig von der Unterstützung der exklusiven Sperre. Mit dem Journaling werden alle Änderungen an einem Image in der Reihenfolge festgehalten, in der sie vorgenommen wurden. Die RBD-Spiegelung (siehe <xref linkend="ceph-rbd-mirror"/>) reproduziert anhand des Journals ein absturzkonsistentes Image auf einem Remote-Cluster.
     </para>
     <para>
      Der interne Wert ist gleich 64, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>RBD-Zuordnung mit älteren Kernel-Clients</title>

  <para>
   Ältere Clients (z. B. SLE11 SP4) können RBD-Images unter Umständen nicht zuordnen, da ein mit SUSE Enterprise Storage 6 bereitgestellter Cluster bestimmte Funktionen erzwingt (sowohl auf Ebene der RBD-Images als auch auf RADOS-Ebene), die diese älteren Clients nicht unterstützen. In diesem Fall enthalten die OSD-Protokolle die folgenden oder ähnliche Meldungen:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Änderung der CRUSH-Zuordnung-Bucket-Typen löst einen massiven Ausgleich aus</title>
   <para>
    Sollen die CRUSH-Zuordnung-Bucket-Typen zwischen „straw“ und „straw2“ gewechselt werden, gehen Sie dabei wohlgeplant vor. Erwarten Sie erhebliche Auswirkungen auf die Cluster-Last, da eine Änderung des Bucket-Typs einen massiven Cluster-Ausgleich auslöst.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Deaktivieren Sie alle nicht unterstützten RBD-Image-Funktionen. Beispiel:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephadm@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Stellen Sie die CRUSH-Zuordnung-Bucket-Typen von „straw2“ auf „straw“ um:
    </para>
    <substeps>
     <step>
      <para>
       Speichern Sie die CRUSH-Zuordnung:
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Dekompilieren Sie die CRUSH-Zuordnung:
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Bearbeiten Sie die CRUSH-Zuordnung und ersetzen Sie „straw2“ durch „straw“.
      </para>
     </step>
     <step>
      <para>
       Kompilieren Sie die CRUSH-Zuordnung neu:
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Legen Sie die neue CRUSH-Zuordnung fest:
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
</chapter>
