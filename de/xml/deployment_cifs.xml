<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha.ses.cifs">

 <title>Exportieren eines CephFS mittels Samba</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>Bearbeiten</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>Ja</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  In diesem Abschnitt wird beschrieben, wie ein CephFS mittels Samba/CIFS-Freigabe exportiert wird. Samba-Freigaben können bei Windows* Clients verwendet werden.
 </para>
 <warning>
  <title>Technology Preview</title>
  <para>
   Ab SUSE Enterprise Storage 5 wird der Export von Samba-Freigaben als Technology Preview betrachtet und nicht unterstützt. 
  </para>
 </warning>
 <sect1 xml:id="sec.ses.cifs.example">
  <title>Installationsbeispiel</title>

  <para>
   Das Exportieren eines CephFS ist ein Technology Preview und wird nicht unterstützt. Zum Exportieren einer Samba-Freigabe müssen Sie Samba manuell in einem Cluster Node installieren und konfigurieren. Die Failover-Funktionalität kann mit CTDB und mit der SUSE Linux Enterprise High Availability Extension zur Verfügung gestellt werden.
  </para>

  <procedure>
   <step>
    <para>
     Vergewissern Sie sich, dass sich in Ihrem Cluster bereits ein funktionierendes CephFS befindet. Weitere Informationen finden Sie in <xref linkend="cha.ceph.as.cephfs"/>.
    </para>
   </step>
   <step>
    <para>
     Erstellen Sie einen für Samba Gateway spezifischen Schlüsselbund auf dem Salt Master und kopieren Sie ihn in den Samba Gateway Node:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     Ersetzen Sie <replaceable>SAMBA_NODE</replaceable> durch den Namen des Samba Gateway Node.
    </para>
   </step>
   <step>
    <para>
     Die folgenden Schritte werden im Samba Gateway Node ausgeführt. Installieren Sie den Samba Daemon im Samba Gateway Node:
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     Bearbeiten Sie die Datei <filename>/etc/samba/smb.conf</filename> und fügen Sie den folgenden Abschnitt hinzu:
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     Starten und aktivieren Sie den Samba Daemon:
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ses.cifs.ha">
  <title>Hochverfügbarkeitskonfiguration</title>

  <para>
   In diesem Abschnitt finden Sie ein Beispiel, wie eine Hochverfügbarkeitskonfiguration aus Samba-Servern mit zwei Nodes eingerichtet wird. Für die Einrichtung ist die SUSE Linux Enterprise High Availability Extension erforderlich. Die beiden Nodes werden <systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>) und <systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>) genannt.
  </para>

  <para>
   Detaillierte Informationen zu SUSE Linux Enterprise High Availability Extension finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <para>
   Außerdem können Clients über zwei virtuelle IP-Adressen nach dem Floating-IP-Prinzip eine Verbindung mit dem Service herstellen, und zwar unabhängig davon, auf welchem physischen Node er ausgeführt wird. <systemitem class="ipaddress">192.168.1.10</systemitem> wird für die Cluster-Verwaltung mit Hawk2 verwendet und <systemitem class="ipaddress">192.168.2.1</systemitem> exklusiv für die CIFS-Exporte. Dies erleichtert später die Anwendung von Sicherheitsbeschränkungen.
  </para>

  <para>
   Das folgende Verfahren beschreibt das Installationsbeispiel. Weitere Informationen finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
  </para>

  <procedure xml:id="proc.sec.ses.cifs.ha">
   <step>
    <para>
     Erstellen Sie einen für das Samba Gateway spezifischen Schlüsselbund auf dem Salt Master und kopieren Sie ihn in beide Nodes:
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     Bereiten Sie <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem> darauf vor, den Samba Service zu hosten:
    </para>
    <substeps>
     <step>
      <para>
       Vergewissern Sie sich, dass die folgenden Pakete installiert sind, bevor Sie fortfahren:
       <package>ctdb</package>, <package>tdb-tools</package> und
       <package>samba</package> (für smb- und nmb-Ressourcen erforderlich).
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       Vergewissern Sie sich, dass die Services <literal>ctdb</literal>, <literal>smb</literal> und <literal>nmb</literal> gestoppt und deaktiviert wurden:
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       Öffnen Sie Port <literal>4379</literal> Ihrer Firewall in allen Nodes. Dies ist erforderlich, damit CTDB mit anderen Cluster Nodes kommunizieren kann.
      </para>
     </step>
     <step>
      <para>
       Erstellen Sie ein Verzeichnis für die CTDB-Sperre auf dem freigegebenen Dateisystem:
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Erstellen Sie in <systemitem class="domainname">earth</systemitem> die Konfigurationsdateien für Samba. Diese werden später automatisch mit <systemitem class="domainname">mars</systemitem> synchronisiert.
    </para>
    <substeps>
     <step>
      <para>
       Fügen Sie in <filename>/etc/ctdb/nodes</filename> alle Nodes ein, die alle privaten IP-Adressen der einzelnen Nodes im Cluster enthalten:
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       Konfigurieren Sie Samba. Fügen Sie die folgenden Zeilen im Abschnitt <literal>[global]</literal> von <filename>/etc/samba/smb.conf</filename> hinzu. Wählen Sie den gewünschten Hostnamen anstelle von "CTDB-SERVER" (alle Nodes im Cluster werden sinnvollerweise als ein großer Node mit diesem Namen angezeigt):
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       Weitere Informationen zu <command>csync2</command> finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Installieren Sie den SUSE Linux Enterprise High Availability Cluster und führen Sie ein Bootstrap durch.
    </para>
    <substeps>
     <step>
      <para>
       Registrieren Sie die SUSE Linux Enterprise High Availability Extension in <systemitem class="domainname">earth</systemitem> und <systemitem class="domainname">mars</systemitem>.
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       Installieren Sie <package>ha-cluster-bootstrap</package> in beiden Nodes:
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       Initialisieren Sie den Cluster in <systemitem class="domainname">earth</systemitem>:
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       Lassen Sie <systemitem class="domainname">mars</systemitem> dem Cluster beitreten:
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Prüfen Sie den Status des Clusters. Sie sollten zwei Nodes sehen, die im Cluster hinzugefügt wurden:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     Führen Sie die folgenden Kommandos in <systemitem class="domainname">earth</systemitem> aus, um die CTDB-Ressource zu konfigurieren:
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     Die Binärdatei <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> in der Konfigurationsoption <literal>ctdb_recovery_lock</literal> enthält die Parameter <replaceable>CLUSTER_NAME</replaceable>
     <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable>
     <replaceable>CEPHX_USER</replaceable> in dieser Reihenfolge.
    </para>
   </step>
   <step>
    <para>
     Fügen Sie eine geclusterte IP-Adresse hinzu:
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     Wenn <literal>unique_clone_address</literal> auf <literal>true</literal> festgelegt ist, fügt der Ressourcenagent IPaddr2 eine Klon-ID zur angegebenen Adresse hinzu, was zu drei verschiedenen IP-Adressen führt. Diese werden normalerweise nicht benötigt, sind jedoch nützlich beim Lastausgleich. Weitere Informationen zu diesem Thema finden Sie unter <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>.
    </para>
   </step>
   <step>
    <para>
     Überprüfen Sie das Ergebnis:
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     Testen Sie es an einem Client-Rechner. Führen Sie auf einem Linux Client das folgende Kommando aus, um zu sehen, ob Sie Dateien vom und zum System kopieren können:
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
