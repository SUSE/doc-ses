<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha: Exportieren von Ceph-Daten über NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>Bearbeiten</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>Ja</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha ist ein NFS-Server (weitere Informationen hierzu finden Sie unter <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Sharing File Systems with NFS (Freigabe von Dateisystemen mit NFS)</link> ), der in einem Benutzeradressenbereich anstatt als Teil des Betriebssystem-Kernels ausgeführt wird. Mit NFS Ganesha binden Sie Ihren eigenen Speichermechanismus ein, wie zum Beispiel Ceph, und greifen von einem beliebigen NFS Client darauf zu.
 </para>
 <para>
  S3 Buckets werden pro Benutzer zu NFS exportiert, beispielsweise über den Pfad <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
 </para>
 <para>
  Ein CephFS wird standardmäßig über den Pfad <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename> exportiert.
 </para>
 <note>
  <title>NFS Ganesha-Leistung</title>
  <para>
   Angesichts des erhöhten Protokoll-Overheads und der zusätzlichen Latenz durch zusätzliche Netzwerk-Hops zwischen Client und Speicher kann ein Zugriff auf Ceph über einen NFS-Gateway die Anwendungsleistung im Vergleich zu nativen CephFS- oder Object Gateway-Clients erheblich senken.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Installation</title>

  <para>
   Die Installationsanweisungen finden Sie unter <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Konfiguration</title>

  <para>
   Eine Liste aller Parameter, die in der Konfigurationsdatei verfügbar sind, finden Sie unter:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> für Optionen zur Dateisystem-Abstraktionsschicht (File System Abstraction Layer, FSAL) zu CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> für FSAL-Optionen zum Object Gateway.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Dieser Abschnitt enthält Informationen, die Ihnen beim Konfigurieren des NFS Ganesha-Servers für den Export der Cluster-Daten über das Object Gateway und CephFS helfen.
  </para>

  <para>
   Die NFS Ganesha-Konfiguration setzt sich aus zwei Teilen zusammen: Servicekonfiguration und Exportkonfiguration. Die Servicekonfiguration wird durch <filename>/etc/ganesha/ganesha.conf</filename> gesteuert. Beachten Sie, dass Änderungen an dieser Datei überschrieben werden, wenn DeepSea-Phase 4 ausgeführt wird. Bearbeiten Sie zum dauerhaften Ändern der Einstellungen die Datei <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>, die sich am Salt Master befindet. Die Exportkonfiguration wird im Ceph Cluster als RADOS-Objekte gespeichert.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Servicekonfiguration</title>
   <para>
    Die Servicekonfiguration wird in <filename>/etc/ganesha/ganesha.conf</filename> gespeichert und steuert alle Einstellungen für NFS Ganesha-Daemons, u. a. den Speicherort der Exportkonfiguration im Ceph Cluster. Beachten Sie, dass Änderungen an dieser Datei überschrieben werden, wenn DeepSea-Phase 4 ausgeführt wird. Bearbeiten Sie zum dauerhaften Ändern der Einstellungen die Datei <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>, die sich am Salt Master befindet.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>Abschnitt RADOS_URLS</title>
    <para>
     Im Abschnitt <literal>RADOS_URLS</literal> wird der Ceph Cluster-Zugriff zum Auslesen der NFS Ganesha-Konfiguration aus RADOS-Objekten konfiguriert.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        Dateipfad der Ceph-Konfiguration.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        cephx-Benutzer-ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        RADOS-Objekt-URL, die auf Neustartbenachrichtigungen überwacht werden soll .
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>RGW-Abschnitt</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Zeigt auf die Datei <filename>ceph.conf</filename>. Bei der Bereitstellung mit DeepSea ist es nicht erforderlich, diesen Wert zu ändern.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        Der Name des Ceph Client-Benutzers. der von NFS Ganesha verwendet wird.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Name des Ceph Clusters. SUSE Enterprise Storage 6 unterstützt derzeit nur einen Cluster-Namen, nämlich standardmäßig <literal>ceph</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>RADOS-Objekt-URL</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     NFS Ganesha unterstützt das Auslesen der Konfiguration aus einem RADOS-Objekt. Mit der Anweisung <literal>%url</literal> kann eine RADOS-URL angegeben werden, die den Speicherort des RADOS-Objekts bezeichnet.
    </para>
    <para>
     Für eine RADOS-URL stehen zwei Formate zur Auswahl: <literal>rados://&lt;POOL&gt;/&lt;OBJECT&gt;</literal> oder <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</literal>. <literal>POOL</literal> bezeichnet hierbei den RADOS-Pool, in dem das Objekt gespeichert ist, <literal>NAMESPACE</literal> ist der Pool-Namespace, in dem das Objekt gespeichert ist, und <literal>OBJECT</literal> entspricht dem Objektnamen.
    </para>
    <para>
     Zur Unterstützung der NFS Ganesha-Verwaltungsfunktionen im Ceph Dashboard muss das RADOS-Objekt für die einzelnen Service-Daemons bestimmte Namenskonventionen 
einhalten. Der Name des Objekts muss als <literal>conf-<replaceable>MINION_ID</replaceable></literal> angegeben werden, wobei MINION_ID die Salt Minion-ID des Knotens bezeichnet, auf dem dieser Service ausgeführt wird.
    </para>
    <para>
     Diese URL wird in DeepSea ordnungsgemäß erzeugt und Sie müssen keine Änderungen vornehmen.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Ändern der standardmäßigen NFS Ganesha Ports</title>
    <para>
     NFS Ganesha verwendet standardmäßig Port 2049 für NFS und 875 für die Unterstützung von „rquota“. Die standardmäßigen Portnummern ändern Sie mit den Optionen <option>NFS_Port</option> und <option>RQUOTA_Port</option> im Abschnitt <literal>NFS_CORE_PARAM</literal>. Beispiel:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Exportkonfiguration</title>
   <para>
    Die Exportkonfiguration wird als RADOS-Objekte im Ceph Cluster gespeichert. Die einzelnen Exportblöcke werden jeweils in getrennten RADOS-Objekten mit dem Namen <literal>export-&lt;id&gt;</literal> gespeichert, wobei <literal>&lt;id&gt;</literal> mit dem Attribut <literal>Export_ID</literal> der Exportkonfiguration übereinstimmen muss. Die Exporte und die NFS Ganesha-Services werden mithilfe der <literal>conf-MINION_ID</literal>-Objekte verknüpft. Die einzelnen Serviceobjekte enthalten eine Liste mit RADOS-URLs für die verschiedenen Exporte, die mit dem betreffenden Service exportiert wurden. Ein Exportblock sieht wie folgt aus:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    Zum Erstellen des RADOS-Objekts für den obigen Exportblock muss zunächst der Exportblock-Code in einer Datei gespeichert werden. Anschließend können Sie den Inhalt der zuvor gespeicherten Datei mit dem RADOS CLI-Werkzeug in einem RADOS-Objekt speichern.
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    Weil sie das Exportobjekt gestellt haben, können Sie den Export mit einer Serviceinstanz verknüpfen. Ergänzen Sie hierzu das Serviceobjekt mit der entsprechenden RADOS-URL des Exportobjekts. In den folgenden Abschnitten erfahren Sie, wie Sie einen Exportblock konfigurieren.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Export-Hauptabschnitt</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Jeder Export benötigt eine eindeutige „Export_Id“ (obligatorisch).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Exportpfad im entsprechenden CephFS Pool (obligatorisch). Damit können Unterverzeichnisse vom CephFS exportiert werden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Pfad für das NFS-Exportziel (obligatorisch für NFSv4). Damit wird definiert, unter welchem NFS-Exportpfad die exportierten Daten verfügbar sind.
       </para>
       <para>
        Beispiel: Mit dem Wert <literal>/cephfs/</literal> und nach dem Ausführen von
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        Die CephFS-Daten sind im Verzeichnis <filename>/mnt/cephfs/</filename> am Client verfügbar.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        „RO“ für schreibgeschützten Zugriff, „RW“ für Schreib-/Lesezugriff und „None“ für „kein Zugriff“.
       </para>
       <tip>
        <title>Beschränkung des Zugriffs auf Clients</title>
        <para>
         Wenn Sie <literal>Access_Type = RW</literal> im Hauptabschnitt <literal>EXPORT</literal> beibehalten und den Zugriff auf einen bestimmten Client im Abschnitt <literal>CLIENT</literal> beschränken, können andere Clients dennoch eine Verbindung herstellen. Soll der Zugriff auf alle Clients deaktiviert und nur für bestimmte Clients aktiviert werden, geben Sie <literal>Access_Type = None</literal> im Abschnitt <literal>EXPORT</literal> an und legen Sie dann im Abschnitt <literal>CLIENT</literal> einen weniger restriktiven Zugriffsmodus für mindestens einen Client fest:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS Squash-Option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exportieren der Dateisystem-Abstraktionsschicht (File System Abstraction Layer, FSAL). Weitere Informationen hierzu finden Sie in <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>FSAL-Unterabschnitt</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Definiert, welches Back-End NFS Ganesha verwendet. Zulässige Werte sind <literal>CEPH</literal> für CephFS oder <literal>RGW</literal> für das Object Gateway. Je nach Auswahl muss eine <literal>role-mds</literal> oder <literal>role-rgw</literal> in der <filename>policy.cfg</filename> definiert werden.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Benutzerdefinierte NFS Ganesha-Rollen</title>

  <para>
   Sie können NFS Ganesha-Rollen für Cluster Nodes definieren. Diese Rollen werden dann den Nodes in der <filename>policy.cfg</filename> zugewiesen. Die Rollen ermöglichen Folgendes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Getrennte NFS Ganesha Nodes für den Zugriff auf das Object Gateway und CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     Zuweisen verschiedener Object Gateway-Benutzer zu NFS Ganesha Nodes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Mit verschiedenen Object Gateway-Benutzern können NFS Ganesha Nodes auf verschiedene S3 Buckets zugreifen. S3 Buckets werden für die Zugriffskontrolle verwendet. Hinweis: S3 Buckets dürfen nicht mit Ceph Buckets verwechselt werden, die in der CRUSH Map verwendet werden.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Verschiedene Object Gateway-Benutzer für NFS Ganesha</title>
   <para>
    Im folgenden Verfahrensbeispiel für den Salt Master sehen Sie, wie zwei NFS Ganesha-Rollen mit verschiedenen Object Gateway-Benutzern erstellt werden. In diesem Beispiel werden die Rollen <literal>gold</literal> und <literal>silver</literal> verwendet, für die DeepSea bereits Beispiele zu Konfigurationsdateien bereitstellt.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Öffnen Sie die Datei <filename>/srv/pillar/ceph/stack/global.yml</filename> mit einem Editor Ihrer Wahl. Erstellen Sie die Datei, falls noch nicht vorhanden.
     </para>
    </step>
    <step>
     <para>
      Die Datei muss folgende Zeilen enthalten:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      Diese Rollen werden später in der <filename>policy.cfg</filename> zugewiesen.
     </para>
    </step>
    <step>
     <para>
      Erstellen Sie eine Datei <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> und fügen Sie folgenden Inhalt hinzu:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Erstellen sie eine Datei <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> und fügen Sie folgenden Inhalt hinzu:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Nun müssen für jede Rolle Schablonen für die <filename>ganesha.conf</filename> erstellt werden. Die Originalschablone von DeepSea ist ein guter Anfang. Erstellen Sie zwei Kopien:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      Für die neuen Rollen sind Schlüsselbunde für den Zugriff auf den Cluster erforderlich. Kopieren Sie <filename>ganesha.j2</filename>, um den Zugriff zu gewähren:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Kopieren Sie den Schlüsselbund für das Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway benötigt auch die Konfiguration für die verschiedenen Rollen:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Weisen Sie die neu erstellten Rollen zu den Cluster Nodes in der Datei <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> zu:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Ersetzen Sie <replaceable>NODE1</replaceable> und <replaceable>NODE2</replaceable> durch die Namen der Nodes, denen Sie die Rollen zuweisen möchten.
     </para>
    </step>
    <step>
     <para>
      Führen Sie die DeepSea-Phasen 0 bis 4 aus.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Trennen der CephFS- und Object Gateway-FSAL</title>
   <para>
    Im folgenden Verfahrensbeispiel für den Salt Master sehen Sie, wie zwei neue verschiedene Rollen erstellt werden, die CephFS und Object Gateway verwenden:
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Öffnen Sie die Datei <filename>/srv/pillar/ceph/rgw.sls</filename> mit einem Editor Ihrer Wahl. Erstellen Sie die Datei, falls noch nicht vorhanden.
     </para>
    </step>
    <step>
     <para>
      Die Datei muss folgende Zeilen enthalten:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      Diese Rollen werden später in der <filename>policy.cfg</filename> zugewiesen.
     </para>
    </step>
    <step>
     <para>
      Nun müssen für jede Rolle Schablonen für die <filename>ganesha.conf</filename> erstellt werden. Die Originalschablone von DeepSea ist ein guter Anfang. Erstellen Sie zwei Kopien:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Bearbeiten Sie die Datei <filename>ganesha_rgw.conf.j2</filename> und entfernen Sie den Abschnitt:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Bearbeiten Sie die Datei <filename>ganesha_cfs.conf.j2</filename> und entfernen Sie den Abschnitt:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Für die neuen Rollen sind Schlüsselbunde für den Zugriff auf den Cluster erforderlich. Kopieren Sie <filename>ganesha.j2</filename>, um den Zugriff zu gewähren:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      Die Zeile <literal>caps mds = "allow *"</literal> kann aus der Datei <filename>ganesha_rgw.j2</filename> entfernt werden.
     </para>
    </step>
    <step>
     <para>
      Kopieren Sie den Schlüsselbund für das Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway benötigt die Konfiguration für die neue Rolle:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Weisen Sie die neu erstellten Rollen zu den Cluster Nodes in der Datei <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> zu:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Ersetzen Sie <replaceable>NODE1</replaceable> und <replaceable>NODE2</replaceable> durch die Namen der Nodes, denen Sie die Rollen zuweisen möchten.
     </para>
    </step>
    <step>
     <para>
      Führen Sie die DeepSea-Phasen 0 bis 4 aus.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Unterstützte Vorgänge</title>
   <para>
    Die RGW-NFS-Schnittstelle unterstützt die meisten Operationen für Dateien und Verzeichnisse, mit den folgenden Einschränkungen:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links (u. a. symbolische Links) werden nicht unterstützt.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS-Zugriffssteuerungslisten (ACLs) werden nicht unterstützt.</emphasis> Unix-Benutzer- und Gruppeneigentum/-berechtigungen <emphasis>werden</emphasis> unterstützt.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Verzeichnisse können nicht verschoben oder umbenannt werden.</emphasis> Sie <emphasis>können</emphasis> Dateien zwischen Verzeichnissen verschieben.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Es wird nur eine vollständige, sequenzielle Schreib-E/A unterstützt.</emphasis> Schreiboperationen werden daher als Uploads erzwungen. Zahlreiche typische E/A-Operationen, z. B. die Bearbeitung von Dateien im Speicherort, scheitern daher zwangsläufig, da sie nichtsequenzielle Speichervorgänge umfassen. Bestimmte Dateidienstprogramme schreiben scheinbar sequenziell (z. B. bestimmte Versionen von GNU-<command>tar</command>), scheitern jedoch aufgrund der seltenen nichtsequenziellen Speichervorgänge. Beim Einhängen per NFS kann im Allgemeinen durch synchrones Einhängen erzwungen werden, dass die sequenzielle E/A einer Anwendung sequenzielle Schreibvorgänge auf den NFS-Server durchführt (Option <option>-o sync</option>). NFS-Clients, die nicht synchron eingehängt werden können (z. B. Microsoft Windows*), können keine Dateien hochladen.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS-RGW unterstützt Schreib-/Leseoperationen lediglich für Blockgrößen unter 4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starten oder Neustarten von NFS Ganesha</title>

  <para>
   Führen Sie zum Aktivieren und Starten des NFS Ganesha Service folgende  Kommandos aus:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Starten Sie NFS Ganesha neu mit:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   Wenn NFS Ganesha gestartet oder neu gestartet wird, hat es eine Kulanzzeitüberschreitung von 90 Sekunden für NFS v4. Während des Kulanzzeitraums werden neue Anforderungen von Clients aktiv abgelehnt. Somit erfahren Clients möglicherweise eine Verlangsamung ihrer Anforderungen, wenn sich NFS im Kulanzzustand befindet.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Festlegen des Protokollierumfangs</title>

  <para>
   Sie ändern die standardmäßige Fehlersuchestufe <literal>NIV_EVENT</literal> durch Bearbeiten der Datei <filename>/etc/sysconfig/nfs-ganesha</filename>. Ersetzen Sie <literal>NIV_EVENT</literal> durch <literal>NIV_DEBUG</literal> oder <literal>NIV_FULL_DEBUG</literal>. Durch Erhöhen der Protokollausführlichkeit können in den Protokolldateien große Datenmengen produziert werden.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   Nach dem Ändern des Protokollierumfangs ist ein Neustart des Service erforderlich.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verifizieren der exportierten NFS-Freigabe</title>

  <para>
   In NFS v3 können Sie überprüfen, ob die NFS-Freigaben im NFS Ganesha Server Node exportiert werden:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Einhängen der exportierten NFS-Freigabe</title>

  <para>
   Führen Sie zum Einhängen der exportierten NFS-Freigabe (wie in <xref linkend="ceph-nfsganesha-config"/> konfiguriert) auf einem Client-Host folgendes Kommando aus:
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
