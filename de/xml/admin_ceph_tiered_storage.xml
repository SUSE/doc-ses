<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_tiered_storage.xml" version="5.0" xml:id="cha-ceph-tiered">

 <title>Cache Tiering</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>Bearbeiten</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>Ja</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Eine <emphasis>Cache-Schicht</emphasis> ist eine zusätzliche Speicherebene, die zwischen dem Client und dem Standardspeicher implementiert wird. Er wurde entwickelt, um den Zugriff auf Pools zu beschleunigen, die sich auf langsamen Festplatten oder in Erasure Coded Pools befinden.
 </para>
 <para>
  Normalerweise wird beim Cache Tiering (Aufteilung des Caches in mehrere Schichten) ein Pool von relativ schnellen Speichergeräten (zum Beispiel SSD-Laufwerken) erstellt, der als Cache-Schicht fungiert, sowie ein Hintergrund-Pool aus langsameren und günstigeren Geräten, die als Speicherschicht konfiguriert sind. Die Größe des Cache-Pools liegt in der Regel bei 10–20 % des Speicher-Pools.
 </para>
 <sect1>
  <title>Terminologie zur Speicherung in Schichten</title>

  <para>
   Cache Tiering erkennt zwei Arten von Pools: einen <emphasis>Cache Pool</emphasis> und einen <emphasis>Speicher-Pool</emphasis>.
  </para>

  <tip>
   <para>
    Allgemeine Informationen zu Pools finden Sie in <xref linkend="ceph-pools"/>.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>Speicher-Pool</term>
    <listitem>
     <para>
      Entweder ein standardmäßiger reproduzierter Pool, in dem mehrere Kopien eines Objekts im Ceph Storage Cluster gespeichert sind, oder ein Erasure Coded Pool (weitere Informationen hierzu finden Sie in <xref linkend="cha-ceph-erasure"/>).
     </para>
     <para>
      Der Speicher-Pool wird manchmal als „Backing“ (Hintergrundspeicher) oder als „Cold Storage“ bezeichnet.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Cache Pool</term>
    <listitem>
     <para>
      Ein standardmäßiger reproduzierter Pool, der auf einem relativ kleinen, doch schnellen Speichergerät gespeichert ist und über einen eigenen Regelsatz in einer CRUSH Map verfügt.
     </para>
     <para>
      Der Cache Pool wird auch als „Hot Storage“ bezeichnet.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-caution">
  <title>Zu berücksichtigende Aspekte</title>

  <para>
   Durch ein Cache Tiering wird die Cluster-Leistung bei bestimmten Workloads möglicherweise <emphasis>beeinträchtigt</emphasis>. Nachfolgend sehen Sie einige Aspekte, die Sie berücksichtigen sollten:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Workload-abhängig</emphasis>: Ob ein Cache die Leistung verbessert, hängt von seinem Workload ab. Da das Verschieben von Objekten in einen Cache oder aus einem Cache heraus mit Aufwand verbunden ist, kann es effizienter sein, wenn die meisten Anforderungen sich nur auf eine kleine Anzahl von Objekten beziehen. Der Cache Pool sollte groß genug sein, um den Arbeitssatz für Ihren Workload zu erfassen und Überlastung zu vermeiden.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Schwer zu vergleichen</emphasis>: Die meisten Leistungsvergleiche zeigen bei Cache Tiering eine ungenügende Leistung. Der Grund dafür besteht darin, dass sie eine große Anzahl von Objekten anfordern und es lange dauert bis der Cache "aufgewärmt" ist.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Möglicherweise schlechte Leistung</emphasis>: Bei Workloads, die nicht für ein Cache Tiering geeignet sind, ist die Leistung oft schlechter als bei einem normalen reproduzierten Pool ohne aktiviertem Cache Tiering.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis><systemitem>librados</systemitem>-Objektauflistung</emphasis>: Wenn Ihre Anwendung direkt mit <systemitem>librados</systemitem> arbeitet und auf Objektauflistung basiert, funktioniert ein Cache Tiering möglicherweise nicht wie erwartet. (Dies ist kein Problem bei Object Gateway, RBD oder CephFS.)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1>
  <title>Wofür ein Cache Tiering geeignet ist</title>

  <para>
   Ein Cache Tiering kann in folgenden Fällen nützlich sein:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Ihre Pools mit Löschcodierung sind in FileStore gespeichert und Sie müssen über RADOS Block Device darauf zugreifen. Weitere Informationen zu RBD finden Sie in <xref linkend="ceph-rbd"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ihre Pools mit Löschcodierung sind in FileStore gespeichert und Sie müssen über iSCSI darauf zugreifen. Weitere Informationen zu iSCSI finden Sie in <xref linkend="cha-ceph-iscsi"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Sie verfügen über relativ wenig Speicher mit hoher Leistung und über viel Speicher mit geringer Leistung und müssen schneller auf die gespeicherten Daten zugreifen.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-cachemodes">
  <title>Cache-Modi</title>

  <para>
   Der Cache Tiering Agent verarbeitet die Migration der Daten zwischen Cache-Schicht und Hintergrundspeicherschicht. Administratoren haben die Möglichkeit, den Ablauf dieser Migration zu konfigurieren. Dafür gibt es zwei Szenarien:
  </para>

  <variablelist>
   <varlistentry>
    <term>Zurückschreiben-Modus (Write-Back)</term>
    <listitem>
     <para>
      Im Zurückschreiben-Modus schreiben Ceph Clients Daten an die Cache-Schicht und erhalten von der Cache-Schicht eine Bestätigung zurück. Zu gegebener Zeit werden die an die Cache-Schicht geschriebenen Daten zur Speicherschicht migriert und dann aus der Cache-Schicht entfernt. Dem Konzept nach ist die Cache-Schicht „vor“ der Hintergrundspeicherschicht angesiedelt. Wenn ein Ceph Client Daten benötigt, die sich in der Speicherschicht befinden, dann migriert der Cache Tiering Agent die Daten zur Cache-Schicht beim Lesen. Anschließend werden die Daten zum Ceph Client gesendet. Danach kann der Ceph Client über die Cache-Schicht so lange E/A-Operationen durchführen bis die Daten inaktiv werden. Dies ist ideal für änderbare Daten wie bei Foto- oder Videobearbeitung, oder auch für Transaktionsdaten.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Nur-Lesen-Modus</term>
    <listitem>
     <para>
      Im Nur-Lesen-Modus schreiben Ceph Clients Daten direkt an die Hintergrundspeicherschicht. Beim Lesen kopiert Ceph die angeforderten Objekte von der Hintergrundspeicherschicht zur Cache-Schicht. Veraltete Daten werden entsprechend der definierten Richtlinie aus der Cache-Schicht entfernt. Diese Vorgehensweise ist ideal bei unveränderbaren Daten wie Präsentationsbilder oder Videos in einem sozialen Netzwerk, DNS-Daten oder Röntgenaufnahmen, weil das Lesen von einem Cache Pool, der eventuell veraltete Daten enthält, inkonsistente Ergebnisse ergibt. Verwenden Sie den Nur-Lesen-Modus nicht für veränderbare Daten.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>Erasure Coded Pool und Cache Tiering</title>

  <para>
   Pools mit Löschcodierung belegen mehr Ressourcen als reproduzierte Pools. Es wird empfohlen, eine Cache-Schicht vor dem Pool mit Löschcodierung zu erstellen, um diese Beschränkungen auszugleichen. Bei Verwendung von FileStore ist dies zwingend erforderlich.
  </para>

  <para>
   Wenn beispielsweise der Pool <quote>hot-storage</quote>zum Schnellspeichern angelegt ist, dann kann der <quote>ecpool</quote>, der in <xref linkend="cha-ceph-erasure-erasure-profiles"/> erstellt wurde, beschleunigt werden mit:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add ecpool hot-storage
<prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   Dadurch wird der Pool <quote>hot-storage</quote> als Schicht von „ecpool“ in den Zurückschreiben-Modus versetzt, sodass jeder Schreib- und Lesevorgang zu „ecpool“ tatsächlich den „hot storage“-Pool verwendet und von dessen Flexibilität und Geschwindigkeit profitiert.
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   Weitere Informationen zum Cache Tiering finden Sie in <xref linkend="cha-ceph-tiered"/>.
  </para>
 </sect1>
 <sect1 xml:id="ses-tiered-storage">
  <title>Einrichten eines Beispiels für einen Speicher mit mehreren Schichten</title>

  <para>
   In diesem Abschnitt wird die Vorgehensweise zur Einrichtung einer schnellen SSD Cache-Schicht (Hot Storage) vor einer Standardfestplatte (Cold Storage) veranschaulicht.
  </para>

  <tip>
   <para>
    Das folgende Beispiel dient nur zu Demonstrationszwecken. Es umfasst eine Einrichtung mit einem Root und einer Regel für den SSD-Part in einem einzelnen Ceph Node.
   </para>
   <para>
    In der Produktionsumgebung umfassen Cluster-Einstellungen normalerweise mehrere Root- und Regeleinträge für Hot Storage sowie gemischte Nodes mit SSDs und SATA-Festplatten.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Erstellen Sie die beiden zusätzlichen CRUSH-Regeln „replicated_ssd“ für die schnelle SSD-Caching-Geräteklasse und „replicated_hdd“ für die langsamere HDD-Geräteklasse:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_ssd default host ssd
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_hdd default host hdd
</screen>
   </step>
   <step>
    <para>
     Stellen Sie alle vorhandenen Pools auf die Regel „replicated_hdd“ um. Damit wird verhindert, dass Ceph Daten auf den soeben hinzugefügten SSD-Geräten gespeichert:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> crush_rule replicated_hdd
</screen>
   </step>
   <step>
    <para>
     Machen Sie den Rechner mit DeepSea zu einem Ceph Node. Installieren Sie die Software und konfigurieren Sie den Host-Rechner wie in <xref linkend="salt-adding-nodes"/> beschrieben. Nehmen wir dafür den Namen <replaceable>node-4</replaceable> an. Dieser Node benötigt 4 OSD-Festplatten.
    </para>
<screen>[...]
host node-4 {
   id -5  # do not change unnecessarily
   # weight 0.012
   alg straw
   hash 0  # rjenkins1
   item osd.6 weight 0.003
   item osd.7 weight 0.003
   item osd.8 weight 0.003
   item osd.9 weight 0.003
}
[...]</screen>
   </step>
   <step>
    <para>
     Bearbeiten Sie die CRUSH Map für den Hot Storage Pool, der den OSDs zugeordnet ist und über schnelle SSD-Laufwerke im Hintergrund verfügt. Definieren Sie eine zweite Hierarchie mit einem Root Node für die SSDs (als „root ssd“). Ändern Sie zusätzlich die Gewichtung und fügen Sie eine CRUSH-Regel für die SSDs hinzu. Weitere Informationen zur CRUSH-Zuordnung finden Sie in <xref linkend="op-crush"/>.
    </para>
    <para>
     Bearbeiten Sie die CRUSH-Zuordnung direkt mit Kommandozeilen-Werkzeugen wie <command>getcrushmap</command> und <command>crushtool</command>:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rm-device-class osd.6 osd.7 osd.8 osd.9
<prompt>cephadm@adm &gt; </prompt>ceph osd crush set-device-class ssd osd.6 osd.7 osd.8 osd.9
</screen>
   </step>
   <step>
    <para>
     Erstellen Sie den Hot Storage Pool für das Cache Tiering. Verwenden Sie dazu die neue „ssd“-Regel:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create hot-storage 100 100 replicated ssd</screen>
   </step>
   <step>
    <para>
     Erstellen Sie den Cold Storage Pool mit der standardmäßigen Regel "replicated_ruleset":
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</screen>
   </step>
   <step>
    <para>
     Dann wird zur Einrichtung einer Cache-Schicht ein Hintergrundspeicher-Pool mit einem Cache Pool verknüpft, in diesem Fall Cold Storage (= Speicher-Pool) mit Hot Storage (= Cache Pool):
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add cold-storage hot-storage</screen>
   </step>
   <step>
    <para>
     Führen Sie folgendes Kommando aus, um den Cache-Modus auf „writeback“ (Zurückschreiben) festzulegen:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback</screen>
    <para>
     Weitere Informationen zu den Cache-Modi finden Sie in <xref linkend="sec-ceph-tiered-cachemodes"/>.
    </para>
    <para>
     Cache-Schichten im Zurückschreiben-Modus überlagern die Hintergrundspeicherschicht. Daher muss als zusätzlicher Schritt der gesamte Datenverkehr des Clients vom Speicher-Pool zum Cache Pool geleitet werden. Führen Sie als Beispiel folgendes Kommando aus, um den Client-Datenverkehr direkt zum Cache Pool zu leiten:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay cold-storage hot-storage</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cache-tier-configure">
  <title>Konfigurieren einer Cache-Schicht</title>

  <para>
   Zum Konfigurieren von Cache-Schichten stehen verschiedene Optionen zur Verfügung. Verwenden Sie folgende Syntax:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>

  <sect2 xml:id="ses-tiered-hitset">
   <title>Treffersatz (Hit Set)</title>
   <para>
    <emphasis>Treffersatz</emphasis>-Parameter ermöglichen eine Optimierung der <emphasis>Cache Pools</emphasis>. Treffersätze in Ceph sind normalerweise Bloomfilter. Sie sind eine speichereffiziente Methode zum Nachverfolgen von Objekten, die sich bereits im Cache Pool befinden.
   </para>
   <para>
    Der Treffersatz ist ein Bit Array, mit dem das Ergebnis einer Reihe von Hashing-Funktionen gespeichert werden, die auf Objektnamen angewendet wurden. Anfänglich sind alle Bits auf <literal>0</literal> festgelegt. Wenn ein Objekt zum Treffersatz hinzugefügt wird, erhält dessen Name ein Hash und das Ergebnis wird an verschiedenen Positionen im Treffersatz zugeordnet. Dort wird der Wert des Bit dann auf <literal>1</literal> festgelegt.
   </para>
   <para>
    Dem Namen eines Objekts wird erneut ein Hash hinzugefügt, um herauszufinden, ob dieses Objekt im Cache vorhanden ist. Wenn ein Bit den Wert <literal>0</literal> aufweist, ist das Objekt definitiv nicht im Cache vorhanden und muss vom Cold Storage abgerufen werden.
   </para>
   <para>
    Es ist möglich, dass die Ergebnisse von verschiedenen Objekten am selben Standort wie der Treffersatz gespeichert sind. Durch Zufall können alle Bits den Wert <literal>1</literal> aufweisen und das Objekt ist trotzdem nicht im Cache vorhanden. Daher können nur Treffersätze mit einem Bloomfilter wirklich angeben, ob ein Objekt definitiv nicht im Cache vorhanden ist und daher vom Cold Storage abgerufen werden muss.
   </para>
   <para>
    In einem Cache Pool kann sich im Lauf der Zeit mehr als ein Treffersatz befinden, der den Dateizugriff verfolgt. Durch die Einstellung <literal>hit_set_count</literal> wird definiert, wie viele Treffersätze verwendet werden. Mit <literal>hit_set_period</literal> wird definiert, wie lange die einzelnen Treffersätze verwendet werden. Nach Ablauf des Zeitraums wird der nächste Treffersatz verwendet. Wenn die Anzahl der Treffersätze verbraucht ist, wird der Arbeitsspeicher des ältesten Treffersatzes freigegeben und ein neuer Treffersatz wird erstellt. Die Werte von <literal>hit_set_count</literal> und <literal>hit_set_period</literal> miteinander multipliziert definieren den Zeitrahmen insgesamt, in dem der Zugriff auf Objekte verfolgt wurde.
   </para>
   <figure xml:id="ses-tiered-hitset-overview-bloom">
    <title>Bloomfilter mit drei gespeicherten Objekten</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="bloom-filter.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="bloom-filter.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Verglichen mit der Anzahl der Objekte mit Hash ist ein Treffersatz mit Bloomfilter sehr speichereffizient. Es sind weniger als 10 Bits erforderlich, um die falschpositive Wahrscheinlichkeit auf unter 1 % zu reduzieren. Die falschpositive Wahrscheinlichkeit wird definiert mit <literal>hit_set_fpp</literal>. Ceph berechnet die Größe des Treffersatzes automatisch basierend auf der Anzahl der Objekte in einer Placement Group und der falschpositiven Wahrscheinlichkeit.
   </para>
   <para>
    Der erforderliche Speicherplatz im Cache Pool wird beschränkt mit <literal>min_write_recency_for_promote</literal> und <literal>min_read_recency_for_promote</literal>. Wird der Wert auf <literal>0</literal> festgelegt, werden alle Objekte in den Cache Pool hochgestuft, sobald sie gelesen oder geschrieben werden. Dies dauert so lange an, bis die Objekte entfernt werden. Werte größer <literal>0</literal> definieren die Anzahl der Treffersätze geordnet nach Alter, die nach dem Objekt durchsucht werden. Wird das Objekt in einem Treffersatz gefunden, so wird es zum Cache Pool hochgestuft. Denken Sie daran, dass Objekte beim Sichern eventuell auch zum Cache hochgestuft werden. Bei einer vollständigen Sicherung mit dem Wert „0“ werden möglicherweise alle Daten zur Cache-Schicht hochgestuft, während aktive Daten aus der Cache-Schicht entfernt werden. Unter Umständen ist daher zu empfehlen, diese Einstellung je nach Sicherungsstrategie zu ändern.
   </para>
   <note>
    <para>
     Je größer der Zeitraum und je höher die Werte <option>min_read_recency_for_promote</option> und <option>min_write_recency_for_promote</option>, desto mehr RAM verbraucht der <systemitem class="process">ceph-osd</systemitem>-Daemon. Alle <option>hit_set_count</option>-Treffersätze werden in den RAM geladen, wenn insbesondere der Agent aktiv ist, um Cache-Objekte zu verschieben oder zu entfernen.
    </para>
   </note>
   <sect3 xml:id="ceph-tier-gmt-hitset">
    <title>MGZ für Treffersatz</title>
    <para>
     Cache-Schicht-Einrichtungen enthalten einen Bloomfilter namens <emphasis>Treffersatz</emphasis>. Der Filter testet, ob ein Objekt zu einem Satz von Hot Storage-Objekten oder Cold Storage-Objekten gehört. Die Objekte werden dem Treffersatz mit Zeitstempeln hinzugefügt, die ihren Namen angehängt werden.
    </para>
    <para>
     Wenn Cluster-Rechner in verschiedenen Zeitzonen aufgestellt und die Zeitstempel von der lokalen Zeit abgeleitet werden, haben Objekte in einem Treffersatz möglicherweise irreführende Namen, die aus Zeitstempeln der Zukunft oder Vergangenheit bestehen. Im schlimmsten Fall sind Objekte gar nicht im Treffersatz vorhanden.
    </para>
    <para>
     Um dies zu verhindert, wird bei neu erstellten Cache-Schicht-Einrichtungen für <option>use_gmt_hitset</option> der Standardwert "1" festgelegt. Auf diese Weise erzwingen Sie, dass OSDs beim Erstellen der Objektnamen für den Treffersatz die MGZ-Zeitstempel (mittlere Greenwich-Zeit) verwenden.
    </para>
    <warning>
     <title>Standardwert belassen</title>
     <para>
      Ändern Sie nicht den Standardwert "1" für <option>use_gmt_hitset</option>. Wenn Fehler in Bezug auf diese Option nicht durch Ihre Cluster-Einrichtung verursacht werden, ändern Sie den Wert niemals manuell. Das Cluster-Verhalten wird andernfalls unvorhersehbar.
     </para>
    </warning>
   </sect3>
  </sect2>

  <sect2>
   <title>Cache-Größe</title>
   <para>
    Der Cache Tiering Agent übt zwei Hauptfunktionen aus:
   </para>
   <variablelist>
    <varlistentry>
     <term>Verschieben</term>
     <listitem>
      <para>
       Der Agent erkennt geänderte („dirty“ bzw. fehlerhafte) Objekte und leitet sie für die Langzeitspeicherung an den Speicher-Pool weiter.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Entfernen</term>
     <listitem>
      <para>
       Der Agent erkennt Objekte, die nicht bearbeitet wurden („clean“ bzw. intakt) und entfernt die ältesten davon aus dem Cache.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <sect3 xml:id="cache-tier-config-absizing">
    <title>Absolute Größe</title>
    <para>
     Der Cache Tiering Agent verschiebt oder entfernt Objekte basierend auf der Gesamtanzahl von Bytes oder der Gesamtanzahl von Objekten. Führen Sie folgendes Kommando aus, um die maximale Anzahl von Bytes anzugeben:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes <replaceable>num_of_bytes</replaceable></screen>
    <para>
     Führen Sie folgendes Kommando aus, um die maximale Anzahl von Objekten anzugeben:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_objects <replaceable>num_of_objects</replaceable></screen>
    <note>
     <para>
      Ceph kann nicht die Größe eines Cache Pools automatisch festlegen, daher muss die absolute Größe hier konfiguriert werden. Andernfalls funktioniert das Verschieben und Entfernen nicht. Wenn Sie beide Grenzwerte angeben, beginnt der Cache Tiering Agent mit dem Verschieben oder Entfernen, sobald einer der Schwellwerte ausgelöst wird.
     </para>
    </note>
    <note>
     <para>
      Alle Client-Anforderungen werden erst blockiert, wenn <option>target_max_bytes</option> oder <option>target_max_objects</option> erreicht ist.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="cache-tier-config-relsizing">
    <title>Relative Größe</title>
    <para>
     Der Cache Tiering Agent kann Objekte relativ zur Größe des Cache Pools (angegeben durch <option>target_max_bytes</option> oder <option>target_max_objects</option> in <xref linkend="cache-tier-config-absizing"/>) verschieben oder entfernen. Wenn der Cache Pool einen bestimmten Prozentsatz von bearbeiteten (fehlerhaften) Objekten aufweist, verschiebt der Cache Tiering Agent diese in den Speicher-Pool. Führen Sie zum Festlegen von <option>cache_target_dirty_ratio</option> folgendes Kommando aus:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_ratio <replaceable>0.0...1.0</replaceable></screen>
    <para>
     Beispielsweise wird bei Wert 0,4 mit dem Verschieben bearbeiteter (dirty) Objekte begonnen, sobald sie 40 % der Kapazität des Cache Pools erreichen:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</screen>
    <para>
     Wenn die fehlerhaften Objekte einen bestimmten Prozentsatz der Kapazität erreichen, können Sie mit höherer Geschwindigkeit verschoben werden. Verwenden Sie dazu <option>cache_target_dirty_high_ratio</option>:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_high_ratio <replaceable>0.0..1.0</replaceable></screen>
    <para>
     Erreicht der Cache Pool einen bestimmten Prozentsatz seiner Kapazität, entfernt der Cache Tiering Agent einige Objekte, um freie Speicherkapazität zu erhalten. Führen Sie zum Festlegen von <option>cache_target_full_ratio</option> folgendes Kommando aus:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_full_ratio <replaceable>0.0..1.0</replaceable></screen>
   </sect3>
  </sect2>

  <sect2>
   <title>Cache-Alter</title>
   <para>
    Geben Sie das Mindestalter an, das ein kürzlich bearbeitetes (dirty) Objekt erreichen muss, bevor der Cache Tiering Agent es in den Hintergrundspeicher-Pool verschiebt. Dies gilt nur dann, wenn tatsächlich Objekte aus dem Cache entfernt werden müssen:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_flush_age <replaceable>num_of_seconds</replaceable></screen>
   <para>
    Geben Sie das Mindestalter an, das ein Objekt erreichen muss, bevor es aus der Cache-Schicht entfernt wird:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_evict_age <replaceable>num_of_seconds</replaceable></screen>
  </sect2>

  <sect2 xml:id="ses-tiered-hitset-examples">
   <title>Beispiele</title>
   <sect3 xml:id="ses-tiered-hitset-examples-memory">
    <title>Großer Cache Pool und kleiner Arbeitsspeicher</title>
    <para>
     Wenn viel Speicherplatz, doch nur wenig RAM verfügbar ist, können alle Objekte sofort bei Zugriff in den Cache Pool hochgestuft werden. Der Treffersatz bleibt klein. Nachfolgend sehen Sie eine Reihe von Beispielen für Konfigurationswerte:
    </para>
<screen>hit_set_count = 1
hit_set_period = 3600
hit_set_fpp = 0.05
min_write_recency_for_promote = 0
min_read_recency_for_promote = 0</screen>
   </sect3>
   <sect3 xml:id="ses-tiered-hitset-examples-storage">
    <title>Kleiner Cache Pool und großer Arbeitsspeicher</title>
    <para>
     Bei einem kleinen Speicher und vergleichsweise großem Arbeitsspeicher kann die Cache-Schicht so konfiguriert werden, dass eine begrenzte Anzahl von Objekten in den Cache Pool hochgestuft wird. Zwölf Treffersätze, die jeweils für einen Zeitraum von 14.400 Sekunden verwendet werden, ermöglichen eine Statusüberwachung von insgesamt 48 Stunden. Wenn in den letzten acht Stunden auf ein Objekt zugegriffen wurde, wird es zum Cache Pool hochgestuft. Nachfolgend sehen Sie eine Reihe von Beispielen für Konfigurationswerte:
    </para>
<screen>hit_set_count = 12
hit_set_period = 14400
hit_set_fpp = 0.01
min_write_recency_for_promote = 2
min_read_recency_for_promote = 2</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
