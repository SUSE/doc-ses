<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Cluster-Dateisystem</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>Bearbeiten</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>Ja</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In diesem Kapitel werden Verwaltungsaufgaben beschrieben, die normalerweise nach der Einrichtung des Clusters und dem Export des CephFS ausgeführt werden. Weitere Informationen zum Einrichten des CephFS finden Sie im <xref linkend="cha-ceph-as-cephfs"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Einhängen des CephFS</title>

  <para>
   Wenn das Dateisystem erstellt und der MDS aktiv ist, sind Sie bereit, das Dateisystem von einem Client-Host aus einzuhängen.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Vorbereitung des Clients</title>
   <para>
    Wenn auf dem Client-Host SUSE Linux Enterprise 12 SP2 oder SP3 ausgeführt wird, können Sie diesen Abschnitt überspringen, weil das System bereit ist, das CephFS ohne weitere Anpassung einzuhängen. 
   </para>
   <para>
    Wenn auf dem Client-Host SUSE Linux Enterprise 12 SP1 ausgeführt wird, müssen Sie alle neuesten Patches anwenden, bevor Sie das CephFS einhängen.
   </para>
   <para>
    In jedem Fall ist in SUSE Linux Enterprise alles enthalten, was zum Einhängen des CephFS benötigt wird. SUSE Enterprise Storage 6 wird nicht benötigt.
   </para>
   <para>
    Zur Unterstützung der vollständigen <command>mount</command>-Syntax sollte das Paket
    <package>ceph-common</package> (das im Lieferumfang von SUSE Linux Enterprise enthalten ist) installiert werden, bevor Sie versuchen, das CephFS einzuhängen.
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Erstellen einer Geheimnisdatei</title>
   <para>
    Der Ceph Cluster wird mit standardmäßig eingeschalteter Authentifizierung ausgeführt. Sie sollten eine Datei erstellen, in der Ihr geheimer Schlüssel (nicht der Schlüsselbund selbst) gespeichert wird. Gehen Sie folgendermaßen vor, um den geheimen Schlüssel für einen bestimmten Benutzer abzurufen und dann die Datei zu erstellen:
   </para>
   <procedure>
    <title>Erstellen eines geheimen Schlüssels</title>
    <step>
     <para>
      Sehen Sie sich den Schlüssel für den bestimmten Benutzer in einer Schlüsselbunddatei an:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Kopieren Sie den Schlüssel des Benutzers, der das eingehängte Ceph FS-Dateisystem verwenden wird. Normalerweise sieht der Schlüssel in etwa so aus:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine Datei mit dem Benutzernamen als Teil des Dateinamens, wie zum Beispiel <filename>/etc/ceph/admin.secret</filename> für den Benutzer  <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Fügen Sie den Schlüsselwert in der im vorigen Schritt erstellten Datei ein.
     </para>
    </step>
    <step>
     <para>
      Legen Sie die ordnungsgemäßen Zugriffsrechte für die Datei fest. Ausschließlich der Benutzer sollte die Datei lesen können. Andere dürfen keine Zugriffsrechte dafür erhalten.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Einhängen des CephFS</title>
   <para>
    Das CephFS wird mit dem Befehl <command>mount</command> eingehängt. Sie müssen den Hostnamen oder die IP-Adresse des Monitors angeben. Da in SUSE Enterprise Storage standardmäßig die <systemitem>cephx</systemitem>-Authentifizierung aktiviert ist, müssen Sie einen Benutzernamen und das entsprechende Geheimnis angeben:
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Da das vorige Kommando im Shell-Verlauf bleibt, ist es sicherer, das Geheimnis aus einer Datei zu lesen:
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Beachten Sie, dass die Geheimnisdatei nur das tatsächliche Schlüsselbundgeheimnis enthalten sollte. In unserem Beispiel enthält die Datei dann nur die folgende Zeile:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>Mehrere Monitors angeben</title>
    <para>
     Es ist sinnvoll, in der <command>mount</command>-Kommandozeile mehrere Monitors durch Komma getrennt anzugeben, für den Fall, dass zum Zeitpunkt des Einhängens ein Monitor zufällig inaktiv ist. Jede Monitoradresse hat die Form <literal>host[:port]</literal>. Wenn der Port nicht angegeben wird, ist es standardmäßig Port 6789.
    </para>
   </tip>
   <para>
    Erstellen sie den Einhängepunkt am lokalen Host:
   </para>
<screen><prompt>root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Hängen Sie das CephFS ein:
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Ein Unterverzeichnis <filename>subdir</filename> kann angegeben werden, wenn eine Teilmenge des Dateisystems eingehängt werden soll:
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Sie können mehr als einen Monitor-Host im Kommando <command>mount</command> angeben:
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>Lesezugriff auf das Stammverzeichnis</title>
    <para>
     Wenn Clients mit Pfadbeschränkungen verwendet werden, muss der MDS Lesezugriff auf das Stammverzeichnis enthalten. Ein Schlüsselbund sieht beispielsweise in etwa wie folgt aus:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     Der Abschnitt <literal>allow r path=/</literal> bedeutet, dass pfadbeschränkte Clients das Root-Volume sehen können, jedoch nicht darauf schreiben dürfen. In Anwendungsfällen, in denen die vollständige Isolierung vorausgesetzt wird, könnte dies ein Problem darstellen.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Aushängen des CephFS</title>

  <para>
   Das CephFS wird mit dem Kommando <command>umount</command> ausgehängt:
  </para>

<screen><prompt>root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>CephFS in <filename>/etc/fstab</filename></title>

  <para>
   Fügen Sie zum automatischen Einhängen des CephFS bei Client-Start die entsprechende Zeile in die Tabelle <filename>/etc/fstab</filename> seines Dateisystems ein:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Mehrere aktive MDS-Daemons (Aktiv/Aktiv-MDS)</title>

  <para>
   CephFS wird standardmäßig für einen einzelnen aktiven MDS-Daemon konfiguriert. Zum Skalieren der Metadatenleistung für große Systeme können Sie mehrere aktive MDS-Daemons aktivieren. Dadurch wird der Metadaten-Workload untereinander aufgeteilt.
  </para>

  <sect2>
   <title>Anwendungsfall eines Aktiv/Aktiv-MDS</title>
   <para>
    Erwägen Sie, mehrere aktive MDS-Daemons zu verwenden, falls Ihre Metadatenleistung bei einem standardmäßigen einzelnen MDS einen Engpass erfahren würde.
   </para>
   <para>
    Das Hinzufügen weiterer Daemons erhöht nicht die Leistung bei allen Workload-Typen. Beispielsweise profitiert eine Einzelanwendung, die auf einem einzelnen Client ausgeführt wird, nicht von einer höheren Anzahl von MDS-Daemons, es sei denn, die Anwendung führt sehr viele Metadaten-Operationen gleichzeitig aus.
   </para>
   <para>
    Workloads, die normalerweise von einer höheren Anzahl aktiver MDS-Daemons profitieren, sind Workloads mit vielen Clients, die eventuell in vielen verschiedenen Verzeichnissen arbeiten.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Vergrößern des aktiven MDS Clusters</title>
   <para>
    Jedes CephFS-Dateisystem verfügt über eine <option>max_mds</option>-Einstellung, die steuert, wie viele Rangstufen erstellt werden. Die tatsächliche Anzahl der Rangstufen im Dateisystem wird nur dann erhöht, wenn ein Ersatz-Daemon verfügbar ist, der die neue Rangstufe übernehmen kann. Wenn beispielsweise nur ein MDS-Daemon ausgeführt wird und <option>max_mds</option> auf "2" festgelegt ist, wird keine zweite Rangstufe erstellt.
   </para>
   <para>
    Im folgenden Beispiel legen wir die Option <option>max_mds</option> auf "2" fest, um eine neue Rangstufe abgesehen von der standardmäßigen Rangstufe zu erstellen. Führen Sie zum Anzeigen der Änderungen <command>ceph status</command> aus bevor und nachdem Sie <option>max_mds</option> festlegen und beobachten Sie die Zeile, die <literal>fsmap</literal> enthält:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 2
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    Die neu erstellte Rangstufe (1) durchläuft den Zustand „wird erstellt“ und wird dann in den Zustand „aktiv“ versetzt.
   </para>
   <important>
    <title>Standby-Daemons</title>
    <para>
     Auch bei mehreren aktiven Daemons benötigt ein hochverfügbares System weiterhin Standby-Daemons, die übernehmen, wenn einer der Server, auf dem ein aktiver Daemon ausgeführt wird, ausfällt.
    </para>
    <para>
     Folglich ist die praktische maximale Anzahl von <option>max_mds</option> bei hochverfügbaren Systemen ein Server weniger als die Gesamtanzahl der MDS-Server in Ihrem System. Um im Fall mehrerer Serverausfälle verfügbar zu bleiben, müssen Sie die Anzahl der Standby-Daemons im System entsprechend der Anzahl der Serverausfälle anpassen, die sie kompensieren müssen.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Reduzieren der Anzahl von Rangstufen</title>
   <para>
    Alle Rangstufen, einschließlich der zu entfernenden Rangstufen, müssen zunächst aktiv sein. Dies bedeutet, dass mindestens <option>max_mds</option> MDS-Daemons verfügbar sein müssen.
   </para>
   <para>
    Legen Sie zunächst <option>max_mds</option> auf eine kleinere Anzahl fest. Gehen Sie beispielsweise zu einem einzelnen aktiven MDS zurück:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 1
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    Beachten Sie, dass wir immer noch zwei aktive MDS haben. Die Rangstufe ist immer noch vorhanden, obwohl wir <option>max_mds</option> reduziert haben, weil <option>max_mds</option> nur die Erstellung neuer Rangstufen beschränkt.
   </para>
   <para>
    Entfernen Sie als nächstes die nicht benötigten Rangstufe mit dem Kommando <command>ceph mds deactivate <replaceable>rank</replaceable></command>:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    Die deaktivierte Rangstufe erhält zunächst den Zustand „wird gestoppt“ und zwar für den Zeitraum, den er benötigt, um seinen Teil der Metadaten an die verbleibenden aktiven Daemons abzugeben. Diese Phase kann Sekunden, aber auch Minuten dauern. Falls der MDS im Zustand „wird gestoppt“ hängen bleibt, sollten Sie prüfen, ob es sich dabei um einen möglichen Fehler handelt.
   </para>
   <para>
    Wenn ein MDS-Daemon im Zustand „wird gestoppt“ abstürzt oder beendet wird, übernimmt ein Standby-Daemon und die Rangstufe wird wieder auf „aktiv“ gesetzt. Sie können versuchen, ihn erneut zu deaktivieren, falls er sich wieder erholt.
   </para>
   <para>
    Wenn ein Daemon den Stoppvorgang abgeschlossen hat, startet er erneut und ist wieder ein Standby-Daemon.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Manuelles Anheften von Verzeichnisbäumen an eine Rangstufe</title>
   <para>
    Bei Konfigurationen mit mehreren aktiven Metadaten-Servern wird ein Ausgleichsprogramm ausgeführt, das die Metadatenlast gleichmäßig im Cluster verteilt. Dies funktioniert normalerweise bei den meisten Benutzern ganz gut, doch manchmal ist es wünschenswert, das dynamische Ausgleichsprogramm durch explizite Zuordnungen von Metadaten zu bestimmten Rangstufen außer Kraft zu setzen. Dadurch erhalten der Administrator oder die Benutzer die Möglichkeit, die Anwendungslast gleichmäßig zu verteilen oder die Auswirkungen auf die Metadatenanforderungen der Benutzer auf den gesamten Cluster einzuschränken.
   </para>
   <para>
    Der zu diesem Zweck bereitgestellte Mechanismus wird "Export-Pin" genannt. Es handelt sich um ein erweitertes Attribut von Verzeichnissen. Der Name dieses erweiterten Attributs ist <literal>ceph.dir.pin</literal>. Benutzer können dieses Attribut mit Standardkommandos festlegen:
   </para>
<screen><prompt>root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    Der Wert (<option>-v</option>) des erweiterten Attributs ist die Rangstufe, zu dem der Verzeichnisunterbaum zugewiesen wird. Ein Standardwert von -1 gibt an, dass das Verzeichnis nicht angeheftet wird.
   </para>
   <para>
    Ein Verzeichnisexport-Pin wird vom nächstgelegenen übergeordneten Verzeichnis mit einem festgelegten Export-Pin übernommen. Daher betrifft die Festlegung eines Export-Pins auch alle untergeordneten Verzeichnisse. Der Pin des übergeordneten Verzeichnisses kann jedoch durch Festlegen des Verzeichnisexport-Pins des untergeordneten Verzeichnisses außer Kraft gesetzt werden. Beispiel:
   </para>
<screen><prompt>root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Failover-Verwaltung</title>

  <para>
   Wenn ein MDS-Daemon die Kommunikation mit dem Monitor stoppt, dann wartet der Monitor <option>mds_beacon_grace</option> Sekunden (standardmäßig 15 Sekunden), bevor er den Daemon als <emphasis>laggy</emphasis> (langsam) kennzeichnet. Sie können einen oder mehrere „standby“-Daemons konfigurieren, die bei einem MDS-Daemon-Failover übernehmen.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Konfigurieren von Standby-Daemons</title>
   <para>
    Mit verschiedenen Konfigurationseinstellungen wird das Verhalten des Daemons im Standby-Modus gesteuert. Sie werden in der Datei <filename>ceph.conf</filename> am Host angegeben, auf dem der MDS-Daemon ausgeführt wird. Der Daemon lädt diese Einstellungen beim Starten und sendet sie zum Monitor.
   </para>
   <para>
    Wenn keine dieser Einstellungen verwendet wird, werden alle MDS-Daemons ohne Rangstufe standardmäßig als „Standbys“ für alle Rangstufen verwendet.
   </para>
   <para>
    Die Einstellungen, durch die ein Standby-Daemon einem bestimmten Namen oder einer bestimmten Rangstufe zugeordnet wird, garantieren nicht, dass dieser Daemon für nur eine Rangstufe verwendet wird. Es bedeutet nur, dass der zugeordnete Standby-Daemon verwendet wird, wenn mehrere Standbys verfügbar sind. Wenn eine Rangstufe ausfällt und ein Standby verfügbar ist, wird er verwendet, auch wenn er einer anderen Rangstufe oder einem benannten Daemon zugeordnet ist.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       Wenn diese Option auf „true“ festgelegt ist, liest der Standby-Daemon kontinuierlich das Metadatenjournal einer aktiven Rangstufe. Dadurch erhält er einen betriebsbereiten Metadaten-Cache und der Vorgang des Failovers wird beschleunigt, wenn der Daemon, der die Rangstufe bedient, ausfällt.
      </para>
      <para>
       Einer aktiven Rangstufe ist möglicherweise nur ein Standby Replay-Daemon zugewiesen. Wenn beide Daemons als Standby Replay festgelegt sind, wird willkürlich einer der beiden herangezogen und der andere wird ein normaler Nicht-Replay Standby.
      </para>
      <para>
       Wenn ein Daemon den Status Standby Replay hat, wird er nur als Standby für die Rangstufe verwendet, der er folgt. Wenn eine andere Rangstufe ausfällt, wird dieser Standby Replay-Daemon nicht als Ersatz verwendet, auch wenn keine anderen Standbys verfügbar sind.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Legen Sie diese Option fest, wenn der Standby-Daemon nur eine ausgefallen Rangstufe übernehmen soll, wenn der letzte Daemon, der diese enthalten hat, mit diesem Namen übereinstimmt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Legen Sie diese Option fest, wenn der Standby-Daemon nur die angegebene Rangstufe übernehmen soll. Wenn eine andere Rangstufe ausfällt, wird dieser Daemon nicht als Ersatz dafür verwendet.
      </para>
      <para>
       Verwenden Sie diese Option zusammen mit <option>mds_standby_for_fscid</option>, um genau anzugeben, in welchem Dateisystem die Rangstufe sich befindet, falls mehrere Dateisysteme vorhanden sind.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       Wenn <option>mds_standby_for_rank</option> festgelegt ist, wird damit nur angegebenen, auf die Rangstufe von welchem Dateisystem Bezug genommen wird.
      </para>
      <para>
       Wenn <option>mds_standby_for_rank</option> nicht festgelegt ist, greift dieser Daemon durch Festlegen der FSCID auf jede Rangstufe in der angegebenen FSCID zu. Verwenden Sie diese Option, wenn Sie über einen Daemon verfügen, den Sie für beliebige Rangstufen verwenden möchten, allerdings nur innerhalb eines Dateisystems.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       Diese Einstellung wird für Monitor-Hosts verwendet. Die Standardeinstellung ist „true“.
      </para>
      <para>
       Wenn sie auf „false“ festgelegt ist, werden Daemons, die mit <option>standby_replay=true</option> konfiguriert sind, nur aktiv, wenn die Rangstufe/der Name, der sie laut Konfiguration folgen sollen, ausfällt. Andererseits wird einem Daemon, der mit <option>standby_replay=true</option> konfiguriert ist, möglicherweise eine andere Rangstufe zugewiesen, wenn diese Einstellung auf „true“ festgelegt ist.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>Beispiele</title>
   <para>
    Nachfolgend sehen Sie einige Beispiele zur <filename>ceph.conf</filename>-Konfiguration. Sie können entweder eine <filename>ceph.conf</filename> mit der Konfiguration aller Daemons zu allen Ihren Servern kopieren oder Sie können eine andere Datei auf jedem Server erstellen, die die Daemon-Konfiguration dieses Servers enthält.
   </para>
   <sect3>
    <title>Einfaches Paar</title>
    <para>
     Zwei MDS-Daemons „a“ und „b“, die als Paar fungieren. Der Daemon, dem aktuell keine Rangstufe zugewiesen ist, wird zum Standby Replay des anderen.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Festlegen von CephFS-Kontingenten</title>

  <para>
   Sie können Kontingente für beliebige Unterverzeichnisse des Ceph-Dateisystems anlegen. Das Kontingent beschränkt entweder die Anzahl der <emphasis role="bold">Byte</emphasis> oder die Anzahl der <emphasis role="bold">Dateien</emphasis>, die unterhalb des angegebenen Punkts in der Verzeichnishierarchie gespeichert werden können.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Einschränkungen</title>
   <para>
    Kontingente in CephFS unterliegen folgenden Einschränkungen:
   </para>
   <variablelist>
    <varlistentry>
     <term>Kontingente sind kooperativ und konkurrieren nicht untereinander.</term>
     <listitem>
      <para>
       Ceph-Kontingente sind darauf angewiesen, dass der Client, der das Dateisystem einhängt, nicht mehr darin schreibt, sobald ein bestimmter Grenzwert erreicht ist. Der Server-Teil kann nicht verhindern, dass ein böswilliger Client beliebig viele Daten schreibt. Es ist nicht zulässig, das Füllen des Dateisystems in Umgebungen, in denen die Clients als überhaupt nicht verbürgt gelten, mithilfe von Kontingenten zu verhindern.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Kontingente sind nicht absolut genau.</term>
     <listitem>
      <para>
       Prozesse, die in das Dateisystem schreiben, werden kurz nach Erreichen des Kontingentgrenzwerts angehalten. Es ist nicht zu verhindern, dass eine gewisse Datenmenge geschrieben wird, die den konfigurierten Grenzwert übersteigt. Client-Schreibvorgänge werden innerhalb weniger Zehntelsekunden nach dem Überschreiten des konfigurierten Grenzwerts angehalten.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Kontingente werden ab Version 4.17 im Kernel-Client implementiert.</term>
     <listitem>
      <para>
       Kontingente werden durch den Benutzerbereichs-Client (libcephfs, ceph-fuse) unterstützt. Die Linux-Kernel-Clients 4.17 und höher unterstützen CephFS-Kontingente auf Clustern mit SUSE Enterprise Storage 6. Kernel-Clients (selbst die neueren Versionen) können Kontingente nicht auf älteren Clustern verarbeiten, auch wenn sie die erweiterten Attribute der Kontingente festlegen können.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Konfigurieren Sie die Kontingente nur mit Vorsicht, wenn pfadbasierte Einhängeeinschränkungen gelten.</term>
     <listitem>
      <para>
       Der Client kann die Kontingente nur dann durchsetzen, wenn er den Zugriff auf den Verzeichnis-Inode besitzt, auf dem die Kontingente konfiguriert sind. Wenn der Zugriff des Clients auf einen bestimmten Pfad (z. B. <filename>/home/user</filename>) gemäß der MDS-Capability eingeschränkt ist und Sie ein Kontingent für ein übergeordnetes Verzeichnis konfigurieren, auf das der Client nicht zugreifen kann (<filename>/home</filename>), kann der Client das Kontingent nicht erzwingen. Bei pfadbasierten Zugriffseinschränkungen muss das Kontingent für das Verzeichnis konfiguriert werden, auf das der Client zugreifen kann (<filename>/home/user</filename>), oder auf ein Verzeichnis unter diesem Verzeichnis.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Konfiguration</title>
   <para>
    CephFS-Kontingente werden mit virtuellen erweiterten Attributen konfiguriert:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Konfiguriert einen Grenzwert für die Anzahl der <emphasis>Dateien</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Konfiguriert einen Grenzwert für die Anzahl der <emphasis>Bytes</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Wenn Attribute für einen Verzeichnis-Inode angezeigt werden, ist dort ein Kontingent konfiguriert. Fehlen die Attribute, wurde kein Kontingent für das Verzeichnis festgelegt (eventuell jedoch für ein übergeordnetes Verzeichnis).
   </para>
   <para>
    Mit folgendem Kommando legen Sie ein Kontingent von 100 MB fest:
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Mit folgendem Kommando legen Sie ein Kontingent von 10.000 Dateien fest:
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Mit folgendem Kommando rufen Sie die Kontingenteinstellung ab:
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>Kein Kontingent festgelegt</title>
    <para>
     Wenn das erweiterte Attribut den Wert „0“ aufweist, ist kein Kontingent festgelegt.
    </para>
   </note>
   <para>
    Mit folgendem Kommando entfernen Sie ein Kontingent:
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Verwalten von CephFS-Snapshots</title>

  <para>
   Ein CephFS-Snapshot ist eine schreibgeschützte Ansicht des Dateisystems zu dem Zeitpunkt, zu dem der Snapshot erstellt wurde. Sie können Snapshots in beliebigen Verzeichnissen erstellen. Der Snapshot deckt alle Daten im Dateisystem unter dem angegebenen Verzeichnis ab. Nach dem Erstellen des Snapshots werden die zwischengespeicherten Daten asynchron von verschiedenen Clients verschoben. Die Snapshot-Erstellung ist daher sehr schnell.
  </para>

  <important>
   <title>Mehrere Dateisysteme</title>
   <para>
    Greifen mehrere CephFS-Dateisysteme auf einen einzelnen Pool zu (über Namespaces), prallen ihre Snapshots aufeinander. Wenn Sie dann einen einzelnen Snapshot löschen, fehlen die entsprechenden Daten in anderen Snapshots, die denselben Pool nutzen.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Erstellen von Snapshots</title>
   <para>
    Die CephFS-Snapshot-Funktion ist auf neuen Dateisystemen standardmäßig aktiviert. Mit folgendem Kommando aktivieren Sie die Funktion auf vorhandenen Dateisystemen:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Sobald Sie die Snapshots aktiviert haben, enthalten alle Verzeichnisse im CephFS das besondere Unterverzeichnis <filename>.snap</filename>.
   </para>
   <para>
    Für CephFS-Kernel-Clients gilt eine Einschränkung: Diese Clients können maximal 400 Snapshots in einem Verzeichnisbaum verarbeiten. Halten Sie die Anzahl der Snapshots stets unter diesem Grenzwert, unabhängig vom tatsächlichen Client. Wenn Sie mit älteren CephFS-Clients arbeiten (z. B. mit SLE12-SP3), bedenken Sie, dass eine Anzahl von mehr als 400 Snapshots zum Absturz des Clients führt und damit die Abläufe erheblich beeinträchtigt.
   </para>
   <tip>
    <title>Benutzerdefinierter Name für das Snapshot-Unterverzeichnis</title>
    <para>
     Mit der Einstellung <option>client snapdir</option> können Sie einen anderen Namen für das Snapshot-Unterverzeichnis konfigurieren.
    </para>
   </tip>
   <para>
    Zum Erstellen eines Snapshots legen Sie ein Unterverzeichnis mit einem benutzerdefinierten Namen unter dem Verzeichnis <filename>.snap</filename> an. Mit folgendem Kommando erstellen Sie beispielsweise einen Snapshot des Verzeichnisses <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Löschen von Snapshots</title>
   <para>
    Soll ein Snapshot gelöscht werden, entfernen Sie das zugehörige Unterverzeichnis aus dem Verzeichnis <filename>.snap</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
