<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph.monitor">
 <title>Ermitteln des Cluster-Zustands</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>Ja</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Die Überwachung eines aktiven Clusters ist mit dem Werkzeug <command>ceph</command> möglich. Zur Ermittlung des Cluster-Zustands wird normalerweise der Status des OSD, des Monitors, der Placement Group und des Metadata Servers überprüft. <remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>Interaktiver Modus</title>
  <para>
   Geben Sie zur Ausführung des Werkzeugs <command>ceph</command> im interaktiven Modus <command>ceph</command> in der Kommandozeile ohne Argumente ein. Der interaktive Modus ist praktischer, wenn Sie vorhaben, mehrere <command>ceph</command>-Kommandos in einer Zeile einzugeben. Beispiel:
  </para>
<screen><prompt>cephadm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Überprüfen des Cluster-Zustands</title>

  <para>
   Überprüfen Sie den Zustand Ihres Clusters nach dessen Start und bevor Sie mit dem Lesen und/oder Schreiben von Daten beginnen:
  </para>

<screen><prompt>root # </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   Der Ceph Cluster gibt einen der folgenden Zustandscodes zurück:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs sind als inaktiv gekennzeichnet. Der OSD Daemon wurde möglicherweise gestoppt oder Peer-OSDs erreichen den OSD möglicherweise nicht über das Netzwerk. Üblicherweise handelt es sich dabei um einen gestoppten oder abgestürzten Daemon, einen inaktiven Host oder einen Netzwerkausfall.
     </para>
     <para>
      Verifizieren Sie, dass sich der Host in einem ordnungsgemäßen Zustand befindet, der Daemon gestartet ist und das Netzwerk funktioniert. Falls der Daemon abgestürzt ist, enthält die Protokolldatei des Daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) möglicherweise Informationen zur Fehlersuche.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>CRUSH-Typ</replaceable>_DOWN, beispielsweise OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Alle OSDs in einem bestimmten CRUSH-Teilbaum sind als inaktiv gekennzeichnet, wie zum Beispiel alle OSDs auf einem Host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      In der CRUSH Map-Hierarchie wird auf einen OSD verwiesen, er ist jedoch nicht vorhanden. Der OSD kann aus der CRUSH-Hierarchie entfernt werden mit:
     </para>
<screen><prompt>root # </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Die Auslastungsgrenzwerte für <emphasis>backfillfull</emphasis>, <emphasis>nearfull</emphasis>, <emphasis>full</emphasis> und/oder <emphasis>failsafe_full</emphasis> sind nicht aufsteigend. Insbesondere erwarten wir <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> und <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>. Die Grenzwerte können angepasst werden mit:
     </para>
<screen><prompt>root # </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>full</emphasis> überschritten, was verhindert, dass der Cluster Schreibvorgänge ausführt. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
     <para>
      Der zurzeit definierte Grad <emphasis>full</emphasis> wird angezeigt mit:
     </para>
<screen><prompt>root # </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Eine kurzfristige Behelfslösung zum Wiederherstellen der Schreibverfügbarkeit ist die geringfügige Erhöhung des Grenzwerts:
     </para>
<screen><prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Fügen Sie neuen Speicherplatz zum Cluster hinzu, indem Sie weitere OSDs bereitstellen, oder löschen Sie bestehende Daten, um Speicherplatz freizugeben.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>backfillfull</emphasis> überschritten, wodurch verhindert wird, dass auf diesem Gerät ein Datenausgleich stattfindet. Hierbei handelt es sich um eine frühzeitige Warnung, dass der Ausgleich möglicherweise nicht durchgeführt werden kann und der Cluster fast voll ist. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>nearfull</emphasis> überschritten. Hierbei handelt es sich um eine frühzeitige Warnung, dass der Cluster fast voll ist. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Eine oder mehrere interessante Cluster-Flaggen wurden gesetzt. Mit Ausnahme von <emphasis>full</emphasis> können diese Flaggen festgelegt oder gelöscht werden mit:
     </para>
<screen><prompt>root # </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>root # </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Die folgenden Flaggen sind verfügbar:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         Der Cluster wird als voll gekennzeichnet und kann keine Schreibvorgänge mehr bedienen.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr </term>
       <listitem>
        <para>
         Lese- und Schreibvorgänge werden ausgesetzt.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs dürfen nicht gestartet werden.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD-Fehlerberichte werden ignoriert, sodass die Monitors keine OSDs mit <emphasis>down</emphasis> kennzeichnen.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs, die vorher mit <emphasis>out</emphasis> gekennzeichnet wurden, werden beim Starten nicht wieder mit <emphasis>in</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs werden nach dem konfigurierten Intervall nicht automatisch mit <emphasis>out</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Wiederherstellung oder Datenausgleich ist angehalten.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing (Informationen hierzu finden Sie in <xref linkend="scrubbing"/>) ist deaktiviert.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Die Cache Tiering-Aktivität ist angehalten.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Für eine oder mehrere OSDs wurde eine Interessensflagge pro OSD gesetzt. Die folgenden Flaggen sind verfügbar:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         Der OSD darf nicht starten.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Fehlerberichte für diesen OSD werden ignoriert.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Wenn dieser OSD vorher automatisch nach einem Fehler mit <emphasis>out</emphasis> markiert wurde, wird er beim Start nicht mit <emphasis>in</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Wenn dieser OSD inaktiv ist, wird er nach dem konfigurierten Intervall nicht automatisch mit <emphasis>out</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Pro-OSD-Flaggen werden gesetzt oder gelöscht mit:
     </para>
<screen><prompt>root # </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>root # </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      Die CRUSH Map verwendet sehr alte Einstellungen und sollte aktualisiert werden. Die ältesten Tunables, die verwendet werden können (d. h. die älteste Client-Version, die eine Verbindung zum Cluster herstellen kann), ohne diese Zustandswarnung auszulösen, werden durch die Konfigurationsoption <option>mon_crush_min_required_version</option> festgelegt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      Die CRUSH Map verwendet eine ältere, nicht optimale Methode zur Berechnung von Gewichtungszwischenwerten für Straw Buckets. Die CRUSH Map sollte aktualisiert werden, um die neuere Methode zu verwenden (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Einer oder mehrere Cache Pools wurden nicht mit einem Treffersatz zum Verfolgen der Auslastung konfiguriert. Dadurch wird verhindert, dass der Tiering-Agent unbenutzte Objekte erkennt, die aus dem Cache entfernt werden müssen. Treffersätze werden im Cache Pool konfiguriert mit:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      Es werden keine OSDs vor Luminous Version 12 ausgeführt, doch die Flagge <option>sortbitwise</option> wurde nicht gesetzt. Die Flagge <option>sortbitwise</option> muss gesetzt werden, damit OSDs ab Luminous Version 12 starten:
     </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben das Kontingent erreicht und lassen keine weiteren Schreibvorgänge zu. Pool-Kontingente und Auslastungswerte werden mit folgendem Kommando festgelegt:
     </para>
<screen><prompt>root # </prompt>ceph df detail</screen>
     <para>
      Legen Sie entweder das Pool-Kontingent mit 
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      fest oder löschen Sie einige vorhandene Daten, um die Auslastung zu verringern.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Die Datenverfügbarkeit ist reduziert. Dies bedeutet, dass der Cluster potenzielle Lese- oder Schreibanforderungen für einige Daten im Cluster nicht bedienen kann. Dies ist insbesondere dann der Fall, wenn sich eine oder mehrere PGs in einem Zustand befinden, der die Ausführung von E/A-Anforderungen nicht zulässt. Problematische PG-Zustände sind <emphasis>peering</emphasis>, <emphasis>stale</emphasis>, <emphasis>incomplete</emphasis> und das Fehlen von <emphasis>active</emphasis> (wenn diese Zustände nicht schnell gelöscht werden). Detaillierte Informationen dazu, welche PGs betroffen sind, erhalten Sie durch Ausführen von:
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      In den meisten Fällen besteht die Ursache darin, dass einer oder mehrere OSDs aktuell inaktiv sind. Der Zustand bestimmter problematischer PGs kann abgefragt werden mit:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Die Datenredundanz ist für einige Daten reduziert. Dies bedeutet, dass der Cluster nicht über die gewünschte Anzahl der Reproduktionen für alle Daten (für reproduzierte Pools) oder Erasure Code-Fragmente (für Erasure Coded Pools) verfügt. Dies ist insbesondere dann der Fall, wenn bei einer oder mehreren PGs die Flagge <emphasis>degraded</emphasis> oder <emphasis>undersized</emphasis> gesetzt wurde (es sind nicht genügend Instanzen dieser Placement Group im Cluster vorhanden) oder wenn einige Zeit die Flagge <emphasis>clean</emphasis> nicht gesetzt war. Detaillierte Informationen dazu, welche PGs betroffen sind, erhalten Sie durch Ausführen von:
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      In den meisten Fällen besteht die Ursache darin, dass einer oder mehrere OSDs aktuell inaktiv sind. Der Zustand bestimmter problematischer PGs kann abgefragt werden mit:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Die Datenredundanz ist möglicherweise für einige Daten reduziert oder in Gefahr, weil freier Speicherplatz im Cluster fehlt. Dies ist insbesondere dann der Fall, wenn für eine oder mehrere PGs die Flagge <emphasis>backfill_toofull</emphasis> oder <emphasis>recovery_toofull</emphasis> gesetzt wurde. Dies bedeutet, dass der Cluster keine Daten migrieren oder wiederherstellen kann, weil einer oder mehrere OSDs den Grenzwert <emphasis>backfillfull</emphasis> überschritten haben.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Beim Daten-Scrubbing (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing"/>) wurden einige Probleme bezüglich der Datenkonsistenz im Cluster erkannt. Dies ist insbesondere dann der Fall, wenn für eine oder mehrere PGs die Flagge <emphasis>inconsistent</emphasis> oder <emphasis>snaptrim_error</emphasis> gesetzt wurde. Dadurch wird angegeben, dass bei einem früheren Scrubbing-Vorgang ein Problem gefunden wurde. Möglicherweise wurde auch die Flagge <emphasis>repair</emphasis> gesetzt, was bedeutet, dass eine derartige Inkonsistenz gerade repariert wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Bei kürzlich durchgeführten OSD-Scrubbing-Vorgängen wurden Inkonsistenzen erkannt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Ein Cache-Schicht-Pool ist fast voll. In diesem Kontext wird "full" durch die Eigenschaften <emphasis>target_max_bytes</emphasis> und <emphasis>target_max_objects</emphasis> des Cache Pools bestimmt. Wenn der Pool den Grenzwert erreicht, werden Schreibanforderungen an den Pool möglicherweise blockiert und Daten aus dem Cache entfernt und gelöscht. Dieser Zustand führt normalerweise zu sehr hohen Latenzen und schlechter Leistung. Der Grenzwert für die Größe des Pools kann angepasst werden mit:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Die normale Aktivität zum Leeren des Cache wird möglicherweise auch gedrosselt durch die reduzierte Verfügbarkeit oder Leistung der Basisschicht oder durch die Cluster-Last insgesamt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      Die Anzahl der verwendeten PGs liegt unterhalb des konfigurierbaren Grenzwerts der <option>mon_pg_warn_min_per_osd</option>-PGs pro OSD. Dies führt eventuell dazu, dass die Daten nicht optimal auf die OSDs im Cluster verteilt und ausgeglichen werden und die Leistung insgesamt verschlechtert wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      Die Anzahl der verwendeten PGs liegt oberhalb des konfigurierbaren Grenzwerts der <option>mon_pg_warn_min_per_osd</option>-PGs pro OSD. Dies führt möglicherweise zu höherer Arbeitsspeicherauslastung für OSD-Daemons, langsamerem Peering nach Änderungen des Cluster-Zustands (beispielsweise OSD-Neustarts, Hinzufügen oder Entfernen) sowie höherer Last für Ceph Managers und Ceph Monitors.
     </para>
     <para>
      Während sich der <option>pg_num</option>-Wert für bestehende Pools nicht verkleinern lässt, ist dies beim <option>pgp_num</option>-Wert durchaus möglich. Dadurch werden einige PGs effizient auf denselben Gruppen von OSDs angeordnet, wodurch einige der oben beschriebenen negativen Auswirkungen abgeschwächt werden. Der <option>pgp_num</option>-Wert kann angepasst werden mit:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      Bei einem oder mehreren Pools ist der <option>pgp_num</option>-Wert kleiner als der <option>pg_num</option>-Wert. Dies ist normalerweise ein Anzeichen dafür, dass die PG-Anzahl erhöht wurde, ohne auch das Platzierungsverhalten zu erhöhen. Dieses Problem wird normalerweise dadurch behoben, dass <option>pgp_num</option> mit <option>pg_num</option> abgeglichen wird, was die Datenmigration auslöst. Verwenden Sie hierzu:
     </para>
<screen>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Bei einem oder mehreren Pools ist die durchschnittliche Anzahl von Objekten pro PG erheblich höher als der Durchschnitt im Cluster insgesamt. Der spezifische Grenzwert wird gesteuert durch den Konfigurationswert<option>mon_pg_warn_max_object_skew</option>. Dies ist normalerweise ein Anzeichen dafür, dass die Pools mit den meisten Daten im Cluster zu wenige PGs enthalten und/oder dass andere Pools mit weniger Daten zu viele PGs enthalten. Um die Zustandswarnung abzustellen, kann der Grenzwert durch Anpassen der Konfigurationsoption <option>mon_pg_warn_max_object_skew</option> an den Monitors erhöht werden.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Es ist zwar ein Pool mit einem oder mehreren Objekten vorhanden, wurde jedoch nicht für die Verwendung durch eine bestimmte Anwendung gekennzeichnet. Heben Sie diese Warnmeldung auf, indem Sie den Pool für die Verwendung durch eine Anwendung kennzeichnen. Beispiel, wenn der Pool von RBD verwendet wird:
     </para>
<screen><prompt>root # </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Wenn der Pool durch eine benutzerdefinierte Anwendung "foo" verwendet wird, können Sie ihn auch mit dem Kommando der untersten Ebene kennzeichnen:
     </para>
<screen><prompt>root # </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben das Kontingent erreicht (oder erreichen es bald). Der Grenzwert zum Auslösen dieser Fehlerbedingung wird durch die Konfigurationsoption <option>mon_pool_quota_crit_threshold</option> gesteuert. Pool-Kontingente werden nach oben oder unten angepasst (oder entfernt) mit:
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Durch Festlegen des Kontingentwerts auf 0 wird das Kontingent deaktiviert.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben bald das Kontingent erreicht. Der Grenzwert zum Auslösen dieser Fehlerbedingung wird durch die Konfigurationsoption <option>mon_pool_quota_warn_threshold</option> gesteuert. Pool-Kontingente werden nach oben oder unten angepasst (oder entfernt) mit:
     </para>
<screen><prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Durch Festlegen des Kontingentwerts auf 0 wird das Kontingent deaktiviert.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Ein oder mehrere Objekte im Cluster sind nicht in dem Node gespeichert, in dem der Cluster es erwartet. Dies ist ein Anzeichen dafür, dass die Datenmigration aufgrund einer kürzlich vorgenommenen Änderung am Cluster noch nicht abgeschlossen ist. Falsch platzierte Daten stellen an sich keine Gefahr dar. Die Datenkonsistenz ist niemals in Gefahr und alte Kopien von Objekten werden niemals entfernt, bevor die gewünschte Anzahl an neuen Kopien (an den gewünschten Speicherorten) vorhanden ist.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      Ein oder mehrere Objekte im Cluster werden nicht gefunden. Dies ist insbesondere dann der Fall, wenn die OSDs wissen, dass eine neue oder aktualisierte Kopie eines Objekts vorhanden sein sollte, jedoch keine Kopie dieser Version des Objekts auf OSDs gefunden wurden, die zurzeit online sind. Lese- oder Schreibanforderungen an die "unfound"-Objekte werden blockiert. Idealerweise kann ein inaktiver OSD, der die neueste Kopie des nicht gefundenen Objekts enthält, wieder online gehen. Dafür in Frage kommende OSDs können anhand des Peering-Zustands für die PGs identifiziert werden, die für das nicht gefundene Objekt zuständig sind:
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      Die Verarbeitung einer oder mehrerer OSD-Anforderungen dauert sehr lange. Dies ist möglicherweise ein Anzeichen für eine extreme Last, ein langsames Speichergerät oder einen Softwarefehler. Mit folgendem Kommando, das auf dem OSD-Host ausgeführt wird, fragen Sie die Anforderungswarteschlange auf den in Frage kommenden OSDs ab:
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      Sie sehen eine Zusammenfassung der langsamsten kürzlich vorgenommenen Anforderungen:
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      Sie finden den Standort eines OSDs mit:
     </para>
<screen><prompt>root # </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Eine oder mehrere OSD-Anforderungen wurden extrem lange blockiert. Dies ist ein Anzeichen dafür, dass sich entweder der Cluster für einen längeren Zeitraum in einem fehlerhaften Zustand befindet (beispielsweise wenn nicht genügend OSDs ausgeführt werden) oder wenn interne Probleme am OSD aufgetreten sind.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      Es wurde länger kein Scrubbing bei einem oder mehreren PGs durchgeführt (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing"/>). Bei PGs wird normalerweise alle <option>mon_scrub_interval</option> Sekunden ein Scrubbing durchgeführt und diese Warnmeldung wird ausgelöst, wenn diese <option>mon_warn_not_scrubbed</option>-Intervalle ohne Scrubbing abgelaufen sind. Bei PGs wird kein Scrubbing durchgeführt, wenn sie nicht als intakt ("clean") gekennzeichnet sind. Dies kann vorkommen, wenn sie falsch platziert wurden oder eingeschränkt leistungsfähig sind (weitere Informationen hierzu finden Sie oben unter PG_AVAILABILITY und PG_DEGRADED). Ein Scrubbing einer intakten PG wird manuell initiiert mit:
     </para>
<screen><prompt>root # </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Bei einer oder mehreren PGs wurden länger kein Deep Scrubbing durchgeführt (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing"/>). Bei PGs wird normalerweise alle <option>osd_deep_mon_scrub_interval</option> Sekunden ein Scrubbing durchgeführt und diese Warnmeldung wird ausgelöst, wenn diese <option>mon_warn_not_deep_scrubbed</option>-Intervalle ohne Scrubbing abgelaufen sind. Bei PGs wird kein (Deep) Scrubbing durchgeführt, wenn sie nicht als intakt ("clean") gekennzeichnet sind. Dies kann vorkommen, wenn sie falsch platziert wurden oder eingeschränkt leistungsfähig sind (weitere Informationen hierzu finden Sie oben unter PG_AVAILABILITY und PG_DEGRADED). Ein Scrubbing einer intakten PG wird manuell initiiert mit:
     </para>
<screen><prompt>root # </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Wenn Sie nicht standardmäßige Standorte für Ihre Konfiguration oder Ihren Schlüsselbund angegeben haben, können Sie diese angeben mit:
   </para>
<screen><prompt>root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Beobachten eines Clusters</title>

  <para>
   Mit <command>ceph -s</command> rufen Sie den aktuellen Zustand des Clusters ab. Beispielsweise gibt ein sehr kleiner Ceph Cluster bestehend aus einem Monitor und zwei OSDs Folgendes aus, wenn ein Workload ausgeführt wird:
  </para>

<screen>cluster:
  id:     6586341d-4565-3755-a4fd-b50f51bee248
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum blueshark1,blueshark2,blueshark3
  mgr: blueshark3(active), standbys: blueshark2, blueshark1
  osd: 15 osds: 15 up, 15 in

data:
  pools:   8 pools, 340 pgs
  objects: 537 objects, 1985 MB
  usage:   23881 MB used, 5571 GB / 5595 GB avail
  pgs:     340 active+clean

io:
  client:   100 MB/s rd, 26256 op/s rd, 0 op/s wr</screen>

  <para>
   Die Ausgabe bietet die folgenden Informationen:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster-ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster-Zustand
    </para>
   </listitem>
   <listitem>
    <para>
     Die Monitor-Zuordnungsepoche und den Status des Monitor-Quorums
    </para>
   </listitem>
   <listitem>
    <para>
     Die OSD-Zuordnungsepoche und den Status der OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     Die Version der Placement Group-Zuordnung
    </para>
   </listitem>
   <listitem>
    <para>
     Die Anzahl der Placement Groups und Pools
    </para>
   </listitem>
   <listitem>
    <para>
     Die <emphasis>nominelle</emphasis> Menge der gespeicherten Daten und die Anzahl der gespeicherten Objekte und schließlich
    </para>
   </listitem>
   <listitem>
    <para>
     Die Gesamtmenge der gespeicherten Daten
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Wie Ceph die Datennutzung berechnet</title>
   <para>
    Der Wert <literal>used</literal> bezeichnet den tatsächlich belegten Basisspeicherplatz. Der Wert <literal>xxx GB / xxx GB</literal> bezeichnet den verfügbaren Speicherplatz (der kleinere Wert) der gesamten Speicherkapazität des Clusters. Die nominelle Menge spiegelt die Menge der gespeicherten Daten wider, bevor diese reproduziert oder geklont werden oder ein Snapshot davon erstellt wird. Daher übersteigt die Menge der tatsächlich gespeicherten Daten normalerweise die nominelle gespeicherte Menge, weil Ceph Reproduktionen der Daten erstellt und die Speicherkapazität möglicherweise auch zum Klonen und für Snapshots verwendet.
   </para>
  </tip>

  <para>
   Mit den folgenden Kommandos werden die Statusinformationen ebenfalls sofort angezeigt:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Stellen Sie zum Aktualisieren der Informationen in Echtzeit diese Kommandos (einschließlich <command>ceph -s</command>) in eine Warteschleife, wie zum Beispiel:
  </para>

<screen><systemitem class="username">root</systemitem>while true ; do ceph -s ; sleep 10 ; done</screen>

  <para>
   Drücken Sie <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>, wenn Sie den Vorgang nicht länger beobachten möchten.
  </para>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Überprüfen der Nutzungsstatistik</title>

  <para>
   Die Datennutzung eines Clusters und die Datenverteilung auf die Pools überprüfen Sie mit der Option <command>df</command>. Die Option ist der Linux-Option <command>df</command> ähnlich. Führen Sie Folgendes aus:
  </para>

<screen><prompt>root # </prompt>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    55886G     55826G       61731M          0.11
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    testpool     1          0         0        17676G           0
    ecpool       2      4077M      0.01        35352G        2102
    test1        3          0         0        17676G           0
    rbd          4         16         0        17676G           3
    rbd1         5         16         0        17676G           3
    ecpool1      6      5708M      0.02        35352G        2871</screen>

  <para>
   Der Abschnitt <literal>GLOBAL</literal> der Ausgabe gibt einen Überblick über den Speicherplatz, den Ihr Cluster für die Daten nutzt.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: Die gesamte Speicherkapazität des Clusters.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: Der verfügbare freie Speicherplatz im Cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: Der genutzte Basisspeicherplatz
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: Der Prozentsatz des genutzten Basisspeicherplatzes. Verwenden Sie diese Zahl in Verbindung mit <literal>full ratio</literal> und <literal>near full ratio</literal>, um sicherzustellen, dass die Kapazität des Clusters nicht voll ausgelastet wird. Weitere Details finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Speicherkapazität</link>.
    </para>
    <note>
     <title>Füllstand des Clusters</title>
     <para>
      Ein Füllstand des Basisspeichers von 70 % bis 80 % gibt an, dass neuer Speicherplatz zum Cluster hinzugefügt werden muss. Eine höhere Auslastung führt möglicherweise zu einzelnen vollen OSDs und Problemen mit dem Cluster-Zustand.
     </para>
     <para>
      Listen Sie mit dem Kommando <command>ceph osd df tree</command> den Füllstand aller OSDs auf.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   Im Abschnitt <literal>POOLS</literal> der Ausgabe finden Sie eine Liste der Pools und die nominelle Auslastung der einzelnen Pools. Die Ausgabe in diesem Abschnitt umfasst <emphasis>keine</emphasis> Reproduktionen, Klone oder Snapshots. Beispiel: Wenn Sie ein Objekt mit 1 MB Daten speichern, beträgt die nominelle Auslastung 1 MB. Die tatsächliche Auslastung beträgt jedoch möglicherweise 2 MB oder mehr, je nach Anzahl der Reproduktionen, Klone und Snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: Der Name des Pools.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: Die Pool-ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: Die nominelle Menge der Daten in Kilobyte, falls an die Zahl nicht M für Megabyte oder G für Gigabyte angehängt ist.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: Der nominelle Prozentsatz des genutzten Speichers pro Pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: Der maximal verfügbare Speicherplatz im jeweiligen Pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: Die nominelle Anzahl der pro Pool gespeicherten Objekte.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Die Zahlen im Abschnitt POOLS sind nominell. Sie sind nicht inklusive der Anzahl der Reproduktionen, Snapshots oder Klone zu verstehen. Folglich entspricht die Summe der Zahlen unter USED and %USED nicht den Zahlen unter RAW USED und %RAW USED im Abschnitt %GLOBAL der Ausgabe.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Überprüfen des Status eines Clusters</title>

  <para>
   Führen Sie zum Überprüfen des Status eines Clusters Folgendes aus:
  </para>

<screen><prompt>root # </prompt>ceph status</screen>

  <para>
   oder
  </para>

<screen><prompt>root # </prompt>ceph -s</screen>

  <para>
   Geben Sie im interaktiven Modus <command>status</command> ein und drücken Sie auf <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph gibt den Cluster-Status aus. Beispielsweise gibt ein sehr kleiner Cluster bestehend aus einem Monitor und zwei OSDs möglicherweise Folgendes aus:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Überprüfen des OSD-Status</title>

  <para>
   Mit folgenden Kommandos prüfen Sie die OSDs, um sicherzustellen, dass sie aktiv sind:
  </para>

<screen><prompt>root # </prompt>ceph osd stat</screen>

  <para>
   oder
  </para>

<screen><prompt>root # </prompt>ceph osd dump</screen>

  <para>
   OSDs lassen sich auch entsprechend ihrer Position in der CRUSH-Map anzeigen.
  </para>

<screen><prompt>root # </prompt>ceph osd tree</screen>

  <para>
   Ceph gibt einen CRUSH-Baum mit einem Host und dessen OSDs, den Status der Aktivität und deren Gewicht aus.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Suchen nach vollen OSDs</title>

  <para>
   Ceph verhindert, dass Sie an einen vollen OSD schreiben, damit Sie keine Daten verlieren. In einem betriebsbereiten Cluster sollten Sie eine Warnmeldung erhalten, wenn der Cluster annähernd voll ist. Der Wert für <command>mon osd full ratio</command> beträgt standardmäßig 0,95 bzw. 95 % der Kapazität. Wenn dieser Wert erreicht ist, werden die Clients davon abgehalten, Daten zu schreiben. Der Wert <command>mon osd nearfull ratio</command> beträgt standardmäßig 0,85 bzw. 85 % der Kapazität. Wenn dieser Wert erreicht ist, wird eine Zustandswarnung generiert.
  </para>

  <para>
   Volle OSD-Nodes werden durch <command>ceph health</command> gemeldet:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   oder
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   Am besten fügen Sie bei einem vollen Cluster neue OSD Nodes hinzu. Dadurch kann der Cluster Daten auf den neu verfügbaren Speicherplatz verteilen.
  </para>

  <para>
   Wenn sich ein OSD nicht starten lässt, weil er voll ist, können Sie einige Daten löschen, indem Sie einige Placement Group-Verzeichnisse im vollen OSD löschen.
  </para>

  <tip>
   <title>Verhindern voller OSDs</title>
   <para>
    Wenn ein OSD voll wird, also 100 % der Festplattenkapazität nutzt, stürzt er normalerweise schnell und ohne Warnmeldung ab. Nachfolgend sehen Sie einige Tipps, die Sie beim Verwalten von OSD Nodes beachten sollten.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Der Festplattenspeicherplatz der einzelnen OSDs (normalerweise eingehängt unter<filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) muss jeweils auf eine dedizierte zugrundeliegende Festplatte oder Partition gestellt werden.
     </para>
    </listitem>
    <listitem>
     <para>
      Überprüfen Sie die Ceph-Konfigurationsdateien und stellen Sie sicher, dass Ceph die Protokolldatei nicht auf den Festplatten/Partitionen speichert, die explizit für die OSDs vorgesehen sind.
     </para>
    </listitem>
    <listitem>
     <para>
      Vergewissern Sie sich, dass kein anderer Prozess auf die Festplatten/Partitionen schreibt, die explizit für die OSDs vorgesehen sind.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Überprüfen des Monitor-Status</title>

  <para>
   Wenn Ihr Cluster über mehrere Monitors verfügt (was wahrscheinlich ist), dann sollten Sie nach dem Start des Clusters und vor dem Lesen und/oder Schreiben von Daten den Status des Monitor-Quorums prüfen. Ein Quorum muss vorhanden sein, wenn mehrere Monitors ausgeführt werden. Sie sollten auch regelmäßig den Monitor-Status prüfen, um sicherzustellen, dass sie ausgeführt werden.
  </para>

  <para>
   Führen Sie zum Anzeigen der Monitor-Zuordnung Folgendes aus:
  </para>

<screen><prompt>root # </prompt>ceph mon stat</screen>

  <para>
   oder
  </para>

<screen><prompt>root # </prompt>ceph mon dump</screen>

  <para>
   Führen Sie zum Prüfen des Quorum-Status für den Monitor-Cluster Folgendes aus:
  </para>

<screen><prompt>root # </prompt>ceph quorum_status</screen>

  <para>
   Ceph gibt den Quorum-Status zurück. Beispielsweise gibt ein Ceph Cluster, der aus drei Monitors besteht, möglicherweise Folgendes zurück:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Überprüfen des Zustands von Placement Groups</title>

  <para>
   Placement Groups ordnen Objekte zu OSDs zu. Bei der Überwachung Ihrer Placement Groups sollten diese <literal>active</literal> (aktiv) und <literal>clean</literal> (intakt) sein. Eine detaillierte Erläuterung zu diesem Thema finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Überwachen von OSDs und Placement Groups</link>.
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Verwenden des Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark> Mit einem Ceph Admin Socket haben Sie die Möglichkeit, einen Daemon über eine Socket-Schnittstelle abzufragen. Ceph Sockets befinden sich standardmäßig unter <filename>/var/run/ceph</filename>. Melden Sie sich für den Zugriff auf einen Daemon über den Admin Socket beim Host an, auf dem der Daemon ausgeführt wird, und führen Sie folgendes Kommando aus:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   Führen Sie folgendes Kommando aus, um die verfügbaren Admin Socket-Kommandos anzuzeigen:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   Mit dem Admin Socket-Kommando können Sie Ihre Konfiguration während der Laufzeit anzeigen und festlegen. Weitere Details finden Sie unter <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Anzeigen einer Konfiguration während der Laufzeit</link>.
  </para>

  <para>
   Außerdem können Sie die Konfigurationswerte während der Laufzeit direkt festlegen (der Admin Socket umgeht den Monitor, im Gegensatz zum Kommando <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable> injectargs, das sich auf den Monitor verlässt. Dazu brauchen Sie sich jedoch nicht direkt beim betreffenden Host anzumelden).
  </para>
 </sect1>
</chapter>
