<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_troubleshooting.xml" version="5.0" xml:id="storage-troubleshooting">
 <title>Fehlersuche</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>Ja</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  In diesem Kapitel werden verschiedene Probleme erläutert, die möglicherweise bei Ihrer Arbeit mit einem Ceph Cluster auftreten.
 </para>
 <sect1 xml:id="storage-bp-report-bug">
  <title>Melden von Softwareproblemen</title>

  <para>
   Falls bei der Ausführung von SUSE Enterprise Storage bei einigen der Komponenten wie Ceph oder Object Gateway ein Problem auftritt, melden Sie es bitte dem Technischen Support von SUSE. Die dazu empfohlene Methode ist das Dienstprogramm <command>supportconfig</command>.
  </para>

  <tip>
   <para>
    Da es sich bei <command>supportconfig</command> um eine modulare Software handelt, sollten Sie sicherstellen, dass das Paket <systemitem>supportutils-plugin-ses</systemitem> installiert ist.
   </para>
<screen>rpm -q supportutils-plugin-ses</screen>
   <para>
    Falls es am Ceph Server fehlt, installieren Sie es mit
   </para>
<screen>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</screen>
  </tip>

  <para>
   Auch wenn Sie <command>supportconfig</command> in der Kommandozeile verwenden können, empfehlen wir doch das entsprechende YaST-Modul. Weitere Informationen zu <command>supportconfig</command> finden Sie unter <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html#sec.admsupport.supportconfig"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-cluster-mntc-rados-striping">
  <title>Fehler beim Senden großer Objekte mit <command>rados</command> bei vollem OSD</title>

  <para>
   <command>rados</command> ist ein Kommandozeilenprogramm zur Verwaltung des RADOS-Objektspeichers. Weitere Informationen finden Sie unter <command>man 8 rados</command>.
  </para>

  <para>
   Wenn Sie mit dem <command>rados</command>-Dienstprogramm ein großes Objekt an einen Ceph Cluster senden, wie zum Beispiel
  </para>

<screen>rados -p mypool put myobject /file/to/send</screen>

  <para>
   füllt das Objekt möglicherweise den gesamten OSD-Speicherplatz aus und verursacht erhebliche Probleme bei der Cluster-Leistung.
  </para>
 </sect1>
 <sect1 xml:id="ceph-xfs-corruption">
  <title>Beschädigtes XFS-Dateisystem</title>

  <para>
   Unter seltenen Umständen wie zum Beispiel einem Kernel-Fehler oder fehlerhafter/falsch konfigurierter Hardware könnte das zugrundeliegende Dateisystem (XFS), in dem ein OSD seine Daten speichert, beschädigt werden oder nicht mehr einhängbar sein.
  </para>

  <para>
   Wenn Sie sicher wissen, dass es keine Probleme mit der Hardware gibt und das System ordnungsgemäß konfiguriert ist, melden Sie einen Fehler am XFS-Teilsystem des SUSE Linux Enterprise Server-Kernels und kennzeichnen Sie den betreffenden OSD als ausgefallen:
  </para>

<screen>ceph osd down <replaceable>OSD identification</replaceable></screen>

  <warning>
   <title>Das beschädigte Gerät darf nicht formatiert oder anderweitig bearbeitet werden</title>
   <para>
    Es mag vielleicht vernünftig erscheinen, das Problem im Dateisystem mit <command>xfs_repair</command> zu beheben, doch das Kommando ändert das Dateisystem und darf daher nicht verwendet werden. Der OSD startet zwar möglicherweise, doch seine Funktionsweise könnte beeinträchtigt sein.
   </para>
  </warning>

  <para>
   Nun löschen Sie die zugrundeliegende Festplatte und erstellen den OSD mit folgendem Kommando neu:
  </para>

<screen>ceph-disk prepare --zap $OSD_DISK_DEVICE $OSD_JOURNAL_DEVICE"</screen>

  <para>
   Beispiel:
  </para>

<screen>ceph-disk prepare --zap /dev/sdb /dev/sdd2</screen>
 </sect1>
 <sect1 xml:id="storage-bp-recover-toomanypgs">
  <title>Statusmeldung "Too Many PGs per OSD" (Zu viele PGs pro OSD)</title>

  <para>
   Wenn Sie die Nachricht <literal>Too Many PGs per OSD</literal> (Zu viele PGs pro OSD) erhalten, nachdem Sie <command>ceph status</command> ausgeführt haben, bedeutet dies, dass der Wert <option>mon_pg_warn_max_per_osd</option> (standardmäßig 300) überschritten wurde. Dieser Wert wird mit der Anzahl der PGs pro OSD-Kontingent verglichen. Dies bedeutet, dass die Cluster-Einrichtung nicht optimal ist.
  </para>

  <para>
   Die Anzahl der PGs kann nach Erstellung des Pools nicht verringert werden. Pools, die noch keine Daten enthalten, können sicher gelöscht und dann mit einer kleineren Anzahl von PGs neu erstellt werden. Im Fall, dass Pools bereits Daten enthalten, besteht die einzige Lösung darin, OSDs zum Cluster hinzuzufügen, sodass das Kontingent von PGs pro OSD verkleinert wird.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-stuckinactive">
  <title>Statusmeldung "<emphasis>nn</emphasis> pg stuck inactive" (NN PG hängen geblieben und inaktiv)</title>

  <para>
   Wenn Sie eine Statusmeldung <literal>stuck inactive</literal> (hängen geblieben und inaktiv) erhalten, nachdem Sie <command>ceph status</command> ausgeführt haben, bedeutet es, dass Ceph nicht weiß, wo die gespeicherten Daten reproduziert werden sollen, um die Reproduktionsregeln zu erfüllen. Dies kann kurz nach der Ersteinrichtung von Ceph vorkommen und wird automatisch repariert. In anderen Fällen ist möglicherweise ein manuelles Eingreifen nötig, wie zum Beispiel Aktivieren eines fehlerhaften OSDs oder Hinzufügen eines neuen OSDs zum Cluster. In sehr seltenen Fällen mag es helfen, die Reproduktionsstufe zu reduzieren.
  </para>

  <para>
   Wenn die Placement Groups permanent hängen bleiben, müssen Sie die Ausgabe von <command>ceph osd tree</command> überprüfen. Die Ausgabe sollte eine Baumstruktur aufweisen, ähnlich dem Beispiel in <xref linkend="storage-bp-recover-osddown"/>.
  </para>

  <para>
   Wenn die Ausgabe von <command>ceph osd tree</command> eher flach ist wie im folgenden Beispiel
  </para>

<screen>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</screen>

  <para>
   dann sollten Sie prüfen, ob die entsprechende CRUSH Map eine Baumstruktur aufweist. Wenn diese ebenfalls flach ist oder keine Hosts enthält wie im obigen Beispiel, dann könnte dies bedeuten, dass die Hostnamenauflösung im Cluster nicht korrekt funktioniert.
  </para>

  <para>
   Wenn die Hierarchie inkorrekt ist (beispielsweise wenn der Stamm zwar Hosts enthält, doch die OSDs sich auf der obersten Ebene befinden und keinen Hosts zugewiesen sind), dann müssen Sie die OSDs an die korrekte Stelle in der Hierarchie verschieben. Dies kann mit den Kommandos <command>ceph osd crush move</command> und/oder <command>ceph osd crush set</command> erfolgen. Weitere Detailinformationen hierzu finden Sie in <xref linkend="op-crush"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-osdweight">
  <title>OSD Weight is 0 (OSD-Gewicht ist 0)</title>

  <para>
   Einem OSD wird beim Starten ein Gewicht zugewiesen. Je höher das Gewicht, desto größer ist die Chance, dass der Cluster Daten an den OSD schreibt. Das Gewicht wird entweder in einer Cluster CRUSH Map angegeben oder vom Startskript des OSDs berechnet.
  </para>

  <para>
   In einigen Fällen kann der berechnete Wert für das Gewicht des OSDs auf Null abgerundet werden. Dies bedeutet, dass der OSD nicht zum Speichern von Daten geplant ist und keine Daten an ihn geschrieben werden. Normalerweise besteht der Grund dafür darin, dass die Festplatte zu klein ist (kleiner als 15 GB) und durch eine größere ersetzt werden sollte.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-osddown">
  <title>OSD is Down (OSD ist inaktiv)</title>

  <para>
   Der OSD-Daemon wird entweder ausgeführt oder ist gestoppt/inaktiv. Es gibt drei allgemeine Gründe dafür, dass ein OSD inaktiv ist:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Festplattenfehler.
    </para>
   </listitem>
   <listitem>
    <para>
     Der OSD ist abgestürzt.
    </para>
   </listitem>
   <listitem>
    <para>
     Der Server ist abgestürzt.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Den detaillierten Status von OSDs sehen Sie durch Ausführen von
  </para>

<screen>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</screen>

  <para>
   Die Beispielauflistung zeigt, dass <literal>osd.2</literal> inaktiv ist. Prüfen Sie dann, ob die Festplatte, auf der sich der OSD befindet, eingehängt ist:
  </para>

<screen>lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</screen>

  <para>
   Den Grund für die Inaktivität des OSDs finden Sie in der Protokolldatei <filename>/var/log/ceph/ceph-osd.2.log</filename>. Nachdem Sie den Grund dafür, dass der OSD nicht ausgeführt wird, gefunden und beseitigt haben, starten Sie ihn mit
  </para>

<screen>sudo systemctl start ceph-osd@2.service</screen>

  <para>
   Vergessen Sie nicht, <literal>2</literal> durch die tatsächliche Nummer Ihres gestoppten OSDs zu ersetzen.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-performance-slowosd">
  <title>Suchen langsamer OSDs</title>

  <para>
   Beim Abstimmen der Cluster-Leistung ist es sehr wichtig, langsame Speicher/OSDs im Cluster zu erkennen. Wenn die Daten nämlich an die langsam(st)e Festplatte geschrieben werden, verlangsamt sich der gesamte Schreibvorgang, da immer gewartet wird, bis der Vorgang auf allen entsprechenden Festplatten beendet ist.
  </para>

  <para>
   Es ist nicht unbedeutend, den Speicherengpass zu finden. Sie müssen jeden einzelnen OSD untersuchen, um herauszufinden, welche OSDs den Schreibvorgang verlangsamen. Führen Sie zum Ausführen eines Vergleichs auf einem einzelnen OSD Folgendes aus:
  </para>

<screen role="ceph_tell_osd_bench"><command>ceph tell</command> osd.<replaceable>OSD_ID_NUMBER</replaceable> bench</screen>

  <para>
   Beispiel:
  </para>

<screen><prompt>root # </prompt>ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</screen>

  <para>
   Dann müssen Sie dieses Kommando auf jedem OSD ausführen und den Wert <literal>bytes_per_sec</literal> vergleichen, um die langsam(st)en OSDs zu erkennen.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-clockskew">
  <title>Beheben von Taktversatzwarnungen</title>

  <para>
   Die Zeitinformationen in allen Cluster Nodes müssen synchronisiert werden. Wenn die Uhrzeit eines Nodes nicht vollständig synchronisiert ist, erhalten Sie möglicherweise Taktversatzwarnungen, wenn Sie den Zustand des Clusters überprüfen.
  </para>

  <para>
   Die Zeitsynchronisierung wird mit NTP verwaltet (weitere Informationen hierzu finden Sie unter <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol"/>). Legen Sie jeden Node so fest, dass seine Uhrzeit mit einem oder mehreren NTP-Servern synchronisiert wird, vorzugsweise für dieselbe Gruppe von NTP-Servern. Wenn der Zeitversatz an einem Node weiterhin besteht, führen Sie die folgenden Schritte aus, um das Problem zu beheben:
  </para>

<screen>systemctl stop ntpd.service
systemctl stop ceph-mon.target
systemctl start ntpd.service
systemctl start ceph-mon.target</screen>

  <para>
   Fragen Sie dann die NTP-Peers ab und prüfen Sie den Zeitversatz mit <command>sudo ntpq -p</command>.
  </para>

  <para>
   Die Uhren der Ceph Monitors müssen untereinander auf 0,05 Sekunden synchronisiert sein. Weitere Informationen finden Sie in <xref linkend="Cluster-Time-Setting"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-performance-net-issues">
  <title>Schlechte Cluster-Leistung durch Netzwerkprobleme</title>

  <para>
   Es kann mehrere Gründe dafür geben, dass sich die Cluster-Leistung verschlechtert. Einer davon können Netzwerkprobleme sein. In diesem Fall bemerken Sie möglicherweise, dass der Cluster sein Quorum erreicht, der OSD und die Monitor Nodes offline gehen, die Datenübertragungen lange dauern oder oft versucht wird, die Verbindung neu herzustellen.
  </para>

  <para>
   Sehen Sie sich die Protokolldateien im Verzeichnis <filename>/var/log/ceph</filename> an, um zu prüfen, ob sich die Cluster-Leistung aufgrund von Netzwerkproblemen verschlechtert hat.
  </para>

  <para>
   Konzentrieren Sie sich auf die folgenden Punkte, um Netzwerkprobleme am Cluster zu beheben:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Allgemeine Netzwerkdiagnose. Versuchen Sie, mit der Diagnose-Tool-Ausführung <literal>net.ping</literal> von DeepSea zwischen den Cluster Nodes zu pingen und dadurch zu sehen, ob einzelne Schnittstellen spezifische Schnittstellen erreichen und wie die durchschnittliche Antwortzeit ist. Eine spezifische Antwortzeit, die viel länger ist als die durchschnittliche, wird ebenfalls gemeldet. Beispiel:
    </para>
<screen><prompt>root@master # </prompt>salt-run net.ping
  Succeeded: 8 addresses from 7 minions average rtt 0.15 ms</screen>
    <para>
     Versuchen Sie, alle Schnittstellen mit der JumboFrame-Aktivierung zu validieren:
    </para>
<screen><prompt>root@master # </prompt>salt-run net.jumbo_ping
  Succeeded: 8 addresses from 7 minions average rtt 0.26 ms</screen>
   </listitem>
   <listitem>
    <para>
     Netzwerkleistungsvergleich. Versuchen Sie, mit der Netzwerkleistungsausführung <literal>net.iperf</literal> von DeepSee die Node-interne Netzwerkbandbreite zu testen. In einem vorliegenden Cluster Node werden einige <command>iperf</command>-Prozesse (entsprechend der Anzahl von CPU-Kernen) als Server gestartet. Die restlichen Cluster Nodes werden als Clients verwendet, um Netzwerkdatenverkehr zu generieren. Die akkumulierte Bandbreite aller <command>iperf</command>-Prozesse pro Node wird gemeldet. Dies sollte den maximal erreichbaren Netzwerkdurchsatz in allen Cluster Nodes widerspiegeln. Beispiel:
    </para>
<screen><prompt>root@master # </prompt>salt-run net.iperf cluster=ceph output=full
192.168.128.1:
    8644.0 Mbits/sec
192.168.128.2:
    10360.0 Mbits/sec
192.168.128.3:
    9336.0 Mbits/sec
192.168.128.4:
    9588.56 Mbits/sec
192.168.128.5:
    10187.0 Mbits/sec
192.168.128.6:
    10465.0 Mbits/sec</screen>
   </listitem>
   <listitem>
    <para>
     Prüfen Sie die Firewalleinstellungen in den Cluster Nodes. Stellen Sie sicher, dass die von Ceph benötigten Ports/Protokolle nicht blockiert werden. In <xref linkend="storage-bp-net-firewall"/> finden Sie weitere Informationen zu Firewalleinstellungen.
    </para>
   </listitem>
   <listitem>
    <para>
     Prüfen Sie, ob die Netzwerkhardware wie Netzwerkkarten, Kabel oder Schalter ordnungsgemäß funktionieren.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Separates Netzwerk</title>
   <para>
    Richten Sie ein separates Netzwerk exklusiv für die Cluster OSD und Monitor Nodes ein, um eine schnelle und sichere Netzwerkkommunikation sicherzustellen.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="trouble-jobcache">
  <title><filename>/var</filename> überfüllt</title>

  <para>
   Der Salt Master speichert standardmäßig die Rückgabe aller Minions für jeden Auftrag in seinem <emphasis>Auftrags-Cache</emphasis>. Im Cache können dann später die Ergebnisse zu früheren Aufträgen nachgesehen werden. Das Cache-Verzeichnis ist standardmäßig <filename>/var/cache/salt/master/jobs/</filename>.
  </para>

  <para>
   Jede Auftragsrückgabe von jedem Minion wird in einer einzelnen Datei gespeichert. Im Lauf der Zeit kann dieses Verzeichnis sehr groß werden, abhängig von der Anzahl der veröffentlichten Aufträge und des Werts der Option <option>keep_jobs</option> in der Datei <filename>/etc/salt/master</filename>. Mit <option>keep_jobs</option> wird festgelegt, wie lange in Stunden (standardmäßig 24) Informationen zu früheren Minion-Aufträgen beibehalten werden.
  </para>

<screen>keep_jobs: 24</screen>

  <important>
   <title><option>keep_jobs nicht auf 0 festlegen</option></title>
   <para>
    Wenn <option>keep_jobs</option> auf "0" festgelegt wird, dann wird die Auftrags-Cache-Bereinigung <emphasis>niemals</emphasis> durchgeführt, was möglicherweise zu einer vollen Partition führt.
   </para>
  </important>

  <para>
   Wenn Sie den Auftrags-Cache deaktivieren möchten, legen Sie <option>job_cache</option> auf "False" fest:
  </para>

<screen>job_cache: False</screen>

  <tip>
   <title>Wiederherstellen einer Partition, die aufgrund des Auftrags-Cache voll ist</title>
   <para>
    Wenn die Partition mit Auftrags-Cache-Dateien aufgrund einer falschen <option>keep_jobs</option>-Einstellung voll wird, führen Sie die folgenden Schritte aus, um Speicherplatz freizugeben und die Einstellungen für den Auftrags-Cache zu verbessern:
   </para>
   <procedure>
    <step>
     <para>
      Stoppen Sie das Salt Master-Gerät:
     </para>
<screen><prompt>root@master # </prompt>systemctl stop salt-master</screen>
    </step>
    <step>
     <para>
      Ändern Sie die Salt Master-Konfiguration in Bezug auf den Auftrags-Cache, indem Sie <filename>/etc/salt/master</filename> bearbeiten:
     </para>
<screen>job_cache: False
keep_jobs: 1</screen>
    </step>
    <step>
     <para>
      Löschen Sie den Salt Master-Auftrags-Cache
     </para>
<screen>rm -rfv /var/cache/salt/master/jobs/*</screen>
    </step>
    <step>
     <para>
      Starten Sie den Salt Master-Service:
     </para>
<screen><prompt>root@master # </prompt>systemctl start salt-master</screen>
    </step>
   </procedure>
  </tip>
 </sect1>
</chapter>
