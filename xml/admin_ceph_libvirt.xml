<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-libvirt">
 <title>Using &libvirt; with &ceph;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The &libvirt; library creates a virtual machine abstraction layer between
  hypervisor interfaces and the software applications that use them. With
  &libvirt;, developers and system administrators can focus on a common
  management framework, common API, and common shell interface
  (<command>virsh</command>) to many different hypervisors, including
  &qemu;/&kvm;, &xen;, LXC, or VirtualBox.
 </para>
 <para>
  &ceph; block devices support &qemu;/&kvm;. You can use &ceph; block devices
  with software that interfaces with &libvirt;. The cloud solution uses
  &libvirt; to interact with &qemu;/&kvm;, and &qemu;/&kvm; interacts with Ceph
  block devices via <systemitem>librbd</systemitem>.
 </para>
 <para>
  To create VMs that use &ceph; block devices, use the procedures in the
  following sections. In the examples, we have used
  <literal>libvirt-pool</literal> for the pool name,
  <literal>client.libvirt</literal> for the user name, and
  <literal>new-libvirt-image</literal> for the image name. You may use any
  value you like, but ensure you replace those values when executing commands
  in the subsequent procedures.
 </para>
 <sect1 xml:id="ceph-libvirt-cfg-ceph">
  <title>Configuring &ceph;</title>

  <para>
   To configure &ceph; for use with &libvirt;, perform the following steps:
  </para>

  <procedure>
   <step>
    <para>
     Create a pool. The following example uses the pool name
     <literal>libvirt-pool</literal> with 128 placement groups.
    </para>
<screen>&prompt.cephuser;ceph osd pool create libvirt-pool 128 128</screen>
    <para>
     Verify that the pool exists.
    </para>
<screen>&prompt.cephuser;ceph osd lspools</screen>
   </step>
   <step>
    <para>
     Create a &ceph; User. The following example uses the &ceph; user name
     <literal>client.libvirt</literal> and references
     <literal>libvirt-pool</literal>.
    </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.libvirt mon 'profile rbd' osd \
 'profile rbd pool=libvirt-pool'</screen>
    <para>
     Verify the name exists.
    </para>
<screen>&prompt.cephuser;ceph auth list</screen>
    <note>
     <title>User Name or ID</title>
     <para>
      &libvirt; will access &ceph; using the ID <literal>libvirt</literal>, not
      the &ceph; name <literal>client.libvirt</literal>. See
      <xref linkend="cephx-user"/> for a detailed explanation of the difference
      between ID and name.
     </para>
    </note>
   </step>
   <step>
    <para>
     Use &qemu; to create an image in your RBD pool. The following example uses
     the image name <literal>new-libvirt-image</literal> and references
     <literal>libvirt-pool</literal>.
    </para>
    <tip>
     <title>Keyring File Location</title>
     <para>
      The &libvirt; user key is stored in a keyring file placed in the
      <filename>/etc/ceph</filename> directory. The keyring file needs to have
      an appropriate name that includes the name of the &ceph; cluster it
      belongs to. For the default cluster name 'ceph', the keyring file name is
      <filename>/etc/ceph/ceph.client.libvirt.keyring</filename>.
     </para>
     <para>
      If the keyring does not exist, create it with:
     </para>
<screen>&prompt.cephuser;ceph auth get client.libvirt &gt; /etc/ceph/ceph.client.libvirt.keyring</screen>
    </tip>
<screen>&prompt.root;qemu-img create -f raw rbd:libvirt-pool/new-libvirt-image:id=libvirt 2G</screen>
    <para>
     Verify the image exists.
    </para>
<screen>&prompt.cephuser;rbd -p libvirt-pool ls</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-libvirt-virt-manager">
  <title>Preparing the VM Manager</title>

  <para>
   You may use &libvirt; without a VM manager, but you may find it simpler to
   create your first domain with <command>virt-manager</command>.
  </para>

  <procedure>
   <step>
    <para>
     Install a virtual machine manager.
    </para>
<screen>&prompt.root;zypper in virt-manager</screen>
   </step>
   <step>
    <para>
     Prepare/download an OS image of the system you want to run virtualized.
    </para>
   </step>
   <step>
    <para>
     Launch the virtual machine manager.
    </para>
<screen>virt-manager</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-libvirt-create-vm">
  <title>Creating a VM</title>

  <para>
   To create a VM with <command>virt-manager</command>, perform the following
   steps:
  </para>

  <procedure>
   <step>
    <para>
     Choose the connection from the list, right-click it, and select
     <guimenu>New</guimenu>.
    </para>
   </step>
   <step>
    <para>
     <guimenu>Import existing disk image</guimenu> by providing the path to the
     existing storage. Specify OS type, memory settings, and
     <guimenu>Name</guimenu> the virtual machine, for example
     <literal>libvirt-virtual-machine</literal>.
    </para>
   </step>
   <step>
    <para>
     Finish the configuration and start the VM.
    </para>
   </step>
   <step>
    <para>
     Verify that the newly created domain exists with <command>sudo virsh
     list</command>. If needed, specify the connection string, such as
    </para>
<screen role="ceph.libvirt.create_vm1"><command>virsh -c qemu+ssh://root@vm_host_hostname/system list</command>
Id    Name                           State
-----------------------------------------------
[...]
 9     libvirt-virtual-machine       running</screen>
   </step>
   <step>
    <para>
     Log in to the VM and stop it before configuring it for use with &ceph;.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-libvirt-cfg-vm">
  <title>Configuring the VM</title>

  <para>
   In this chapter, we focus on configuring VMs for integration with &ceph;
   using <command>virsh</command>. <command>virsh</command> commands often
   require root privileges (<command>sudo</command>) and will not return
   appropriate results or notify you that root privileges are required. For a
   reference of <command>virsh</command> commands, refer to <command>man 1
   virsh</command> (requires the package <package>libvirt-client</package> to
   be installed).
<!-- 2018-07-31 tbazant: commenting as we allow in-house links only
   <link xlink:href="http://www.libvirt.org/virshcmdref.html">Virsh Command Reference</link>.
   -->
  </para>

  <procedure>
   <step>
    <para>
     Open the configuration file with <command>virsh edit</command>
     <replaceable>vm-domain-name</replaceable>.
    </para>
<screen>&prompt.root;virsh edit libvirt-virtual-machine</screen>
   </step>
   <step>
    <para>
     Under &lt;devices&gt; there should be a &lt;disk&gt; entry.
    </para>
<screen>&lt;devices&gt;
    &lt;emulator&gt;/usr/bin/qemu-system-<replaceable>SYSTEM-ARCH</replaceable>&lt;/emulator&gt;
    &lt;disk type='file' device='disk'&gt;
      &lt;driver name='qemu' type='raw'/&gt;
      &lt;source file='/path/to/image/recent-linux.img'/&gt;
      &lt;target dev='vda' bus='virtio'/&gt;
      &lt;address type='drive' controller='0' bus='0' unit='0'/&gt;
    &lt;/disk&gt;</screen>
    <para>
     Replace <filename>/path/to/image/recent-linux.img</filename> with the path
     to the OS image.
    </para>
    <important>
     <para>
      Use <command>sudo virsh edit</command> instead of a text editor. If you
      edit the configuration file under <filename>/etc/libvirt/qemu</filename>
      with a text editor, &libvirt; may not recognize the change. If there is a
      discrepancy between the contents of the XML file under
      <filename>/etc/libvirt/qemu</filename> and the result of <command>sudo
      virsh dumpxml</command> <replaceable>vm-domain-name</replaceable>, then
      your VM may not work properly.
     </para>
    </important>
   </step>
   <step>
    <para>
     Add the &ceph; RBD image you previously created as a &lt;disk&gt; entry.
    </para>
<screen>&lt;disk type='network' device='disk'&gt;
        &lt;source protocol='rbd' name='libvirt-pool/new-libvirt-image'&gt;
                &lt;host name='<replaceable>monitor-host</replaceable>' port='6789'/&gt;
        &lt;/source&gt;
        &lt;target dev='vda' bus='virtio'/&gt;
&lt;/disk&gt;</screen>
    <para>
     Replace <replaceable>monitor-host</replaceable> with the name of your
     host, and replace the pool and/or image name as necessary. You may add
     multiple &lt;host&gt; entries for your &ceph; monitors. The
     <literal>dev</literal> attribute is the logical device name that will
     appear under the <filename>/dev</filename> directory of your VM. The
     optional bus attribute indicates the type of disk device to emulate. The
     valid settings are driver specific (for example ide, scsi, virtio, xen,
     usb or sata).
    </para>
   </step>
   <step>
    <para>
     Save the file.
    </para>
   </step>
   <step>
    <para>
     If your &ceph; cluster has authentication enabled (it does by default),
     you must generate a secret. Open an editor of your choice and create a
     file called <filename>secret.xml</filename> with the following content:
    </para>
<screen>&lt;secret ephemeral='no' private='no'&gt;
        &lt;usage type='ceph'&gt;
                &lt;name&gt;client.libvirt secret&lt;/name&gt;
        &lt;/usage&gt;
&lt;/secret&gt;</screen>
   </step>
   <step>
    <para>
     Define the secret.
    </para>
<screen>&prompt.root;virsh secret-define --file secret.xml
&lt;uuid of secret is output here&gt;</screen>
   </step>
   <step>
    <para>
     Get the <literal>client.libvirt</literal> key and save the key string to a
     file.
    </para>
<screen>&prompt.cephuser;ceph auth get-key client.libvirt | sudo tee client.libvirt.key</screen>
   </step>
   <step>
    <para>
     Set the UUID of the secret.
    </para>
<screen>&prompt.root;virsh secret-set-value --secret <replaceable>uuid of secret</replaceable> \
--base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</screen>
    <para>
     You must also set the secret manually by adding the following
     <literal>&lt;auth&gt;</literal> entry to the
     <literal>&lt;disk&gt;</literal> element you entered earlier (replacing the
     uuid value with the result from the command line example above).
    </para>
<screen>&prompt.root;virsh edit libvirt-virtual-machine</screen>
    <para>
     Then, add <literal>&lt;auth&gt;&lt;/auth&gt;</literal> element to the
     domain configuration file:
    </para>
<screen>...
&lt;/source&gt;
&lt;auth username='libvirt'&gt;
        &lt;secret type='ceph' uuid='9ec59067-fdbc-a6c0-03ff-df165c0587b8'/&gt;
&lt;/auth&gt;
&lt;target ...</screen>
    <note>
     <para>
      The exemplary ID is <literal>libvirt</literal>, not the &ceph; name
      <literal>client.libvirt</literal> as generated at step 2 of
      <xref linkend="ceph-libvirt-cfg-ceph"/>. Ensure you use the ID component
      of the Ceph name you generated. If for some reason you need to regenerate
      the secret, you will need to execute <command>sudo virsh
      secret-undefine</command> <replaceable>uuid</replaceable> before
      executing <command>sudo virsh secret-set-value</command> again.
     </para>
    </note>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-libvirt-summary">
  <title>Summary</title>

  <para>
   Once you have configured the VM for use with &ceph;, you can start the VM.
   To verify that the VM and &ceph; are communicating, you may perform the
   following procedures.
  </para>

  <procedure>
   <step>
    <para>
     Check to see if Ceph is running:
    </para>
<screen>&prompt.cephuser;ceph health</screen>
   </step>
   <step>
    <para>
     Check to see if the VM is running:
    </para>
<screen>&prompt.root;virsh list</screen>
   </step>
   <step>
    <para>
     Check to see if the VM is communicating with Ceph. Replace
     <replaceable>vm-domain-name</replaceable> with the name of your VM domain:
    </para>
<screen>&prompt.root;virsh qemu-monitor-command --hmp <replaceable>vm-domain-name</replaceable> 'info block'</screen>
   </step>
   <step>
    <para>
     Check to see if the device from <literal>&amp;target dev='hdb'
     bus='ide'/&gt;</literal> appears under <filename>/dev</filename> or under
     <filename>/proc/partitions</filename>:
    </para>
<screen>&prompt.user;ls /dev
&prompt.user;cat /proc/partitions</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
