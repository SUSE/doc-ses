<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-rbd">
 <title>&rbd;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like &ceph;.
 </para>
 <para>
  &ceph; block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a &ceph; cluster. &ceph; block
  devices leverage &rados; capabilities such as snapshotting, replication, and
  consistency. &ceph;'s &rbd;s (RBD) interact with OSDs using kernel modules or
  the <systemitem>librbd</systemitem> library.
 </para>
 <figure>
  <title>&rados; Protocol</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  &ceph;'s block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as &qemu;, or
  cloud-based computing systems such as &ostack; that rely on &libvirt;. You
  can use the same cluster to operate the &rgw;, &cephfs;, and &rbd;s
  simultaneously.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Block Device Commands</title>

  <para>
   The <command>rbd</command> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creating a Block Device Image in a Replicated Pool</title>
   <para>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <xref linkend="ceph-pools"/>):
   </para>
<screen>
&prompt.cephuser;rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Image Size Units</title>
    <para>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creating a Block Device Image in an Erasure Coded Pool</title>
   <para>
    As of &productname; 5, it is possible to store data of a block device image
    directly in erasure coded (EC) pools. A &rbd; image consists of
    <emphasis>data</emphasis> and <emphasis>metadata</emphasis> parts. You can
    store only the 'data' part of a &rbd; image in an EC pool. The pool needs
    to have the 'overwrite' flag set to <emphasis>true</emphasis>, and that is
    only possible if all OSDs where the pool is stored use &bluestore;.
   </para>
   <para>
    You cannot store the image's 'metadata' part in an EC pool. You need to
    specify the replicated pool for storing the image's metadata with the
    <option>--pool=</option> option of the <command>rbd create</command>
    command.
   </para>
   <para>
    Use the following steps to create an RBD image in a newly created EC pool:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> osd pool create <replaceable>POOL_NAME</replaceable> 12 12 erasure
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> allow_ec_overwrites true

#Metadata will reside in pool "<replaceable>OTHER_POOL</replaceable>", and data in pool "<replaceable>POOL_NAME</replaceable>"
&prompt.cephuser;<command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable> --pool=<replaceable>OTHER_POOL</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Listing Block Device Images</title>
   <para>
    To list block devices in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Retrieving Image Information</title>
   <para>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Resizing a Block Device Image</title>
   <para>
    &rbd; images are thin provisioned&mdash;they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <option>--size</option> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </para>
<screen>
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Removing a Block Device Image</title>
   <para>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Mounting and Unmounting</title>

  <para>
   After you create a &rbd;, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </para>
  <para>
   The <command>rbd</command> command defaults to accessing the cluster using
   the &ceph; <literal>admin</literal> user account. This account has full
   administrative access to the cluster. This runs the risk of accidentally
   causing damage, similarly to logging into a Linux workstation as &rootuser;.
   Thus, it is preferable to create user accounts with fewer privileges and use
   these accounts for normal read/write &rbd; access.
  </para>
   
  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Creating a &ceph; User Account</title>
   <para>
    To create a new user account with &mgr;, &mon;, and &osd; capabilities, use
    the <command>ceph</command> command with the <command>auth get-or-create</command>
    subcommand:
   </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    For example, to create a user called <replaceable>qemu</replaceable> with read-write
    access to the pool <replaceable>vms</replaceable> and read-only access to the pool
    <replaceable>images</replaceable>, execute the following:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    The output from the <command>ceph auth get-or-create</command> command will
    be the keyring for the specified user, which can be written to
    <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     When using the <command>rbd</command> command, you can specify the user ID by providing the
     optional <command>--id</command> <replaceable>ID</replaceable> argument. 
    </para>
   </note>
   <para>
    For more details on managing &ceph; user accounts, refer to <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>
  
  <sect2 xml:id="ceph-rbd-auth">
   <title>User Authentication</title>
   <para>
    To specify a user name, use <option>--id
     <replaceable>user-name</replaceable></option>. If you use
    <systemitem>cephx</systemitem> authentication, you also need to specify a
    secret. It may come from a keyring or a file containing the secret:
   </para>
   <screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    or
   </para>
   <screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>
  
  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparing a &rbd; for Use</title>
   
   <procedure>
    <step>
     <para>
      Make sure your &ceph; cluster includes a pool with the disk image you want
      to map. Assume the pool is called <literal>mypool</literal> and the image
      is <literal>myimage</literal>.
     </para>
     <screen>&prompt.cephuser;rbd list mypool</screen>
    </step>
    <step>
     <para>
      Map the image to a new block device:
     </para>
     <screen>&prompt.cephuser;rbd map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>&prompt.cephuser;rbd showmapped
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      The device we want to work on is <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>RBD Device Path</title>
      <para>
       Instead of
       <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>,
       you can use
       <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename>
       as a persistent device path. For example:
      </para>
      <screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Make an XFS file system on the <filename>/dev/rbd0</filename> device:
     </para>
     <screen>&prompt.root;mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Replacing <filename>/mnt</filename> with your mount point, mount the
      device and check it is correctly mounted: 
     </para>
     <screen>&prompt.root;mount /dev/rbd0 /mnt
      &prompt.root;mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Now you can move data to and from the device as if it was a local
      directory.
     </para>
     <tip>
      <title>Increasing the Size of RBD Device</title>
      <para>
       If you find that the size of the RBD device is no longer enough, you can
       easily increase it.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Increase the size of the RBD image, for example up to 10 GB.
        </para>
        <screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Grow the file system to fill up the new size of the device:
        </para>
<screen>&prompt.root;xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      After you finish accessing the device, you can unmap and unmount it.
     </para>
<screen>
&prompt.cephuser;rbd unmap /dev/rbd0
&prompt.root;unmount /mnt
</screen>
    </step>
   </procedure>
   
   <tip>
    <title>Manual Mounting and Unmounting</title>
    <para>
     A <command>rbdmap</command> script and &systemd; unit is provided to make
     the process of mapping and mounting RBDs after boot and unmounting them 
     before shutdown smoother. Refer to <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>
  
  <sect2 xml:id="ceph-rbd-rbdmap">
   <title>rbdmap: Map RBD Devices at Boot Time</title>
   <para>
    <command>rbdmap</command> is a shell script that automates <command>rbd
    map</command> and <command>rbd unmap</command> operations on one or more
    RBD images. Although you can run the script manually at any time, the main
    advantage is automatic mapping and mounting of RBD images at boot time (and
    unmounting and unmapping at shutdown), as triggered by the Init system. A
    &systemd; unit file, <filename>rbdmap.service</filename> is included with
    the <systemitem>ceph-common</systemitem> package for this purpose.
   </para>
   <para>
    The script takes a single argument, which can be either
    <option>map</option> or <option>unmap</option>. In either case, the script
    parses a configuration file. It defaults to
    <filename>/etc/ceph/rbdmap</filename>, but can be overridden via an
    environment variable <literal>RBDMAPFILE</literal>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </para>
   <para>
    The configuration file has the following format:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Path to an image within a pool. Specify as
       <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       An optional list of parameters to be passed to the underlying
       <command>rbd map</command> command. These parameters and their values
       should be specified as a comma-separated string, for example:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       The example makes the <command>rbdmap</command> script run the following
       command:
      </para>
<screen>&prompt.cephuser;rbd map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </para>
<screen>&prompt.cephuser;rbdmap map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    When run as <command>rbdmap map</command>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using <command>the rbd map</command> command) and then mount
    the image.
   </para>
   <para>
    When run as <command>rbdmap unmap</command>, images listed in the
    configuration file will be unmounted and unmapped.
   </para>
   <para>
    <command>rbdmap unmap-all</command> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </para>
   <para>
    If successful, the rbd map operation maps the image to a /dev/rbdX device,
    at which point a udev rule is triggered to create a friendly device name
    symbolic link
    <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename>
    pointing to the real mapped device.
   </para>
   <para>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <filename>/etc/fstab</filename>.
    When writing <filename>/etc/fstab</filename> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early&mdash;before the device in
    question even exists, as <filename>rbdmap.service</filename> is typically
    triggered quite late in the boot sequence.
   </para>
   <para>
    For a complete list of <command>rbd</command> options, see the
    <command>rbd</command> manual page (<command>man 8 rbd</command>).
   </para>
   <para>
    For examples of the <command>rbdmap</command> usage, see the
    <command>rbdmap</command> manual page (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2>
   <title>Increasing the Size of RBD Device</title>
   <para>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Increase the size of the RBD image, for example up to 10GB.
     </para>
<screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Grow the file system to fill up the new size of the device.
     </para>
<screen>&prompt.root;xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Snapshots</title>

  <para>
   An RBD snapshot is a snapshot of a &rbd; image. With snapshots, you retain a
   history of the image's state. &ceph; also supports snapshot layering, which
   allows you to clone VM images quickly and easily. &ceph; supports block
   device snapshots using the <command>rbd</command> command and many
   higher-level interfaces, including &qemu;, <systemitem>libvirt</systemitem>,
   &ostack;, and CloudStack.
  </para>

  <note>
   <para>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </para>
  </note>

  <sect2>
   <title>Cephx Notes</title>
   <para>
    When <systemitem>cephx</systemitem> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <xref linkend="cha-storage-cephx"/> for more details. You may
    also add the <systemitem>CEPH_ARGS</systemitem> environment variable to
    avoid re-entry of the following parameters.
   </para>
<screen>&prompt.cephuser;rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
     environment variable so that you do not need to enter them each time.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Snapshot Basics</title>
   <para>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <command>rbd</command> command on the command line.
   </para>
   <sect3>
    <title>Create Snapshot</title>
    <para>
     To create a snapshot with <command>rbd</command>, specify the <option>snap
     create</option> option, the pool name, and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap create --snap snapshot1 image1
&prompt.cephuser;rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>List Snapshots</title>
    <para>
     To list snapshots of an image, specify the pool name and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap ls image1
&prompt.cephuser;rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Rollback Snapshot</title>
    <para>
     To rollback to a snapshot with <command>rbd</command>, specify the
     <option>snap rollback</option> option, the pool name, the image name, and
     the snapshot name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rollback --snap snapshot1 image1
&prompt.cephuser;rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <emphasis>faster to
      clone</emphasis> from a snapshot <emphasis>than to rollback</emphasis> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Delete a Snapshot</title>
    <para>
     To delete a snapshot with <command>rbd</command>, specify the <option>snap
     rm</option> option, the pool name, the image name, and the user name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rm --snap snapshot1 image1
&prompt.cephuser;rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      &ceph; OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Purge Snapshots</title>
    <para>
     To delete all snapshots for an image with <command>rbd</command>, specify
     the <option>snap purge</option> option and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap purge image1
&prompt.cephuser;rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Layering</title>
   <para>
    &ceph; supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables &ceph; block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics&mdash;making it possible to create clones rapidly.
   </para>
   <note>
    <para>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a &ceph; block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </para>
   </note>
   <para>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </para>
   <para>
    A COW clone of a snapshot behaves exactly like any other &ceph; block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <emphasis>must</emphasis> protect the snapshot before you clone it.
   </para>
   <note>
    <title><option>--image-format 1</option> Not Supported</title>
    <para>
     You cannot create snapshots of images created with the deprecated
     <command>rbd create --image-format 1</command> option. &ceph; only
     supports cloning of the default <emphasis>format 2</emphasis> images.
    </para>
   </note>
   <sect3>
    <title>Getting Started with Layering</title>
    <para>
     &ceph; block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </para>
    <para>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image Template</emphasis>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, &sls;), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <command>zypper ref &amp;&amp; zypper patch</command>
       followed by <command>rbd snap create</command>). As the image matures,
       the user can clone any one of the snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Extended Template</emphasis>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Template Pool</emphasis>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image Migration/Recovery</emphasis>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Protecting a Snapshot</title>
    <para>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap protect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      You cannot delete a protected snapshot.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cloning a Snapshot</title>
    <para>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
&prompt.cephuser;rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Unprotecting a Snapshot</title>
    <para>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <emphasis>not</emphasis> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Listing Children of a Snapshot</title>
    <para>
     To list the children of a snapshot, execute the following:
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
&prompt.cephuser;rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 children --image image1 --snap snapshot1
&prompt.cephuser;rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten">
    <title>Flattening a Cloned Image</title>
    <para>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
&prompt.cephuser;rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 flatten --image image1
&prompt.cephuser;rbd flatten pool1/image1</screen>
    <note>
     <para>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Mirroring</title>

  <para>
   RBD images can be asynchronously mirrored between two &ceph; clusters. This
   capability uses the RBD journaling image feature to ensure crash-consistent
   replication between clusters. Mirroring is configured on a per-pool basis
   within peer clusters and can be configured to automatically mirror all
   images within a pool or only a specific subset of images. Mirroring is
   configured using the <command>rbd</command> command. The
   <systemitem>rbd-mirror</systemitem> daemon is responsible for pulling image
   updates from the remote peer cluster and applying them to the image within
   the local cluster.
  </para>

  <note>
   <title>rbd-mirror Daemon</title>
   <para>
    To use RBD mirroring, you need to have two &ceph; clusters, each running
    the <systemitem>rbd-mirror</systemitem> daemon.
   </para>
  </note>

  <important>
   <title>&rbd;s Exported via &iscsi;</title>
   <para>
    You cannot mirror RBD devices that are exported via &iscsi; using
    kernel-based &igw;.
   </para>
   <para>
    Refer to <xref linkend="cha-ceph-iscsi"/> for more details on &iscsi;.
   </para>
  </important>

  <sect2 xml:id="rbd-mirror-daemon">
   <title>rbd-mirror Daemon</title>
   <para>
    The two <systemitem>rbd-mirror</systemitem> daemons are responsible for
    watching image journals on the remote, peer cluster and replaying the
    journal events against the local cluster. The RBD image journaling feature
    records all modifications to the image in the order they occur. This
    ensures that a crash-consistent mirror of the remote image is available
    locally.
   </para>
   <para>
    The <systemitem>rbd-mirror</systemitem> daemon is available in the
    <package>rbd-mirror</package> package. You can install the package on OSD
    nodes, gateway nodes, or even on dedicated nodes. We do not recommend
    installing the <package>rbd-mirror</package> on the &adm;. Install, enable,
    and start <package>rbd-mirror</package>:
   </para>
<screen>&prompt.sminion;zypper install rbd-mirror
&prompt.sminion;systemctl enable ceph-rbd-mirror@<replaceable>server_name</replaceable>.service
&prompt.sminion;systemctl start ceph-rbd-mirror@<replaceable>server_name</replaceable>.service</screen>
   <important>
    <para>
     Each <systemitem>rbd-mirror</systemitem> daemon requires the ability to
     connect to both clusters simultaneously.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Pool Configuration</title>
   <para>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <command>rbd</command> command. Mirroring is configured on a per-pool basis
    within the &ceph; clusters.
   </para>
   <para>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named 'local' and 'remote', are
    accessible from a single host for clarity.
   </para>
   <para>
    See the <command>rbd</command> manual page (<command>man 8 rbd</command>)
    for additional details on how to connect to different &ceph; clusters.
   </para>
   <tip>
    <title>Multiple Clusters</title>
    <para>
     The cluster name in the following examples corresponds to a &ceph;
     configuration file of the same name
     <filename>/etc/ceph/remote.conf</filename>.
    </para>
   </tip>
   <sect3>
    <title>Enable Mirroring on a Pool</title>
    <para>
     To enable mirroring on a pool, specify the <command>mirror pool
     enable</command> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        Mirroring needs to be explicitly enabled on each image. See
        <xref linkend="rbd-mirror-enable-image-mirroring"/> for more
        information.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
&prompt.cephuser;rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3>
    <title>Disable Mirroring</title>
    <para>
     To disable mirroring on a pool, specify the <command>mirror pool
     disable</command> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
&prompt.cephuser;rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Add Cluster Peer</title>
    <para>
     In order for the <systemitem>rbd-mirror</systemitem> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <command>mirror pool peer
     add</command> subcommand, the pool name, and a cluster specification:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool peer add <replaceable>POOL_NAME</replaceable> client.remote@remote
&prompt.cephuser;rbd --cluster remote mirror pool peer add <replaceable>POOL_NAME</replaceable> client.local@local</screen>
   </sect3>
   <sect3>
    <title>Remove Cluster Peer</title>
    <para>
     To remove a mirroring peer cluster, specify the <command>mirror pool peer
     remove</command> subcommand, the pool name, and the peer UUID (available
     from the <command>rbd mirror pool info</command> command):
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
&prompt.cephuser;rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Image Configuration</title>
   <para>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer &ceph; cluster.
   </para>
   <para>
    Mirrored RBD images are designated as either <emphasis>primary</emphasis>
    or <emphasis>non-primary</emphasis>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </para>
   <para>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <xref linkend="rbd-mirror-enable-image-mirroring"/>) by the
    <command>rbd</command> command).
   </para>
   <sect3>
    <title>Image Journaling Support</title>
    <para>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. Before an image can be
     mirrored to a peer cluster, the journaling feature must be enabled. The
     feature can be enabled at the time of image creation by providing the
     <option>--image-feature exclusive-lock,journaling</option> option to the
     <command>rbd</command> command.
    </para>
    <para>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <command>feature enable</command> subcommand, the pool and image name, and
     the feature name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Option Dependency</title>
     <para>
      The <option>journaling</option> feature is dependent on the
      <option>exclusive-lock</option> feature. If the
      <option>exclusive-lock</option> feature is not already enabled, you need
      to enable it prior to enabling the <option>journaling</option> feature.
     </para>
    </note>
    <warning>
     <title>Journaling on All New Images</title>
     <para>
      You can enable journaling on all new images by default by appending the
      <literal>journaling</literal> value to the <option>rbd default
      features</option> option in the &ceph; configuration file. For example:
     </para>
<screen>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</screen>
     <para>
      Before applying such a change, carefully consider if enabling journaling
      on all new images is good for your deployment, because it can have a
      negative performance impact.
     </para>
    </warning>
   </sect3>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Enable Image Mirroring</title>
    <para>
     If mirroring is configured in the 'image' mode, then it is necessary to
     explicitly enable mirroring for each image within the pool. To enable
     mirroring for a specific image, specify the <command>mirror image
     enable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Disable Image Mirroring</title>
    <para>
     To disable mirroring for a specific image, specify the <command>mirror
     image disable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Image Promotion and Demotion</title>
    <para>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </para>
    <note>
     <title>Forced Promotion</title>
     <para>
      Promotion can be forced using the <option>--force</option> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <command>resync</command> subcommand is issued.
     </para>
    </note>
    <para>
     To demote a specific image to non-primary, specify the <command>mirror
     image demote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To demote all primary images within a pool to non-primary, specify the
     <command>mirror pool demote</command> subcommand along with the pool name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     To promote a specific image to primary, specify the <command>mirror image
     promote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To promote all non-primary images within a pool to primary, specify the
     <command>mirror pool promote</command> subcommand along with the pool
     name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Split I/O Load</title>
     <para>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </para>
    </tip>
   </sect3>
   <sect3>
    <title>Force Image Resync</title>
    <para>
     If a split-brain event is detected by the
     <systemitem>rbd-mirror</systemitem> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out of date and then request a
     resync to the primary image. To request an image resync, specify the
     <command>mirror image resync</command> subcommand along with the pool and
     image name:
    </para>
<screen>&prompt.cephuser;rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Mirror Status</title>
   <para>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <command>mirror image
    status</command> and <command>mirror pool status</command> subcommands:
   </para>
   <para>
    To request the mirror image status, specify the <command>mirror image
    status</command> subcommand along with the pool and image name:
   </para>
<screen>&prompt.cephuser;rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    To request the mirror pool summary status, specify the <command>mirror pool
    status</command> subcommand along with the pool name:
   </para>
<screen>&prompt.cephuser;rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Adding the <option>--verbose</option> option to the <command>mirror pool
     status</command> subcommand will additionally output status details for
     every mirroring image in the pool.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Cache Settings</title>

  <para>
   The user space implementation of the &ceph; block device
   (<systemitem>librbd</systemitem>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <emphasis>Least
   Recently Used</emphasis> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </para>

  <para>
   &ceph; supports write-back caching for RBD. To enable it, add
  </para>

<screen>
[client]
...
rbd cache = true
</screen>

  <para>
   to the <literal>[client]</literal> section of your
   <filename>ceph.conf</filename> file. By default,
   <systemitem>librbd</systemitem> does not perform any caching. Writes and
   reads go directly to the storage cluster, and writes return only when the
   data is on disk on all replicas. With caching enabled, writes return
   immediately, unless there are more unflushed bytes than set in the
   <option>rbd cache max dirty</option> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </para>

  <para>
   &ceph; supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, set
  </para>

<screen>
rbd cache max dirty = 0
</screen>

  <para>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </para>

  <para>
   The <filename>ceph.conf</filename> file settings for RBD should be set in
   the <literal>[client]</literal> section of your configuration file. The
   settings include:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Enable caching for &rbd; (RBD). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      The RBD cache size in bytes. Default is 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <option>rbd cache max dirty</option> needs to be less than <option>rbd
      cache size</option>. If set to 0, uses write-through caching. Default is
      24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <systemitem>rbd</systemitem>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS Settings</title>

  <para>
   Generally, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </para>

  <important>
   <title>Not Supported by &iscsi;</title>
   <para>
    The following QoS settings are used only by the userspace RBD
    implementation <systemitem class="daemon">librbd</systemitem> and
    <emphasis>not</emphasis> used by the <systemitem>kRBD</systemitem>
    implementation. Because &iscsi; uses <systemitem>kRBD</systemitem>, it does
    not use the QoS settings. However, for &iscsi; you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      The desired limit of read operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Read-ahead Settings</title>

  <para>
   &rbd; supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </para>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Advanced Features</title>

  <para>
   &rbd; supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the &ceph; configuration file by using the
   <option>rbd_default_features</option> option.
  </para>

  <para>
   You can specify the values of the <option>rbd_default_features</option>
   option in two ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     As a sum of features' internal values. Each feature has its own internal
     value&mdash;for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     As a comma-separated list of features. The previous example will look as
     follows:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Features Not Supported by &iscsi;</title>
   <para>
    RBD images with the following features will not be supported by &iscsi;:
    <option>deep-flatten</option>, <option>object-map</option>,
    <option>journaling</option>, <option>fast-diff</option>,
    <option>striping</option>
   </para>
  </note>

  <para>
   A list of advanced RBD features follows:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Layering enables you to use cloning.
     </para>
     <para>
      Internal value is 1, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy &rbd;s.
     </para>
     <para>
      Internal value is 2, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </para>
     <para>
      Internal value is 8, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </para>
     <para>
      Internal value is 16, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      Deep-flatten makes the <command>rbd flatten</command> (see
      <xref linkend="rbd-flatten" />) work on all the snapshots of an image, in
      addition to the image itself. Without it, snapshots of an image will
      still rely on the parent, therefore you will not be able to delete the
      parent image until the snapshots are deleted. Deep-flatten makes a parent
      independent of its clones, even if they have snapshots.
     </para>
     <para>
      Internal value is 32, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <xref linkend="ceph-rbd-mirror" />) uses the journal to replicate a crash
      consistent image to a remote cluster.
     </para>
     <para>
      Internal value is 64, default is 'no'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Mapping RBD Using Old Kernel Clients</title>

  <para>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with &productname; &productnumber; forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 >> \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Changing &crushmap; Bucket Types Causes Massive Rebalancing</title>
   <para>
    If you intend to switch the &crushmap; bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Disable any RBD image features that are not supported. For example:
    </para>
<screen>
&prompt.cephuser;rbd feature disable pool1/image1 object-map
&prompt.cephuser;rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Change the &crushmap; bucket types from 'straw2' to 'straw':
    </para>
    <substeps>
     <step>
      <para>
       Save the &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Decompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Edit the &crushmap; and replace 'straw2' with 'straw'.
      </para>
     </step>
     <step>
      <para>
       Recompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Set the new &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
</chapter>
