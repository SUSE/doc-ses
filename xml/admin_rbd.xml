<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.rbd">
 <title>&rbd;</title>
 <para>
  A block is a sequence of bytes, for example a 512-byte block of data.
  Block-based storage interfaces are the most common way to store data with
  rotating media such as hard disks, CDs, floppy disks. The ubiquity of block
  device interfaces makes a virtual block device an ideal candidate to interact
  with a mass data storage system like &ceph;.
 </para>
 <para>
  &ceph; block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a &ceph; cluster. &ceph; block
  devices leverage &rados; capabilities such as snapshotting, replication and
  consistency. &ceph;'s &rbd;d (RBD) interact with OSDs using kernel modules or
  the <systemitem>librbd</systemitem> library.
 </para>
 <figure>
  <title>&rados; Protocol</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  &ceph;'s block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as &qemu;, or
  cloud-based computing systems such as &ostack; that rely on &libvirt;.You can
  use the same cluster to operate the &rgw;, &cephfs;, and &rbd;s
  simultaneously.
 </para>
 <sect1 xml:id="ceph.rbd.commands">
  <title>Block Device Commands</title>

  <para>
   The <command>rbd</command> command enables you to create, list, introspect
   and remove block device images. You can also use it for example to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </para>

  <tip>
   <title>Access to a Cluster</title>
   <para>
    To use &rbd; commands, you must have access to a running &ceph; cluster.
   </para>
  </tip>

  <sect2 xml:id="ceph.rbd.cmds.create">
   <title>Creating a Block Device Image</title>
   <para>
    Before you can add a block device to a node, you must create an image for
    it in the cluster first. To create a block device image, execute the
    following:
   </para>
<screen>&prompt.root;rbd create --size <replaceable>megabytes</replaceable> <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
   <para>
    For example, to create a 1GB image named 'bar' that stores information in a
    pool named 'swimmingpool', execute the following:
   </para>
<screen>&prompt.root;rbd create --size 1024 swimmingpool/bar</screen>
   <tip>
    <title>Default Pool</title>
    <para>
     If you do not specify a pool when creating an image, it will be stored in
     the default pool 'rbd'.
    </para>
   </tip>
   <note>
    <title></title>
    <para>
     You need to create a pool first before you can specify it as a source. See
     <xref linkend="ceph.pools"/> for more details.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ceph.rbd.cmds.create-ec">
   <title>Creating a Block Device Image in an Erasure Coded Pool</title>
   <para>
    As of &productname; 5 it is possible to store a Block Device Image data
    in Erasure Coded pools. Only the "data" part of an RBD image can be
    stored in an EC pool. Moreover, the EC pool must have the
    "overwrite flag" set to true, and setting this flag to
    true is only possible if all OSDs use Bluestore.
   </para>
   <para>
    The image metadata cannot reside in an EC pool. The metadata can
    reside either in the default "rbd" pool or in the pool that the user
    specifies explicitly with the parameter <parameter>--pool=</parameter>
    in the <command>rbd create</command> command.
   </para>
   <note>
    <title>Bluestore Required</title>
    <para>All Nodes require Bluestore to use Erasure Coded pools for Block Device Images.</para>
   </note>
   <para>Use the following steps to create an RBD image in an EC pool:</para>
<screen>&prompt.root;<command>ceph</command> osd pool create <replaceable>POOL_NAME</replaceable> 12 12 erasure
&prompt.root;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> allow_ec_overwrites true

# Metadata will reside in pool "rbd", and data in pool "<replaceable>POOL_NAME</replaceable>"
&prompt.root;<command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable>

#Metadata will reside in pool "<replaceable>OTHER_POOL</replaceable>", and data in pool "<replaceable>POOL_NAME</replaceable>"
&prompt.root;<command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable> --pool=<replaceable>OTHER_POOL</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph.rbd.cmds.list">
   <title>Listing Block Device Images</title>
   <para>
    To list block devices in the 'rbd' pool, execute the following ('rbd' is
    the default pool name):
   </para>
<screen>&prompt.root;rbd ls</screen>
   <para>
    To list block devices in a pool named 'swimmingpool', execute the
    following:
   </para>
<screen>&prompt.root;rbd ls swimmingpool</screen>
  </sect2>

  <sect2 xml:id="ceph.rbd.cmds.info">
   <title>Retrieving Image Information</title>
   <para>
    To retrieve information from an image 'bar' within a pool named
    'swimmingpool', run the following:
   </para>
<screen>&prompt.root;rbd info swimmingpool/bar</screen>
  </sect2>

  <sect2 xml:id="ceph.rbd.cmds.resize">
   <title>Resizing a Block Device Image</title>
   <para>
    &rbd;s images are thin provisioned&mdash;they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <option>--size</option> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </para>
<screen>&prompt.root;rbd resize --size 2048 foo # to increase
rbd resize --size 2048 foo --allow-shrink # to decrease</screen>
  </sect2>

  <sect2 xml:id="ceph.rbd.cmds.rm">
   <title>Removing a Block Device Image</title>
   <para>
    To remove a block device that corresponds to an image 'bar' in a pool named
    'swimmingpool', run the following:
   </para>
<screen>&prompt.root;rbd rm swimmingpool/bar</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.integration.mount_rbd">
  <title>Mounting and Unmounting RBD Images</title>

  <para>
   After you create &rbd;, you can then format it, mount it to be able to
   exchange files, and unmount it when done.
  </para>

  <procedure>
   <step>
    <para>
     Make sure your &ceph; cluster includes a pool with the disk image you want
     to mount. Assume the pool is called <literal>mypool</literal> and the
     image is <literal>myimage</literal>.
    </para>
<screen>rbd list mypool</screen>
   </step>
   <step>
    <para>
     Map the image to a new block device.
    </para>
<screen>&prompt.root;rbd map --pool mypool myimage</screen>
    <tip>
     <title>User Name and Authentication</title>
     <para>
      To specify a user name, use <option>--id
      <replaceable>user-name</replaceable></option>. Moreover, if you use
      <systemitem>cephx</systemitem> authentication, you must also specify a
      secret. It may come from a keyring or a file containing the secret:
     </para>
<screen>&prompt.root;rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
     <para>
      or
     </para>
<screen>&prompt.root;rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
    </tip>
   </step>
   <step>
    <para>
     List all mapped devices:
    </para>
<screen>&prompt.root;rbd showmapped
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    <para>
     The device we want to work on is <filename>/dev/rbd0</filename>.
    </para>
   </step>
   <step>
    <para>
     Make an XFS file system on the <filename>/dev/rbd0</filename> device.
    </para>
<screen>&prompt.root;mkfs.xfs /dev/rbd0
 log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
 log stripe unit adjusted to 32KiB
 meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
          =                       sectsz=512   attr=2, projid32bit=1
          =                       crc=0        finobt=0
 data     =                       bsize=4096   blocks=2097152, imaxpct=25
          =                       sunit=1024   swidth=1024 blks
 naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
 log      =internal log           bsize=4096   blocks=2560, version=2
          =                       sectsz=512   sunit=8 blks, lazy-count=1
 realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   </step>
   <step>
    <para>
     Mount the device and check it is correctly mounted. Replace
     <filename>/mnt</filename> with your mount point.
    </para>
<screen>&prompt.root;mount /dev/rbd0 /mnt
&prompt.root;mount | grep rbd0
/dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
    <para>
     Now you can move data from/to the device as if it was a local directory.
    </para>
    <tip>
     <title>Increasing the Size of RBD Device</title>
     <para>
      If you find that the size of the RBD device is no longer enough, you can
      easily increase it.
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Increase the size of the RBD image, for example up to 10GB.
       </para>
<screen>&prompt.root;rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
      </listitem>
      <listitem>
       <para>
        Grow the file system to fill up the new size of the device.
       </para>
<screen>&prompt.root;xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
      </listitem>
     </orderedlist>
    </tip>
   </step>
   <step>
    <para>
     After you finish accessing the device, you can unmount it.
    </para>
<screen>&prompt.root;unmount /mnt</screen>
   </step>
  </procedure>

  <tip>
   <title>Manual (Un)mounting</title>
   <para>
    Since manually mapping/mounting RBD images after boot, and
    unmounting/unmapping them before shutdown can be tedious, a
    <command>rbdmap</command> script and &systemd; unit is provided. Refer to
    <command>man 8 rbdmap</command> for more details.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.rbd">
  <title>Block Device Snapshots</title>

  <para>
   RBD snapshot is a snapshot of a RADOS block device image. With snapshots you
   retain a history of the image’s state. &ceph; also supports snapshot
   layering, which allows you to clone VM images quickly and easily. &ceph;
   supports block device snapshots using the <command>rbd</command> command and
   many higher level interfaces, including &qemu;,
   <systemitem>libvirt</systemitem>, OpenStack and CloudStack.
  </para>

  <note>
   <para>
    Stop input/output operations before snapshotting an image. If the image
    contains a file system, the file system must be in a consistent state
    <emphasis>before</emphasis> snapshotting.
   </para>
  </note>

  <sect2>
   <title>Cephx Notes</title>
   <para>
    When <systemitem>cephx</systemitem> is enabled (see
    <link xlink:href="http://ceph.com/docs/master/rados/configuration/auth-config-ref/"/>
    for more information), you must specify a user name or ID and a path to the
    keyring containing the corresponding key for the user. See
    <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
    Management</link> for more details. You may also add the
    <systemitem>CEPH_ARGS</systemitem> environment variable to avoid re-entry
    of the following parameters.
   </para>
<screen>&prompt.root;rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
&prompt.root;rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.root;rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
&prompt.root;rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
     environment variable so that you do not need to enter them each time.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Snapshot Basics</title>
   <para>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <command>rbd</command> command on the command line.
   </para>
   <sect3>
    <title>Create Snapshot</title>
    <para>
     To create a snapshot with <command>rbd</command>, specify the <option>snap
     create</option> option, the pool name and the image name.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.root;rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool rbd snap create --snap snapshot1 image1
&prompt.root;rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>List Snapshots</title>
    <para>
     To list snapshots of an image, specify the pool name and the image name.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
&prompt.root;rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool rbd snap ls image1
&prompt.root;rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Rollback Snapshot</title>
    <para>
     To rollback to a snapshot with <command>rbd</command>, specify the
     <option>snap rollback</option> option, the pool name, the image name and
     the snap name.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.root;rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 snap rollback --snap snapshot1 image1
&prompt.root;rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <emphasis>faster to
      clone</emphasis> from a snapshot <emphasis>than to rollback</emphasis> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Delete a Snapshot</title>
    <para>
     To delete a snapshot with <command>rbd</command>, specify the <option>snap
     rm</option> option, the pool name, the image name and the user name.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.root;rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 snap rm --snap snapshot1 image1
&prompt.root;rbd snap rm pool1/imag1@snapshot1</screen>
    <note>
     <para>
      &ceph; OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Purge Snapshots</title>
    <para>
     To delete all snapshots for an image with <command>rbd</command>, specify
     the <option>snap purge</option> option and the image name.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
&prompt.root;rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 snap purge image1
&prompt.root;rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.snapshoti.layering">
   <title>Layering</title>
   <para>
    &ceph; supports the ability to create many copy-on-write (COW) clones of a
    block device snapshot. Snapshot layering enables &ceph; block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it; then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics&mdash;making it possible to create clones rapidly.
   </para>
   <note>
    <para>
     The terms “parent” and “child” mentioned in the command line
     examples below mean a &ceph; block device snapshot (parent), and the
     corresponding image cloned from the snapshot (child).
    </para>
   </note>
   <para>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </para>
   <para>
    A COW clone of a snapshot behaves exactly like any other &ceph; block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <emphasis>must</emphasis> protect the snapshot before you clone it.
   </para>
   <note>
    <para>
     &ceph; only supports cloning for <emphasis>format 2</emphasis> images
     (that is created with <command>rbd create --image-format 2</command>).
    </para>
   </note>
   <sect3>
    <title>Getting Started with Layering</title>
    <para>
     &ceph; block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     Once you have performed these steps, you can begin cloning the snapshot.
    </para>
    <para>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID and snapshot ID. The inclusion of the pool ID means that
     you may clone snapshots from one pool to images in another pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image Template</emphasis>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example &sls;), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example <command>zypper ref &amp;&amp; zypper patch</command>
       followed by <command>rbd snap create</command>). As the image matures,
       the user can clone any one of the snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Extended Template</emphasis>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example a database, a content management
       system, an analytics system, etc.) and then snapshot the extended image,
       which itself may be updated same as the base image.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Template Pool</emphasis>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image Migration/Recovery</emphasis>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Protecting a Snapshot</title>
    <para>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
&prompt.root;rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 snap protect --image image1 --snap snapshot1
&prompt.root;rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      You cannot delete a protected snapshot.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cloning a Snapshot</title>
    <para>
     To clone a snapshot, you need to specify the parent pool, image and
     snapshot, the child pool and image name. You must protect the snapshot
     before you can clone it.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
&prompt.root;rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Unprotecting a Snapshot</title>
    <para>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <emphasis>not</emphasis> delete snapshots that have
     references from clones. You must flatten each clone of a snapshot, before
     you can delete the snapshot.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
&prompt.root;rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
&prompt.root;rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Listing Children of a Snapshot</title>
    <para>
     To list the children of a snapshot, execute the following:
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
&prompt.root;rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 children --image image1 --snap snapshot1
&prompt.root;rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Flattening a Cloned Image</title>
    <para>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     “flatten” the image by copying the information from the snapshot to
     the clone. The time it takes to flatten a clone increases with the size of
     the snapshot. To delete a snapshot, you must flatten the child images
     first.
    </para>
<screen>&prompt.root;rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
&prompt.root;rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --pool pool1 flatten --image image1
&prompt.root;rbd flatten pool1/image1</screen>
    <note>
     <para>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rbd.mirror">
  <title>&rbd; Mirroring</title>

  <para>
   RBD images can be asynchronously mirrored between two &ceph; clusters. This
   capability uses the RBD journaling image feature to ensure crash-consistent
   replication between clusters. Mirroring is configured on a per-pool basis
   within peer clusters and can be configured to automatically mirror all
   images within a pool or only a specific subset of images. Mirroring is
   configured using the <command>rbd</command> command. The
   <systemitem>rbd-mirror</systemitem> daemon is responsible for pulling image
   updates from the remote peer cluster and applying them to the image within
   the local cluster.
  </para>

  <important>
   <title>rbd-mirror Daemon</title>
   <para>
    To use RBD mirroring, you need to have two &ceph; clusters, each running
    the <systemitem>rbd-mirror</systemitem> daemon.
   </para>
  </important>

  <sect2 xml:id="rbd.mirror.daemon">
   <title>rbd-mirror Daemon</title>
   <para>
    The two <systemitem>rbd-mirror</systemitem> daemons are responsible for
    watching image journals on the remote, peer cluster and replaying the
    journal events against the local cluster. The RBD image journaling feature
    records all modifications to the image in the order they occur. This
    ensures that a crash-consistent mirror of the remote image is available
    locally.
   </para>
   <para>
    The <systemitem>rbd-mirror</systemitem> daemon is available in the
    <systemitem>rbd-mirror</systemitem> package. Install, enable, and start it
    on one of cluster nodes:
   </para>
<screen>&prompt.sminion;zypper install rbd-mirror
&prompt.sminion;systemctl enable rbd-mirror.service
&prompt.sminion;systemctl start rbd-mirror.service</screen>
   <important>
    <para>
     Each <systemitem>rbd-mirror</systemitem> daemon requires the ability to
     connect to both clusters simultaneously.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ceph.rbd.mirror.poolconfig">
   <title>Pool Configuration</title>
   <para>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <command>rbd</command> command. Mirroring is configured on a per-pool basis
    within the &ceph; clusters.
   </para>
   <para>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named 'local' and 'remote', are
    accessible from a single host for clarity.
   </para>
   <para>
    See the <command>rbd</command> manual page (<command>man 8 rbd</command>)
    for additional details of how to connect to different &ceph; clusters.
   </para>
   <tip>
    <title>Multiple Clusters</title>
    <para>
     The cluster name in the following examples corresponds to a &ceph;
     configuration file of the same name
     <filename>/etc/ceph/remote.conf</filename>. See the
     <link
      xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#running-multiple-clusters">ceph-conf</link>
     documentation for how to configure multiple clusters.
    </para>
   </tip>
   <sect3>
    <title>Enable Mirroring</title>
    <para>
     To enable mirroring on a pool, specify the <command>mirror pool
     enable</command> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        Mirroring needs to be explicitly enabled on each image. See
        <xref
         linkend="rbd.mirror.enable_image_mirroring"/> for more
        information.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example:
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool enable image-pool pool
&prompt.root;rbd --cluster remote mirror pool enable image-pool pool</screen>
   </sect3>
   <sect3>
    <title>Disable Mirroring</title>
    <para>
     To disable mirroring on a pool, specify the <command>mirror pool
     disable</command> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool disable image-pool
&prompt.root;rbd --cluster remote mirror pool disable image-pool</screen>
   </sect3>
   <sect3>
    <title>Add Cluster Peer</title>
    <para>
     In order for the <systemitem>rbd-mirror</systemitem> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <command>mirror pool peer
     add</command> subcommand, the pool name, and a cluster specification:
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool peer add image-pool client.remote@remote
&prompt.root;rbd --cluster remote mirror pool peer add image-pool client.local@local</screen>
   </sect3>
   <sect3>
    <title>Remove Cluster Peer</title>
    <para>
     To remove a mirroring peer cluster, specify the <command>mirror pool peer
     remove</command> subcommand, the pool name, and the peer UUID (available
     from the <command>rbd mirror pool info</command> command):
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool peer remove image-pool \
 55672766-c02b-4729-8567-f13a66893445
&prompt.root;rbd --cluster remote mirror pool peer remove image-pool \
 60c0e299-b38f-4234-91f6-eed0a367be08</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd.mirror.imageconfig">
   <title>Image Configuration</title>
   <para>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer &ceph; cluster.
   </para>
   <para>
    Mirrored RBD images are designated as either <emphasis>primary</emphasis>
    or <emphasis>non-primary</emphasis>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </para>
   <para>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly
    enabled (see <xref linkend="rbd.mirror.enable_image_mirroring"/>) by the
    <command>rbd</command> command).
   </para>
   <sect3>
    <title>Enable Image Journaling Support</title>
    <para>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. Before an image can be
     mirrored to a peer cluster, the journaling feature must be enabled. The
     feature can be enabled at image creation time by providing the
     <option>--image-feature exclusive-lock,journaling</option> option to the
     <command>rbd</command> command.
    </para>
    <para>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <command>feature enable</command> subcommand, the pool and image name, and
     the feature name:
    </para>
<screen>&prompt.root;rbd --cluster local feature enable image-pool/image-1 journaling</screen>
    <note>
     <title>Option Dependency</title>
     <para>
      The <option>journaling</option> feature is dependent on the
      <option>exclusive-lock</option> feature. If the
      <option>exclusive-lock</option> feature is not already enabled, you need
      to enable it prior to enabling the <option>journaling</option> feature.
     </para>
    </note>
    <tip>
     <title>Journaling on All New Images</title>
     <para>
      You can enable journaling on all new images by default by adding the
      following line to your &ceph; configuration file:
     </para>
<screen>rbd default features = 125</screen>
    </tip>
   </sect3>
   <sect3 xml:id="rbd.mirror.enable_image_mirroring">
    <title>Enable Image Mirroring</title>
    <para>
     If the mirroring is configured in 'image' mode for the image's pool, then
     it is necessary to explicitly enable mirroring for each image within the
     pool. To enable mirroring for a specific image, specify the
     <command>mirror image enable</command> subcommand along with the pool and
     image name:
    </para>
<screen>&prompt.root;rbd --cluster local mirror image enable image-pool/image-1</screen>
   </sect3>
   <sect3>
    <title>Disable Image Mirroring</title>
    <para>
     To disable mirroring for a specific image, specify the <command>mirror
     image disable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.root;rbd --cluster local mirror image disable image-pool/image-1</screen>
   </sect3>
   <sect3>
    <title>Image Promotion and Demotion</title>
    <para>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop the access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </para>
    <para>
     To demote a specific image to non-primary, specify the <command>mirror
     image demote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.root;rbd --cluster local mirror image demote image-pool/image-1</screen>
    <para>
     To demote all primary images within a pool to non-primary, specify the
     <command>mirror pool demote</command> subcommand along with the pool name:
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool demote image-pool</screen>
    <para>
     To promote a specific image to primary, specify the <command>mirror image
     promote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.root;rbd --cluster remote mirror image promote image-pool/image-1</screen>
    <para>
     To promote all non-primary images within a pool to primary, specify the
     <command>mirror pool promote</command> subcommand along with the pool
     name:
    </para>
<screen>&prompt.root;rbd --cluster local mirror pool promote image-pool</screen>
    <tip>
     <title>Split I/O Load</title>
     <para>
      Since the primary / non-primary status is per-image, it is possible to
      have two clusters split the IO load and stage failover / failback.
     </para>
    </tip>
    <note>
     <title>Forced Promotion</title>
     <para>
      Promotion can be forced using the <option>--force</option> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example in case of cluster failure or communication outage).
      This will result in a split-brain scenario between the two peers and the
      image will no longer be in-sync until a
      <command>resync</command>subcommand is issued.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Force Image Resync</title>
    <para>
     If a split-brain event is detected by the
     <systemitem>rbd-mirror</systemitem> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out-of-date and then request a
     resync to the primary image. To request an image resync, specify the
     <command>mirror image resync</command> subcommand along with the pool and
     image name:
    </para>
<screen>&prompt.root;rbd mirror image resync image-pool/image-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd.mirror.status">
   <title>Mirror Status</title>
   <para>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <command>mirror image
    status</command> and <command>mirror pool status</command> subcommands:
   </para>
   <para>
    To request the mirror image status, specify the <command>mirror image
    status</command> subcommand along with the pool and image name:
   </para>
<screen>&prompt.root;rbd mirror image status image-pool/image-1</screen>
   <para>
    To request the mirror pool summary status, specify the <command>mirror pool
    status</command> subcommand along with the pool name:
   </para>
<screen>&prompt.root;rbd mirror pool status image-pool</screen>
   <tip>
    <title></title>
    <para>
     Adding <option>--verbose</option> option to the <command>mirror pool
     status</command> subcommand will additionally output status details for
     every mirroring image in the pool.
    </para>
   </tip>
  </sect2>
 </sect1>
</chapter>
