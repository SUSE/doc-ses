<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-rbd">
 <title>&rbd;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like &ceph;.
 </para>
 <para>
  &ceph; block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a &ceph; cluster. &ceph; block
  devices leverage &rados; capabilities such as snapshotting, replication, and
  consistency. &ceph;'s &rbd;s (RBD) interact with OSDs using kernel modules or
  the <systemitem>librbd</systemitem> library.
 </para>
 <figure>
  <title>&rados; protocol</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  &ceph;'s block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as &qemu;, or
  cloud-based computing systems such as &ostack; that rely on &libvirt;. You
  can use the same cluster to operate the &ogw;, &cephfs;, and &rbd;s
  simultaneously.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Block device commands</title>

  <para>
   The <command>rbd</command> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creating a block device image in a replicated pool</title>
   <para>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <xref linkend="ceph-pools"/>):
   </para>
<screen>
&prompt.cephuser;rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Image size units</title>
    <para>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creating a block device image in an erasure coded pool</title>
   <para>
    It is possible to store data of a block device image directly in erasure
    coded (EC) pools. A &rbd; image consists of <emphasis>data</emphasis> and
    <emphasis>metadata</emphasis> parts. You can store only the data part of a
    &rbd; image in an EC pool. The pool needs to have the
    <option>overwrite</option> flag set to <emphasis>true</emphasis>, and that
    is only possible if all OSDs where the pool is stored use &bluestore;.
   </para>
   <para>
    You cannot store the image's metadata part in an EC pool. You can specify
    the replicated pool for storing the image's metadata with the
    <option>--pool=</option> option of the <command>rbd create</command>
    command or specify <option>pool/</option> as a prefix to the image name.
   </para>
   <para>
    Create an EC pool:
   </para>
<screen>&prompt.cephuser;ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
&prompt.cephuser;ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Specify the replicated pool for storing metadata:
   </para>
<screen>
&prompt.cephuser;rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    Or:
   </para>
<screen>
&prompt.cephuser;rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Listing block device images</title>
   <para>
    To list block devices in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Retrieving image information</title>
   <para>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Resizing a block device image</title>
   <para>
    &rbd; images are thin provisioned&mdash;they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <option>--size</option> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </para>
<screen>
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Removing a block device image</title>
   <para>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Mounting and unmounting</title>

  <para>
   After you create a &rbd;, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </para>

  <para>
   The <command>rbd</command> command defaults to accessing the cluster using
   the &ceph; <literal>admin</literal> user account. This account has full
   administrative access to the cluster. This runs the risk of accidentally
   causing damage, similarly to logging into a Linux workstation as &rootuser;.
   Thus, it is preferable to create user accounts with fewer privileges and use
   these accounts for normal read/write &rbd; access.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Creating a &ceph; user account</title>
   <para>
    To create a new user account with &mgr;, &mon;, and &osd; capabilities, use
    the <command>ceph</command> command with the <command>auth
    get-or-create</command> subcommand:
   </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    For example, to create a user called <replaceable>qemu</replaceable> with
    read-write access to the pool <replaceable>vms</replaceable> and read-only
    access to the pool <replaceable>images</replaceable>, execute the
    following:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    The output from the <command>ceph auth get-or-create</command> command will
    be the keyring for the specified user, which can be written to
    <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     When using the <command>rbd</command> command, you can specify the user ID
     by providing the optional <command>--id</command>
     <replaceable>ID</replaceable> argument.
    </para>
   </note>
   <para>
    For more details on managing &ceph; user accounts, refer to
    <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>User authentication</title>
   <para>
    To specify a user name, use <option>--id
    <replaceable>user-name</replaceable></option>. If you use
    <systemitem>cephx</systemitem> authentication, you also need to specify a
    secret. It may come from a keyring or a file containing the secret:
   </para>
<screen>&prompt.cephuser;rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    or
   </para>
<screen>&prompt.cephuser;rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparing a &rbd; for use</title>
   <procedure>
    <step>
     <para>
      Make sure your &ceph; cluster includes a pool with the disk image you
      want to map. Assume the pool is called <literal>mypool</literal> and the
      image is <literal>myimage</literal>.
     </para>
<screen>&prompt.cephuser;rbd list mypool</screen>
    </step>
    <step>
     <para>
      Map the image to a new block device:
     </para>
<screen>&prompt.cephuser;rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>&prompt.cephuser;rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      The device we want to work on is <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>RBD device path</title>
      <para>
       Instead of
       <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>,
       you can use
       <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename>
       as a persistent device path. For example:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Make an XFS file system on the <filename>/dev/rbd0</filename> device:
     </para>
<screen>&prompt.root;mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Replacing <filename>/mnt</filename> with your mount point, mount the
      device and check it is correctly mounted:
     </para>
<screen>&prompt.root;mount /dev/rbd0 /mnt
      &prompt.root;mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Now you can move data to and from the device as if it was a local
      directory.
     </para>
     <tip>
      <title>Increasing the size of RBD device</title>
      <para>
       If you find that the size of the RBD device is no longer enough, you can
       easily increase it.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Increase the size of the RBD image, for example up to 10 GB.
        </para>
<screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Grow the file system to fill up the new size of the device:
        </para>
<screen>&prompt.root;xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      After you finish accessing the device, you can unmap and unmount it.
     </para>
<screen>
&prompt.cephuser;rbd device unmap /dev/rbd0
&prompt.root;unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>Manual mounting and unmounting</title>
    <para>
     A <command>rbdmap</command> script and &systemd; unit is provided to make
     the process of mapping and mounting RBDs after boot, and unmounting them
     before shutdown, smoother. Refer to <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command> Map RBD devices at boot time</title>
   <para>
    <command>rbdmap</command> is a shell script that automates <command>rbd
    map</command> and <command>rbd device unmap</command> operations on one or
    more RBD images. Although you can run the script manually at any time, the
    main advantage is automatic mapping and mounting of RBD images at boot time
    (and unmounting and unmapping at shutdown), as triggered by the Init
    system. A &systemd; unit file, <filename>rbdmap.service</filename> is
    included with the <systemitem>ceph-common</systemitem> package for this
    purpose.
   </para>
   <para>
    The script takes a single argument, which can be either
    <option>map</option> or <option>unmap</option>. In either case, the script
    parses a configuration file. It defaults to
    <filename>/etc/ceph/rbdmap</filename>, but can be overridden via an
    environment variable <literal>RBDMAPFILE</literal>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </para>
   <para>
    The configuration file has the following format:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Path to an image within a pool. Specify as
       <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       An optional list of parameters to be passed to the underlying
       <command>rbd device map</command> command. These parameters and their
       values should be specified as a comma-separated string, for example:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       The example makes the <command>rbdmap</command> script run the following
       command:
      </para>
<screen>&prompt.cephuser;rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </para>
<screen>&prompt.cephuser;rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    When run as <command>rbdmap map</command>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using the <command>rbd device map</command> command) and
    then mount the image.
   </para>
   <para>
    When run as <command>rbdmap unmap</command>, images listed in the
    configuration file will be unmounted and unmapped.
   </para>
   <para>
    <command>rbdmap unmap-all</command> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </para>
   <para>
    If successful, the <command>rbd device map</command> operation maps the
    image to a <filename>/dev/rbdX</filename> device, at which point a udev
    rule is triggered to create a friendly device name symbolic link
    <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename>
    pointing to the real mapped device.
   </para>
   <para>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <filename>/etc/fstab</filename>.
    When writing <filename>/etc/fstab</filename> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early&mdash;before the device in
    question even exists, as <filename>rbdmap.service</filename> is typically
    triggered quite late in the boot sequence.
   </para>
   <para>
    For a complete list of <command>rbd</command> options, see the
    <command>rbd</command> manual page (<command>man 8 rbd</command>).
   </para>
   <para>
    For examples of the <command>rbdmap</command> usage, see the
    <command>rbdmap</command> manual page (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Increasing the size of RBD devices</title>
   <para>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Increase the size of the RBD image, for example up to 10GB.
     </para>
<screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Grow the file system to fill up the new size of the device.
     </para>
<screen>&prompt.root;xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Snapshots</title>

  <para>
   An RBD snapshot is a snapshot of a &rbd; image. With snapshots, you retain a
   history of the image's state. &ceph; also supports snapshot layering, which
   allows you to clone VM images quickly and easily. &ceph; supports block
   device snapshots using the <command>rbd</command> command and many
   higher-level interfaces, including &qemu;, <systemitem>libvirt</systemitem>,
   &ostack;, and CloudStack.
  </para>

  <note>
   <para>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Enabling and configuring <systemitem>cephx</systemitem></title>
   <para>
    When <systemitem>cephx</systemitem> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <xref linkend="cha-storage-cephx"/> for more details. You may
    also add the <systemitem>CEPH_ARGS</systemitem> environment variable to
    avoid re-entry of the following parameters.
   </para>
<screen>&prompt.cephuser;rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
     environment variable so that you do not need to enter them each time.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Snapshot basics</title>
   <para>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <command>rbd</command> command on the command line.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Creating snapshots</title>
    <para>
     To create a snapshot with <command>rbd</command>, specify the <option>snap
     create</option> option, the pool name, and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap create --snap snapshot1 image1
&prompt.cephuser;rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Listing snapshots</title>
    <para>
     To list snapshots of an image, specify the pool name and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap ls image1
&prompt.cephuser;rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Rolling back snapshots</title>
    <para>
     To rollback to a snapshot with <command>rbd</command>, specify the
     <option>snap rollback</option> option, the pool name, the image name, and
     the snapshot name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rollback --snap snapshot1 image1
&prompt.cephuser;rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <emphasis>faster to
      clone</emphasis> from a snapshot <emphasis>than to rollback</emphasis> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Deleting a snapshot</title>
    <para>
     To delete a snapshot with <command>rbd</command>, specify the <option>snap
     rm</option> option, the pool name, the image name, and the user name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rm --snap snapshot1 image1
&prompt.cephuser;rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      &ceph; OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Purging snapshots</title>
    <para>
     To delete all snapshots for an image with <command>rbd</command>, specify
     the <option>snap purge</option> option and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap purge image1
&prompt.cephuser;rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Snapshot layering</title>
   <para>
    &ceph; supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables &ceph; block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics&mdash;making it possible to create clones rapidly.
   </para>
   <note>
    <para>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a &ceph; block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </para>
   </note>
   <para>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </para>
   <para>
    A COW clone of a snapshot behaves exactly like any other &ceph; block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <emphasis>must</emphasis> protect the snapshot before you clone it.
   </para>
   <note>
    <title><option>--image-format 1</option> not supported</title>
    <para>
     You cannot create snapshots of images created with the deprecated
     <command>rbd create --image-format 1</command> option. &ceph; only
     supports cloning of the default <emphasis>format 2</emphasis> images.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Getting started with layering</title>
    <para>
     &ceph; block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </para>
    <para>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image Template</emphasis>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, &sls;), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <command>zypper ref &amp;&amp; zypper patch</command>
       followed by <command>rbd snap create</command>). As the image matures,
       the user can clone any one of the snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Extended Template</emphasis>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Template Pool</emphasis>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image Migration/Recovery</emphasis>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protecting a snapshot</title>
    <para>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap protect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      You cannot delete a protected snapshot.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Cloning a snapshot</title>
    <para>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
&prompt.cephuser;rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Unprotecting a snapshot</title>
    <para>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <emphasis>not</emphasis> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Listing children of a snapshot</title>
    <para>
     To list the children of a snapshot, execute the following:
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
&prompt.cephuser;rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 children --image image1 --snap snapshot1
&prompt.cephuser;rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Flattening a cloned image</title>
    <para>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
&prompt.cephuser;rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 flatten --image image1
&prompt.cephuser;rbd flatten pool1/image1</screen>
    <note>
     <para>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>RBD image mirrors</title>

  <para>
   RBD images can be asynchronously mirrored between two &ceph; clusters. This
   capability is available in two modes:
  </para>

  <variablelist>
   <varlistentry>
    <term>Journal-based</term>
    <listitem>
     <para>
      This mode uses the RBD journaling image feature to ensure point-in-time,
      crash-consistent replication between clusters. Every write to the RBD
      image is first recorded to the associated journal before modifying the
      actual image. The <literal>remote</literal> cluster will read from the
      journal and replay the updates to its local copy of the image. Since each
      write to the RBD image will result in two writes to the &ceph; cluster,
      expect write latencies to nearly double when using the RBD journaling
      image feature.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Snapshot-based</term>
    <listitem>
     <para>
      This mode uses periodically-scheduled or manually-created RBD image
      mirror-snapshots to replicate crash-consistent RBD images between
      clusters. The <literal>remote</literal> cluster will determine any data
      or metadata updates between two mirror-snapshots, and copy the deltas to
      its local copy of the image. With the help of the RBD fast-diff image
      feature, updated data blocks can be quickly computed without the need to
      scan the full RBD image. Since this mode is not point-in-time consistent,
      the full snapshot delta will need to be synchronized prior to use during
      a failover scenario. Any partially-applied snapshot deltas will be rolled
      back to the last fully synchronized snapshot prior to use.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Mirroring is configured on a per-pool basis within peer clusters. This can
   be configured on a specific subset of images within the pool, or configured
   to automatically mirror all images within a pool when using journal-based
   mirroring only. Mirroring is configured using the <command>rbd</command>
   command. The &rbdmirror; daemon is responsible for pulling image updates
   from the <literal>remote</literal>, peer cluster and applying them to the
   image within the <literal>local</literal> cluster.
  </para>

  <para>
   Depending on the desired needs for replication, RBD mirroring can be
   configured for either one- or two-way replication:
  </para>

  <variablelist>
   <varlistentry>
    <term>One-way Replication</term>
    <listitem>
     <para>
      When data is only mirrored from a primary cluster to a secondary cluster,
      the &rbdmirror; daemon runs only on the secondary cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Two-way Replication</term>
    <listitem>
     <para>
      When data is mirrored from primary images on one cluster to non-primary
      images on another cluster (and vice-versa), the &rbdmirror; daemon runs
      on both clusters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Each instance of the &rbdmirror; daemon needs to be able to connect to both
    the <literal>local</literal> and <literal>remote</literal> &ceph; clusters
    simultaneously. For example, all monitor and OSD hosts. Additionally, the
    network needs to have sufficient bandwidth between the two data centers to
    handle mirroring workload.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Pool configuration</title>
   <para>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <command>rbd</command> command. Mirroring is configured on a per-pool basis
    within the &ceph; clusters.
   </para>
   <para>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named <literal>local</literal> and
    <literal>remote</literal>, are accessible from a single host for clarity.
   </para>
   <para>
    See the <command>rbd</command> manual page (<command>man 8 rbd</command>)
    for additional details on how to connect to different &ceph; clusters.
   </para>
   <tip>
    <title>Multiple clusters</title>
    <para>
     The cluster name in the following examples corresponds to a &ceph;
     configuration file of the same name
     <filename>/etc/ceph/remote.conf</filename> and &ceph; keyring file of the
     same name <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Enable mirroring on a pool</title>
    <para>
     To enable mirroring on a pool, specify the <command>mirror pool
     enable</command> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        Mirroring needs to be explicitly enabled on each image. See
        <xref linkend="rbd-mirror-enable-image-mirroring"/> for more
        information.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
&prompt.cephuser;rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Disable mirroring</title>
    <para>
     To disable mirroring on a pool, specify the <command>mirror pool
     disable</command> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
&prompt.cephuser;rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Bootstrapping peers</title>
    <para>
     In order for the &rbdmirror; daemon to discover its peer cluster, the peer
     needs to be registered to the pool and a user account needs to be created.
     This process can be automated with <command>rbd</command> and the
     <command>mirror pool peer bootstrap create</command> and <command>mirror
     pool peer bootstrap import</command> commands.
    </para>
    <para>
     To manually create a new bootstrap token with <command>rbd</command>,
     specify the <command>mirror pool peer bootstrap create</command> command,
     a pool name, along with an optional friendly site name to describe the
     <literal>local</literal> cluster:
    </para>
<screen>&prompt.cephuser.local;rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     The output of <command>mirror pool peer bootstrap create</command> will be
     a token that should be provided to the <command>mirror pool peer bootstrap
     import</command> command. For example, on the <literal>local</literal>
     cluster:
    </para>
<screen>&prompt.cephuser.local;rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     To manually import the bootstrap token created by another cluster with the
     <command>rbd</command> command, use the following syntax:
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Where:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        An optional friendly site name to describe the <literal>local</literal>
        cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        A mirroring direction. Defaults to <literal>rx-tx</literal> for
        bidirectional mirroring, but can also be set to
        <literal>rx-only</literal> for unidirectional mirroring.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        Name of the pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        A file path to the created token (or <literal>-</literal> to read it
        from the standard input).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example, on the <literal>remote</literal> cluster:
    </para>
<screen>&prompt.cephuser.remote;cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen>&prompt.cephuser;rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Adding a cluster peer manually</title>
    <para>
     Alternatively to bootstrapping peers as described in
     <xref
     linkend="ceph-rbd-mirror-bootstrap-peer"/>, you can specify
     peers manually. The remote &rbdmirror; daemon will need access to the
     local cluster to perform mirroring. Create a new local &ceph; user that
     the remote &rbdmirror; daemon will use, for example
     <literal>rbd-mirror-peer</literal>:
    </para>
<screen>
&prompt.cephuser;ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Use the following syntax to add a mirroring peer &ceph; cluster with the
     <command>rbd</command> command:
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     For example:
    </para>
<screen>
&prompt.cephuser;rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
&prompt.cephuser;rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     By default, the &rbdmirror; daemon needs to have access to the &ceph;
     configuration file located at
     <filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename>.
     It provides IP addresses of the peer cluster’s MONs and a keyring for a
     client named <replaceable>CLIENT_NAME</replaceable> located in the default
     or custom keyring search paths, for example
     <filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>.
    </para>
    <para>
     Alternatively, the peer cluster’s MON and/or client key can be securely
     stored within the local &ceph; config-key store. To specify the peer
     cluster connection attributes when adding a mirroring peer, use the
     <option>--remote-mon-host</option> and <option>--remote-key-file</option>
     options. For example:
    </para>
<screen>
&prompt.cephuser;rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
&prompt.cephuser;rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Remove cluster peer</title>
    <para>
     To remove a mirroring peer cluster, specify the <command>mirror pool peer
     remove</command> subcommand, the pool name, and the peer UUID (available
     from the <command>rbd mirror pool info</command> command):
    </para>
<screen>
&prompt.cephuser;rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
&prompt.cephuser;rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Data pools</title>
    <para>
     When creating images in the destination cluster, &rbdmirror; selects a
     data pool as follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the destination cluster has a default data pool configured (with the
       <option>rbd_default_data_pool</option> configuration option), it will be
       used.
      </para>
     </listitem>
     <listitem>
      <para>
       Otherwise, if the source image uses a separate data pool, and a pool
       with the same name exists on the destination cluster, that pool will be
       used.
      </para>
     </listitem>
     <listitem>
      <para>
       If neither of the above is true, no data pool will be set.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>RBD Image configuration</title>
   <para>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer &ceph; cluster.
   </para>
   <para>
    Mirrored RBD images are designated as either <emphasis>primary</emphasis>
    or <emphasis>non-primary</emphasis>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </para>
   <para>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <xref linkend="rbd-mirror-enable-image-mirroring"/>) by the
    <command>rbd</command> command).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Enabling image mirroring</title>
    <para>
     If mirroring is configured in the <literal>image</literal> mode, then it
     is necessary to explicitly enable mirroring for each image within the
     pool. To enable mirroring for a specific image with
     <command>rbd</command>, specify the <command>mirror image enable</command>
     subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     The mirror image mode can either be <literal>journal</literal> or
     <literal>snapshot</literal>:
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (default)</term>
      <listitem>
       <para>
        When configured in <literal>journal</literal> mode, mirroring will use
        the RBD journaling image feature to replicate the image contents. If
        the RBD journaling image feature is not yet enabled on the image, it
        will be automatically enabled.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        When configured in <literal>snapshot</literal> mode, mirroring will use
        RBD image mirror-snapshots to replicate the image contents. Once
        enabled, an initial mirror-snapshot will automatically be created.
        Additional RBD image mirror-snapshots can be created by the
        <command>rbd</command> command.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image enable image-pool/image-1 snapshot
&prompt.cephuser;rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Enabling the image journaling feature</title>
    <para>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. When using the
     <literal>image</literal> mirroring mode, the journaling feature will be
     automatically enabled if mirroring is enabled on the image. When using the
     <literal>pool</literal> mirroring mode, before an image can be mirrored to
     a peer cluster, the RBD image journaling feature must be enabled. The
     feature can be enabled at image creation time by providing the
     <option>--image-feature exclusive-lock,journaling</option> option to the
     <command>rbd</command> command.
    </para>
    <para>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <command>feature enable</command> subcommand, the pool and image name, and
     the feature name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
&prompt.cephuser;rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Option dependency</title>
     <para>
      The <option>journaling</option> feature is dependent on the
      <option>exclusive-lock</option> feature. If the
      <option>exclusive-lock</option> feature is not already enabled, you need
      to enable it prior to enabling the <option>journaling</option> feature.
     </para>
    </note>
    <tip>
     <para>
      You can enable journaling on all new images by default by adding
      <option>rbd default features =
      layering,exclusive-lock,object-map,deep-flatten,journaling</option> to
      your &ceph; configuration file.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Creating image mirror-snapshots</title>
    <para>
     When using snapshot-based mirroring, mirror-snapshots will need to be
     created whenever it is desired to mirror the changed contents of the RBD
     image. To create a mirror-snapshot manually with <command>rbd</command>,
     specify the <command>mirror image snapshot</command> command along with
     the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     By default only three mirror-snapshots will be created per image. The most
     recent mirror-snapshot is automatically pruned if the limit is reached.
     The limit can be overridden via the
     <option>rbd_mirroring_max_mirroring_snapshots</option> configuration
     option if required. Additionally, mirror-snapshots are automatically
     deleted when the image is removed or when mirroring is disabled.
    </para>
    <para>
     Mirror-snapshots can also be automatically created on a periodic basis if
     mirror-snapshot schedules are defined. The mirror-snapshot can be
     scheduled globally, per-pool, or per-image levels. Multiple
     mirror-snapshot schedules can be defined at any level, but only the
     most-specific snapshot schedules that match an individual mirrored image
     will run.
    </para>
    <para>
     To create a mirror-snapshot schedule with <command>rbd</command>, specify
     the <command>mirror snapshot schedule add</command> command along with an
     optional pool or image name, interval, and optional start time.
    </para>
    <para>
     The interval can be specified in days, hours, or minutes using the
     suffixes <option>d</option>, <option>h</option>, or <option>m</option>
     respectively. The optional start time can be specified using the ISO 8601
     time format. For example:
    </para>
<screen>
&prompt.cephuser;rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
&prompt.cephuser;rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     To remove a mirror-snapshot schedule with <command>rbd</command>, specify
     the <command>mirror snapshot schedule remove</command> command with
     options that match the corresponding add schedule command.
    </para>
    <para>
     To list all snapshot schedules for a specific level (global, pool, or
     image) with <command>rbd</command>, specify the <command>mirror snapshot
     schedule ls</command> command along with an optional pool or image name.
     Additionally, the <option>--recursive</option> option can be specified to
     list all schedules at the specified level and below. For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     To find out when the next snapshots will be created for snapshot-based
     mirroring RBD images with <command>rbd</command>, specify the
     <command>mirror snapshot schedule status</command> command along with an
     optional pool or image name. For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Disabling image mirroring</title>
    <para>
     To disable mirroring for a specific image, specify the <command>mirror
     image disable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Promoting and demoting images</title>
    <para>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </para>
    <note>
     <title>Forced promotion</title>
     <para>
      Promotion can be forced using the <option>--force</option> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <command>resync</command> subcommand is issued.
     </para>
    </note>
    <para>
     To demote a specific image to non-primary, specify the <command>mirror
     image demote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To demote all primary images within a pool to non-primary, specify the
     <command>mirror pool demote</command> subcommand along with the pool name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     To promote a specific image to primary, specify the <command>mirror image
     promote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To promote all non-primary images within a pool to primary, specify the
     <command>mirror pool promote</command> subcommand along with the pool
     name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Split I/O load</title>
     <para>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Forcing image resync</title>
    <para>
     If a split-brain event is detected by the &rbdmirror; daemon, it will not
     attempt to mirror the affected image until corrected. To resume mirroring
     for an image, first demote the image determined to be out of date and then
     request a resync to the primary image. To request an image resync, specify
     the <command>mirror image resync</command> subcommand along with the pool
     and image name:
    </para>
<screen>&prompt.cephuser;rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Checking the mirror status</title>
   <para>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <command>mirror image
    status</command> and <command>mirror pool status</command> subcommands:
   </para>
   <para>
    To request the mirror image status, specify the <command>mirror image
    status</command> subcommand along with the pool and image name:
   </para>
<screen>&prompt.cephuser;rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    To request the mirror pool summary status, specify the <command>mirror pool
    status</command> subcommand along with the pool name:
   </para>
<screen>&prompt.cephuser;rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Adding the <option>--verbose</option> option to the <command>mirror pool
     status</command> subcommand will additionally output status details for
     every mirroring image in the pool.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Cache settings</title>

  <para>
   The user space implementation of the &ceph; block device
   (<systemitem>librbd</systemitem>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <emphasis>Least
   Recently Used</emphasis> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </para>

  <para>
   &ceph; supports write-back caching for RBD. To enable it, run
  </para>

<screen>&prompt.cephuser;ceph config set client rbd_cache true</screen>

  <para>
   By default, <systemitem>librbd</systemitem> does not perform any caching.
   Writes and reads go directly to the storage cluster, and writes return only
   when the data is on disk on all replicas. With caching enabled, writes
   return immediately, unless there are more unflushed bytes than set in the
   <option>rbd cache max dirty</option> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </para>

  <para>
   &ceph; supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, run
  </para>

<screen>&prompt.cephuser;ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </para>

  <para>
   The following parameters affect the behavior of &rbd;s. To set them, use the
   <literal>client</literal> category:
  </para>

<screen>&prompt.cephuser;ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Enable caching for &rbd; (RBD). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      The RBD cache size in bytes. Default is 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <option>rbd cache max dirty</option> needs to be less than <option>rbd
      cache size</option>. If set to 0, uses write-through caching. Default is
      24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <systemitem>rbd</systemitem>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS settings</title>

  <para>
   Generally, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </para>

  <important>
   <title>Not supported by &iscsi;</title>
   <para>
    The following QoS settings are used only by the user space RBD
    implementation <systemitem class="daemon">librbd</systemitem> and
    <emphasis>not</emphasis> used by the <systemitem>kRBD</systemitem>
    implementation. Because &iscsi; uses <systemitem>kRBD</systemitem>, it does
    not use the QoS settings. However, for &iscsi; you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      The desired limit of read operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Read-ahead settings</title>

  <para>
   &rbd; supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </para>

  <important>
   <title>Not supported by &iscsi;</title>
   <para>
    The following read-ahead settings are used only by the user space RBD
    implementation <systemitem class="daemon">librbd</systemitem> and
    <emphasis>not</emphasis> used by the <systemitem>kRBD</systemitem>
    implementation. Because &iscsi; uses <systemitem>kRBD</systemitem>, it does
    not use the read-ahead settings. However, for &iscsi; you can configure
    read-ahead on the kernel block device layer using standard kernel
    facilities.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Advanced features</title>

  <para>
   &rbd; supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the &ceph; configuration file by using the
   <option>rbd_default_features</option> option.
  </para>

  <para>
   You can specify the values of the <option>rbd_default_features</option>
   option in two ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     As a sum of features' internal values. Each feature has its own internal
     value&mdash;for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     As a comma-separated list of features. The previous example will look as
     follows:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Features not supported by &iscsi;</title>
   <para>
    RBD images with the following features will not be supported by &iscsi;:
    <option>deep-flatten</option>, <option>object-map</option>,
    <option>journaling</option>, <option>fast-diff</option>,
    <option>striping</option>
   </para>
  </note>

  <para>
   A list of advanced RBD features follows:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Layering enables you to use cloning.
     </para>
     <para>
      Internal value is 1, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy &rbd;s.
     </para>
     <para>
      Internal value is 2, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </para>
     <para>
      Internal value is 8, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </para>
     <para>
      Internal value is 16, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      Deep-flatten makes the <command>rbd flatten</command> (see
      <xref linkend="rbd-flatten-cloned-image" />) work on all the snapshots of
      an image, in addition to the image itself. Without it, snapshots of an
      image will still rely on the parent, therefore you will not be able to
      delete the parent image until the snapshots are deleted. Deep-flatten
      makes a parent independent of its clones, even if they have snapshots.
     </para>
     <para>
      Internal value is 32, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <xref linkend="ceph-rbd-mirror" />) uses the journal to replicate a crash
      consistent image to a <literal>remote</literal> cluster.
     </para>
     <para>
      Internal value is 64, default is 'no'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Mapping RBD using old kernel clients</title>

  <para>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with &productname; &productnumber; forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 >> \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Changing &crushmap; bucket types causes massive rebalancing</title>
   <para>
    If you intend to switch the &crushmap; bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Disable any RBD image features that are not supported. For example:
    </para>
<screen>
&prompt.cephuser;rbd feature disable pool1/image1 object-map
&prompt.cephuser;rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Change the &crushmap; bucket types from 'straw2' to 'straw':
    </para>
    <substeps>
     <step>
      <para>
       Save the &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Decompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Edit the &crushmap; and replace 'straw2' with 'straw'.
      </para>
     </step>
     <step>
      <para>
       Recompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Set the new &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Enabling block devices and &kube;</title>

  <para>
   You can use &ceph; RBD with &kube; v1.13 and higher through the
   <literal>ceph-csi</literal> driver. This driver dynamically provisions RBD
   images to back &kube; volumes, and maps these RBD images as block devices
   (optionally mounting a file system contained within the image) on worker
   nodes running pods that reference an RBD-backed volume.
  </para>

  <para>
   To use &ceph; block devices with &kube;, you must install and configure
   <literal>ceph-csi</literal> within your &kube; environment.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> uses the RBD kernel modules by default which
    may not support all &ceph; CRUSH tunables or RBD image features.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     By default, &ceph; block devices use the RBD pool. Create a pool for
     &kube; volume storage. Ensure your &ceph; cluster is running, then create
     the pool:
    </para>
<screen>&prompt.cephuser;ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Use the RBD tool to initialize the pool:
    </para>
<screen>&prompt.cephuser;rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Create a new user for &kube; and <literal>ceph-csi</literal>. Execute the
     following and record the generated key:
    </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> requires a ConfigMap object stored in &kube;
     to define the &ceph; monitor addresses for the &ceph; cluster. Collect
     both the &ceph; cluster unique fsid and the monitor addresses:
    </para>
<screen>&prompt.cephuser;ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Generate a <filename>csi-config-map.yaml</filename> file similar to the
     example below, substituting the FSID for <literal>clusterID</literal>, and
     the monitor addresses for <literal>monitors</literal>:
    </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     When generated, store the new ConfigMap object in &kube;:
    </para>
<screen>&prompt.kubeuser;kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> requires the cephx credentials for
     communicating with the &ceph; cluster. Generate a
     <filename>csi-rbd-secret.yaml</filename> file similar to the example
     below, using the newly-created &kube; user ID and cephx key:
    </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     When generated, store the new secret object in &kube;:
    </para>
<screen>&prompt.kubeuser;kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Create the required ServiceAccount and RBAC ClusterRole/ClusterRoleBinding
     &kube; objects. These objects do not necessarily need to be customized for
     your &kube; environment, and therefore can be used directly from the
     <literal>ceph-csi</literal> deployment YAML files:
    </para>
<screen>&prompt.kubeuser;kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
&prompt.kubeuser;kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Create the <literal>ceph-csi</literal> provisioner and node plugins:
    </para>
<screen>&prompt.kubeuser;wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
&prompt.kubeuser;kubectl apply -f csi-rbdplugin-provisioner.yaml
&prompt.kubeuser;wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
&prompt.kubeuser;kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      By default, the provisioner and node plugin YAML files will pull the
      development release of the <literal>ceph-csi</literal> container. The
      YAML files should be updated to use a release version.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Using &ceph; block devices in &kube;</title>
   <para>
    The &kube; StorageClass defines a class of storage. Multiple StorageClass
    objects can be created to map to different quality-of-service levels and
    features. For example, NVMe versus HDD-based pools.
   </para>
   <para>
    To create a <literal>ceph-csi</literal> StorageClass that maps to the
    &kube; pool created above, the following YAML file can be used, after
    ensuring that the <literal>clusterID</literal> property matches your &ceph;
    cluster's FSID:
   </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
&prompt.kubeuser;kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    A <literal>PersistentVolumeClaim</literal> is a request for abstract
    storage resources by a user. The <literal>PersistentVolumeClaim</literal>
    would then be associated to a pod resource to provision a
    <literal>PersistentVolume</literal>, which would be backed by a &ceph;
    block image. An optional <option>volumeMode</option> can be included to
    select between a mounted file system (default) or raw block-device-based
    volume.
   </para>
   <para>
    Using <literal>ceph-csi</literal>, specifying <option>Filesystem</option>
    for <option>volumeMode</option> can support both
    <literal>ReadWriteOnce</literal> and <literal>ReadOnlyMany
    accessMode</literal> claims, and specifying <option>Block</option> for
    <option>volumeMode</option> can support <literal>ReadWriteOnce</literal>,
    <literal>ReadWriteMany</literal>, and <literal>ReadOnlyMany
    accessMode</literal> claims.
   </para>
   <para>
    For example, to create a block-based
    <literal>PersistentVolumeClaim</literal> that uses the
    <literal>ceph-csi-based StorageClass</literal> created above, the following
    YAML file can be used to request raw block storage from the
    <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
&prompt.kubeuser;kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    The following demonstrates and example of binding the above
    <literal>PersistentVolumeClaim</literal> to a pod resource as a raw block
    device:
   </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
&prompt.kubeuser;kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    To create a file-system-based <literal>PersistentVolumeClaim</literal> that
    uses the <literal>ceph-csi-based StorageClass</literal> created above, the
    following YAML file can be used to request a mounted file system (backed by
    an RBD image) from the <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
&prompt.kubeuser;kubectl apply -f pvc.yaml</screen>
   <para>
    The following demonstrates an example of binding the above
    <literal>PersistentVolumeClaim</literal> to a pod resource as a mounted
    file system:
   </para>
<screen>&prompt.kubeuser;cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
&prompt.kubeuser;kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
