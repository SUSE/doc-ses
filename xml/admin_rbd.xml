<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-rbd">
 <title>&rbd;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A block is a sequence of bytes, for example a 4 MB block of data. Block-based
  storage interfaces are the most common way to store data with rotating media,
  such as hard disks, CDs, floppy disks. The ubiquity of block device
  interfaces makes a virtual block device an ideal candidate to interact with a
  mass data storage system like &ceph;.
 </para>
 <para>
  &ceph; block devices allow sharing of physical resources, and are resizable.
  They store data striped over multiple OSDs in a &ceph; cluster. &ceph; block
  devices leverage &rados; capabilities such as snapshotting, replication, and
  consistency. &ceph;'s &rbd;s (RBD) interact with OSDs using kernel modules or
  the <systemitem>librbd</systemitem> library.
 </para>
 <figure>
  <title>&rados; Protocol</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  &ceph;'s block devices deliver high performance with infinite scalability to
  kernel modules. They support virtualization solutions such as &qemu;, or
  cloud-based computing systems such as &ostack; that rely on &libvirt;. You
  can use the same cluster to operate the &ogw;, &cephfs;, and &rbd;s
  simultaneously.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Block Device Commands</title>

  <para>
   The <command>rbd</command> command enables you to create, list, introspect,
   and remove block device images. You can also use it, for example, to clone
   images, create snapshots, rollback an image to a snapshot, or view a
   snapshot.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creating a Block Device Image in a Replicated Pool</title>
   <para>
    Before you can add a block device to a client, you need to create a related
    image in an existing pool (see <xref linkend="ceph-pools"/>):
   </para>
<screen>
&prompt.cephuser;rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    For example, to create a 1 GB image named 'myimage' that stores information
    in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Image Size Units</title>
    <para>
     If you omit a size unit shortcut ('G' or 'T'), the image's size is in
     megabytes. Use 'G' or 'T' after the size number to specify gigabytes or
     terabytes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creating a Block Device Image in an Erasure Coded Pool</title>
   <para>
    As of &productname; 5, it is possible to store data of a block device image
    directly in erasure coded (EC) pools. A &rbd; image consists of
    <emphasis>data</emphasis> and <emphasis>metadata</emphasis> parts. You can
    store only the 'data' part of a &rbd; image in an EC pool. The pool needs
    to have the 'overwrite' flag set to <emphasis>true</emphasis>, and that is
    only possible if all OSDs where the pool is stored use &bluestore;.
   </para>
   <para>
    You cannot store the image's 'metadata' part in an EC pool. You need to
    specify the replicated pool for storing the image's metadata with the
    <option>--pool=</option> option of the <command>rbd create</command>
    command.
   </para>
   <para>
    Use the following steps to create an RBD image in a newly created EC pool:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> osd pool create <replaceable>POOL_NAME</replaceable> 12 12 erasure
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> allow_ec_overwrites true

#Metadata will reside in pool "<replaceable>OTHER_POOL</replaceable>", and data in pool "<replaceable>POOL_NAME</replaceable>"
&prompt.cephuser;<command>rbd</command> create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>POOL_NAME</replaceable> --pool=<replaceable>OTHER_POOL</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Listing Block Device Images</title>
   <para>
    To list block devices in a pool named 'mypool', execute the following:
   </para>
<screen>&prompt.cephuser;rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Retrieving Image Information</title>
   <para>
    To retrieve information from an image 'myimage' within a pool named
    'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Resizing a Block Device Image</title>
   <para>
    &rbd; images are thin provisioned&mdash;they do not actually use any
    physical storage until you begin saving data to them. However, they do have
    a maximum capacity that you set with the <option>--size</option> option. If
    you want to increase (or decrease) the maximum size of the image, run the
    following:
   </para>
<screen>
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
&prompt.cephuser;rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Removing a Block Device Image</title>
   <para>
    To remove a block device that corresponds to an image 'myimage' in a pool
    named 'mypool', run the following:
   </para>
<screen>&prompt.cephuser;rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Mounting and Unmounting</title>

  <para>
   After you create a &rbd;, you can use it like any other disk device: format
   it, mount it to be able to exchange files, and unmount it when done.
  </para>
  <para>
   The <command>rbd</command> command defaults to accessing the cluster using
   the &ceph; <literal>admin</literal> user account. This account has full
   administrative access to the cluster. This runs the risk of accidentally
   causing damage, similarly to logging into a Linux workstation as &rootuser;.
   Thus, it is preferable to create user accounts with fewer privileges and use
   these accounts for normal read/write &rbd; access.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Creating a &ceph; User Account</title>
   <para>
    To create a new user account with &mgr;, &mon;, and &osd; capabilities, use
    the <command>ceph</command> command with the <command>auth get-or-create</command>
    subcommand:
   </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    For example, to create a user called <replaceable>qemu</replaceable> with read-write
    access to the pool <replaceable>vms</replaceable> and read-only access to the pool
    <replaceable>images</replaceable>, execute the following:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    The output from the <command>ceph auth get-or-create</command> command will
    be the keyring for the specified user, which can be written to
    <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     When using the <command>rbd</command> command, you can specify the user ID by providing the
     optional <command>--id</command> <replaceable>ID</replaceable> argument.
    </para>
   </note>
   <para>
    For more details on managing &ceph; user accounts, refer to <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>User Authentication</title>
   <para>
    To specify a user name, use <option>--id
     <replaceable>user-name</replaceable></option>. If you use
    <systemitem>cephx</systemitem> authentication, you also need to specify a
    secret. It may come from a keyring or a file containing the secret:
   </para>
   <screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    or
   </para>
   <screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparing a &rbd; for Use</title>

   <procedure>
    <step>
     <para>
      Make sure your &ceph; cluster includes a pool with the disk image you want
      to map. Assume the pool is called <literal>mypool</literal> and the image
      is <literal>myimage</literal>.
     </para>
     <screen>&prompt.cephuser;rbd list mypool</screen>
    </step>
    <step>
     <para>
      Map the image to a new block device:
     </para>
     <screen>&prompt.cephuser;rbd map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen>&prompt.cephuser;rbd showmapped
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      The device we want to work on is <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>RBD Device Path</title>
      <para>
       Instead of
       <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>,
       you can use
       <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename>
       as a persistent device path. For example:
      </para>
      <screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Make an XFS file system on the <filename>/dev/rbd0</filename> device:
     </para>
     <screen>&prompt.root;mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Replacing <filename>/mnt</filename> with your mount point, mount the
      device and check it is correctly mounted:
     </para>
     <screen>&prompt.root;mount /dev/rbd0 /mnt
      &prompt.root;mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Now you can move data to and from the device as if it was a local
      directory.
     </para>
     <tip>
      <title>Increasing the Size of RBD Device</title>
      <para>
       If you find that the size of the RBD device is no longer enough, you can
       easily increase it.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Increase the size of the RBD image, for example up to 10 GB.
        </para>
        <screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Grow the file system to fill up the new size of the device:
        </para>
<screen>&prompt.root;xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      After you finish accessing the device, you can unmap and unmount it.
     </para>
<screen>
&prompt.cephuser;rbd unmap /dev/rbd0
&prompt.root;unmount /mnt
</screen>
    </step>
   </procedure>

   <tip>
    <title>Manual Mounting and Unmounting</title>
    <para>
     A <command>rbdmap</command> script and &systemd; unit is provided to make
     the process of mapping and mounting RBDs after boot, and unmounting them
     before shutdown, smoother. Refer to <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title>rbdmap: Map RBD Devices at Boot Time</title>
   <para>
    <command>rbdmap</command> is a shell script that automates <command>rbd
    map</command> and <command>rbd unmap</command> operations on one or more
    RBD images. Although you can run the script manually at any time, the main
    advantage is automatic mapping and mounting of RBD images at boot time (and
    unmounting and unmapping at shutdown), as triggered by the Init system. A
    &systemd; unit file, <filename>rbdmap.service</filename> is included with
    the <systemitem>ceph-common</systemitem> package for this purpose.
   </para>
   <para>
    The script takes a single argument, which can be either
    <option>map</option> or <option>unmap</option>. In either case, the script
    parses a configuration file. It defaults to
    <filename>/etc/ceph/rbdmap</filename>, but can be overridden via an
    environment variable <literal>RBDMAPFILE</literal>. Each line of the
    configuration file corresponds to an RBD image which is to be mapped, or
    unmapped.
   </para>
   <para>
    The configuration file has the following format:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Path to an image within a pool. Specify as
       <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       An optional list of parameters to be passed to the underlying
       <command>rbd map</command> command. These parameters and their values
       should be specified as a comma-separated string, for example:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       The example makes the <command>rbdmap</command> script run the following
       command:
      </para>
<screen>&prompt.cephuser;rbd map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       In the following example you can see how to specify a user name and a
       keyring with a corresponding secret:
      </para>
<screen>&prompt.cephuser;rbdmap map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    When run as <command>rbdmap map</command>, the script parses the
    configuration file, and for each specified RBD image, it attempts to first
    map the image (using <command>the rbd map</command> command) and then mount
    the image.
   </para>
   <para>
    When run as <command>rbdmap unmap</command>, images listed in the
    configuration file will be unmounted and unmapped.
   </para>
   <para>
    <command>rbdmap unmap-all</command> attempts to unmount and subsequently
    unmap all currently mapped RBD images, regardless of whether they are
    listed in the configuration file.
   </para>
   <para>
    If successful, the rbd map operation maps the image to a /dev/rbdX device,
    at which point a udev rule is triggered to create a friendly device name
    symbolic link
    <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename>
    pointing to the real mapped device.
   </para>
   <para>
    In order for mounting and unmounting to succeed, the 'friendly' device name
    needs to have a corresponding entry in <filename>/etc/fstab</filename>.
    When writing <filename>/etc/fstab</filename> entries for RBD images,
    specify the 'noauto' (or 'nofail') mount option. This prevents the Init
    system from trying to mount the device too early&mdash;before the device in
    question even exists, as <filename>rbdmap.service</filename> is typically
    triggered quite late in the boot sequence.
   </para>
   <para>
    For a complete list of <command>rbd</command> options, see the
    <command>rbd</command> manual page (<command>man 8 rbd</command>).
   </para>
   <para>
    For examples of the <command>rbdmap</command> usage, see the
    <command>rbdmap</command> manual page (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2>
   <title>Increasing the Size of RBD Device</title>
   <para>
    If you find that the size of the RBD device is no longer enough, you can
    easily increase it.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Increase the size of the RBD image, for example up to 10GB.
     </para>
<screen>&prompt.cephuser;rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Grow the file system to fill up the new size of the device.
     </para>
<screen>&prompt.root;xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Snapshots</title>

  <para>
   An RBD snapshot is a snapshot of a &rbd; image. With snapshots, you retain a
   history of the image's state. &ceph; also supports snapshot layering, which
   allows you to clone VM images quickly and easily. &ceph; supports block
   device snapshots using the <command>rbd</command> command and many
   higher-level interfaces, including &qemu;, <systemitem>libvirt</systemitem>,
   &ostack;, and CloudStack.
  </para>

  <note>
   <para>
    Stop input and output operations and flush all pending writes before
    snapshotting an image. If the image contains a file system, the file system
    must be in a consistent state at the time of snapshotting.
   </para>
  </note>

  <sect2>
   <title>Cephx Notes</title>
   <para>
    When <systemitem>cephx</systemitem> is enabled, you must specify a user
    name or ID and a path to the keyring containing the corresponding key for
    the user. See <xref linkend="cha-storage-cephx"/> for more details. You may
    also add the <systemitem>CEPH_ARGS</systemitem> environment variable to
    avoid re-entry of the following parameters.
   </para>
<screen>&prompt.cephuser;rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
&prompt.cephuser;rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Add the user and secret to the <systemitem>CEPH_ARGS</systemitem>
     environment variable so that you do not need to enter them each time.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>Snapshot Basics</title>
   <para>
    The following procedures demonstrate how to create, list, and remove
    snapshots using the <command>rbd</command> command on the command line.
   </para>
   <sect3>
    <title>Create Snapshot</title>
    <para>
     To create a snapshot with <command>rbd</command>, specify the <option>snap
     create</option> option, the pool name, and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap create --snap snapshot1 image1
&prompt.cephuser;rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>List Snapshots</title>
    <para>
     To list snapshots of an image, specify the pool name and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool rbd snap ls image1
&prompt.cephuser;rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3>
    <title>Rollback Snapshot</title>
    <para>
     To rollback to a snapshot with <command>rbd</command>, specify the
     <option>snap rollback</option> option, the pool name, the image name, and
     the snapshot name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rollback --snap snapshot1 image1
&prompt.cephuser;rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Rolling back an image to a snapshot means overwriting the current version
      of the image with data from a snapshot. The time it takes to execute a
      rollback increases with the size of the image. It is <emphasis>faster to
      clone</emphasis> from a snapshot <emphasis>than to rollback</emphasis> an
      image to a snapshot, and it is the preferred method of returning to a
      pre-existing state.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Delete a Snapshot</title>
    <para>
     To delete a snapshot with <command>rbd</command>, specify the <option>snap
     rm</option> option, the pool name, the image name, and the user name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap rm --snap snapshot1 image1
&prompt.cephuser;rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      &ceph; OSDs delete data asynchronously, so deleting a snapshot does not
      free up the disk space immediately.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Purge Snapshots</title>
    <para>
     To delete all snapshots for an image with <command>rbd</command>, specify
     the <option>snap purge</option> option and the image name.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
&prompt.cephuser;rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap purge image1
&prompt.cephuser;rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Layering</title>
   <para>
    &ceph; supports the ability to create multiple copy-on-write (COW) clones
    of a block device snapshot. Snapshot layering enables &ceph; block device
    clients to create images very quickly. For example, you might create a
    block device image with a Linux VM written to it, then, snapshot the image,
    protect the snapshot, and create as many copy-on-write clones as you like.
    A snapshot is read-only, so cloning a snapshot simplifies
    semantics&mdash;making it possible to create clones rapidly.
   </para>
   <note>
    <para>
     The terms 'parent' and 'child' mentioned in the command line examples
     below mean a &ceph; block device snapshot (parent) and the corresponding
     image cloned from the snapshot (child).
    </para>
   </note>
   <para>
    Each cloned image (child) stores a reference to its parent image, which
    enables the cloned image to open the parent snapshot and read it.
   </para>
   <para>
    A COW clone of a snapshot behaves exactly like any other &ceph; block
    device image. You can read to, write from, clone, and resize cloned images.
    There are no special restrictions with cloned images. However, the
    copy-on-write clone of a snapshot refers to the snapshot, so you
    <emphasis>must</emphasis> protect the snapshot before you clone it.
   </para>
   <note>
    <title><option>--image-format 1</option> Not Supported</title>
    <para>
     You cannot create snapshots of images created with the deprecated
     <command>rbd create --image-format 1</command> option. &ceph; only
     supports cloning of the default <emphasis>format 2</emphasis> images.
    </para>
   </note>
   <sect3>
    <title>Getting Started with Layering</title>
    <para>
     &ceph; block device layering is a simple process. You must have an image.
     You must create a snapshot of the image. You must protect the snapshot.
     After you have performed these steps, you can begin cloning the snapshot.
    </para>
    <para>
     The cloned image has a reference to the parent snapshot, and includes the
     pool ID, image ID, and snapshot ID. The inclusion of the pool ID means
     that you may clone snapshots from one pool to images in another pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image Template</emphasis>: A common use case for block device
       layering is to create a master image and a snapshot that serves as a
       template for clones. For example, a user may create an image for a Linux
       distribution (for example, &sls;), and create a snapshot for it.
       Periodically, the user may update the image and create a new snapshot
       (for example, <command>zypper ref &amp;&amp; zypper patch</command>
       followed by <command>rbd snap create</command>). As the image matures,
       the user can clone any one of the snapshots.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Extended Template</emphasis>: A more advanced use case
       includes extending a template image that provides more information than
       a base image. For example, a user may clone an image (a VM template) and
       install other software (for example, a database, a content management
       system, or an analytics system), and then snapshot the extended image,
       which itself may be updated in the same way as the base image.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Template Pool</emphasis>: One way to use block device layering
       is to create a pool that contains master images that act as templates,
       and snapshots of those templates. You may then extend read-only
       privileges to users so that they may clone the snapshots without the
       ability to write or execute within the pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image Migration/Recovery</emphasis>: One way to use block
       device layering is to migrate or recover data from one pool into another
       pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Protecting a Snapshot</title>
    <para>
     Clones access the parent snapshots. All clones would break if a user
     inadvertently deleted the parent snapshot. To prevent data loss, you need
     to protect the snapshot before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap protect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      You cannot delete a protected snapshot.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cloning a Snapshot</title>
    <para>
     To clone a snapshot, you need to specify the parent pool, image, snapshot,
     the child pool, and the image name. You need to protect the snapshot
     before you can clone it.
    </para>
<screen>&prompt.cephuser;rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
&prompt.cephuser;rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      You may clone a snapshot from one pool to an image in another pool. For
      example, you may maintain read-only images and snapshots as templates in
      one pool, and writable clones in another pool.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Unprotecting a Snapshot</title>
    <para>
     Before you can delete a snapshot, you must unprotect it first.
     Additionally, you may <emphasis>not</emphasis> delete snapshots that have
     references from clones. You need to flatten each clone of a snapshot
     before you can delete the snapshot.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
&prompt.cephuser;rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
&prompt.cephuser;rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3>
    <title>Listing Children of a Snapshot</title>
    <para>
     To list the children of a snapshot, execute the following:
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
&prompt.cephuser;rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 children --image image1 --snap snapshot1
&prompt.cephuser;rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten">
    <title>Flattening a Cloned Image</title>
    <para>
     Cloned images retain a reference to the parent snapshot. When you remove
     the reference from the child clone to the parent snapshot, you effectively
     'flatten' the image by copying the information from the snapshot to the
     clone. The time it takes to flatten a clone increases with the size of the
     snapshot. To delete a snapshot, you must flatten the child images first.
    </para>
<screen>&prompt.cephuser;rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
&prompt.cephuser;rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --pool pool1 flatten --image image1
&prompt.cephuser;rbd flatten pool1/image1</screen>
    <note>
     <para>
      Since a flattened image contains all the information from the snapshot, a
      flattened image will take up more storage space than a layered clone.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Mirroring</title>

  <para>
   RBD images can be asynchronously mirrored between two &ceph; clusters. This
   capability is available in two modes:
  </para>
  <variablelist>
    <varlistentry>
      <term>Journal-based</term>
      <listitem>
        <para>
          This mode uses the RBD journaling image feature to ensure point-in-time,
          crash-consistent replication between clusters. Every write to the
          RBD image is first recorded to the associated journal before modifying
          the actual image. The <literal>remote</literal> cluster will read from the
          journal and replay the updates to its local copy of the image. Since
          each write to the RBD image will result in two writes to the &ceph;
          cluster, expect write latencies to nearly double when using the RBD
          journaling image feature.
        </para>
      </listitem>
      </varlistentry>
      <varlistentry>
        <term>Snapshot-based</term>
        <listitem>
          <para>
            This mode uses periodically-scheduled or manually-created RBD image
            mirror-snapshots to replicate crash-consistent RBD images between
            clusters. The <literal>remote</literal> cluster will determine any data or metadata
            updates between two mirror-snapshots, and copy the deltas to its
            local copy of the image. With the help of the RBD fast-diff image
            feature, updated data blocks can be quickly computed without the
            need to scan the full RBD image. Since this mode is not point-in-time
            consistent, the full snapshot delta will need to be synchronized prior to
            use during a failover scenario. Any partially-applied snapshot
            deltas will be rolled back to the last fully synchronized snapshot prior
            to use.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      Mirroring is configured on a per-pool basis within peer clusters. This can
      be configured on a specific subset of images within the pool, or configured
      to automatically mirror all images within a pool when using journal-based
      mirroring only. Mirroring is configured using the <command>rbd</command> command. The
      <systemitem class="daemon">rbd-mirror</systemitem> daemon is responsible for pulling image
      updates from the <literal>remote</literal>, peer cluster and applying them to the image
      within the <literal>local</literal> cluster.
    </para>
    <para>
      Depending on the desired needs for replication, RBD mirroring can be
      configured for either one- or two-way replication:
    </para>
    <variablelist>
      <varlistentry>
        <term>One-way Replication</term>
        <listitem>
          <para>
            When data is only mirrored from a primary cluster to a secondary
            cluster, the <literal>rbd-mirror</literal> daemon runs only on the secondary cluster.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>Two-way Replication</term>
        <listitem>
          <para>
            When data is mirrored from primary images on one cluster to
            non-primary images on another cluster (and vice-versa), the
            <literal>rbd-mirror</literal> daemon runs on both clusters.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
      <important>
        <para>
          Each instance of the <literal>rbd-mirror</literal> daemon needs to be able to connect to
          both the <literal>local</literal> and <literal>remote</literal> &ceph; clusters simultaneously. For example, all
          monitor and OSD hosts. Additionally, the network needs to have sufficient
          bandwidth between the two data centers to handle mirroring workload.
        </para>
      </important>
  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Pool Configuration</title>
   <para>
    The following procedures demonstrate how to perform the basic
    administrative tasks to configure mirroring using the
    <command>rbd</command> command. Mirroring is configured on a per-pool basis
    within the &ceph; clusters.
   </para>
   <para>
    You need to perform the pool configuration steps on both peer clusters.
    These procedures assume two clusters, named <literal>local</literal> and <literal>remote</literal>, are
    accessible from a single host for clarity.
   </para>
   <para>
    See the <command>rbd</command> manual page (<command>man 8 rbd</command>)
    for additional details on how to connect to different &ceph; clusters.
   </para>
   <tip>
    <title>Multiple Clusters</title>
    <para>
     The cluster name in the following examples corresponds to a &ceph;
     configuration file of the same name
     <filename>/etc/ceph/remote.conf</filename>.
    </para>
   </tip>
   <sect3>
    <title>Enable Mirroring on a Pool</title>
    <para>
     To enable mirroring on a pool, specify the <command>mirror pool
     enable</command> subcommand, the pool name, and the mirroring mode. The
     mirroring mode can either be pool or image:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        All images in the pool with the journaling feature enabled are
        mirrored.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        Mirroring needs to be explicitly enabled on each image. See
        <xref linkend="rbd-mirror-enable-image-mirroring"/> for more
        information.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
&prompt.cephuser;rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3>
    <title>Disable Mirroring</title>
    <para>
     To disable mirroring on a pool, specify the <command>mirror pool
     disable</command> subcommand and the pool name. When mirroring is disabled
     on a pool in this way, mirroring will also be disabled on any images
     (within the pool) for which mirroring was enabled explicitly.
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
&prompt.cephuser;rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>

   <sect3>
     <title>Boostrap Peers</title>
     <para>
       In order for the <literal>rbd-mirror</literal> daemon to discover its peer cluster, the
       peer needs to be registered to the pool and a user account needs to
       be created. This process can be automated with <command>rbd</command> and the
       <command>mirror pool peer bootstrap create</command> and
       <command>mirror pool peer bootstrap import</command> commands.
     </para>
     <para>
       To manually create a new bootstrap token with <command>rbd</command>, specify the <command>mirror pool
       peer bootstrap create</command> command, a pool name, along with an optional
       friendly site name to describe the <literal>local</literal> cluster:
     </para>
<screen>&prompt.cephuser;rbd mirror pool peer bootstrap create [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
     <para>
       The output of <command>mirror pool peer bootstrap create</command> will be a token that
       should be provided to the <command>mirror pool peer bootstrap import</command> command.
       For example, on <literal>local</literal>:
     </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
     <para>
       To manually import the bootstrap token created by another cluster with the
       <command>rbd</command> command, specify the following:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <command>mirror pool peer bootstrap import</command>
         </para>
       </listitem>
       <listitem>
         <para>
           The pool name
         </para>
       </listitem>
       <listitem>
         <para>
           A file path to the created token (or <literal>-</literal> to read it from the standard input)
         </para>
       </listitem>
       <listitem>
         <para>
           An optional friendly site name to describe the <literal>local</literal> cluster
         </para>
       </listitem>
       <listitem>
         <para>
           A mirroring direction (defaults to <literal>rx-tx</literal> for
           bidirectional mirroring, but can also be set to <literal>rx-only</literal>
           for unidirectional mirroring)
         </para>
       </listitem>
     </itemizedlist>
<screen>&prompt.cephuser;rbd mirror pool peer bootstrap import [--site-name {local-site-name}] [--direction {rx-only or rx-tx}] {pool-name} {token-path}</screen>
     <para>
       For example, on <literal>remote</literal>:
     </para>
<screen>&prompt.cephuser;cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen>&prompt.cephuser;rbd --cluster remote mirror pool peer bootstrap import --site-name remote image-pool token</screen>
   </sect3>

   <sect3>
    <title>Add Cluster Peer</title>
    <para>
     In order for the <systemitem>rbd-mirror</systemitem> daemon to discover
     its peer cluster, the peer needs to be registered to the pool. To add a
     mirroring peer cluster, specify the <command>mirror pool peer
     add</command> subcommand, the pool name, and a cluster specification:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool peer add <replaceable>POOL_NAME</replaceable> client.remote@remote
&prompt.cephuser;rbd --cluster remote mirror pool peer add <replaceable>POOL_NAME</replaceable> client.local@local</screen>
   </sect3>
   <sect3>
    <title>Remove Cluster Peer</title>
    <para>
     To remove a mirroring peer cluster, specify the <command>mirror pool peer
     remove</command> subcommand, the pool name, and the peer UUID (available
     from the <command>rbd mirror pool info</command> command):
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
&prompt.cephuser;rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08</screen>
   </sect3>
   <sect3>
     <title>Data Pools</title>
     <para>
       When creating images in the destination cluster, <literal>rbd-mirror</literal> selects
       a data pool as follows:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           If the destination cluster has a default data pool configured (with
           the <option>rbd_default_data_pool</option> configuration option), it will be used.
         </para>
       </listitem>
       <listitem>
         <para>
           Otherwise, if the source image uses a separate data pool, and a
           pool with the same name exists on the destination cluster, that
           pool will be used.
         </para>
       </listitem>
       <listitem>
         <para>
           If neither of the above is true, no data pool will be set.
         </para>
       </listitem>
     </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Image Configuration</title>
   <para>
    Unlike pool configuration, image configuration only needs to be performed
    against a single mirroring peer &ceph; cluster.
   </para>
   <para>
    Mirrored RBD images are designated as either <emphasis>primary</emphasis>
    or <emphasis>non-primary</emphasis>. This is a property of the image and
    not the pool. Images that are designated as non-primary cannot be modified.
   </para>
   <para>
    Images are automatically promoted to primary when mirroring is first
    enabled on an image (either implicitly if the pool mirror mode was 'pool'
    and the image has the journaling image feature enabled, or explicitly (see
    <xref linkend="rbd-mirror-enable-image-mirroring"/>) by the
    <command>rbd</command> command).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Enable Image Mirroring</title>
    <para>
     If mirroring is configured in the <literal>image</literal> mode, then it is necessary to
     explicitly enable mirroring for each image within the pool. To enable
     mirroring for a specific image with <command>rbd</command>, specify the <command>mirror image
     enable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
      The mirror image mode can either be <literal>journal</literal> or <literal>snapshot</literal>:
    </para>
    <variablelist>
      <varlistentry>
        <term>journal (default)</term>
        <listitem>
          <para>
            When configured in <literal>journal</literal> mode, mirroring will utilize the RBD
            journaling image feature to replicate the image contents. If the
            RBD journaling image feature is not yet enabled on the image, it
            will be automatically enabled.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term>snapshot</term>
        <listitem>
          <para>
            When configured in <literal>snapshot</literal> mode, mirroring will utilize RBD
            image mirror-snapshots to replicate the image contents. Once
            enabled, an initial mirror-snapshot will automatically be created.
            Additional RBD image mirror-snapshots can be created by the
            <command>rbd</command> command.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      For example:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image enable image-pool/image-1 snapshot
&prompt.cephuser;rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3>
    <title>Enable Image Journaling Feature</title>
    <para>
     RBD mirroring uses the RBD journaling feature to ensure that the
     replicated image always remains crash-consistent. When using the <literal>image</literal>
     mirroring mode, the journaling feature will be automatically enabled
     if mirroring is enabled on the image. When using the <literal>pool</literal> mirroring
     mode, before an image can be mirrored to a peer cluster, the RBD image
     journaling feature must be enabled. The feature can be enabled at image
     creation time by providing the <option>--image-feature exclusive-lock,journaling</option>
     option to the <command>rbd</command> command.
    </para>
    <para>
     Alternatively, the journaling feature can be dynamically enabled on
     pre-existing RBD images. To enable journaling, specify the
     <command>feature enable</command> subcommand, the pool and image name, and
     the feature name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Option Dependency</title>
     <para>
      The <option>journaling</option> feature is dependent on the
      <option>exclusive-lock</option> feature. If the
      <option>exclusive-lock</option> feature is not already enabled, you need
      to enable it prior to enabling the <option>journaling</option> feature.
     </para>
    </note>
    <tip>
      <para>
        You can enable journaling on all new images by default by adding
        <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option>
        to your &ceph; configuration file.
      </para>
    </tip>
   </sect3>

   <sect3>
     <title>Create Image Mirror-Snapshots</title>
     <para>
       When using snapshot-based mirroring, mirror-snapshots will need to be
       created whenever it is desired to mirror the changed contents of the
       RBD image. To create a mirror-snapshot manually with <command>rbd</command>, specify the
       <command>mirror image snapshot</command> command along with the pool and image name:
     </para>
<screen>&prompt.cephuser;rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
     <para>
       For example:
     </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image snapshot image-pool/image-1</screen>
     <para>
       By default only three mirror-snapshots will be created per image. The most
       recent mirror-snapshot is automatically pruned if the limit is reached.
       The limit can be overridden via the <option>rbd_mirroring_max_mirroring_snapshots</option>
       configuration option if required. Additionally, mirror-snapshots are
       automatically deleted when the image is removed or when mirroring is
       disabled.
     </para>
     <para>
       Mirror-snapshots can also be automatically created on a periodic basis
       if mirror-snapshot schedules are defined. The mirror-snapshot can be
       scheduled globally, per-pool, or per-image levels. Multiple mirror-snapshot
       schedules can be defined at any level, but only the most-specific
       snapshot schedules that match an individual mirrored image will run.
     </para>
     <para>
       To create a mirror-snapshot schedule with <command>rbd</command>, specify the
       <command>mirror snapshot schedule add</command> command along with an optional pool or
       image name, interval, and optional start time.
     </para>
     <para>
       The interval can be specified in days, hours, or minutes using the suffixes <option>d</option>, <option>h</option>, or <option>m</option>
       respectively. The optional start time can be specified using
       the ISO 8601 time format. For example:
     </para>
<screen>
&prompt.cephuser;rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
&prompt.cephuser;rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
     <para>
       To remove a mirror-snapshot schedule with <command>rbd</command>, specify
       the <command>mirror snapshot schedule remove</command> command with options
       that match the corresponding add schedule command.
     </para>
     <para>
       To list all snapshot schedules for a specific level (global, pool, or
       image) with <command>rbd</command>, specify the <command>mirror snapshot schedule ls</command> command along
       with an optional pool or image name. Additionally, the <option>--recursive</option> option
       can be specified to list all schedules at the specified level and below.
       For example:
     </para>
<screen>&prompt.cephuser;rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
     <para>
       To find out when the next snapshots will be created for
       snapshot-based mirroring RBD images with <command>rbd</command>, specify the <command>mirror
       snapshot schedule status</command> command along with an optional pool or image name.
       For example:
     </para>
<screen>&prompt.cephuser;rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>

   <sect3>
    <title>Disable Image Mirroring</title>
    <para>
     To disable mirroring for a specific image, specify the <command>mirror
     image disable</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3>
    <title>Image Promotion and Demotion</title>
    <para>
     In a failover scenario where the primary designation needs to be moved to
     the image in the peer cluster, you need to stop access to the primary
     image, demote the current primary image, promote the new primary image,
     and resume access to the image on the alternate cluster.
    </para>
    <note>
     <title>Forced Promotion</title>
     <para>
      Promotion can be forced using the <option>--force</option> option. Forced
      promotion is needed when the demotion cannot be propagated to the peer
      cluster (for example, in case of cluster failure or communication
      outage). This will result in a split-brain scenario between the two
      peers, and the image will no longer be synchronized until a
      <command>resync</command> subcommand is issued.
     </para>
    </note>
    <para>
     To demote a specific image to non-primary, specify the <command>mirror
     image demote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To demote all primary images within a pool to non-primary, specify the
     <command>mirror pool demote</command> subcommand along with the pool name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     To promote a specific image to primary, specify the <command>mirror image
     promote</command> subcommand along with the pool and image name:
    </para>
<screen>&prompt.cephuser;rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     To promote all non-primary images within a pool to primary, specify the
     <command>mirror pool promote</command> subcommand along with the pool
     name:
    </para>
<screen>&prompt.cephuser;rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Split I/O Load</title>
     <para>
      Since the primary or non-primary status is per-image, it is possible to
      have two clusters split the I/O load and stage failover or failback.
     </para>
    </tip>
   </sect3>
   <sect3>
    <title>Force Image Resync</title>
    <para>
     If a split-brain event is detected by the
     <systemitem>rbd-mirror</systemitem> daemon, it will not attempt to mirror
     the affected image until corrected. To resume mirroring for an image,
     first demote the image determined to be out of date and then request a
     resync to the primary image. To request an image resync, specify the
     <command>mirror image resync</command> subcommand along with the pool and
     image name:
    </para>
<screen>&prompt.cephuser;rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Mirror Status</title>
   <para>
    The peer cluster replication status is stored for every primary mirrored
    image. This status can be retrieved using the <command>mirror image
    status</command> and <command>mirror pool status</command> subcommands:
   </para>
   <para>
    To request the mirror image status, specify the <command>mirror image
    status</command> subcommand along with the pool and image name:
   </para>
<screen>&prompt.cephuser;rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    To request the mirror pool summary status, specify the <command>mirror pool
    status</command> subcommand along with the pool name:
   </para>
<screen>&prompt.cephuser;rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Adding the <option>--verbose</option> option to the <command>mirror pool
     status</command> subcommand will additionally output status details for
     every mirroring image in the pool.
    </para>
   </tip>
  </sect2>
  <sect2 xml:id="rbd-mirror-daemon">
    <title>rbd-mirror Daemon</title>
    <para>
      The two <systemitem class="daemon">rbd-mirror</systemitem> daemons are responsible for watching image journals
      on the <literal>remote</literal> peer cluster and replaying the journal events against
      the <literal>local</literal> cluster. The RBD image journaling feature records all
      modifications to the image in the order they occur. This ensures that
      a crash-consistent mirror of the remote image is available locally.
    </para>
    <important>
      <para>
        Each <command>rbd-mirror</command> daemon requires the ability to connect
        to both clusters simultaneously.
      </para>
    </important>
    <para>
      Each <systemitem class="daemon">rbd-mirror</systemitem> daemon should use a unique &ceph; user ID. To create a
      &ceph; user, use the <command>ceph auth get-or-create</command> command, followed by a user name,
      monitor caps, and OSD caps:
    </para>
<screen>&prompt.cephuser;ceph auth get-or-create client.rbd-mirror.<replaceable>UNIQUE-ID</replaceable> mon 'profile rbd-mirror' osd 'profile rbd'</screen>
    <para>
      The <systemitem class="daemon">rbd-mirror</systemitem> daemon can be managed by &systemd; by
      specifying the user ID as the daemon instance:
    </para>
<screen>&prompt.sminion;systemctl enable ceph-rbd-mirror@rbd-mirror.<replaceable>UNIQUE-ID</replaceable></screen>
    <para>
      The <command>rbd-mirror</command> can also be run in foreground by <command>rbd-mirror</command> command:
    </para>
<screen>&prompt.cephuser;rbd-mirror -f --log-file=<replaceable>LOG-PATH</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Cache Settings</title>

  <para>
   The user space implementation of the &ceph; block device
   (<systemitem>librbd</systemitem>) cannot take advantage of the Linux page
   cache. Therefore, it includes its own in-memory caching. RBD caching behaves
   similar to hard disk caching. When the OS sends a barrier or a flush
   request, all 'dirty' data is written to the OSDs. This means that using
   write-back caching is just as safe as using a well-behaved physical hard
   disk with a VM that properly sends flushes. The cache uses a <emphasis>Least
   Recently Used</emphasis> (LRU) algorithm, and in write-back mode it can
   merge adjacent requests for better throughput.
  </para>

  <para>
   &ceph; supports write-back caching for RBD. To enable it, run
  </para>

<screen>&prompt.cephuser;ceph config set client rbd_cache true</screen>

  <para>
   to the <literal>[client]</literal> section of your
   <filename>ceph.conf</filename> file. By default,
   <systemitem>librbd</systemitem> does not perform any caching. Writes and
   reads go directly to the storage cluster, and writes return only when the
   data is on disk on all replicas. With caching enabled, writes return
   immediately, unless there are more unflushed bytes than set in the
   <option>rbd cache max dirty</option> option. In such a case, the write
   triggers writeback and blocks until enough bytes are flushed.
  </para>

  <para>
   &ceph; supports write-through caching for RBD. You can set the size of the
   cache, and you can set targets and limits to switch from write-back caching
   to write-through caching. To enable write-through mode, run
  </para>

<screen>&prompt.cephuser;ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   This means writes return only when the data is on disk on all replicas, but
   reads may come from the cache. The cache is in memory on the client, and
   each RBD image has its own cache. Since the cache is local to the client,
   there is no coherency if there are others accessing the image. Running GFS
   or OCFS on top of RBD will not work with caching enabled.
  </para>

  <para>
   The following parameters affect the behavior of &rbd;s. To set them,
   use the <literal>client</literal> category:
  </para>

<screen>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Enable caching for &rbd; (RBD). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      The RBD cache size in bytes. Default is 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      The 'dirty' limit in bytes at which the cache triggers write-back.
      <option>rbd cache max dirty</option> needs to be less than <option>rbd
      cache size</option>. If set to 0, uses write-through caching. Default is
      24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      The 'dirty target' before the cache begins writing data to the data
      storage. Does not block writes to the cache. Default is 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      The number of seconds dirty data is in the cache before writeback starts.
      Default is 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Start out in write-through mode, and switch to write-back after the first
      flush request is received. Enabling this is a conservative but safe
      setting in case virtual machines running on <systemitem>rbd</systemitem>
      are too old to send flushes (for example, the virtio driver in Linux
      before kernel 2.6.32). Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS Settings</title>

  <para>
   Generally, Quality of Service (QoS) refers to methods of traffic
   prioritization and resource reservation. It is particularly important for
   the transportation of traffic with special requirements.
  </para>

  <important>
   <title>Not Supported by &iscsi;</title>
   <para>
    The following QoS settings are used only by the userspace RBD
    implementation <systemitem class="daemon">librbd</systemitem> and
    <emphasis>not</emphasis> used by the <systemitem>kRBD</systemitem>
    implementation. Because &iscsi; uses <systemitem>kRBD</systemitem>, it does
    not use the QoS settings. However, for &iscsi; you can configure QoS on the
    kernel block device layer using standard kernel facilities.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      The desired limit of I/O bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      The desired limit of read operations per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      The desired limit of write operations per second. Default is 0 (no
      limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      The desired limit of read bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      The desired limit of write bytes per second. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of I/O bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write operations. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of read bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      The desired burst limit of write bytes. Default is 0 (no limit).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      The minimum schedule tick (in milliseconds) for QoS. Default is 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Read-ahead Settings</title>

  <para>
   &rbd; supports read-ahead/prefetching to optimize small, sequential reads.
   This should normally be handled by the guest OS in the case of a virtual
   machine, but boot loaders may not issue efficient reads. Read-ahead is
   automatically disabled if caching is disabled.
  </para>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Number of sequential read requests necessary to trigger read-ahead.
      Default is 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Maximum size of a read-ahead request. If set to 0, read-ahead is
      disabled. Default is 512 kB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      After this many bytes have been read from an RBD image, read-ahead is
      disabled for that image until it is closed. This allows the guest OS to
      take over read-ahead when it is booted. If set to 0, read-ahead stays
      enabled. Default is 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Advanced Features</title>

  <para>
   &rbd; supports advanced features that enhance the functionality of RBD
   images. You can specify the features either on the command line when
   creating an RBD image, or in the &ceph; configuration file by using the
   <option>rbd_default_features</option> option.
  </para>

  <para>
   You can specify the values of the <option>rbd_default_features</option>
   option in two ways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     As a sum of features' internal values. Each feature has its own internal
     value&mdash;for example 'layering' has 1 and 'fast-diff' has 16. Therefore
     to activate these two feature by default, include the following:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     As a comma-separated list of features. The previous example will look as
     follows:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Features Not Supported by &iscsi;</title>
   <para>
    RBD images with the following features will not be supported by &iscsi;:
    <option>deep-flatten</option>, <option>object-map</option>,
    <option>journaling</option>, <option>fast-diff</option>,
    <option>striping</option>
   </para>
  </note>

  <para>
   A list of advanced RBD features follows:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Layering enables you to use cloning.
     </para>
     <para>
      Internal value is 1, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Striping spreads data across multiple objects and helps with parallelism
      for sequential read/write workloads. It prevents single node bottlenecks
      for large or busy &rbd;s.
     </para>
     <para>
      Internal value is 2, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      When enabled, it requires a client to get a lock on an object before
      making a write. Enable the exclusive lock only when a single client is
      accessing an image at the same time. Internal value is 4. Default is
      'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Object map support depends on exclusive lock support. Block devices are
      thin provisioned, meaning that they only store data that actually exists.
      Object map support helps track which objects actually exist (have data
      stored on a drive). Enabling object map support speeds up I/O operations
      for cloning, importing and exporting a sparsely populated image, and
      deleting.
     </para>
     <para>
      Internal value is 8, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Fast-diff support depends on object map support and exclusive lock
      support. It adds another property to the object map, which makes it much
      faster to generate diffs between snapshots of an image and the actual
      data usage of a snapshot.
     </para>
     <para>
      Internal value is 16, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      Deep-flatten makes the <command>rbd flatten</command> (see
      <xref linkend="rbd-flatten" />) work on all the snapshots of an image, in
      addition to the image itself. Without it, snapshots of an image will
      still rely on the parent, therefore you will not be able to delete the
      parent image until the snapshots are deleted. Deep-flatten makes a parent
      independent of its clones, even if they have snapshots.
     </para>
     <para>
      Internal value is 32, default is 'yes'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      Journaling support depends on exclusive lock support. Journaling records
      all modifications to an image in the order they occur. RBD mirroring (see
      <xref linkend="ceph-rbd-mirror" />) uses the journal to replicate a crash
      consistent image to a <literal>remote</literal> cluster.
     </para>
     <para>
      Internal value is 64, default is 'no'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Mapping RBD Using Old Kernel Clients</title>

  <para>
   Old clients (for example, SLE11 SP4) may not be able to map RBD images
   because a cluster deployed with &productname; &productnumber; forces some
   features (both RBD image level features and RADOS level features) that these
   old clients do not support. When this happens, the OSD logs will show
   messages similar to the following:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 >> \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Changing &crushmap; Bucket Types Causes Massive Rebalancing</title>
   <para>
    If you intend to switch the &crushmap; bucket types between 'straw' and
    'straw2', do it in a planned manner. Expect a significant impact on the
    cluster load because changing bucket type will cause massive cluster
    rebalancing.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Disable any RBD image features that are not supported. For example:
    </para>
<screen>
&prompt.cephuser;rbd feature disable pool1/image1 object-map
&prompt.cephuser;rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Change the &crushmap; bucket types from 'straw2' to 'straw':
    </para>
    <substeps>
     <step>
      <para>
       Save the &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Decompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Edit the &crushmap; and replace 'straw2' with 'straw'.
      </para>
     </step>
     <step>
      <para>
       Recompile the &crushmap;:
      </para>
<screen>
&prompt.cephuser;crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Set the new &crushmap;:
      </para>
<screen>
&prompt.cephuser;ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
</chapter>
