<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-cephosd">
<!-- ============================================================== -->
 <title>&ceph; OSD management</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="ceph-osd-management">
  <title>&ceph; OSD management</title>

  <para>
   &ceph; &objstore; Daemons (OSDs) are the heart and soul of the &ceph;
   storage platform. Each OSD manages a local device and together they provide
   the distributed storage. &rook; will automate creation and management of
   OSDs to hide the complexity based on the desired state in the CephCluster CR
   as much as possible. This guide will walk through some of the scenarios to
   configure OSDs where more configuration may be required.
  </para>

  <sect2 xml:id="osd-health">
   <title>Analyzing OSD health</title>
   <para>
    The <literal>rook-ceph-tools</literal> pod provides a simple environment to
    run &ceph; tools. The &ceph; commands mentioned in this document should be
    run from the toolbox.
   </para>
   <para>
    Once created, connect to the pod to execute the <command>ceph</command>
    commands to analyze the health of the cluster, in particular the OSDs and
    placement groups (PGs). Some common commands to analyze OSDs include:
   </para>
<screen>&prompt.cephuser;ceph status
&prompt.cephuser;ceph osd tree
&prompt.cephuser;ceph osd status
&prompt.cephuser;ceph osd df
&prompt.cephuser;ceph osd utilization
</screen>
<screen>kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l &quot;app=rook-ceph-tools&quot; -o jsonpath='{.items[0].metadata.name}') bash</screen>
  </sect2>

  <sect2 xml:id="add-an-osd">
   <title>Adding an OSD</title>
   <para>
    To add more OSDs, &rook; automatically watches for new nodes and devices
    being added to your cluster. If they match the filters or other settings in
    the <literal>storage</literal> section of the cluster CR, the operator will
    create new OSDs.
   </para>
  </sect2>

  <sect2 xml:id="add-an-osd-on-a-pvc">
   <title>Adding an OSD on a PVC</title>
   <para>
    In more dynamic environments where storage can be dynamically provisioned
    with a raw block storage provider, the OSDs can be backed by PVCs.
   </para>
   <para>
    To add more OSDs, you can either increase the <literal>count</literal> of
    the OSDs in an existing device set or you can add more device sets to the
    cluster CR. The operator will then automatically create new OSDs according
    to the updated cluster CR.
   </para>
  </sect2>

  <sect2 xml:id="remove-an-osd">
   <title>Removing an OSD</title>
   <para>
    Removal of OSDs is intentionally not automated. &rook;â€™s charter is to
    keep your data safe, not to delete it. If you are sure you need to remove
    OSDs, it can be done. We just want you to be in control of this action.
   </para>
   <para>
    To remove an OSD due to a failed disk or other re-configuration, consider
    the following to ensure the health of the data through the removal process:
   </para>
   <procedure>
    <step>
     <para>
      Confirm you will have enough space on your cluster after removing your
      OSDs to properly handle the deletion.
     </para>
    </step>
    <step>
     <para>
      Confirm the remaining OSDs and their placement groups (PGs) are healthy
      in order to handle the rebalancing of the data.
     </para>
    </step>
    <step>
     <para>
      Do not remove too many OSDs at once, wait for rebalancing between
      removing multiple OSDs.
     </para>
    </step>
    <step>
     <para>
      On host-based clusters, you may need to stop the &rook; Operator while
      performing OSD removal steps in order to prevent &rook; from detecting
      the old OSD and trying to re-create it before the disk is wiped or
      removed.
     </para>
    </step>
   </procedure>
   <para>
    If all the PGs are <literal>active+clean</literal> and there are no
    warnings about being low on space, this means the data is fully replicated
    and it is safe to proceed. If an OSD is failing, the PGs will not be
    perfectly clean, and you will need to proceed anyway.
   </para>
   <sect3 xml:id="from-the-toolbox">
    <title>From the toolbox</title>
    <orderedlist numeration="arabic" spacing="compact">
     <listitem>
      <para>
       Determine the OSD ID for the OSD to be removed. The OSD pod may be in an
       error state, such as <literal>CrashLoopBackoff</literal>, or the
       <command>ceph</command> commands in the toolbox may show which OSD is
       <literal>down</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Mark the OSD as <literal>out</literal> if not already marked as such by
       &ceph;. This signals &ceph; to start moving (backfilling) the data that
       was on that OSD to another OSD.
      </para>
<screen>ceph osd out osd.<replaceable>ID</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&cephuser;ceph osd out osd.23</screen>
     </listitem>
     <listitem>
      <para>
       Wait for the data to finish backfilling to other OSDs.
      </para>
      <para>
       <command>ceph status</command> will indicate the backfilling is done
       when all of the PGs are <literal>active+clean</literal>. It is safe to
       remove the disk after that.
      </para>
     </listitem>
     <listitem>
      <para>
       Update your CephCluster CR such that the operator will not create an OSD
       on the device anymore. Depending on your CR settings, you may need to
       remove the device from the list or update the device filter. If you are
       using <option>useAllDevices: true</option>, no change to the CR is
       necessary.
      </para>
     </listitem>
     <listitem>
      <para>
       Remove the OSD from the &ceph; cluster:
      </para>
<screen>&prompt.cephuser;ceph osd purge <replaceable>ID</replaceable> --yes-i-really-mean-it</screen>
     </listitem>
     <listitem>
      <para>
       Verify the OSD is removed from the node in the CRUSH map:
      </para>
<screen>&prompt.cephuser;ceph osd tree</screen>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="remove-the-osd-deployment">
    <title>Removing the OSD deployment</title>
    <para>
     The operator can automatically remove OSD deployments that are considered
     <quote>safe-to-destroy</quote> by &ceph;. After the steps above, the OSD
     will be considered safe to remove since the data has all been moved to
     other OSDs. But this will only be done automatically by the operator if
     you have this setting in the cluster CR:
    </para>
<screen>removeOSDsIfOutAndSafeToRemove: true</screen>
    <para>
     Otherwise, you will need to delete the deployment directly:
    </para>
<screen>&prompt.kubeuser;kubectl delete deployment -n rook-ceph rook-ceph-osd-<replaceable>ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="replace-an-osd">
   <title>Replacing an OSD</title>
   <para>
    To replace a disk that has failed:
   </para>
   <procedure>
    <step>
     <para>
      Run the steps in the previous section to <xref linkend="remove-an-osd"/>.
     </para>
    </step>
    <step>
     <para>
      Replace the physical device and verify the new device is attached.
     </para>
    </step>
    <step>
     <para>
      Check if your cluster CR will find the new device. If you are using
      <option>useAllDevices: true</option> you can skip this step. If your
      cluster CR lists individual devices or uses a device filter you may need
      to update the CR.
     </para>
    </step>
    <step>
     <para>
      The operator ideally will automatically create the new OSD within a few
      minutes of adding the new device or updating the CR. If you do not see a
      new OSD automatically created, restart the operator (by deleting the
      operator pod) to trigger the OSD creation.
     </para>
    </step>
    <step>
     <para>
      Verify if the OSD is created on the node by running <command>ceph osd
      tree</command> from the toolbox.
     </para>
    </step>
   </procedure>
   <note>
    <para>
     The OSD might have a different ID than the previous OSD that was replaced.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="remove-an-osd-from-a-pvc">
   <title>Removing an OSD from a PVC</title>
   <para>
    If you have installed your OSDs on top of PVCs and you desire to reduce the
    size of your cluster by removing OSDs:
   </para>
   <procedure>
    <step>
     <para>
      Shrink the number of OSDs in the <literal>storageClassDeviceSet</literal>
      in the CephCluster CR.
     </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph edit cephcluster rook-ceph</screen>
     <para>
      Reduce the <literal>count</literal> of the OSDs to the desired number.
      &rook; will not take any action to automatically remove the extra OSD(s),
      but will effectively stop managing the orphaned OSD.
     </para>
    </step>
    <step>
     <para>
      Identify the orphaned PVC that belongs to the orphaned OSD.
     </para>
     <note>
      <para>
       The orphaned PVC will have the highest index among the PVCs for the
       device set.
      </para>
     </note>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pvc -l ceph.rook.io/DeviceSet=<replaceable>deviceSet</replaceable></screen>
     <para>
      For example if the device set is named <literal>set1</literal> and the
      <literal>count</literal> was reduced from <literal>3</literal> to
      <literal>2</literal>, the orphaned PVC would have the index
      <literal>2</literal> and might be named
      <literal>set1-2-data-vbwcf</literal>
     </para>
    </step>
    <step>
     <para>
      Identify the orphaned OSD.
     </para>
     <note>
      <para>
       The OSD assigned to the PVC can be found in the labels on the PVC.
      </para>
     </note>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pod -l ceph.rook.io/pvc=<replaceable>ORPHANED_PVC</replaceable> -o yaml | grep ceph-osd-id</screen>
     <para>
      For example, this might return:
     </para>
<screen>&prompt.cephuser;ceph-osd-id: &quot;0&quot;</screen>
    </step>
    <step>
     <para>
      Now, proceed with the steps in the section above to
      <xref linkend="remove-an-osd"/> for the orphaned OSD ID.
     </para>
    </step>
    <step>
     <para>
      If desired, delete the orphaned PVC after the OSD is removed.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
