<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="deploy-rook">
 <!-- ============================================================== -->
 <title>Quick start</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &productname; is a distributed storage system designed for scalability,
  reliability, and performance, which is based on the &ceph; technology. The
  traditional way to run a &ceph; cluster is setting up a dedicated cluster to
  provide block, file, and object storage to a variety of clients.
 </para>
 <para>
  &rook; manages &ceph; as a containerized application on &kube; and allows a
  hyper-converged setup, in which a single &kube; cluster runs applications and
  storage together. The primary purpose of &productname; deployed with &rook;
  is to provide storage to other applications running in the &kube; cluster.
  This can be block, file, or object storage.
 </para>
 <para>
  This chapter describes how to quickly deploy containerized &productname;
  &productnumber; on top of a &caasp; 4.5 &kube; cluster.
 </para>
 <sect1 xml:id="rook-deploy-hardware-specs">
  <title>Recommended hardware specifications</title>

  <para>
   For &productname; deployed with &rook;, the minimal configuration is
   preliminary, we will update it based on real customer needs.
   <remark>
       LGHP 2020-10-07: Should we at least link to the CaaSP system requirements?
     </remark>
  </para>

  <para>
   For the purpose of this document, consider the following minimum
   configuration:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A highly available &kube; cluster with 3 master nodes
    </para>
   </listitem>
   <listitem>
    <para>
     Four physical &kube; worker nodes, each with two OSD disks and 5GB of RAM
     per OSD disk
    </para>
   </listitem>
   <listitem>
    <para>
     Allow additional 4GB of RAM per additional daemon deployed on a node
    </para>
   </listitem>
   <listitem>
    <para>
     Dual-10 Gb ethernet as bonded network
    </para>
   </listitem>
   <listitem>
    <para>
     If you are running a hyper-converged infrastructure (HCI), ensure you add
     any additional requirements for your workloads.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="rook-deploy-before-begin">
  <title>Prerequisites</title>

  <para>
   Ensure the following prerequisites are met before continuing with this
   quickstart guide:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Installation of &caasp; 4.5. See the &caasp; documentation for more
     details on how to install:
     <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-deployment/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure <literal>ceph-csi</literal> (and required sidecars) are running in
     your &kube; cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Installation of the LVM2 package on the host where the OSDs are running.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure you have one of the following storage options to configure &ceph;
     properly:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Raw devices (no partitions or formatted file systems)
      </para>
     </listitem>
     <listitem>
      <para>
       Raw partitions (no formatted file system)
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Ensure the &caasp; 4.5 repository is enabled for the installation of Helm
     3.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="getting-started-rook">
  <title>Getting started with &rook;</title>

  <note>
   <para>
    The following instructions are designed for a quick start deployment only.
    For more information on installing Helm, see
    <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-admin/#helm-tiller-install"/>.
   </para>
  </note>

  <procedure>
   <step>
    <para>
     Install Helm v3:
    </para>
<screen>&prompt.root;zypper in helm</screen>
   </step>
   <step>
    <para>
     On a node with access to the &kube; cluster, execute the following:
    </para>
<screen>&prompt.user;export HELM_EXPERIMENTAL_OCI=1</screen>
   </step>
   <step>
    <para>
     Create a local copy of the Helm chart to your local registry:
    </para>
<screen>&prompt.user;helm chart pull registry.suse.com/ses/7.1/charts/rook-ceph:latest</screen>
    <para>
     If you are using a version of Helm >= 3.7, you do not need to specify the subcommand <command>chart</command>.
     The protocol is also explicit and the version is presumed to be <lateral>latest</lateral>.
    </para>
<screen>&prompt.user;helm pull oci://registry.suse.com/ses/7.1/charts/rook-ceph</screen>
   </step>
   <step>
    <para>
     Export the Helm charts to a &rookceph; sub-directory under your current
     working directory:
    </para>
<screen>&prompt.user;helm chart export registry.suse.com/ses/7.1/charts/rook-ceph:latest</screen>
    <para>
     For Helm versions >= 3.7 you just need to extract the tarball yourself:
    </para>
<screen>&prompt.user;tar -xzf rook-ceph-1.8.6.tar.gz</screen>
   </step>
   <step>
    <para>
     Create a file named <filename>myvalues.yaml</filename> based off the
     <filename>rook-ceph/values.yaml file</filename>.
    </para>
   </step>
   <step>
    <para>
     Set local parameters in <filename>myvalues.yaml</filename>.
    </para>
   </step>
   <step>
    <para>
     Create the namespace:
    </para>
<screen>&prompt.kubeuser;kubectl create namespace rook-ceph</screen>
   </step>
   <step>
    <para>
     Install the helm charts:
    </para>
<screen>&prompt.user;helm install -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
   </step>
   <step>
    <para>
     Verify the <literal>rook-operator</literal> is running:
    </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pod -l app=rook-ceph-operator</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-deploy-ceph">
  <title>Deploying &ceph; with &rook;</title>

  <procedure>
   <step>
    <para>
     You need to apply labels to your &kube; nodes before deploying your &ceph;
     cluster. The key <literal>node-role.rook-ceph/cluster</literal> accepts
     one of the following values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>any</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon-mgr</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon-mgr-osd</literal>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Run the following the get the names of your cluster's nodes:
    </para>
<screen>&prompt.kubeuser;kubectl get nodes</screen>
   </step>
   <step>
    <para>
     On the Master node, run the following:
    </para>
<screen>&prompt.kubeuser;kubectl label nodes <replaceable>node-name</replaceable> <replaceable>label-key</replaceable>=<replaceable>label-value</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.kubeuser;kubectl label node <replaceable>k8s-worker-node-1</replaceable> <replaceable>node-role.rook-ceph/cluster</replaceable>=<replaceable>any</replaceable></screen>
   </step>
   <step>
    <para>
     Verify the application of the label by re-running the following command:
    </para>
<screen>&prompt.kubeuser;kubectl get nodes --show-labels</screen>
    <para>
     You can also use the <command>describe</command> command to get the full
     list of labels given to the node. For example:
    </para>
<screen>&prompt.kubeuser;kubectl describe node <replaceable>node-name</replaceable></screen>
   </step>
   <step>
    <para>
     Next, you need to apply a &ceph; cluster manifest file, for example,
     <filename>cluster.yaml</filename>, to your &kube; cluster. You can apply
     the example <filename>cluster.yaml</filename> as is without any additional
     services or requirements from the &rook; Helm chart.
    </para>
    <para>
     To apply the example &ceph; cluster manifest to your &kube; cluster, run
     the following command:
    </para>
<screen>&prompt.user;kubectl create -f rook-ceph/examples/cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-config-ceph">
  <title>Configuring the &ceph; cluster</title>

  <para>
   You can have two types of integration with your &productname; intregrated
   cluster. These types are: &cephfs; or &rados; Block Device (RBD).
  </para>

  <para>
   Before you start the &caasp; and &productname; integration, ensure you have
   met the following prerequisites:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &caasp; cluster must have <literal>ceph-common</literal> and
     <literal>xfsprogs</literal> installed on all nodes. You can check this by
     running the <command>rpm -q ceph-common</command> command or the
     <command>rpm -q xfsprogs</command> command.
    </para>
   </listitem>
   <listitem>
    <para>
     That the &productname; cluster has a pool with a RBD device or &cephfs;
     enabled.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="config-cephfs">
   <title>Configure &cephfs;</title>
   <para>
    For more information on configuring &cephfs;, see
    <link xlink:href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-admin/#_using_cephfs_in_a_pod"/>
    for steps and more information. This section will also provide the
    necessary procedure on attaching a pod to either an &cephfs; static or
    dynamic volume.
   </para>
  </sect2>

  <sect2 xml:id="config-persistent-volumes">
   <title>Configure &rados; block device</title>
   <para>
    For instructions on configuring the &rados; Block Device (RBD) in a pod,
    see
    <link xlink:href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-admin/#_using_rbd_in_a_pod"/>
    for more information. This section will also provide the necessary
    procedure on attaching a pod to either an RBD static or dynamic volume.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="updating-rook-images">
  <title>Updating local images</title>

  <procedure>
   <step>
    <para>
     To update your local image to the latest tag, apply the new parameters in
     <filename>myvalues.yaml</filename>:
    </para>
<screen>
image:
refix: rook
repository: registry.suse.com/ses/7.1/rook/ceph
tag: <replaceable>LATEST_TAG</replaceable>
pullPolicy: IfNotPresent
</screen>
   </step>
   <step>
    <para>
     Re-pull a new local copy of the Helm chart to your local registry:
    </para>
<screen>&prompt.user;helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
   <step>
    <para>
     Export the Helm charts to a &rookceph; sub-directory under your current
     working directory:
    </para>
<screen>&prompt.user;helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
   <step>
    <para>
     Upgrade the Helm charts:
    </para>
<screen>&prompt.user;helm3 upgrade -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="uninstalling-rook">
  <title>Uninstalling</title>

  <procedure>
   <step>
    <para>
     Delete any &kube; applications that are consuming &rook; storage.
    </para>
   </step>
   <step>
    <para>
     Delete all object, file, and block storage artifacts.
    </para>
   </step>
   <step>
    <para>
     Remove the CephCluster:
    </para>
<screen>&prompt.kubeuser;>kubectl delete -f cluster.yaml</screen>
   </step>
   <step>
    <para>
     Uninstall the operator:
    </para>
<screen>&prompt.user;helm uninstall <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
    <para>
     Or if you are using Helm >= 3.7:
    </para>
<screen>&prompt.user;helm uninstall -n rook-ceph rook-ceph</screen>
   </step>
   <step>
    <para>
     Delete any data on the hosts:
    </para>
<screen>&prompt.user;rm -rf /var/lib/rook</screen>
   </step>
   <step>
    <para>
     Wipe the disks if necessary.
    </para>
   </step>
   <step>
    <para>
     Delete the namespace:
    </para>
<screen>&prompt.user;kubectl delete namespace rook-ceph</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
