<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="deploy-rook">
<!-- ============================================================== -->
 <title>Quick Start</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
  <para>
   &productname; is a distributed storage system designed for scalability,
   reliability, and performance, which is based on the &ceph; technology.
   The traditional way to run a &ceph; cluster is setting up a dedicated
   cluster to provide block, file, and object storage to a variety of
   clients.
  </para>
  <para>
    &rook; manages &ceph; as a containerized application on &kube; and
    allows a hyper-converged setup, in which a single &kube; cluster
    runs applications and storage together. &productname; deployed with
    &rook; primarily provides storage to the other applications running
    in the &kube; cluster. This can be block, file, or object storage.
 </para>
 <para>
  This chapter describes how to quickly deploy containerized &productname;
  &productnumber; on top of &casp; 4.5 &kube; cluster.
 </para>
 <sect1 xml:id="rook-deploy-hardware-specs">
   <title>Recommended Hardware Specifications</title>
   <para>
     For &productname; deployed with &rook;, the minimal configuration is
     preliminary, we will update it based on real customer needs.
   </para>
   <para>
     For the purpose of this document, consider the following minimum
     configuration:
   </para>
   <itemizedlist>
     <listitem>
       <para>
          A highly available &kube; cluster with 3 master nodes
       </para>
     </listitem>
     <listitem>
       <para>
         Four physical Kubernetes nodes, each with 2 OSD disks and 5GB
         of RAM per OSD
       </para>
     </listitem>
     <listitem>
       <para>
         Allow additional 4GB of RAM per additional daemon deployed on a node
       </para>
     </listitem>
     <listitem>
       <para>
         Dual-10 Gb ethernet as bonded network
       </para>
     </listitem>
     <listitem>
       <para>
         For HCI, add the requirements of the workload
       </para>
     </listitem>
   </itemizedlist>
 </sect1>
 <sect1 xml:id="rook-deploy-before-begin">
   <title>Prerequisites</title>
   <para>
     Ensure the following prerequisites are met before continuing with this
     quickstart guide:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         Installation of &casp; 4.5. See the &casp; documentation for more
         details on how to install: <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-deployment/"/>.
       </para>
     </listitem>
     <listitem>
       <para>
         Ensure <literal>ceph-csi</literal> (and required sidecars) are running
         in your &kube; cluster.
       </para>
     </listitem>
     <listitem>
       <para>
         Installation of the LVM2 package on the host where the OSDs are running.
       </para>
     </listitem>
     <listitem>
       <para>
         Ensure you have one of the following storage options to configure
         &ceph; properly:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Raw devices (no partitions or formatted file systems)
           </para>
         </listitem>
         <listitem>
           <para>
             Raw partitions (no formatted file system)
           </para>
         </listitem>
       </itemizedlist>
     </listitem>
     <listitem>
       <para>
         Ensure the &casp; 4.5 repository is enabled for the installation of Helm 3.
       </para>
     </listitem>
   </itemizedlist>
 </sect1>
 <sect1 xml:id="getting-started-rook">
   <title>Getting Started with &rook;</title>
   <note>
     <para>
       The following instructions are designed for a quick start deploy only.
       For more information on installing Helm, see
       <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-admin/#helm-tiller-install"/>.
     </para>
   </note>
   <procedure>
     <step>
       <para>
         Install Helm v3:
       </para>
<screen>&prompt.root;zypper in helm3</screen>
     </step>
     <step>
       <para>
         On a node with access to the &kube; cluster, execute the following:
       </para>
<screen>&prompt.user;export HELM_EXPERIMENTAL_OCI=1</screen>
     </step>
     <step>
       <para>
         Create a local copy of the Helm chart to your local registry:
       </para>
<screen>&prompt.user;helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Export the Helm charts to a &rookceph; sub-directory under your current
         working directory:
       </para>
<screen>&prompt.user;helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Create a file named <filename>myvalues.yaml</filename> based off the
         <filename>rook-ceph/values.yaml file</filename>.
       </para>
     </step>
     <step>
       <para>
         Set local parameters in <filename>myvalues.yaml</filename>.
       </para>
     </step>
     <step>
       <para>
         Create the namespace:
       </para>
<screen>&prompt.kubeuser;kubectl create namespace rook-ceph</screen>
     </step>
     <step>
       <para>
         Install the helm charts:
       </para>
<screen>&prompt.user;helm3 install -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
     </step>
     <step>
       <para>
         Verify the <literal>rook-operator</literal> is running:
       </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pod -l app=rook-ceph-operator</screen>
     </step>
   </procedure>
 </sect1>

 <sect1 xml:id="rook-deploy-ceph">
   <title>Deploying &ceph; with &rook;</title>
   <procedure>
     <step>
       <para>
         You need to apply Labels to your &kube; nodes before deploying your
         &ceph; cluster.
       </para>
       <para>
         Run the following the get the names of your cluster's nodes:
       </para>
<screen>&prompt.kubeuser;kubectl get nodes</screen>
     </step>
     <step>
       <para>
         On the node you want to Label, run the following:
       </para>
<screen>&prompt.kubeuser;kubectl label nodes <replaceable>node-name</replaceable> <replaceable>label-key</replaceable>=<replaceable>label-value</replaceable></screen>
       <para>
         For example:
       </para>
<screen>&prompt.kubeuser;kubectl label nodes <replaceable>ses-min-1</replaceable> <replaceable>disktype</replaceable>=<replaceable>ssd</replaceable></screen>
     </step>
     <step>
       <para>
         Verify the application by re-running the following command and checking
         that the node is labelled:
       </para>
<screen>&prompt.kubeuser;kubectl get nodes --show-labels</screen>
       <para>
         You can also use the <command>describe</command> command to get the
         full list of labels given to the node. For example:
       </para>
<screen>&prompt.kubeuser;kubectl describe node <replaceable>node-name</replaceable></screen>
     </step>
     <step>
       <para>
         Next, you need to apply the <filename>cluster.yaml</filename> to your
         &kube; cluster. Currently, you have two options when applying the
         <filename>cluster.yaml</filename>:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             The default <filename>cluster.yaml</filename> can be applied as is
             without any additional services or requirements from a Helm chart.
             To use this specification, edit the <filename>cluster.yaml</filename>
             set the <literal>enabled</literal> configuration to <literal>True</literal>.
           </para>
           <para>
             To apply the default Helm chart to your &kube; cluster, run the following
             command:
           </para>
     <screen>&prompt.user;helm -n rook-ceph install rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
         </listitem>
         <listitem>
           <para>
             You can create your own <filename>cluster.yaml</filename> with any
             additional services or requirements. To use this specification,
             edit the <filename>cluster.yaml</filename>set the <literal>enabled</literal>
             setting to <literal>False</literal>.
           </para>
           <para>
             To apply your customized <filename>cluster.yaml</filename>, run the
             following command:
           </para>
     <screen>&prompt.kubeuser;kubectl apply -f cluster.yaml</screen>
         </listitem>
       </itemizedlist>
     </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-config-ceph">
   <title>Configuring the &ceph; Cluster</title>
   <sect2 xml:id="config-services">
     <title>Configure &ceph; Services</title>
   </sect2>
   <sect2 xml:id="config-cephfs">
     <title>Configure &cephfs;</title>
   </sect2>
   <sect2 xml:id="config-persistent-volumes">
     <title>Configure Persistent Volumes</title>
   </sect2>
 </sect1>
 <sect1 xml:id="updating-rook-images">
   <title>Updating Local Images</title>
   <procedure>
     <step>
       <para>
         To update your local image to the latest tag, apply the new
         parameters in <filename>myvalues.yaml</filename>:
       </para>
<screen>
image:
refix: rook
repository: registry.suse.com/ses/7/rook/ceph
tag: <replaceable>LATEST_TAG</replaceable>
pullPolicy: IfNotPresent
</screen>
     </step>
     <step>
       <para>
         Re-pull a new local copy of the Helm chart to your local registry:
       </para>
<screen>&prompt.user;helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Export the Helm charts to a &rookceph; sub-directory under your
         current working directory:
       </para>
<screen>&prompt.user;helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Upgrade the Helm charts:
       </para>
<screen>&prompt.user;helm3 upgrade -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
     </step>
   </procedure>
 </sect1>
 <sect1 xml:id="uninstalling-rook">
   <title>Uninstalling</title>
   <procedure>
     <step>
       <para>
         Delete any &kube; applications that are consuming &rook; storage.
       </para>
     </step>
     <step>
       <para>
         Delete all object, file, and block storage artifacts.
       </para>
     </step>
     <step>
       <para>
         Remove the CephCluster:
       </para>
<screen>&prompt.kubeuser;>kubectl delete -f cluster.yaml</screen>
     </step>
     <step>
       <para>
         Uninstall the operator:
       </para>
<screen>&prompt.user;helm3 uninstall <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Delete any data on the hosts:
       </para>
<screen>&prompt.user;rm -rf /var/lib/rook</screen>
     </step>
     <step>
       <para>
         Wipe the disks if necessary.
       </para>
     </step>
     <step>
       <para>
         Delete the namespace:
       </para>
<screen>&prompt.user;kubectl delete namespace rook-ceph</screen>
     </step>
   </procedure>
 </sect1>
</chapter>
