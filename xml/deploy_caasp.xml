<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="deploy-rook">
<!-- ============================================================== -->
 <title>Deploying on Top of &casp; Cluster</title>

 <warning>
  <title>Limited Availability</title>
  <para>
   Running containerized &ceph; cluster on &casp; is released with limited
   availability for the &productname; &productnumber; release.
  </para>
  <para>
   Find more details about the &casp; product at
   <link xlink:href="https://documentation.suse.com/suse-caasp/4.0/"/>.
  </para>
 </warning>
 <para>
  This chapter describes how to deploy containerized &productname;
  &productnumber; on top of &casp; 4.5 &kube; cluster only.
 </para>
 <sect1 xml:id="kube-consider">
  <title>Considerations</title>

  <para>
   Before you start deploying, consider the following points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     To run &ceph; in &kube;, &productname; &productnumber; uses an upstream
     project called &rook; (<link xlink:href="https://rook.io/"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     Depending on the configuration, &rook; may consume
     <emphasis>all</emphasis> unused disks on all nodes in a &kube; cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The setup requires privileged containers.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="kube-prereq">
  <title>Prerequisites</title>

<!-- https://rook.io/docs/rook/v1.4/ceph-prerequisites.html
Ignore "Ceph Flexvolume Configuration" section -->

  <para>
   Before you start deploying, you need to have:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A running &casp; 4.5 cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &casp; 4.5 worker nodes with a number of extra disks attached as storage for
     the &ceph; cluster.
    </para>
   </listitem>
  </itemizedlist>
   <note>
     <para>
       For &productname; &productnumber;, extensions need to be enabled on
       the &casp; node. If this is not done, the <filename>rook-k8s-yaml</filename>
       package installation fails in <xref linkend="kube-rook-manifests"/>.
     </para>
   </note>
 </sect1>
 <sect1 xml:id="kube-rook-manifests">
  <title>Get &rook; Manifests</title>

  <para>
   The &rook; orchestrator uses configuration files in YAML format called
   <emphasis>manifests</emphasis>. The manifests you need are included in the
   <package>rook-k8s-yaml</package> RPM package. These are automatically installed
   to <filename>/usr/share/k8s-yaml/rook/</filename>. Install it by running:
  </para>
<screen>&prompt.root;zypper install rook-k8s-yaml</screen>
 </sect1>
 <sect1 xml:id="kube-install">
  <title>Installation</title>

  <!-- https://rook.io/docs/rook/v1.4/ceph-quickstart.html

  Min version should probably be "CaaSP v4.5" for SUSE/SES
  Ignore k8s-pre-reqs.html in "follow these instructions" and link to below instead.
  Ignore "Monitoring" section for now
   -->

  <para>
   &rookceph; includes two main components: the 'operator' which is run by
   &kube; and allows creation of &ceph; clusters, and the &ceph; 'cluster'
   itself which is created and partially managed by the operator.
  </para>

  <sect2 xml:id="kube-configure">
   <title>Configuration</title>
   <sect3 xml:id="kube-configure-global">
    <title>Global Configuration</title>
    <para>
     The manifests used in this setup install all &rook; and &ceph; components
     in the <literal>rook-ceph</literal> namespace. If you need to change it, adapt all
     references to the namespace in the &kube; manifests accordingly.
    </para>
    <para>
     Depending on which features of &rook; you intend to use, alter the 'Pod
     Security Policy' configuration in <filename>common.yaml</filename> to
     limit &rook;'s security requirements. Follow the comments in the manifest
     file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-operator">
    <title>Operator Configuration</title>
    <para>
     The manifest <filename>operator.yaml</filename> configures the &rook;
     operator. Normally, you do not need to change it. Find more information
     following the comments in the manifest file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-ceph">
    <title>&ceph; Cluster Configuration</title>
    <para>
     The manifest <filename>cluster.yaml</filename> is responsible for
     configuring the actual &ceph; cluster which will run in &kube;. Find
     detailed description of all available options in the upstream &rook;
     documentation at
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html"/>.
    </para>
    <para>
     By default, &rook; is configured to use all nodes that are not tainted
     with <option>node-role.kubernetes.io/master:NoSchedule</option> and will
     obey configured placement settings (see
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html#placement-configuration-settings"/>).
     The following example disables such behavior and only uses the nodes
     explicitly listed in the nodes section:
    </para>
<screen>
storage:
  useAllNodes: false
  nodes:
    - name: caasp4-worker-0
    - name: caasp4-worker-1
    - name: caasp4-worker-2
</screen>
    <note>
     <para>
      By default, &rook; is configured to use all free and empty disks on each
      node for use as &ceph; storage.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="kube-config-rook-operator">
   <title>Create the &rook; Operator</title>
   <para>
     TODO: update with helm deploy info soon
   </para>
  </sect2>

  <sect2 xml:id="kube-create-ceph-cluster">
   <title>Create the &ceph; Cluster</title>
   <para>
    After you modify <filename>cluster.yaml</filename> according to your needs,
    you can create the &ceph; cluster. Run the following command on the &casp;
    master node:
   </para>
<screen>&prompt.root;kubectl apply -f cluster.yaml</screen>
   <para>
    Watch the <literal>rook-ceph</literal> namespace to see the &ceph; cluster being created.
    You will see as many &mon;s as configured in the
    <filename>cluster.yaml</filename> manifest (default is 3), one &mgr;, and
    as many &osd;s as you have free disks.
   </para>
   <tip>
    <title>Temporary OSD Pods</title>
    <para>
     While bootstrapping the &ceph; cluster, you will see some pods with the
     name
     <literal>rook-ceph-osd-prepare-<replaceable>NODE-NAME</replaceable></literal>
     run for a while and then terminate with the status 'Completed'. As their
     name implies, these pods provision &osd;s. They are left without being
     deleted so that you can inspect their logs after their termination. For
     example:
    </para>
<screen>&prompt.root;kubectl get pods --namespace rook-ceph
NAME                                         READY  STATUS     RESTARTS  AGE
rook-ceph-agent-57c9j                        1/1    Running    0         22h
rook-ceph-agent-b9j4x                        1/1    Running    0         22h
rook-ceph-mgr-a-6d48564b84-k7dft             1/1    Running    0         22h
rook-ceph-mon-a-cc44b479-5qvdb               1/1    Running    0         22h
rook-ceph-mon-b-6c6565ff48-gm9wz             1/1    Running    0         22h
rook-ceph-operator-cf6fb96-lhbj7             1/1    Running    0         22h
rook-ceph-osd-0-57bf997cbd-4wspg             1/1    Running    0         22h
rook-ceph-osd-1-54cf468bf8-z8jhp             1/1    Running    0         22h
rook-ceph-osd-prepare-caasp4-worker-0-f2tmw  0/2    Completed  0         9m35s
rook-ceph-osd-prepare-caasp4-worker-1-qsfhz  0/2    Completed  0         9m33s
rook-ceph-tools-76c7d559b6-64rkw             1/1    Running    0         22h
rook-discover-mb8gv                          1/1    Running    0         22h
rook-discover-tztz4                          1/1    Running    0         22h</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="kube-using-rook">
  <title>Using &rook; as Storage for Kubernetes Workload</title>

  <para>
   &rook; allows you to use three different types of storage:
  </para>

  <variablelist>
   <varlistentry>
    <term><emphasis role="bold">Object Storage</emphasis></term>
    <listitem>
     <para>
      Object storage exposes an S3 API to the storage cluster for applications
      to put and get data. Refer to <xref linkend="admin-caasp-object-storage"/> for
      a detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Shared File System</term>
    <listitem>
     <para>
      A shared file system can be mounted with read/write permission from
      multiple pods. This is useful for applications that are clustered using a
      shared file system. Refer to <xref linkend="admin-caasp-cephfs"/>
      for a detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Block Storage</term>
    <listitem>
     <para>
      Block storage allows you to mount storage to a single pod. Refer to
      <xref linkend="admin-caasp-block-storage"/> for a
      detailed description.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="kube-uninstall">
  <title>Uninstalling &rook;</title>

  <para>
   To uninstall &rook;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Delete any &kube; applications that are consuming &rook; storage.
    </para>
   </step>
   <step>
    <para>
     Delete all object, file, and/or block storage artifacts that you created
     by following <xref linkend="kube-using-rook"/>.
    </para>
   </step>
   <step>
    <para>
     Delete the &ceph; cluster, operator, and related resources:
    </para>
<screen>
&prompt.root;kubectl delete -f cluster.yaml
# or whatever your CephCluster file is
# now uninstall operator via helm uninstall steps
</screen>
   </step>
   <step>
    <para>
     Delete the data on hosts:
    </para>
<screen>&prompt.root;rm -rf /var/lib/rook</screen>
   </step>
   <step>
    <para>
     If necessary, wipe the disks that were used by &rook;. Refer to
     <link xlink:href="https://rook.io/docs/rook/master/ceph-teardown.html"/>
     for more details.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
