<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="deploy-rook">
<!-- ============================================================== -->
 <title>Deploying on Top of &casp; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <warning>
  <title>Limited Availability</title>
  <para>
   Running containerized &ceph; cluster on &casp; is released with limited
   availability for the &productname; &productnumber; release.
  </para>
  <para>
   Find more details about the &casp; product at
   <link xlink:href="https://documentation.suse.com/suse-caasp/4.0/"/>.
  </para>
 </warning>
 <para>
  This chapter describes how to deploy containerized &productname;
  &productnumber; on top of &casp; 4.5 &kube; cluster only.
 </para>
 <sect1 xml:id="kube-consider">
  <title>Considerations</title>

  <para>
   Before you start deploying, consider the following points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     To run &ceph; in &kube;, &productname; &productnumber; uses an upstream
     project called &rook; (<link xlink:href="https://rook.io/"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     Depending on the configuration, &rook; may consume
     <emphasis>all</emphasis> unused disks on all nodes in a &kube; cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The setup requires privileged containers.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="kube-prereq">
  <title>Prerequisites</title>

  <para>
   Before you start deploying, you need to have:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A running &casp; 4.5 cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     &casp; 4.5 worker nodes with a number of extra disks attached as storage
     for the &ceph; cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <package>lvm2</package> package installed on the host where the OSDs are
     running.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    For &productname; &productnumber;, extensions need to be enabled on the
    &casp; node. If this is not done, the <filename>rook-k8s-yaml</filename>
    package installation fails in <xref linkend="kube-rook-manifests"/>.
   </para>
  </note>

  <para>
   In order to configure the &ceph; storage cluster, at least one of these
   local storage options are required:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Raw devices (no partitions or formatted file systems)
    </para>
   </listitem>
   <listitem>
    <para>
     Raw partitions (no formatted file system)
    </para>
   </listitem>
   <listitem>
    <para>
     PVs available from a storage class in <literal>block</literal> mode
    </para>
   </listitem>
  </itemizedlist>

  <para>
   &ceph; requires a Linux kernel built with the RBD module. Many Linux
   distributions have this module, but not all distributions. For example, the
   GKE Container-Optimised OS (COS) does not have RBD.
  </para>

  <note>
   <para>
    You can test your Kubernetes nodes by running the <command>modprobe
    rbd</command> command.. If it says ‘not found’, you may have to rebuild
    your kernel or choose a different Linux distribution.
   </para>
  </note>

  <para>
   If you are creating volumes from &cephfs;, the recommended minimum kernel
   version is 4.17. If you have a kernel version less than 4.17, the requested
   PVC sizes will not be enforced. Storage quotas will only be enforced on
   newer kernels.
  </para>
 </sect1>
 <sect1 xml:id="kube-rook-manifests">
  <title>Get &rook; Manifests</title>

  <para>
   The &rook; orchestrator uses configuration files in YAML format called
   <emphasis>manifests</emphasis>. The manifests you need are included in the
   <package>rook-k8s-yaml</package> RPM package. These are automatically
   installed to <filename>/usr/share/k8s-yaml/rook/</filename>. Install it by
   running:
  </para>

<screen>&prompt.root;zypper install rook-k8s-yaml</screen>
 </sect1>
 <sect1 xml:id="kube-install">
  <title>Installation</title>

  <para>
   &rookceph; includes two main components: the 'operator' which is run by
   &kube; and allows creation of &ceph; clusters, and the &ceph; 'cluster'
   itself which is created and partially managed by the operator.
  </para>

  <sect2 xml:id="kube-configure">
   <title>Configuration</title>
   <sect3 xml:id="kube-configure-global">
    <title>Global Configuration</title>
    <para>
     The manifests used in this setup install all &rook; and &ceph; components
     in the <literal>rook-ceph</literal> namespace. If you need to change it,
     adapt all references to the namespace in the &kube; manifests accordingly.
    </para>
    <para>
     Depending on which features of &rook; you intend to use, alter the 'Pod
     Security Policy' configuration in <filename>common.yaml</filename> to
     limit &rook;'s security requirements. Follow the comments in the manifest
     file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-operator">
    <title>Operator Configuration</title>
    <para>
     The manifest <filename>operator.yaml</filename> configures the &rook;
     operator. Normally, you do not need to change it. Find more information
     following the comments in the manifest file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-ceph">
    <title>&ceph; Cluster Configuration</title>
    <para>
     The manifest <filename>cluster.yaml</filename> is responsible for
     configuring the actual &ceph; cluster which will run in &kube;. Find
     detailed description of all available options in the upstream &rook;
     documentation at
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html"/>.
    </para>
    <para>
     By default, &rook; is configured to use all nodes that are not tainted
     with <option>node-role.kubernetes.io/master:NoSchedule</option> and will
     obey configured placement settings (see
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html#placement-configuration-settings"/>).
     The following example disables such behavior and only uses the nodes
     explicitly listed in the nodes section:
    </para>
<screen>
storage:
  useAllNodes: false
  nodes:
    - name: caasp4-worker-0
    - name: caasp4-worker-1
    - name: caasp4-worker-2
</screen>
    <note>
     <para>
      By default, &rook; is configured to use all free and empty disks on each
      node for use as &ceph; storage.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2>
   <title>Assign Labels to Nodes</title>
   <para>
    You need to assign labels to cluster nodes for their specific role within
    the &ceph; cluster. You can either assign labels to all nodes at once, or
    to a specific node only. The following labels are available:
    <literal>mon</literal>, <literal>mgr</literal>, <literal>osd</literal>,
    <literal>mon-mgr-osd</literal>, or the generic <literal>any</literal>.
   </para>
   <para>
    To label all nodes at once, run:
   </para>
<screen>&prompt.kubeuser;kubectl label nodes --all \
 node-role.rook-ceph/cluster=<replaceable>LABEL</replaceable></screen>
   <para>
    To label a specific node, run:
   </para>
<screen>&prompt.kubeuser;kubectl label node <replaceable>NODE_NAME</replaceable> \
 node-role.rook-ceph/cluster=<replaceable>LABEL</replaceable></screen>
   <para>
    For more details, see <xref linkend="rook-annotations-and-labels"/>.
   </para>
  </sect2>

  <sect2 xml:id="kube-config-rook-operator">
   <title>Create the &rook; Operator</title>
   <para>
    TODO: update with helm deploy info soon
   </para>
  </sect2>

  <sect2 xml:id="kube-create-ceph-cluster">
   <title>Create the &ceph; Cluster</title>
   <para>
    After you modify <filename>cluster.yaml</filename> according to your needs,
    you can create the &ceph; cluster. Run the following command on the &casp;
    master node:
   </para>
<screen>&prompt.kubeuser;kubectl apply -f cluster.yaml</screen>
   <para>
    Watch the <literal>rook-ceph</literal> namespace to see the &ceph; cluster
    being created. You will see as many &mon;s as configured in the
    <filename>cluster.yaml</filename> manifest (default is 3), one &mgr;, and
    as many &osd;s as you have free disks.
   </para>
   <tip>
    <title>Temporary OSD Pods</title>
    <para>
     While bootstrapping the &ceph; cluster, you will see some pods with the
     name
     <literal>rook-ceph-osd-prepare-<replaceable>NODE-NAME</replaceable></literal>
     run for a while and then terminate with the status 'Completed'. As their
     name implies, these pods provision &osd;s. They are left without being
     deleted so that you can inspect their logs after their termination. For
     example:
    </para>
<screen>
&prompt.kubeuser;kubectl get pods --namespace rook-ceph
NAME                                         READY  STATUS     RESTARTS  AGE
rook-ceph-mgr-a-6d48564b84-k7dft             1/1    Running    0         22h
rook-ceph-mon-a-cc44b479-5qvdb               1/1    Running    0         22h
rook-ceph-mon-b-6c6565ff48-gm9wz             1/1    Running    0         22h
rook-ceph-operator-cf6fb96-lhbj7             1/1    Running    0         22h
rook-ceph-osd-0-57bf997cbd-4wspg             1/1    Running    0         22h
rook-ceph-osd-1-54cf468bf8-z8jhp             1/1    Running    0         22h
rook-ceph-osd-prepare-caasp4-worker-0-f2tmw  0/2    Completed  0         9m35s
rook-ceph-osd-prepare-caasp4-worker-1-qsfhz  0/2    Completed  0         9m33s
rook-ceph-tools-76c7d559b6-64rkw             1/1    Running    0         22h
rook-discover-mb8gv                          1/1    Running    0         22h
rook-discover-tztz4                          1/1    Running    0         22h
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="kube-using-rook">
  <title>Using &rook; as Storage for Kubernetes Workload</title>

  <para>
   &rook; allows you to use three different types of storage:
  </para>

  <variablelist>
   <varlistentry>
    <term><emphasis role="bold">Object Storage</emphasis></term>
    <listitem>
     <para>
      Object storage exposes an S3 API to the storage cluster for applications
      to put and get data. Refer to
      <xref linkend="admin-caasp-object-storage"/> for a detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Shared File System</term>
    <listitem>
     <para>
      A shared file system can be mounted with read/write permission from
      multiple pods. This is useful for applications that are clustered using a
      shared file system. Refer to <xref linkend="admin-caasp-cephfs"/> for a
      detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Block Storage</term>
    <listitem>
     <para>
      Block storage allows you to mount storage to a single pod. Refer to
      <xref linkend="admin-caasp-block-storage"/> for a detailed description.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="kube-uninstall">
  <title>Uninstalling &rook;</title>

  <para>
   To uninstall &rook;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Delete any &kube; applications that are consuming &rook; storage.
    </para>
   </step>
   <step>
    <para>
     Delete all object, file, and/or block storage artifacts that you created
     by following <xref linkend="kube-using-rook"/>.
    </para>
   </step>
   <step>
    <para>
     Delete the &ceph; cluster, operator, and related resources:
    </para>
<screen>
&prompt.root;kubectl delete -f cluster.yaml
# or whatever your CephCluster file is
# now uninstall operator via helm uninstall steps
</screen>
   </step>
   <step>
    <para>
     Delete the data on hosts:
    </para>
<screen>&prompt.root;rm -rf /var/lib/rook</screen>
   </step>
   <step>
    <para>
     If necessary, wipe the disks that were used by &rook;. Refer to
     <link xlink:href="https://rook.io/docs/rook/master/ceph-teardown.html"/>
     for more details.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
