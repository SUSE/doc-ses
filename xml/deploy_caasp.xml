<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="deploy-rook">
<!-- ============================================================== -->
 <title>Quick Start</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
  <para>
   &productname; is a distributed storage system designed for scalability,
   reliability, and performance, which is based on the &ceph; technology.
   The traditional way to run a &ceph; cluster is setting up a dedicated
   cluster to provide block, file, and object storage to a variety of
   clients.
  </para>
  <para>
    &rook; manages &ceph; as a containerized application on &kube; and
    allows a hyper-converged setup, in which a single &kube; cluster
    runs applications and storage together. The primary purpose of &productname;
    deployed with &rook; is to provide storage to other applications running
    in the &kube; cluster. This can be block, file, or object storage.
 </para>
 <para>
  This chapter describes how to quickly deploy containerized &productname;
  &productnumber; on top of a &caasp; 4.5 &kube; cluster.
 </para>
 <sect1 xml:id="rook-deploy-hardware-specs">
   <title>Recommended Hardware Specifications</title>
   <para>
     For &productname; deployed with &rook;, the minimal configuration is
     preliminary, we will update it based on real customer needs.
     <remark>
       LGHP 2020-10-07: Should we at least link to the CaaSP system requirements? 
     </remark>
   </para>
   <para>
     For the purpose of this document, consider the following minimum
     configuration:
   </para>
   <itemizedlist>
     <listitem>
       <para>
          A highly available &kube; cluster with 3 master nodes
       </para>
     </listitem>
     <listitem>
       <para>
         Four physical &kube; worker nodes, each with two OSD disks and 5GB
         of RAM per OSD disk
       </para>
     </listitem>
     <listitem>
       <para>
         Allow additional 4GB of RAM per additional daemon deployed on a node
       </para>
     </listitem>
     <listitem>
       <para>
         Dual-10 Gb ethernet as bonded network
       </para>
     </listitem>
     <listitem>
       <para>
         If you are running a hyper-converged infrastructure (HCI), ensure
         you add any additional requirements for your workloads.
       </para>
     </listitem>
   </itemizedlist>
 </sect1>
 <sect1 xml:id="rook-deploy-before-begin">
   <title>Prerequisites</title>
   <para>
     Ensure the following prerequisites are met before continuing with this
     quickstart guide:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         Installation of &caasp; 4.5. See the &caasp; documentation for more
         details on how to install: <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-deployment/"/>.
       </para>
     </listitem>
     <listitem>
       <para>
         Ensure <literal>ceph-csi</literal> (and required sidecars) are running
         in your &kube; cluster.
       </para>
     </listitem>
     <listitem>
       <para>
         Installation of the LVM2 package on the host where the OSDs are running.
       </para>
     </listitem>
     <listitem>
       <para>
         Ensure you have one of the following storage options to configure
         &ceph; properly:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Raw devices (no partitions or formatted file systems)
           </para>
         </listitem>
         <listitem>
           <para>
             Raw partitions (no formatted file system)
           </para>
         </listitem>
       </itemizedlist>
     </listitem>
     <listitem>
       <para>
         Ensure the &caasp; 4.5 repository is enabled for the installation of Helm 3.
       </para>
     </listitem>
   </itemizedlist>
 </sect1>
 <sect1 xml:id="getting-started-rook">
   <title>Getting Started with &rook;</title>
   <note>
     <para>
       The following instructions are designed for a quick start deployment only.
       For more information on installing Helm, see
       <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/single-html/caasp-admin/#helm-tiller-install"/>.
     </para>
   </note>
   <procedure>
     <step>
       <para>
         Install Helm v3:
       </para>
<screen>&prompt.root;zypper in helm3</screen>
     </step>
     <step>
       <para>
         On a node with access to the &kube; cluster, execute the following:
       </para>
<screen>&prompt.user;export HELM_EXPERIMENTAL_OCI=1</screen>
     </step>
     <step>
       <para>
         Create a local copy of the Helm chart to your local registry:
       </para>
<screen>&prompt.user;helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Export the Helm charts to a &rookceph; sub-directory under your current
         working directory:
       </para>
<screen>&prompt.user;helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Create a file named <filename>myvalues.yaml</filename> based off the
         <filename>rook-ceph/values.yaml file</filename>.
       </para>
     </step>
     <step>
       <para>
         Set local parameters in <filename>myvalues.yaml</filename>.
       </para>
     </step>
     <step>
       <para>
         Create the namespace:
       </para>
<screen>&prompt.kubeuser;kubectl create namespace rook-ceph</screen>
     </step>
     <step>
       <para>
         Install the helm charts:
       </para>
<screen>&prompt.user;helm3 install -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
     </step>
     <step>
       <para>
         Verify the <literal>rook-operator</literal> is running:
       </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pod -l app=rook-ceph-operator</screen>
     </step>
   </procedure>
 </sect1>

 <sect1 xml:id="rook-deploy-ceph">
   <title>Deploying &ceph; with &rook;</title>
   <procedure>
     <step>
       <para>
         You need to apply Labels to your &kube; nodes before deploying your
         &ceph; cluster. The key <literal>node-rol.rook-ceph/cluster</literal>
         accepts one of the following values:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             <literal>any</literal>
           </para>
         </listitem>
         <listitem>
           <para>
             <literal>mon</literal>
           </para>
         </listitem>
         <listitem>
           <para>
             <literal>mon-mgr</literal>
           </para>
         </listitem>
         <listitem>
           <para>
             <literal>mon-mgr-osd</literal>
           </para>
         </listitem>
       </itemizedlist>
       <para>
         Run the following the get the names of your cluster's nodes:
       </para>
<screen>&prompt.kubeuser;kubectl get nodes</screen>
     </step>
     <step>
       <para>
         On the node you want to Label, run the following:
       </para>
<screen>&prompt.kubeuser;kubectl label nodes <replaceable>node-name</replaceable> <replaceable>label-key</replaceable>=<replaceable>label-value</replaceable></screen>
       <para>
         For example:
       </para>
<screen>&prompt.kubeuser;kubectl label node <replaceable>k8s-worker-node-1</replaceable> <replaceable>node-role.rook-ceph/cluster</replaceable>=<replaceable>any</replaceable></screen>
     </step>
     <step>
       <para>
         Verify the application of the Label by re-running the following
         command:
       </para>
<screen>&prompt.kubeuser;kubectl get nodes --show-labels</screen>
       <para>
         You can also use the <command>describe</command> command to get the
         full list of labels given to the node. For example:
       </para>
<screen>&prompt.kubeuser;kubectl describe node <replaceable>node-name</replaceable></screen>
     </step>
     <step>
       <para>
         Next, you need to apply the <filename>cluster.yaml</filename> to your
         &kube; cluster. The default <filename>cluster.yaml</filename> can be
         applied as is without any additional services or requirements from a
         Helm chart.
       </para>
       <para>
         To apply the default Helm chart to your &kube; cluster, run the
         following command:
       </para>
<screen>&prompt.user;helm -n rook-ceph install rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
     </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-config-ceph">
   <title>Configuring the &ceph; Cluster</title>
   <para>
     You can have two types of integration with your &productname; intregrated
     cluster. These types are: &cephfs; or &rados; Block Device (RBD).
   </para>
   <para>
     Before you start the &caasp; and &productname; integration, ensure you
     have met the following prerequisites:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         The &caasp; cluster must have <literal>ceph-common</literal> and
         <literal>xfsprogs</literal> installed on all nodes. You can check
         this by running the <command>rpm -q ceph-common</command> command or
         the <command>rpm -q xfsprogs</command> command.
       </para>
     </listitem>
     <listitem>
       <para>
         That the &productname; cluster has a pool with a RBD device or
         &cephfs; enabled.
       </para>
     </listitem>
   </itemizedlist>
   <sect2 xml:id="config-cephfs">
     <title>Configure &cephfs;</title>
     <para>
       For more information on configuring &cephfs;, see
       <link xlink:href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-admin/#_using_cephfs_in_a_pod"/>
       for steps and more information. This section will also provide the necessary
       procedure on attaching a pod to either an &cephfs; static or dynamic volume.
     </para>
   </sect2>
   <sect2 xml:id="config-persistent-volumes">
     <title>Configure &rados; Block Device</title>
     <para>
       For instructions on configuring the &rados; Block Device (RBD)
       in a pod, see <link xlink:href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-admin/#_using_rbd_in_a_pod"/>
       for more information. This section will also provide the necessary
       procedure on attaching a pod to either an RBD static or dynamic volume.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="updating-rook-images">
   <title>Updating Local Images</title>
   <procedure>
     <step>
       <para>
         To update your local image to the latest tag, apply the new
         parameters in <filename>myvalues.yaml</filename>:
       </para>
<screen>
image:
refix: rook
repository: registry.suse.com/ses/7/rook/ceph
tag: <replaceable>LATEST_TAG</replaceable>
pullPolicy: IfNotPresent
</screen>
     </step>
     <step>
       <para>
         Re-pull a new local copy of the Helm chart to your local registry:
       </para>
<screen>&prompt.user;helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Export the Helm charts to a &rookceph; sub-directory under your
         current working directory:
       </para>
<screen>&prompt.user;helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Upgrade the Helm charts:
       </para>
<screen>&prompt.user;helm3 upgrade -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
     </step>
   </procedure>
 </sect1>
 <sect1 xml:id="uninstalling-rook">
   <title>Uninstalling</title>
   <procedure>
     <step>
       <para>
         Delete any &kube; applications that are consuming &rook; storage.
       </para>
     </step>
     <step>
       <para>
         Delete all object, file, and block storage artifacts.
       </para>
     </step>
     <step>
       <para>
         Remove the CephCluster:
       </para>
<screen>&prompt.kubeuser;>kubectl delete -f cluster.yaml</screen>
     </step>
     <step>
       <para>
         Uninstall the operator:
       </para>
<screen>&prompt.user;helm3 uninstall <replaceable>REGISTRY_URL</replaceable></screen>
     </step>
     <step>
       <para>
         Delete any data on the hosts:
       </para>
<screen>&prompt.user;rm -rf /var/lib/rook</screen>
     </step>
     <step>
       <para>
         Wipe the disks if necessary.
       </para>
     </step>
     <step>
       <para>
         Delete the namespace:
       </para>
<screen>&prompt.user;kubectl delete namespace rook-ceph</screen>
     </step>
   </procedure>
 </sect1>
</chapter>
