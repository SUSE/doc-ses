<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-bootstrap">
  <info>
    <title>Deploying the bootstrap cluster using &cephsalt;</title>
    <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
      <dm:translation>yes</dm:translation>
      <dm:release>SES 7</dm:release>
    </dm:docmanager>
  </info>
  <para>
    This section guides you through the process of deploying a basic &ceph;
    cluster. Read the following subsections carefully and execute the included
    commands in the given order.
  </para>
  <sect1 xml:id="deploy-cephadm-cephsalt">
    <title>Installing &cephsalt;</title>

    <para>
      &cephsalt; provides tools for deploying &ceph; clusters managed by
      &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
      management&mdash;for example, software updates or time
      synchronization&mdash;and defining roles for &sminion;s.
    </para>

    <para>
      On the &smaster;, install the <package>ceph-salt</package> package:
    </para>

<screen>&prompt.smaster;zypper install ceph-salt</screen>

    <para>
      The above command installed <package>ceph-salt-formula</package> as a
      dependency which modified the &smaster; configuration by inserting
      additional files in the <filename>/etc/salt/master.d</filename>
      directory. To apply the changes, restart
      <systemitem class="daemon">salt-master.service</systemitem> and
      synchronize &salt; modules:
    </para>

<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect1>
  <sect1 xml:id="deploy-cephadm-configure">
    <title>Configuring cluster properties</title>

    <para>
      Use the <command>ceph-salt config</command> command to configure the
      basic properties of the cluster.
    </para>

    <important>
      <para>
        The <filename>/etc/ceph/ceph.conf</filename> file is managed by
        &cephadm; and users <emphasis>should not</emphasis> edit it. &ceph;
        configuration parameters should be set using the new <command>ceph
        config</command> command. See
        <xref linkend="cha-ceph-configuration-db"/> for more information.
      </para>
    </important>

    <sect2 xml:id="deploy-cephadm-configure-shell">
      <title>Using the &cephsalt; shell</title>
      <para>
        If you run <command>ceph-salt config</command> without any path or
        subcommand, you will enter an interactive &cephsalt; shell. The shell
        is convenient if you need to configure multiple properties in one batch
        and do not want type the full command syntax.
      </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
      <para>
        As you can see from the output of &cephsalt;'s <command>ls</command>
        command, the cluster configuration is organized in a tree structure. To
        configure a specific property of the cluster in the &cephsalt; shell,
        you have two options:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            Run the command from the current position and enter the absolute
            path to the property as the first argument:
          </para>
<screen>
<prompt>/></prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/> /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
        </listitem>
        <listitem>
          <para>
            Change to the path whose property you need to configure and run the
            command:
          </para>
<screen>
<prompt>/></prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions></prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
        </listitem>
      </itemizedlist>
      <tip>
        <title>Autocompletion of configuration snippets</title>
        <para>
          While in a &cephsalt; shell, you can use the autocompletion feature
          similar to a normal Linux shell (Bash) autocompletion. It completes
          configuration paths, subcommands, or &sminion; names. When
          autocompleting a configuration path, you have two options:
        </para>
        <itemizedlist>
          <listitem>
            <para>
              To let the shell finish a path relative to your current
              position,press the TAB key <keycap function="tab"></keycap>
              twice.
            </para>
          </listitem>
          <listitem>
            <para>
              To let the shell finish an absolute path, enter
              <keycap>/</keycap> and press the TAB key
              <keycap function="tab"></keycap> twice.
            </para>
          </listitem>
        </itemizedlist>
      </tip>
      <tip>
        <title>Navigating with the cursor keys</title>
        <para>
          If you enter <command>cd</command> from the &cephsalt; shell without
          any path, the command will print a tree structure of the cluster
          configuration with the line of the current path active. You can use
          the up and down cursor keys to navigate through individual lines.
          After you confirm with <keycap function="enter"></keycap>, the
          configuration path will change to the last active one.
        </para>
      </tip>
      <important>
        <title>Convention</title>
        <para>
          To keep the documentation consistent, we will use a single command
          syntax without entering the &cephsalt; shell. For example, you can
          list the cluster configuration tree by using the following command:
        </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
      </important>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-minions">
      <title>Adding &sminion;s</title>
      <para>
        Include all or a subset of &sminion;s that we deployed and accepted in
        <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You
        can either specify the &sminion;s by their full names, or use a glob
        expressions '*' and '?' to include multiple &sminion;s at once. Use the
        <command>add</command> subcommand under the
        <literal>/ceph_cluster/minions</literal> path. The following command
        includes all accepted &sminion;s:
      </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
      <para>
        Verify that the specified &sminion;s were added:
      </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ............................................... [Minions: 5]
  o- ses-main.example.com .................................. [no roles]
  o- ses-node1.example.com ................................. [no roles]
  o- ses-node2.example.com ................................. [no roles]
  o- ses-node3.example.com ................................. [no roles]
  o- ses-node4.example.com ................................. [no roles]
</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-cephadm">
      <title>Specifying &sminion;s managed by &cephadm;</title>
      <para>
        Specify which nodes will belong to the &ceph; cluster and will be
        managed by &cephadm;. Include all nodes that will run &ceph; services
        as well as the &adm;:
      </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-admin">
      <title>Specifying &adm;</title>
      <para>
        The &adm; is the node where the <filename>ceph.conf</filename>
        configuration file and the &ceph; admin keyring is installed. You
        usually run &ceph; related commands on the &adm;.
      </para>
      <tip>
        <title>&smaster; and &adm; on the Same Node</title>
        <para>
          In a homogeneous environment where all or most hosts belong to
          &productname;, we recommend having the &adm; on the same host as the
          &smaster;.
        </para>
        <para>
          In a heterogeneous environment where one &salt; infrastructure hosts
          more than one cluster, for example, &productname; together with
          &susemgr;, do <emphasis>not</emphasis> place the &adm; on the same
          host as &smaster;.
        </para>
      </tip>
      <para>
        To specify the &adm;, run the following command:
      </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-main.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-main.example.com ...................... [Other roles: cephadm]
</screen>
      <tip>
        <title>Install <filename>ceph.conf</filename> and the admin keyring on multiple nodes</title>
        <para>
          You can install the &ceph; configuration file and admin keyring on
          multiple nodes if your deployment requires it. For security reasons,
          avoid installing them on all the cluster's nodes.
        </para>
      </tip>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-mon">
      <title>Specifying first MON/MGR node</title>
      <para>
        You need to specify which of the cluster's &sminion;s will bootstrap
        the cluster. This minion will become the first one running &mon; and
        &mgr; services.
      </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-node1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-node1.example.com]
</screen>
      <para>
        Additionally, you need to specify the bootstrap MON's IP address on the
        public network to ensure that the <option>public_network</option>
        parameter is set correctly, for example:
      </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-tuned-profiles">
      <title>Specifying tuned profiles</title>
      <para>
        You need to specify which of the cluster's minions have actively tuned
        profiles. To do so, add these roles explicitly with the following
        commands:
      </para>
      <note>
        <para>
          One minion cannot have both the <literal>latency</literal> and
          <literal>throughput</literal> roles.
        </para>
      </note>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/latency add ses-node1.example.com
Adding ses-node1.example.com...
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-node2.example.com
Adding ses-node2.example.com...
1 minion added.
</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-ssh">
      <title>Generating an SSH key pair</title>
      <para>
        &cephadm; uses the SSH protocol to communicate with cluster nodes. A
        user account named <literal>cephadm</literal> is automatically created
        and used for SSH communication.
      </para>
      <para>
        You need to generate the private and public part of the SSH key pair:
      </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-ntp">
      <title>Configuring the time server</title>
      <para>
        All cluster nodes need to have their time synchronized with a reliable
        time source. There are several scenarios to approach time
        synchronization:
      </para>
      <itemizedlist>
        <listitem>
          <para>
            If all cluster nodes are already configured to synchronize their
            time using an NTP service of choice, disable time server handling
            completely:
          </para>
<screen>
&prompt.smaster;ceph-salt config /time_server disable
</screen>
        </listitem>
        <listitem>
          <para>
            If your site already has a single source of time, specify the host
            name of the time source:
          </para>
<screen>
 &prompt.smaster;ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
        </listitem>
        <listitem>
          <para>
            Alternatively, &cephsalt; has the ability to configure one of the
            &sminion; to serve as the time server for the rest of the cluster.
            This is sometimes referred to as an "internal time server". In this
            scenario, &cephsalt; will configure the internal time server (which
            should be one of the &sminion;) to synchronize its time with an
            external time server, such as <literal>pool.ntp.org</literal>, and
            configure all the other minions to get their time from the internal
            time server. This can be achieved as follows:
          </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/servers add ses-main.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
          <para>
            The <option>/time_server/subnet</option> option specifies the
            subnet from which NTP clients are allowed to access the NTP server.
            It is automatically set when you specify
            <option>/time_server/servers</option>. If you need to change it or
            specify it manually, run:
          </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
        </listitem>
      </itemizedlist>
      <para>
        Check the time server settings:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-main.example.com ......  ............................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
      <para>
        Find more information on setting up time synchronization in
        <link
      xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
      </para>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-dashboardlogin">
      <title>Configuring the &dashboard; login credentials</title>
      <para>
        &dashboard; will be available after the basic cluster is deployed. To
        access it, you need to set a valid user name and password, for example:
      </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/username set admin
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
      <tip>
        <title>Forcing password update</title>
        <para>
          By default, the first dashboard user will be forced to change their
          password on first login to the dashboard. To disable this feature,
          run the following command:
        </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
      </tip>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-registry">
      <title>Using the container registry</title>
      <note>
        <para>
          These instructions assume the private registry will reside on the SES
          &adm;. In cases where the private registry will reside on another
          host, the instructions will need to be adjusted to meet the new
          environment.
        </para>
        <para>
          Also, when migrating from <literal>registry.suse.com</literal> to a
          private registry, migrate the current version of container images
          being used. After the current configuration has been migrated and
          validated, pull new images from <literal>registry.suse.com</literal>
          and upgrade to the new images.
        </para>
      </note>
      <sect3 xml:id="deploy-cephadm-create-local-registry">
        <title>Creating the local registry</title>
        <para>
          To create the local registry, follow these steps:
        </para>
        <procedure>
          <step>
            <para>
              Verify that the Containers Module extension is enabled:
            </para>
<screen>
&prompt.root;SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP3 x86_64 (Activated)
</screen>
          </step>
          <step>
            <para>
              Verify that the following packages are installed:
              <package>apache2-utils</package> (if enabling a secure registry),
              <package>cni</package>, <package>cni-plugins</package>,
              <package>podman</package>, <package>podman-cni-config</package>,
              and <package>skopeo</package>:
            </para>
<screen>&prompt.root;zypper in apache2-utils cni cni-plugins podman podman-cni-config skopeo</screen>
          </step>
          <step>
            <para>
              Gather the following information:
            </para>
            <itemizedlist>
              <listitem>
                <para>
                  Fully qualified domain name of the registry host
                  (<option>REG_HOST_FQDN</option>).
                </para>
              </listitem>
              <listitem>
                <para>
                  An available port number used to map to the registry
                  container port of 5000 (<option>REG_HOST_PORT</option>):
                </para>
<screen>&prompt.user;ss -tulpn | grep :5000</screen>
              </listitem>
              <listitem>
                <para>
                  Whether the registry will be secure or insecure (the
                  <option>insecure=[true|false]</option> option).
                </para>
              </listitem>
              <listitem>
                <para>
                  If the registry will be a secure private registry, determine
                  a username and password
                  (<replaceable>REG_USERNAME</replaceable>,
                  <replaceable>REG_PASSWORD</replaceable>).
                </para>
              </listitem>
              <listitem>
                <para>
                  If the registry will be a secure private registry, a
                  self-signed certificate can be created, or a root certificate
                  and key will need to be provided
                  (<option>REG_CERTIFICATE.CRT</option>,
                  <option>REG_PRIVATE-KEY.KEY</option>).
                </para>
              </listitem>
              <listitem>
                <para>
                  Record the current &cephsalt; containers configuration:
                </para>
<screen>
&prompt.cephuser;ceph-salt config ls /containers
o- containers .......................................................... [...]
o- registries_conf ................................................. [enabled]
| o- registries ...................................................... [empty]
o- registry_auth ....................................................... [...]
o- password ........................................................ [not set]
o- registry ........................................................ [not set]
o- username ........................................................ [not set]
</screen>
              </listitem>
            </itemizedlist>
          </step>
        </procedure>
        <sect4>
          <title>Starting an insecure private registry (without SSL encryption)</title>
          <procedure>
            <step>
              <para>
                Configure &cephsalt; for the insecure registry:
              </para>
<screen>
&prompt.cephuser;ceph-salt config containers/registries_conf enable
&prompt.cephuser;ceph-salt config containers/registries_conf/registries \
 add prefix=REG_HOST_FQDN insecure=true \
 location=REG_HOST_PORT:5000
&prompt.cephuser;ceph-salt apply --non-interactive
</screen>
            </step>
            <step>
              <para>
                Start the insecure registry by creating the necessary
                directory, for example, <filename>/var/lib/registry</filename>
                and starting the registry with the <command>podman</command>
                command:
              </para>
<screen>
&prompt.root;mkdir -p /var/lib/registry
&prompt.root;podman run --privileged -d --name registry \
 -p REG_HOST_PORT:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
              <tip>
                <para>
                  The <option>--net=host</option> parameter allows the &podman;
                  service to be reachable from other nodes.
                </para>
              </tip>
            </step>
            <step>
              <para>
                To have the registry start after a reboot, create a &systemd;
                unit file for it and enable it:
              </para>
<screen>
&prompt.root;podman generate systemd --files --name registry
&prompt.root;mv container-registry.service /etc/systemd/system/
&prompt.root;systemctl enable container-registry.service
</screen>
            </step>
            <step>
              <para>
                Test if the registry is running on port 5000:
              </para>
<screen>&prompt.user;ss -tulpn | grep :5000</screen>
            </step>
            <step>
              <para>
                Test to see if the registry port
                (<option>REG_HOST_PORT</option>) is in an
                <literal>open</literal> state using the <command>nmap</command>
                command. A <option>filtering</option> state will indicate that
                the service will not be reachable from other nodes.
              </para>
<screen>&prompt.user;nmap <replaceable>REG_HOST_FQDN</replaceable></screen>
            </step>
            <step>
              <para>
                The configuration is complete now. Continue by following
                <xref linkend="deploy-populate-local-secure-registry"/>.
              </para>
            </step>
          </procedure>
        </sect4>
        <sect4>
          <title>Starting a secure private registry (with SSL encryption)</title>
          <note>
            <para>
              If an insecure registry was previously configured on the host,
              you need to remove the insecure registry configuration before
              proceeding with the secure private registry configuration.
            </para>
          </note>
          <procedure>
            <step>
              <para>
                Create the necessary directories:
              </para>
<screen>&prompt.root;mkdir -p /var/lib/registry/{auth,certs}</screen>
            </step>
            <step>
              <para>
                Generate an SSL certificate:
              </para>
<screen>
&prompt.user;openssl req -newkey rsa:4096 -nodes -sha256 \
  -keyout /var/lib/registry/certs/REG_PRIVATE-KEY.KEY -x509 -days 365 \
  -out /var/lib/registry/certs/REG_CERTIFICATE.CRT
</screen>
              <tip>
                <para>
                  In this example and throughout the instructions, the key and
                  certificate files are named
                  <replaceable>REG_PRIVATE-KEY.KEY</replaceable> and
                  <replaceable>REG_CERTIFICATE.CRT</replaceable>.
                </para>
              </tip>
              <note>
                <para>
                  Set the <literal>CN=[value]</literal> value to the fully
                  qualified domain name of the host
                  (<option>REG_HOST_FQDN</option>).
                </para>
              </note>
              <tip>
                <para>
                  If a certificate signed by a certificate authority is
                  provided, copy the *.key, *.crt, and intermediate
                  certificates to
                  <filename>/var/lib/registry/certs/.</filename>
                </para>
              </tip>
            </step>
            <step>
              <para>
                Copy the certificate to all cluster nodes and refresh the
                certificate cache:
              </para>
<screen>
&prompt.smaster;salt-cp '*' /var/lib/registry/certs/REG_CERTIFICATE.CRT \
 /etc/pki/trust/anchors/
&prompt.smaster;salt '*' cmd.shell "update-ca-certificates"
</screen>
              <tip>
                <para>
                  &podman; default path is
                  <filename>/etc/containers/certs.d</filename>. Refer to the
                  <literal>containers-certs.d</literal> man page (<command>man
                  5 containers-certs.d</command>) for more details. If a
                  certificate signed by a certificate authority is provided,
                  also copy the intermediate certificates, if provided.
                </para>
              </tip>
            </step>
            <step>
              <para>
                Copy the certificates to
                <filename>/etc/containers/certs.d</filename>:
              </para>
<screen>
&prompt.smaster;salt '*' cmd.shell "mkdir /etc/containers/certs.d"
&prompt.smaster;salt-cp '*' /var/lib/registry/certs/REG_CERTIFICATE.CRT \
 /etc/containers/certs.d/
</screen>
              <para>
                If a certificate signed by a certificate authority is provided,
                also copy the intermediate certificates if provided.
              </para>
            </step>
            <step>
              <para>
                Generate a user name and password combination for
                authentication to the registry:
              </para>
<screen>&prompt.root;htpasswd2 -bBc
              /var/lib/registry/auth/htpasswd
<replaceable>REG_USERNAME</replaceable> <replaceable>REG_PASSWORD</replaceable></screen>
            </step>
            <step>
              <para>
                Start the secure registry. Use the
                <option>REGISTRY_STORAGE_DELETE_ENABLED=true</option> flag so
                that you can delete images afterward with the <command>skopeo
                delete</command> command. Replace
                <option>REG_HOST_PORT</option> with the desired port
                <literal>5000</literal> and provide correct names for
                <replaceable>REG_PRIVATE-KEY.KEY</replaceable>,
                <replaceable>REG_CERTIFICATE.CRT</replaceable>. The registry
                will be <literal>name registry</literal>. If you wish to use a
                different name, rename <option>--name registry</option>.
              </para>
<screen>
&prompt.root;podman run --name registry --net=host -p REG_HOST_PORT:5000 \
  -v /var/lib/registry:/var/lib/registry \
  -v /var/lib/registry/auth:/auth:z \
  -e "REGISTRY_AUTH=htpasswd" \
  -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
  -v /var/lib/registry/certs:/certs:z \
  -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/REG_CERTIFICATE.CRT" \
  -e "REGISTRY_HTTP_TLS_KEY=/certs/REG_PRIVATE-KEY.KEY " \
  -e REGISTRY_STORAGE_DELETE_ENABLED=true \
  -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
              <note>
                <para>
                  The <option>--net=host</option> parameter was added to allow
                  the &podman; service to be reachable from other nodes.
                </para>
              </note>
            </step>
            <step>
              <para>
                To have the registry start after a reboot, create a &systemd;
                unit file for it and enable it:
              </para>
<screen>
&prompt.root;podman generate systemd --files --name registry
&prompt.root;mv container-registry.service /etc/systemd/system/
&prompt.root;systemctl enable container-registry.service
</screen>
              <tip>
                <para>
                  As an alternative, the following will also start &podman;:
                </para>
<screen>
&prompt.root;podman container list --all
&prompt.root;podman start registry
</screen>
              </tip>
            </step>
            <step>
              <para>
                Test if the registry is running on port 5000:
              </para>
<screen>&prompt.user;ss -tulpn | grep :5000</screen>
            </step>
            <step>
              <para>
                Test to see if the registry port
                (<option>REG_HOST_PORT</option>) is in an <quote>open</quote>
                state by using the <command>nmap</command> command. A
                <quote>filtering</quote> state will indicate the service will
                not be reachable from other nodes.
              </para>
<screen>&prompt.user;nmap REG_HOST_FQDN</screen>
            </step>
            <step>
              <para>
                Test secure access to the registry and see the list of
                repositories:
              </para>
<screen>
&prompt.user;curl https://REG_HOST_FQDN:REG_HOST_PORT/v2/_catalog \
 -u <replaceable>REG_USERNAME</replaceable>:<replaceable>REG_PASSWORD</replaceable>
</screen>
              <para>
                Alternatively, use the <option>-k</option> option:
              </para>
<screen>
&prompt.user;curl -k https:// REG_HOST_FQDN:5000/v2/_catalog \
 -u <replaceable>REG_USERNAME</replaceable>:<replaceable>REG_PASSWORD</replaceable>
</screen>
            </step>
            <step>
              <para>
                You can verify the certificate by using the following command:
              </para>
<screen>openssl s_client -connect REG_HOST_FQDN:5000 -servername REG_HOST_FQDN</screen>
            </step>
            <step>
              <para>
                Modify <filename>/etc/environment</filename> and include the
                <option>export GODEBUG=x509ignoreCN=0</option> option for
                self-signed certificates. Ensure that the existing
                configuration in <filename>/etc/environment</filename> is not
                overwritten:
              </para>
<screen>salt '*' cmd.shell 'echo "export GODEBUG=x509ignoreCN=0" >>/etc/environment</screen>
              <para>
                In the current terminal, run the following command:
              </para>
<screen>&prompt.user;export GODEBUG=x509ignoreCN=0</screen>
            </step>
            <step>
              <para>
                Configure the URL of the local registry with username and
                password, then apply the configuration:
              </para>
<screen>
&prompt.cephuser;ceph-salt config /containers/registry_auth/registry \
  set REG_HOST_FQDN:5000
&prompt.cephuser;ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable>
&prompt.cephuser;ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable>
</screen>
            </step>
            <step>
              <para>
                Verify configuration change, for example:
              </para>
<screen>
&prompt.cephuser;ceph-salt config ls /containers
o- containers .......................................................... [...]
o- registries_conf ................................................. [enabled]
| o- registries ...................................................... [empty]
o- registry_auth ....................................................... [...]
o- password ................................................... [REG_PASSWORD]
o- registry .......................................... [host.example.com:5000]
o- username ................................................... [REG_USERNAME]
</screen>
              <para>
                If the configuration is correct, run:
              </para>
<screen>&prompt.cephuser;ceph-salt apply</screen>
              <para>
                or
              </para>
<screen>&prompt.cephuser;ceph-salt apply --non-interactive</screen>
            </step>
            <step>
              <para>
                Update &cephadm; credentials:
              </para>
<screen>
&prompt.cephuser;ceph cephadm registry-login REG_HOST_FQDN:5000 \
 <replaceable>REG_USERNAME </replaceable> <replaceable>REG_PASSWORD</replaceable>
&prompt.cephuser;ceph cephadm registry-login REG_HOST_FQDN:5000 \
 <replaceable>REG_USERNAME</replaceable> <replaceable>REG_PASSWORD</replaceable>
</screen>
              <para>
                If the command fails, double check
                <replaceable>REG_HOST_FQDN</replaceable>,
                <replaceable>REG_USERNAME</replaceable>, and
                <replaceable>REG_PASSWORD</replaceable> values to ensure they
                are correct. If the command fails again, fail the active MGR to
                a different MGR daemon:
              </para>
<screen>&prompt.cephuser;ceph mgr fail</screen>
              <para>
                Validate the configuration:
              </para>
<screen>&prompt.cephuser;ceph config-key dump | grep 'registry_credentials</screen>
            </step>
            <step>
              <para>
                If the cluster is using a proxy configuration to reach
                <literal>registry.suse.com</literal>, the configuration should
                be removed. The node running the private registry will still
                need access to <literal>registry.suse.com</literal> and may
                still need the proxy configuration. For example:
              </para>
<screen>
&exampleuserII;:/etc/containers&prompt.user;grep -A1 "env =" containers.conf
env = ["https_proxy=http://some.url.example.com:8080",
"no_proxy=s3.some.url.exmple.com"]
</screen>
            </step>
            <step>
              <para>
                The configuration is complete now. Continue by following
                <xref linkend="deploy-populate-local-secure-registry"/>.
              </para>
            </step>
          </procedure>
        </sect4>
        <sect4 xml:id="deploy-populate-local-secure-registry">
          <title>Populating the secure local registry</title>
          <procedure>
            <step>
              <para>
                When the local registry is created, you need to synchronize
                container images from the official &suse; registry at
                <literal>registry.suse.com</literal> to the local one. You can
                use the <command>skopeo sync</command> command found in the
                <package>skopeo</package> package for that purpose. For more
                details, refer to the man page (<command>man 1
                skopeo-sync</command>). Consider the following examples:
              </para>
              <example>
                <title>Viewing MANIFEST files on &suse; registry</title>
<screen>
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/ceph | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/grafana | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.3.0 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.23.0 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/haproxy:2.0.14 | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/keepalived:2.0.19 | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://registry.suse.com/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1 | \
  jq .RepoTags
</screen>
                <para>
                  In the event that &suse; has provided a PTF, you can also
                  inspect the PTF using the path provided by &suse;.
                </para>
              </example>
            </step>
            <step>
              <para>
                Log in to the secure private registry.
              </para>
<screen>
&prompt.root;podman login REG_HOST_FQDN:5000 \
 -u <replaceable>REG_USERNAME</replaceable> -p <replaceable>REG_PASSWORD</replaceable>
</screen>
              <note>
                <para>
                  An insecure private registry does not require login.
                </para>
              </note>
            </step>
            <step>
              <para>
                Pull images to the local registry, push images to the private
                registry.
              </para>
<screen>
&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/ceph:latest
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/ceph:latest \
  REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/grafana:8.3.10
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/grafana:8.3.10 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/grafana:8.3.10
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/grafana:8.3.10

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-server:2.32.1
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-server:2.32.1

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.23.0
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.23.0 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-alertmanager:0.23.0
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-alertmanager:0.23.0

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.3.0
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.3.0 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-node-exporter:1.3.0
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-node-exporter:1.3.0

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/haproxy:2.0.14
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/haproxy:2.0.14 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/haproxy:2.0.14
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/haproxy:2.0.14

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/keepalived:2.0.19
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/keepalived:2.0.19 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/keepalived:2.0.19
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/keepalived:2.0.19

&prompt.root;podman pull registry.suse.com/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1
&prompt.root;podman tag registry.suse.com/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1 \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1
&prompt.root;podman push REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1
</screen>
            </step>
            <step>
              <para>
                Log out of the private registry:
              </para>
<screen>&prompt.root;podman logout REG_HOST_FQDN:5000</screen>
            </step>
            <step>
              <para>
                List images with <command>podman images</command> or
                <command>podman images | sort</command>.
              </para>
            </step>
            <step>
              <para>
                List repositories:
              </para>
<screen>
&prompt.user;curl -sSk https://REG_HOST_FQDN:5000/v2/_catalog \
 -u <replaceable>REG_USERNAME</replaceable>:<replaceable>REG_PASSWORD</replaceable> | jq
</screen>
            </step>
            <step>
              <para>
                Use <command>skopeo</command> to inspect the private registry.
                Note that you need to log in to make the following commands
                work:
              </para>
<screen>
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/ceph | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/grafana:8.3.10 | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-server:2.32.1 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-alertmanager:0.23.0 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-node-exporter:1.3.0 | \
  jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/haproxy:2.0.14 | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/keepalived:2.0.19 | jq .RepoTags
&prompt.user;skopeo inspect \
  docker://REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1 | \
  jq .RepoTags
</screen>
            </step>
            <step>
              <para>
                List repository tags example:
              </para>
<screen>
&prompt.user;curl -sSk https://REG_HOST_FQDN:5000/v2/ses/7.1/ceph/ceph/tags/list \
 -u REG_CERTIFICATE.CRT:REG_PRIVATE-KEY.KEY
{"name":"ses/7.1/ceph/ceph","tags":["latest"]}
</screen>
            </step>
            <step>
              <para>
                Test pulling an image from another node.
              </para>
              <substeps>
                <step>
                  <para>
                    Log in to the private registry. Note that an insecure
                    private registry does not require login.
                  </para>
<screen>
&prompt.root;podman login REG_HOST_FQDN:5000 -u REG_CERTIFICATE.CRT \
  -p REG_PRIVATE-KEY.KEY
</screen>
                </step>
                <step>
                  <para>
                    Pull an image from the private registry. Optionally, you
                    can specify login credentials on the command line. For
                    example:
                  </para>
<screen>
&prompt.root;podman pull REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest
&prompt.root;podman pull --creds= <replaceable>REG_USERNAME</replaceable>:<replaceable>REG_PASSWORD</replaceable> \
  REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest
&prompt.root;podman pull --cert-dir=/etc/containers/certs.d \
  REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest
</screen>
                </step>
                <step>
                  <para>
                    Log out of the private registry when done:
                  </para>
<screen>&prompt.root;podman logout REG_HOST_FQDN:5000</screen>
                </step>
              </substeps>
            </step>
          </procedure>
        </sect4>
        <sect4>
          <title>Configuring &ceph; to pull images from the private registry</title>
          <procedure>
            <step>
              <para>
                List configuration settings that need to change:
              </para>
<screen>&prompt.cephuser;ceph config ls | grep container_image</screen>
            </step>
            <step>
              <para>
                View current configurations:
              </para>
<screen>
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_alertmanager
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_base
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_grafana
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_haproxy
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_keepalived
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_node_exporter
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_prometheus
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_snmp_gateway
</screen>
            </step>
            <step>
              <para>
                Configure the cluster to use the private registry.
              </para>
              <para>
                Do <emphasis role="bold">not</emphasis> set the following
                options:
              </para>
<screen>
ceph config set mgr mgr/cephadm/container_image_base
ceph config set global container_image
</screen>
              <para>
                Set the following options:
              </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_alertmanager \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-alertmanager:0.23.0
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_grafana \
  REG_HOST_FQDN:5000/ses/7.1/ceph/grafana:8.3.10
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_haproxy \
  REG_HOST_FQDN:5000/ses/7.1/ceph/haproxy:2.0.14
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_keepalived \
  REG_HOST_FQDN:5000/ses/7.1/ceph/keepalived:2.0.19
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_node_exporter \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-node-exporter:1.3.0
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_prometheus \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-server:2.32.1
&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_snmp_gateway \
  REG_HOST_FQDN:5000/ses/7.1/ceph/prometheus-snmp_notifier:1.2.1
</screen>
            </step>
            <step>
              <para>
                Double-check if there are other custom configurations that need
                to be modified or removed and if the correct configuration was
                applied:
              </para>
<screen>&prompt.cephuser;ceph config dump | grep container_image</screen>
            </step>
            <step>
              <para>
                All the daemons need to be redeployed to ensure the correct
                images are being used:
              </para>
<screen>
&prompt.cephuser;ceph orch upgrade start \
  --image REG_HOST_FQDN:5000/ses/7.1/ceph/ceph:latest
</screen>
              <tip>
                <para>
                  Monitor the redeployment by running the following command:
                </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
              </tip>
            </step>
            <step>
              <para>
                After the upgrade is complete, validate that the daemons have
                been updated. Note the <literal>VERSION</literal> and
                <literal>IMAGE ID</literal> values in the right columns of the
                following table:
              </para>
<screen>&prompt.cephuser;ceph orch ps</screen>
              <tip>
                <para>
                  For more details, run the following command:
                </para>
<screen>
&prompt.cephuser;ceph orch ps --format=yaml | \
  egrep "daemon_name|container_image_name"
</screen>
              </tip>
            </step>
            <step>
              <para>
                If needed, redeploy services or daemons:
              </para>
<screen>
&prompt.cephuser;ceph orch redeploy <replaceable>SERVICE_NAME</replaceable>
&prompt.cephuser;ceph orch daemon rm <replaceable>DAEMON_NAME</replaceable>
</screen>
            </step>
          </procedure>
        </sect4>
      </sect3>
      <sect3 xml:id="deploy-cephadm-configure-imagepath">
        <title>Configuring the path to container images</title>
        <important>
          <para>
            This section helps you configure the path to container images of
            the bootstrap cluster (deployment of the first &mon; and &mgr;
            pair). The path does not apply to container images of additional
            services, for example the monitoring stack.
          </para>
        </important>
        <tip>
          <title>Configuring HTTPS proxy</title>
          <para>
            If you need to use a proxy to communicate with the container
            registry server, perform the following configuration steps on all
            cluster nodes:
          </para>
          <procedure>
            <step>
              <para>
                Copy the configuration file for containers:
              </para>
<screen>
&prompt.sudo;cp /usr/share/containers/containers.conf \
  /etc/containers/containers.conf
</screen>
            </step>
            <step>
              <para>
                Edit the newly copied file and add the
                <option>http_proxy</option> setting to its
                <literal>[engine]</literal> section, for example:
              </para>
<screen>
&prompt.user;cat /etc/containers/containers.conf
[engine]
env = ["http_proxy=http://proxy.example.com:<replaceable>PORT</replaceable>"]
[...]
</screen>
              <para>
                If the container needs to communicate over HTTP(S) to a
                specific host or list of hosts bypassing the proxy server, add
                an exception using the <option>no_proxy</option> option:
              </para>
<screen>
&prompt.user;cat /etc/containers/containers.conf
[engine]
env = ["http_proxy=http://proxy.example.com:<replaceable>PORT</replaceable>",
 "no_proxy=rgw.example.com"]
[...]
</screen>
            </step>
          </procedure>
        </tip>
        <para>
          &cephadm; needs to know a valid URI path to container images. Verify
          the default setting by executing
        </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
        <para>
          If you do not need an alternative or local registry, specify the
          default &suse; container registry:
        </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set \
  registry.suse.com/ses/7.1/ceph/ceph
</screen>
        <para>
          If your deployment requires a specific path, for example, a path to a
          local registry, configure it as follows:
        </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
      </sect3>
    </sect2>

    <sect2 xml:id="deploy-cephadm-inflight-encryption">
      <title>Enabling data in-flight encryption (msgr2)</title>
      <para>
        The Messenger v2 protocol (MSGR2) is &ceph;'s on-wire protocol. It
        provides a security mode that encrypts all data passing over the
        network, encapsulation of authentication payloads, and the enabling of
        future integration of new authentication modes (such as Kerberos).
      </para>
      <important>
        <para>
          msgr2 is not currently supported by Linux kernel Ceph clients, such
          as &cephfs; and &rbd;.
        </para>
      </important>
      <para>
        &ceph; daemons can bind to multiple ports, allowing both legacy &ceph;
        clients and new v2-capable clients to connect to the same cluster. By
        default, MONs now bind to the new IANA-assigned port 3300 (CE4h or
        0xCE4) for the new v2 protocol, while also binding to the old default
        port 6789 for the legacy v1 protocol.
      </para>
      <para>
        The v2 protocol (MSGR2) supports two connection modes:
      </para>
      <variablelist>
        <varlistentry>
          <term>crc mode</term>
          <listitem>
            <para>
              A strong initial authentication when the connection is
              established and a CRC32C integrity check.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>secure mode</term>
          <listitem>
            <para>
              A strong initial authentication when the connection is
              established and full encryption of all post-authentication
              traffic, including a cryptographic integrity check.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        For most connections, there are options that control which modes are
        used:
      </para>
      <variablelist>
        <varlistentry>
          <term>ms_cluster_mode</term>
          <listitem>
            <para>
              The connection mode (or permitted modes) used for intra-cluster
              communication between &ceph; daemons. If multiple modes are
              listed, the modes listed first are preferred.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>ms_service_mode</term>
          <listitem>
            <para>
              A list of permitted modes for clients to use when connecting to
              the cluster.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>ms_client_mode</term>
          <listitem>
            <para>
              A list of connection modes, in order of preference, for clients
              to use (or allow) when talking to a &ceph; cluster.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        There are a parallel set of options that apply specifically to
        monitors, allowing administrators to set different (usually more
        secure) requirements on communication with the monitors.
      </para>
      <variablelist>
        <varlistentry>
          <term>ms_mon_cluster_mode</term>
          <listitem>
            <para>
              The connection mode (or permitted modes) to use between monitors.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>ms_mon_service_mode</term>
          <listitem>
            <para>
              A list of permitted modes for clients or other &ceph; daemons to
              use when connecting to monitors.
            </para>
          </listitem>
        </varlistentry>
        <varlistentry>
          <term>ms_mon_client_mode</term>
          <listitem>
            <para>
              A list of connection modes, in order of preference, for clients
              or non-monitor daemons to use when connecting to monitors.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>
      <para>
        In order to enable MSGR2 encryption mode during the deployment, you
        need to add some configuration options to the &cephsalt; configuration
        before running <command>ceph-salt apply</command>.
      </para>
      <para>
        To use <literal>secure</literal> mode, run the following commands.
      </para>
      <para>
        Add the global section to <filename>ceph_conf</filename> in the
        &cephsalt; configuration tool:
      </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
      <para>
        Set the following options:
      </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
      <note>
        <para>
          Ensure <literal>secure</literal> precedes <literal>crc</literal>.
        </para>
      </note>
      <para>
        To <emphasis>force</emphasis> <literal>secure</literal> mode, run the
        following commands:
      </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
      <tip xml:id="update-inflight-encryption-settings">
        <title>Updating settings</title>
        <para>
          If you want to change any of the above settings, set the
          configuration changes in the monitor configuration store. This is
          achieved using the <command>ceph config set</command> command.
        </para>
<screen>&prompt.smaster;ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
        <para>
          For example:
        </para>
<screen>&prompt.smaster;ceph config set global ms_cluster_mode "secure crc"</screen>
        <para>
          If you want to check the current value, including default value, run
          the following command:
        </para>
<screen>&prompt.smaster;ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
        <para>
          For example, to get the <literal>ms_cluster_mode</literal> for OSD's,
          run:
        </para>
<screen>&prompt.smaster;ceph config get osd ms_cluster_mode</screen>
      </tip>
    </sect2>

    <sect2 xml:id="deploy-cephadm-enable-network">
      <title>Configuring the cluster network</title>
      <para>
        Optionally, if you are running a separate cluster network, you may need
        to set the cluster network IP address followed by the subnet mask part
        after the slash sign, for example <literal>192.168.10.22/24</literal>.
      </para>
      <para>
        Run the following commands to enable
        <literal>cluster_network</literal>:
      </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-verify">
      <title>Verifying the cluster configuration</title>
      <para>
        The minimal cluster configuration is finished. Inspect it for obvious
        errors:
      </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-main.example.com .......  ........................... [admin]
  | | o- ses-node1.example.com ........................ [bootstrap, admin]
  | | o- ses-node2.example.com ................................ [no roles]
  | | o- ses-node3.example.com ................................ [no roles]
  | | o- ses-node4.example.com ................................ [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-main.example.com ......................... [no other roles]
  |   | o- ses-node1.example.com ................ [other roles: bootstrap]
  |   o- bootstrap ............................... [ses-node1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path ............. [registry.suse.com/ses/7.1/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-main.example.com ...................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
      <tip>
        <title>Status of cluster configuration</title>
        <para>
          You can check if the configuration of the cluster is valid by running
          the following command:
        </para>
<screen>
&prompt.smaster;ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
      </tip>
    </sect2>

    <sect2 xml:id="deploy-cephadm-configure-export">
      <title>Exporting cluster configurations</title>
      <para>
        After you have configured the basic cluster and its configuration is
        valid, it is a good idea to export its configuration to a file:
      </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
      <warning>
        <para>
          The output of the <command>ceph-salt export</command> includes the
          SSH private key. If you are concerned about the security
          implications, do not execute this command without taking appropriate
          precautions.
        </para>
      </warning>
      <para>
        In case you break the cluster configuration and need to revert to a
        backup state, run:
      </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
    </sect2>
  </sect1>
  <sect1 xml:id="deploy-cephadm-deploy">
    <title>Updating nodes and bootstrap minimal cluster</title>

    <para>
      Before you deploy the cluster, update all software packages on all nodes:
    </para>

<screen>&prompt.smaster;ceph-salt update</screen>

    <para>
      If a node reports <literal>Reboot is needed</literal> during the update,
      important OS packages&mdash;such as the kernel&mdash;were updated to a
      newer version and you need to reboot the node to apply the changes.
    </para>

    <para>
      To reboot all nodes that require rebooting, either append the
      <option>--reboot</option> option
    </para>

<screen>&prompt.smaster;ceph-salt update --reboot</screen>

    <para>
      Or, reboot them in a separate step:
    </para>

<screen>&prompt.smaster;ceph-salt reboot</screen>

    <important>
      <para>
        The &smaster; is never rebooted by <command>ceph-salt update
        --reboot</command> or <command>ceph-salt reboot</command> commands. If
        the &smaster; needs rebooting, you need to reboot it manually.
      </para>
    </important>

    <para>
      After the nodes are updated, bootstrap the minimal cluster:
    </para>

<screen>&prompt.smaster;ceph-salt apply</screen>

    <note>
      <para>
        When bootstrapping is complete, the cluster will have one &mon; and one
        &mgr;.
      </para>
    </note>

    <para>
      The above command will open an interactive user interface that shows the
      current progress of each minion.
    </para>

    <figure>
      <title>Deployment of a minimal cluster</title>
      <mediaobject>
        <imageobject role="fo">
          <imagedata fileref="cephadm_deploy.png" width="75%"/>
        </imageobject>
        <imageobject role="html">
          <imagedata fileref="cephadm_deploy.png" width="75%"/>
        </imageobject>
      </mediaobject>
    </figure>

    <tip>
      <title>Non-interactive mode</title>
      <para>
        If you need to apply the configuration from a script, there is also a
        non-interactive mode of deployment. This is also useful when deploying
        the cluster from a remote machine because constant updating of the
        progress information on the screen over the network may become
        distracting:
      </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
    </tip>
  </sect1>
  <sect1 xml:id="deploy-min-cluster-final-steps">
    <title>Reviewing final steps</title>

    <para>
      After the <command>ceph-salt apply</command> command has completed, you
      should have one &mon; and one &mgr;. You should be able to run the
      <command>ceph status</command> command successfully on any of the minions
      that were given the <literal>admin</literal> role as
      <literal>root</literal> or the <literal>cephadm</literal> user using
      <literal>sudo</literal>.
    </para>

    <para>
      The next steps involve using the &cephadm; to deploy additional &mon;,
      &mgr;, OSDs, the Monitoring Stack, and Gateways.
    </para>

    <para>
      Before you continue, review your new cluster's network settings. At this
      point, the <literal>public_network</literal> setting has been populated
      based on what was entered for
      <literal>/cephadm_bootstrap/mon_ip</literal> in the
      <literal>ceph-salt</literal> configuration. However, this setting was
      only applied to &mon;. You can review this setting with the following
      command:
    </para>

<screen>&prompt.smaster;ceph config get mon public_network</screen>

    <para>
      This is the minimum that &ceph; requires to work, but we recommend making
      this <literal>public_network</literal> setting <literal>global</literal>,
      which means it will apply to all types of &ceph; daemons, and not only to
      MONs:
    </para>

<screen>&prompt.smaster;ceph config set global public_network "$(ceph config get mon public_network)"</screen>

    <note>
      <para>
        This step is not required. However, if you do not use this setting, the
        &ceph; OSDs and other daemons (except &mon;) will listen on
        <emphasis>all addresses</emphasis>.
      </para>
      <para>
        If you want your OSDs to communicate amongst themselves using a
        completely separate network, run the following command:
      </para>
<screen>&prompt.smaster;ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
      <para>
        Executing this command will ensure that the OSDs created in your
        deployment will use the intended cluster network from the start.
      </para>
    </note>

    <para>
      If your cluster is set to have dense nodes (greater than 62 OSDs per
      host), make sure to assign sufficient ports for &ceph; OSDs. The default
      range (6800-7300) currently allows for no more than 62 OSDs per host. For
      a cluster with dense nodes, adjust the setting
      <literal>ms_bind_port_max</literal> to a suitable value. Each OSD will
      consume eight additional ports. For example, given a host that is set to
      run 96 OSDs, 768 ports will be needed.
      <literal>ms_bind_port_max</literal> should be set at least to 7568 by
      running the following command:
    </para>

<screen>&prompt.smaster;ceph config set osd.* ms_bind_port_max 7568</screen>

    <para>
      You will need to adjust your firewall settings accordingly for this to
      work. See <xref linkend="storage-bp-net-firewall"/> for more information.
    </para>
  </sect1>
  <sect1 xml:id="deploy-min-cluster-disable-insecure">
    <title>Disable insecure clients</title>

    <para>
      Since &cephname; v15.2.11, a new health warning was introduced that
      informs you that insecure clients are allowed to join the cluster. This
      warning is <emphasis>on</emphasis> by default. The &dashboard; will show
      the cluster in the <literal>HEALTH_WARN</literal> status and verifying
      the cluster status on the command line informs you as follows:
    </para>

<screen>
&prompt.cephuser;ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>

    <para>
      This warning means that the &mon;s are still allowing old, unpatched
      clients to connect to the cluster. This ensures existing clients can
      still connect while the cluster is being upgraded, but warns you that
      there is a problem that needs to be addressed. When the cluster and all
      clients are upgraded to the latest version of &ceph;, disallow unpatched
      clients by running the following command:
    </para>

<screen>&prompt.cephuser;ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
  </sect1>
</chapter>
