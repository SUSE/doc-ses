<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-bootstrap">
 <info>
  <title>Deploying the bootstrap cluster using &cephsalt;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section guides you through the process of deploying a basic &ceph;
  cluster. Read the following subsections carefully and execute the included
  commands in the given order.
 </para>
 <sect1 xml:id="deploy-cephadm-cephsalt">
  <title>Installing &cephsalt;</title>

  <para>
   &cephsalt; provides tools for deploying &ceph; clusters managed by
   &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
   management&mdash;for example, software updates or time
   synchronization&mdash;and defining roles for &sminion;s.
  </para>

  <para>
   On the &smaster;, install the <package>ceph-salt</package> package:
  </para>

<screen>&prompt.smaster;zypper install ceph-salt</screen>

  <para>
   The above command installed <package>ceph-salt-formula</package> as a
   dependency which modified the &smaster; configuration by inserting
   additional files in the <filename>/etc/salt/master.d</filename> directory.
   To apply the changes, restart
   <systemitem class="daemon">salt-master.service</systemitem> and synchronize
   &salt; modules:
  </para>

<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
 </sect1>
 <sect1 xml:id="deploy-cephadm-configure">
  <title>Configuring cluster properties</title>

  <para>
   Use the <command>ceph-salt config</command> command to configure the basic
   properties of the cluster.
  </para>

  <important>
   <para>
    The <filename>/etc/ceph/ceph.conf</filename> file is managed by &cephadm;
    and users <emphasis>should not</emphasis> edit it. &ceph; configuration
    parameters should be set using the new <command>ceph config</command>
    command. See <xref linkend="cha-ceph-configuration-db"/> for more
    information.
   </para>
  </important>

  <sect2 xml:id="deploy-cephadm-configure-shell">
   <title>Using the &cephsalt; shell</title>
   <para>
    If you run <command>ceph-salt config</command> without any path or
    subcommand, you will enter an interactive &cephsalt; shell. The shell is
    convenient if you need to configure multiple properties in one batch and do
    not want type the full command syntax.
   </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
   <para>
    As you can see from the output of &cephsalt;'s <command>ls</command>
    command, the cluster configuration is organized in a tree structure. To
    configure a specific property of the cluster in the &cephsalt; shell, you
    have two options:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run the command from the current position and enter the absolute path to
      the property as the first argument:
     </para>
<screen>
<prompt>/></prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/> /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
    </listitem>
    <listitem>
     <para>
      Change to the path whose property you need to configure and run the
      command:
     </para>
<screen>
<prompt>/></prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions></prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Autocompletion of configuration snippets</title>
    <para>
     While in a &cephsalt; shell, you can use the autocompletion feature
     similar to a normal Linux shell (Bash) autocompletion. It completes
     configuration paths, subcommands, or &sminion; names. When autocompleting
     a configuration path, you have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       To let the shell finish a path relative to your current position,press
       the TAB key <keycap function="tab"></keycap> twice.
      </para>
     </listitem>
     <listitem>
      <para>
       To let the shell finish an absolute path, enter <keycap>/</keycap> and
       press the TAB key <keycap function="tab"></keycap> twice.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <tip>
    <title>Navigating with the cursor keys</title>
    <para>
     If you enter <command>cd</command> from the &cephsalt; shell without any
     path, the command will print a tree structure of the cluster configuration
     with the line of the current path active. You can use the up and down
     cursor keys to navigate through individual lines. After you confirm with
     <keycap function="enter"></keycap>, the configuration path will change to
     the last active one.
    </para>
   </tip>
   <important>
    <title>Convention</title>
    <para>
     To keep the documentation consistent, we will use a single command syntax
     without entering the &cephsalt; shell. For example, you can list the
     cluster configuration tree by using the following command:
    </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
   </important>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-minions">
   <title>Adding &sminion;s</title>
   <para>
    Include all or a subset of &sminion;s that we deployed and accepted in
    <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
    either specify the &sminion;s by their full names, or use a glob
    expressions '*' and '?' to include multiple &sminion;s at once. Use the
    <command>add</command> subcommand under the
    <literal>/ceph_cluster/minions</literal> path. The following command
    includes all accepted &sminion;s:
   </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
   <para>
    Verify that the specified &sminion;s were added:
   </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-cephadm">
   <title>Specifying &sminion;s managed by &cephadm;</title>
   <para>
    Specify which nodes will belong to the &ceph; cluster and will be managed
    by &cephadm;. Include all nodes that will run &ceph; services as well as
    the &adm;:
   </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-admin">
   <title>Specifying &adm;</title>
   <para>
    The &adm; is the node where the <filename>ceph.conf</filename>
    configuration file and the &ceph; admin keyring is installed. You usually
    run &ceph; related commands on the &adm;.
   </para>
   <tip>
    <title>&smaster; and &adm; on the Same Node</title>
    <para>
     In a homogeneous environment where all or most hosts belong to
     &productname;, we recommend having the &adm; on the same host as the
     &smaster;.
    </para>
    <para>
     In a heterogeneous environment where one &salt; infrastructure hosts more
     than one cluster, for example, &productname; together with &susemgr;, do
     <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
    </para>
   </tip>
   <para>
    To specify the &adm;, run the following command:
   </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
   <tip>
    <title>Install <filename>ceph.conf</filename> and the admin keyring on multiple nodes</title>
    <para>
     You can install the &ceph; configuration file and admin keyring on
     multiple nodes if your deployment requires it. For security reasons, avoid
     installing them on all the cluster's nodes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-mon">
   <title>Specifying first MON/MGR node</title>
   <para>
    You need to specify which of the cluster's &sminion;s will bootstrap the
    cluster. This minion will become the first one running &mon; and &mgr;
    services.
   </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
   <para>
    Additionally, you need to specify the bootstrap MON's IP address on the
    public network to ensure that the <option>public_network</option> parameter
    is set correctly, for example:
   </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-tuned-profiles">
   <title>Specifying tuned profiles</title>
   <para>
    You need to specify which of the cluster's minions have actively tuned
    profiles. To do so, add these roles explicitly with the following commands:
   </para>
   <note>
    <para>
     One minion cannot have both the <literal>latency</literal> and
     <literal>throughput</literal> roles.
    </para>
   </note>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ssh">
   <title>Generating an SSH key pair</title>
   <para>
    &cephadm; uses the SSH protocol to communicate with cluster nodes. A user
    account named <literal>cephadm</literal> is automatically created and used
    for SSH communication.
   </para>
   <para>
    You need to generate the private and public part of the SSH key pair:
   </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ntp">
   <title>Configuring the time server</title>
   <para>
    All cluster nodes need to have their time synchronized with a reliable time
    source. There are several scenarios to approach time synchronization:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      If all cluster nodes are already configured to synchronize their time
      using an NTP service of choice, disable time server handling completely:
     </para>
<screen>
&prompt.smaster;ceph-salt config /time_server disable
</screen>
    </listitem>
    <listitem>
     <para>
      If your site already has a single source of time, specify the host name
      of the time source:
     </para>
<screen>
 &prompt.smaster;ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
    </listitem>
    <listitem>
     <para>
      Alternatively, &cephsalt; has the ability to configure one of the
      &sminion; to serve as the time server for the rest of the cluster. This
      is sometimes referred to as an "internal time server". In this scenario,
      &cephsalt; will configure the internal time server (which should be one
      of the &sminion;) to synchronize its time with an external time server,
      such as <literal>pool.ntp.org</literal>, and configure all the other
      minions to get their time from the internal time server. This can be
      achieved as follows:
     </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/servers add ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
     <para>
      The <option>/time_server/subnet</option> option specifies the subnet from
      which NTP clients are allowed to access the NTP server. It is
      automatically set when you specify <option>/time_server/servers</option>.
      If you need to change it or specify it manually, run:
     </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
    </listitem>
   </itemizedlist>
   <para>
    Check the time server settings:
   </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
   <para>
    Find more information on setting up time synchronization in
    <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
   </para>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-dashboardlogin">
   <title>Configuring the &dashboard; login credentials</title>
   <para>
    &dashboard; will be available after the basic cluster is deployed. To
    access it, you need to set a valid user name and password, for example:
   </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/username set admin
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
   <tip>
    <title>Forcing password update</title>
    <para>
     By default, the first dashboard user will be forced to change their
     password on first login to the dashboard. To disable this feature, run the
     following command:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-registry">
   <title>Using the container registry</title>
   <para>
    The &ceph; cluster needs to have access to a container registry so that it
    can download and deploy containerized &ceph; services. There are two ways
    to access the registry:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      If your cluster can access the default registry at
      <literal>registry.suse.com</literal> (directly or via proxy), you can
      point &cephsalt; directly to this URL without creating a local registry.
      Continue by following the steps in
      <xref linkend="deploy-cephadm-configure-imagepath"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      If your cluster cannot access the default registry&mdash;for example, for
      an air-gapped deployment&mdash;you need to configure a local container
      registry. After the local registry is created and configured, you need to
      point &cephsalt; to it.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="updating-ceph-local-registry">
    <title>Creating and configuring the local registry (optional)</title>
    <important>
     <para>
      There are numerous methods of creating a local registry. The instructions
      in this section are examples of creating secure and insecure registries.
     </para>
    </important>
    <tip>
     <title>Placement and port usage</title>
     <para>
      Deploy the registry on a machine accessible by all nodes in the cluster.
      We recommend the &adm;. By default, the registry listens on port 5000.
     </para>
     <para>
      On the registry node, use the following command to ensure that the port
      is free:
     </para>
<screen>ss -tulpn | grep :5000</screen>
     <para>
      If other processes (such as <literal>iscsi-tcmu</literal>) are already
      listening on port 5000, determine another free port which can be used to
      map to port 5000 in the registry container.
     </para>
    </tip>
    <procedure>
     <title>Creating the local registry</title>
     <step>
      <para>
       Verify that the <package>Containers Module</package> extension is
       enabled:
      </para>
<screen>
&prompt.user;SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP2 x86_64 (Activated)
</screen>
     </step>
     <step>
      <para>
       Verify that the following packages are installed:
       <package>apache2-utils</package> (if enabling a secure registry),
       <package>cni</package>, <package>cni-plugins</package>,
       <package>podman</package>, <package>podman-cni-config</package>, and
       <package>skopeo</package>.
      </para>
     </step>
     <step>
      <para>
       Gather the following information:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Fully qualified domain name of the registry host
         (<option>REG_HOST_FQDN</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         An available port number used to map to the registry container port of
         5000 (<option>REG_HOST_PORT</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Whether the registry will be secure or insecure
         (<option>insecure=[true|false]</option>).
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       To start an insecure registry (without SSL encryption), follow these
       steps:
      </para>
      <substeps>
       <step>
        <para>
         Configure &cephsalt; for the insecure registry:
        </para>
<screen>
&prompt.cephuser;ceph-salt config containers/registries_conf enable
&prompt.cephuser;ceph-salt config containers/registries_conf/registries \
 add prefix=<option>REG_HOST_FQDN</option> insecure=true \
 location=<option>REG_HOST_PORT</option>:5000
</screen>
       </step>
       <step>
        <para>
         Start the insecure registry by creating the necessary directory (for
         example, <filename>/var/lib/registry</filename>) and starting the
         registry with the <command>podman</command> command:
        </para>
<screen>
&prompt.root;mkdir -p /var/lib/registry
&prompt.root;podman run --privileged -d --name registry \
 -p <option>REG_HOST_PORT</option>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
       </step>
       <step>
        <para>
         To have the registry start after a reboot, create a &systemd; unit
         file for it and enable it:
        </para>
<screen>
&prompt.sudo;podman generate systemd --files --name registry
&prompt.sudo;mv container-registry.service /etc/systemd/system/
&prompt.sudo;systemctl enable container-registry.service
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       To start a secure registry, follow these steps:
      </para>
      <substeps>
       <step>
        <para>
         Create the necessary directories:
        </para>
<screen>&prompt.root;mkdir -p /var/lib/registry/{auth,certs}</screen>
       </step>
       <step>
        <para>
         Generate an SSL certificate:
        </para>
<screen>
&prompt.root;openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt
</screen>
        <note>
         <para>
          Set the <literal>CN=[value]</literal> value to the fully qualified
          domain name of the host ([<option>REG_HOST_FQDN</option>]).
         </para>
        </note>
       </step>
       <step>
        <para>
         Copy the certificate to all cluster nodes and refresh the certificate
         cache:
        </para>
<screen>
&prompt.root;salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
&prompt.root;salt '*' cmd.shell "update-ca-certificates"
</screen>
       </step>
       <step>
        <para>
         Generate a username and password combination for authentication to the
         registry:
        </para>
<screen>
&prompt.root;htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <option>REG_USERNAME</option> <option>REG_PASSWORD</option>
</screen>
       </step>
       <step>
        <para>
         Start the secure registry:
        </para>
<screen>
podman run --name myregistry -p <option>REG_HOST_PORT</option>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
       </step>
       <step>
        <para>
         Test secure access to the registry:
        </para>
<screen>
&prompt.user;curl https://<option>REG_HOST_FQDN</option>:<option>REG_HOST_PORT</option>/v2/_catalog \
 -u <option>REG_USERNAME</option>:<option>REG_PASSWORD</option>
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       When the local registry is created, you need to synchronize container
       images from the official &suse; registry at
       <literal>registry.suse.com</literal> to the local one. You can use the
       <command>skopeo sync</command> command found in the
       <package>skopeo</package> package for that purpose. For more details,
       refer to the manual page (<command>man 1 skopeo-sync</command>).
      </para>
     </step>
    </procedure>
    <procedure>
     <title>Configure the local registry and access credentials</title>
     <step>
      <para>
       Configure the URL of the local registry:
      </para>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/registry set <replaceable>REG_HOST_URL</replaceable></screen>
     </step>
     <step>
      <para>
       Configure the user name and password to access the local registry:
      </para>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable></screen>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable></screen>
     </step>
    </procedure>
    <tip>
     <title>Registry cache</title>
     <para>
      To avoid re-syncing the local registry when new updated containers
      appear, you can configure a <emphasis>registry cache</emphasis>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configuring the path to container images</title>
    <important>
     <para>
      This section helps you configure the path to container images of the
      bootstrap cluster (deployment of the first &mon; and &mgr; pair). The
      path does not apply to container images of additional services, for
      example the monitoring stack.
     </para>
    </important>
    <tip>
     <title>Configuring HTTPS proxy</title>
     <para>
      If you need to use a proxy to communicate with the container registry
      server, perform the following configuration steps on all cluster nodes:
     </para>
     <procedure>
      <step>
       <para>
        Copy the configuration file for containers:
       </para>
<screen>&prompt.sudo;cp /usr/share/containers/containers.conf /etc/containers/containers.conf</screen>
      </step>
      <step>
       <para>
        Edit the newly-copied file and add the <option>http_proxy</option>
        setting to its <literal>[engine]</literal> section, for example:
       </para>
<screen>&prompt.user;cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]
 </screen>
      </step>
     </procedure>
    </tip>
    <para>
     &cephadm; needs to know a valid URI path to container images. Verify the
     default setting by executing
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
    <para>
     If you do not need an alternative or local registry, specify the default
     &suse; container registry:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7/ceph/ceph</screen>
    <para>
     If your deployment requires a specific path, for example, a path to a
     local registry, configure it as follows:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-inflight-encryption">
   <title>Enabling data in-flight encryption (msgr2)</title>
   <para>
    The Messenger v2 protocol (MSGR2) is &ceph;'s on-wire protocol. It provides
    a security mode that encrypts all data passing over the network,
    encapsulation of authentication payloads, and the enabling of future
    integration of new authentication modes (such as Kerberos).
   </para>
   <important>
    <para>
     msgr2 is not currently supported by Linux kernel Ceph clients, such as
     &cephfs; and &rbd;.
    </para>
   </important>
   <para>
    &ceph; daemons can bind to multiple ports, allowing both legacy &ceph;
    clients and new v2-capable clients to connect to the same cluster. By
    default, MONs now bind to the new IANA-assigned port 3300 (CE4h or 0xCE4)
    for the new v2 protocol, while also binding to the old default port 6789
    for the legacy v1 protocol.
   </para>
   <para>
    The v2 protocol (MSGR2) supports two connection modes:
   </para>
   <variablelist>
    <varlistentry>
     <term>crc mode</term>
     <listitem>
      <para>
       A strong initial authentication when the connection is established and a
       CRC32C integrity check.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>secure mode</term>
     <listitem>
      <para>
       A strong initial authentication when the connection is established and
       full encryption of all post-authentication traffic, including a
       cryptographic integrity check.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For most connections, there are options that control which modes are used:
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_cluster_mode</term>
     <listitem>
      <para>
       The connection mode (or permitted modes) used for intra-cluster
       communication between &ceph; daemons. If multiple modes are listed, the
       modes listed first are preferred.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_service_mode</term>
     <listitem>
      <para>
       A list of permitted modes for clients to use when connecting to the
       cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_client_mode</term>
     <listitem>
      <para>
       A list of connection modes, in order of preference, for clients to use
       (or allow) when talking to a &ceph; cluster.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    There are a parallel set of options that apply specifically to monitors,
    allowing administrators to set different (usually more secure) requirements
    on communication with the monitors.
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_mon_cluster_mode</term>
     <listitem>
      <para>
       The connection mode (or permitted modes) to use between monitors.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_service_mode</term>
     <listitem>
      <para>
       A list of permitted modes for clients or other &ceph; daemons to use
       when connecting to monitors.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_client_mode</term>
     <listitem>
      <para>
       A list of connection modes, in order of preference, for clients or
       non-monitor daemons to use when connecting to monitors.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    In order to enable MSGR2 encryption mode during the deployment, you need to
    add some configuration options to the &cephsalt; configuration before
    running <command>ceph-salt apply</command>.
   </para>
   <para>
    To use <literal>secure</literal> mode, run the following commands.
   </para>
   <para>
    Add the global section to <filename>ceph_conf</filename> in the &cephsalt;
    configuration tool:
   </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
   <para>
    Set the following options:
   </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
   <note>
    <para>
     Ensure <literal>secure</literal> precedes <literal>crc</literal>.
    </para>
   </note>
   <para>
    To <emphasis>force</emphasis> <literal>secure</literal> mode, run the
    following commands:
   </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
   <tip xml:id="update-inflight-encryption-settings">
    <title>Updating settings</title>
    <para>
     If you want to change any of the above settings, set the configuration
     changes in the monitor configuration store. This is achieved using the
     <command>ceph config set</command> command.
    </para>
<screen>&prompt.smaster;ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
    <para>
     For example:
    </para>
<screen>&prompt.smaster;ceph config set global ms_cluster_mode "secure crc"</screen>
    <para>
     If you want to check the current value, including default value, run the
     following command:
    </para>
<screen>&prompt.smaster;ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
    <para>
     For example, to get the <literal>ms_cluster_mode</literal> for OSD's, run:
    </para>
<screen>&prompt.smaster;ceph config get osd ms_cluster_mode</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-enable-network">
   <title>Configuring the cluster network</title>
   <para>
    Optionally, if you are running a separate cluster network, you may need to
    set the cluster network IP address followed by the subnet mask part after
    the slash sign, for example <literal>192.168.10.22/24</literal>.
   </para>
   <para>
    Run the following commands to enable <literal>cluster_network</literal>:
   </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-verify">
   <title>Verifying the cluster configuration</title>
   <para>
    The minimal cluster configuration is finished. Inspect it for obvious
    errors:
   </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path ............... [registry.suse.com/ses/7/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
   <tip>
    <title>Status of cluster configuration</title>
    <para>
     You can check if the configuration of the cluster is valid by running the
     following command:
    </para>
<screen>
&prompt.smaster;ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-export">
   <title>Exporting cluster configurations</title>
   <para>
    After you have configured the basic cluster and its configuration is valid,
    it is a good idea to export its configuration to a file:
   </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
   <warning>
    <para>
     The output of the <command>ceph-salt export</command> includes the SSH
     private key. If you are concerned about the security implications, do not
     execute this command without taking appropriate precautions.
    </para>
   </warning>
   <para>
    In case you break the cluster configuration and need to revert to a backup
    state, run:
   </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-deploy">
  <title>Updating nodes and bootstrap minimal cluster</title>

  <para>
   Before you deploy the cluster, update all software packages on all nodes:
  </para>

<screen>&prompt.smaster;ceph-salt update</screen>

  <para>
   If a node reports <literal>Reboot is needed</literal> during the update,
   important OS packages&mdash;such as the kernel&mdash;were updated to a newer
   version and you need to reboot the node to apply the changes.
  </para>

  <para>
   To reboot all nodes that require rebooting, either append the
   <option>--reboot</option> option
  </para>

<screen>&prompt.smaster;ceph-salt update --reboot</screen>

  <para>
   Or, reboot them in a separate step:
  </para>

<screen>&prompt.smaster;ceph-salt reboot</screen>

  <important>
   <para>
    The &smaster; is never rebooted by <command>ceph-salt update
    --reboot</command> or <command>ceph-salt reboot</command> commands. If the
    &smaster; needs rebooting, you need to reboot it manually.
   </para>
  </important>

  <para>
   After the nodes are updated, bootstrap the minimal cluster:
  </para>

<screen>&prompt.smaster;ceph-salt apply</screen>

  <note>
   <para>
    When bootstrapping is complete, the cluster will have one &mon; and one
    &mgr;.
   </para>
  </note>

  <para>
   The above command will open an interactive user interface that shows the
   current progress of each minion.
  </para>

  <figure>
   <title>Deployment of a minimal cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>Non-interactive mode</title>
   <para>
    If you need to apply the configuration from a script, there is also a
    non-interactive mode of deployment. This is also useful when deploying the
    cluster from a remote machine because constant updating of the progress
    information on the screen over the network may become distracting:
   </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
  </tip>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-final-steps">
  <title>Reviewing final steps</title>

  <para>
   After the <command>ceph-salt apply</command> command has completed, you
   should have one &mon; and one &mgr;. You should be able to run the
   <command>ceph status</command> command successfully on any of the minions
   that were given the <literal>admin</literal> role as <literal>root</literal>
   or the <literal>cephadm</literal> user using <literal>sudo</literal>.
  </para>

  <para>
   The next steps involve using the &cephadm; to deploy additional &mon;,
   &mgr;, OSDs, the Monitoring Stack, and Gateways.
  </para>

  <para>
   Before you continue, review your new cluster's network settings. At this
   point, the <literal>public_network</literal> setting has been populated
   based on what was entered for <literal>/cephadm_bootstrap/mon_ip</literal>
   in the <literal>ceph-salt</literal> configuration. However, this setting was
   only applied to &mon;. You can review this setting with the following
   command:
  </para>

<screen>&prompt.smaster;ceph config get mon public_network</screen>

  <para>
   This is the minimum that &ceph; requires to work, but we recommend making
   this <literal>public_network</literal> setting <literal>global</literal>,
   which means it will apply to all types of &ceph; daemons, and not only to
   MONs:
  </para>

<screen>&prompt.smaster;ceph config set global public_network "$(ceph config get mon public_network)"</screen>

  <note>
   <para>
    This step is not required. However, if you do not use this setting, the
    &ceph; OSDs and other daemons (except &mon;) will listen on <emphasis>all
    addresses</emphasis>.
   </para>
   <para>
    If you want your OSDs to communicate amongst themselves using a completely
    separate network, run the following command:
   </para>
<screen>&prompt.smaster;ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
   <para>
    Executing this command will ensure that the OSDs created in your deployment
    will use the intended cluster network from the start.
   </para>
  </note>

  <para>
   If your cluster is set to have dense nodes (greater than 62 OSDs per host),
   make sure to assign sufficient ports for &ceph; OSDs. The default range
   (6800-7300) currently allows for no more than 62 OSDs per host. For a
   cluster with dense nodes, adjust the setting
   <literal>ms_bind_port_max</literal> to a suitable value. Each OSD will
   consume eight additional ports. For example, given a host that is set to run
   96 OSDs, 768 ports will be needed. <literal>ms_bind_port_max</literal>
   should be set at least to 7568 by running the following command:
  </para>

<screen>&prompt.smaster;ceph config set osd.* ms_bind_port_max 7568</screen>

  <para>
   You will need to adjust your firewall settings accordingly for this to work.
   See <xref linkend="storage-bp-net-firewall"/> for more information.
  </para>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-disable-insecure">
  <title>Disable insecure clients</title>

  <para>
   Since &cephname; v15.2.11, a new health warning was introduced that informs
   you that insecure clients are allowed to join the cluster. This warning is
   <emphasis>on</emphasis> by default. The &dashboard; will show the cluster in
   the <literal>HEALTH_WARN</literal> status and verifying the cluster status
   on the command line informs you as follows:
  </para>

<screen>
&prompt.cephuser;ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>

  <para>
   This warning means that the &mon;s are still allowing old, unpatched clients
   to connect to the cluster. This ensures existing clients can still connect
   while the cluster is being upgraded, but warns you that there is a problem
   that needs to be addressed. When the cluster and all clients are upgraded to
   the latest version of &ceph;, disallow unpatched clients by running the
   following command:
  </para>

<screen>&prompt.cephuser;ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
 </sect1>
</chapter>
