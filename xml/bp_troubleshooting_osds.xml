<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-osds">
 <title>Troubleshooting OSDs</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Before troubleshooting your OSDs, check your monitors and network first.
  If you execute <command>ceph health</command> or <command>ceph -s</command>
  on the command line and &ceph; returns a health status, it means that the monitors
  have a quorum. If you do not have a monitor quorum or if there are errors with the monitor status,
  see <xref linkend="bp-troubleshooting-monitors"/>. Check your networks to ensure they are
  running properly, because networks may have a significant impact on OSD
  operation and performance.
 </para>
 <sect1 xml:id="bp-troubleshooting-osd-data">
   <title>Obtaining Data About OSDs</title>
   <para>
     To begin troubleshooting the OSDs, obtain any information including the
     information collected from <xref linkend="op-mon-osd-pg"/>.
   </para>
  <sect2 xml:id ="troubleshooting-ceph-logs">
    <title>&ceph; Logs</title>
    <para>
      If you have not changed the default path, you can find &ceph; log
      files at <filename>/var/log/ceph</filename>:
    </para>
<screen>ls /var/log/ceph</screen>
    <para>
      If you do not get enough detail, change your logging level. See
      <xref linkend="bp-troubleshooting-logging"/> for more information.
    </para>
  </sect2>
  <sect2 xml:id ="troubleshooting-admin-socket">
    <title>Admin Socket</title>
    <para>
      Use the admin socket tool to retrieve runtime information. For
      details, list the sockets for your &ceph; processes:
    </para>
<screen>ls /var/run/ceph</screen>
    <para>
      Execute the following:
    </para>
<screen>ceph daemon <replaceable>DAEMON-NAME</replaceable> help</screen>
    <para>
      Alternatively, specify a <literal>SOCKET-FILE</literal>:
    </para>
<screen>ceph daemon <replaceable>SOCKET-FILE</replaceable> help</screen>
    <para>
      The admin socket, among other things, allows you to:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          List your configuration at runtime
        </para>
      </listitem>
      <listitem>
        <para>
          Dump historic operations
        </para>
      </listitem>
      <listitem>
        <para>
          Dump the operation priority queue state
        </para>
      </listitem>
      <listitem>
        <para>
          Dump operations in flight
        </para>
      </listitem>
      <listitem>
        <para>
          Dump perfcounters
        </para>
      </listitem>
    </itemizedlist>
  </sect2>
  <sect2 xml:id ="troubleshooting-display-freespace">
    <title>Display Freespace</title>
    <para>
      Filesystem issues may arise. To display your file system’s
      free space, execute <command>df</command>.
    </para>
<screen>df -h</screen>
    <para>
      Execute <command>df --help</command> for additional usage.
    </para>
  </sect2>
  <sect2 xml:id ="troubleshooting-admin-statistics">
    <title>I/O Statistics</title>
    <para>
      Use iostat to identify I/O-related issues.
    </para>
<screen>iostat -x</screen>
  </sect2>
  <sect2 xml:id ="troubleshooting-admin-diagnostic">
    <title>Diagnostic Messages</title>
    <para>
      To retrieve diagnostic messages, use <command>dmesg</command> with <option>less</option>,
      <option>more</option>, <option>grep</option> or <option>tail</option>. For example:
    </para>
<screen>dmesg | grep scsi</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-rebalancing">
   <title>Stopping Without Rebalancing</title>
   <para>
     Periodically you may need to perform maintenance on a subset of your
     cluster or resolve a problem that affects a failure domain.
     If you do not want CRUSH to automatically rebalance
     the cluster as you stop OSDs for maintenance, set the cluster to
     <option>noout</option> first:
   </para>
<screen>ceph osd set noout</screen>
   <para>
     Once the cluster is set to <option>noout</option>, you can begin stopping
     the OSDs within the failure domain that requires maintenance work:
   </para>
<screen>stop ceph-osd id={num}</screen>
   <note>
     <para>
       Placement groups within the OSDs you stop will become degraded
       while you are addressing issues with within the failure domain.
     </para>
   </note>
   <para>
     Once you have completed your maintenance, restart the OSDs:
   </para>
<screen>start ceph-osd id={num}</screen>
   <para>
     Finally, unset the cluster from <option>noout</option>:
   </para>
<screen>ceph osd unset noout</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-not-running">
  <title>OSDs Not Running</title>
  <para>
    Under normal circumstances, restarting the <literal>ceph-osd</literal>
    daemon will allow it to rejoin the cluster and recover. If this is not
    true, continue on to the next section.
  </para>
  <sect2 xml:id="troubleshooting-osd-start">
    <title>An OSD Will Not Start</title>
    <para>
      If you start your cluster and an OSD will not start, check the following:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          <emphasis>Configuration File</emphasis>: If you were not able to get
          OSDs running from a new installation, check your configuration file
          to ensure it conforms. For example, host not hostname.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis>Check Paths</emphasis>: Check the paths in the configuration, and the actual
          paths themselves for data and journals. If you separate the OSD
          data from the journal data and there are errors in the
          configuration file or in the actual mounts, you may have trouble
          starting OSDs. If you want to store the journal on a block device,
          partition the journal disk and assign one partition per OSD.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis>Check Max Threadcount</emphasis>: If you have a node with a
          lot of OSDs, you may be hitting the default maximum number of threads,
          especially during recovery. You can increase the
          number of threads using <command>sysctl</command> to see if increasing the maximum
          number of threads to the maximum possible number of threads allowed
          helps. For example:
        </para>
<screen>sysctl -w kernel.pid_max=4194303</screen>
        <para>
          If increasing the maximum thread count resolves the issue, you can
          make it permanent by including a <literal>kernel.pid_max</literal> setting in
          the <filename>/etc/sysctl.conf</filename> file. For example:
        </para>
<screen>kernel.pid_max = 4194303</screen>
      </listitem>
      <listitem>
        <para>
          <emphasis>Kernel Version</emphasis>: Identify the kernel version and distribution you
          are using. &ceph; uses some third party tools by default, which may
          be buggy or may conflict with certain distributions or kernel
          versions.
        </para>
      </listitem>
      <listitem>
        <para>
          <emphasis>Segment Fault</emphasis>: If there is a segment fault, turn
          your logging up (if it is not already), and try again.
        </para>
      </listitem>
    </itemizedlist>
  </sect2>
  <sect2 xml:id="troubleshooting-osd-fail">
    <title>An OSD Failed</title>
    <para>
      When a <literal>ceph-osd</literal> process dies, the monitor learns about the
      failure from surviving <literal>ceph-osd</literal> daemons and reports it
      via the <command>ceph health</command> command:
    </para>
<screen>
ceph health
HEALTH_WARN 1/3 in osds are down
</screen>
    <para>
      A warning displays whenever there are <literal>ceph-osd</literal>
      processes that are marked <option>in</option> and <option>down</option>.
      Identify which <literal>ceph-osd</literal>s are down with:
    </para>
<screen>
ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
</screen>
    <para>
      If there is a disk failure or other fault preventing <literal>ceph-osd</literal> from
      functioning or restarting, an error message should be present in its
      log file in <filename>/var/log/ceph</filename>.
      If the daemon stopped because of a heartbeat failure, the underlying
      kernel file system may be unresponsive. Check <command>dmesg</command> output for disk
      or other kernel errors.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-osd-space">
    <title>No Free Drive Space</title>
    <para>
      &ceph; prevents writing to a full OSD so that you do not lose
      data. In an operational cluster, you should receive a warning when
      your cluster is getting near its full ratio. The <literal>mon osd full ratio</literal>
      defaults to 0.95, or 95% of capacity before it stops clients from
      writing data. The <literal>mon osd backfillfull ratio</literal> defaults to 0.90, or
      90% of capacity when it blocks backfills from starting. The OSD
      nearfull ratio defaults to 0.85, or 85% of capacity when it generates
      a health warning.
      Change this using the following command:
    </para>
<screen>ceph osd set-nearfull-ratio &lt;float[0.0-1.0]></screen>
    <para>
      Full cluster issues usually arise when testing how &ceph; handles an
      OSD failure on a small cluster. When one node has a high percentage
      of the cluster’s data, the cluster can easily eclipse its nearfull
      and full ratio immediately. If you are testing how &ceph; reacts to
      OSD failures on a small cluster, you should leave ample free disk
      space and consider temporarily lowering the OSD <option>full ratio</option>,
      OSD <option>backfillfull</option> ratio and OSD <option>nearfull</option>
      ratio using these commands:
    </para>
<screen>
  ceph osd set-nearfull-ratio &lt;float[0.0-1.0]>
  ceph osd set-full-ratio &lt;float[0.0-1.0]>
  ceph osd set-backfillfull-ratio &lt;float[0.0-1.0]>
</screen>
    <para>
      Full <literal>ceph-osds</literal> will be reported by <command>ceph health</command>:
    </para>
<screen>
  ceph health
  HEALTH_WARN 1 nearfull osd(s)
</screen>
   <para>
     Or:
   </para>
<screen>
  ceph health detail
  HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
  osd.3 is full at 97%
  osd.4 is backfill full at 91%
  osd.2 is near full at 87%
</screen>
   <para>
      We recommend adding new <literal>ceph-osds</literal> to deal with a full cluster,
      allowing the cluster to redistribute data to the newly available storage.
      If you cannot start an OSD because it is full, you may delete some
      data by deleting some placement group directories in the full OSD.
   </para>
   <important>
     <para>
        If you choose to delete a placement group directory on a full OSD,
        do not delete the same placement group directory on another full
        OSD, or you may loose data. You must maintain at least one copy of
        your data on at least one OSD.
     </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-osd-slow">
   <title>OSDs Are Slow Or Unresponsive</title>
   <para>
     A commonly recurring issue involves slow or unresponsive OSDs.
     Ensure that you have eliminated other troubleshooting possibilities
     before delving into OSD performance issues. For example, ensure
     that your network(s) is working properly and your OSDs are running.
     Check to see if OSDs are throttling recovery traffic.
   </para>
  <sect2 xml:id="troubleshooting-drive-config">
    <title>Drive Configuration</title>
    <para>
      A storage drive should only support one OSD. Sequential read and
      sequential write throughput can bottleneck if other processes
      share the drive, including journals, operating systems,
      monitors, other OSDs and non-&ceph; processes.
    </para>
    <para>
      &ceph; acknowledges writes after journaling, so fast SSDs are an
      attractive option to accelerate the response time–particularly when
      using the XFS or ext4 file systems. By contrast, the btrfs file
      system can write and journal simultaneously.
    </para>
    <note>
      <para>
         Partitioning a drive does not change its total throughput or
         sequential read/write limits. Running a journal in a separate
         partition may help, but we recommend a separate physical drive.
      </para>
    </note>
  </sect2>
  <sect2 xml:id="troubleshooting-bad-sector">
    <title>Bad Sectors and Fragmented Disks</title>
    <para>
      Check your disks for bad sectors and fragmentation. This can cause
      total throughput to drop substantially.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-monitor-osd">
    <title>Co-Resident Monitors and OSDs</title>
    <para>
      Monitors are generally light-weight processes but often perform
      fsync(). This can interfere with other workloads, particularly if
      monitors run on the same drive as the OSDs. Additionally, if
      you run monitors on the same host as the OSDs, you may incur
      performance issues related to:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Running an older kernel (pre-3.0)
        </para>
      </listitem>
      <listitem>
        <para>
          Running a kernel with no syncfs(2) syscall.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      In these cases, multiple OSDs running on the same host can drag
      each other down by doing lots of commits. That often leads
      to the bursty writes.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-coresident-process">
    <title>Co-Resident Processes</title>
    <para>
      Spinning up co-resident processes such as a cloud-based solution,
      virtual machines and other applications that write data to &ceph;
      while operating on the same hardware as OSDs can introduce
      significant OSD latency. We recommend optimizing a host for use
      with &ceph; and using other hosts for other processes. The practice
      of separating &ceph; operations from other applications may help
      improve performance and may streamline troubleshooting and maintenance.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-log-levels">
    <title>Logging Levels</title>
    <para>
      If you turned logging levels up to track an issue and then forgot to
      turn logging levels back down, the OSD may be putting a lot of
      logs onto the disk. If you intend to keep logging levels high, you
      may consider mounting a drive to the default path for logging.
      For example, <filename>/var/log/ceph/$cluster-$name.log</filename>.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-recovery">
    <title>Recovery Throttling</title>
    <para>
      Depending upon your configuration, &ceph; may reduce recovery rates
      to maintain performance or it may increase recovery rates to the point
      that recovery impacts OSD performance. Check to see if the OSD is recovering.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-kernel-version">
    <title>Kernel Version</title>
    <para>
      Check the kernel version you are running. Older kernels may not
      receive new backports that &ceph; depends upon for better performance.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-kernal-issues">
    <title>Kernel Issues with Syncfs</title>
    <para>
      Try running one OSD per host to see if performance improves. Old
      kernels might not have a recent enough version of glibc to support
      <command>syncfs(2)</command>.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-filesystem-issues">
    <title>Filesystem Issues</title>
    <para>
      Currently, we recommend deploying clusters with XFS.
    </para>
<!--    <para>
      We recommend against using btrfs or ext4. The btrfs file system has
      many attractive features, but bugs in the file system may lead to
      performance issues and spurious ENOSPC errors. We do not
      recommend ext4 because xattr size limitations break our
      support for long object names (needed for RGW).
    </para> -->
  </sect2>
  <sect2 xml:id="troubleshooting-ram">
    <title>Insufficient RAM</title>
    <para>
      We recommend 1.5 GB of RAM per TB of raw OSD capacity for each
      Object Storage Node. You may notice that during
      normal operations, the OSD only uses a fraction of that amount.
      Unused RAM makes it tempting to use the excess RAM for co-resident
      applications, VMs and so forth.
      However, when OSDs go into recovery mode, their memory utilization
      spikes. If there is no RAM available, the OSD performance will
      slow considerably.
    </para>
  </sect2>
  <sect2 xml:id="troubleshooting-requests">
    <title>Old or Slow Requests</title>
    <para>
      If a <literal>ceph-osd</literal> daemon is slow to respond to a request,
      log messages will generate complaining about requests that are
      taking too long. The warning threshold defaults to 30 seconds,
      and is configurable via the <option>osd op complaint time</option> option.
      When this happens, the cluster log receives messages.
      Legacy versions of &ceph; complain about old requests:
    </para>
<screen>osd.0 192.168.106.220:6800/18813 312 : [WRN] old request \
  osd_op(client.5099.0:790 fatty_26485_object789 [write 0~4096] 2.5e54f643) \
  v4 received at 2012-03-06 15:42:56.054801 currently waiting for sub ops
</screen>
    <para>
      Newer versions of &ceph; complain about <literal>slow requests</literal>:
    </para>
<screen>
{date} {osd.num} [WRN] 1 slow requests, 1 included below; oldest blocked for > 30.005692 secs
{date} {osd.num}  [WRN] slow request 30.005692 seconds old, received at \
{date-time}: osd_op(client.4240.0:8 benchmark_data_ceph-1_39426_object7 \
[write 0~4194304] 0.69848840) v4 currently waiting for subops from [610]
</screen>
    <para>
      Possible causes include:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          A bad drive (check <command>dmesg</command> output)
        </para>
      </listitem>
      <listitem>
        <para>
          A bug in the kernel file system (check <command>dmesg</command> output)
        </para>
      </listitem>
      <listitem>
        <para>
          An overloaded cluster (check system load, iostat, etc.)
        </para>
      </listitem>
      <listitem>
        <para>
          A bug in the <literal>ceph-osd</literal> daemon.
        </para>
      </listitem>
    </itemizedlist>
    <para>
      Possible solutions:
    </para>
    <itemizedlist>
      <listitem>
        <para>
          Remove VMs from &ceph; hosts
        </para>
      </listitem>
      <listitem>
        <para>
          Upgrade kernel
        </para>
      </listitem>
      <listitem>
        <para>
          Upgrade &ceph;
        </para>
      </listitem>
      <listitem>
        <para>
          Restart OSDs
        </para>
      </listitem>
    </itemizedlist>
  </sect2>
  <sect2 xml:id="troubleshooting-slow-requests">
    <title>Debugging Slow Requests</title>
    <para>
      If you run <command>ceph daemon osd.&lt;id> dump_historic_ops</command>
      or <command>ceph daemon osd.&lt;id> dump_ops_in_flight</command>, you
      see a set of operations and a list of events each operation went
      through. These are briefly described below.
    </para>
    <para>
      Events from the Messenger layer:
    </para>
    <variablelist>
    <varlistentry>
      <term><literal>header_read</literal></term>
      <listitem>
        <para>
          When the messenger first started reading the message off the wire.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><literal>throttled</literal></term>
      <listitem>
        <para>
          When the messenger tried to acquire memory throttle space to
          read the message into memory.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><literal>all_read</literal></term>
      <listitem>
        <para>
          When the messenger finished reading the message off the wire.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><literal>dispatched</literal></term>
      <listitem>
        <para>
          When the messenger gave the message to the OSD.
        </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term><literal>initiated</literal></term>
      <listitem>
        <para>
          This is identical to <literal>header_read</literal>. The existence
          of both is a historical oddity.
        </para>
      </listitem>
    </varlistentry>
   </variablelist>
   <para>
     Events from the OSD as it prepares operations:
   </para>
   <variablelist>
     <varlistentry>
       <term><literal>queued_for_pg</literal></term>
       <listitem>
         <para>
           The op has been put into the queue for processing by its PG.
         </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term><literal>reached_pg</literal></term>
       <listitem>
         <para>
           The PG has started doing the op.
         </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term><literal>waiting for \*</literal></term>
       <listitem>
         <para>
           The op is waiting for some other work to complete before it can
           proceed (e.g. a new OSDMap; for its object target to scrub;
           for the PG to finish peering; all as specified in the message).
         </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term><literal>started</literal></term>
       <listitem>
         <para>
           The op has been accepted as something the OSD should do
           and is now being performed.
         </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term><literal>waiting for subops from</literal></term>
       <listitem>
         <para>
           The op has been sent to replica OSDs.
         </para>
       </listitem>
     </varlistentry>
    </variablelist>
    <para>
      Events from the FileStore:
    </para>
    <variablelist>
      <varlistentry>
        <term><literal>commit_queued_for_journal_write:</literal></term>
        <listitem>
          <para>
            The op has been given to the FileStore.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>write_thread_in_journal_buffer</literal></term>
        <listitem>
          <para>
            The op is in the journal’s buffer and waiting to be
            persisted (as the next disk write).
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>journaled_completion_queued</literal></term>
        <listitem>
          <para>
            The op was journaled to disk and its callback queued for invocation.
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      Events from the OSD after stuff has been given to local disk:
    </para>
    <variablelist>
      <varlistentry>
        <term><literal>op_commit</literal></term>
        <listitem>
          <para>
            The op has been committed by the primary OSD.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>op_applied</literal></term>
        <listitem>
          <para>
            The op has been <literal>write()’en</literal> to the backing FS
            on the primary.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>sub_op_applied: op_applied</literal></term>
        <listitem>
          <para>
            For a replica’s “subop”.
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>sub_op_committed: op_commit</literal></term>
        <listitem>
          <para>
            For a replica’s sub-op (only for EC pools).
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>sub_op_commit_rec/sub_op_apply_rec from &lt;X></literal></term>
        <listitem>
          <para>
            The primary marks this when it hears about the above, but
            for a particular replica (i.e. &lt;X>).
          </para>
        </listitem>
      </varlistentry>
      <varlistentry>
        <term><literal>commit_sent</literal></term>
        <listitem>
          <para>
            We sent a reply back to the client (or primary OSD, for sub ops).
          </para>
        </listitem>
      </varlistentry>
    </variablelist>
    <para>
      Many of these events are seemingly redundant, but cross important
      boundaries in the internal code (such as passing data
      across locks into new threads).
    </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-recover-osdweight">
  <title>OSD Weight is 0</title>
  <para>
   When OSD starts, it is assigned a weight. The higher the weight, the bigger
   the chance that the cluster writes data to the OSD. The weight is either
   specified in a cluster &crushmap;, or calculated by the OSDs' start-up
   script.
  </para>
  <para>
   In some cases, the calculated value for OSDs' weight may be rounded down to
   zero. It means that the OSD is not scheduled to store data, and no data is
   written to it. The reason is usually that the disk is too small (smaller
   than 15GB) and should be replaced with a bigger one.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-osddown">
  <title>OSD is Down</title>
  <para>
   OSD daemon is either running, or stopped/down. There are 3 general reasons
   why an OSD is down:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Hard disk failure.
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD crashed.
    </para>
   </listitem>
   <listitem>
    <para>
     The server crashed.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   You can see the detailed status of OSDs by running
  </para>
<screen>&prompt.cephuser;ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down
</screen>
  <para>
   The example listing shows that the <literal>osd.2</literal> is down. Then
   you may check if the disk where the OSD is located is mounted:
  </para>
<screen>&prompt.root;lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2
</screen>
  <para>
   You can track the reason why the OSD is down by inspecting its log file
   <filename>/var/log/ceph/ceph-osd.2.log</filename>. After you find and fix
   the reason why the OSD is not running, start it with
  </para>
<screen>&prompt.root;systemctl start ceph-<replaceable>FSID</replaceable>@<replaceable>osd.2</replaceable></screen>
  <para>
   Do not forget to replace <literal>2</literal> with the actual number of your
   stopped OSD.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-performance-slowosd">
  <title>Finding Slow OSDs</title>
  <para>
   When tuning the cluster performance, it is very important to identify slow
   storage/OSDs within the cluster. The reason is that if the data is written
   to the slow(est) disk, the complete write operation slows down as it always
   waits until it is finished on all the related disks.
  </para>
  <para>
   It is not trivial to locate the storage bottleneck. You need to examine each
   and every OSD to find out the ones slowing down the write process. To do a
   benchmark on a single OSD, run:
  </para>
<screen role="ceph_tell_osd_bench"><command>ceph tell</command> osd.<replaceable>OSD_ID_NUMBER</replaceable> bench</screen>
  <para>
   For example:
  </para>
<screen>&prompt.cephuser;ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</screen>
  <para>
   Then you need to run this command on each OSD and compare the
   <literal>bytes_per_sec</literal> value to get the slow(est) OSDs.
  </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-flapping-osds">
   <title>Flapping OSDs</title>
   <para>
     We recommend using both a public (front-end) network and a cluster
     (back-end) network so that you can better meet the capacity requirements
     of object replication. Another advantage is that you can run a cluster
     network such that it is not connected to the internet, thereby
     preventing some denial of service attacks. When OSDs peer and check
     heartbeats, they use the cluster (back-end) network when it’s available.
   </para>
   <para>
     However, if the cluster (back-end) network fails or develops significant
     latency while the public (front-end) network operates optimally,
     OSDs currently do not handle this situation well. What happens is
     that OSDs mark each other down on the monitor, while marking themselves
     up. This is called <emphasis>flapping</emphasis>.
   </para>
   <para>
     If something is causing OSDs to flap (repeatedly getting marked down
     and then up again), you can force the monitors to stop the flapping with:
   </para>
<screen>
  ceph osd set noup      # prevent OSDs from getting marked up
  ceph osd set nodown    # prevent OSDs from getting marked down
</screen>
   <para>
     These flags are recorded in the osdmap structure:
   </para>
<screen>
ceph osd dump | grep flags
flags no-up,no-down
</screen>
   <para>
     You can clear the flags with:
   </para>
<screen>
  ceph osd unset noup
  ceph osd unset nodown
</screen>
   <para>
     Two other flags are supported, <option>noin</option> and <option>noout</option>,
     which prevent booting OSDs from being marked in (allocated data) or
     protect OSDs from eventually being marked out (regardless of what the
     current value for <command>mon osd down out</command> interval is).
   </para>
   <note>
     <para>
       <option>noup</option>, <option>noout</option>, and <option>nodown</option> are temporary in the sense that once
       the flags are cleared, the action they were blocking should
       occur shortly after. The <option>noin</option> flag, on the other hand, prevents
       OSDs from being marked <option>in</option> on boot, and any daemons that started
       while the flag was set will remain that way.
     </para>
   </note>
 </sect1>
 </chapter>
