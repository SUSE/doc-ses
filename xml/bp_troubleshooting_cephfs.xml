<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-cephfs">
 <title>Troubleshooting &ceph; File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="bp-troubleshooting-slow-ops">
   <title>Slow or Stuck Operations</title>
   <para>
     If you are experiencing apparent hung operations, the first task is
     to identify where the problem is occurring: in the client, the MDS,
     or the network connecting them. Start by looking to see if either
     side has stuck operations and narrow it down from there.
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-rados-health">
   <title>RADOS Health</title>
   <para>
     If part of the CephFS metadata or data pools is unavailable and
     CephFS is not responding, it is probably because RADOS itself is
     unhealthy. <!-- See RADOS troubleshooting -->
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-mds">
   <title>The MDS</title>
   <para>
     If an operation is hung inside the MDS, it will eventually show
     up in <command>ceph health</command>, identifying “slow requests are blocked”. It
     may also identify clients as “failing to respond” or misbehaving in
     other ways. If the MDS identifies specific clients as misbehaving,
     you should investigate why they are doing so. Generally, it will
     be the result of:
     <itemizedlist>
       <listitem>
         <para>
           Overloading the system
         </para>
       </listitem>
       <listitem>
         <para>
            Running an older (misbehaving) client
          </para>
        </listitem>
        <listitem>
          <para>
            Underlying RADOS issues
          </para>
        </listitem>
      </itemizedlist>
   </para>
   <sect2 xml:id="mds-slow-requests">
     <title>MDS Slow Requests</title>
     <para>
       You can list current operations via the admin socket by running
       the following command from the MDS host:
     </para>
<screen>
ceph daemon mds.<replaceable>NAME</replaceable> dump_ops_in_flight
</screen>
     <para>
        Identify the stuck commands and examine why they are stuck.
        Usually the last event will have been an attempt to gather
        locks, or sending the operation off to the MDS log. If it is
        waiting on the OSDs, fix them. If operations are stuck on a
        specific inode, you probably have a client holding caps which
        prevent others from using it. This can be because the client is trying
        to flush out dirty data or because you have encountered a bug
        in CephFS’ distributed file lock code (the file “capabilities”
        [“caps”] system).
     </para>
     <para>
       If it is a result of a bug in the capabilities code, restarting
       the MDS is likely to resolve the problem.
     </para>
     <para>
       If there are no slow requests reported on the MDS, and it is not
       reporting that clients are misbehaving, either the client has
       a problem or its requests are not reaching the MDS.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephfuse-debugging">
   <title>Ceph-Fuse Debugging</title>
   <para>
     <command>ceph-fuse</command> also supports <option>dump_ops_in_flight</option>.
     See if it has any and where they are stuck.
   </para>
   <sect2 xml:id="debug-output">
     <title>Debug Output</title>
     <para>
       To get more debugging information from <command>ceph-fuse</command>, try running
       in the foreground with logging to the console (<option>-d</option>) and enabling
       client debug (<option>--debug-client=20</option>), enabling prints for each
       message sent (<option>--debug-ms=1</option>).
     </para>
     <para>
       If you suspect a potential monitor issue, enable monitor
       debugging as well (<option>--debug-monc=20</option>).
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-kernelmount-debugging">
   <title>Kernel Mount Debugging</title>
   <sect2 xml:id="kernel-mount-slow-requests">
     <title>Slow Requests</title>
     <para>
       Unfortunately, the kernel client does not support the admin
       socket, but it has similar (if limited) interfaces if your
       kernel has <option>debugfs</option> enabled. There will be a folder in
       <filename>sys/kernel/debug/ceph/</filename>, and that folder
       contains a variety of files that output interesting output
       when you cat them. These files are described below; the most
       interesting when debugging slow requests are probably the <filename>mdsc</filename>
       and <filename>osdc</filename> files.
     </para>
     <variablelist>
       <varlistentry>
         <term>bdi</term>
         <listitem>
           <para>
             BDI info about the &ceph; system (blocks dirtied, written, etc)
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>caps</term>
         <listitem>
           <para>
             Counts of file caps structures in-memory and used
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>client_options</term>
         <listitem>
           <para>
              Dumps the options provided to the CephFS mount
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>dentry_Iru</term>
         <listitem>
           <para>
             Dumps the CephFS dentries currently in-memory
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>mdsc</term>
         <listitem>
           <para>
             Dumps current requests to the MDS
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>mdsmap</term>
         <listitem>
           <para>
             Dumps the current MDSMap epoch and MDSes
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>mds_sessions</term>
         <listitem>
           <para>
             Dumps the current sessions to MDSes
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>monc</term>
         <listitem>
           <para>
             Dumps the current maps from the monitor, and any subscriptions held
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>monmap</term>
         <listitem>
           <para>
             Dumps the current monitor map epoch and monitors
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>osdc</term>
         <listitem>
           <para>
             Dumps the current ops in-flight to OSDs (ie, file data IO)
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>osdmap</term>
         <listitem>
           <para>
             Dumps the current OSDMap epoch, pools, and OSDs
           </para>
         </listitem>
       </varlistentry>
     </variablelist>
   </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-disconnected-remounted-fs">
   <title>Disconnected and Remounted File System</title>
   <para>
     Because CephFS has a consistent cache, if your network connection
     is disrupted for a long enough time the client will be forcibly
     disconnected from the system. At this point, the kernel client
     is in a bind: it cannot safely write back dirty data, and many
     applications do not handle IO errors correctly on <literal>close()</literal>.
     At the moment, the kernel client will remount the FS, but
     outstanding filesystem IO may or may not be satisfied. In these
     cases, you may need to reboot your client system.
   </para>
   <para>
     You can identify you are in this situation if <filename>dmesg/kern.log</filename>
     report something like:
   </para>
<screen>
  Jul 20 08:14:38 teuthology kernel: [3677601.123718] ceph: mds0 closed our session
  Jul 20 08:14:38 teuthology kernel: [3677601.128019] ceph: mds0 reconnect start
  Jul 20 08:14:39 teuthology kernel: [3677602.093378] ceph: mds0 reconnect denied
  Jul 20 08:14:39 teuthology kernel: [3677602.098525] ceph:  dropping dirty+flushing Fw state for ffff8802dc150518 1099935956631
  Jul 20 08:14:39 teuthology kernel: [3677602.107145] ceph:  dropping dirty+flushing Fw state for ffff8801008e8518 1099935946707
  Jul 20 08:14:39 teuthology kernel: [3677602.196747] libceph: mds0 172.21.5.114:6812 socket closed (con state OPEN)
  Jul 20 08:14:40 teuthology kernel: [3677603.126214] libceph: mds0 172.21.5.114:6812 connection reset
  Jul 20 08:14:40 teuthology kernel: [3677603.132176] libceph: reset on mds0
</screen>
   <para>
     This is an area of ongoing work to improve the behavior. Kernels will
     soon be reliably issuing error codes to in-progress IO, although
     your application(s) may not deal with them well. In the longer-term,
     we hope to allow reconnect and reclaim of data in cases where
     it will not violate POSIX semantics.
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-mounting">
   <title>Mounting</title>
   <sect2 xml:id="mount-5-error">
     <title>Mount Five Error</title>
     <para>
       A mount 5 error typically occurs if a MDS server is laggy
       or if it crashed. Ensure at least one MDS is up and running,
       and the cluster is <literal>active + healthy</literal>.
     </para>
   </sect2>
   <sect2 xml:id="mount-12-error">
     <title>Mount 12 Error</title>
     <para>
       A mount 12 error with <literal>cannot allocate memory</literal> usually
       occurs if you have a version mismatch between the &ceph; Client version
       and the &ceph; Storage Cluster version. Check the versions using:
     </para>
<screen>
ceph -v
</screen>
     <para>
       If the &ceph; Client is behind the &ceph; cluster, try to upgrade it:
     </para>
<screen>
sudo zypper up
sudo zypper in ceph-common
</screen>
     <para>
       You may need to uninstall, autoclean and autoremove <command>ceph-common</command>
       and then reinstall it so that you have the latest version.
     </para>
   </sect2>
 </sect1>
</chapter>
