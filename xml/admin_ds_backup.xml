<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-deployment-backup">
 <title>Backup and restore</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter explains which parts of the &ceph; cluster you should back up in
  order to be able to restore its functionality.
 </para>
 <sect1 xml:id="backrest-ceph">
  <title>Back Up Cluster Configuration and Data</title>

  <sect2 xml:id="backrest-ceph-cephsalt">
   <title>Back up &cephsalt; configuration</title>
   <para>
    Export the cluster configuration. Find more information in
    <xref linkend="deploy-cephadm-configure-export"/>.
   </para>
  </sect2>

  <sect2 xml:id="backup-ceph">
   <title>Back up &ceph; configuration</title>
   <para>
    Back up the <filename>/etc/ceph</filename> directory. It contains crucial
    cluster configuration. For example, you will need a backup of
    <filename>/etc/ceph</filename> when you need to replace the &adm;.
   </para>
  </sect2>

  <sect2 xml:id="sec-deployment-backup-salt">
   <title>Back up &salt; configuration</title>
   <para>
    You need to back up the <filename>/etc/salt/</filename> directory. It
    contains the &salt; configuration files, for example the &smaster; key and
    accepted client keys.
   </para>
   <para>
    The &salt; files are not strictly required for backing up the &adm;, but
    make redeploying the &salt; cluster easier. If there is no backup of these
    files, the &salt; minions need to be registered again at the new &adm;.
   </para>
   <note>
    <title>Security of the Salt Master Private Key</title>
    <para>
     Make sure that the backup of the &smaster; private key is stored in a safe
     location. The &smaster; key can be used to manipulate all cluster nodes.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="backup-config-files">
   <title>Back up custom configurations</title>
   <itemizedlist>
    <listitem>
     <para>
      &prometheus; data and customization.
     </para>
    </listitem>
    <listitem>
     <para>
      &grafana; customization.
     </para>
    </listitem>
    <listitem>
     <para>
      Manual changes to the &iscsi; configuration.
     </para>
    </listitem>
    <listitem>
     <para>
      &ceph; keys.
     </para>
    </listitem>
    <listitem>
     <para>
      &crushmap; and CRUSH rules. Save the decompiled &crushmap; including
      CRUSH rules into <filename>crushmap-backup.txt</filename> by running the
      following command:
     </para>
<screen>&prompt.cephuser;ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</screen>
    </listitem>
    <listitem>
     <para>
      &sgw; configuration. If you are using a single gateway, backup
      <filename>/etc/samba/smb.conf</filename>. If you are using a HA setup,
      also backup the CTDB and Pacemaker configuration files. Refer to
      <xref linkend="cha-ses-cifs"/> for details on what configuration is used
      by &sgw;s.
     </para>
    </listitem>
    <listitem>
     <para>
      &ganesha; configuration. Only needed when using a HA setup. Refer to
      <xref linkend="cha-ceph-nfsganesha"/> for details on what configuration
      is used by &ganesha;.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="restore-ceph">
  <title>Restoring a &ceph; node</title>

  <para>
   The procedure to recover a node from backup is to reinstall the node,
   replace its configuration files, and then re-orchestrate the cluster so that
   the replacement node is re-added.
  </para>

  <para>
   If you need to redeploy the &adm;, refer to
   <xref
    linkend="moving-saltmaster"/>.
  </para>

  <para>
   For minions, it is usually easier to simply rebuild and redeploy.
  </para>

  <orderedlist>
   <listitem>
    <para>
     Re-install the node. Find more information in <xref linkend="deploy-os"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Install &salt; Find more information in <xref linkend="deploy-salt"/>
    </para>
   </listitem>
   <listitem>
    <para>
     After restoring the <filename>/etc/salt</filename> directory from a
     backup, enable and restart applicable &salt; services, for example:
    </para>
<screen>
&prompt.smaster;<command>systemctl</command> enable salt-master
&prompt.smaster;<command>systemctl</command> start salt-master
&prompt.smaster;<command>systemctl</command> enable salt-minion
&prompt.smaster;<command>systemctl</command> start salt-minion
</screen>
   </listitem>
   <listitem>
    <para>
     Remove the public master key for the old &smaster; node from all the
     minions.
    </para>
<screen>
&prompt.smaster;<command>rm</command> /etc/salt/pki/minion/minion_master.pub
&prompt.smaster;<command>systemctl</command> restart salt-minion
</screen>
   </listitem>
   <listitem>
    <para>
     Restore anything that was local to the &adm;.
    </para>
   </listitem>
   <listitem>
    <para>
     Import the cluster configuration from the previously exported JSON file.
     Refer to <xref linkend="deploy-cephadm-configure-export"/> for more
     details.
    </para>
   </listitem>
   <listitem>
    <para>
     Apply the imported cluster configuration:
    </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   </listitem>
  </orderedlist>
 </sect1>
</chapter>
