<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-deployment-backup">
 <title>Backup and Restore</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter explains which parts of the &ceph; cluster you should back up in
  order to be able to restore its functionality.
 </para>
 <sect1 xml:id="backrest-ceph">
  <title>Backing Up Cluster Configuration and Data</title>
 <sect2 xml:id="backup-ceph">
  <title>Back Up &ceph; Configuration</title>

  <para>
   Back up the <filename>/etc/ceph</filename> directory. It contains crucial
   cluster configuration. You will need the backup of
   <filename>/etc/ceph</filename> for example when you need to replace the
   &adm;.
  </para>
 </sect2>
 <sect2 xml:id="sec-deployment-backup-salt">
  <title>Back Up &salt; Configuration</title>

  <para>
   You need to back up the <filename>/etc/salt/</filename> directory. It
   contains the &salt; configuration files, for example the &smaster; key and
   accepted client keys.
  </para>

  <para>
   The &salt; files are not strictly required for backing up the &adm;, but
   make redeploying the &salt; cluster easier. If there is no backup of these
   files, the &salt; minions need to be registered again at the new &adm;.
  </para>

  <note>
   <title>Security of the Salt Master Private Key</title>
   <para>
    Make sure that the backup of the &smaster; private key is stored in a safe
    location. The &smaster; key can be used to manipulate all cluster nodes.
   </para>
  </note>

  <para>
   After restoring the <filename>/etc/salt</filename> directory from a backup,
   restart the &salt; services:
  </para>

<screen>&prompt.smaster;<command>systemctl</command> restart salt-master
&prompt.smaster;<command>systemctl</command> restart salt-minion</screen>
 </sect2>
 <sect2 xml:id="sec-deployment-backup-deepsea">
  <title>Back Up &deepsea; Configuration</title>

  <para>
   All files required by &deepsea; are stored in
   <filename>/srv/pillar/</filename>, <filename>/srv/salt/</filename> and
   <filename>/etc/salt/master.d</filename>.
  </para>

  <para>
   If you need to redeploy the &adm;, install the &deepsea; package on the new
   node and move the backed up data back into the directories. &deepsea; can
   then be used again without any further changes being required. Before using
   &deepsea; again, make sure that all &salt; minions are correctly registered
   on the &adm;.
  </para>
 </sect2>
 <sect2 xml:id="backup-config-files">
  <title>Back Up Custom Configurations</title>

  <itemizedlist>
   <listitem>
    <para>
     &prometheus; data and customization.
    </para>
   </listitem>
   <listitem>
    <para>
     &grafana; customization.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that you have a record of existing &oa; users so that you can
     create new accounts for these users in the &dashboard;.
    </para>
   </listitem>
   <listitem>
    <para>
     Manual changes to <filename>ceph.conf</filename> outside of &deepsea;.
    </para>
   </listitem>
   <listitem>
    <para>
     Manual changes to the &iscsi; configuration outside of &deepsea;.
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; keys.
    </para>
   </listitem>
   <listitem>
    <para>
     &crushmap; and CRUSH rules. Save the decompiled &crushmap; including CRUSH
     rules into <filename>crushmap-backup.txt</filename> by running the
     following command:
    </para>
<screen>&prompt.cephuser;ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</screen>
   </listitem>
   <listitem>
    <para>
     &sgw; configuration. If you are using a single gateway, backup
     <filename>/etc/samba/smb.conf</filename>. If you are using HA setup,
     backup also CTDB and Pacemaker configuration files. Refer to
     <xref linkend="cha-ses-cifs"/> for details on what configuration is used
     by &sgw;s.
    </para>
   </listitem>
   <listitem>
    <para>
     &ganesha; configuration. Only needed when using HA setup. Refer to
     <xref linkend="cha-ceph-nfsganesha"/> for details on what configuration is
     used by &ganesha;.
    </para>
   </listitem>
  </itemizedlist>
 </sect2>
 </sect1>
 <sect1 xml:id="restore-ceph">
  <title>Restoring a Ceph Node</title>
  <para>
   The procedure to recover a node from backup is to reinstall the
   node, replace its configuration files, and then re-orchestrate the cluster
   so that the replacement node is re-added. 
  </para>
  <orderedlist>
   <listitem>
    <para>
     Re-install the node. (See <link
     xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#ceph-install-stack"/>
     steps 1-4.)
    </para>
   </listitem>
   <listitem>
    <para>
     Update to the latest patch level (See: <link
      xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-zypper-softup-update"/>.)
    </para>
   </listitem>
   <listitem>
    <para>
     Install &salt; and &deepsea;. (See <link
      xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#ceph-install-stack"/>
     steps 5-7.)
    </para>
   </listitem>
   <listitem>
    <para>
     Restore the old, saved &deepsea; and &salt; configuration files.
    </para>
    <para>
     It may be necessary to set the permissions appropriately on the replaced
     files and manually enable services. For instance:
<screen>&prompt.smaster; chown salt:root /etc/salt/pki/master/master.pem
&prompt.smaster; chown salt:root /etc/salt/pki/master/master.pub
&prompt.smaster; chown -R salt:salt /srv/pillar/ceph/
&prompt.smaster; systemctl enable salt-api
&prompt.smaster; systemctl start salt-api
&prompt.smaster; systemctl enable salt-minion
&prompt.smaster; systemctl start salt-minion
&prompt.smaster; systemctl enable salt-master
&prompt.smaster; systemctl start salt-master</screen>     
    </para>
   </listitem>
   <listitem>
    <para>
     Register the minions against the new &salt; master. (See <link
      xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#ceph-install-stack"/>
     steps 8-11.)
    </para>
   </listitem>
   <listitem>
    <para>
     Restore anything that was local to the admin node.
    </para>
   </listitem>
   <listitem>
    <para>
     Run stages 0-2. (See <link
      xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#ds-depl-stages"/>
     steps 1-7.)
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure grains and pillar data are set.
    </para>
   </listitem>
   <listitem>
    <para>
     Make sure roles and drivegroups are correct.
    </para>
   </listitem>
   <listitem>
    <para>
     Run stages 3-4. (See <link
      xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#ds-depl-stages"/>
     steps 8-10.)
    </para>
   </listitem>
  </orderedlist>

 <!-- backing up the drivegroup file is probably something that needs to be added to the backup chapter -->
 </sect1>
</chapter>
