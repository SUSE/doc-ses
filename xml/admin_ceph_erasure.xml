<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-erasure">
 <title>Erasure Coded Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  A &ceph; pool is associated to a type to sustain the loss of an OSD (that is
  a disk since most of the time there is one OSD per disk). The default choice
  when creating a pool is replicated, meaning every object is copied on
  multiple disks. The Erasure Code pool type can be used instead to save space.
 </para>
 <para>
  For background information on Erasure Code, see
  <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   You cannot access erasure coded pools with the rbd interface unless you have
   a cache tier configured. Refer to <xref linkend="ceph-tier-erasure"/> for
   more details.
  </para>
 </note>
 <sect1>
  <title>Creating a Sample Erasure Coded Pool</title>

  <para>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created
<prompt role="user">&gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt role="user">&gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The <literal>12</literal> in the <command>pool create</command> command
   stands for the number of placement groups.
  </para>
 </sect1>
 <sect1>
  <title>Erasure Code Profiles</title>

  <para>
   Some terminology hints:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <literal>k = 2</literal> a
      10KB object will be divided into <literal>k</literal> objects of 5KB
      each.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The default erasure code profile sustains the loss of a single OSD. It is
   equivalent to a replicated pool of size two but requires 1.5TB instead of
   2TB to store 1TB of data. The default profile can be displayed with:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
ruleset-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Choosing the right profile is important because it cannot be modified after
   the pool is created: a new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new.
  </para>

  <para>
   The most important parameters of the profile are <literal>k</literal>,
   <literal>m</literal> and <literal>ruleset-failure-domain</literal> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 40% overhead, the following profile can be defined:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   ruleset-failure-domain=rack
<prompt role="user">&gt; </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt role="user">&gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt role="user">&gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The NYAN object will be divided in three (<literal>k=3</literal>) and two
   additional chunks will be created (<literal>m=2</literal>). The value of
   <literal>m</literal> defines how many OSDs can be lost simultaneously
   without losing any data. The <literal>ruleset-failure-domain=rack</literal>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   For more information about the erasure code profiles, see
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>Erasure Coded Pool And Cache Tiering</title>

  <para>
   Erasure coded pools require more resources than replicated pools and lack
   some functionalities such as partial writes. To overcome these limitations,
   it is recommended to set a cache tier before the erasure coded pool.
  </para>

  <para>
   For example, if the <quote>hot-storage</quote> pool is made of fast storage:
  </para>

<screen><prompt role="user">&gt; </prompt>ceph osd tier add ecpool hot-storage
<prompt role="user">&gt; </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt role="user">&gt; </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   This will place the <quote>hot-storage</quote> pool as tier of ecpool in
   write-back mode so that every write and read to the ecpool is actually using
   the hot-storage and benefits from its flexibility and speed.
  </para>

  <para>
   It is not possible to create an RBD image on an erasure coded pool because
   it requires partial writes. It is however possible to create an RBD image on
   an erasure coded pool when a replicated pool tier set a cache tier:
  </para>

<screen><prompt role="user">&gt; </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   For more information about cache tiering, see
   <xref linkend="cha-ceph-tiered"/>.
  </para>
 </sect1>
</chapter>
