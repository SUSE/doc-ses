<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.erasure">
 <title>Erasure Coded Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; provides an alternative to the normal replication of data in pools,
  called <emphasis>erasure</emphasis> or <emphasis>erasure coded</emphasis>
  pool. Erasure pools do not provide all functionalities of
  <emphasis>replicated</emphasis> pools, but require less
  raw storage. A default erasure pool capable of storing 1 TB of data
  requires 1,5 TB of raw storage. This compares favorably to a replicated
  pool which needs 2 TB of raw storage for the same amount of data.
 </para>
 <para>
  For background information on Erasure Code, see
  <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   You cannot access erasure coded pools with the rbd interface unless you have
   a cache tier configured. Refer to <xref linkend="ceph.tier.erasure"/> for
   more details.
  </para>
 </note>
 <note>
  <para>
   Ensure that the CRUSH rules for <emphasis>erasure pools</emphasis>
   use <literal>indep</literal> for <literal>step</literal>.
   For details see <xref linkend="datamgm.rules.step.mode" />.
  </para>
 </note>
 <sect1 xml:id="cha.ceph.erasure.default-profile">
  <title>Creating a Sample Erasure Coded Pool</title>

  <para>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </para>
  <procedure>
   <step>
    <para>
     The command <command>ceph osd pool create</command> is used to create
     a pool with type <emphasis>erasure</emphasis>. The <literal>12</literal>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </para>
<screen>&prompt.root;ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</screen>
   </step>
   <step>
    <para>
     The string <literal>ABCDEFGHI</literal> is written into an object called
     <literal>NYAN</literal>.
    </para>
<screen>&prompt.cephuser;echo ABCDEFGHI | rados --pool ecpool put NYAN -</screen>
   </step>
   <step>
    <para>
     For testing purposes OSDs can now be disabled, for example by disconnecting
     them from the network.
    </para>
   </step>
   <step>
    <para>
     To test whether the pool can handle the failure of devices, the content
     of the file can be accessed with the <command>rados</command> command.
    </para>
<screen>&prompt.root;rados --pool ecpool get NYAN -
ABCDEFGHI</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cha.ceph.erasure.erasure-profiles">
  <title>Erasure Code Profiles</title>
  <para>When the <command>ceph osd pool create</command> command is invoked
   to create an <emphasis>erasure pool</emphasis>, the default profile is
   used, unless another profile is specified. Profiles basically define the
   redundancy of data. This is done by setting two parameters, arbitrarily
   named <literal>k</literal> and <literal>m</literal>. k and m define
   on how many OSDs a <literal>chunk</literal> of data is stored and how
   many of those OSDs are allowed to fail without loosing data.
  </para>
  <para>
   Definitions required for erasure pool profiles:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <literal>k = 2</literal> a
      10KB object will be divided into <literal>k</literal> objects of 5KB
      each.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ruleset-failure-domain</term>
    <listitem>
     <para>
      defines to which devices the chunks are distributed. A bucket type has
      to be set as value. For all bucket types, see <xref linkend="datamgm.buckets" />.
      If the failure domain is <literal>rack</literal>, the chunks will be
      stored on different racks to increase the resilience in case of rack
      failures.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   With the default erasure code profile used in
   <xref linkend="cha.ceph.erasure.default-profile"/>, you will not lose cluster
   data if a single OSD fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage
   are required for 1 TB of data. This is equivalent to a common RAID 5
   configuration. For comparison: a replicated pool needs 2 TB of raw
   storage to store 1 TB of data.
  </para>
  <para>The settings of the default profile can be displayed with:
  </para>

<screen>&prompt.root;ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
ruleset-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new.
  </para>

  <para>
   The most important parameters of the profile are <literal>k</literal>,
   <literal>m</literal> and <literal>ruleset-failure-domain</literal> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66% overhead, the following profile can be defined:
  </para>

<screen>&prompt.root;ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   ruleset-failure-domain=rack</screen>

  <para>
   The example <xref linkend="cha.ceph.erasure.default-profile"/>
   can be repeated with this new profile:
  </para>

<screen>&prompt.root;ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
&prompt.cephuser;echo ABCDEFGHI | rados --pool ecpool put NYAN -
&prompt.root;rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The NYAN object will be divided in three (<literal>k=3</literal>) and two
   additional chunks will be created (<literal>m=2</literal>). The value of
   <literal>m</literal> defines how many OSDs can be lost simultaneously
   without losing any data. The <literal>ruleset-failure-domain=rack</literal>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <para>
   For more information about the erasure code profiles, see
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/erasure-code-profile"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.tier.erasure">
  <title>Erasure Coded Pool And Cache Tiering</title>

  <para>
   Erasure coded pools require more resources than replicated pools and lack
   some functionalities such as partial writes. To overcome these limitations,
   it is recommended to set a cache tier before the erasure coded pool.
  </para>

  <para>
   For example, if the <quote>hot-storage</quote> pool is made of fast storage,
   the <quote>ecpool</quote> created in <xref linkend="cha.ceph.erasure.erasure-profiles" />
   can be sped up with:
  </para>

<screen>&prompt.root;ceph osd tier add ecpool hot-storage
&prompt.root;ceph osd tier cache-mode hot-storage writeback
&prompt.root;ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   This will place the <quote>hot-storage</quote> pool as a tier of ecpool in
   write-back mode so that every write and read to the ecpool is actually using
   the hot-storage and benefits from its flexibility and speed.
  </para>

  <para>
   It is not possible to create an RBD image on an erasure coded pool because
   it requires partial writes. It is however possible to create an RBD image on
   an erasure coded pool when a replicated pool tier set a cache tier:
  </para>

<screen>&prompt.root;rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   For more information about cache tiering, see
   <xref linkend="cha.ceph.tiered"/>.
  </para>
 </sect1>
</chapter>
