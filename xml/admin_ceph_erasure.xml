<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.erasure">
 <title>Erasure Coded Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; provides an alternative to the normal replication of data in pools,
  called <emphasis>erasure</emphasis> or <emphasis>erasure coded</emphasis>
  pool. Erasure pools do not provide all functionality of
  <emphasis>replicated</emphasis> pools (for example it cannot store metadata
  for RBD pools), but require less raw storage. A default erasure pool capable
  of storing 1 TB of data requires 1,5 TB of raw storage, allowing a single
  disk failure. This compares favorably to a replicated pool which needs 2 TB
  of raw storage for the same purpose.
 </para>
 <para>
  For background information on Erasure Code, see
  <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   When using &filestore;, you cannot access erasure coded pools with the RBD
   interface unless you have a cache tier configured. Refer to
   <xref linkend="ceph.tier.erasure"/> for more details, or use the default
   &bluestore; (see <xref linkend="about.bluestore" />).
  </para>
 </note>
 <sect1 xml:id="ec.prerequisite">
  <title>Prerequisite for Erasure Coded Pools</title>

  <para>
   To make use of erasure coding, you need to:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Define an erasure rule in the &crushmap;.
    </para>
   </listitem>
   <listitem>
    <para>
     Define an erasure code profile that specifies the coding algorithm to be
     used.
    </para>
   </listitem>
   <listitem>
    <para>
     Create a pool using the previously mentioned rule and profile.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Keep in mind that changing the profile and the details in the profile will
   not be possible after the pool was created and has data.
  </para>

  <para>
   Ensure that the CRUSH rules for <emphasis>erasure pools</emphasis> use
   <literal>indep</literal> for <literal>step</literal>. For details see
   <xref linkend="datamgm.rules.step.mode"/>.
  </para>
 </sect1>
 <sect1 xml:id="cha.ceph.erasure.default-profile">
  <title>Creating a Sample Erasure Coded Pool</title>

  <para>
   The simplest erasure coded pool is equivalent to RAID5 and requires at least
   three hosts. This procedure describes how to create a pool for testing
   purposes.
  </para>

  <procedure>
   <step>
    <para>
     The command <command>ceph osd pool create</command> is used to create a
     pool with type <emphasis>erasure</emphasis>. The <literal>12</literal>
     stands for the number of placement groups. With default parameters, the
     pool is able to handle the failure of one OSD.
    </para>
<screen>&prompt.cephuser;ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</screen>
   </step>
   <step>
    <para>
     The string <literal>ABCDEFGHI</literal> is written into an object called
     <literal>NYAN</literal>.
    </para>
<screen>&prompt.cephuser;echo ABCDEFGHI | rados --pool ecpool put NYAN -</screen>
   </step>
   <step>
    <para>
     For testing purposes OSDs can now be disabled, for example by
     disconnecting them from the network.
    </para>
   </step>
   <step>
    <para>
     To test whether the pool can handle the failure of devices, the content of
     the file can be accessed with the <command>rados</command> command.
    </para>
<screen>&prompt.cephuser;rados --pool ecpool get NYAN -
ABCDEFGHI</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cha.ceph.erasure.erasure-profiles">
  <title>Erasure Code Profiles</title>

  <para>
   When the <command>ceph osd pool create</command> command is invoked to
   create an <emphasis>erasure pool</emphasis>, the default profile is used,
   unless another profile is specified. Profiles define the redundancy of data.
   This is done by setting two parameters, arbitrarily named
   <literal>k</literal> and <literal>m</literal>. k and m define in how many
   <literal>chunks</literal> a piece of data is split and how many coding
   chunks are created. Redundant chunks are then stored on different OSDs.
  </para>

  <para>
   Definitions required for erasure pool profiles:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      when the encoding function is called, it returns chunks of the same size:
      data chunks which can be concatenated to reconstruct the original object
      and coding chunks which can be used to rebuild a lost chunk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      the number of data chunks, that is the number of chunks into which the
      original object is divided. For example if <literal>k = 2</literal> a
      10KB object will be divided into <literal>k</literal> objects of 5KB
      each.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      the number of coding chunks, that is the number of additional chunks
      computed by the encoding functions. If there are 2 coding chunks, it
      means 2 OSDs can be out without losing data.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>crush-failure-domain</term>
    <listitem>
     <para>
      defines to which devices the chunks are distributed. A bucket type needs
      to be set as value. For all bucket types, see
      <xref linkend="datamgm.buckets"/>. If the failure domain is
      <literal>rack</literal>, the chunks will be stored on different racks to
      increase the resilience in case of rack failures. Keep in mind that this
      requires k+m racks.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   With the default erasure code profile used in
   <xref linkend="cha.ceph.erasure.default-profile"/>, you will not lose
   cluster data if a single OSD or host fails. Therefore, to store 1 TB of data
   it needs another 0.5 TB of raw storage. That means 1.5 TB of raw storage are
   required for 1 TB of data (due to k=2,m=1). This is equivalent to a common
   RAID 5 configuration. For comparison: a replicated pool needs 2 TB of raw
   storage to store 1 TB of data.
  </para>

  <para>
   The settings of the default profile can be displayed with:
  </para>

<screen>&prompt.cephuser;ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   Choosing the right profile is important because it cannot be modified after
   the pool is created. A new pool with a different profile needs to be created
   and all objects from the previous pool moved to the new one (see
   <xref linkend="pools.migration"/>).
  </para>

  <para>
   The most important parameters of the profile are <literal>k</literal>,
   <literal>m</literal> and <literal>crush-failure-domain</literal> because
   they define the storage overhead and the data durability. For example, if
   the desired architecture must sustain the loss of two racks with a storage
   overhead of 66% overhead, the following profile can be defined. Note that
   this is only valid with a &crushmap; that has buckets of type 'rack':
  </para>

<screen>&prompt.cephuser;ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</screen>

  <para>
   The example <xref linkend="cha.ceph.erasure.default-profile"/> can be
   repeated with this new profile:
  </para>

<screen>&prompt.cephuser;ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
&prompt.cephuser;echo ABCDEFGHI | rados --pool ecpool put NYAN -
&prompt.cephuser;rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   The NYAN object will be divided in three (<literal>k=3</literal>) and two
   additional chunks will be created (<literal>m=2</literal>). The value of
   <literal>m</literal> defines how many OSDs can be lost simultaneously
   without losing any data. The <literal>crush-failure-domain=rack</literal>
   will create a CRUSH ruleset that ensures no two chunks are stored in the
   same rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <sect2 xml:id="ec.create">
   <title>Creating a New Erasure Code Profile</title>
   <para>
    The following command creates a new erasure code profile:
   </para>
<screen>
&prompt.root;ceph osd erasure-code-profile set <replaceable>NAME</replaceable> \
 directory=<replaceable>DIRECTORY</replaceable> \
 plugin=<replaceable>PLUGIN</replaceable> \
 stripe_unit=<replaceable>STRIPE_UNIT</replaceable> \
 <replaceable>KEY</replaceable>=<replaceable>VALUE</replaceable> ... \
 --force
</screen>
   <variablelist>
    <varlistentry>
     <term>DIRECTORY</term>
     <listitem>
      <para>
       Optional. Set the directory name from which the erasure code plugin is
       loaded. Default is <filename>/usr/lib/ceph/erasure-code</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PLUGIN</term>
     <listitem>
      <para>
       Optional. Use the erasure code plugin to compute coding chunks and
       recover missing chunks. Available plugins are 'jerasure', 'isa', 'lrc',
       and 'shes'. Default is 'jerasure'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STRIPE_UNIT</term>
     <listitem>
      <para>
       Optional. The amount of data in a data chunk, per stripe. For example, a
       profile with 2 data chunks and stripe_unit=4K would put the range 0-4K
       in chunk 0, 4K-8K in chunk 1, then 8K-12K in chunk 0 again. This should
       be a multiple of 4K for best performance. The default value is taken
       from the monitor config option
       <option>osd_pool_erasure_code_stripe_unit</option> when a pool is
       created. The 'stripe_width' of a pool using this profile will be the
       number of data chunks multiplied by this 'stripe_unit'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>KEY=VALUE</term>
     <listitem>
      <para>
       Key/value pairs of options specific to the selected erasure code plugin.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--force</term>
     <listitem>
      <para>
       Optional. Override an existing profile by the same name, and allow
       setting a non-4K-aligned stripe_unit.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ec.rm">
   <title>Removing an Erasure Code Profile</title>
   <para>
    The following command removes an erasure code profile as identified by its
    <replaceable>NAME</replaceable>:
   </para>
<screen>
&prompt.root;ceph osd erasure-code-profile rm <replaceable>NAME</replaceable>
</screen>
   <important>
    <para>
     If the profile is referenced by a pool, the deletion will fail.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ec.get">
   <title>Displaying an Erasure Coded Profile Details</title>
   <para>
    The following command displays details of an erasure code profile as
    identified by its <replaceable>NAME</replaceable>:
   </para>
<screen>
&prompt.root;ceph osd erasure-code-profile get <replaceable>NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="ec.ls">
   <title>Listing Erasure Code Profiles</title>
   <para>
    The following command lists the names of all erasure code profiles:
   </para>
<screen>
&prompt.root;ceph osd erasure-code-profile ls
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ec.rbd">
  <title>Erasure Coded Pools with &rbd;</title>

  <para>
   To mark an EC pool as a RBD pool, tag it accordingly:
  </para>

<screen>
&prompt.cephuser;ceph osd pool application enable rbd <replaceable>ec_pool_name</replaceable>
</screen>

  <para>
   RBD can store image <emphasis>data</emphasis> in EC pools. However, the
   image header and metadata still needs to be stored in a replicated pool.
   Assuming you have the pool named 'rbd' for this purpose:
  </para>

<screen>
&prompt.cephuser;rbd create rbd/<replaceable>image_name</replaceable> --size 1T --data-pool <replaceable>ec_pool_name</replaceable>
</screen>

  <para>
   You can use the image normally like any other image, except that all of the
   data will be stored in the <replaceable>ec_pool_name</replaceable> pool
   instead of 'rbd' pool.
  </para>
 </sect1>
</chapter>
