<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-cephadm">
 <title>Troubleshooting &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   As &cephadm; deploys daemons as containers, troubleshooting daemons is
   slightly different. Here are a few tools and commands to help
   investigating issues.
 </para>
 <sect1 xml:id="bp-troubleshooting-cephadm-pause-disable">
   <title>Pausing or Disabling &cephadm;</title>
   <para>
     If something goes wrong and &cephadm; is behaving in a
     way you do not like, you can pause most background activity by executing
     the following:
   </para>
<screen>
&prompt.cephuser;ceph orch pause
</screen>
   <para>
     This will stop any changes, but &cephadm; will still periodically
     check hosts to refresh its inventory of daemons and devices.
     You can disable &cephadm; completely with:
   </para>
<screen>
&prompt.cephuser;ceph orch set backend ''
&prompt.cephuser;ceph mgr module disable cephadm
</screen>
   <para>
     This disables all of the <command>ceph orch ...</command> CLI commands
     but the previously deployed daemon containers will continue to exist
     and start as they did before.
   </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-check-logs">
   <title>Checking &cephadm; Logs</title>
   <para>
     You can monitor the &cephadm; log in real time:
   </para>
<screen>
&prompt.cephuser;ceph -W cephadm
</screen>
   <para>
     To view the last few messages, execute:
   </para>
<screen>
&prompt.cephuser;ceph log last cephadm
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-logs">
   <title>Accessing &ceph; Daemon Logs</title>
   <para>
     &productname; &productnumber; supports &ceph; logging via systemd-journald. To access
     the logs of &ceph; daemons in  &productname; &productnumber;, follow the
     instructions below.
   </para>
   <procedure>
     <step>
       <para>
         Use the <command>ceph orch ps</command> command (or <command>ceph orch ps <replaceable>node_name</replaceable></command>
         or <command>ceph orch ps --daemon-type <replaceable>daemon_type</replaceable></command>)
         to find the &cephadm; name of the daemon where the host is running.
       </para>
<screen>&prompt.cephuser;ceph orch ps --daemon-type <replaceable>DAEMON_TYPE_NAME</replaceable></screen>
       <para>
         For example, if you want to view the logs for &prometheus;, run the
         following command to find the name of the daemon:
       </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon-type prometheus
NAME               HOST    STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                             IMAGE ID      CONTAINER ID
prometheus.master  master  running (65m)  7m ago     2h   2.18.0   registry.suse.com/caasp/v4.5/prometheus-server:2.18.0  e77db6e75e78  7b11062150ab
</screen>
       <para>
         In this example, <literal>prometheus.master</literal> is the name and the
         host is <literal>master</literal>.
       </para>
     </step>
     <step>
       <para>
         To view the daemon's logs, execute the following on the host where
         the daemon is running:
       </para>
<screen>&prompt.cephuser;cephadm logs --name <replaceable>DAEMON_NAME</replaceable></screen>
       <para>
         For example, to view the &prometheus; logs from above:
       </para>
<screen>&prompt.cephuser;cephadm logs --name prometheus.master</screen>
     </step>
   </procedure>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-systemctl">
   <title>Collecting systemd Status</title>
   <para>
     To print the state of a <literal>systemd</literal> unit, run:
   </para>
<screen>
systemctl status "ceph-$(cephadm shell ceph fsid)@<replaceable>service name</replaceable>.service";
</screen>
   <para>
     To fetch all state of all daemons of a given host, run:
   </para>
<screen>
  fsid="$(cephadm shell ceph fsid)"
  for name in $(cephadm ls | jq -r '.[].name') ; do
    systemctl status "ceph-$fsid@$name.service" > $name;
  done
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-container-images">
   <title>List all Downloaded Container Images</title>
   <title>List All Downloaded Container Images</title>
   <para>
     To list all container images that are downloaded on a host:
   </para>
   <note>
     <para>
       <literal>Image</literal> may also be called <literal>ImageID</literal>.
     </para>
   </note>
<screen>
  podman ps -a --format json | jq '.[].Image'
  "docker.io/library/centos:8"
  "registry.opensuse.org/opensuse/leap:15.2"
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-manual">
   <title>Manually Running Containers</title>
   <para>
     &cephadm; writes small wrappers that run a containers.
     Refer to <filename>/var/lib/ceph/<replaceable>cluster-fsid</replaceable>/<replaceable>service-name</replaceable>/unit.run</filename>
     for the container execution command.
   </para>
   <sect2 xml:id="bp-troubleshooting-ssh-errors">
     <title>SSH Errors</title>
     <para>
       If you receive the following error:
     </para>
<screen>
xxxxxx.gateway_bootstrap.HostNotFound: -F /tmp/cephadm-conf-kbqvkrkw root@10.10.1.2
raise OrchestratorError('Failed to connect to %s (%s).  Check that the host is reachable and accepts  connections using the cephadm SSH key' % (host, addr)) from
orchestrator._interface.OrchestratorError: Failed to connect to 10.10.1.2 (10.10.1.2).  Check that the host is reachable and accepts connections using the cephadm SSH key
</screen>
     <para>
       You can verify the issue by trialing a few different options.
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Ensure &cephadm; has an SSH identity key:
         </para>
<screen>&prompt.smaster;cephadm shell -- ceph config-key get mgr/cephadm/ssh_identity_key > key
INFO:cephadm:Inferring fsid f8edc08a-7f17-11ea-8707-000c2915dd98
INFO:cephadm:Using recent ceph image docker.io/ceph/ceph:v15 obtained 'mgr/cephadm/ssh_identity_key'
&prompt.smaster;chmod 0600 key
</screen>
         <para>
           If this fails, &cephadm; does not have a key. Fix this by running
           the following command:
         </para>
<screen>
&prompt.smaster;cephadm shell -- ceph cephadm generate-ssh-key
</screen>
         <para>
           Or:
         </para>
<screen>
&prompt.smaster;cat key | cephadm shell -- ceph cephadm set-ssk-key -i -
</screen>
       </listitem>
       <listitem>
         <para>
           Ensure that the SSH config is correct:
         </para>
<screen>
&prompt.smaster;cephadm shell -- ceph cephadm get-ssh-config > config
</screen>
       </listitem>
       <listitem>
         <para>
           Verify the connection to the host:
         </para>
<screen>
&prompt.smaster;ssh -F config -i key root@mon1
</screen>
       </listitem>
     </itemizedlist>
   </sect2>
   <sect2 xml:id="bp-troubleshooting-verify-pubkey">
     <title>Verify Public Key is in <filename>authorized_keys</filename></title>
     <para>
       To verify that the public key is in the <filename>authorized_keys</filename> file, run
       the following commands:
     </para>
<screen>
&prompt.smaster;cephadm shell -- ceph config-key get mgr/cephadm/ssh_identity_pub > key.pub
&prompt.smaster;grep "`cat key.pub`"  /root/.ssh/authorized_keys
</screen>
   </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cidr-error">
   <title>Failed To Infer CIDR Network Error</title>
   <para>
     If you see one of the following errors:
   </para>
<screen>ERROR: Failed to infer CIDR network for mon ip ***; pass --skip-mon-network to configure it later</screen>
   <para>
    or
   </para>
<screen>Must set public_network config option or specify a CIDR network, ceph addrvec, or plain IP</screen>
   <para>
    You need to specify a subnet for &mon;s:
   </para>
<screen>&prompt.cephuser;ceph config set mon public_network <replaceable>MON_NETWORK</replaceable></screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-admin-socket">
   <title>Accessing The Admin Socket</title>
   <para>
     Each &ceph; daemon provides an admin socket that bypasses the MONs. To
     access the admin socket, enter the daemon container on the host:
   </para>
<screen>
&prompt.smaster;cephadm enter --name <replaceable>daemon-name</replaceable>
&prompt.smaster;ceph --admin-daemon /var/run/ceph/ceph-<replaceable>daemon-name</replaceable>.asok config show
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-manual-deploy-mgr">
   <title>Manually Deploying a &mgr;</title>
   <para>
     Use the following steps to deploy a new &mgr; on a host.
   </para>
   <procedure>
     <step>
       <para>
         Disable the &cephadm; scheduler, in order to prevent &cephadm; from
         removing the new &mgr;:
       </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/cephadm/pause true
</screen>
     </step>
     <step>
       <para>
         Create the auth entry:
       </para>
<screen>
&prompt.cephuser;ceph auth create <replaceable>DAEMON_NAME</replaceable> mon "profile mgr" osd "allow *" mds "allow *"
</screen>
       <para>
         If you already have an auth entry, you can get the entry with the
         following command:
       </para>
<screen>
&prompt.cephuser;ceph auth get <replaceable>DAEMON_NAME</replaceable> mon "profile mgr" osd "allow *" mds "allow *"
</screen>
     </step>
     <step>
       <para>
         Generate the <filename>ceph.conf</filename>:
       </para>
<screen>
&prompt.cephuser;ceph config generate-minimal-conf
</screen>
     </step>
     <step>
       <para>
         Get the container image:
       </para>
<screen>
&prompt.cephuser;ceph config get "<replaceable>DAEMON_NAME</replaceable>" container_image
</screen>
     </step>
     <step>
       <para>
         Create the config json:
       </para>
<screen>
cat config-json.json
{
"config": "# minimal ceph.conf for 8255263a-a97e-4934-822c-00bfe029b28f\n[global]\n\tfsid = 8255263a-a97e-4934-822c-00bfe029b28f\n\tmon_host = [v2:192.168.178.28:40483/0,v1:192.168.178.28:40484/0]\n",
"keyring": "[mgr.hostname.smfvfd]\n\tkey = AQDSb0ZfJqfDNxAAhgAVuH8Wg0yaRUAHwXdTQA==\n"
}
</screen>
     </step>
     <step>
       <para>
         Deploy the daemon:
       </para>
<screen>
&prompt.cephuser;cephadm --image <replaceable>container-image</replaceable> deploy --fsid <replaceable>fsid</replaceable> --name <replaceable>DAEMON_NAME</replaceable> --config-json config-json.json --tcp-ports 42483
</screen>
     </step>
   </procedure>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-ptfs">
   <title>Distributing a Program Temporary Fix (PTF)</title>
   <para>
     Occasionally, in order to fix a given issue, &suse; will provide a set of
     packages known as a Program Temporary Fix (PTF). Such a PTF is fully
     supported by &suse; until the Maintenance Update (MU) containing a permanent fix
     has been released via the regular update repositories. Customers running
     PTF fixes will be notified through the related Service Request when a
     permanent patch for a PTF has been released.
   </para>
   <para>
     With &cephadm;, deploying the PTF is already possible by redeploying a
     specific daemon with a specific <literal>container_image</literal>.
   </para>
   <para>
     Use the following command to deploy a specific daemon with a new container
     image:
   </para>
<screen>
&prompt.cephuser;ceph orch daemon redeploy <replaceable>daemon</replaceable> --image=myregistry.com/<replaceable>myimage</replaceable>:<replaceable>mytag</replaceable>
</screen>
 </sect1>
</chapter>
