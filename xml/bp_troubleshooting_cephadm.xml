<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-cephadm">
 <title>Troubleshooting &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  As &cephadm; deploys daemons as containers, troubleshooting daemons is
  slightly different. Here are a few tools and commands to help investigating
  issues.
 </para>
 <sect1 xml:id="bp-troubleshooting-cephadm-pause-disable">
  <title>Pausing or disabling &cephadm;</title>

  <para>
   If something goes wrong and &cephadm; is behaving in a way you do not like,
   you can pause most background activity by executing the following:
  </para>

<screen>
&prompt.cephuser;ceph orch pause
</screen>

  <para>
   This will stop any changes, but &cephadm; will still periodically check
   hosts to refresh its inventory of daemons and devices. You can disable
   &cephadm; completely with:
  </para>

<screen>
&prompt.cephuser;ceph orch set backend ''
&prompt.cephuser;ceph mgr module disable cephadm
</screen>

  <para>
   This disables all of the <command>ceph orch ...</command> CLI commands but
   the previously deployed daemon containers will continue to exist and start
   as they did before.
  </para>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-check-logs">
  <title>Checking &cephadm; logs</title>

  <para>
   You can monitor the &cephadm; log in real time:
  </para>

<screen>
&prompt.cephuser;ceph -W cephadm
</screen>

  <para>
   To view the last few messages, execute:
  </para>

<screen>
&prompt.cephuser;ceph log last cephadm
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-logs">
  <title>Accessing &ceph; daemon logs</title>

  <para>
   &productname; &productnumber; supports &ceph; logging via systemd-journald.
   To access the logs of &ceph; daemons in &productname; &productnumber;,
   follow the instructions below.
  </para>

  <procedure>
   <step>
    <para>
     Use the <command>ceph orch ps</command> command (or <command>ceph orch ps
     <replaceable>node_name</replaceable></command> or <command>ceph orch ps
     --daemon-type <replaceable>daemon_type</replaceable></command>) to find
     the &cephadm; name of the daemon where the host is running.
    </para>
<screen>&prompt.cephuser;ceph orch ps --daemon-type <replaceable>DAEMON_TYPE_NAME</replaceable></screen>
    <para>
     For example, if you want to view the logs for &prometheus;, run the
     following command to find the name of the daemon:
    </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon-type prometheus
NAME               HOST    STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                                   IMAGE ID      CONTAINER ID
prometheus.main    main    running (65m)  7m ago     2h   2.32.1   registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1  e77db6e75e78  7b11062150ab
</screen>
    <para>
     In this example, <literal>prometheus.main</literal> is the name and the
     host is <literal>main</literal>.
    </para>
   </step>
   <step>
    <para>
     To view the daemon's logs, execute the following on the host where the
     daemon is running:
    </para>
<screen>&prompt.cephuser;cephadm logs --name <replaceable>DAEMON_NAME</replaceable></screen>
    <para>
     For example, to view the &prometheus; logs from above:
    </para>
<screen>&prompt.cephuser;cephadm logs --name prometheus.main</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-systemctl">
  <title>Collecting <literal>systemd</literal> status</title>

  <para>
   To print the state of a <literal>systemd</literal> unit, run:
  </para>

<screen>
systemctl status "ceph-$(cephadm shell ceph fsid)@<replaceable>service name</replaceable>.service";
</screen>

  <para>
   To fetch all state of all daemons of a given host, run:
  </para>

<screen>
  fsid="$(cephadm shell ceph fsid)"
  for name in $(cephadm ls | jq -r '.[].name') ; do
    systemctl status "ceph-$fsid@$name.service" > $name;
  done
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-configured-container-images">
  <title>Listing configured container images</title>

  <para>
   &cephadm; has a set of container images configured by default.
  </para>

  <para>
   To get the value of the configured default container:
  </para>

<screen>&prompt.cephuser;ceph config get mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable></screen>

  <para>
   Where <replaceable>OPTION_NAME</replaceable> is any of the following names:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     container_image_base
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_prometheus
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_node_exporter
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_alertmanager
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_grafana
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_haproxy
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_keepalived
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For example:
  </para>

<screen>
&prompt.cephuser;ceph config get mgr mgr/cephadm/container_image_base \
 registry.suse.com/ses/7.1/ceph/ceph
</screen>

  <note>
   <para>
    <literal>mgr/cephadm/container_image_base</literal> is the default &ceph;
    container image used by all services except the monitoring stack and the ingress stack.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-downloaded-container-images">
  <title>Listing all downloaded container images</title>

  <para>
   To list all container images that are downloaded on a host:
  </para>

  <note>
   <para>
    <literal>Image</literal> may also be called <literal>ImageID</literal>.
   </para>
  </note>

<screen>
  podman ps -a --format json | jq '.[].Image'
  "docker.io/library/centos:8"
  "registry.opensuse.org/opensuse/leap:15.2"
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cephadm-manual">
  <title>Running containers manually</title>

  <para>
   &cephadm; writes small wrappers that run a containers. Refer to
   <filename>/var/lib/ceph/<replaceable>cluster-fsid</replaceable>/<replaceable>service-name</replaceable>/unit.run</filename>
   for the container execution command.
  </para>

  <sect2 xml:id="bp-troubleshooting-ssh-errors">
   <title>Assessing SSH errors</title>
   <para>
    If you receive the following error:
   </para>
<screen>
xxxxxx.gateway_bootstrap.HostNotFound: -F /tmp/cephadm-conf-kbqvkrkw root@10.10.1.2
raise OrchestratorError('Failed to connect to %s (%s).  Check that the host is reachable and accepts  connections using the cephadm SSH key' % (host, addr)) from
orchestrator._interface.OrchestratorError: Failed to connect to 10.10.1.2 (10.10.1.2).  Check that the host is reachable and accepts connections using the cephadm SSH key
</screen>
   <para>
    You can verify the issue by trialing a few different options.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Ensure &cephadm; has an SSH identity key:
     </para>
<screen>&prompt.smaster;cephadm shell -- ceph config-key get mgr/cephadm/ssh_identity_key > key
INFO:cephadm:Inferring fsid f8edc08a-7f17-11ea-8707-000c2915dd98
INFO:cephadm:Using recent ceph image docker.io/ceph/ceph:v15 obtained 'mgr/cephadm/ssh_identity_key'
&prompt.smaster;chmod 0600 key
</screen>
     <para>
      If this fails, &cephadm; does not have a key. Fix this by running the
      following command:
     </para>
<screen>
&prompt.smaster;cephadm shell -- ceph cephadm generate-ssh-key
</screen>
     <para>
      Or:
     </para>
<screen>
&prompt.smaster;cat key | cephadm shell -- ceph cephadm set-ssk-key -i -
</screen>
    </listitem>
    <listitem>
     <para>
      Ensure that the SSH config is correct:
     </para>
<screen>
&prompt.smaster;cephadm shell -- ceph cephadm get-ssh-config > config
</screen>
    </listitem>
    <listitem>
     <para>
      Verify the connection to the host:
     </para>
<screen>
&prompt.smaster;ssh -F config -i key root@mon1
</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="bp-troubleshooting-verify-pubkey">
   <title>Verifying public key is in <filename>authorized_keys</filename></title>
   <para>
    To verify that the public key is in the
    <filename>authorized_keys</filename> file, run the following commands:
   </para>
<screen>
&prompt.smaster;cephadm shell -- ceph config-key get mgr/cephadm/ssh_identity_pub > key.pub
&prompt.smaster;grep "`cat key.pub`"  /root/.ssh/authorized_keys
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-cidr-error">
  <title>Failing to infer CIDR network error</title>

  <para>
   If you see one of the following errors:
  </para>

<screen>ERROR: Failed to infer CIDR network for mon ip ***; pass --skip-mon-network to configure it later</screen>

  <para>
   or
  </para>

<screen>Must set public_network config option or specify a CIDR network, ceph addrvec, or plain IP</screen>

  <para>
   You need to specify a subnet for &mon;s:
  </para>

<screen>&prompt.cephuser;ceph config set mon public_network <replaceable>MON_NETWORK</replaceable></screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-admin-socket">
  <title>Accessing the admin socket</title>

  <para>
   Each &ceph; daemon provides an admin socket that bypasses the MONs. To
   access the admin socket, enter the daemon container on the host:
  </para>

<screen>
&prompt.smaster;cephadm enter --name <replaceable>daemon-name</replaceable>
&prompt.smaster;ceph --admin-daemon /var/run/ceph/ceph-<replaceable>daemon-name</replaceable>.asok config show
</screen>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-manual-deploy-mgr">
  <title>Deploying a &mgr; manually</title>

  <para>
   Use the following steps to deploy a new &mgr; on a host.
  </para>

  <procedure>
   <step>
    <para>
     Disable the &cephadm; scheduler, in order to prevent &cephadm; from
     removing the new &mgr;:
    </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/cephadm/pause true
</screen>
   </step>
   <step>
    <para>
     Create the auth entry:
    </para>
<screen>
&prompt.cephuser;ceph auth create <replaceable>DAEMON_NAME</replaceable> mon "profile mgr" osd "allow *" mds "allow *"
</screen>
    <para>
     If you already have an auth entry, you can get the entry with the
     following command:
    </para>
<screen>
&prompt.cephuser;ceph auth get <replaceable>DAEMON_NAME</replaceable> mon "profile mgr" osd "allow *" mds "allow *"
</screen>
   </step>
   <step>
    <para>
     Generate the <filename>ceph.conf</filename>:
    </para>
<screen>
&prompt.cephuser;ceph config generate-minimal-conf
</screen>
   </step>
   <step>
    <para>
     Get the container image:
    </para>
<screen>
&prompt.cephuser;ceph config get "<replaceable>DAEMON_NAME</replaceable>" container_image
</screen>
   </step>
   <step>
    <para>
     Create the config json:
    </para>
<screen>
cat config-json.json
{
"config": "# minimal ceph.conf for 8255263a-a97e-4934-822c-00bfe029b28f\n[global]\n\tfsid = 8255263a-a97e-4934-822c-00bfe029b28f\n\tmon_host = [v2:192.168.178.28:40483/0,v1:192.168.178.28:40484/0]\n",
"keyring": "[mgr.hostname.smfvfd]\n\tkey = AQDSb0ZfJqfDNxAAhgAVuH8Wg0yaRUAHwXdTQA==\n"
}
</screen>
   </step>
   <step>
    <para>
     Deploy the daemon:
    </para>
<screen>
&prompt.cephuser;cephadm --image <replaceable>container-image</replaceable> deploy --fsid <replaceable>fsid</replaceable> --name <replaceable>DAEMON_NAME</replaceable> --config-json config-json.json --tcp-ports 42483
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-ptfs">
  <title>Distributing a program temporary fix (PTF)</title>

  <para>
   Occasionally, in order to fix a given issue, &suse; will provide a set of
   packages known as a Program Temporary Fix (PTF). Such a PTF is fully
   supported by &suse; until the Maintenance Update (MU) containing a permanent
   fix has been released via the regular update repositories. Customers running
   PTF fixes will be notified through the related Service Request when a
   permanent patch for a PTF has been released.
  </para>

  <para>
   PTFs are published on the <literal>registry.suse.com</literal> server
   through a path similar to:
  </para>

<screen>registry.suse.com/ptf/<replaceable>PTF_NUMBER</replaceable>/ses/7.1/ceph/ceph:PTF-<replaceable>PTF_NUMBER</replaceable></screen>

  <para>
   The following steps describe how to deploy PTF images on an existing &ceph;
   cluster:
  </para>

  <procedure>
   <step>
    <para>
     Note the <replaceable>PTF_NUMBER</replaceable> used in the L3 PTF process.
    </para>
   </step>
   <step>
    <para>
     Determine the &mon; key that corresponds to the relevant service:
    </para>
<screen>&prompt.cephuser;ceph auth ls | grep <replaceable>DAEMON_TYPE</replaceable></screen>
   </step>
   <step>
    <para>
     Receive the PTF:
    </para>
<screen>
&prompt.root;podman pull \
registry.suse.com/ptf/<replaceable>PTF_NUMBER</replaceable>/ses/7.1/ceph/ceph:PTF-<replaceable>PTF_NUMBER</replaceable>
</screen>
   </step>
   <step>
    <para>
     Verify that the PTF is listed:
    </para>
<screen>&prompt.root;podman image ls</screen>
   </step>
   <step>
    <para>
     Set the PTF image for the relevant service, for example, &iscsi;:
    </para>
<screen>
&prompt.cephuser;ceph config set <replaceable>client.iscsi</replaceable> container_image \
registry.suse.com/ptf/<replaceable>PTF_NUMBER</replaceable>/ses/7.1/ceph/ceph:PTF-<replaceable>PTF_NUMBER</replaceable>
</screen>
   </step>
   <step>
    <para>
     Remove the specific &iscsi; daemon that is causing problems, for example:
    </para>
<screen>&prompt.cephuser;ceph orch daemon rm iscsi.igw.sesblade16.zkomeh</screen>
   </step>
   <step>
    <para>
     The removed daemon will be deployed automatically using the PTF.
    </para>
   </step>
  </procedure>

  <important>
   <para>
    If you plan to run the <command>ceph-salt update</command> command after
    applying a PTF (as described in <xref linkend="rolling-updates-running"/>),
    it will not affect the manual changes made by the temporary fix.
   </para>
   <para>
    Conversely, running the <command>ceph orch upgrade</command> command (see
    <xref linkend="deploy-cephadm-day2-cephupdate-start"/>) will upgrade
    daemons manually deployed by the PTF.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-adding-hosts">
  <title>Failure When Adding Hosts with &cephadm;</title>

  <para>
   When &cephadm; fails to add a new cluster host&mdash;no matter whether by
   means of &cephsalt; (see <xref linkend="deploy-cephadm-configure-minions"/>)
   or manually using the <command>ceph orch host add</command>
   command&mdash;you can diagnose the reason for failure. The failure message
   has the following pattern:
  </para>

<screen>Failed to connect to <replaceable>HOSTNAME</replaceable> (<replaceable>FQDN</replaceable>).</screen>

  <para>
   For example:
  </para>

<screen>
&prompt.cephuser;ceph orch host add node5.example.com
Failed to connect to node5 (node5.example.com).
</screen>

  <para>
   If the message includes a fully qualified domain name (FQDN,
   <literal>node5.example com</literal> in our example) as the
   <replaceable>HOSTNAME</replaceable>, try adding the host using its FQDN:
  </para>

<screen>&prompt.cephuser;ceph orch host add node5.example.com</screen>

  <para>
   If the message includes a short host name (<literal>node5</literal> in our
   example) as the <replaceable>HOSTNAME</replaceable>, try adding the host
   using both its short name and FQDN:
  </para>

<screen>&prompt.cephuser;ceph orch host add node5 node5.example.com</screen>

  <para>
   If either of the previous two commands succeed, &cephsalt; is probably
   causing the problem. Otherwise, try adding the host using its short host
   name and IP address:
  </para>

<screen>&prompt.cephuser;ceph orch host add node5 &wsIIIip;</screen>

  <para>
   If this succeeds, the cluster does not have proper name resolution. Refer to
   <xref linkend="deploy-sles"/> for more details.
  </para>

  <tip>
   <para>
    For more information about resolving full and bare host names via DNS,
    refer to
    <link
    xlink:href="https://docs.ceph.com/en/octopus/cephadm/concepts/"/>.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="bp-troubleshooting-unmanaged-daemons">
  <title>Disabling automatic deployment of daemons</title>

  <para>
   By default, &ceph; daemons are automatically deployed to new hosts after you
   add these hosts to the placement specification and apply it (refer to
   <xref linkend="deploy-core"/> for more details).
  </para>

  <para>
   If you need to disable the automated deployment of &ceph; daemons, add
   <literal>unmanaged: true</literal> to its specification file and apply it,
   for example:
  </para>

<screen>
cat mgr.yaml
service_type: mgr
unmanaged: true
placement:
  label: mgr
[...]

&prompt.cephuser;ceph orch apply -i mgr.yaml
</screen>

  <para>
   After applying this specification, &cephadm; will no longer deploy any new
   daemons on hosts that match the placement specification.
  </para>

  <para>
   To manually deploy a daemon on a new host, run:
  </para>

<screen>ceph orch daemon add <replaceable>DAEMON_TYPE</replaceable> --placement=<replaceable>PLACEMENT_SPEC</replaceable></screen>

  <para>
   For example:
  </para>

<screen>&prompt.cephuser;ceph orch daemon add mgr --placement=ses-node3</screen>

  <para>
   To manually remove a daemon, run:
  </para>

<screen>ceph orch daemon rm <replaceable>DAEMON_NAME</replaceable> [--force]</screen>

  <para>
   For example:
  </para>

<screen>&prompt.cephuser;ceph orch daemon rm mgr.ses-node3.xietsy</screen>
 </sect1>
</chapter>
