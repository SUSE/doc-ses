<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.tips">
 <title>Hints and Tips</title>
 <para>
  The chapter provides information to help you enhance performance of your
  &ceph; cluster and provides tips how to set the cluster up.
 </para>
 <sect1 xml:id="Cluster_Time_Setting">
  <title>Time Synchronization of Nodes</title>

  <para>
   &ceph; requires precise time synchronization between particular nodes. You
   should set up a node with your own NTP server. Even though you can point all
   ntpd instances to a remote public time server, we do not recommend it with
   &ceph;. With such a configuration, each node in the cluster has its own NTP
   daemon that communicate continually over the Internet with a set of three or
   four time servers, all of which are quite some hops away. This solution
   introduces a large degree of latency variability that makes it difficult or
   impossible to keep the clock drift under 0.05 seconds (which is what the
   &ceph; monitors require).
  </para>

  <para>
   Thus use a single machine as the NTP server for the whole cluster. Your NTP
   server ntpd instance may then point to the remote (public) NTP server or it
   can have its own time source. The ntpd instances on all nodes are then
   pointed to this local server. Such a solution has several advantages
   like&mdash;eliminating unnecessary network traffic and clock skews,
   decreasing load on the public NTP servers. For details how to set up the NTP
   server refer to
   <link xlink:href="https://www.suse.com/documentation/sled11/book_sle_admin/data/cha_netz_xntp.html">&sls;
   Administration Guide</link>.
  </para>

  <para>
   Then to change the time on your cluster, do the following:
  </para>

  <important>
   <title>Setting Time</title>
   <para>
    You may face a situation when you need to set the time back, e.g. if the
    time changes from the summer to the standard time. We do not recommend to
    move the time backward for a longer period than the cluster is down. Moving
    the time forward does not cause any trouble.
   </para>
  </important>

  <procedure>
   <title>Time Synchronization on the Cluster</title>
   <step>
    <para>
     Stop all clients accessing the &ceph; cluster, especially those using
     iSCSI.
    </para>
   </step>
   <step>
    <para>
     Shut down your &ceph; cluster. On each node run:
    </para>
<screen>systemctl stop ceph.target</screen>
    <note>
     <para>
      If you use &ceph; and &ocloud;, stop also the &ocloud;.
     </para>
    </note>
   </step>
   <step>
    <para>
     Verify that your NTP server is set up correctly&mdash;all ntpd daemons get
     their time from a source or sources in the local network.
    </para>
   </step>
   <step>
    <para>
     Set the correct time on your NTP server.
    </para>
   </step>
   <step>
    <para>
     Verify that NTP is running and working properly, on all nodes run:
    </para>
<screen>status ntpd.service</screen>
    <para>
     or
    </para>
<screen>ntpq -p</screen>
   </step>
   <step>
    <para>
     Start all monitoring nodes and verify that there is no clock skew:
    </para>
<screen>systemctl start <replaceable>target</replaceable></screen>
   </step>
   <step>
    <para>
     Start all OSD nodes.
    </para>
   </step>
   <step>
    <para>
     Start other &ceph; services.
    </para>
   </step>
   <step>
    <para>
     Start the &ocloud; if you have it.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells &ceph; how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given rule set, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written&mdash;you can increase their weight to
   have &ceph; write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint.fds_inc">
  <title>Increasing File Descriptors</title>

  <para>
   For OSD daemons, the read/write operations are critical to keep the &ceph;
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </para>

  <para>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <filename>/etc/ceph/ceph.conf</filename>, for example:
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   After you change <option>max_open_files</option>, you need to restart the
   OSD service on the relevant &ceph; node.
  </para>
 </sect1>
 <sect1 xml:id="bp.osd_on_exisitng_partitions">
  <title>How to Use Existing Partitions for OSDs Including OSD Journals</title>

  <important>
   <para>
    This section describes an advanced topic that only storage experts and
    developers should examine. It is mostly needed when using non-standard OSD
    journal sizes. If the OSD partition's size is less than 10GB, its initial
    weight is rounded to 0 and because no data are therefore placed on it, you
    should increase its weight. We take no responsibility for overfilled
    journals.
   </para>
  </important>

  <para>
   If you need to use existing disk partitions as an OSD node, the OSD journal
   and data partitions need to be in a GPT partition table.
  </para>

  <para>
   You need to set the correct partition types to the OSD partitions so that
   <systemitem>udev</systemitem> recognizes them correctly and sets their
   ownership to <literal>ceph:ceph</literal>.
  </para>

  <para>
   For example, to set the partition type for the journal partition
   <filename>/dev/vdb1</filename> and data partition
   <filename>/dev/vdb2</filename>, run the following:
  </para>

<screen>sudo sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/vdb
sudo sgdisk --typecode=2:4fbd7e29-9d25-41b8-afd0-062c0ceff05d /dev/vdb</screen>

  <tip>
   <para>
    The &ceph; partition table types are listed in
    <filename>/usr/lib/udev/rules.d/95-ceph-osd.rules</filename>:
   </para>
<screen>cat /usr/lib/udev/rules.d/95-ceph-osd.rules
# OSD_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER="ceph", GROUP="ceph", MODE="660"

# JOURNAL_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER="ceph", GROUP="ceph", MODE="660"
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage.admin.integration">
  <title>Integration with Virtualization Software</title>

  <sect2 xml:id="storage.bp.integration.kvm">
   <title>Storing &kvm; Disks in &ceph; Cluster</title>
   <para>
    You can create a disk image for &kvm;-driven virtual machine, store it in a
    &ceph; pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <command>qemu-kvm</command> making use of
    the disk image stored in the cluster. For more detailed information, see
    <xref linkend="cha.ceph.kvm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.libvirt">
   <title>Storing &libvirt; Disks in &ceph; Cluster</title>
   <para>
    Similar to &kvm; (see <xref linkend="storage.bp.integration.kvm"/>), you
    can use &ceph; to store virtual machines driven by &libvirt;. The advantage
    is that you can run any &libvirt;-supported virtualization solution, such
    as &kvm;, &xen;, or LXC. For more information, see
    <xref linkend="cha.ceph.libvirt"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.xen">
   <title>Storing &xen; Disks in &ceph; Cluster</title>
   <para>
    One way to use &ceph; for storing &xen; disks is to make use of &libvirt;
    as described in <xref linkend="cha.ceph.libvirt"/>.
   </para>
   <para>
    Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
    block device driver directly:
   </para>
   <procedure>
    <step>
     <para>
      If you have no disk image prepared for &xen;, create a new one:
     </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      List images in the pool <literal>mypool</literal> and check if your new
      image is there:
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Create a new block device by mapping the <literal>myimage</literal> image
      to the <systemitem>rbd</systemitem> kernel module:
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
      <para>
       or
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen><command>rbd showmapped</command>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Now you can configure &xen; to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <command>xl</command>-style domain configuration file:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.net.firewall">
  <title>Firewall Settings for &ceph;</title>

  <para>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <menuchoice><guimenu>&yast;</guimenu><guimenu>Security and
   Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
   Services</guimenu></menuchoice>.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     For &ceph; monitor nodes, enable the "Ceph MON" service (port 6789).
    </para>
   </listitem>
   <listitem>
    <para>
     For &ceph; OSD (or MDS) nodes, enable the "Ceph OSD/MDS" service (ports
     6800-7300).
    </para>
   </listitem>
   <listitem>
    <para>
     For &igw;, open port 3260.
    </para>
   </listitem>
   <listitem>
    <para>
     For &rgw;, open the port where &rgw; communication occurs. It is set in
     <filename>/etc/ceph.conf</filename> on the line starting with <literal>rgw
     frontends =</literal>. Default is 80.
    </para>
   </listitem>
   <listitem>
    <para>
     For &ganesha;, open ports 2049 (NFS service) and 875 (Rquota support).
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
