<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.tips">
 <title>Hints and Tips</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The chapter provides information to help you enhance performance of your
  &ceph; cluster and provides tips how to set the cluster up.
 </para>
 <sect1 xml:id="tips.orphaned.partitions">
  <title>Identifying Orphaned Partitions</title>

  <para>
   To identify possibly orphaned journal/WAL/DB devices, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Pick the device that may have orphaned partitions and save the list of its
     partitions to a file:
    </para>
<screen>
&prompt.sminion;ls /dev/sdd?* &gt; /tmp/partitions
</screen>
   </step>
   <step>
    <para>
     Run <command>readlink</command> against all block.wal, block.db, and
     journal devices, and compare the output to the previously saved list of
     partitions:
    </para>
<screen>
&prompt.sminion;readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
    <para>
     The output is the list of partitions that are <emphasis>not</emphasis>
     used by &ceph;.
    </para>
   </step>
   <step>
    <para>
     Remove the orphaned partitions that do not belong to &ceph; with your
     preferred command (for example <command>fdisk</command>,
     <command>parted</command>, or <command>sgdisk</command>).
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="tips.scrubbing">
  <title>Adjusting Scrubbing</title>

  <para>
   By default, &ceph; performs light scrubbing (find more details in
   <xref linkend="scrubbing"/>) daily and deep scrubbing weekly.
   <emphasis>Light</emphasis> scrubbing checks object sizes and checksums to
   ensure that placement groups are storing the same object data.
   <emphasis>Deep</emphasis> scrubbing checks an objectâ€™s content with that
   of its replicas to ensure that the actual contents are the same. The price
   for checking data integrity is increased I/O load on the cluster during the
   scrubbing procedure.
  </para>

  <para>
   The default settings allow &osd;s to initiate scrubbing at inappropriate
   times, such as during periods of heavy loads. Customers may experience
   latency and poor performance when scrubbing operations conflict with their
   operations. &ceph; provides several scrubbing settings that can limit
   scrubbing to periods with lower loads or during off-peak hours.
  </para>

  <para>
   If the cluster experiences high loads during the day and low loads late at
   night, consider restricting scrubbing to night time hours, such as 11pm till
   6am:
  </para>

<screen>
[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6
</screen>

  <para>
   If time restriction is not an effective method of determining a scrubbing
   schedule, consider using the <option>osd_scrub_load_threshold</option>
   option. The default value is 0.5, but it could be modified for low load
   conditions:
  </para>

<screen>
[osd]
osd_scrub_load_threshold = 0.25
</screen>
 </sect1>
 <sect1 xml:id="tips.stopping_osd_without_rebalancing">
  <title>Stopping OSDs without Rebalancing</title>

  <para>
   You may need to stop OSDs for maintenance periodically. If you do not want
   CRUSH to automatically rebalance the cluster in order to avoid huge data
   transfers, set the cluster to <literal>noout</literal> first:
  </para>

<screen>
&prompt.sminion;ceph osd set noout
</screen>

  <para>
   When the cluster is set to <literal>noout</literal>, you can begin stopping
   the OSDs within the failure domain that requires maintenance work:
  </para>

<screen>
&prompt.sminion;systemctl stop ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   Find more information in
   <xref linkend="ceph.operating.services.individual"/>.
  </para>

  <para>
   After you complete the maintenance, start OSDs again:
  </para>

<screen>
&prompt.sminion;systemctl start ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   After OSD services are started, unset the cluster from
   <literal>noout</literal>:
  </para>

<screen>
&prompt.cephuser;ceph osd unset noout
</screen>
 </sect1>
 <sect1 xml:id="Cluster_Time_Setting">
  <title>Time Synchronization of Nodes</title>

  <para>
   &ceph; requires precise time synchronization between all nodes.
  </para>

  <para>
   We recommend synchronizing all &ceph; cluster nodes with at least three
   reliable time sources that are located on the internal network. The internal
   time sources can point to a public time server or have their own time
   source.
  </para>

  <important>
   <title>Public Time Servers</title>
   <para>
    Do not synchronize all &ceph; cluster nodes directly with remote public
    time servers. With such a configuration, each node in the cluster has its
    own NTP daemon that communicates continually over the Internet with a set
    of three or four time servers that may provide slightly different time.
    This solution introduces a large degree of latency variability that makes
    it difficult or impossible to keep the clock drift under 0.05 seconds which
    is what the &ceph; monitors require.
   </para>
  </important>

  <para>
   For details how to set up the NTP server refer to
   <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_ntp.html">&sls;
   Administration Guide</link>.
  </para>

  <para>
   Then to change the time on your cluster, do the following:
  </para>

  <important>
   <title>Setting Time</title>
   <para>
    You may face a situation when you need to set the time back, for example if
    the time changes from the summer to the standard time. We do not recommend
    to move the time backward for a longer period than the cluster is down.
    Moving the time forward does not cause any trouble.
   </para>
  </important>

  <procedure>
   <title>Time Synchronization on the Cluster</title>
   <step>
    <para>
     Stop all clients accessing the &ceph; cluster, especially those using
     iSCSI.
    </para>
   </step>
   <step>
    <para>
     Shut down your &ceph; cluster. On each node run:
    </para>
<screen>&prompt.root;systemctl stop ceph.target</screen>
    <note>
     <para>
      If you use &ceph; and &ocloud;, stop also the &ocloud;.
     </para>
    </note>
   </step>
   <step>
    <para>
     Verify that your NTP server is set up correctly&mdash;all
     <systemitem class="daemon">chronyd</systemitem> daemons get their time
     from a source or sources in the local network.
    </para>
   </step>
   <step>
    <para>
     Set the correct time on your NTP server.
    </para>
   </step>
   <step>
    <para>
     Verify that NTP is running and working properly, on all nodes run:
    </para>
<screen>&prompt.root;systemctl status chronyd.service</screen>
   </step>
   <step>
    <para>
     Start all monitoring nodes and verify that there is no clock skew:
    </para>
<screen>&prompt.root;systemctl start <replaceable>target</replaceable></screen>
   </step>
   <step>
    <para>
     Start all OSD nodes.
    </para>
   </step>
   <step>
    <para>
     Start other &ceph; services.
    </para>
   </step>
   <step>
    <para>
     Start the &ocloud; if you have it.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells &ceph; how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given ruleset, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written&mdash;you can increase their weight to
   have &ceph; write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.tips.ceph_btrfs_subvol">
  <title>Btrfs Subvolume for <filename>/var/lib/ceph</filename> on &mon; Nodes</title>

  <para>
   &sle; is by default installed on a Btrfs partition. &mon;s store their state
   and database in the <filename>/var/lib/ceph</filename> directory. To prevent
   corruption of a &mon; from a system rollback of a previous snapshot, create
   a Btrfs subvolume for <filename>/var/lib/ceph</filename>. A dedicated
   subvolume excludes the monitor data from snapshots of the root subvolume.
  </para>

  <para>
   Therefore it is necessary to create a Btrfs subvolume for
   <filename>/var/lib/ceph</filename> and exclude it from snapshots and
   rollbacks. You need to create the subvolume before running &deepsea; stage.0
   because stage.0 installs &ceph; related packages and creates the
   <filename>/var/lib/ceph</filename> directory.
  </para>

  <para>
   &deepsea; stage.3 then verifies whether <filename>@/var/lib/ceph</filename>
   is a Btrfs subvolume and fails if it is a normal directory.
  </para>

  <sect2 xml:id="btrfs_subvol.requirements">
   <title>Requirements</title>
   <sect3 xml:id="tips.ceph_btrfs_subvol.new">
    <title>New Deployments</title>
    <para>
     &salt; and &deepsea; need to be properly installed and working.
    </para>
   </sect3>
   <sect3 xml:id="storage.tips.ceph_btrfs_subvol.req-existing">
    <title>Existing Deployments</title>
    <para>
     If your cluster is already installed, the following requirements must be
     met:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nodes are upgraded to &productname; &productnumber; and cluster is under
       &deepsea; control.
      </para>
     </listitem>
     <listitem>
      <para>
       &ceph; cluster is up and healthy.
      </para>
     </listitem>
     <listitem>
      <para>
       Upgrade process has synchronized &salt; and &deepsea; modules to all
       minion nodes.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="btrfs_subvol.upgrades">
   <title>Steps Required during &ceph; Cluster Upgrade</title>
   <para>
    On &productname; &prevproductnumber;, the <filename>/var</filename>
    directory is not on a Btrfs subvolume but its subfolders&mdash;such as
    <filename>/var/log</filename> or <filename>/var/cache</filename>) are Btrfs
    subvolumes under '@'. Creating <filename>@/var/lib/ceph</filename>
    subvolumes requires mounting the '@' subvolume first (it is not mounted by
    default) and creating <filename>@/var/lib/ceph</filename> subvolume under
    it.
   </para>
   <para>
    Following are example commands that illustrate the process:
   </para>
<screen>
&prompt.root;mkdir -p /mnt/btrfs
&prompt.root;mount -o subvol=@ <replaceable>ROOT_DEVICE</replaceable> /mnt/btrfs
&prompt.root;btrfs subvolume create /mnt/btrfs/var/lib/ceph
&prompt.root;umount /mnt/btrfs
</screen>
   <para>
    At this point the <filename> @/var/lib/ceph</filename> subvolume is created
    and you can continue as described in
    <xref linkend="storage.tips.ceph_btrfs_subvol.automatic"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.tips.ceph_btrfs_subvol.automatic">
   <title>Automatic Setup</title>
   <para>
    Prior to running &deepsea; stage.0, apply the following commands to each of
    the &sminion;s that will become &mon;s:
   </para>
<screen>
&prompt.smaster;salt '<replaceable>MONITOR_NODES</replaceable>' saltutil.sync_all
&prompt.smaster;salt '<replaceable>MONITOR_NODES</replaceable>' state.apply ceph.subvolume
</screen>
   <para>
    The <command>ceph.subvolume</command> command does the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Creates <filename>/var/lib/ceph</filename> as a
      <literal>@/var/lib/ceph</literal> Btrfs subvolume.
     </para>
    </listitem>
    <listitem>
     <para>
      Mount the new subvolume and update <filename>/etc/fstab</filename>
      appropriately.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="var_lib_ceph.subvol.manual">
   <title>Manual Setup</title>
   <para>
    Automatic setup of the <filename>@/var/lib/ceph</filename> Btrfs subvolume
    on the &mon; nodes may not be suitable for all scenarios. You can migrate
    your <filename>/var/lib/ceph</filename> directory to a
    <filename>@/var/lib/ceph</filename> subvolume by following these steps:
   </para>
   <procedure>
    <step>
     <para>
      Terminate running &ceph; processes.
     </para>
    </step>
    <step>
     <para>
      Unmount OSDs on the node.
     </para>
    </step>
    <step>
     <para>
      Create <literal>@/var/lib/ceph</literal> Btrfs subvolume and migrate
      existing <filename>/var/lib/ceph</filename> data there.
     </para>
    </step>
    <step>
     <para>
      Mount the new subvolume and update <filename>/etc/fstab</filename>
      appropriately.
     </para>
    </step>
    <step>
     <para>
      Remount OSDs.
     </para>
    </step>
    <step>
     <para>
      Restart &ceph; daemons.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="var_lib_ceph.subvol.moreinfo">
   <title>For More Information</title>
   <para>
    Find more detailed information about manual setup in the file
    <filename>/srv/salt/ceph/subvolume/README.md</filename> on the &smaster;
    node.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint.fds_inc">
  <title>Increasing File Descriptors</title>

  <para>
   For OSD daemons, the read/write operations are critical to keep the &ceph;
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </para>

  <para>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <filename>/etc/ceph/ceph.conf</filename>, for example:
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   After you change <option>max_open_files</option>, you need to restart the
   OSD service on the relevant &ceph; node.
  </para>
 </sect1>
 <sect1 xml:id="storage.admin.integration">
  <title>Integration with Virtualization Software</title>

  <sect2 xml:id="storage.bp.integration.kvm">
   <title>Storing &kvm; Disks in &ceph; Cluster</title>
   <para>
    You can create a disk image for &kvm;-driven virtual machine, store it in a
    &ceph; pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <command>qemu-kvm</command> making use of
    the disk image stored in the cluster. For more detailed information, see
    <xref linkend="cha.ceph.kvm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.libvirt">
   <title>Storing &libvirt; Disks in &ceph; Cluster</title>
   <para>
    Similar to &kvm; (see <xref linkend="storage.bp.integration.kvm"/>), you
    can use &ceph; to store virtual machines driven by &libvirt;. The advantage
    is that you can run any &libvirt;-supported virtualization solution, such
    as &kvm;, &xen;, or LXC. For more information, see
    <xref linkend="cha.ceph.libvirt"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.xen">
   <title>Storing &xen; Disks in &ceph; Cluster</title>
   <para>
    One way to use &ceph; for storing &xen; disks is to make use of &libvirt;
    as described in <xref linkend="cha.ceph.libvirt"/>.
   </para>
   <para>
    Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
    block device driver directly:
   </para>
   <procedure>
    <step>
     <para>
      If you have no disk image prepared for &xen;, create a new one:
     </para>
<screen>&prompt.cephuser;rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      List images in the pool <literal>mypool</literal> and check if your new
      image is there:
     </para>
<screen>&prompt.cephuser;rbd list mypool</screen>
    </step>
    <step>
     <para>
      Create a new block device by mapping the <literal>myimage</literal> image
      to the <systemitem>rbd</systemitem> kernel module:
     </para>
<screen>&prompt.cephuser;rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
      <para>
       or
      </para>
<screen>&cephuser;rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen><command>rbd showmapped</command>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Now you can configure &xen; to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <command>xl</command>-style domain configuration file:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.net.firewall">
  <title>Firewall Settings for &ceph;</title>

  <warning>
   <title>&deepsea; Stages Fail with Firewall</title>
   <para>
    &deepsea; deployment stages fail when firewall is active (and even
    configured). To pass the stages correctly, you need to either turn the
    firewall off by running
   </para>
<screen>
&prompt.root;systemctl stop SuSEfirewall2.service
</screen>
   <para>
    or set the <option>FAIL_ON_WARNING</option> option to 'False' in
    <filename>/srv/pillar/ceph/stack/global.yml</filename>:
   </para>
<screen>
FAIL_ON_WARNING: False
</screen>
  </warning>

  <para>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <menuchoice><guimenu>&yast;</guimenu><guimenu>Security and
   Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
   Services</guimenu></menuchoice>.
  </para>

  <para>
   Following is a list of &ceph; related services and numbers of the ports that
   they normally use:
  </para>

  <variablelist>
   <varlistentry>
    <term>&mon;</term>
    <listitem>
     <para>
      Enable the <guimenu>Ceph MON</guimenu> service or port 6789 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&osd; or &mds;</term>
    <listitem>
     <para>
      Enable the <guimenu>Ceph OSD/MDS</guimenu> service, or ports 6800-7300
      (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&igw;</term>
    <listitem>
     <para>
      Open port 3260 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&rgw;</term>
    <listitem>
     <para>
      Open the port where &rgw; communication occurs. It is set in
      <filename>/etc/ceph.conf</filename> on the line starting with
      <literal>rgw frontends =</literal>. Default is 80 for HTTP and 443 for
      HTTPS (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&ganesha;</term>
    <listitem>
     <para>
      By default, &ganesha; uses ports 2049 (NFS service, TCP) and 875 (rquota
      support, TCP). Refer to <xref linkend="ganesha.nfsport"/> for more
      information on changing the default &ganesha; ports.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Apache based services, such as &smt;, or &susemgr;</term>
    <listitem>
     <para>
      Open ports 80 for HTTP and 443 for HTTPS (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>
      Open port 22 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Open port 123 (UDP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&salt;</term>
    <listitem>
     <para>
      Open ports 4505 and 4506 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&grafana;</term>
    <listitem>
     <para>
      Open port 3000 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&prometheus;</term>
    <listitem>
     <para>
      Open port 9100 (TCP).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.bp.network_test">
  <title>Testing Network Performance</title>

  <para>
   To test the network performance, &deepsea;'s <literal>net</literal> runner
   provides the following commands:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A simple ping to all nodes:
    </para>
<screen>&prompt.smaster;<command>salt-run</command> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</screen>
   </listitem>
   <listitem>
    <para>
     A jumbo ping to all nodes:
    </para>
<screen>&prompt.smaster;<command>salt-run</command> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</screen>
   </listitem>
   <listitem>
    <para>
     A bandwidth test:
    </para>
<screen>&prompt.smaster;<command>salt-run</command> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</screen>
    <tip>
     <title>Stop 'iperf3' Processes Manually</title>
     <para>
      When running a test using the <command>net.iperf</command> runner, the
      'iperf3' server processes that are started do not stop automatically when
      a test is completed. To stop the processes, use the following runner:
     </para>
<screen>&prompt.smaster;salt '*' multi.kill_iperf_cmd</screen>
    </tip>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="bp.flash_led_lights">
  <title>How to Locate Physical Disks using LED Lights</title>

  <para>
   This section describes using <systemitem>libstoragemgmt</systemitem> and/or
   third party tools to adjust the LED lights on physical disks. This
   capability may not be available for all hardware platforms.
  </para>

  <para>
   Matching an OSD disk to a physical disk can be challenging, especially on
   nodes with a high density of disks. Some hardware environments include LED
   lights than can be adjusted via software to flash or illuminate a different
   color for identification purposes. &productname; offers support for this
   capability through &salt;, <systemitem>libstoragemgmt</systemitem> and third
   party tools specific to the hardware in use. The configuration for this
   capability is defined in the
   <filename>/srv/pillar/ceph/disk_led.sls</filename> &spillar;:
  </para>

<screen>&prompt.root; cat /srv/pillar/ceph/disk_led.sls
# This is the default configuration for the storage enclosure LED blinking.
# The placeholder {device_file} will be replaced with the device file of
# the disk when the command is executed.
#
# Have a look into the /srv/pillar/ceph/README file to find out how to
# customize this configuration per minion/host.

disk_led:
  cmd:
    ident:
      'on': lsmcli local-disk-ident-led-on --path '{device_file}'
      'off': lsmcli local-disk-ident-led-off --path '{device_file}'
    fault:
      'on': lsmcli local-disk-fault-led-on --path '{device_file}'
      'off': lsmcli local-disk-fault-led-off --path '{device_file}'</screen>

  <para>
   The default configuration for <filename>disk_led.sls</filename> offers disk
   LED support through the <systemitem>libstoragemgmt</systemitem> layer.
   However, <systemitem>libstoragemgmt</systemitem> provides this support
   through a hardware-specific plugin and third party tools. Unless both the
   <systemitem>libstoragemgmt</systemitem> plugin and the third party tools
   appropriate for the hardware are installed,
   <systemitem>libstoragemgmt</systemitem> will not be able to adjust the LEDs.
  </para>

  <para>
   With or without <systemitem>libstoragemgmt</systemitem>, third party tools
   may be required to adjust LED lights. These third party tools are available
   through various hardware vendors. Some of the common vendors and tools are:
  </para>

  <table>
   <title>Third Party Storage Tools</title>
<?dbhtml table-width="50%" ?>
<?dbfo table-width="50%" ?>
   <tgroup cols="2">
    <thead>
     <row>
      <entry>Vendor/Disk Controller</entry>
      <entry>Tool</entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry>HPE SmartArray</entry>
      <entry>hpssacli</entry>
     </row>
     <row>
      <entry>LSI MegaRAID</entry>
      <entry>storcli</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   &sls; also provides the <package>ledmon</package> package, and
   <command>ledctl</command> tool. This tool may also work for hardware
   environments utilizing Intel storage enclosures. Proper syntax when using
   this tool is as follows:
  </para>

<screen>&prompt.root; cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'
    fault:
      'on': ledctl locate='{device_file}'
      'off': ledctl locate_off='{device_file}'</screen>

  <para>
   If you are on supported hardware, with all required third party tools, LEDs
   can be enabled or disabled using the following command syntax from the
   &smaster; node:
  </para>

<screen>
&prompt.root;salt-run disk_led.device <replaceable>NODE</replaceable> <replaceable>DISK</replaceable> <replaceable>fault|ident</replaceable> <replaceable>on|off</replaceable>
</screen>

  <para>
   For example, to enable or disable LED identification or fault lights on
   <filename>/dev/sdd</filename> on OSD node <filename>srv16.ceph</filename>,
   run the following:
  </para>

<screen>&prompt.root;salt-run disk_led.device srv16.ceph sdd ident on
&prompt.root;salt-run disk_led.device srv16.ceph sdd ident off
&prompt.root;salt-run disk_led.device srv16.ceph sdd fault on
&prompt.root;salt-run disk_led.device srv16.ceph sdd fault off</screen>

  <note>
   <title>Device naming</title>
   <para>
    The device name used in the <command>salt-run</command> command needs to
    match the name recognized by salt. The following command can be used to
    display these names:
   </para>
<screen>&prompt.smaster;salt '<replaceable>minion_name</replaceable>' grains.get disks
</screen>
  </note>

  <para>
   In many environments, the <filename>/srv/pillar/ceph/disk_led.sls</filename>
   configuration will require changes in order to adjust the LED lights for
   specific hardware needs. Simple changes may be performed through replacing
   <command>lsmcli</command> with another tool, or adjusting command line
   parameters. Complex changes may be accomplished through calling an external
   script in place of the <filename>lsmcli</filename> command. When making any
   changes to <filename>/srv/pillar/ceph/disk_led.sls</filename>, follow these
   steps:
  </para>

  <procedure>
   <step>
    <para>
     Make required changes to
     <filename>/srv/pillar/ceph/disk_led.sls</filename> on the &smaster; node.
    </para>
   </step>
   <step>
    <para>
     Verify that the changes are reflected correctly in the pillar data:
    </para>
<screen>&prompt.root;salt '<replaceable>SALT MASTER</replaceable>*' pillar.get disk_led</screen>
   </step>
   <step>
    <para>
     Refresh the pillar data on all nodes using:
    </para>
<screen>&prompt.root;salt '*' saltutil.pillar_refresh</screen>
   </step>
  </procedure>

  <para>
   It is possible to use an external script to directly use third-party tools
   to adjust LED lights. The following examples show how to adjust
   <filename>/srv/pillar/ceph/disk_led.sls</filename> to support an external
   script, and two sample scripts for HP and LSI environments.
  </para>

  <para>
   Modified <filename>/srv/pillar/ceph/disk_led.sls</filename> which calls an
   external script:
  </para>

<screen>&prompt.root;cat /srv/pillar/ceph/disk_led.sls
disk_led:
  cmd:
    ident:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off
    fault:
      'on': /usr/local/bin/flash_led.sh '{device_file}' on
      'off': /usr/local/bin/flash_led.sh '{device_file}' off</screen>

  <para>
   Sample script for flashing LED lights on HP hardware using the
   <systemitem>hpssacli</systemitem> utilities:
  </para>

<screen>&prompt.root;cat /usr/local/bin/flash_led_hp.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

FOUND=0
MAX_CTRLS=10
MAX_DISKS=50

for i in $(seq 0 $MAX_CTRLS); do
  # Search for valid controllers
  if hpssacli ctrl slot=$i show summary >/dev/null; then
    # Search all disks on the current controller
    for j in $(seq 0 $MAX_DISKS); do
      if hpssacli ctrl slot=$i ld $j show | grep -q $1; then
        FOUND=1
        echo "Found $1 on ctrl=$i, ld=$j. Turning LED $2."
        hpssacli ctrl slot=$i ld $j modify led=$2
        break;
      fi
    done
    [[ "$FOUND" = "1" ]] &amp;&amp; break
  fi
done</screen>

  <para>
   Sample script for flashing LED lights on LSI hardware using the
   <systemitem>storcli</systemitem> utilities:
  </para>

<screen>&prompt.root;cat /usr/local/bin/flash_led_lsi.sh
#!/bin/bash
# params:
#   $1 device (e.g. /dev/sda)
#   $2 on|off

[[ "$2" = "on" ]] &amp;&amp; ACTION="start" || ACTION="stop"

# Determine serial number for the disk
SERIAL=$(lshw -class disk | grep -A2 $1 | grep serial | awk '{print $NF}')
if [ ! -z "$SERIAL" ]; then
  # Search for disk serial number across all controllers and enclosures
  DEVICE=$(/opt/MegaRAID/storcli/storcli64 /call/eall/sall show all | grep -B6 $SERIAL | grep Drive | awk '{print $2}')
  if [ ! -z "$DEVICE" ]; then
    echo "Found $1 on device $DEVICE. Turning LED $2."
    /opt/MegaRAID/storcli/storcli64 $DEVICE $ACTION locate
  else
    echo "Device not found!"
    exit -1
  fi
else
  echo "Disk serial number not found!"
  exit -1
fi</screen>
 </sect1>
</chapter>
