<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-block-storage">
<!-- ============================================================== -->
 <title>&blockstore;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &blockstore; allows a single pod to mount storage. This guide shows how to
  create a simple, multi-tier web application on &kube; using persistent
  volumes enabled by &rook;.
 </para>
 <sect1 xml:id="rook-block-storage-provision-storage">
  <title>Provision Storage</title>

  <para>
   Before &rook; can provision storage, a <literal>StorageClass</literal> and a
   <literal>CephBlockPool</literal> need to be created. This will allow &kube;
   to interoperate with &rook; when provisioning persistent volumes.
  </para>

  <note>
   <para>
    This sample requires <emphasis>at least 1 OSD per node</emphasis>, with
    each OSD located on <emphasis>3 different nodes</emphasis>.
   </para>
  </note>

  <para>
   Each OSD must be located on a different node, because the
   <link xlink:href="ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
   is set to <literal>host</literal> and the <literal>replicated.size</literal>
   is set to <literal>3</literal>.
  </para>

  <note>
   <para>
    This example uses the CSI driver, which is the preferred driver going
    forward for K8s 1.13 and newer. Examples are found in the
    <link xlink:href="https://github.com/rook/rook/tree/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/csi/rbd">CSI
    RBD</link> directory.
   </para>
  </note>

  <para>
   Save this <literal>StorageClass</literal> definition as
   <literal>storageclass.yaml</literal>:
  </para>

<screen>
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
# Change &quot;rook-ceph&quot; provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # &ceph; pool into which the RBD image shall be created
    pool: replicapool

    # RBD image format. Defaults to &quot;2&quot;.
    imageFormat: &quot;2&quot;

    # RBD image features. Available for imageFormat: &quot;2&quot;. CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain &ceph; admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
</screen>

  <para>
   If you have deployed the &rook; operator in a namespace other than
   <quote>rook-ceph</quote>, change the prefix in the provisioner to match the
   namespace you used. For example, if the &rook; operator is running in the
   namespace <quote>my-namespace</quote> the provisioner value should be
   <quote>my-namespace.rbd.csi.ceph.com</quote>.
  </para>

  <para>
   Create the storage class.
  </para>

<screen>kubectl create -f cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml</screen>

  <note>
   <para>
    As
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">specified
    by &kube;</link>, when using the <literal>Retain</literal> reclaim policy,
    any &ceph; RBD image that is backed by a
    <literal>PersistentVolume</literal> will continue to exist even after the
    <literal>PersistentVolume</literal> has been deleted. These &ceph; RBD
    images will need to be cleaned up manually using <literal>rbd rm</literal>.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="consume-the-storage-wordpress-sample">
  <title>Consume the storage: Wordpress sample</title>

  <para>
   We create a sample app to consume the block storage provisioned by &rook;
   with the classic wordpress and mysql apps. Both of these apps will make use
   of block volumes provisioned by &rook;.
  </para>

  <para>
   Start mysql and wordpress from the
   <literal>cluster/examples/kubernetes</literal> folder:
  </para>

<screen>
        kubectl create -f mysql.yaml
        kubectl create -f wordpress.yaml
      </screen>

  <para>
   Both of these apps create a block volume and mount it to their respective
   pod. You can see the &kube; volume claims by running the following:
  </para>

<screen>
        $ kubectl get pvc
        NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
        mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
        wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
      </screen>

  <para>
   Once the wordpress and mysql pods are in the <literal>Running</literal>
   state, get the cluster IP of the wordpress app and enter it in your browser:
  </para>

<screen>
        $ kubectl get svc wordpress
        NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
        wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m
      </screen>

  <para>
   You should see the wordpress app running.
  </para>

  <para>
   If you are using Minikube, the Wordpress URL can be retrieved with this
   one-line command:
  </para>

<screen>
        echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')
      </screen>

  <note>
   <para>
    When running in a vagrant environment, there will be no external IP address
    to reach wordpress with. You will only be able to reach wordpress via the
    <literal>CLUSTER-IP</literal> from inside the &kube; cluster.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="consume-the-storage-toolbox">
  <title>Consume the Storage: Toolbox</title>

  <para>
   With the pool that was created above, we can also create a block image and
   mount it directly in a pod.
  </para>
 </sect1>
 <sect1 xml:id="rook-block-storage-teardown">
  <title>Teardown</title>

  <para>
   To clean up all the artifacts created by the block demo:
  </para>

<screen>
kubectl delete -f wordpress.yaml
kubectl delete -f mysql.yaml
kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
kubectl delete storageclass rook-ceph-block
</screen>
 </sect1>
 <sect1 xml:id="advanced-example-erasure-coded-block-storage">
  <title>Advanced Example: Erasure Coded Block Storage</title>

  <para>
   If you want to use erasure coded pool with RBD, your OSDs must use
   <literal>bluestore</literal> as their <literal>storeType</literal>.
   Additionally the nodes that are going to mount the erasure coded RBD block
   storage must have Linux kernel &gt;= <literal>4.11</literal>.
  </para>

  <para>
   This example requires <emphasis>at least 3 bluestore OSDs</emphasis>, with
   each OSD located on a <emphasis>different node</emphasis>.
  </para>

  <para>
   The OSDs must be located on different nodes, because the
   <literal>failureDomain</literal> is set to <literal>host</literal> and the
   <literal>erasureCoded</literal> chunk settings require at least 3 different
   OSDs (2 <literal>dataChunks</literal> + 1 <literal>codingChunks</literal>).
  </para>

  <para>
   To be able to use an erasure coded pool you need to create two pools (as
   seen below in the definitions): one erasure coded and one replicated. &gt;
   This example requires <emphasis>at least 3 bluestore OSDs</emphasis>, with
   each OSD located on a <emphasis>different node</emphasis>.
  </para>

  <para>
   The OSDs must be located on different nodes, because the
   <literal>failureDomain</literal> is set to <literal>host</literal> and the
   <literal>erasureCoded</literal> chunk settings require at least 3 different
   OSDs (2 <literal>dataChunks</literal> + 1 <literal>codingChunks</literal>).
  </para>

  <sect2 xml:id="erasure-coded-csi-driver">
   <title>Erasure Coded CSI Driver</title>
   <para>
    The erasure coded pool must be set as the <literal>dataPool</literal>
    parameter in
    <link xlink:href="https://github.com/rook/rook/blob/%7B%7B%20branchName%20%7D%7D/cluster/examples/kubernetes/ceph/csi/rbd/storage-class-ec.yaml"><literal>storageclass-ec.yaml</literal></link>
    It is used for the data of the RBD images.
   </para>
  </sect2>
 </sect1>
</chapter>
