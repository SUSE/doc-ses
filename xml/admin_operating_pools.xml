<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-pools">
 <title>Manage storage pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, &ceph; uses
  the default pools for storing data. The following important highlights relate
  to &ceph; pools:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resilience</emphasis>: &ceph; pools provide resilience by
    replicating or encoding the data contained within them. Each pool can be
    set to either <literal>replicated</literal> or <literal>erasure
    coding</literal>. For replicated pools, you further set the number of
    replicas, or copies, which each data object within the pool will have. The
    number of copies (OSDs, CRUSH buckets/leaves) that can be lost is one less
    than the number of replicas. With erasure coding, you set the values of
    <option>k</option> and <option>m</option>, where <option>k</option> is the
    number of data chunks and <option>m</option> is the number of coding
    chunks. For erasure coded pools, it is the number of coding chunks that
    determines how many OSDs (CRUSH buckets/leaves) can be lost without losing
    data.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Placement Groups</emphasis>: You can set the number of placement
    groups for the pool. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, objects
    and its replicas (or chunks in case of erasure coded pools) are placed
    according to the CRUSH ruleset mapped to the pool. You can create a custom
    CRUSH rule for your pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshots</emphasis>: When you create snapshots with
    <command>ceph osd pool mksnap</command>, you effectively take a snapshot of
    a particular pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </para>
 <sect1 xml:id="ceph-pools-operate-add-pool">
  <title>Creating a pool</title>

  <para>
   A pool can be created as either <literal>replicated</literal> to recover
   from lost OSDs by keeping multiple copies of the objects or
   <literal>erasure</literal> to have generalized RAID 5 or 6 capability.
   Replicated pools require more raw storage, while erasure coded pools require
   less raw storage. The default setting is <literal>replicated</literal>. For
   more information on erasure coded pools, see
   <xref linkend="cha-ceph-erasure"/>.
  </para>

  <para>
   To create a replicated pool, execute:
  </para>

<screen>&prompt.cephuser;ceph osd pool create <replaceable>POOL_NAME</replaceable></screen>

  <note>
   <para>
    The autoscaler will take care of the remaining optional arguments. For more
    information, see <xref linkend="op-pgs-autoscaler"/>.
   </para>
  </note>

  <para>
   To create an erasure coded pool, execute:
  </para>

<screen>&prompt.cephuser;ceph osd pool create <replaceable>POOL_NAME</replaceable> erasure <replaceable>CRUSH_RULESET_NAME</replaceable> \
<replaceable>EXPECTED_NUM_OBJECTS</replaceable></screen>

  <para>
   The <command>ceph osd pool create</command> command can fail if you exceed
   the limit of placement groups per OSD. The limit is set with the option
   <option>mon_max_pg_per_osd</option>.
  </para>

  <variablelist>
   <varlistentry>
    <term>POOL_NAME</term>
    <listitem>
     <para>
      The name of the pool. It must be unique. This option is required.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_TYPE</term>
    <listitem>
     <para>
      The pool type which may either be <literal>replicated</literal> to
      recover from lost OSDs by keeping multiple copies of the objects or
      <literal>erasure</literal> to get a kind of generalized RAID 5
      capability. The replicated pools require more raw storage but implement
      all Ceph operations. The erasure pools require less raw storage but only
      implement a subset of the available operations. The default
      <literal>POOL_TYPE</literal> is <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CRUSH_RULESET_NAME</term>
    <listitem>
     <para>
      The name of the CRUSH ruleset for this pool. If the specified ruleset
      does not exist, the creation of replicated pools will fail with -ENOENT.
      For replicated pools it is the ruleset specified by the <varname>osd pool
      default CRUSH replicated ruleset</varname> configuration variable. This
      ruleset must exist. For erasure pools it is 'erasure-code' if the default
      erasure code profile is used or <replaceable>POOL_NAME</replaceable>
      otherwise. This ruleset will be created implicitly if it does not exist
      already.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>erasure_code_profile=profile</term>
    <listitem>
     <para>
      For erasure coded pools only. Use the erasure code profile. It must be an
      existing profile as defined by <command>osd erasure-code-profile
      set</command>.
     </para>
     <note>
      <para>
       If for any reason the autoscaler has been disabled
       (<literal>pg_autoscale_mode</literal> set to off) on a pool, you can
       calculate and set the PG numbers manually. See <xref linkend="op-pgs"/>
       for details on calculating an appropriate number of placement groups for
       your pool.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>EXPECTED_NUM_OBJECTS</term>
    <listitem>
     <para>
      The expected number of objects for this pool. By setting this value
      (together with a negative <option>filestore merge threshold</option>),
      the PG folder splitting happens at the pool creation time. This avoids
      the latency impact with a runtime folder splitting.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-listing-pools">
  <title>Listing pools</title>

  <para>
   To list your cluster’s pools, execute:
  </para>

<screen>&prompt.cephuser;ceph osd pool ls</screen>
 </sect1>
 <sect1 xml:id="ceph-renaming-pool">
  <title>Renaming a pool</title>

  <para>
   To rename a pool, execute:
  </para>

<screen>&prompt.cephuser;ceph osd pool rename <replaceable>CURRENT_POOL_NAME</replaceable> <replaceable>NEW_POOL_NAME</replaceable></screen>

  <para>
   If you rename a pool and you have per-pool capabilities for an authenticated
   user, you must update the user’s capabilities with the new pool name.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pools-operate-del-pool">
  <title>Deleting a pool</title>

  <warning>
   <title>Pool deletion is not reversible</title>
   <para>
    Pools may contain important data. Deleting a pool causes all data in the
    pool to disappear, and there is no way to recover it.
   </para>
  </warning>

  <para>
   Because inadvertent pool deletion is a real danger, &ceph; implements two
   mechanisms that prevent pools from being deleted. Both mechanisms must be
   disabled before a pool can be deleted.
  </para>

  <para>
   The first mechanism is the <literal>NODELETE</literal> flag. Each pool has
   this flag, and its default value is 'false'. To find out the value of this
   flag on a pool, run the following command:
  </para>

<screen>&prompt.cephuser;ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>

  <para>
   If it outputs <literal>nodelete: true</literal>, it is not possible to
   delete the pool until you change the flag using the following command:
  </para>

<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>

  <para>
   The second mechanism is the cluster-wide configuration parameter <option>mon
   allow pool delete</option>, which defaults to 'false'. This means that, by
   default, it is not possible to delete a pool. The error message displayed
   is:
  </para>

<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>

  <para>
   To delete the pool in spite of this safety setting, you can temporarily set
   <option>mon allow pool delete</option> to 'true', delete the pool, and then
   return the parameter to 'false':
  </para>

<screen>&prompt.cephuser;ceph tell mon.* injectargs --mon-allow-pool-delete=true
&prompt.cephuser;ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
&prompt.cephuser;ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>

  <para>
   The <command>injectargs</command> command displays the following message:
  </para>

<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>

  <para>
   This is merely confirming that the command was executed successfully. It is
   not an error.
  </para>

  <para>
   If you created your own rulesets and rules for a pool you created, you
   should consider removing them when you no longer need your pool.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pool-other-operations">
  <title>Other operations</title>

  <sect2 xml:id="ceph-pools-associate">
   <title>Associating pools with an application</title>
   <para>
    Before using pools, you need to associate them with an application. Pools
    that will be used with &cephfs;, or pools that are automatically created by
    &ogw; are automatically associated.
   </para>
   <para>
    For other cases, you can manually associate a free-form application name
    with a pool:
   </para>
<screen>&prompt.cephuser;ceph osd pool application enable <replaceable>POOL_NAME</replaceable> <replaceable>APPLICATION_NAME</replaceable></screen>
   <tip>
    <title>Default application names</title>
    <para>
     &cephfs; uses the application name <literal>cephfs</literal>, &rbd; uses
     <literal>rbd</literal>, and &ogw; uses <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    A pool can be associated with multiple applications, and each application
    can have its own metadata. To list the application (or applications)
    associated with a pool, issue the following command:
   </para>
<screen>&prompt.cephuser;ceph osd pool application get <replaceable>pool_name</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-set-pool-quotas">
   <title>Setting pool quotas</title>
   <para>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>POOL_NAME</replaceable> <replaceable>MAX_OBJECTS</replaceable> <replaceable>OBJ_COUNT</replaceable> <replaceable>MAX_BYTES</replaceable> <replaceable>BYTES</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    To remove a quota, set its value to 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph-showing-pool-statistics">
   <title>Showing pool statistics</title>
   <para>
    To show a pool’s usage statistics, execute:
   </para>
<screen>&prompt.cephuser;rados df
 POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
 .rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
 cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
 cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
 default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
 default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
 default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
 default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
 example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
 iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
 mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
 pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
 pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
 pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
 </screen>
   <para>
    A description of individual columns follow:
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       Number of bytes used by the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       Number of objects stored in the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       Number of clones stored in the pool. When a snapshot is created and one
       writes to an object, instead of modifying the original object its clone
       is created so the original snapshotted object content is not modified.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>COPIES</term>
     <listitem>
      <para>
       Number of object replicas. For example, if a replicated pool with the
       replication factor 3 has 'x' objects, it will normally have 3 * x
       copies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       Number of objects in the degraded state (not all copies exist) while the
       copy is missing on the primary OSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       Number of unfound objects.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Number of degraded objects.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       Total number of read operations requested for this pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       Total number of bytes read from this pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       Total number of write operations requested for this pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       Total number of bytes written to the pool. Note that it is not the same
       as the pool's usage because you can write to the same object many times.
       The result is that the pool's usage will remain the same but the number
       of bytes written to the pool will grow.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       Number of bytes allocated for compressed data.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       Number of bytes that the compressed data occupy when it is not
       compressed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-getting-pool-values">
   <title>Getting pool values</title>
   <para>
    To get a value from a pool, run the following <command>get</command>
    command:
   </para>
<screen>&prompt.cephuser;ceph osd pool get <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable></screen>
   <para>
    You can get values for keys listed in <xref linkend="ceph-pools-values"/>
    plus the following keys:
   </para>
   <variablelist>
    <varlistentry>
     <term>PG_NUM</term>
     <listitem>
      <para>
       The number of placement groups for the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PGP_NUM</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than <option>PG_NUM</option>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>All of a pool's values</title>
    <para>
     To list all values related to a specific pool, run:
    </para>
<screen>
 &prompt.cephuser;ceph osd pool get <replaceable>POOL_NAME</replaceable> all
 </screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>Setting pool values</title>
   <para>
    To set a value to a pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable> <replaceable>VALUE</replaceable></screen>
   <para>
    The following is a list of pool values sorted by a pool type:
   </para>
   <variablelist>
    <title>Common pool values</title>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The number of placement groups for the pool. If you add new OSDs to the
       cluster, verify the value for placement groups on all pools targeted for
       the new OSDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       The ruleset to use for mapping object placement in the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to the
       default 0, the cluster starts backfilling to have a correct placement of
       all PGs again. Be aware that this can create quite substantial I/O load
       on a cluster, therefore do not enable the flag from 0 to 1 on highly
       loaded production clusters.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Prevents the pool from being removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Prevents the pool's <option>pg_num</option> and <option>pgp_num</option>
       from being changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Disables (deep) scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Set or unset the <literal>WRITE_FADVISE_DONTNEED</literal> flag on a
       given pool's read/write requests to bypass putting data into cache.
       Default is <literal>false</literal>. Applies to both replicated and EC
       pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <literal>0</literal> means that the
       <option>osd_scrub_min_interval</option> value from the &ceph;
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <literal>0</literal> means that the
       <option>osd_scrub_max_interval</option> value from the &ceph;
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       The interval in seconds for the pool <emphasis>deep</emphasis>
       scrubbing. The default <literal>0</literal> means that the
       <option>osd_deep_scrub</option> value from the &ceph; configuration file
       is used.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>Replicated pool values</title>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Sets the number of replicas for objects in the pool. See
       <xref linkend="ceph-pools-options-num-of-replicas"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Sets the minimum number of replicas required for I/O. See
       <xref linkend="ceph-pools-options-num-of-replicas"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Prevents the pool's size from being changed. When a pool is created, the
       default value is taken from the value of the
       <option>osd_pool_default_flag_nosizechange</option> parameter which is
       <literal>false</literal> by default. Applies to replicated pools only
       because you cannot change size for EC pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Enables hit set tracking for cache pools. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. This option can have the
       following values: <literal>bloom</literal>,
       <literal>explicit_hash</literal>, <literal>explicit_object</literal>.
       Default is <literal>bloom</literal>, other values are for testing only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <systemitem>ceph-osd</systemitem> daemon.
       Default is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <systemitem>ceph-osd</systemitem> daemon. When a pool is created, the
       default value is taken from the value of the
       <option>osd_tier_default_cache_hit_set_period</option> parameter, which
       is <literal>1200</literal> by default. Applies to replicated pools only
       because EC pools cannot be used as a cache tier.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       The false positive probability for the bloom hit set type. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. Valid range is 0.0 - 1.0
       Default is <literal>0.05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Force OSDs to use GMT (Greenwich Mean Time) time stamps when creating a
       hit set for cache tiering. This ensures that nodes in different time
       zones return the same result. Default is <literal>1</literal>. This
       value should not be changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <literal>0.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <literal>0.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <literal>0.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       &ceph; will begin flushing or evicting objects when the
       <option>max_bytes</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       &ceph; will begin flushing or evicting objects when the
       <option>max_objects</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Temperature decay rate between two successive
       <literal>hit_set</literal>s. Default is <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Count at most <literal>N</literal> appearances in
       <literal>hit_set</literal>s for temperature calculation. Default is
       <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist xml:id="pool-values-ec">
    <title>Erasure coded pool values</title>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <emphasis>jerasure</emphasis> and <emphasis>isa</emphasis> erasure
       plug-ins, when the first <literal>K</literal> replies return, then the
       client’s request is served immediately using the data decoded from these
       replies. This approach causes more CPU load and less disk/network load.
       Currently, this flag is only supported for erasure coding pools. Default
       is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>Setting the number of object replicas</title>
   <para>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    The <replaceable>num-replicas</replaceable> includes the object itself. For
    example if you want the object and two copies of the object for a total of
    three instances of the object, specify 3.
   </para>
   <warning>
    <title>Do not set less than 3 replicas</title>
    <para>
     If you set the <replaceable>num-replicas</replaceable> to 2, there will be
     only <emphasis>one</emphasis> copy of your data. If you lose one object
     instance, you need to trust that the other copy has not been corrupted,
     for example since the last scrubbing during recovery (refer to
     <xref linkend="scrubbing-pgs"/> for details).
    </para>
    <para>
     Setting a pool to one replica means that there is exactly
     <emphasis>one</emphasis> instance of the data object in the pool. If the
     OSD fails, you lose the data. A possible usage for a pool with one replica
     is storing temporary data for a short time.
    </para>
   </warning>
   <tip>
    <title>Setting more than 3 replicas</title>
    <para>
     Setting 4 replicas for a pool increases the reliability by 25%.
    </para>
    <para>
     In case of two data centers, you need to set at least 4 replicas for a
     pool to have two copies in each data center so that if one data center is
     lost, two copies still exist and you can still lose one disk without
     losing data.
    </para>
   </tip>
   <note>
    <para>
     An object might accept I/Os in degraded mode with fewer than <literal>pool
     size</literal> replicas. To set a minimum number of required replicas for
     I/O, you should use the <literal>min_size</literal> setting. For example:
    </para>
<screen>&prompt.cephuser;ceph osd pool set data min_size 2</screen>
    <para>
     This ensures that no object in the data pool will receive I/O with fewer
     than <literal>min_size</literal> replicas.
    </para>
   </note>
   <tip>
    <title>Get the number of object replicas</title>
    <para>
     To get the number of object replicas, execute the following:
    </para>
<screen>&prompt.cephuser;ceph osd dump | grep 'replicated size'</screen>
    <para>
     &ceph; will list the pools, with the <literal>replicated size</literal>
     attribute highlighted. By default, &ceph; creates two replicas of an
     object (a total of three copies, or a size of 3).
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>Pool migration</title>

  <para>
   When creating a pool (see <xref linkend="ceph-pools-operate-add-pool"/>) you
   need to specify its initial parameters, such as the pool type or the number
   of placement groups. If you later decide to change any of these
   parameters&mdash;for example when converting a replicated pool into an
   erasure coded one, or decreasing the number of placement groups&mdash;you
   need to migrate the pool data to another one whose parameters suit your
   deployment.
  </para>

  <para>
   This section describes two migration methods&mdash;a <emphasis>cache
   tier</emphasis> method for general pool data migration, and a method using
   <command>rbd migrate</command> sub-commands to migrate RBD images to a new
   pool. Each method has its specifics and limitations.
  </para>

  <sect2 xml:id="pool-migrate-limits">
   <title>Limitations</title>
   <itemizedlist>
    <listitem>
     <para>
      You can use the <emphasis>cache tier</emphasis> method to migrate from a
      replicated pool to either an EC pool or another replicated pool.
      Migrating from an EC pool is not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      You cannot migrate RBD images and &cephfs; exports from a replicated pool
      to an EC pool. The reason is that EC pools do not support
      <literal>omap</literal>, while RBD and &cephfs; use
      <literal>omap</literal> to store its metadata. For example, the header
      object of the RBD will fail to be flushed. But you can migrate data to EC
      pool, leaving metadata in replicated pool.
     </para>
    </listitem>
    <listitem>
     <para>
      The <command>rbd migration</command> method allows migrating images with
      minimal client downtime. You only need to stop the client before the
      <option>prepare</option> step and start it afterward. Note that only a
      <systemitem>librbd</systemitem> client that supports this feature (&ceph;
      Nautilus or newer) will be able to open the image just after the
      <option>prepare</option> step, while older
      <systemitem>librbd</systemitem> clients or the
      <systemitem>krbd</systemitem> clients will not be able to open the image
      until the <option>commit</option> step is executed.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>Migrating using cache tier</title>
   <para>
    The principle is simple&mdash;include the pool that you need to migrate
    into a cache tier in reverse order. The following example migrates a
    replicated pool named 'testpool' to an erasure coded pool:
   </para>
   <procedure>
    <title>Migrating replicated to erasure coded pool</title>
    <step>
     <para>
      Create a new erasure coded pool named 'newpool'. Refer to
      <xref linkend="ceph-pools-operate-add-pool"/> for a detailed explanation
      of pool creation parameters.
     </para>
<screen>
 &prompt.cephuser;ceph osd pool create newpool erasure default
</screen>
     <para>
      Verify that the used client keyring provides at least the same
      capabilities for 'newpool' as it does for 'testpool'.
     </para>
     <para>
      Now you have two pools: the original replicated 'testpool' filled with
      data, and the new empty erasure coded 'newpool':
     </para>
     <figure>
      <title>Pools before migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Set up the cache tier and configure the replicated pool 'testpool' as a
      cache pool. The <option>-force-nonempty</option> option allows adding a
      cache tier even if the pool already has data:
     </para>
<screen>
&prompt.cephuser;ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
&prompt.cephuser;ceph osd tier add newpool testpool --force-nonempty
&prompt.cephuser;ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>Cache tier setup</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Force the cache pool to move all objects to the new pool:
     </para>
<screen>
&prompt.cephuser;rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Data flushing</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Until all the data has been flushed to the new erasure coded pool, you
      need to specify an overlay so that objects are searched on the old pool:
     </para>
<screen>
&prompt.cephuser;ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      With the overlay, all operations are forwarded to the old replicated
      'testpool':
     </para>
     <figure>
      <title>Setting overlay</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Now you can switch all the clients to access objects on the new pool.
     </para>
    </step>
    <step>
     <para>
      After all data is migrated to the erasure coded 'newpool', remove the
      overlay and the old cache pool 'testpool':
     </para>
<screen>
&prompt.cephuser;ceph osd tier remove-overlay newpool
&prompt.cephuser;ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration complete</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Run
     </para>
<screen>
&prompt.cephuser;ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>Migrating RBD images</title>
   <para>
    The following is the recommended way to migrate RBD images from one
    replicated pool to another replicated pool.
   </para>
   <procedure>
    <step>
     <para>
      Stop clients (such as a virtual machine) from accessing the RBD image.
     </para>
    </step>
    <step>
     <para>
      Create a new image in the target pool, with the parent set to the source
      image:
     </para>
<screen>
&prompt.cephuser;rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>Migrate only data to an erasure coded pool</title>
      <para>
       If you need to migrate only the image data to a new EC pool and leave
       the metadata in the original replicated pool, run the following command
       instead:
      </para>
<screen>
&prompt.cephuser;rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
    </step>
    <step>
     <para>
      Let clients access the image in the target pool.
     </para>
    </step>
    <step>
     <para>
      Migrate data to the target pool:
     </para>
<screen>
&prompt.cephuser;rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      Remove the old image:
     </para>
<screen>
&prompt.cephuser;rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>Pool snapshots</title>

  <para>
   Pool snapshots are snapshots of the state of the whole &ceph; pool. With
   pool snapshots, you can retain the history of the pool's state. Creating
   pool snapshots consumes storage space proportional to the pool size. Always
   check the related storage for enough disk space before creating a snapshot
   of a pool.
  </para>

  <sect2 xml:id="ceph-make-snapshot-pool">
   <title>Making a snapshot of a pool</title>
   <para>
    To make a snapshot of a pool, run:
   </para>
<screen>
&prompt.cephuser;ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2 xml:id="ceph-listing-snapshots-pool">
   <title>Listing snapshots of a pool</title>
   <para>
    To list existing snapshots of a pool, run:
   </para>
<screen>
&prompt.cephuser;rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    For example:
   </para>
<screen>
&prompt.cephuser;rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2 xml:id="ceph-removing-snapshot-pool">
   <title>Removing a snapshot of a pool</title>
   <para>
    To remove a snapshot of a pool, run:
   </para>
<screen>&prompt.cephuser;ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>Data compression</title>

  <para>
   &bluestore; (find more details in <xref linkend="about-bluestore"/>)
   provides on-the-fly data compression to save disk space. The compression
   ratio depends on the data stored in the system. Note that
   compression/decompression requires additional CPU power.
  </para>

  <para>
   You can configure data compression globally (see
   <xref linkend="sec-ceph-pool-bluestore-compression-options"/>) and then
   override specific compression settings for each individual pool.
  </para>

  <para>
   You can enable or disable pool data compression, or change the compression
   algorithm and mode at any time, regardless of whether the pool contains data
   or not.
  </para>

  <para>
   No compression will be applied to existing data after enabling the pool
   compression.
  </para>

  <para>
   After disabling the compression of a pool, all its data will be
   decompressed.
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>Enabling compression</title>
   <para>
    To enable data compression for a pool named
    <replaceable>POOL_NAME</replaceable>, run the following command:
   </para>
<screen>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>Disabling pool compression</title>
    <para>
     To disable data compression for a pool, use 'none' as the compression
     algorithm:
    </para>
<screen>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>Pool compression options</title>
   <para>
    A full list of compression settings:
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Possible values are <literal>none</literal>, <literal>zstd</literal>,
       <literal>snappy</literal>. Default is <literal>snappy</literal>.
      </para>
      <para>
       Which compression algorithm to use depends on the specific use case.
       Several recommendations follow:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Use the default <literal>snappy</literal> as long as you do not have a
         good reason to change it.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal> offers a good compression ratio, but causes
         high CPU overhead when compressing small amounts of data.
        </para>
       </listitem>
       <listitem>
        <para>
         Run a benchmark of these algorithms on a sample of your actual data
         while keeping an eye on the CPU and memory usage of your cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       Possible values are <literal>none</literal>,
       <literal>aggressive</literal>, <literal>passive</literal>,
       <literal>force</literal>. Default is <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: compress never
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: compress if hinted
         <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: compress unless hinted
         <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: compress always
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Value: Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Default is
       <literal>0.875</literal>, which means that if the compression does not
       reduce the occupied space by at least 12.5%, the object will not be
       compressed.
      </para>
      <para>
       Objects above this ratio will not be stored compressed because of the
       low net gain.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Maximum size of objects that are compressed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Minimum size of objects that are compressed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>Global compression options</title>
   <para>
    The following configuration options can be set in the &ceph; configuration
    and apply to all OSDs and not only a single pool. The pool specific
    configuration listed in <xref linkend="sec-ceph-pool-compression-options"/>
    takes precedence.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       See <xref linkend="compr-algorithm"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       See <xref linkend="compr-mode"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       See <xref linkend="compr-ratio"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Minimum size of objects that are compressed. The setting is ignored by
       default in favor of
       <option>bluestore_compression_min_blob_size_hdd</option> and
       <option>bluestore_compression_min_blob_size_ssd</option>. It takes
       precedence when set to a non-zero value.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Maximum size of objects that are compressed before they will be split
       into smaller chunks. The setting is ignored by default in favor of
       <option>bluestore_compression_max_blob_size_hdd</option> and
       <option>bluestore_compression_max_blob_size_ssd</option>. It takes
       precedence when set to a non-zero value.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>8K</literal>
      </para>
      <para>
       Minimum size of objects that are compressed and stored on solid-state
       drive.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>64K</literal>
      </para>
      <para>
       Maximum size of objects that are compressed and stored on solid-state
       drive before they will be split into smaller chunks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>128K</literal>
      </para>
      <para>
       Minimum size of objects that are compressed and stored on hard disks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>512K</literal>
      </para>
      <para>
       Maximum size of objects that are compressed and stored on hard disks
       before they will be split into smaller chunks.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
