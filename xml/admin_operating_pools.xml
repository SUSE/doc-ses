<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.pools">
 <title>Managing Storage Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; stores data within pools. Pools are logical groups for storing
  objects. When you first deploy a cluster without creating a pool, &ceph; uses
  the default pools for storing data. The following important highlights relate
  to &ceph; pools:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resilience</emphasis>: You can set how many OSDs, buckets, or
    leaves are allowed to fail without losing data. For replicated pools, it is
    the desired number of copies/replicas of an object. New pools are created
    with a default count of replicas set to 3. For erasure coded pools, it is
    the number of coding chunks (that is <emphasis>m=2</emphasis> in the
    erasure code profile).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Placement Groups</emphasis>: are internal data structures for
    storing data in a pool across OSDs. The way &ceph; stores data into PGs is
    defined in a &crushmap;. You can set the number of placement groups for a
    pool at its creation. A typical configuration uses approximately 100
    placement groups per OSD to provide optimal balancing without using up too
    many computing resources. When setting up multiple pools, be careful to
    ensure you set a reasonable number of placement groups for both the pool
    and the cluster as a whole. A
    <link xlink:href="https://ceph.com/pgcalc/">Ceph PGs per Pool
    Calculator</link> can help you.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH Rules</emphasis>: When you store data in a pool, objects
    and its replicas (or chunks in case of erasure coded pools) are placed
    according to the CRUSH ruleset mapped to the pool. You can create a custom
    CRUSH rule for your pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshots</emphasis>: When you create snapshots with
    <command>ceph osd pool mksnap</command>, you effectively take a snapshot of
    a particular pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  To organize data into pools, you can list, create, and remove pools. You can
  also view the usage statistics for each pool.
 </para>
 <sect1 xml:id="ceph.pools.associate">
  <title>Associate Pools with an Application</title>

  <para>
   Before using pools, you need to associate them with an application. Pools
   that will be used with &cephfs;, or pools that are automatically created by
   &rgw; are automatically associated.
  </para>

  <para>
   For other cases, you can manually associate a free-form application name
   with a pool:
  </para>

<screen>&prompt.cephuser;ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>Default Application Names</title>
   <para>
    &cephfs; uses the application name <literal>cephfs</literal>, &rbd; uses
    <literal>rbd</literal>, and &rgw; uses <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   A pool can be associated with multiple applications, and each application
   can have its own metadata. You can display the application metadata for a
   given pool using the following command:
  </para>

<screen>&prompt.cephuser;ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.pools.operate">
  <title>Operating Pools</title>

  <para>
   This section introduces practical information to perform basic tasks with
   pools. You can find out how to list, create, and delete pools, as well as
   show pool statistics or manage snapshots of a pool.
  </para>

  <sect2>
   <title>List Pools</title>
   <para>
    To list your cluster’s pools, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool ls</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Create a Pool</title>
   <para>
    A pool can either be 'replicated' to recover from lost OSDs by keeping
    multiple copies of the objects or 'erasure' to get a kind of generalized
    RAID5/6 capability. The replicated pools require more raw storage, while
    erasure coded pools require less raw storage. Default is 'replicated'.
   </para>
   <para>
    To create a replicated pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    To create an erasure coded pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    The <command>ceph osd pool create</command> can fail if you exceed the
    limit of placement groups per OSD. The limit is set with the option
    <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       The name of the pool. It must be unique. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The total number of placement groups for the pool. This option is
       required. Default value is 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The total number of placement groups for placement purposes. This should
       be equal to the total number of placement groups, except for placement
       group splitting scenarios. This option is required. Default value is 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       The name of the crush ruleset for this pool. If the specified ruleset
       does not exist, the creation of replicated pool will fail with -ENOENT.
       For replicated pools it is the ruleset specified by the <varname>osd
       pool default crush replicated ruleset</varname> configuration variable.
       This ruleset must exist. For erasure pools it is 'erasure-code' if the
       default erasure code profile is used or
       <replaceable>POOL_NAME</replaceable> otherwise. This ruleset will be
       created implicitly if it does not exist already.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       For erasure coded pools only. Use the erasure code profile. It must be
       an existing profile as defined by <command>osd erasure-code-profile
       set</command>.
      </para>
      <para>
       When you create a pool, set the number of placement groups to a
       reasonable value. Consider the total number of placement groups per OSD
       too. Placement groups are computationally expensive, so performance will
       degrade when you have many pools with many placement groups (for example
       50 pools with 100 placement groups each).
      </para>
      <para>
       See <xref linkend="op.pgs"/> for details on calculating an appropriate
       number of placement groups for your pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       The expected number of objects for this pool. By setting this value
       (together with a negative <option>filestore merge threshold</option>),
       the PG folder splitting happens at the pool creation time. This avoids
       the latency impact with a runtime folder splitting.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Set Pool Quotas</title>
   <para>
    You can set pool quotas for the maximum number of bytes and/or the maximum
    number of objects per pool.
   </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    To remove a quota, set its value to 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Delete a Pool</title>
   <warning>
    <title>Pool Deletion is Not Reversible</title>
    <para>
     Pools may contain important data. Deleting a pool causes all data in the
     pool to disappear, and there is no way to recover it.
    </para>
   </warning>
   <para>
    Because inadvertent pool deletion is a real danger, &ceph; implements two
    mechanisms that prevent pools from being deleted. Both mechanisms must be
    disabled before a pool can be deleted.
   </para>
   <para>
    The first mechanism is the <literal>NODELETE</literal> flag. Each pool has
    this flag, and its default value is 'false'. To find out the value of this
    flag on a pool, run the following command:
   </para>
<screen>&prompt.cephuser;ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    If it outputs <literal>nodelete: true</literal>, it is not possible to
    delete the pool until you change the flag using the following command:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    The second mechanism is the cluster-wide configuration parameter
    <option>mon allow pool delete</option>, which defaults to 'false'. This
    means that, by default, it is not possible to delete a pool. The error
    message displayed is:
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    To delete the pool in spite of this safety setting, you can temporarily set
    <option>mon allow pool delete</option> to 'true', delete the pool, and then
    return the parameter to 'false':
   </para>
<screen>&prompt.cephuser;ceph tell mon.* injectargs --mon-allow-pool-delete=true
&prompt.cephuser;ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
&prompt.cephuser;ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    The <command>injectargs</command> command displays the following message:
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    This is merely confirming that the command was executed successfully. It is
    not an error.
   </para>
   <para>
    If you created your own rulesets and rules for a pool you created, you
    should consider removing them when you no longer need your pool.
   </para>
  </sect2>

  <sect2>
   <title>Rename a Pool</title>
   <para>
    To rename a pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    If you rename a pool and you have per-pool capabilities for an
    authenticated user, you must update the user’s capabilities with the new
    pool name.
   </para>
  </sect2>

  <sect2>
   <title>Show Pool Statistics</title>
   <para>
    To show a pool’s usage statistics, execute:
   </para>
<screen>&prompt.cephuser;rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2>
   <title>Get Pool Values</title>
   <para>
    To get a value from a pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    You can get values for keys listed in <xref linkend="ceph.pools.values"/>
    plus the following keys:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The number of placement groups for the pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement. Valid range is equal to or less than
       <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>All Pool's Values</title>
    <para>
     To list all values related to a specific pool, run:
    </para>
<screen>
&prompt.cephuser;ceph osd pool get <replaceable>POOL_NAME</replaceable> all
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Set Pool Values</title>
   <para>
    To set a value to a pool, execute:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    You may set values for the following keys:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Sets the number of replicas for objects in the pool. See
       <xref linkend="ceph.pools.options.num_of_replicas"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Sets the minimum number of replicas required for I/O. See
       <xref linkend="ceph.pools.options.num_of_replicas"/> for further
       details. Replicated pools only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       The number of seconds to allow clients to replay acknowledged, but
       uncommitted requests.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       The number of placement groups for the pool. If you add new OSDs to the
       cluster, verify the value for placement groups on all pools targeted for
       the new OSDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       The effective number of placement groups to use when calculating data
       placement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       The ruleset to use for mapping object placement in the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Set (1) or unset (0) the HASHPSPOOL flag on a given pool. Enabling this
       flag changes the algorithm to better distribute PGs to OSDs. After
       enabling this flag on a pool whose HASHPSPOOL flag was set to the
       default 0, the cluster starts backfilling to have a correct placement of
       all PGs again. Be aware that this can create quite substantial I/O load
       on a cluster, therefore do not enable the flag from 0 to 1 on a highly
       loaded production clusters.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Prevents the pool from being removed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Prevents the pool's <option>pg_num</option> and <option>pgp_num</option>
       from being changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Prevents the pool's size from being changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Set/Unset the <literal>WRITE_FADVISE_DONTNEED</literal> flag on a given
       pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Disables (deep)-scrubbing of the data for the specific pool to resolve
       temporary high I/O load.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Enables hit set tracking for cache pools. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. This option can have the
       following values: <literal>bloom</literal>,
       <literal>explicit_hash</literal>, <literal>explicit_object</literal>.
       Default is <literal>bloom</literal>, other values are for testing only.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       The number of hit sets to store for cache pools. The higher the number,
       the more RAM consumed by the <systemitem>ceph-osd</systemitem> daemon.
       Default is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       The duration of a hit set period in seconds for cache pools. The higher
       the number, the more RAM consumed by the
       <systemitem>ceph-osd</systemitem> daemon.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       The false positive probability for the bloom hit set type. See
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom
       Filter</link> for additional information. Valid range is 0.0 - 1.0
       Default is <literal>0.05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Force OSDs to use GMT (Greenwich Mean Time) time stamps when creating a
       hit set for cache tiering. This ensures that nodes in different time
       zones return the same result. Default is <literal>1</literal>. This
       value should not be changed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool. Default is <literal>0.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing modified (dirty) objects
       before the cache tiering agent will flush them to the backing storage
       pool with a higher speed. Default is <literal>0.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       The percentage of the cache pool containing unmodified (clean) objects
       before the cache tiering agent will evict them from the cache pool.
       Default is <literal>0.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       &ceph; will begin flushing or evicting objects when the
       <option>max_bytes</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       &ceph; will begin flushing or evicting objects when the
       <option>max_objects</option> threshold is triggered.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Temperature decay rate between two successive
       <literal>hit_set</literal>s. Default is <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Count at most <literal>N</literal> appearances in
       <literal>hit_set</literal>s for temperature calculation. Default is
       <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will flush an
       object from the cache pool to the storage pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       The time (in seconds) before the cache tiering agent will evict an
       object from the cache pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       If this flag is enabled on erasure coding pools, then the read request
       issues sub-reads to all shards, and waits until it receives enough
       shards to decode to serve the client. In the case of
       <emphasis>jerasure</emphasis> and <emphasis>isa</emphasis> erasure
       plug-ins, when the first <literal>K</literal> replies return, then the
       client’s request is served immediately using the data decoded from
       these replies. This approach cause more CPU load and less disk/network
       load. Currently, this flag is only supported for erasure coding pools.
       Default is <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       The minimum interval in seconds for pool scrubbing when the cluster load
       is low. The default <literal>0</literal> means that the
       <option>osd_scrub_min_interval</option> value from the &ceph;
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       The maximum interval in seconds for pool scrubbing, regardless of the
       cluster load. The default <literal>0</literal> means that the
       <option>osd_scrub_max_interval</option> value from the &ceph;
       configuration file is used.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       The interval in seconds for the pool <emphasis>deep</emphasis>
       scrubbing. The default <literal>0</literal> means that the
       <option>osd_deep_scrub</option> value from the &ceph; configuration file
       is used.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Set the Number of Object Replicas</title>
   <para>
    To set the number of object replicas on a replicated pool, execute the
    following:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    The <replaceable>num-replicas</replaceable> includes the object itself. For
    example if you want the object and two copies of the object for a total of
    three instances of the object, specify 3.
   </para>
   <warning>
    <title>Do not Set Less than 3 Replicas</title>
    <para>
     If you set the <replaceable>num-replicas</replaceable> to 2, there will be
     only <emphasis>one</emphasis> copy of your data. If you lose one object
     instance, you need to trust that the other copy has not been corrupted for
     example since the last scrubbing during recovery (refer to
     <xref linkend="scrubbing"/> for details).
    </para>
    <para>
     Setting a pool to one replica means that there is exactly
     <emphasis>one</emphasis> instance of the data object in the pool. If the
     OSD fails, you lose the data. A possible usage for a pool with one replica
     is storing temporary data for a short time.
    </para>
   </warning>
   <tip>
    <title>Setting More than 3 Replicas</title>
    <para>
     Setting 4 replicas for a pool increases the reliability by 25%.
    </para>
    <para>
     In case of two data centers, you need to set at least 4 replicas for a
     pool to have two copies in each data center so that in case one data
     center is lost, still two copies exist and you can still lose one disk
     without loosing data.
    </para>
   </tip>
   <note>
    <para>
     An object might accept I/Os in degraded mode with fewer than <literal>pool
     size</literal> replicas. To set a minimum number of required replicas for
     I/O, you should use the <literal>min_size</literal> setting. For example:
    </para>
<screen>&prompt.cephuser;ceph osd pool set data min_size 2</screen>
    <para>
     This ensures that no object in the data pool will receive I/O with fewer
     than <literal>min_size</literal> replicas.
    </para>
   </note>
   <tip>
    <title>Get the Number of Object Replicas</title>
    <para>
     To get the number of object replicas, execute the following:
    </para>
<screen>&prompt.cephuser;ceph osd dump | grep 'replicated size'</screen>
    <para>
     &ceph; will list the pools, with the <literal>replicated size</literal>
     attribute highlighted. By default, &ceph; creates two replicas of an
     object (a total of three copies, or a size of 3).
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools.migration">
  <title>Pool Migration</title>

  <para>
   When creating a pool (see <xref linkend="ceph.pools.operate.add_pool"/>) you
   need to specify its initial parameters, such as the pool type or the number
   of placement groups. If you later decide to change any of these
   parameters&mdash;for example when converting a replicated pool into an
   erasure coded one, or decreasing the number of placement groups&mdash;, you
   need to migrate the pool data to another one whose parameters suit your
   deployment.
  </para>

  <para>
   There are several methods of pool migration. We recommend using
   <emphasis>cache tier</emphasis> because this method is transparent, reduces
   cluster downtime, and avoids duplicating all pool's data.
  </para>

  <sect2 xml:id="pool.migrate.cache_tier">
   <title>Migrate using Cache Tier</title>
   <tip>
    <title>Migrating Replicated Pool Only</title>
    <para>
     You can use the cache tier method to migrate from a replicated pool to
     either an erasure coded or another replicated pool. The reason is that
     having an erasure coded pool as a cache tier is not supported.
    </para>
   </tip>
   <para>
    The principle is simple&mdash;include the pool that you need to migrate
    into a cache tier in reverse order. Find more details on cache tiers in
    <xref linkend="cha.ceph.tiered"/>. The following example migrates a
    replicated pool named 'testpool' to an erasure coded pool:
   </para>
   <procedure>
    <title>Migrating Replicated to Erasure Coded Pool</title>
    <step>
     <para>
      Create a new erasure coded pool named 'newpool'. Refer to
      <xref linkend="ceph.pools.operate.add_pool"/> for detailed explanation of
      pool creation parameters.
     </para>
<screen>
 &prompt.cephuser;ceph osd pool create newpool <replaceable>PG_NUM</replaceable> <replaceable>PGP_NUM</replaceable>erasure default
</screen>
     <para>
      Do not forget to add the capabilities of 'testpool' to 'newpool'.
     </para>
     <para>
      Now you have two pools: the original replicated 'testpool' filled with
      data, and the new empty erasure coded 'newpool':
     </para>
     <figure>
      <title>Pools before Migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Setup the cache tier and configure the replicated pool 'testpool' as a
      cache pool. The <option>-force-nonempty</option> option allows adding a
      cache tier even if the pool already has data:
     </para>
<screen>
&prompt.cephuser;ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
&prompt.cephuser;ceph osd tier add newpool testpool --force-nonempty
&prompt.cephuser;ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>Cache Tier Setup</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Force the cache pool to move all objects to the new pool:
     </para>
<screen>
&prompt.cephuser;rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Data Flushing</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Until all the data has been flushed to the new erasure coded pool, you
      need to specify an overlay so that objects are searched on the old pool:
     </para>
<screen>
&prompt.cephuser;ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      With the overlay, all operations are forwarded to the old replicated
      'testpool':
     </para>
     <figure>
      <title>Setting Overlay</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Now you can switch all the clients to access objects on the new pool.
     </para>
    </step>
    <step>
     <para>
      After all data is migrated to the erasure coded 'newpool', remove the
      overlay and the old cache pool 'testpool':
     </para>
<screen>
&prompt.cephuser;ceph osd tier remove-overlay newpool
&prompt.cephuser;ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration Complete</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Run
     </para>
<screen>
&prompt.cephuser;ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
   <warning>
    <title>You Cannot Migrate RBD Images and &cephfs; Exports to an EC Pool</title>
    <para>
     You cannot migrate RBD images and &cephfs; exports from a replicated pool
     to an EC pool. EC pools can store data but not metadata. The header object
     of the RBD will fail to be flushed. The same applies for &cephfs;.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="migrate.rbd.image">
   <title>Migrating a &rbd; Image</title>
   <para>
    Following is the recommended way to migrate RBD images from one replicated
    poll to another replicated pool.
   </para>
   <procedure>
    <step>
     <para>
      Stop clients (such as a virtual machine) from accessing the RBD image.
     </para>
    </step>
    <step>
     <para>
      Create a new image in the target pool, with the parent set to the source
      image:
     </para>
<screen>
&prompt.cephuser;rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>Migrate Only Data to an EC Pool</title>
      <para>
       If you need to migrate only the image data to a new EC pool and leaving
       the metadata in the original replicated pool, run the following command
       instead:
      </para>
<screen>
&prompt.cephuser;rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
     <note>
      <title>Client Support and Downtime</title>
      <para>
       The <command>rbd migration</command> method allows migrating images with
       minimum client downtime. You only need to stop the client before the
       'prepare' step and start it afterward. Note that only a
       <systemitem>librbd</systemitem> client that supports this feature
       (&ceph; Nautilus or newer) will be able to open the image just after the
       'prepare' step, while older <systemitem>librbd</systemitem> clients or
       the <systemitem>krbd</systemitem> clients will not be able to open the
       image until the 'commit' step is executed.
      </para>
     </note>
    </step>
    <step>
     <para>
      Let clients access the image in the target pool.
     </para>
    </step>
    <step>
     <para>
      Migrate data to the target pool:
     </para>
<screen>
&prompt.cephuser;rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      Remove the old image:
     </para>
<screen>
&prompt.cephuser;rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Pool Snapshots</title>

  <para>
   Pool snapshots are snapshots of the state of the whole &ceph; pool. With
   pool snapshots, you can retain the history of the pool's state. Creating
   pool snapshots consumes storage space proportional to the pool size. Always
   check the related storage for enough disk space before creating a snapshot
   of a pool.
  </para>

  <sect2>
   <title>Make a Snapshot of a Pool</title>
   <para>
    To make a snapshot of a pool, run:
   </para>
<screen>
&prompt.cephuser;ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2>
   <title>List Snapshots of a Pool</title>
   <para>
    To list existing snapshots of a pool, run:
   </para>
<screen>
&prompt.cephuser;rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    For example:
   </para>
<screen>
&prompt.cephuser;rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2>
   <title>Remove a Snapshot of a Pool</title>
   <para>
    To remove a snapshot of a pool, run:
   </para>
<screen>&prompt.cephuser;ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.pool.compression">
  <title>Data Compression</title>

  <para>
   &bluestore; (find more details in <xref linkend="about.bluestore"/>)
   provides on-the-fly data compression to save disk space. The compression
   ratio depends on the data stored in the system. Note that compression /
   de-compression requires additional CPU power.
  </para>

  <para>
   You can configure data compression globally (see
   <xref linkend="sec.ceph.pool.bluestore_compression.options"/>), and then
   override specific compression settings for each individual pool.
  </para>

  <para>
   You can enable or disable pool data compression, or change the compression
   algorithm and mode at any time, regardless of whether the pool contains data
   or not.
  </para>

  <para>
   No compression will be applied to existing data after enabling the pool
   compression.
  </para>

  <para>
   After disabling the compression of a pool, all its data will be
   decompressed.
  </para>

  <sect2 xml:id="sec.ceph.pool.compression.enable">
   <title>Enable Compression</title>
   <para>
    To enable data compression for a pool named
    <replaceable>POOL_NAME</replaceable>, run the following command:
   </para>
<screen>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>Disabling Pool Compression</title>
    <para>
     To disable data compression for a pool, use 'none' as the compression
     algorithm:
    </para>
<screen>
&prompt.cephuser;<command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.compression.options">
   <title>Pool Compression Options</title>
   <para>
    A full list of compression settings:
   </para>
   <variablelist>
    <varlistentry xml:id="compr.algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Possible values are <literal>none</literal>, <literal>zstd</literal>,
       <literal>snappy</literal>. Default is <literal>snappy</literal>.
      </para>
      <para>
       Which compression algorithm to use depends on the specific use case.
       Several recommendations follow:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Use the default <literal>snappy</literal> as long as you do not have a
         good reason to change it.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal> offers a good compression ratio, but causes
         high CPU overhead when compressing small amounts of data.
        </para>
       </listitem>
<!--   <listitem>
        <para>
         If you want to prevent high CPU overhead, try <literal>lz4</literal>.
        </para>
       </listitem> -->
       <listitem>
        <para>
         Run a benchmark of these algorithms on a sample of your actual data
         while keeping an eye on the CPU and memory usage of your cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr.mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       Possible values are <literal>none</literal>,
       <literal>aggressive</literal>, <literal>passive</literal>,
       <literal>force</literal>. Default is <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: compress never
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: compress if hinted
         <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: compress unless hinted
         <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: compress always
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr.ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Value: Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Default is
       <literal>0.875</literal>, which means that if the compression does not
       reduce the occupied space by at least 12,5%, the object will not be
       compressed.
      </para>
      <para>
       Objects above this ratio will not be stored compressed because of the
       low net gain.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Maximum size of objects that are compressed.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Minimum size of objects that are compressed.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.bluestore_compression.options">
   <title>Global Compression Options</title>
   <para>
    The following configuration options can be set in the &ceph; configuration
    and apply to all OSDs and not only a single pool. The pool specific
    configuration listed in <xref linkend="sec.ceph.pool.compression.options"/>
    take precedence.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       See <xref linkend="compr.algorithm"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       See <xref linkend="compr.mode"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       See <xref linkend="compr.ratio"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Minimum size of objects that are compressed. The setting is ignored by
       default in favor of
       <option>bluestore_compression_min_blob_size_hdd</option> and
       <option>bluestore_compression_min_blob_size_ssd</option>. It takes
       precedence when set to a non-zero value.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>0</literal>
      </para>
      <para>
       Maximum size of objects that are compressed before they will be split
       into smaller chunks. The setting is ignored by default in favor of
       <option>bluestore_compression_max_blob_size_hdd</option> and
       <option>bluestore_compression_max_blob_size_ssd</option>. It takes
       precedence when set to a non-zero value.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>8K</literal>
      </para>
      <para>
       Minimum size of objects that are compressed and stored on solid-state
       drive.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>64K</literal>
      </para>
      <para>
       Maximum size of objects that are compressed and stored on solid-state
       drive before they will be split into smaller chunks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>128K</literal>
      </para>
      <para>
       Minimum size of objects that are compressed and stored on hard disks.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Value: Unsigned Integer, size in bytes. Default: <literal>512K</literal>
      </para>
      <para>
       Maximum size of objects that are compressed and stored on hard disks
       before they will be split into smaller chunks.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
