<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.monitor">
 <title>Determining Cluster State</title>
 <para>
  Once you have a running cluster, you may use the <command>ceph</command> tool
  to monitor your cluster. Determining the cluster state typically involves
  checking the status of OSD, monitor, placement group and metadata server.
  <remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </para>

<screen>&prompt.root;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   The &ceph; cluster returns one of the following health codes:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </para>
     <para>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<filename>/var/log/ceph/ceph-osd.*</filename>) may contain debugging
      information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, for example
  OSD_HOST_DOWN</term>
    <listitem>
     <para>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </para>
<screen>&prompt.root;ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      The utilization thresholds for <emphasis>backfillfull</emphasis>,
      <emphasis>nearfull</emphasis>, <emphasis>full</emphasis>, and/or
      <emphasis>failsafe_full</emphasis> are not ascending. In particular, we
      expect <emphasis>backfillfull</emphasis> &lt;
      <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt;
      <emphasis>full</emphasis>, and <emphasis>full</emphasis> &lt;
      <emphasis>failsafe_full</emphasis>. The thresholds can be adjusted with:
     </para>
<screen>&prompt.root;ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
&prompt.root;ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
&prompt.root;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>full</emphasis> threshold and
      is preventing the cluster from servicing writes. Utilization by pool can
      be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
     <para>
      The currently defined <emphasis>full</emphasis> ratio can be seen with:
     </para>
<screen>&prompt.root;ceph osd dump | grep full_ratio</screen>
     <para>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </para>
<screen>&prompt.root;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>backfillfull</emphasis>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Utilization by pool
      can be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>nearfull</emphasis>
      threshold. This is an early warning that the cluster is approaching full.
      Utilization by pool can be checked with:
     </para>
<screen>&prompt.root;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      One or more cluster flags of interest has been set. With the exception of
      <emphasis>full</emphasis>, these flags can be set or cleared with:
     </para>
<screen>&prompt.root;ceph osd set <replaceable>flag</replaceable>
&prompt.root;ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         The cluster is flagged as full and cannot service writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr </term>
       <listitem>
        <para>
         Paused reads or writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs are not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <emphasis>down</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs that were previously marked <emphasis>out</emphasis> will not be
         marked back <emphasis>in</emphasis> when they start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs will not automatically be marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Recovery or data rebalancing is suspended.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing is disabled.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Cache tiering activity is suspended.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSD is not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Failure reports for this OSD will be ignored.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         If this OSD was previously marked <emphasis>out</emphasis>
         automatically after a failure, it will not be marked
         <emphasis>in</emphasis> when it starts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         If this OSD is down, it will not be automatically marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Per-OSD flags can be set and cleared with:
     </para>
<screen>&prompt.root;ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
&prompt.root;ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      The &crushmap; is using very old settings and should be updated. The
      oldest tunables that can be used (i.e., the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <option>mon_crush_min_required_version</option> config
      option.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      The &crushmap; is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The &crushmap; should be
      updated to use the newer method (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      One or more cache pools is not configured with a hit set to track
      utilization, which prevents the tiering agent from identifying cold
      objects to flush and evict from the cache. Hit sets can be configured on
      the cache pool with:
     </para>
<screen>&prompt.root;ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
&prompt.root;ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
&prompt.root;ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
&prompt.root;ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      No pre-luminous v12 OSDs are running but the <option>sortbitwise</option>
      flag has not been set. You need to set the <option>sortbitwise</option>
      flag before luminous v12 or newer OSDs can start:
     </para>
<screen>&prompt.root;ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and utilization with:
     </para>
<screen>&prompt.root;ceph df detail</screen>
     <para>
      You can either raise the pool quota with
     </para>
<screen>&prompt.root;ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
&prompt.root;ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      or delete some existing data to reduce utilization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow IO
      requests to be serviced. Problematic PG states include
      <emphasis>peering</emphasis>, <emphasis>stale</emphasis>,
      <emphasis>incomplete</emphasis>, and the lack of
      <emphasis>active</emphasis> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </para>
<screen>&prompt.root;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.root;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <emphasis>degraded</emphasis> or
      <emphasis>undersized</emphasis> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <emphasis>clean</emphasis> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </para>
<screen>&prompt.root;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.root;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Data redundancy may be reduced or at risk for some data due to a lack of
      free space in the cluster. Specifically, one or more PGs has the
      <emphasis>backfill_toofull</emphasis> or
      <emphasis>recovery_toofull</emphasis> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <emphasis>backfillfull</emphasis> threshold.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Data scrubbing has discovered some problems with data consistency in the
      cluster. Specifically, one or more PGs has the
      <emphasis>inconsistent</emphasis> or <emphasis>snaptrim_error</emphasis>
      flag is set, indicating an earlier scrub operation found a problem, or
      that the <emphasis>repair</emphasis> flag is set, meaning a repair for
      such an inconsistency is currently in progress.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Recent OSD scrubs have uncovered inconsistencies.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      A cache tier pool is nearly full. Full in this context is determined by
      the <emphasis>target_max_bytes</emphasis> and
      <emphasis>target_max_objects</emphasis> properties on the cache pool.
      Once the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </para>
<screen>&prompt.root;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
&prompt.root;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Normal cache flush and evict activity may also be throttled due to
      reduced availability or performance of the base tier, or overall cluster
      load.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is below the configurable threshold of
      <option>mon_pg_warn_min_per_osd</option> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is above the configurable threshold of
      <option>mon_pg_warn_max_per_osd</option> PGs per OSD. This can lead to
      higher memory utilization for OSD daemons, slower peering after cluster
      state changes (for example OSD restarts, additions, or removals), and
      higher load on the &mgr;s and &mon;s.
     </para>
     <para>
      While the <option>pg_num</option> value for existing pools cannot be
      reduced. The <option>pgp_num</option> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <option>pgp_num</option> value can
      be adjusted with:
     </para>
<screen>&prompt.root;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      One or more pools has a <option>pgp_num</option> value less than
      <option>pg_num</option>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <option>pgp_num</option> to match
      <option>pg_num</option>, triggering the data migration, with:
     </para>
<screen>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      One or more pools has an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <option>mon_pg_warn_max_object_skew</option> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <option>mon_pg_warn_max_object_skew</option> config option on the
      monitors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED¶</term>
    <listitem>
     <para>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </para>
<screen>&prompt.root;rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </para>
<screen>&prompt.root;ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <option>mon_pool_quota_crit_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.root;ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.root;ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <option>mon_pool_quota_warn_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.root;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.root;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      One or more objects in the cluster is not stored on the node the cluster
      would like it to be stored on. This is an indication that data migration
      due to some recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in itself. Data consistency is never at
      risk, and old copies of objects are never removed until the desired
      number of new copies (in the desired locations) are present.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to the 'unfound' objects will
      block. Ideally, a down OSD can be brought back online that has the more
      recent copy of the unfound object. Candidate OSDs can be identified from
      the peering state for the PG(s) responsible for the unfound object:
     </para>
<screen>&prompt.root;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </para>
<screen>&prompt.root;ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      You can see a summary of the slowest recent requests:
     </para>
<screen>&prompt.root;ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      You can find tThe location of an OSD with:
     </para>
<screen>&prompt.root;ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      One or more OSD requests has been blocked for an extremely long time.
      This is an indication that either the cluster has been unhealthy for an
      extended period of time (for example not enough running OSDs) or there is
      some internal problem with the OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs has not been scrubbed recently. PGs are normally scrubbed
      every <option>mon_scrub_interval</option> seconds, and this warning
      triggers when <option>mon_warn_not_scrubbed</option> such intervals have
      elapsed without a scrub. PGs will not scrub if they are not flagged as
      clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </para>
<screen>&prompt.root;ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs has not been deep scrubbed recently. PGs are normally
      scrubbed every <option>osd_deep_mon_scrub_interval</option> seconds, and
      this warning triggers when <option>mon_warn_not_deep_scrubbed</option>
      such intervals have elapsed without a scrub. PGs will not (deep)scrub if
      they are not flagged as clean, which may happen if they are misplaced or
      degraded (see PG_AVAILABILITY and PG_DEGRADED above). You can manually
      initiate a scrub of a clean PG with:
     </para>
<screen>&prompt.root;ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.root;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Watching a Cluster</title>

  <para>
   You can find the immediate state of the cluster using <command>ceph
    -s</command>. For example, a tiny &ceph; cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </para>

<screen>cluster:
  id:     6586341d-4565-3755-a4fd-b50f51bee248
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum blueshark1,blueshark2,blueshark3
  mgr: blueshark3(active), standbys: blueshark2, blueshark1
  osd: 15 osds: 15 up, 15 in

data:
  pools:   8 pools, 340 pgs
  objects: 537 objects, 1985 MB
  usage:   23881 MB used, 5571 GB / 5595 GB avail
  pgs:     340 active+clean

io:
  client:   100 MB/s rd, 26256 op/s rd, 0 op/s wr</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>
  <para>
   Other commands that display immediate status information are:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>
  <para>
   To get the information updated in real time, just put any of these commands
   (including <command>ceph -s</command>) in a wait loop, for example:
  </para>
  <screen>&rootuser;while true ; do ceph -s ; sleep 10 ; done</screen>
  <para>
   Hit <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>
   when you are tired of watching.
  </para>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Checking a Cluster's Usage Stats</title>

  <para>
   To check a cluster’s data usage and data distribution among pools, you can
   use the <command>df</command> option. It is similar to Linux
   <command>df</command>. Execute the following:
  </para>

<screen>&prompt.root;ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    55886G     55826G       61731M          0.11
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    testpool     1          0         0        17676G           0
    ecpool       2      4077M      0.01        35352G        2102
    test1        3          0         0        17676G           0
    rbd          4         16         0        17676G           3
    rbd1         5         16         0        17676G           3
    ecpool1      6      5708M      0.02        35352G        2871</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
     Capacity</link> for additional details.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: The maximum available space in the given
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Checking a Cluster's Status</title>

  <para>
   To check a cluster's status, execute the following:
  </para>

<screen>&prompt.root;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.root;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.root;ceph osd tree</screen>

  <para>
   &ceph; will print out a CRUSH tree with a host, its OSDs, whether they are
   up and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD nodes allowing
   the cluster to redistribute data to the newly available storage.
  </para>

  <para>
   If you cannot start an OSD because it is full, you may delete some data by
   deleting some placement group directories in the full OSD.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Checking Monitor Status</title>

  <para>
   If your cluster has multiple monitors (likely), you should check the monitor
   quorum status after you start the cluster before reading and/or writing
   data. A quorum must be present when multiple monitors are running. You
   should also check monitor status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.root;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.root;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
   OSDs and Placement Groups.</link>
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to
   <link
   xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing
   a Configuration at Runtime</link>for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
</chapter>
