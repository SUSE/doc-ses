<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.monitor">
 <title>Determining Cluster State</title>
 <para>
  Once you have a running cluster, you may use the <command>ceph</command> tool
  to monitor your cluster. Determining the cluster state typically involves
  checking the status of OSD, monitor, placement group and metadata server.
  <remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster, and before you start reading and/or writing
   data, check your cluster’s health first. You can check on the health of
   your &ceph; cluster with the following:
  </para>

<screen>&prompt.root;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   If you specified non-default locations for your configuration or keyring,
   you may specify their locations:
  </para>

<screen>&prompt.root;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>

  <para>
   Upon starting the &ceph; cluster, you will likely encounter a health warning
   such as <literal>HEALTH_WARN XXX num placement groups stale</literal>. Wait
   a few moments and check it again. When your cluster is ready, <command>ceph
   health</command> should return a message such as
   <literal>HEALTH_OK</literal>. At that point, it is okay to begin using the
   cluster.
  </para>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Watching a Cluster</title>

  <para>
   To watch the cluster’s ongoing events, open a new terminal and enter:
  </para>

<screen>&prompt.root;ceph -w</screen>

  <para>
   &ceph; will print each event. For example, a tiny &ceph; cluster consisting
   of one monitor, and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41338: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
             952 active+clean

2014-06-02 15:45:21.655871 osd.0 [INF] 17.71 deep-scrub ok
2014-06-02 15:45:47.880608 osd.1 [INF] 1.0 scrub ok
2014-06-02 15:45:48.865375 osd.1 [INF] 1.3 scrub ok
2014-06-02 15:45:50.866479 osd.1 [INF] 1.4 scrub ok
[...]
2014-06-02 15:45:55.720929 mon.0 [INF] pgmap v41343: 952 pgs: \
 1 active+clean+scrubbing+deep, 951 active+clean; 17130 MB data, 115 GB used, \
 167 GB / 297 GB avail</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Checking a Cluster’s Usage Stats</title>

  <para>
   To check a cluster’s data usage and data distribution among pools, you can
   use the <command>df</command> option. It is similar to Linux
   <command>df</command>. Execute the following:
  </para>

<screen>&prompt.root;ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    27570M     27304M         266M          0.97
POOLS:
    NAME             ID     USED     %USED     MAX AVAIL     OBJECTS
    data             0       120         0         5064M           4
    metadata         1         0         0         5064M           0
    rbd              2         0         0         5064M           0
    hot-storage      4       134         0         4033M           2
    cold-storage     5      227k         0         5064M           1
    pool1            6         0         0         5064M           0</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
     Capacity</link> for additional details.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Checking a Cluster’s Status</title>

  <para>
   To check a cluster’s status, execute the following:
  </para>

<screen>&prompt.root;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.root;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.root;ceph osd tree</screen>

  <para>
   &ceph; will print out a CRUSH tree with a host, its OSDs, whether they are
   up and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD nodes allowing
   the cluster to redistribute data to the newly available storage.
  </para>

  <para>
   If you cannot start an OSD because it is full, you may delete some data by
   deleting some placement group directories in the full OSD.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Checking Monitor Status</title>

  <para>
   If your cluster has multiple monitors (likely), you should check the monitor
   quorum status after you start the cluster before reading and/or writing
   data. A quorum must be present when multiple monitors are running. You
   should also check monitor status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.root;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.root;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.root;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
<!-- 2015-11-27 tbazant: MDS not supported yet
  <sect1 xml:id="monitor.mdsstatus">
   <title>Checking MDS Status</title>
   <para>
    Metadata servers provide metadata services for &ceph; FS. Metadata
    servers have two sets of states: <literal>up | down</literal> and
    <literal>active | inactive</literal>. To ensure your metadata servers
    are <literal>up</literal> and <literal>active</literal>, execute the
    following:
   </para>
<screen>ceph mds stat</screen>
   <para>
    To display details of the metadata cluster, execute the following:
   </para>
<screen>ceph mds dump</screen>
  </sect1>
  -->
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
   OSDs and Placement Groups.</link>
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to
   <link
   xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing
   a Configuration at Runtime</link>for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
</chapter>
