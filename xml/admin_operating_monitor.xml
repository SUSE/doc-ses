<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.monitor">
 <title>Determining Cluster State</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  When you have a running cluster, you may use the <command>ceph</command> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of &osd;s, &mon;s, placement groups and &mds;s.
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.status">
  <title>Checking a Cluster's Status</title>

  <para>
   To check a cluster's status, execute the following:
  </para>

<screen>&prompt.cephuser;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </para>

<screen>&prompt.cephuser;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.cephuser;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   The &ceph; cluster returns one of the following health codes:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </para>
     <para>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<filename>/var/log/ceph/ceph-osd.*</filename>) may contain debugging
      information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, for example OSD_HOST_DOWN</term>
    <listitem>
     <para>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </para>
<screen>&prompt.cephuser;ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      The usage thresholds for <emphasis>backfillfull</emphasis> (defaults to
      0.90), <emphasis>nearfull</emphasis> (defaults to 0.85),
      <emphasis>full</emphasis> (defaults to 0.95), and/or
      <emphasis>failsafe_full</emphasis> are not ascending. In particular, we
      expect <emphasis>backfillfull</emphasis> &lt;
      <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt;
      <emphasis>full</emphasis>, and <emphasis>full</emphasis> &lt;
      <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      To read the current values, run:
     </para>
<screen>
&prompt.cephuser;ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      The thresholds can be adjusted with the following commands:
     </para>
<screen>&prompt.cephuser;ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>full</emphasis> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
     <para>
      The currently defined <emphasis>full</emphasis> ratio can be seen with:
     </para>
<screen>&prompt.cephuser;ceph osd dump | grep full_ratio</screen>
     <para>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </para>
<screen>&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>backfillfull</emphasis>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>nearfull</emphasis>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      One or more cluster flags of interest has been set. With the exception of
      <emphasis>full</emphasis>, these flags can be set or cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd set <replaceable>flag</replaceable>
&prompt.cephuser;ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         The cluster is flagged as full and cannot service writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Paused reads or writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs are not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <emphasis>down</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs that were previously marked <emphasis>out</emphasis> will not be
         marked back <emphasis>in</emphasis> when they start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs will not automatically be marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Recovery or data rebalancing is suspended.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing (see <xref linkend="scrubbing"/>) is disabled.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Cache tiering activity is suspended.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSD is not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Failure reports for this OSD will be ignored.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         If this OSD was previously marked <emphasis>out</emphasis>
         automatically after a failure, it will not be marked
         <emphasis>in</emphasis> when it starts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         If this OSD is down, it will not be automatically marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Per-OSD flags can be set and cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
&prompt.cephuser;ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      The &crushmap; is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <option>mon_crush_min_required_version</option>
      configuration option.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      The &crushmap; is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The &crushmap; should be
      updated to use the newer method (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
     <para>
      For more information on cache tiering, see
      <xref linkend="cha.ceph.tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      No pre-luminous v12 OSDs are running but the <option>sortbitwise</option>
      flag has not been set. You need to set the <option>sortbitwise</option>
      flag before luminous v12 or newer OSDs can start:
     </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </para>
<screen>&prompt.cephuser;ceph df detail</screen>
     <para>
      You can either raise the pool quota with
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      or delete some existing data to reduce usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow IO
      requests to be serviced. Problematic PG states include
      <emphasis>peering</emphasis>, <emphasis>stale</emphasis>,
      <emphasis>incomplete</emphasis>, and the lack of
      <emphasis>active</emphasis> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <emphasis>degraded</emphasis> or
      <emphasis>undersized</emphasis> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <emphasis>clean</emphasis> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <emphasis>backfill_toofull</emphasis> or
      <emphasis>recovery_toofull</emphasis> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <emphasis>backfillfull</emphasis> threshold.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Data scrubbing (see <xref linkend="scrubbing"/>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <emphasis>inconsistent</emphasis> or
      <emphasis>snaptrim_error</emphasis> flag is set, indicating an earlier
      scrub operation found a problem, or that the <emphasis>repair</emphasis>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Recent OSD scrubs have uncovered inconsistencies.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      A cache tier pool is nearly full. Full in this context is determined by
      the <emphasis>target_max_bytes</emphasis> and
      <emphasis>target_max_objects</emphasis> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </para>
     <para>
      Find more information about cache tiering in
      <xref linkend="cha.ceph.tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is below the configurable threshold of
      <option>mon_pg_warn_min_per_osd</option> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is above the configurable threshold of
      <option>mon_pg_warn_max_per_osd</option> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the &mgr;s and &mon;s.
     </para>
     <para>
      While the <option>pg_num</option> value for existing pools cannot be
      reduced. The <option>pgp_num</option> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <option>pgp_num</option> value can
      be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      One or more pools has a <option>pgp_num</option> value less than
      <option>pg_num</option>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <option>pgp_num</option> to match
      <option>pg_num</option>, triggering the data migration, with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <option>mon_pg_warn_max_object_skew</option> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <option>mon_pg_warn_max_object_skew</option> configuration option on the
      monitors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED¶</term>
    <listitem>
     <para>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </para>
<screen>&prompt.cephuser;rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </para>
<screen>&prompt.cephuser;ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <option>mon_pool_quota_crit_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <option>mon_pool_quota_warn_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      One or more objects in the cluster are not stored on the node where the
      cluster wants it. This is an indication that data migration caused by a
      recent cluster change has not yet completed. Misplaced data is not a
      dangerous condition in itself. Data consistency is never at risk, and old
      copies of objects are never removed until the desired number of new
      copies (in the desired locations) are present.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, a down OSD can be brought back online that has the more
      recent copy of the unfound object. Candidate OSDs can be identified from
      the peering state for the PG(s) responsible for the unfound object:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      You can see a summary of the slowest recent requests:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      You can find the location of an OSD with:
     </para>
<screen>&prompt.cephuser;ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      One or more OSD requests have been blocked for a longer time, for example
      4096 seconds. This is an indication that either the cluster has been
      unhealthy for an extended period of time (for example not enough running
      OSDs or inactive PGs) or there is some internal problem with the OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs have not been scrubbed (see <xref linkend="scrubbing"/>)
      recently. PGs are normally scrubbed every
      <option>mon_scrub_interval</option> seconds, and this warning triggers
      when <option>mon_warn_not_scrubbed</option> such intervals have elapsed
      without a scrub. PGs will not scrub if they are not flagged as clean,
      which may happen if they are misplaced or degraded (see PG_AVAILABILITY
      and PG_DEGRADED above). You can manually initiate a scrub of a clean PG
      with:
     </para>
<screen>&prompt.cephuser;ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs has not been deep scrubbed (see
      <xref linkend="scrubbing"/>) recently. PGs are normally scrubbed every
      <option>osd_deep_mon_scrub_interval</option> seconds, and this warning
      triggers when <option>mon_warn_not_deep_scrubbed</option> seconds have
      elapsed without a scrub. PGs will not (deep)scrub if they are not flagged
      as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </para>
<screen>&prompt.cephuser;ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.root;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Watching a Cluster</title>

  <para>
   You can find the immediate state of the cluster using <command>ceph
   -s</command>. For example, a tiny &ceph; cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </para>

<screen>
&prompt.cephuser;ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 > max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean
</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &mgr;s.
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &ogw;s.
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>

  <para>
   Other commands that display immediate status information are:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To get the information updated in real time, put any of these commands
   (including <command>ceph -s</command>) as an argument of the
   <command>watch</command> command:
  </para>

<screen>&prompt.root;watch -n 10 'ceph -s'</screen>

  <para>
   Press <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>
   when you are tired of watching.
  </para>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Checking a Cluster's Usage Stats</title>

  <para>
   To check a cluster’s data usage and distribution among pools, use the
   <command>ceph df</command> command. To get more details, use <command>ceph
   df detail</command>.
  </para>

<screen>
&prompt.cephuser;ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    65886G     45826G        7731M            16
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    data         1      1726M        10        17676G        1629
    rbd          4      5897M        27        22365G        3547
    ecpool       6        69M       0.2        35352G          31
[...]
</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See <xref linkend="storage_capacity"/> for
     additional details.
    </para>
    <note>
     <title>Cluster Fill Level</title>
     <para>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </para>
     <para>
      Use the command <command>ceph osd df tree</command> to list the fill
      level of all OSDs.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: The maximum available space in the given
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.cephuser;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.cephuser;ceph osd tree</screen>

  <para>
   &ceph; will print a CRUSH tree with a host, its OSDs, whether they are up
   and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Checking Monitor Status</title>

  <para>
   After you start the cluster and before first reading and/or writing data,
   check the &mon;s quorum status. When the cluster is already serving
   requests, check the &mon;s status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.cephuser;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.cephuser;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <xref linkend="op.mon_osd_pg"/>.
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to <xref linkend="ceph.config.runtime"/> for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
 <sect1 xml:id="storage_capacity">
  <title>Storage Capacity</title>

  <para>
   When a &ceph; storage cluster gets close to its maximum capacity, &ceph;
   prevents you from writing to or reading from &osd;s as a safety measure to
   prevent data loss. Therefore, letting a production cluster approach its full
   ratio is not a good practice, because it sacrifices high availability. The
   default full ratio is set to .95, meaning 95% of capacity. This a very
   aggressive setting for a test cluster with a small number of OSDs.
  </para>

  <tip>
   <title>Increase Storage Capacity</title>
   <para>
    When monitoring your cluster, be alert to warnings related to the
    <literal>nearfull</literal> ratio. It means that a failure of some OSDs
    could result in a temporary service disruption if one or more OSDs fails.
    Consider adding more OSDs to increase storage capacity.
   </para>
  </tip>

  <para>
   A common scenario for test clusters involves a system administrator removing
   a &osd; from the &ceph; storage cluster to watch the cluster rebalance. Then
   removing another &osd;, and so on until the cluster eventually reaches the
   full ratio and locks up. We recommend a bit of capacity planning even with a
   test cluster. Planning enables you to estimate how much spare capacity you
   will need in order to maintain high availability. Ideally, you want to plan
   for a series of &osd; failures where the cluster can recover to an
   <literal>active + clean</literal> state without replacing those &osd;s
   immediately. You can run a cluster in an <literal>active +
   degraded</literal> state, but this is not ideal for normal operating
   conditions.
  </para>

  <para>
   The following diagram depicts a simplistic &ceph; storage cluster containing
   33 &ceph; nodes with one &osd; per host, each of them reading from and
   writing to a 3TB drive. This exemplary cluster has a maximum actual capacity
   of 99TB. The <option>mon osd full ratio</option> option is set to 0.95. If
   the cluster falls to 5TB of the remaining capacity, it will not allow the
   clients to read and write data. Therefore the storage cluster’s operating
   capacity is 95TB, not 99TB.
  </para>

  <figure>
   <title>&ceph; Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   It is normal in such a cluster for one or two OSDs to fail. A less frequent
   but reasonable scenario involves a rack’s router or power supply failing,
   which brings down multiple OSDs simultaneously (for example, OSDs 7-12). In
   such a scenario, you should still strive for a cluster that can remain
   operational and achieve an <literal>active + clean</literal> state–even if
   that means adding a few hosts with additional OSDs in short order. If your
   capacity utilization is too high, you may not lose data. But you could still
   sacrifice data availability while resolving an outage within a failure
   domain if capacity utilization of the cluster exceeds the full ratio. For
   this reason, we recommend at least some rough capacity planning.
  </para>

  <para>
   Identify two numbers for your cluster:
  </para>

  <orderedlist>
   <listitem>
    <para>
     The number of OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     The total capacity of the cluster.
    </para>
   </listitem>
  </orderedlist>

  <para>
   If you divide the total capacity of your cluster by the number of OSDs in
   your cluster, you will find the mean average capacity of an OSD within your
   cluster. Consider multiplying that number by the number of OSDs you expect
   will fail simultaneously during normal operations (a relatively small
   number). Finally multiply the capacity of the cluster by the full ratio to
   arrive at a maximum operating capacity. Then, subtract the number of amount
   of data from the OSDs you expect to fail to arrive at a reasonable full
   ratio. Repeat the foregoing process with a higher number of OSD failures (a
   rack of OSDs) to arrive at a reasonable number for a near full ratio.
  </para>

  <para>
   The following settings only apply on cluster creation and are then stored in
   the OSD map:
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    These settings only apply during cluster creation. Afterwards they need to
    be changed in the OSD Map using the <command>ceph osd
    set-nearfull-ratio</command> and <command>ceph osd set-full-ratio</command>
    commands.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered
      <literal>full</literal>. Default is .95
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered too
      <literal>full</literal> to backfill. Default is .90
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered
      <literal>nearfull</literal>. Default is .85
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Check OSD Weight</title>
   <para>
    If some OSDs are <literal>nearfull</literal>, but others have plenty of
    capacity, you may have a problem with the CRUSH weight for the
    <literal>nearfull</literal> OSDs.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op.mon_osd_pg">
  <title>Monitoring OSDs and Placement Groups</title>

  <para>
   High availability and high reliability require a fault-tolerant approach to
   managing hardware and software issues. &ceph; has no single
   point-of-failure, and can service requests for data in a 'degraded' mode.
   &ceph;’s data placement introduces a layer of indirection to ensure that
   data does not bind directly to particular OSD addresses. This means that
   tracking down system faults requires finding the placement group and the
   underlying OSDs at root of the problem.
  </para>

  <tip>
   <title>Access in Case of Failure</title>
   <para>
    A fault in one part of the cluster may prevent you from accessing a
    particular object. That does not mean that you cannot access other objects.
    When you run into a fault, follow the steps for monitoring your OSDs and
    placement groups. Then begin troubleshooting.
   </para>
  </tip>

  <para>
   &ceph; is generally self-repairing. However, when problems persist,
   monitoring OSDs and placement groups will help you identify the problem.
  </para>

  <sect2 xml:id="op.mon_osds">
   <title>Monitoring OSDs</title>
   <para>
    An OSD’s status is either <emphasis>in the cluster</emphasis> ('in') or
    <emphasis>out of the cluster</emphasis> ('out'). At the same time, it is
    either <emphasis>up and running</emphasis> ('up'), or it is <emphasis>down
    and not running</emphasis> ('down'). If an OSD is 'up', it may be either in
    the cluster (you can read and write data) or it is out of the cluster. If
    it was in the cluster and recently moved out of the cluster, &ceph; will
    migrate placement groups to other OSDs. If an OSD is out of the cluster,
    CRUSH will not assign placement groups to it. If an OSD is 'down', it
    should also be 'out'.
   </para>
   <note>
    <title>Unhealthy State</title>
    <para>
     If an OSD is 'down' and 'in', there is a problem and the cluster will not
     be in a healthy state.
    </para>
   </note>
   <para>
    If you execute a command such as <command>ceph health</command>,
    <command>ceph -s</command> or <command>ceph -w</command>, you may notice
    that the cluster does not always echo back <literal>HEALTH OK</literal>.
    With respect to OSDs, you should expect that the cluster will
    <emphasis>not</emphasis> echo <literal>HEALTH OK</literal> under the
    following circumstances:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You have not started the cluster yet (it will not respond).
     </para>
    </listitem>
    <listitem>
     <para>
      You have just started or restarted the cluster and it is not ready yet,
      because the placement groups are getting created and the OSDs are in the
      process of peering.
     </para>
    </listitem>
    <listitem>
     <para>
      You just added or removed an OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just modified your cluster map.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    An important aspect of monitoring OSDs is to ensure that when the cluster
    is up and running that all OSDs that are in the cluster are up and running,
    too. To see if all OSDs are running, execute:
   </para>
<screen>
&prompt.root;ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    The result should tell you the total number of OSDs (x), how many are 'up'
    (y), how many are 'in' (z) and the map epoch (eNNNN). If the number of OSDs
    that are 'in' the cluster is more than the number of OSDs that are 'up',
    execute the following command to identify the <literal>ceph-osd</literal>
    daemons that are not running:
   </para>
<screen>
&prompt.root;ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    If an OSD with for example ID 1 is down, start it:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd@1.service
</screen>
   <para>
    See <xref linkend="op.osd_not_running"/> for problems associated with OSDs
    that stopped, or will not restart.
   </para>
  </sect2>

  <sect2 xml:id="op.pgsets">
   <title>Placement Group Sets</title>
   <para>
    When CRUSH assigns placement groups to OSDs, it looks at the number of
    replicas for the pool and assigns the placement group to OSDs such that
    each replica of the placement group gets assigned to a different OSD. For
    example, if the pool requires three replicas of a placement group, CRUSH
    may assign them to <literal>osd.1</literal>, <literal>osd.2</literal> and
    <literal>osd.3</literal> respectively. CRUSH actually seeks a pseudo-random
    placement that will take into account failure domains you set in your
    &crushmap;, so you will rarely see placement groups assigned to nearest
    neighbor OSDs in a large cluster. We refer to the set of OSDs that should
    contain the replicas of a particular placement group as the &actingset;. In
    some cases, an OSD in the acting set is down or otherwise not able to
    service requests for objects in the placement group. When these situations
    arise, it may match one of the following scenarios:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You added or removed an OSD. Then, CRUSH reassigned the placement group
      to other OSDs and therefore changed the composition of the &actingset;,
      causing the migration of data with a 'backfill' process.
     </para>
    </listitem>
    <listitem>
     <para>
      An OSD was 'down', was restarted, and is now recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      An OSD in the &actingset; is 'down' or unable to service requests, and
      another OSD has temporarily assumed its duties.
     </para>
     <para>
      &ceph; processes a client request using the &upset;, which is the set of
      OSDs that will actually handle the requests. In most cases, the &upset;
      and the &actingset; are virtually identical. When they are not, it may
      indicate that &ceph; is migrating data, an OSD is recovering, or that
      there is a problem (for example, &ceph; usually echoes a <literal>HEALTH
      WARN</literal> state with a 'stuck stale' message in such scenarios).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To retrieve a list of placement groups, run:
   </para>
<screen>
&prompt.root;ceph pg dump
</screen>
   <para>
    To view which OSDs are within the &actingset; or the &upset; for a given
    placement group, run:
   </para>
<screen>
&prompt.root;ceph pg map<replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -> up [0,1,2] acting [0,1,2]
</screen>
   <para>
    The result should tell you the osdmap epoch (eNNN), the placement group
    number (<replaceable>PG_NUM</replaceable>), the OSDs in the &upset; ('up'),
    and the OSDs in the &actingset; ('acting'):
   </para>
   <tip>
    <title>Cluster Problem Indicator</title>
    <para>
     If the &upset; and &actingset; do not match, this may be an indicator
     either of the cluster rebalancing itself, or of a potential problem with
     the cluster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op.peering">
   <title>Peering</title>
   <para>
    Before you can write data to a placement group, it must be in an 'active'
    state, and it should be in a 'clean' state. For &ceph; to determine the
    current state of a placement group, the primary OSD of the placement group
    (the first OSD in the &actingset;), peers with the secondary and tertiary
    OSDs to establish agreement on the current state of the placement group
    (assuming a pool with 3 replicas of the PG).
   </para>
   <figure>
    <title>Peering Schema</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op.mon_pg_states">
   <title>Monitoring Placement Group States</title>
   <para>
    If you execute a command such as <command>ceph health</command>,
    <command>ceph -s</command> or <command>ceph -w</command>, you may notice
    that the cluster does not always echo back the <literal>HEALTH OK</literal>
    message. After you check to see if the OSDs are running, you should also
    check placement group states.
   </para>
   <para>
    Expect that the cluster will <emphasis role="bold">not</emphasis> echo
    <literal>HEALTH OK</literal> in a number of placement group peering-related
    circumstances:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You have just created a pool and placement groups have not peered yet.
     </para>
    </listitem>
    <listitem>
     <para>
      The placement groups are recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just added an OSD to or removed an OSD from the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just modified your &crushmap; and your placement groups are
      migrating.
     </para>
    </listitem>
    <listitem>
     <para>
      There is inconsistent data in different replicas of a placement group.
     </para>
    </listitem>
    <listitem>
     <para>
      &ceph; is scrubbing a placement group’s replicas.
     </para>
    </listitem>
    <listitem>
     <para>
      &ceph; does not have enough storage capacity to complete backfilling
      operations.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If one of the above mentioned circumstances causes &ceph; to echo
    <literal>HEALTH WARN</literal>, do not panic. In many cases, the cluster
    will recover on its own. In some cases, you may need to take action. An
    important aspect of monitoring placement groups is to ensure that when the
    cluster is up and running that all placement groups are 'active', and
    preferably in the 'clean state'. To see the status of all placement groups,
    run:
   </para>
<screen>
&prompt.root;ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    The result should tell you the total number of placement groups (x), how
    many placement groups are in a particular state such as 'active+clean' (y)
    and the amount of data stored (z).
   </para>
   <para>
    In addition to the placement group states, &ceph; will also echo back the
    amount of storage capacity used (aa), the amount of storage capacity
    remaining (bb), and the total storage capacity for the placement group.
    These numbers can be important in a few cases:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You are reaching your <option>near full ratio</option> or <option>full
      ratio</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Your data is not getting distributed across the cluster due to an error
      in your CRUSH configuration.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Placement Group IDs</title>
    <para>
     Placement group IDs consist of the pool number (not pool name) followed by
     a period (.) and the placement group ID–a hexadecimal number. You can
     view pool numbers and their names from the output of <command>ceph osd
     lspools</command>. For example, the default pool <literal>rbd</literal>
     corresponds to pool number 0. A fully qualified placement group ID has the
     following form:
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     And it typically looks like this:
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    To retrieve a list of placement groups, run the following:
   </para>
<screen>
&prompt.root;ceph pg dump
</screen>
   <para>
    You can also format the output in JSON format and save it to a file:
   </para>
<screen>
&prompt.root;ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    To query a particular placement group, run the following:
   </para>
<screen>
&prompt.root;ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    The following list describes the common placement group states in detail.
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING</term>
     <listitem>
      <para>
       When you create a pool, it will create the number of placement groups
       you specified. &ceph; will echo 'creating' when it is creating one or
       more placement groups. Once they are created, the OSDs that are part of
       the placement group’s &actingset; will peer. Once peering is complete,
       the placement group status should be 'active+clean', which means that a
       &ceph; client can begin writing to the placement group.
      </para>
      <figure>
       <title>Placement Groups Status</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING</term>
     <listitem>
      <para>
       When &ceph; is peering a placement group, it is bringing the OSDs that
       store the replicas of the placement group into agreement about the state
       of the objects and metadata in the placement group. When &ceph;
       completes peering, this means that the OSDs that store the placement
       group agree about the current state of the placement group. However,
       completion of the peering process does
       <emphasis role="bold">not</emphasis> mean that each replica has the
       latest contents.
      </para>
      <note>
       <title>Authoritative History</title>
       <para>
        &ceph; will <emphasis role="bold">not</emphasis> acknowledge a write
        operation to a client, until all OSDs of the &actingset; persist the
        write operation. This practice ensures that at least one member of the
        &actingset; will have a record of every acknowledged write operation
        since the last successful peering operation.
       </para>
       <para>
        With an accurate record of each acknowledged write operation, &ceph;
        can construct and enlarge a new authoritative history of the placement
        group&mdash;a complete, and fully ordered set of operations that, if
        performed, would bring an OSD’s copy of a placement group up to date.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ACTIVE</term>
     <listitem>
      <para>
       Once &ceph; completes the peering process, a placement group may become
       'active'. The 'active' state means that the data in the placement group
       is generally available in the primary placement group and the replicas
       for read and write operations.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN</term>
     <listitem>
      <para>
       When a placement group is in the 'clean' state, the primary OSD and the
       replica OSDs have successfully peered and there are no stray replicas
       for the placement group. &ceph; replicated all objects in the placement
       group the correct number of times.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       When a client writes an object to the primary OSD, the primary OSD is
       responsible for writing the replicas to the replica OSDs. After the
       primary OSD writes the object to storage, the placement group will
       remain in a 'degraded' state until the primary OSD has received an
       acknowledgement from the replica OSDs that &ceph; created the replica
       objects successfully.
      </para>
      <para>
       The reason a placement group can be 'active+degraded' is that an OSD may
       be 'active' even though it does not hold all of the objects yet. If an
       OSD goes down, &ceph; marks each placement group assigned to the OSD as
       'degraded'. The OSDs must peer again when the OSD comes back online.
       However, a client can still write a new object to a degraded placement
       group if it is 'active'.
      </para>
      <para>
       If an OSD is 'down' and the 'degraded' condition persists, &ceph; may
       mark the down OSD as 'out' of the cluster and remap the data from the
       'down' OSD to another OSD. The time between being marked 'down' and
       being marked 'out' is controlled by the <option>mon osd down out
       interval</option> option, which is set to 600 seconds by default.
      </para>
      <para>
       A placement group can also be 'degraded', because &ceph; cannot find one
       or more objects that should be in the placement group. While you cannot
       read or write to unfound objects, you can still access all of the other
       objects in the 'degraded' placement group.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING</term>
     <listitem>
      <para>
       &ceph; was designed for fault-tolerance at a scale where hardware and
       software problems are ongoing. When an OSD goes 'down', its contents may
       fall behind the current state of other replicas in the placement groups.
       When the OSD is back 'up', the contents of the placement groups must be
       updated to reflect the current state. During that time period, the OSD
       may reflect a 'recovering' state.
      </para>
      <para>
       Recovery is not always trivial, because a hardware failure may cause a
       cascading failure of multiple OSDs. For example, a network switch for a
       rack or cabinet may fail, which can cause the OSDs of a number of host
       machines to fall behind the current state of the cluster. Each one of
       the OSDs must recover once the fault is resolved.
      </para>
      <para>
       &ceph; provides a number of settings to balance the resource contention
       between new service requests and the need to recover data objects and
       restore the placement groups to the current state. The <option>osd
       recovery delay start</option> setting allows an OSD to restart, re-peer
       and even process some replay requests before starting the recovery
       process. The <option>osd recovery thread timeout</option> sets a thread
       timeout, because multiple OSDs may fail, restart and re-peer at
       staggered rates. The <option>osd recovery max active</option> setting
       limits the number of recovery requests an OSD will process
       simultaneously to prevent the OSD from failing to serve. The <option>osd
       recovery max chunk</option> setting limits the size of the recovered
       data chunks to prevent network congestion.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING</term>
     <listitem>
      <para>
       When a new OSD joins the cluster, CRUSH will reassign placement groups
       from OSDs in the cluster to the newly added OSD. Forcing the new OSD to
       accept the reassigned placement groups immediately can put excessive
       load on the new OSD. Backfilling the OSD with the placement groups
       allows this process to begin in the background. Once backfilling is
       complete, the new OSD will begin serving requests when it is ready.
      </para>
      <para>
       During the backfill operations, you may see one of several states:
       'backfill_wait' indicates that a backfill operation is pending, but is
       not yet in progress; 'backfill' indicates that a backfill operation is
       in progress; 'backfill_too_full' indicates that a backfill operation was
       requested, but could not be completed due to insufficient storage
       capacity. When a placement group cannot be backfilled, it may be
       considered 'incomplete'.
      </para>
      <para>
       &ceph; provides a number of settings to manage the load associated with
       reassigning placement groups to an OSD (especially a new OSD). By
       default, <option>osd max backfills</option> sets the maximum number of
       concurrent backfills to or from an OSD to 10. The <option>backfill full
       ratio</option> enables an OSD to refuse a backfill request if the OSD is
       approaching its full ratio (90%, by default) and change with
       <command>ceph osd set-backfillfull-ratio</command> command. If an OSD
       refuses a backfill request, the <option>osd backfill retry
       interval</option> enables an OSD to retry the request (after 10 seconds,
       by default). OSDs can also set o<option>sd backfill scan min</option>
       and <option>osd backfill scan max</option> to manage scan intervals (64
       and 512, by default).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED</term>
     <listitem>
      <para>
       When the &actingset; that services a placement group changes, the data
       migrates from the old &actingset; to the new &actingset;. It may take
       some time for a new primary OSD to service requests. So it may ask the
       old primary to continue to service requests until the placement group
       migration is complete. Once data migration completes, the mapping uses
       the primary OSD of the new &actingset;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STALE</term>
     <listitem>
      <para>
       While &ceph; uses heartbeats to ensure that hosts and daemons are
       running, the <literal>ceph-osd</literal> daemons may also get into a
       'stuck' state where they are not reporting statistics in a timely manner
       (for example, a temporary network fault). By default, OSD daemons report
       their placement group, boot and failure statistics every half second
       (0.5), which is more frequent than the heartbeat thresholds. If the
       Primary OSD of a placement group’s &actingset; fails to report to the
       monitor or if other OSDs have reported the primary OSD 'down', the
       monitors will mark the placement group 'stale'.
      </para>
      <para>
       When you start your cluster, it is common to see the 'stale' state until
       the peering process completes. After your cluster has been running for
       awhile, seeing placement groups in the 'stale' state indicates that the
       primary OSD for those placement groups is down or not reporting
       placement group statistics to the monitor.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op.pg_stuck_states">
   <title>Identifying Troubled Placement Groups</title>
   <para>
    As previously noted, a placement group is not necessarily problematic just
    because its state is not 'active+clean'. Generally, &ceph;’s ability to
    self repair may not be working when placement groups get stuck. The stuck
    states include:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Unclean</emphasis>: Placement groups contain
      objects that are not replicated the required number of times. They should
      be recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Inactive</emphasis>: Placement groups cannot
      process reads or writes because they are waiting for an OSD with the most
      up-to-date data to come back up.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Stale</emphasis>: Placement groups are in an
      unknown state, because the OSDs that host them have not reported to the
      monitor cluster in a while (configured by the <option>mon osd report
      timeout</option> option).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To identify stuck placement groups, run the following:
   </para>
<screen>
&prompt.root;ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
</screen>
  </sect2>

  <sect2 xml:id="op.pg_objectfinding">
   <title>Finding an Object Location</title>
   <para>
    To store object data in the &ceph; Object Store, a &ceph; client needs to
    set an object name and specify a related pool. The &ceph; client retrieves
    the latest cluster map and the CRUSH algorithm calculates how to map the
    object to a placement group, and then calculates how to assign the
    placement group to an OSD dynamically. To find the object location, all you
    need is the object name and the pool name. For example:
   </para>
<screen>
&prompt.root;ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>Locating an Object</title>
    <para>
     As an example, let us create an object. Specify an object name
     'test-object-1', a path to an example file 'testfile.txt' containing some
     object data, and a pool name 'data' using the <command>rados put</command>
     command on the command line:
    </para>
<screen>
&prompt.root;rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     To verify that the Ceph Object Store stored the object, run the following:
    </para>
<screen>
&prompt.root;rados -p data ls
</screen>
    <para>
     Now, identify the object location. &ceph; will output the object’s
     location:
    </para>
<screen>
&prompt.root;ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -> pg 0.d1743484 \
(0.4) -> up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     To remove the example object, simply delete it using the <command>rados
     rm</command> command:
    </para>
<screen>
&prompt.root;rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
 <sect1 xml:id="op.osd_not_running">
  <title>OSD is Not Running</title>

  <para>
   Under normal circumstances, simply restarting the
   <literal>ceph-osd</literal> daemon will allow it to rejoin the cluster and
   recover.
  </para>

  <sect2 xml:id="op.osd_not_start">
   <title>An OSD will Not Start</title>
   <para>
    If you start your cluster and an OSD will not start, check the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Configuration File</emphasis>: If you were not able
      to get OSDs running from a new installation, check your configuration
      file to ensure it conforms (for example, <literal>host</literal> and not
      <literal>hostname</literal>).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Check Paths</emphasis>: Check the paths in your
      configuration, and the actual paths themselves for data and journals. If
      you separate the OSD data from the journal data and there are errors in
      your configuration file or in the actual mounts, you may have trouble
      starting OSDs. If you want to store the journal on a block device, you
      need to partition your journal disk and assign one partition per OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Check Max Threadcount</emphasis>: If you have a
      node with a lot of OSDs, you may be hitting the default maximum number of
      threads (usually 32,000), especially during recovery. You can increase
      the number of threads using the <command>sysctl</command> command to see
      if increasing the maximum number of threads to the maximum possible
      number of threads allowed (for example 4194303) will help:
     </para>
<screen>
&prompt.root;sysctl -w kernel.pid_max=4194303
</screen>
     <para>
      If increasing the maximum thread count resolves the issue, you can make
      it permanent by including a <option>kernel.pid_max</option> setting in
      the <filename>/etc/sysctl.conf</filename> file:
     </para>
<screen>
kernel.pid_max = 4194303
</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="op.osd_failed">
   <title>An OSD Failed</title>
   <para>
    When the <literal>ceph-osd</literal> process dies, the monitor will learn
    about the failure from surviving <literal>ceph-osd</literal> daemons and
    report it via the <command>ceph health</command> command:
   </para>
<screen>
&prompt.root;ceph health
HEALTH_WARN 1/3 in osds are down
</screen>
   <para>
    Specifically, you will get a warning whenever there are
    <literal>ceph-osd</literal> processes that are marked 'in' and 'down'. You
    can identify which<literal> ceph-osds</literal> are down with:
   </para>
<screen>
&prompt.root;ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
</screen>
   <para>
    If there is a disk failure or other fault preventing
    <literal>ceph-osd</literal> from functioning or restarting, an error
    message should be present in its log file in
    <filename>/var/log/ceph</filename>.
   </para>
   <para>
    If the daemon stopped because of a heartbeat failure, the underlying kernel
    file system may be unresponsive. Check the <command>dmesg</command> command
    output for disk or other kernel errors.
   </para>
  </sect2>

  <sect2 xml:id="op.no_disk_space">
   <title>No Free Disk Space</title>
   <para>
    &ceph; prevents you from writing to a full OSD to prevent losing data. In
    an operational cluster, you should receive a warning when your cluster is
    getting near its full ratio. The <option>mon osd full ratio</option> option
    defaults to 0.95, or 95% of capacity before it stops clients from writing
    data. The <option>mon osd backfillfull ratio</option> defaults to 0.90, or
    90 % of capacity when it blocks backfills from starting. The OSD nearfull
    ratio defaults to 0.85, or 85% of capacity when it generates a health
    warning. You can change the value of 'nearfull' with the following command:
   </para>
<screen>
&prompt.root;ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    Full cluster issues usually arise when testing how &ceph; handles an OSD
    failure on a small cluster. When one node has a high percentage of the
    cluster’s data, the cluster can easily eclipse its 'nearfull' and 'full'
    ratio immediately. If you are testing how &ceph; reacts to OSD failures on
    a small cluster, you should leave sufficient free disk space and consider
    temporarily lowering the OSD full ratio, OSD backfillfull ratio and OSD
    nearfull ratio using these commands:
   </para>
<screen>
&prompt.root;ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
&prompt.root;ceph osd set-full-ratio <replaceable>0.0 to 1.0</replaceable>
&prompt.root;ceph osd set-backfillfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    Full <literal>ceph-osds</literal> will be reported by <command>ceph
    health</command>:
   </para>
<screen>
&prompt.root;ceph health
HEALTH_WARN 1 nearfull osd(s)
</screen>
   <para>
    or
   </para>
<screen>
&prompt.root;ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
   <para>
    The best way to deal with a full cluster is to add new
    <literal>ceph-osd</literal>s, allowing the cluster to redistribute data to
    the newly available storage.
   </para>
   <para>
    If you cannot start an OSD because it is full, you may delete some data by
    deleting some placement group directories in the full OSD.
   </para>
   <important>
    <title>Deleting a Placement Group Directory</title>
    <para>
     If you choose to delete a placement group directory on a full OSD,
     <emphasis role="bold">do not</emphasis> delete the same placement group
     directory on another full OSD, or you may <emphasis role="bold">lose
     data</emphasis>. You <emphasis role="bold">must</emphasis> maintain at
     least one copy of your data on at least one OSD.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="monitoring_alerting">
  <title>Monitoring and Alerting</title>

  <para>
   By default, &deepsea; deploys a monitoring and alerting stack on the
   &smaster;. It consists of the following components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">Prometheus</emphasis> monitoring and alerting
     toolkit.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Grafana</emphasis> visualization and alerting
     software.
    </para>
   </listitem>
   <listitem>
    <para>
     The <systemitem class="daemon">prometheus-ceph_exporter</systemitem>
     service running on the &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     The <systemitem class="daemon">prometheus-node_exporter</systemitem>
     service running on all &sminion;s.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The Prometheus configuration and <emphasis>scrape</emphasis> targets
   (exporting daemons) are setup automatically by &deepsea;. &deepsea; also
   deploys a list of default alerts, for example <literal>health
   error</literal>, <literal>10% OSDs down</literal>, or <literal>pgs
   inactive</literal>.
  </para>

  <sect2 xml:id="alerting.alermanager">
   <title>Alertmanager</title>
   <para>
    The Alertmanager handles alerts sent by the Prometheus server. It takes
    care of deduplicating, grouping, and routing them to the correct receiver.
    It also takes care of silencing of alerts. Alertmanager is configured via
    the command line flags and a configuration file that defines inhibition
    rules, notification routing and notification receivers.
   </para>
   <sect3>
    <title>Configuration File</title>
    <para>
     Alertmanager's configuration is different for each deployment. Therefore,
     &deepsea; does not ship any related defaults. You need to provide your own
     <filename>alertmanager.yml</filename> configuration file. The
     <package>alertmanager</package> package by default installs a
     configuration file <filename>/etc/prometheus/alertmanager.yml</filename>
     which can serve as an example configuration. If you prefer to have your
     Alertmanager configuration managed by &deepsea;, add the following key to
     your pillar, for example to the
     <filename>/srv/pillar/ceph/stack/ceph/minions/<replaceable>YOUR_SALT_MASTER_MINION_ID</replaceable>.sls</filename>
     file:
    </para>
    <para>
     For a complete example of Alertmanager's configuration file, see
     <xref linkend="app.alerting.default"/>.
    </para>
<screen>
monitoring:
  alertmanager_config:
    /path/to/your/alertmanager/config.yml
</screen>
    <para>
     Alertmanager's configuration file is written in the YAML format. It
     follows the scheme described below. Parameters in brackets are optional.
     For non-list parameters the default value is used. The following generic
     placeholders are used in the scheme:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>DURATION</replaceable></term>
      <listitem>
       <para>
        A duration matching the regular expression
        <literal>[0-9]+(ms|[smhdwy])</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>LABELNAME</replaceable></term>
      <listitem>
       <para>
        A string matching the regular expression
        <literal>[a-zA-Z_][a-zA-Z0-9_]*</literal>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>LABELVALUE</replaceable></term>
      <listitem>
       <para>
        A string of Unicode characters.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>FILEPATH</replaceable></term>
      <listitem>
       <para>
        A valid path in the current working directory.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>BOOLEAN</replaceable></term>
      <listitem>
       <para>
        A boolean that can take the values 'true' or 'false'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>STRING</replaceable></term>
      <listitem>
       <para>
        A regular string.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>SECRET</replaceable></term>
      <listitem>
       <para>
        A regular string that is a secret, for example a password.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TMPL_STRING</replaceable></term>
      <listitem>
       <para>
        A string which is template-expanded before usage.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TMPL_SECRET</replaceable></term>
      <listitem>
       <para>
        A secret string which is template-expanded before usage.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <example>
     <title>Global Configuration</title>
     <para>
      Parameters in the <literal>global:</literal> configuration are valid in
      all other configuration contexts. They also serve as defaults for other
      configuration sections.
     </para>
<screen>
global:
 # the time after which an alert is declared resolved if it has not been updated
 [ resolve_timeout: <replaceable>DURATION</replaceable> | default = 5m ]

 # The default SMTP From header field.
 [ smtp_from: <replaceable>TMPL_STRING</replaceable> ]
 # The default SMTP smarthost used for sending emails, including port number.
 # Port number usually is 25, or 587 for SMTP over TLS
 # (sometimes referred to as STARTTLS).
 # Example: smtp.example.org:587
 [ smtp_smarthost: <replaceable>STRING</replaceable> ]
 # The default host name to identify to the SMTP server.
 [ smtp_hello: <replaceable>STRING</replaceable> | default = "localhost" ]
 [ smtp_auth_username: <replaceable>STRING</replaceable> ]
 # SMTP Auth using LOGIN and PLAIN.
 [ smtp_auth_password: <replaceable>SECRET</replaceable> ]
 # SMTP Auth using PLAIN.
 [ smtp_auth_identity: <replaceable>STRING</replaceable> ]
 # SMTP Auth using CRAM-MD5.
 [ smtp_auth_secret: <replaceable>SECRET</replaceable> ]
 # The default SMTP TLS requirement.
 [ smtp_require_tls: <replaceable>BOOL</replaceable> | default = true ]

 # The API URL to use for Slack notifications.
 [ slack_api_url: <replaceable>STRING</replaceable> ]
 [ victorops_api_key: <replaceable>STRING</replaceable> ]
 [ victorops_api_url: <replaceable>STRING</replaceable> | default = "https://victorops.example.com/integrations/alert/" ]
 [ pagerduty_url: <replaceable>STRING</replaceable> | default = "https://pagerduty.example.com/v2/enqueue" ]
 [ opsgenie_api_key: <replaceable>STRING</replaceable> ]
 [ opsgenie_api_url: <replaceable>STRING</replaceable> | default = "https://opsgenie.example.com/" ]
 [ hipchat_api_url: <replaceable>STRING</replaceable> | default = "https://hipchat.example.com/" ]
 [ hipchat_auth_token: <replaceable>SECRET</replaceable> ]
 [ wechat_api_url: <replaceable>STRING</replaceable> | default = "https://wechat.example.com/cgi-bin/" ]
 [ wechat_api_secret: <replaceable>SECRET</replaceable> ]
 [ wechat_api_corp_id: <replaceable>STRING</replaceable> ]

 # The default HTTP client configuration
 [ http_config: <replaceable>HTTP_CONFIG</replaceable> ]

# Files from which custom notification template definitions are read.
# The last component may use a wildcard matcher, e.g. 'templates/*.tmpl'.
templates:
 [ - <replaceable>FILEPATH</replaceable> ... ]

# The root node of the routing tree.
route: <replaceable>ROUTE</replaceable>

# A list of notification receivers.
receivers:
 - <replaceable>RECEIVER</replaceable> ...

# A list of inhibition rules.
inhibit_rules:
 [ - <replaceable>INHIBIT_RULE</replaceable> ... ]
</screen>
    </example>
    <example>
     <title><replaceable>ROUTE</replaceable></title>
     <para>
      A <replaceable>ROUTE</replaceable> block defines a node in a routing
      tree. Unspecified parameters are inherited from its parent node. Every
      alert enters the routing tree at the configured top-level route, which
      needs to match all alerts. It then traverses the child nodes. If the
      <option>continue</option> option is set to 'false', the traversing stops
      after the first matched child. Setting the option to 'true' on a matched
      node, the alert will continue matching against subsequent siblings. If an
      alert does not match any children of a node, the alert is handled based
      on the configuration parameters of the current node.
     </para>
<screen>
[ receiver: <replaceable>STRING</replaceable> ]
[ group_by: '[' <replaceable>LABELNAME</replaceable>, ... ']' ]

# If an alert should continue matching subsequent sibling nodes.
[ continue: <replaceable>BOOLEAN</replaceable> | default = false ]

# A set of equality matchers an alert has to fulfill to match a node.
match:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>LABELVALUE</replaceable>, ... ]

# A set of regex-matchers an alert has to fulfill to match a node.
match_re:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>REGEX</replaceable>, ... ]

# Time to wait before sending a notification for a group of alerts.
[ group_wait: <replaceable>DURATION</replaceable> | default = 30s ]

# Time to wait before sending a notification about new alerts
# added to a group of alerts for which an initial notification has
# already been sent.
[ group_interval: <replaceable>DURATION</replaceable> | default = 5m ]

# Time to wait before re-sending a notification
[ repeat_interval: <replaceable>DURATION</replaceable> | default = 4h ]

# Possible child routes.
routes:
	[ - <replaceable>ROUTE</replaceable> ... ]
</screen>
    </example>
    <example>
     <title><replaceable>INHIBIT_RULE</replaceable></title>
     <para>
      An inhibition rule mutes a target alert that matches a set of matchers
      when a source alert exists that matches another set of matchers. Both
      alerts need to share the same label values for the label names in the
      <option>equal</option> list.
     </para>
     <para>
      Alerts can match and therefore inhibit themselves. Do not write
      inhibition rules where an alert matches both source and target.
     </para>
<screen>
# Matchers that need to be fulfilled for the alerts to be muted.
target_match:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>LABELVALUE</replaceable>, ... ]
target_match_re:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>REGEX</replaceable>, ... ]

# Matchers for which at least one alert needs to exist so that the
# inhibition occurs.
source_match:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>LABELVALUE</replaceable>, ... ]
source_match_re:
	[ <replaceable>LABELNAME</replaceable>: <replaceable>REGEX</replaceable>, ... ]

# Labels with an equal value in the source and target
# alert for the inhibition to take effect.
[ equal: '[' <replaceable>LABELNAME</replaceable>, ... ']' ]
</screen>
    </example>
    <example>
     <title><replaceable>HTTP_CONFIG</replaceable></title>
     <para>
      <replaceable>HTTP_CONFIG</replaceable> configures the HTTP client used by
      the receiver to communicate with API services.
     </para>
     <para>
      Note that <option>basic_auth</option>, <option>bearer_token</option> and
      <option>bearer_token_file</option> options are mutually exclusive.
     </para>
<screen>
# Sets the 'Authorization' header with the user name and password.
basic_auth:
 [ username: <replaceable>STRING</replaceable> ]
 [ password: <replaceable>SECRET</replaceable> ]

# Sets the 'Authorization' header with the bearer token.
[ bearer_token: <replaceable>SECRET</replaceable> ]

# Sets the 'Authorization' header with the bearer token read from a file.
[ bearer_token_file: <replaceable>FILEPATH</replaceable> ]

# TLS settings.
tls_config:
 # CA certificate to validate the server certificate with.
 [ ca_file: <replaceable>FILEPATH</replaceable> ]
 # Certificate and key files for client cert authentication to the server.
 [ cert_file: <replaceable>FILEPATH</replaceable> ]
 [ key_file: <replaceable>FILEPATH</replaceable> ]
 # ServerName extension to indicate the name of the server.
 # http://tools.ietf.org/html/rfc4366#section-3.1
 [ server_name: <replaceable>STRING</replaceable> ]
 # Disable validation of the server certificate.
 [ insecure_skip_verify: <replaceable>BOOLEAN</replaceable> | default = false]

# Optional proxy URL.
[ proxy_url: <replaceable>STRING</replaceable> ]
</screen>
    </example>
    <example>
     <title><replaceable>RECEIVER</replaceable></title>
     <para>
      Receiver is a named configuration for one or more notification
      integrations.
     </para>
     <para>
      Instead of adding new receivers, we recommend implementing custom
      notification integrations using the webhook receiver (see
      <xref linkend="alert.webhook"/>).
     </para>
<screen>
# The unique name of the receiver.
name: <replaceable>STRING</replaceable>

# Configurations for several notification integrations.
email_configs:
 [ - <replaceable>EMAIL_CONFIG</replaceable>, ... ]
hipchat_configs:
 [ - <replaceable>HIPCHAT_CONFIG</replaceable>, ... ]
pagerduty_configs:
 [ - <replaceable>PAGERDUTY_CONFIG</replaceable>, ... ]
pushover_configs:
 [ - <replaceable>PUSHOVER_CONFIG</replaceable>, ... ]
slack_configs:
 [ - <replaceable>SLACK_CONFIG</replaceable>, ... ]
opsgenie_configs:
 [ - <replaceable>OPSGENIE_CONFIG</replaceable>, ... ]
webhook_configs:
 [ - <replaceable>WEBHOOK_CONFIG</replaceable>, ... ]
victorops_configs:
 [ - <replaceable>VICTOROPS_CONFIG</replaceable>, ... ]
wechat_configs:
 [ - <replaceable>WECHAT_CONFIG</replaceable>, ... ]
</screen>
    </example>
    <example>
     <title><replaceable>EMAIL_CONFIG</replaceable></title>
<screen>
# Whether to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = false ]

# The email address to send notifications to.
to: <replaceable>TMPL_STRING</replaceable>

# The sender address.
[ from: <replaceable>TMPL_STRING</replaceable> | default = global.smtp_from ]

# The SMTP host through which emails are sent.
[ smarthost: <replaceable>STRING</replaceable> | default = global.smtp_smarthost ]

# The host name to identify to the SMTP server.
[ hello: <replaceable>STRING</replaceable> | default = global.smtp_hello ]

# SMTP authentication details.
[ auth_username: <replaceable>STRING</replaceable> | default = global.smtp_auth_username ]
[ auth_password: <replaceable>SECRET</replaceable> | default = global.smtp_auth_password ]
[ auth_secret: <replaceable>SECRET</replaceable> | default = global.smtp_auth_secret ]
[ auth_identity: <replaceable>STRING</replaceable> | default = global.smtp_auth_identity ]

# The SMTP TLS requirement.
[ require_tls: <replaceable>BOOL</replaceable> | default = global.smtp_require_tls ]

# The HTML body of the email notification.
[ html: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "email.default.html" . }}' ]
# The text body of the email notification.
[ text: <replaceable>TMPL_STRING</replaceable> ]

# Further headers email header key/value pairs. Overrides any headers
# previously set by the notification implementation.
[ headers: { <replaceable>STRING</replaceable>: <replaceable>TMPL_STRING</replaceable>, ... } ]
</screen>
    </example>
    <example>
     <title><replaceable>HIPCHAT_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = false ]

# The HipChat Room ID.
room_id: <replaceable>TMPL_STRING</replaceable>
# The authentication token.
[ auth_token: <replaceable>SECRET</replaceable> | default = global.hipchat_auth_token ]
# The URL to send API requests to.
[ api_url: <replaceable>STRING</replaceable> | default = global.hipchat_api_url ]

# A label to be shown in addition to the sender's name.
[ from:  <replaceable>TMPL_STRING</replaceable> | default = '{{ template "hipchat.default.from" . }}' ]
# The message body.
[ message:  <replaceable>TMPL_STRING</replaceable> | default = '{{ template "hipchat.default.message" . }}' ]
# Whether this message will trigger a user notification.
[ notify:  <replaceable>BOOLEAN</replaceable> | default = false ]
# Determines how the message is treated by the alertmanager and rendered inside HipChat. Valid values are 'text' and 'html'.
[ message_format:  <replaceable>STRING</replaceable> | default = 'text' ]
# Background color for message.
[ color:  <replaceable>TMPL_STRING</replaceable> | default = '{{ if eq .Status "firing" }}red{{ else }}green{{ end }}' ]

# Configuration of the HTTP client.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example>
     <title><replaceable>PAGERDUTY_CONFIG</replaceable></title>
     <para>
      The <option>routing_key</option> and <option>service_key</option> options
      are mutually exclusive.
     </para>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = true ]

# The PagerDuty integration key (when using 'Events API v2').
routing_key: <replaceable>TMPL_SECRET</replaceable>
# The PagerDuty integration key (when using 'Prometheus').
service_key: <replaceable>TMPL_SECRET</replaceable>

# The URL to send API requests to.
[ url: <replaceable>STRING</replaceable> | default = global.pagerduty_url ]

# The client identification of the Alertmanager.
[ client:  <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pagerduty.default.client" . }}' ]
# A backlink to the notification sender.
[ client_url:  <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pagerduty.default.clientURL" . }}' ]

# The incident description.
[ description: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pagerduty.default.description" .}}' ]

# Severity of the incident.
[ severity: <replaceable>TMPL_STRING</replaceable> | default = 'error' ]

# A set of arbitrary key/value pairs that provide further details.
[ details: { <replaceable>STRING</replaceable>: <replaceable>TMPL_STRING</replaceable>, ... } | default = {
	firing:       '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
	resolved:     '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
	num_firing:   '{{ .Alerts.Firing | len }}'
	num_resolved: '{{ .Alerts.Resolved | len }}'
} ]

# The HTTP client's configuration.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example>
     <title><replaceable>PUSHOVER_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = true ]

# The recipient user key.
user_key: <replaceable>SECRET</replaceable>

# Registered application’s API token.
token: <replaceable>SECRET</replaceable>

# Notification title.
[ title: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pushover.default.title" . }}' ]

# Notification message.
[ message: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pushover.default.message" . }}' ]

# A supplementary URL displayed together with the message.
[ url: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "pushover.default.url" . }}' ]

# Priority.
[ priority: <replaceable>TMPL_STRING</replaceable> | default = '{{ if eq .Status "firing" }}2{{ else }}0{{ end }}' ]

# How often the Pushover servers will send the same notification (at least 30 seconds).
[ retry: <replaceable>DURATION</replaceable> | default = 1m ]

# How long your notification will continue to be retried (unless the user
# acknowledges the notification).
[ expire: <replaceable>DURATION</replaceable> | default = 1h ]

# Configuration of the HTTP client.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example>
     <title><replaceable>SLACK_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = false ]

# The Slack webhook URL.
[ api_url: <replaceable>SECRET</replaceable> | default = global.slack_api_url ]

# The channel or user to send notifications to.
channel: <replaceable>TMPL_STRING</replaceable>

# API request data as defined by the Slack webhook API.
[ icon_emoji: <replaceable>TMPL_STRING</replaceable> ]
[ icon_url: <replaceable>TMPL_STRING</replaceable> ]
[ link_names: <replaceable>BOOLEAN</replaceable> | default = false ]
[ username: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.username" . }}' ]
# The following parameters define the attachment.
actions:
 [ <replaceable>ACTION_CONFIG</replaceable> ... ]
[ color: <replaceable>TMPL_STRING</replaceable> | default = '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}' ]
[ fallback: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.fallback" . }}' ]
fields:
 [ <replaceable>FIELD_CONFIG</replaceable> ... ]
[ footer: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.footer" . }}' ]
[ pretext: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.pretext" . }}' ]
[ short_fields: <replaceable>BOOLEAN</replaceable> | default = false ]
[ text: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.text" . }}' ]
[ title: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.title" . }}' ]
[ title_link: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "slack.default.titlelink" . }}' ]
[ image_url: <replaceable>TMPL_STRING</replaceable> ]
[ thumb_url: <replaceable>TMPL_STRING</replaceable> ]

# Configuration of the HTTP client.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example>
     <title><replaceable>ACTION_CONFIG</replaceable> for <replaceable>SLACK_CONFIG</replaceable></title>
<screen>
# Provide a button to tell Slack you want to render a button.
type: <replaceable>TMPL_STRING</replaceable>
# Label for the button.
text: <replaceable>TMPL_STRING</replaceable>
# http or https URL to deliver users to. If you specify invalid URLs, the message will be posted with no button.
url: <replaceable>TMPL_STRING</replaceable>
#  If set to 'primary', the button will be green, indicating the best forward action to take
#  'danger' turns the button red, indicating a destructive action.
[ style: <replaceable>TMPL_STRING</replaceable> [ default = '' ]
</screen>
    </example>
    <example>
     <title><replaceable>FIELD_CONFIG</replaceable> for <replaceable>SLACK_CONFIG</replaceable></title>
<screen>
# A bold heading without markup above the <option>value</option> text.
title: <replaceable>TMPL_STRING</replaceable>
# The text of the field. It can span across several lines.
value: <replaceable>TMPL_STRING</replaceable>
# A flag indicating if <option>value</option> is short enough to be displayed together with other values.
[ short: <replaceable>BOOLEAN</replaceable> | default = slack_config.short_fields ]
</screen>
    </example>
    <example>
     <title><replaceable>OPSGENIE_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = true ]

# The API key to use with the OpsGenie API.
[ api_key: <replaceable>SECRET</replaceable> | default = global.opsgenie_api_key ]

# The host to send OpsGenie API requests to.
[ api_url: <replaceable>STRING</replaceable> | default = global.opsgenie_api_url ]

# Alert text (maximum is 130 characters).
[ message: <replaceable>TMPL_STRING</replaceable> ]

# A description of the incident.
[ description: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "opsgenie.default.description" . }}' ]

# A backlink to the sender.
[ source: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "opsgenie.default.source" . }}' ]

# A set of arbitrary key/value pairs that provide further detail.
[ details: { <replaceable>STRING</replaceable>: <replaceable>TMPL_STRING</replaceable>, ... } ]

# Comma separated list of team responsible for notifications.
[ teams: <replaceable>TMPL_STRING</replaceable> ]

# Comma separated list of tags attached to the notifications.
[ tags: <replaceable>TMPL_STRING</replaceable> ]

# Additional alert note.
[ note: <replaceable>TMPL_STRING</replaceable> ]

# Priority level of alert, one of P1, P2, P3, P4, and P5.
[ priority: <replaceable>TMPL_STRING</replaceable> ]

# Configuration of the HTTP.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example>
     <title><replaceable>VICTOROPS_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = true ]

# The API key for talking to the VictorOps API.
[ api_key: <replaceable>SECRET</replaceable> | default = global.victorops_api_key ]

# The VictorOps API URL.
[ api_url: <replaceable>STRING</replaceable> | default = global.victorops_api_url ]

# A key used to map the alert to a team.
routing_key: <replaceable>TMPL_STRING</replaceable>

# Describes the behavior of the alert (one of 'CRITICAL', 'WARNING', 'INFO').
[ message_type: <replaceable>TMPL_STRING</replaceable> | default = 'CRITICAL' ]

# Summary of the alerted problem.
[ entity_display_name: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "victorops.default.entity_display_name" . }}' ]

# Long explanation of the alerted problem.
[ state_message: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "victorops.default.state_message" . }}' ]

# The monitoring tool the state message is from.
[ monitoring_tool: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "victorops.default.monitoring_tool" . }}' ]

# Configuration of the HTTP client.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
    </example>
    <example xml:id="alert.webhook">
     <title><replaceable>WEBHOOK_CONFIG</replaceable></title>
     <para>
      You can utilize the webhook receiver to configure a generic receiver.
     </para>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = true ]

# The endpoint for sending HTTP POST requests.
url: <replaceable>STRING</replaceable>

# Configuration of the HTTP client.
[ http_config: <replaceable>HTTP_CONFIG</replaceable> | default = global.http_config ]
</screen>
     <para>
      Alertmanager sends HTTP POST requests in the following JSON format:
     </para>
<screen>
{
  "version": "4",
  "groupKey": <replaceable>STRING</replaceable>, // identifycation of the group of alerts (to deduplicate)
  "status": "&lt;resolved|firing>",
  "receiver": <replaceable>STRING</replaceable>,
  "groupLabels": <replaceable>OBJECT</replaceable>,
  "commonLabels": <replaceable>OBJECT</replaceable>,
  "commonAnnotations": <replaceable>OBJECT</replaceable>,
  "externalURL": <replaceable>STRING</replaceable>, // backlink to Alertmanager.
  "alerts": [
    {
      "status": "&lt;resolved|firing>",
      "labels": <replaceable>OBJECT</replaceable>,
      "annotations": <replaceable>OBJECT</replaceable>,
      "startsAt": "&lt;rfc3339>",
      "endsAt": "&lt;rfc3339>",
      "generatorURL": <replaceable>STRING</replaceable> // identifies the entity that caused the alert
    },
    ...
  ]
}
</screen>
     <para>
      The webhook receiver allows for integration with the following
      notification mechanisms:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        DingTalk (https://github.com/timonwong/prometheus-webhook-dingtalk)
       </para>
      </listitem>
      <listitem>
       <para>
        IRC Bot (https://github.com/multimfi/bot)
       </para>
      </listitem>
      <listitem>
       <para>
        JIRAlert (https://github.com/free/jiralert)
       </para>
      </listitem>
      <listitem>
       <para>
        Phabricator / Maniphest (https://github.com/knyar/phalerts)
       </para>
      </listitem>
      <listitem>
       <para>
        prom2teams: forwards notifications to Microsoft Teams
        (https://github.com/idealista/prom2teams)
       </para>
      </listitem>
      <listitem>
       <para>
        SMS: supports multiple providers
        (https://github.com/messagebird/sachet)
       </para>
      </listitem>
      <listitem>
       <para>
        Telegram bot (https://github.com/inCaller/prometheus_bot)
       </para>
      </listitem>
     </itemizedlist>
    </example>
    <example>
     <title><replaceable>WECHAT_CONFIG</replaceable></title>
<screen>
# Whether or not to notify about resolved alerts.
[ send_resolved: <replaceable>BOOLEAN</replaceable> | default = false ]

# The API key to use for the WeChat API.
[ api_secret: <replaceable>SECRET</replaceable> | default = global.wechat_api_secret ]

# The WeChat API URL.
[ api_url: <replaceable>STRING</replaceable> | default = global.wechat_api_url ]

# The corp id used to authenticate.
[ corp_id: <replaceable>STRING</replaceable> | default = global.wechat_api_corp_id ]

# API request data as defined by the WeChat API.
[ message: <replaceable>TMPL_STRING</replaceable> | default = '{{ template "wechat.default.message" . }}' ]
[ agent_id: <replaceable>STRING</replaceable> | default = '{{ template "wechat.default.agent_id" . }}' ]
[ to_user: <replaceable>STRING</replaceable> | default = '{{ template "wechat.default.to_user" . }}' ]
[ to_party: <replaceable>STRING</replaceable> | default = '{{ template "wechat.default.to_party" . }}' ]
[ to_tag: <replaceable>STRING</replaceable> | default = '{{ template "wechat.default.to_tag" . }}' ]
</screen>
    </example>
   </sect3>
   <sect3>
    <title>Custom Alerts</title>
    <para>
     You can define your custom alert conditions to send notifications to an
     external service. Prometheus uses its own expression language for defining
     custom alerts. Following is an example of a rule with an alert:
    </para>
<screen>
groups:
- name: example
  rules:
				# alert on high deviation from average PG count
				- alert: high pg count deviation
						expr: abs(((ceph_osd_pgs > 0) - on (job) group_left avg(ceph_osd_pgs > 0) by (job)) / on (job) group_left avg(ceph_osd_pgs > 0) by (job)) > 0.35
						for: 5m
						labels:
								severity: warning
								type: ses_default
						annotations:
								description: >
												OSD {{ $labels.osd }} deviates by more then 30% from average PG count
</screen>
    <para>
     The optional <literal>for</literal> clause specifies the time Prometheus
     will wait between first encountering a new expression output vector
     element and counting an alert as firing. In this case, Prometheus will
     check that the alert continues to be active for 5 minutes before firing
     the alert. Elements in a pending state are active, but not firing yet.
    </para>
    <para>
     The <literal>labels</literal> clause specifies a set of additional labels
     attached to the alert. Conflicting labels will be overwritten. Labels can
     be templated (see <xref linkend="alertmanager.templates"/> for more
     details on templating).
    </para>
    <para>
     The <literal>annotations</literal> clause specifies informational labels.
     You can use them to store additional information, for example alert
     descriptions or runbook links. Annotations can be templated (see
     <xref linkend="alertmanager.templates"/> for more details on templating).
    </para>
    <para>
     To add your custom alerts to &productname;, either
    </para>
    <itemizedlist>
     <listitem>
      <para>
       place your YAML files with custom alerts in the
       <filename>/etc/prometheus/alerts</filename> directory
      </para>
     </listitem>
    </itemizedlist>
    <para>
     or
    </para>
    <itemizedlist>
     <listitem>
      <para>
       provide a list of paths to your custom alert files in the Pillar under
       the <option>monitoring:custom_alerts</option> key. &deepsea; Stage 2 or
       the <command>salt <replaceable>SALT_MASTER</replaceable> state.apply
       ceph.monitoring.prometheus</command> command will add your alert files
       in the right place.
      </para>
      <example>
       <title>Adding Custom Alerts to &storage;</title>
       <para>
        A file with custom alerts is in
        <filename>/root/my_alerts/my_alerts.yml</filename> on the Salt master.
        If you add
       </para>
<screen>
monitoring:
  custom_alerts:
    - /root/my_alerts/my_alerts.yml
</screen>
       <para>
        to the
        <filename>/srv/pillar/ceph/cluster/<replaceable>YOUR_SALT_MASTER_MINION_ID</replaceable>.sls</filename>
        file, &deepsea; will create the
        <filename>/etc/prometheus/alerts/my_alerts.yml</filename> file and
        restart Prometheus.
       </para>
      </example>
     </listitem>
    </itemizedlist>
    <sect4 xml:id="alertmanager.templates">
     <title>Templates</title>
     <para>
      You can use templates for label and annotation values. The
      <varname>$labels</varname> variable includes the label key/value pairs of
      an alert instance, while <varname>$value</varname> holds the evaluated
      value of an alert instance.
     </para>
     <para>
      The following example inserts a firing element label and value:
     </para>
<screen>
{{ $labels.<replaceable>LABELNAME</replaceable> }}
{{ $value }}
</screen>
    </sect4>
    <sect4>
     <title>Inspecting Alerts at Runtime</title>
     <para>
      If you need to verify which alerts are active, you have several options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Navigate to the <guimenu>Alerts</guimenu> tab of Prometheus. It will
        show you the exact label sets for which defined alerts are active.
        Prometheus also stores synthetic time series for pending and firing
        alerts. They have the following form:
       </para>
<screen>
ALERTS{alertname="<replaceable>ALERT_NAME</replaceable>", alertstate="pending|firing", <replaceable>ADDITIONAL_ALERT_LABELS</replaceable>}
</screen>
       <para>
        The sample value is 1 if the alert is active (pending or firing). The
        series is marked 'stale' when the alert is inactive.
       </para>
      </listitem>
      <listitem>
       <para>
        In the Prometheus Web interface at the URL address
        http://<replaceable>PROMETHEUS_HOST_IP</replaceable>:9090/alerts,
        inspect alerts and their state (INACTIVE, PENDING or FIRING).
       </para>
      </listitem>
      <listitem>
       <para>
        In the Alertmanager Web interface at the URL address
        http://:<replaceable>PROMETHEUS_HOST_IP</replaceable>9093/#/alerts,
        inspect alerts and silence them if desired.
       </para>
      </listitem>
     </itemizedlist>
    </sect4>
   </sect3>
  </sect2>
 </sect1>
</chapter>
