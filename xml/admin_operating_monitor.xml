<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-monitor">
 <title>Determining Cluster State</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  When you have a running cluster, you may use the <command>ceph</command> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of &osd;s, &mon;s, placement groups, and &mds;s.
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>Checking a Cluster's Status</title>

  <para>
   To check a cluster's status, execute the following:
  </para>

<screen>&prompt.cephuser;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </para>

<screen>&prompt.cephuser;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.cephuser;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   The &ceph; cluster returns one of the following health codes:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </para>
     <para>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<filename>/var/log/ceph/ceph-osd.*</filename>) may contain debugging
      information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, for example OSD_HOST_DOWN</term>
    <listitem>
     <para>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </para>
<screen>&prompt.cephuser;ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      The usage thresholds for <emphasis>backfillfull</emphasis> (defaults to
      0.90), <emphasis>nearfull</emphasis> (defaults to 0.85),
      <emphasis>full</emphasis> (defaults to 0.95), and/or
      <emphasis>failsafe_full</emphasis> are not ascending. In particular, we
      expect <emphasis>backfillfull</emphasis> &lt;
      <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt;
      <emphasis>full</emphasis>, and <emphasis>full</emphasis> &lt;
      <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      To read the current values, run:
     </para>
<screen>
&prompt.cephuser;ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      The thresholds can be adjusted with the following commands:
     </para>
<screen>&prompt.cephuser;ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>full</emphasis> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
     <para>
      The currently defined <emphasis>full</emphasis> ratio can be seen with:
     </para>
<screen>&prompt.cephuser;ceph osd dump | grep full_ratio</screen>
     <para>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </para>
<screen>&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>backfillfull</emphasis>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>nearfull</emphasis>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      One or more cluster flags of interest has been set. With the exception of
      <emphasis>full</emphasis>, these flags can be set or cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd set <replaceable>flag</replaceable>
&prompt.cephuser;ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         The cluster is flagged as full and cannot service writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Paused reads or writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs are not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <emphasis>down</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs that were previously marked <emphasis>out</emphasis> will not be
         marked back <emphasis>in</emphasis> when they start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs will not automatically be marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Recovery or data rebalancing is suspended.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing (see <xref linkend="scrubbing"/>) is disabled.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Cache tiering activity is suspended.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSD is not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Failure reports for this OSD will be ignored.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         If this OSD was previously marked <emphasis>out</emphasis>
         automatically after a failure, it will not be marked
         <emphasis>in</emphasis> when it starts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         If this OSD is down, it will not be automatically marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Per-OSD flags can be set and cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
&prompt.cephuser;ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      The &crushmap; is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <option>mon_crush_min_required_version</option>
      configuration option.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      The &crushmap; is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The &crushmap; should be
      updated to use the newer method (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
     <para>
      For more information on cache tiering, see
      <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      No pre-Luminous v12 OSDs are running but the <option>sortbitwise</option>
      flag has not been set. You need to set the <option>sortbitwise</option>
      flag before Luminous v12 or newer OSDs can start:
     </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </para>
<screen>&prompt.cephuser;ceph df detail</screen>
     <para>
      You can either raise the pool quota with
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      or delete some existing data to reduce usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow I/O
      requests to be serviced. Problematic PG states include
      <emphasis>peering</emphasis>, <emphasis>stale</emphasis>,
      <emphasis>incomplete</emphasis>, and the lack of
      <emphasis>active</emphasis> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <emphasis>degraded</emphasis> or
      <emphasis>undersized</emphasis> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <emphasis>clean</emphasis> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <emphasis>backfill_toofull</emphasis> or
      <emphasis>recovery_toofull</emphasis> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <emphasis>backfillfull</emphasis> threshold.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Data scrubbing (see <xref linkend="scrubbing"/>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <emphasis>inconsistent</emphasis> or
      <emphasis>snaptrim_error</emphasis> flag is set, indicating an earlier
      scrub operation found a problem, or that the <emphasis>repair</emphasis>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Recent OSD scrubs have uncovered inconsistencies.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      A cache tier pool is nearly full. Full in this context is determined by
      the <emphasis>target_max_bytes</emphasis> and
      <emphasis>target_max_objects</emphasis> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </para>
     <para>
      Find more information about cache tiering in
      <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is below the configurable threshold of
      <option>mon_pg_warn_min_per_osd</option> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is above the configurable threshold of
      <option>mon_pg_warn_max_per_osd</option> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the &mgr;s and &mon;s.
     </para>
     <para>
      While the <option>pg_num</option> value for existing pools cannot be
      reduced. The <option>pgp_num</option> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <option>pgp_num</option> value can
      be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      One or more pools has a <option>pgp_num</option> value less than
      <option>pg_num</option>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <option>pgp_num</option> to match
      <option>pg_num</option>, triggering the data migration, with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <option>mon_pg_warn_max_object_skew</option> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <option>mon_pg_warn_max_object_skew</option> configuration option on the
      monitors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED¶</term>
    <listitem>
     <para>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </para>
<screen>&prompt.cephuser;rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </para>
<screen>&prompt.cephuser;ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <option>mon_pool_quota_crit_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <option>mon_pool_quota_warn_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      One or more objects in the cluster are not stored on the node where the
      cluster wants them to be. This is an indication that data migration
      caused by a recent cluster change has not yet completed. Misplaced data
      is not a dangerous condition in itself. Data consistency is never at
      risk, and old copies of objects are never removed until the desired
      number of new copies (in the desired locations) are present.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on the OSDs that
      are currently up. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, the down OSD that has the most recent copy of the
      unfound object can be brought back up. Candidate OSDs can be identified
      from the peering state for the PG(s) responsible for the unfound object:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      You can see a summary of the slowest recent requests:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      You can find the location of an OSD with:
     </para>
<screen>&prompt.cephuser;ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      One or more OSD requests have been blocked for a relatively long time,
      for example 4096 seconds. This is an indication that either the cluster
      has been unhealthy for an extended period of time (for example, not
      enough running OSDs or inactive PGs) or there is some internal problem
      with the OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs have not been scrubbed (see <xref linkend="scrubbing"/>)
      recently. PGs are normally scrubbed every
      <option>mon_scrub_interval</option> seconds, and this warning triggers
      when <option>mon_warn_not_scrubbed</option> such intervals have elapsed
      without a scrub. PGs will not scrub if they are not flagged as clean,
      which may happen if they are misplaced or degraded (see PG_AVAILABILITY
      and PG_DEGRADED above). You can manually initiate a scrub of a clean PG
      with:
     </para>
<screen>&prompt.cephuser;ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs has not been deep scrubbed (see
      <xref linkend="scrubbing"/>) recently. PGs are normally scrubbed every
      <option>osd_deep_mon_scrub_interval</option> seconds, and this warning
      triggers when <option>mon_warn_not_deep_scrubbed</option> seconds have
      elapsed without a scrub. PGs will not (deep) scrub if they are not
      flagged as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </para>
<screen>&prompt.cephuser;ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.root;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-watch">
  <title>Watching a Cluster</title>

  <para>
   You can find the immediate state of the cluster using <command>ceph
   -s</command>. For example, a tiny &ceph; cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </para>

<screen>
&prompt.cephuser;ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 > max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean
</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &mgr;s
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &ogw;s
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>

  <para>
   Other commands that display immediate status information are:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To get the information updated in real time, put any of these commands
   (including <command>ceph -s</command>) as an argument of the
   <command>watch</command> command:
  </para>

<screen>&prompt.root;watch -n 10 'ceph -s'</screen>

  <para>
   Press <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>
   when you are tired of watching.
  </para>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Checking a Cluster's Usage Stats</title>

  <para>
   To check a cluster’s data usage and distribution among pools, use the
   <command>ceph df</command> command. To get more details, use <command>ceph
   df detail</command>.
  </para>

<screen>
&prompt.cephuser;ceph df
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
    TOTAL     40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
POOLS:
    POOL             ID     STORED     OBJECTS    USED       %USED    MAX AVAIL
    iscsi-images      1     3.9 KiB          8    769 KiB        0       10 GiB
    cephfs_data       2     1.6 KiB          5    960 KiB        0       10 GiB
    cephfs_metadata   3      54 KiB         22    1.5 MiB        0       10 GiB
[...]
</screen>

  <para>
   The <literal>RAW STORAGE</literal> section of the output provides an
   overview of the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>CLASS</literal>: The storage class of the device. Refer to
     <xref linkend="crush-devclasses"/> for more details on device classes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The space (accumulated over all OSDs) allocated
     purely for data objects kept at block device.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The sum of 'USED' space and space
     allocated/reserved at block device for &ceph; purposes, for example BlueFS
     part for &bluestore;.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See <xref linkend="storage-capacity"/> for
     additional details.
    </para>
    <note>
     <title>Cluster Fill Level</title>
     <para>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </para>
     <para>
      Use the command <command>ceph osd df tree</command> to list the fill
      level of all OSDs.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>POOL</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>STORED</literal>: The amount of data stored by the user.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The amount of space allocated purely for data by
     all OSD nodes in kB.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: The maximum available space in the given
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    <literal>USED</literal>and %<literal>USED</literal> amounts will not add up
    to the <literal>RAW USED</literal> and <literal>%RAW USED</literal> amounts
    in the <literal>RAW STORAGE</literal> section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.cephuser;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.cephuser;ceph osd tree</screen>

  <para>
   &ceph; will print a CRUSH tree with a host, its OSDs, whether they are up
   and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>&prompt.cephuser;ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Checking Monitor Status</title>

  <para>
   After you start the cluster and before first reading and/or writing data,
   check the &mon;s' quorum status. When the cluster is already serving
   requests, check the &mon;s' status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.cephuser;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.cephuser;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <xref linkend="op-mon-osd-pg"/>.
  </para>
 </sect1>
 <sect1 xml:id="monitor-adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to <xref linkend="ceph-config-runtime"/> for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
 <sect1 xml:id="storage-capacity">
  <title>Storage Capacity</title>

  <para>
   When a &ceph; storage cluster gets close to its maximum capacity, &ceph;
   prevents you from writing to or reading from &osd;s as a safety measure to
   prevent data loss. Therefore, letting a production cluster approach its full
   ratio is not a good practice, because it sacrifices high availability. The
   default full ratio is set to .95, meaning 95% of capacity. This a very
   aggressive setting for a test cluster with a small number of OSDs.
  </para>

  <tip>
   <title>Increase Storage Capacity</title>
   <para>
    When monitoring your cluster, be alert to warnings related to the
    <literal>nearfull</literal> ratio. It means that a failure of some OSDs
    could result in a temporary service disruption if one or more OSDs fails.
    Consider adding more OSDs to increase storage capacity.
   </para>
  </tip>

  <para>
   A common scenario for test clusters involves a system administrator removing
   a &osd; from the &ceph; storage cluster to watch the cluster rebalance. Then
   removing another &osd;, and so on until the cluster eventually reaches the
   full ratio and locks up. We recommend a bit of capacity planning even with a
   test cluster. Planning enables you to estimate how much spare capacity you
   will need in order to maintain high availability. Ideally, you want to plan
   for a series of &osd; failures where the cluster can recover to an
   <literal>active + clean</literal> state without replacing those &osd;s
   immediately. You can run a cluster in an <literal>active +
   degraded</literal> state, but this is not ideal for normal operating
   conditions.
  </para>

  <para>
   The following diagram depicts a simplistic &ceph; storage cluster containing
   33 &ceph; nodes with one &osd; per host, each of them reading from and
   writing to a 3 TB drive. This exemplary cluster has a maximum actual
   capacity of 99 TB. The <option>mon osd full ratio</option> option is set to
   0.95. If the cluster falls to 5 TB of the remaining capacity, it will not
   allow the clients to read and write data. Therefore the storage cluster’s
   operating capacity is 95 TB, not 99 TB.
  </para>

  <figure>
   <title>&ceph; Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   It is normal in such a cluster for one or two OSDs to fail. A less frequent
   but reasonable scenario involves a rack’s router or power supply failing,
   which brings down multiple OSDs simultaneously (for example, OSDs 7-12). In
   such a scenario, you should still strive for a cluster that can remain
   operational and achieve an <literal>active + clean</literal>
   state&mdash;even if that means adding a few hosts with additional OSDs in
   short order. If your capacity usage is too high, you may not lose data. But
   you could still sacrifice data availability while resolving an outage within
   a failure domain if capacity usage of the cluster exceeds the full ratio.
   For this reason, we recommend at least some rough capacity planning.
  </para>

  <para>
   Identify two numbers for your cluster:
  </para>

  <orderedlist>
   <listitem>
    <para>
     The number of OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     The total capacity of the cluster.
    </para>
   </listitem>
  </orderedlist>

  <para>
   If you divide the total capacity of your cluster by the number of OSDs in
   your cluster, you will find the mean average capacity of an OSD within your
   cluster. Consider multiplying that number by the number of OSDs you expect
   will fail simultaneously during normal operations (a relatively small
   number). Finally, multiply the capacity of the cluster by the full ratio to
   arrive at a maximum operating capacity. Then, subtract the number of the
   amount of data from the OSDs you expect to fail to arrive at a reasonable
   full ratio. Repeat the foregoing process with a higher number of OSD
   failures (a rack of OSDs) to arrive at a reasonable number for a near full
   ratio.
  </para>

  <para>
   The following settings only apply on cluster creation and are then stored in
   the OSD map:
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    These settings only apply during cluster creation. Afterward they need to
    be changed in the OSD Map using the <command>ceph osd
    set-nearfull-ratio</command> and <command>ceph osd set-full-ratio</command>
    commands.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered
      <literal>full</literal>. Default is .95
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered too
      <literal>full</literal> to backfill. Default is .90
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      The percentage of disk space used before an OSD is considered
      <literal>nearfull</literal>. Default is .85
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Check OSD Weight</title>
   <para>
    If some OSDs are <literal>nearfull</literal>, but others have plenty of
    capacity, you may have a problem with the CRUSH weight for the
    <literal>nearfull</literal> OSDs.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op-mon-osd-pg">
  <title>Monitoring OSDs and Placement Groups</title>

  <para>
   High availability and high reliability require a fault-tolerant approach to
   managing hardware and software issues. &ceph; has no single
   point-of-failure, and can service requests for data in a 'degraded' mode.
   &ceph;’s data placement introduces a layer of indirection to ensure that
   data does not bind directly to particular OSD addresses. This means that
   tracking down system faults requires finding the placement group and the
   underlying OSDs at root of the problem.
  </para>

  <tip>
   <title>Access in Case of Failure</title>
   <para>
    A fault in one part of the cluster may prevent you from accessing a
    particular object. That does not mean that you cannot access other objects.
    When you run into a fault, follow the steps for monitoring your OSDs and
    placement groups. Then begin troubleshooting.
   </para>
  </tip>

  <para>
   &ceph; is generally self-repairing. However, when problems persist,
   monitoring OSDs and placement groups will help you identify the problem.
  </para>

  <sect2 xml:id="op-mon-osds">
   <title>Monitoring OSDs</title>
   <para>
    An OSD’s status is either <emphasis>in the cluster</emphasis> ('in') or
    <emphasis>out of the cluster</emphasis> ('out'). At the same time, it is
    either <emphasis>up and running</emphasis> ('up') or it is <emphasis>down
    and not running</emphasis> ('down'). If an OSD is 'up', it may be either in
    the cluster (you can read and write data) or out of the cluster. If it was
    in the cluster and recently moved out of the cluster, &ceph; will migrate
    placement groups to other OSDs. If an OSD is out of the cluster, CRUSH will
    not assign placement groups to it. If an OSD is 'down', it should also be
    'out'.
   </para>
   <note>
    <title>Unhealthy State</title>
    <para>
     If an OSD is 'down' and 'in', there is a problem and the cluster will not
     be in a healthy state.
    </para>
   </note>
   <para>
    If you execute a command such as <command>ceph health</command>,
    <command>ceph -s</command> or <command>ceph -w</command>, you may notice
    that the cluster does not always echo back <literal>HEALTH OK</literal>.
    With regard to OSDs, you should expect that the cluster will
    <emphasis>not</emphasis> echo <literal>HEALTH OK</literal> under the
    following circumstances:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You have not started the cluster yet (it will not respond).
     </para>
    </listitem>
    <listitem>
     <para>
      You have just started or restarted the cluster and it is not ready yet,
      because the placement groups are being created and the OSDs are in the
      process of peering.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just added or removed an OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just modified your cluster map.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    An important aspect of monitoring OSDs is to ensure that when the cluster
    is up and running, all the OSDs in the cluster are up and running, too. To
    see if all the OSDs are running, execute:
   </para>
<screen>
&prompt.root;ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    The result should tell you the total number of OSDs (x), how many are 'up'
    (y), how many are 'in' (z), and the map epoch (eNNNN). If the number of
    OSDs that are 'in' the cluster is more than the number of OSDs that are
    'up', execute the following command to identify the
    <literal>ceph-osd</literal> daemons that are not running:
   </para>
<screen>
&prompt.root;ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    If an OSD with, for example, ID 1 is down, start it:
   </para>
<screen>
&prompt.cephuser.osd;sudo systemctl start ceph-osd@1.service
</screen>
   <para>
    See <xref linkend="op-osd-not-running"/> for problems associated with OSDs
    that have stopped or that will not restart.
   </para>
  </sect2>

  <sect2 xml:id="op-pgsets">
   <title>Placement Group Sets</title>
   <para>
    When CRUSH assigns placement groups to OSDs, it looks at the number of
    replicas for the pool and assigns the placement group to OSDs such that
    each replica of the placement group gets assigned to a different OSD. For
    example, if the pool requires three replicas of a placement group, CRUSH
    may assign them to <literal>osd.1</literal>, <literal>osd.2</literal> and
    <literal>osd.3</literal> respectively. CRUSH actually seeks a pseudo-random
    placement that will take into account failure domains you set in your
    &crushmap;, so you will rarely see placement groups assigned to nearest
    neighbor OSDs in a large cluster. We refer to the set of OSDs that should
    contain the replicas of a particular placement group as the &actingset;. In
    some cases, an OSD in the acting set is down or otherwise not able to
    service requests for objects in the placement group. When these situations
    arise, it may match one of the following scenarios:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You added or removed an OSD. Then, CRUSH reassigned the placement group
      to other OSDs and therefore changed the composition of the &actingset;,
      causing the migration of data with a 'backfill' process.
     </para>
    </listitem>
    <listitem>
     <para>
      An OSD was 'down', was restarted, and is now recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      An OSD in the &actingset; is 'down' or unable to service requests, and
      another OSD has temporarily assumed its duties.
     </para>
     <para>
      &ceph; processes a client request using the &upset;, which is the set of
      OSDs that will actually handle the requests. In most cases, the &upset;
      and the &actingset; are virtually identical. When they are not, it may
      indicate that &ceph; is migrating data, an OSD is recovering, or that
      there is a problem (for example, &ceph; usually echoes a <literal>HEALTH
      WARN</literal> state with a 'stuck stale' message in such scenarios).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To retrieve a list of placement groups, run:
   </para>
<screen>
&prompt.cephuser;;ceph pg dump
</screen>
   <para>
    To view which OSDs are within the &actingset; or the &upset; for a given
    placement group, run:
   </para>
<screen>
&prompt.cephuser;ceph pg map<replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -> up [0,1,2] acting [0,1,2]
</screen>
   <para>
    The result should tell you the osdmap epoch (eNNN), the placement group
    number (<replaceable>PG_NUM</replaceable>), the OSDs in the &upset; ('up'),
    and the OSDs in the &actingset; ('acting'):
   </para>
   <tip>
    <title>Cluster Problem Indicator</title>
    <para>
     If the &upset; and &actingset; do not match, this may be an indicator
     either of the cluster rebalancing itself, or of a potential problem with
     the cluster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-peering">
   <title>Peering</title>
   <para>
    Before you can write data to a placement group, it must be in an 'active'
    state, and it should be in a 'clean' state. For &ceph; to determine the
    current state of a placement group, the primary OSD of the placement group
    (the first OSD in the &actingset;), peers with the secondary and tertiary
    OSDs to establish agreement on the current state of the placement group
    (assuming a pool with three replicas of the PG).
   </para>
   <figure>
    <title>Peering Schema</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op-mon-pg-states">
   <title>Monitoring Placement Group States</title>
   <para>
    If you execute a command such as <command>ceph health</command>,
    <command>ceph -s</command> or <command>ceph -w</command>, you may notice
    that the cluster does not always echo back the <literal>HEALTH OK</literal>
    message. After you check to see if the OSDs are running, you should also
    check placement group states.
   </para>
   <para>
    Expect that the cluster will <emphasis role="bold">not</emphasis> echo
    <literal>HEALTH OK</literal> in a number of placement group peering-related
    circumstances:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You have just created a pool and placement groups have not peered yet.
     </para>
    </listitem>
    <listitem>
     <para>
      The placement groups are recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just added an OSD to or removed an OSD from the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      You have just modified your &crushmap; and your placement groups are
      migrating.
     </para>
    </listitem>
    <listitem>
     <para>
      There is inconsistent data in different replicas of a placement group.
     </para>
    </listitem>
    <listitem>
     <para>
      &ceph; is scrubbing a placement group’s replicas.
     </para>
    </listitem>
    <listitem>
     <para>
      &ceph; does not have enough storage capacity to complete backfilling
      operations.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If one of the above mentioned circumstances causes &ceph; to echo
    <literal>HEALTH WARN</literal>, do not panic. In many cases, the cluster
    will recover on its own. In some cases, you may need to take action. An
    important aspect of monitoring placement groups is to ensure that when the
    cluster is up and running, all placement groups are 'active' and preferably
    in the 'clean state'. To see the status of all placement groups, run:
   </para>
<screen>
&prompt.cephuser;ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    The result should tell you the total number of placement groups (x), how
    many placement groups are in a particular state such as 'active+clean' (y)
    and the amount of data stored (z).
   </para>
   <para>
    In addition to the placement group states, &ceph; will also echo back the
    amount of storage capacity used (aa), the amount of storage capacity
    remaining (bb), and the total storage capacity for the placement group.
    These numbers can be important in a few cases:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      You are reaching your <option>near full ratio</option> or <option>full
      ratio</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Your data is not getting distributed across the cluster because of an
      error in your CRUSH configuration.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Placement Group IDs</title>
    <para>
     Placement group IDs consist of the pool number (not pool name) followed by
     a period (.) and the placement group ID&mdash;a hexadecimal number. You
     can view pool numbers and their names from the output of <command>ceph osd
     lspools</command>. For example, the default pool <literal>rbd</literal>
     corresponds to pool number 0. A fully qualified placement group ID has the
     following form:
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     And it typically looks like this:
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    To retrieve a list of placement groups, run the following:
   </para>
<screen>
&prompt.cephuser;ceph pg dump
</screen>
   <para>
    You can also format the output in JSON format and save it to a file:
   </para>
<screen>
&prompt.cephuser;ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    To query a particular placement group, run the following:
   </para>
<screen>
&prompt.cephuser;ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    The following list describes the common placement group states in detail.
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING</term>
     <listitem>
      <para>
       When you create a pool, it will create the number of placement groups
       you specified. &ceph; will echo 'creating' when it is creating one or
       more placement groups. When they are created, the OSDs that are part of
       the placement group’s &actingset; will peer. When peering is complete,
       the placement group status should be 'active+clean', which means that a
       &ceph; client can begin writing to the placement group.
      </para>
      <figure>
       <title>Placement Groups Status</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING</term>
     <listitem>
      <para>
       When &ceph; is peering a placement group, it is bringing the OSDs that
       store the replicas of the placement group into agreement about the state
       of the objects and metadata in the placement group. When &ceph;
       completes peering, this means that the OSDs that store the placement
       group agree about the current state of the placement group. However,
       completion of the peering process does
       <emphasis role="bold">not</emphasis> mean that each replica has the
       latest contents.
      </para>
      <note>
       <title>Authoritative History</title>
       <para>
        &ceph; will <emphasis role="bold">not</emphasis> acknowledge a write
        operation to a client until all OSDs of the &actingset; persist the
        write operation. This practice ensures that at least one member of the
        &actingset; will have a record of every acknowledged write operation
        since the last successful peering operation.
       </para>
       <para>
        With an accurate record of each acknowledged write operation, &ceph;
        can construct and enlarge a new authoritative history of the placement
        group&mdash;a complete and fully ordered set of operations that, if
        performed, would bring an OSD’s copy of a placement group up to date.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ACTIVE</term>
     <listitem>
      <para>
       When &ceph; completes the peering process, a placement group may become
       'active'. The 'active' state means that the data in the placement group
       is generally available in the primary placement group and the replicas
       for read and write operations.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN</term>
     <listitem>
      <para>
       When a placement group is in the 'clean' state, the primary OSD and the
       replica OSDs have successfully peered and there are no stray replicas
       for the placement group. &ceph; replicated all objects in the placement
       group the correct number of times.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       When a client writes an object to the primary OSD, the primary OSD is
       responsible for writing the replicas to the replica OSDs. After the
       primary OSD writes the object to storage, the placement group will
       remain in a 'degraded' state until the primary OSD has received an
       acknowledgement from the replica OSDs that &ceph; created the replica
       objects successfully.
      </para>
      <para>
       The reason a placement group can be 'active+degraded' is that an OSD may
       be 'active' even though it does not hold all of the objects yet. If an
       OSD goes down, &ceph; marks each placement group assigned to the OSD as
       'degraded'. The OSDs must peer again when the OSD comes back up.
       However, a client can still write a new object to a degraded placement
       group if it is 'active'.
      </para>
      <para>
       If an OSD is 'down' and the 'degraded' condition persists, &ceph; may
       mark the down OSD as 'out' of the cluster and remap the data from the
       'down' OSD to another OSD. The time between being marked 'down' and
       being marked 'out' is controlled by the <option>mon osd down out
       interval</option> option, which is set to 600 seconds by default.
      </para>
      <para>
       A placement group can also be 'degraded' because &ceph; cannot find one
       or more objects that should be in the placement group. While you cannot
       read or write to unfound objects, you can still access all of the other
       objects in the 'degraded' placement group.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING</term>
     <listitem>
      <para>
       &ceph; was designed for fault-tolerance at a scale where hardware and
       software problems are ongoing. When an OSD goes 'down', its contents may
       fall behind the current state of other replicas in the placement groups.
       When the OSD is back 'up', the contents of the placement groups must be
       updated to reflect the current state. During that time period, the OSD
       may reflect a 'recovering' state.
      </para>
      <para>
       Recovery is not always trivial, because a hardware failure may cause a
       cascading failure of multiple OSDs. For example, a network switch for a
       rack or cabinet may fail, which can cause the OSDs of a number of host
       machines to fall behind the current state of the cluster. Each of the
       OSDs must recover when the fault is resolved.
      </para>
      <para>
       &ceph; provides a number of settings to balance the resource contention
       between new service requests and the need to recover data objects and
       restore the placement groups to the current state. The <option>osd
       recovery delay start</option> setting allows an OSD to restart, re-peer
       and even process some replay requests before starting the recovery
       process. The <option>osd recovery thread timeout</option> sets a thread
       timeout, because multiple OSDs may fail, restart and re-peer at
       staggered rates. The <option>osd recovery max active</option> setting
       limits the number of recovery requests an OSD will process
       simultaneously to prevent the OSD from failing to serve. The <option>osd
       recovery max chunk</option> setting limits the size of the recovered
       data chunks to prevent network congestion.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING</term>
     <listitem>
      <para>
       When a new OSD joins the cluster, CRUSH will reassign placement groups
       from OSDs in the cluster to the newly added OSD. Forcing the new OSD to
       accept the reassigned placement groups immediately can put excessive
       load on the new OSD. Backfilling the OSD with the placement groups
       allows this process to begin in the background. When backfilling is
       complete, the new OSD will begin serving requests when it is ready.
      </para>
      <para>
       During the backfill operations, you may see one of several states:
       'backfill_wait' indicates that a backfill operation is pending, but is
       not yet in progress; 'backfill' indicates that a backfill operation is
       in progress; 'backfill_too_full' indicates that a backfill operation was
       requested, but could not be completed because of insufficient storage
       capacity. When a placement group cannot be backfilled, it may be
       considered 'incomplete'.
      </para>
      <para>
       &ceph; provides a number of settings to manage the load associated with
       reassigning placement groups to an OSD (especially a new OSD). By
       default, <option>osd max backfills</option> sets the maximum number of
       concurrent backfills to or from an OSD to 10. The <option>backfill full
       ratio</option> enables an OSD to refuse a backfill request if the OSD is
       approaching its full ratio (90%, by default) and change with
       <command>ceph osd set-backfillfull-ratio</command> command. If an OSD
       refuses a backfill request, the <option>osd backfill retry
       interval</option> enables an OSD to retry the request (after 10 seconds,
       by default). OSDs can also set <option>osd backfill scan min</option>
       and <option>osd backfill scan max</option> to manage scan intervals (64
       and 512, by default).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED</term>
     <listitem>
      <para>
       When the &actingset; that services a placement group changes, the data
       migrates from the old &actingset; to the new &actingset;. It may take
       some time for a new primary OSD to service requests. So it may ask the
       old primary to continue to service requests until the placement group
       migration is complete. When data migration completes, the mapping uses
       the primary OSD of the new &actingset;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STALE</term>
     <listitem>
      <para>
       While &ceph; uses heartbeats to ensure that hosts and daemons are
       running, the <literal>ceph-osd</literal> daemons may also get into a
       'stuck' state where they are not reporting statistics in a timely manner
       (for example, a temporary network fault). By default, OSD daemons report
       their placement group, boot and failure statistics every half second
       (0.5), which is more frequent than the heartbeat thresholds. If the
       primary OSD of a placement group’s &actingset; fails to report to the
       monitor or if other OSDs have reported the primary OSD as 'down', the
       monitors will mark the placement group as 'stale'.
      </para>
      <para>
       When you start your cluster, it is common to see the 'stale' state until
       the peering process completes. After your cluster has been running for a
       while, seeing placement groups in the 'stale' state indicates that the
       primary OSD for those placement groups is down or not reporting
       placement group statistics to the monitor.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-pg-stuck-states">
   <title>Identifying Troubled Placement Groups</title>
   <para>
    As previously noted, a placement group is not necessarily problematic
    because its state is not 'active+clean'. Generally, &ceph;’s ability to
    self repair may not be working when placement groups get stuck. The stuck
    states include:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Unclean</emphasis>: Placement groups contain
      objects that are not replicated the required number of times. They should
      be recovering.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Inactive</emphasis>: Placement groups cannot
      process reads or writes because they are waiting for an OSD with the most
      up-to-date data to come back up.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Stale</emphasis>: Placement groups are in an
      unknown state, because the OSDs that host them have not reported to the
      monitor cluster in a while (configured by the <option>mon osd report
      timeout</option> option).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To identify stuck placement groups, run the following:
   </para>
<screen>
&prompt.cephuser;ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
</screen>
  </sect2>

  <sect2 xml:id="op-pg-objectfinding">
   <title>Finding an Object Location</title>
   <para>
    To store object data in the &ceph; Object Store, a &ceph; client needs to
    set an object name and specify a related pool. The &ceph; client retrieves
    the latest cluster map and the CRUSH algorithm calculates how to map the
    object to a placement group, and then calculates how to assign the
    placement group to an OSD dynamically. To find the object location, all you
    need is the object name and the pool name. For example:
   </para>
<screen>
&prompt.cephuser;ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>Locating an Object</title>
    <para>
     As an example, let us create an object. Specify an object name
     'test-object-1', a path to an example file 'testfile.txt' containing some
     object data, and a pool name 'data' using the <command>rados put</command>
     command on the command line:
    </para>
<screen>
&prompt.cephuser;rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     To verify that the Ceph Object Store stored the object, run the following:
    </para>
<screen>
&prompt.cephuser;rados -p data ls
</screen>
    <para>
     Now, identify the object location. &ceph; will output the object’s
     location:
    </para>
<screen>
&prompt.cephuser;ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -> pg 0.d1743484 \
(0.4) -> up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     To remove the example object, simply delete it using the <command>rados
     rm</command> command:
    </para>
<screen>
&prompt.cephuser;rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
 <sect1 xml:id="op-osd-not-running">
  <title>OSD Is Not Running</title>

  <para>
   Under normal circumstances, simply restarting the
   <literal>ceph-osd</literal> daemon will allow it to rejoin the cluster and
   recover.
  </para>

  <sect2 xml:id="op-osd-not-start">
   <title>An OSD Will Not Start</title>
   <para>
    If you start your cluster and an OSD will not start, check the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Configuration File</emphasis>: If you were not able
      to get OSDs running from a new installation, check your configuration
      file to ensure it conforms (for example, <literal>host</literal> and not
      <literal>hostname</literal>).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Check Paths</emphasis>: Check the paths in your
      configuration, and the actual paths themselves for data and journals. If
      you separate the OSD data from the journal data and there are errors in
      your configuration file or in the actual mounts, you may have trouble
      starting OSDs. If you want to store the journal on a block device, you
      need to partition your journal disk and assign one partition per OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Check Max Threadcount</emphasis>: If you have a
      node with a lot of OSDs, you may be hitting the default maximum number of
      threads (usually 32,000), especially during recovery. You can increase
      the number of threads using the <command>sysctl</command> command to see
      if increasing the maximum number of threads to the maximum possible
      number of threads allowed (for example, 4194303) will help:
     </para>
<screen>
&prompt.root;sysctl -w kernel.pid_max=4194303
</screen>
     <para>
      If increasing the maximum thread count resolves the issue, you can make
      it permanent by including a <option>kernel.pid_max</option> setting in
      the <filename>/etc/sysctl.conf</filename> file:
     </para>
<screen>
kernel.pid_max = 4194303
</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="op-osd-failed">
   <title>An OSD Failed</title>
   <para>
    When the <literal>ceph-osd</literal> process dies, the monitor will learn
    about the failure from surviving <literal>ceph-osd</literal> daemons and
    report it via the <command>ceph health</command> command:
   </para>
<screen>
&prompt.cephuser;ceph health
HEALTH_WARN 1/3 in osds are down
</screen>
   <para>
    Specifically, you will get a warning whenever there are
    <literal>ceph-osd</literal> processes that are marked 'in' and 'down'. You
    can identify which<literal> ceph-osds</literal> are down with:
   </para>
<screen>
&prompt.cephuser;ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
</screen>
   <para>
    If there is a disk failure or other fault preventing
    <literal>ceph-osd</literal> from functioning or restarting, an error
    message should be present in its log file in
    <filename>/var/log/ceph</filename>.
   </para>
   <para>
    If the daemon stopped because of a heartbeat failure, the underlying kernel
    file system may be unresponsive. Check the <command>dmesg</command> command
    output for disk or other kernel errors.
   </para>
  </sect2>

  <sect2 xml:id="op-no-disk-space">
   <title>No Free Disk Space</title>
   <para>
    &ceph; prevents you from writing to a full OSD to prevent losing data. In
    an operational cluster, you should receive a warning when your cluster is
    getting near its full ratio. The <option>mon osd full ratio</option> option
    defaults to 0.95, or 95% of capacity before it stops clients from writing
    data. The <option>mon osd backfillfull ratio</option> defaults to 0.90, or
    90% of capacity when it blocks backfills from starting. The OSD nearfull
    ratio defaults to 0.85, or 85% of capacity when it generates a health
    warning. You can change the value of 'nearfull' with the following command:
   </para>
<screen>
&prompt.cephuser;ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    Full cluster issues usually arise when testing how &ceph; handles an OSD
    failure on a small cluster. When one node has a high percentage of the
    cluster’s data, the cluster can easily eclipse its 'nearfull' and 'full'
    ratio immediately. If you are testing how &ceph; reacts to OSD failures on
    a small cluster, you should leave sufficient free disk space and consider
    temporarily lowering the OSD full ratio, OSD backfillfull ratio and OSD
    nearfull ratio using these commands:
   </para>
<screen>
&prompt.cephuser;ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
&prompt.cephuser;ceph osd set-full-ratio <replaceable>0.0 to 1.0</replaceable>
&prompt.cephuser;ceph osd set-backfillfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    Full &osd; will be reported by <command>ceph health</command>:
   </para>
<screen>
&prompt.cephuser;ceph health
HEALTH_WARN 1 nearfull osd(s)
</screen>
   <para>
    or
   </para>
<screen>
&prompt.cephuser;ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
   <para>
    The best way to deal with a full cluster is to add new &osd;s, allowing the
    cluster to redistribute data to the newly available storage.
   </para>
   <para>
    If you cannot start an OSD because it is full, you may delete some data by
    deleting some placement group directories in the full OSD.
   </para>
   <important>
    <title>Deleting a Placement Group Directory</title>
    <para>
     If you choose to delete a placement group directory on a full OSD,
     <emphasis role="bold">do not</emphasis> delete the same placement group
     directory on another full OSD, or you may <emphasis role="bold">lose
     data</emphasis>. You <emphasis role="bold">must</emphasis> maintain at
     least one copy of your data on at least one OSD.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
