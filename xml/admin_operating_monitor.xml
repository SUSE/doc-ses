<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-monitor">
 <title>Determining Cluster State</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  When you have a running cluster, you may use the <command>ceph</command> tool
  to monitor it. Determining the cluster state typically involves checking the
  status of &osd;s, &mon;s, placement groups and &mds;s.
 </para>
 <tip>
  <title>Interactive Mode</title>
  <para>
   To run the <command>ceph</command> tool in an interactive mode, type
   <command>ceph</command> at the command line with no arguments. The
   interactive mode is more convenient if you are going to enter more
   <command>ceph</command> commands in a row. For example:
  </para>
<screen>&prompt.cephuser;ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>Checking a Cluster's Status</title>

  <para>
   To check a cluster's status, execute the following:
  </para>

<screen>&prompt.cephuser;ceph status</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph -s</screen>

  <para>
   In interactive mode, type <command>status</command> and press
   <keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   &ceph; will print the cluster status. For example, a tiny &ceph; cluster
   consisting of one monitor and two OSDs may print the following:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>Checking Cluster Health</title>

  <para>
   After you start your cluster and before you start reading and/or writing
   data, check your cluster's health:
  </para>

<screen>&prompt.cephuser;ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    If you specified non-default locations for your configuration or keyring,
    you may specify their locations:
   </para>
<screen>&prompt.cephuser;ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   The &ceph; cluster returns one of the following health codes:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      One or more OSDs are marked down. The OSD daemon may have been stopped,
      or peer OSDs may be unable to reach the OSD over the network. Common
      causes include a stopped or crashed daemon, a down host, or a network
      outage.
     </para>
     <para>
      Verify the host is healthy, the daemon is started, and network is
      functioning. If the daemon has crashed, the daemon log file
      (<filename>/var/log/ceph/ceph-osd.*</filename>) may contain debugging
      information.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, for example OSD_HOST_DOWN</term>
    <listitem>
     <para>
      All the OSDs within a particular CRUSH subtree are marked down, for
      example all OSDs on a host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      An OSD is referenced in the CRUSH map hierarchy but does not exist. The
      OSD can be removed from the CRUSH hierarchy with:
     </para>
<screen>&prompt.cephuser;ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      The usage thresholds for <emphasis>backfillfull</emphasis> (defaults to
      0.90), <emphasis>nearfull</emphasis> (defaults to 0.85),
      <emphasis>full</emphasis> (defaults to 0.95), and/or
      <emphasis>failsafe_full</emphasis> are not ascending. In particular, we
      expect <emphasis>backfillfull</emphasis> &lt;
      <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt;
      <emphasis>full</emphasis>, and <emphasis>full</emphasis> &lt;
      <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      To read the current values, run:
     </para>
<screen>
&prompt.cephuser;ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      The thresholds can be adjusted with the following commands:
     </para>
<screen>&prompt.cephuser;ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>full</emphasis> threshold and
      is preventing the cluster from servicing writes. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
     <para>
      The currently defined <emphasis>full</emphasis> ratio can be seen with:
     </para>
<screen>&prompt.cephuser;ceph osd dump | grep full_ratio</screen>
     <para>
      A short-term workaround to restore write availability is to raise the
      full threshold by a small amount:
     </para>
<screen>&prompt.cephuser;ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Add new storage to the cluster by deploying more OSDs, or delete existing
      data in order to free up space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>backfillfull</emphasis>
      threshold, which prevents data from being allowed to rebalance to this
      device. This is an early warning that rebalancing may not be able to
      complete and that the cluster is approaching full. Usage by pool can be
      checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      One or more OSDs has exceeded the <emphasis>nearfull</emphasis>
      threshold. This is an early warning that the cluster is approaching full.
      Usage by pool can be checked with:
     </para>
<screen>&prompt.cephuser;ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      One or more cluster flags of interest has been set. With the exception of
      <emphasis>full</emphasis>, these flags can be set or cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd set <replaceable>flag</replaceable>
&prompt.cephuser;ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         The cluster is flagged as full and cannot service writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Paused reads or writes.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs are not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD failure reports are being ignored, such that the monitors will not
         mark OSDs <emphasis>down</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs that were previously marked <emphasis>out</emphasis> will not be
         marked back <emphasis>in</emphasis> when they start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs will not automatically be marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Recovery or data rebalancing is suspended.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing (see <xref linkend="scrubbing"/>) is disabled.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Cache tiering activity is suspended.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      One or more OSDs has a per-OSD flag of interest set. These flags include:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSD is not allowed to start.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Failure reports for this OSD will be ignored.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         If this OSD was previously marked <emphasis>out</emphasis>
         automatically after a failure, it will not be marked
         <emphasis>in</emphasis> when it starts.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         If this OSD is down, it will not be automatically marked
         <emphasis>out</emphasis> after the configured interval.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Per-OSD flags can be set and cleared with:
     </para>
<screen>&prompt.cephuser;ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
&prompt.cephuser;ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      The &crushmap; is using very old settings and should be updated. The
      oldest tunables that can be used (that is the oldest client version that
      can connect to the cluster) without triggering this health warning is
      determined by the <option>mon_crush_min_required_version</option>
      configuration option.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      The &crushmap; is using an older, non-optimal method for calculating
      intermediate weight values for straw buckets. The &crushmap; should be
      updated to use the newer method (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      One or more cache pools is not configured with a hit set to track usage,
      which prevents the tiering agent from identifying cold objects to flush
      and evict from the cache. Hit sets can be configured on the cache pool
      with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
     <para>
      For more information on cache tiering, see
      <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      No pre-luminous v12 OSDs are running but the <option>sortbitwise</option>
      flag has not been set. You need to set the <option>sortbitwise</option>
      flag before luminous v12 or newer OSDs can start:
     </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools has reached its quota and is no longer allowing writes.
      You can set pool quotas and usage with:
     </para>
<screen>&prompt.cephuser;ceph df detail</screen>
     <para>
      You can either raise the pool quota with
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      or delete some existing data to reduce usage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Data availability is reduced, meaning that the cluster is unable to
      service potential read or write requests for some data in the cluster.
      Specifically, one or more PGs is in a state that does not allow IO
      requests to be serviced. Problematic PG states include
      <emphasis>peering</emphasis>, <emphasis>stale</emphasis>,
      <emphasis>incomplete</emphasis>, and the lack of
      <emphasis>active</emphasis> (if those conditions do not clear quickly).
      Detailed information about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Data redundancy is reduced for some data, meaning the cluster does not
      have the desired number of replicas for all data (for replicated pools)
      or erasure code fragments (for erasure coded pools). Specifically, one or
      more PGs have either the <emphasis>degraded</emphasis> or
      <emphasis>undersized</emphasis> flag set (there are not enough instances
      of that placement group in the cluster), or have not had the
      <emphasis>clean</emphasis> flag set for some time. Detailed information
      about which PGs are affected is available from:
     </para>
<screen>&prompt.cephuser;ceph health detail</screen>
     <para>
      In most cases the root cause is that one or more OSDs is currently down.
      The state of specific problematic PGs can be queried with:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Data redundancy may be reduced or at risk for some data because of a lack
      of free space in the cluster. Specifically, one or more PGs has the
      <emphasis>backfill_toofull</emphasis> or
      <emphasis>recovery_toofull</emphasis> flag set, meaning that the cluster
      is unable to migrate or recover data because one or more OSDs is above
      the <emphasis>backfillfull</emphasis> threshold.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Data scrubbing (see <xref linkend="scrubbing"/>) has discovered some
      problems with data consistency in the cluster. Specifically, one or more
      PGs has the <emphasis>inconsistent</emphasis> or
      <emphasis>snaptrim_error</emphasis> flag is set, indicating an earlier
      scrub operation found a problem, or that the <emphasis>repair</emphasis>
      flag is set, meaning a repair for such an inconsistency is currently in
      progress.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Recent OSD scrubs have uncovered inconsistencies.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      A cache tier pool is nearly full. Full in this context is determined by
      the <emphasis>target_max_bytes</emphasis> and
      <emphasis>target_max_objects</emphasis> properties on the cache pool.
      When the pool reaches the target threshold, write requests to the pool
      may block while data is flushed and evicted from the cache, a state that
      normally leads to very high latencies and poor performance. The cache
      pool target size can be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Normal cache flush and evict activity may also be throttled because of
      reduced availability or performance of the base tier, or overall cluster
      load.
     </para>
     <para>
      Find more information about cache tiering in
      <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is below the configurable threshold of
      <option>mon_pg_warn_min_per_osd</option> PGs per OSD. This can lead to
      suboptimal distribution and balance of data across the OSDs in the
      cluster reduce overall performance.
     </para>
     <para>
      See
      <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement
      Groups</link> for details on calculating an appropriate number of
      placement groups for your pool.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      The number of PGs in use is above the configurable threshold of
      <option>mon_pg_warn_max_per_osd</option> PGs per OSD. This can lead to
      higher memory usage for OSD daemons, slower peering after cluster state
      changes (for example OSD restarts, additions, or removals), and higher
      load on the &mgr;s and &mon;s.
     </para>
     <para>
      While the <option>pg_num</option> value for existing pools cannot be
      reduced. The <option>pgp_num</option> value can. This effectively
      collocates some PGs on the same sets of OSDs, mitigating some of the
      negative impacts described above. The <option>pgp_num</option> value can
      be adjusted with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      One or more pools has a <option>pgp_num</option> value less than
      <option>pg_num</option>. This is normally an indication that the PG count
      was increased without also increasing the placement behavior. This is
      normally resolved by setting <option>pgp_num</option> to match
      <option>pg_num</option>, triggering the data migration, with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      One or more pools have an average number of objects per PG that is
      significantly higher than the overall cluster average. The specific
      threshold is controlled by the
      <option>mon_pg_warn_max_object_skew</option> configuration value. This is
      usually an indication that the pool(s) containing most of the data in the
      cluster have too few PGs, and/or that other pools that do not contain as
      much data have too many PGs. The threshold can be raised to silence the
      health warning by adjusting the
      <option>mon_pg_warn_max_object_skew</option> configuration option on the
      monitors.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED¶</term>
    <listitem>
     <para>
      A pool exists that contains one or more objects but has not been tagged
      for use by a particular application. Resolve this warning by labeling the
      pool for use by an application. For example, if the pool is used by RBD:
     </para>
<screen>&prompt.cephuser;rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      If the pool is being used by a custom application 'foo', you can also
      label it using the low-level command:
     </para>
<screen>&prompt.cephuser;ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      One or more pools have reached (or is very close to reaching) its quota.
      The threshold to trigger this error condition is controlled by the
      <option>mon_pool_quota_crit_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      One or more pools are approaching their quota. The threshold to trigger
      this warning condition is controlled by the
      <option>mon_pool_quota_warn_threshold</option> configuration option. Pool
      quotas can be adjusted up or down (or removed) with:
     </para>
<screen>&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
&prompt.cephuser;ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Setting the quota value to 0 will disable the quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      One or more objects in the cluster are not stored on the node where the
      cluster wants it. This is an indication that data migration caused by a
      recent cluster change has not yet completed. Misplaced data is not a
      dangerous condition in itself. Data consistency is never at risk, and old
      copies of objects are never removed until the desired number of new
      copies (in the desired locations) are present.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      One or more objects in the cluster cannot be found. Specifically, the
      OSDs know that a new or updated copy of an object should exist, but a
      copy of that version of the object has not been found on OSDs that are
      currently online. Read or write requests to the 'unfound' objects will be
      blocked. Ideally, a down OSD can be brought back online that has the more
      recent copy of the unfound object. Candidate OSDs can be identified from
      the peering state for the PG(s) responsible for the unfound object:
     </para>
<screen>&prompt.cephuser;ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      One or more OSD requests is taking a long time to process. This can be an
      indication of extreme load, a slow storage device, or a software bug. You
      can query the request queue on the OSD(s) in question with the following
      command executed from the OSD host:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      You can see a summary of the slowest recent requests:
     </para>
<screen>&prompt.cephuser;ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      You can find the location of an OSD with:
     </para>
<screen>&prompt.cephuser;ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      One or more OSD requests have been blocked for a longer time, for example
      4096 seconds. This is an indication that either the cluster has been
      unhealthy for an extended period of time (for example not enough running
      OSDs or inactive PGs) or there is some internal problem with the OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs have not been scrubbed (see <xref linkend="scrubbing"/>)
      recently. PGs are normally scrubbed every
      <option>mon_scrub_interval</option> seconds, and this warning triggers
      when <option>mon_warn_not_scrubbed</option> such intervals have elapsed
      without a scrub. PGs will not scrub if they are not flagged as clean,
      which may happen if they are misplaced or degraded (see PG_AVAILABILITY
      and PG_DEGRADED above). You can manually initiate a scrub of a clean PG
      with:
     </para>
<screen>&prompt.cephuser;ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      One or more PGs has not been deep scrubbed (see
      <xref linkend="scrubbing"/>) recently. PGs are normally scrubbed every
      <option>osd_deep_mon_scrub_interval</option> seconds, and this warning
      triggers when <option>mon_warn_not_deep_scrubbed</option> seconds have
      elapsed without a scrub. PGs will not (deep)scrub if they are not flagged
      as clean, which may happen if they are misplaced or degraded (see
      PG_AVAILABILITY and PG_DEGRADED above). You can manually initiate a scrub
      of a clean PG with:
     </para>
<screen>&prompt.cephuser;ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="monitor-watch">
  <title>Watching a Cluster</title>

  <para>
   You can find the immediate state of the cluster using <command>ceph
   -s</command>. For example, a tiny &ceph; cluster consisting of one monitor,
   and two OSDs may print the following when a workload is running:
  </para>

<screen>
&prompt.cephuser;ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 > max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean
</screen>

  <para>
   The output provides the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster health status
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor map epoch and the status of the monitor quorum
    </para>
   </listitem>
   <listitem>
    <para>
     The OSD map epoch and the status of OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &mgr;s.
    </para>
   </listitem>
   <listitem>
    <para>
     The status of &ogw;s.
    </para>
   </listitem>
   <listitem>
    <para>
     The placement group map version
    </para>
   </listitem>
   <listitem>
    <para>
     The number of placement groups and pools
    </para>
   </listitem>
   <listitem>
    <para>
     The <emphasis>notional</emphasis> amount of data stored and the number of
     objects stored; and,
    </para>
   </listitem>
   <listitem>
    <para>
     The total amount of data stored.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>How &ceph; Calculates Data Usage</title>
   <para>
    The <literal>used</literal> value reflects the actual amount of raw storage
    used. The <literal>xxx GB / xxx GB</literal> value means the amount
    available (the lesser number) of the overall storage capacity of the
    cluster. The notional number reflects the size of the stored data before it
    is replicated, cloned or snapshot. Therefore, the amount of data actually
    stored typically exceeds the notional amount stored, because &ceph; creates
    replicas of the data and may also use storage capacity for cloning and
    snapshotting.
   </para>
  </tip>

  <para>
   Other commands that display immediate status information are:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   To get the information updated in real time, put any of these commands
   (including <command>ceph -s</command>) as an argument of the
   <command>watch</command> command:
  </para>

<screen>&prompt.root;watch -n 10 'ceph -s'</screen>

  <para>
   Press <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>
   when you are tired of watching.
  </para>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Checking a Cluster's Usage Stats</title>

  <para>
   To check a cluster’s data usage and distribution among pools, use the
   <command>ceph df</command> command. To get more details, use <command>ceph
   df detail</command>.
  </para>

<screen>
&prompt.cephuser;ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    65886G     45826G        7731M            16
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    data         1      1726M        10        17676G        1629
    rbd          4      5897M        27        22365G        3547
    ecpool       6        69M       0.2        35352G          31
[...]
</screen>

  <para>
   The <literal>GLOBAL</literal> section of the output provides an overview of
   the amount of storage your cluster uses for your data.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal>: The overall storage capacity of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: The amount of free space available in the
     cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: The amount of raw storage used.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: The percentage of raw storage used. Use
     this number in conjunction with the <literal>full ratio</literal> and
     <literal>near full ratio</literal> to ensure that you are not reaching
     your cluster’s capacity. See
     <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Storage
     Capacity</link> for additional details.
    </para>
    <note>
     <title>Cluster Fill Level</title>
     <para>
      When a raw storage fill level is getting close to 100%, you need to add
      new storage to the cluster. A higher usage may lead to single full OSDs
      and cluster health problems.
     </para>
     <para>
      Use the command <command>ceph osd df tree</command> to list the fill
      level of all OSDs.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   The <literal>POOLS</literal> section of the output provides a list of pools
   and the notional usage of each pool. The output from this section
   <emphasis>does not</emphasis> reflect replicas, clones or snapshots. For
   example, if you store an object with 1MB of data, the notional usage will be
   1MB, but the actual usage may be 2MB or more depending on the number of
   replicas, clones and snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal>: The name of the pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: The pool ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: The notional amount of data stored in kilobytes,
     unless the number appends M for megabytes or G for gigabytes.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: The notional percentage of storage used per
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: The maximum available space in the given
     pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: The notional number of objects stored per
     pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    The numbers in the POOLS section are notional. They are not inclusive of
    the number of replicas, snapshots or clones. As a result, the sum of the
    USED and %USED amounts will not add up to the RAW USED and %RAW USED
    amounts in the %GLOBAL section of the output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Checking OSD Status</title>

  <para>
   You can check OSDs to ensure they are up and on by executing:
  </para>

<screen>&prompt.cephuser;ceph osd stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph osd dump</screen>

  <para>
   You can also view OSDs according to their position in the CRUSH map.
  </para>

<screen>&prompt.cephuser;ceph osd tree</screen>

  <para>
   &ceph; will print a CRUSH tree with a host, its OSDs, whether they are up
   and their weight.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>&prompt.cephuser;ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD hosts/disks
   allowing the cluster to redistribute data to the newly available storage.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Checking Monitor Status</title>

  <para>
   After you start the cluster and before first reading and/or writing data,
   check the &mon;s quorum status. When the cluster is already serving
   requests, check the &mon;s status periodically to ensure that they are
   running.
  </para>

  <para>
   To display the monitor map, execute the following:
  </para>

<screen>&prompt.cephuser;ceph mon stat</screen>

  <para>
   or
  </para>

<screen>&prompt.cephuser;ceph mon dump</screen>

  <para>
   To check the quorum status for the monitor cluster, execute the following:
  </para>

<screen>&prompt.cephuser;ceph quorum_status</screen>

  <para>
   &ceph; will return the quorum status. For example, a &ceph; cluster
   consisting of three monitors may return the following:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Checking Placement Group States</title>

  <para>
   Placement groups map objects to OSDs. When you monitor your placement
   groups, you will want them to be <literal>active</literal> and
   <literal>clean</literal>. For a detailed discussion, refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Monitoring
   OSDs and Placement Groups.</link>
  </para>
 </sect1>
 <sect1 xml:id="monitor-adminsocket">
  <title>Using the Admin Socket</title>

  <para>
   The &ceph; admin socket allows you to query a daemon via a socket interface.
   By default, &ceph; sockets reside under <filename>/var/run/ceph</filename>.
   To access a daemon via the admin socket, log in to the host running the
   daemon and use the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   To view the available admin socket commands, execute the following command:
  </para>

<screen>&prompt.cephuser;ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   The admin socket command enables you to show and set your configuration at
   runtime. Refer to
   <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Viewing
   a Configuration at Runtime</link>for details.
  </para>

  <para>
   Additionally, you can set configuration values at runtime directly (the
   admin socket bypasses the monitor, unlike <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>
   injectargs, which relies on the monitor but does not require you to log in
   directly to the host in question).
  </para>
 </sect1>
</chapter>
