<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-sysreq">
 <title>System Requirements</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sysreq-osd">
  <title>Minimal Recommendations per Storage Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     2 GB of RAM per each Terabyte of OSD (2â€° of total raw capacity).
    </para>
   </listitem>
   <listitem>
    <para>
     1.5 GHz of a CPU core per OSD.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded or redundant 10GbE networks.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks in JBOD configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks should be exclusively used by &productname; &productnumber;.
    </para>
   </listitem>
   <listitem>
    <para>
     Dedicated disk/SSD for the operating system, preferably in a RAID1
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Additional 4 GB of RAM if cache tiering is used.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Minimal Recommendations per Monitor Node</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     3 SUSE Enterprise Storage monitor nodes recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     2 GB of RAM per monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD or fast hard disk in a RAID1 configuration
    </para>
   </listitem>
   <listitem>
    <para>
     On installations with fewer than seven nodes, these can be hosted on the
     system disk of the OSD nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Nodes should be bare metal, not virtualized, for performance reasons.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or &rgw; nodes with the actual workload is only
     supported if sufficient hardware resources are available. That means,
     that the requirements for all daemons need to be added.
    </para>
   </listitem>
   <listitem>
    <para>
     Configurations may vary from, and frequently exceed, these recommendations
     depending on individual sizing and performance needs.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interfaces for redundancy.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Minimal Recommendations for &rgw; Nodes</title>

  <para>
   &rgw; nodes should have 6-8 CPU cores and 32 GB of RAM (64 GB recommended).
  </para>
  </sect1>
  <sect1 xml:id="sysreq-mds">
  <title>&mds; Nodes</title>

  <para>
   Proper sizing of the &mds; nodes depends on the specific use case.
   Generally, the more open files the &mds; has to handle, the more CPU and RAM
   it needs. Following are the minimal requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3G of RAM per one &mds; daemon.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interface.
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU with at least 2 cores.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>
  <para>
   At least 3 GB of RAM and a quad-core CPU are required. This is includes
   running &oa; on the &smaster;. For large clusters with hundreds of nodes
   5 GB of RAM are suggested.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>Minimal Recommendations for iSCSI Nodes</title>

  <para>
   iSCSI nodes should have 6-8 CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>Network Recommendations</title>

  <para>
   The network environment where you intend to run &ceph; should ideally be a
   bonded set of at least two network interfaces that is logically split into a
   public part and trusted internal part using VLANs. The bonding mode is
   recommended to be 802.3ad when possible to provide maximum bandwidth and
   resiliency.
  </para>

  <para>
   The public VLAN serves for providing the service to the customers, the
   internal part provides for the authenticated &ceph; network communication.
   The main reason is that although &ceph; authentication and protection
   against attacks once secret keys are in place, the messages used to
   configure these keys may be transferred open and are vulnerable.
  </para>

  <tip>
   <title>Nodes Configured via DHCP</title>
   <para>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    &ceph; daemons start. If this happens, the &ceph; MONs and OSDs will not
    start correctly (running <command>systemctl status ceph\*</command> will
    result in "unable to bind" errors), and Calamari may be unable to display
    graphs. To avoid this issue, we recommend increasing the DHCP client
    timeout to at least 30 seconds on each node in your storage cluster. This
    can be done by changing the following settings on each node:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename> set
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename> set
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>
 <sect2 xml:id="storage-bp-net-private">
  <title>Adding a Private Network to a Running Cluster</title>

  <para>
   If you do not specify a cluster network during &ceph; deployment, it assumes
   a single public network environment. While &ceph; operates fine with a
   public network, its performance and security improves when you set a second
   private cluster network.
   To support two networks,
   each &ceph; node needs to have at least two network cards.
  </para>

  <para>
   You need to apply the following changes to each &ceph; node. It is
   comfortable for a small cluster, but can be very time demanding if you have
   a cluster consisting of hundreds or thousands of nodes.
  </para>

  <procedure>
   <step>
    <para>
     Stop &ceph; related services on each cluster node.
    </para>
    <para>
     Add a line to <filename>/etc/ceph/ceph.conf</filename> to define the cluster
     network, for example:
    </para>
    <screen>cluster network = 10.0.0.0/24</screen>
    <para>
      If you need to specifically assign static IP addresses or override
     <option>cluster network</option> settings, you can do so with the optional
     <option>cluster addr</option>.
    </para>
   </step>
   <step>
    <para>
     Check that the private cluster network works as expected on the OS level.
    </para>
   </step>
   <step>
    <para>
     Start &ceph; related services on each cluster node.
    </para>
<screen>sudo rcceph start</screen>
   </step>
  </procedure>
 </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Naming Limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>
</chapter>
