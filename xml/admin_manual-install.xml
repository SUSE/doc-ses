<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="app-storage-manual-inst">
 <title>Example Procedure of Manual &ceph; Installation</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/</dm:editurl>
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  The following procedure shows the commands that you need to install &ceph;
  storage cluster manually.
 </para>
 <procedure>
  <step>
   <para>
    Generate the key secrets for the &ceph; services you plan to run. You can
    use the following command to generate it:
   </para>
<screen>python -c "import os ; import struct ; import time; import base64 ; \
 key = os.urandom(16) ; header = struct.pack('&lt;hiih',1,int(time.time()),0,len(key)) ; \
 print base64.b64encode(header + key)"</screen>
  </step>
  <step>
   <para>
    Add the keys to the related keyrings. First for
    <systemitem class="username">client.admin</systemitem>, then for monitors,
    and then other related services, such as OSD, &rgw;, or MDS:
   </para>
<screen>&prompt.cephuser;ceph-authtool -n client.admin \
 --create-keyring /etc/ceph/ceph.client.admin.keyring \
 --cap mds 'allow *' --cap mon 'allow *' --cap osd 'allow *'
ceph-authtool -n mon. \
 --create-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --set-uid=0 --cap mon 'allow *'
ceph-authtool -n client.bootstrap-osd \
 --create-keyring /var/lib/ceph/bootstrap-osd/ceph.keyring \
 --cap mon 'allow profile bootstrap-osd'
ceph-authtool -n client.bootstrap-rgw \
 --create-keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring \
 --cap mon 'allow profile bootstrap-rgw'
ceph-authtool -n client.bootstrap-mds \
 --create-keyring /var/lib/ceph/bootstrap-mds/ceph.keyring \
 --cap mon 'allow profile bootstrap-mds'</screen>
  </step>
  <step>
   <para>
    Create a monmap&mdash;a database of all monitors in a cluster:
   </para>
<screen>monmaptool --create --fsid eaac9695-4265-4ca8-ac2a-f3a479c559b1 \
 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-02 192.168.43.60 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-03 192.168.43.96 /tmp/tmpuuhxm3/monmap
monmaptool --add osceph-04 192.168.43.80 /tmp/tmpuuhxm3/monmap</screen>
  </step>
  <step>
   <para>
    Create a new keyring and import keys from the admin and monitors' keyrings
    there. Then use them to start the monitors:
   </para>
<screen>&prompt.cephuser;ceph-authtool --create-keyring /tmp/tmpuuhxm3/keyring \
 --import-keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring
ceph-authtool /tmp/tmpuuhxm3/keyring \
 --import-keyring /etc/ceph/ceph.client.admin.keyring
sudo -u ceph ceph-mon --mkfs -i osceph-03 \
 --monmap /tmp/tmpuuhxm3/monmap --keyring /tmp/tmpuuhxm3/keyring
systemctl restart ceph-mon@osceph-03</screen>
  </step>
  <step>
   <para>
    Check the monitors state in &systemd;:
   </para>
<screen>&prompt.root;systemctl show --property ActiveState ceph-mon@osceph-03</screen>
  </step>
  <step>
   <para>
    Check if &ceph; is running and reports the monitor status:
   </para>
<screen>&prompt.cephuser;ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status</screen>
  </step>
  <step>
   <para>
    Check the specific services' status using the existing keys:
   </para>
<screen>&prompt.cephuser;ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status
[...]
ceph --connect-timeout 5 \
 --keyring /var/lib/ceph/bootstrap-mon/ceph-osceph-03.keyring \
 --name mon. -f json-pretty status</screen>
  </step>
  <step>
   <para>
    Import keyring from existing &ceph; services and check the status:
   </para>
<screen>&prompt.cephuser;ceph auth import -i /var/lib/ceph/bootstrap-osd/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-rgw/ceph.keyring
ceph auth import -i /var/lib/ceph/bootstrap-mds/ceph.keyring
ceph --cluster=ceph \
 --admin-daemon /var/run/ceph/ceph-mon.osceph-03.asok mon_status
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin -f json-pretty status</screen>
  </step>
  <step>
   <para>
    Prepare disks/partitions for OSDs, using the XFS file system:
   </para>
<screen>&prompt.cephuser;ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdb
ceph-disk -v prepare --fs-type xfs --data-dev --cluster ceph \
 --cluster-uuid eaac9695-4265-4ca8-ac2a-f3a479c559b1 /dev/vdc
[...]</screen>
  </step>
  <step>
   <para>
    Activate the partitions:
   </para>
<screen>&prompt.cephuser;ceph-disk -v activate --mark-init systemd --mount /dev/vdb1
ceph-disk -v activate --mark-init systemd --mount /dev/vdc1</screen>
  </step>
  <step>
   <para>
    For &productname; version 2.1 and earlier, create the default pools:
   </para>
<screen>&prompt.cephuser;ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.swift 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .intent-log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.gc 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users.uid 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw.control 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .users 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .usage 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .log 16 16
ceph --connect-timeout 5 --keyring /etc/ceph/ceph.client.admin.keyring \
 --name client.admin osd pool create .rgw 16 16</screen>
  </step>
  <step>
   <para>
    Create the &rgw; instance key from the bootstrap key:
   </para>
<screen>&prompt.cephuser;ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-rgw \
 --keyring /var/lib/ceph/bootstrap-rgw/ceph.keyring auth get-or-create \
 client.rgw.0dc1e13033d2467eace46270f0048b39 osd 'allow rwx' mon 'allow rw' \
 -o /var/lib/ceph/radosgw/ceph-rgw.<replaceable>rgw_name</replaceable>/keyring
  </screen>
  </step>
  <step>
   <para>
    Enable and start &rgw;:
   </para>
<screen>&prompt.root;systemctl enable ceph-radosgw@rgw.<replaceable>rgw_name</replaceable>
systemctl start ceph-radosgw@rgw.<replaceable>rgw_name</replaceable></screen>
  </step>
  <step>
   <para>
    Optionally, create the MDS instance key from the bootstrap key, then enable
    and start it:
   </para>
<screen>&prompt.cephuser;ceph --connect-timeout 5 --cluster ceph --name client.bootstrap-mds \
 --keyring /var/lib/ceph/bootstrap-mds/ceph.keyring auth get-or-create \
 mds.mds.<replaceable>rgw_name</replaceable> osd 'allow rwx' mds allow mon \
 'allow profile mds' \
 -o /var/lib/ceph/mds/ceph-mds.<replaceable>rgw_name</replaceable>/keyring
systemctl enable ceph-mds@mds.<replaceable>rgw_name</replaceable>
systemctl start ceph-mds@mds.<replaceable>rgw_name</replaceable></screen>
  </step>
 </procedure>
</appendix>
