<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.crowbar">
 <title>Deploying with &crow;</title>
 <para>
  &crow; (<link xlink:href="http://crowbar.github.io/"/>) is a framework to
  build complete deployments. It helps you transform groups of bare-metal nodes
  into an operational cluster within relatively short time.
 </para>
 <para>
  The deployment process consists of two basic steps: first you need to install
  and set up the &crow; admin server, then use it to deploy the available
  OSD/monitor nodes.
 </para>
 <sect1 xml:id="ceph.install.crowbar.admin_server">
  <title>Installing and Setting Up the &crow; Admin Server</title>

  <para>
   &crow; admin server is a stand-alone host with &cephos; installed, operating
   in the same network as the &ceph; OSD/MON nodes to be deployed. You need to
   configure the &crow; admin server so that it provides software repositories
   required for &ceph; deployment via TFTP protocol and PXE network boot.
  </para>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on the &crow; admin server. Optionally, you
     can install and register the &storage; 4 extension at the same time. For
     more information on &sls; installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_inst.html"/>.
     For more information on the extensions installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
    </para>
    <tip>
     <para>
      &crow; admin server does not require any graphical interface. To save the
      system resources and disk space, it is enough to install the
      <guimenu>Base System</guimenu>, <guimenu>Minimal System</guimenu> and, if
      you chose to install the &storage; 4 extension, <guimenu>SUSE Enterprise
      Storage Crowbar</guimenu> patterns from the <guimenu>Software Selection
      and System Tasks</guimenu> window. If you plan to synchronize
      repositories (see
      <xref linkend="ceph.install.crowbar.admin_server.repos"/>) with &smt;,
      add the <guimenu>Subscription Management Tool</guimenu> pattern as well.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Configure network settings for the &crow; admin server. The server needs
     to have a static IP address assigned, and the full host name including the
     domain name specified (for example
     <literal>crowbar-admin.example.com</literal>). Check with
     <command>hostname -f</command> if the host name resolves correctly. The
     local network where you deploy the cluster needs to have the DHCP server
     disabled as the &crow; admin server runs its own.
    </para>
    <tip>
     <para>
      &crow; admin server default IP address is 192.168.124.10. If it is
      possible to keep that IP in your network environment, you can save some
      time on reconfiguring the &crow; network settings.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Configure NTP to keep the server's time synchronized. See
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"/>
     for more information on the NTP protocol.
    </para>
   </step>
   <step>
    <para>
     Make sure that SSH is enabled and started on the server.
    </para>
   </step>
   <step>
    <para>
     Install and register the &storage; 4 extension if you did not install it
     in step 1. For more information on extension installation, see
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_add-ons.html"/>.
     Then install the <guimenu>SUSE Enterprise Storage Crowbar</guimenu>
     pattern in &yast;. If you prefer the command line, run <command>sudo
     zypper in -t pattern ses_admin</command>.
    </para>
   </step>
   <step>
    <para>
     Mount software repositories required for &ceph; nodes deployment with
     &crow;. See <xref linkend="ceph.install.crowbar.admin_server.repos"/> for
     more detailed information.
    </para>
   </step>
   <step>
    <para>
     If you need to further customize the &crow; admin server settings, refer
     to the <emphasis>Crowbar Setup</emphasis> chapter of the current
     <emphasis>&ocloud; Deployment Guide</emphasis> at
     <link xlink:href="https://www.suse.com/documentation"/>.
    </para>
   </step>
   <step>
    <para>
     Run the following commands to complete the &crow; admin server setup. The
     <command>install-ses-admin</command> script outputs a lot of information
     to the <filename>/var/log/crowbar/install.log</filename> log file which
     can be examined in the case of failure. Run it in the
     <systemitem>screen</systemitem> environment for safety reasons, as the
     network will be reconfigured during its run and interrupts may occur.
    </para>
<screen>sudo systemctl start crowbar-init
sudo crowbarctl database create
screen install-ses-admin</screen>
    <para>
     Be patient as the script takes several minutes to finish.
    </para>
   </step>
   <step>
    <para>
     After the script successfully finishes, you can view the &crow; admin
     server Web UI by pointing your Web browser to the &crow; admin server IP
     address (http://192.168.124.10 by default).
    </para>
   </step>
  </procedure>

  <sect2 xml:id="ceph.install.crowbar.admin_server.repos">
   <title>Prepare Software Repositories</title>
   <para>
    &crow; admin server needs to provide several software repositories so that
    the &ceph; nodes can install required packages from them on PXE boot. These
    repositories need to be mounted/synchronized under
    <filename>/srv/tftpboot/suse-12.2</filename>. The following description is
    based on the AMD64/Intel 64 architecture.
   </para>
   <tip>
    <title>Synchronizing Repositories</title>
    <para>
     There are several ways to provide the content in the repository
     directories. You can, for example, run your local &smt; instance,
     synchronize the repositories, and then export them via NFS and mount them
     on the &crow; admin server.
    </para>
   </tip>
   <variablelist>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/install</term>
     <listitem>
      <para>
       This directory needs to contain the contents of the &cephos; DVD disc
       #1. &ceph; nodes need it for the base &cephos; installation. You can
       either mount the downloaded .iso image as a loop device, or copy its
       content with <command>rsync</command>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Pool</term>
     <listitem>
      <para>
       Base software repository for &cephos;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SLES12-SP2-Updates</term>
     <listitem>
      <para>
       Repository containing updates for &cephos;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Pool</term>
     <listitem>
      <para>
       Base software repository for SES 4.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>/srv/tftpboot/suse-12.2/x86_64/repos/SUSE-Enterprise-Storage-4-Updates</term>
     <listitem>
      <para>
       Repository containing updates for SES 4.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.crowbar.deploy_nodes">
  <title>Deploying the &ceph; Nodes</title>

  <para>
   The &crow; &wi; runs on the &admserv;. It provides an overview of the most
   important deployment details, including an overview of all nodes and their
   assigned roles. It also views the &barcl; proposals, which can be edited
   and deployed. In addition, the &crow; &wi; shows details about the networks
   and switches in your cluster. It also provides graphical access to some
   tools for managing your repositories, back up or restore the
   &admserv;, export the &chef; configuration, or generate a
   <literal>supportconfig</literal> TAR archive with the most important log
   files.
  </para>

  <sect2 xml:id="sec.depl.crow.login">
   <title>Logging In</title>
   <para>
    The &crow; &wi; uses the HTTP protocol and port <literal>80</literal>.
   </para>
   <procedure xml:id="pro.depl.crow.login">
    <title>Logging In to the &crow; Web Interface</title>
    <step>
     <para>
      On any machine, start a Web browser and make sure that JavaScript and
      cookies are enabled.
     </para>
    </step>
    <step>
     <para>
      As URL, enter the IP address of the &admserv;, for example:
     </para>
<screen>http://192.168.124.10/</screen>
    </step>
    <step>
     <para>
      Log in as user <systemitem class="username">crowbar</systemitem>. If you
      have not changed the password, it is <literal>crowbar</literal> by
      default.
     </para>
    </step>
   </procedure>
<!--taroth 2016-02-05: https://bugzilla.suse.com/show_bug.cgi?id=950482-->
   <procedure xml:id="pro.depl.crow.password">
    <title>Changing the Password for the &crow; Web Interface</title>
    <step>
     <para>
      After being logged in to the &crow; &wi;, select <menuchoice>
      <guimenu>&Barcl;</guimenu> <guimenu>Crowbar</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the <literal>&crow;</literal> &barcl; entry and
      <guimenu>Edit</guimenu> the proposal.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Attributes</guimenu> section, click
      <guimenu>Raw</guimenu> to edit the configuration file.
     </para>
    </step>
    <step>
     <para>
      Search for the following entry:
     </para>
<screen>"crowbar": {
     "password": "crowbar"</screen>
    </step>
    <step>
     <para>
      Change the password.
     </para>
    </step>
    <step>
     <para>
      Confirm your change by clicking <guimenu>Save</guimenu> and
      <guimenu>Apply</guimenu>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.inst.nodes.install">
   <title>Node Installation</title>
   <para>
    The &ceph; nodes represent the actual cluster infrastructure. Node
    installation and service deployment is done automatically from the
    &admserv;. Before deploying the &ceph; service, &cephos; will be installed
    on all nodes.
   </para>
   <para>
    To install a node, you need to boot it first using PXE. It will be booted
    with an image that enables the &admserv; to discover the node and make it
    available for installation. When you have allocated the node, it will boot
    using PXE again and the automatic installation will start.
   </para>
   <procedure>
    <step>
     <para>
      Boot all nodes that you want to deploy using PXE. The nodes will boot
      into the <quote>SLEShammer</quote> image, which performs the initial
      hardware discovery.
     </para>
     <important>
      <title>Limit the Number of Concurrent Boots using PXE</title>
      <para>
       Booting many nodes using PXE at the same time will cause heavy load on
       the TFTP server, because all nodes will request the boot image at the
       same time. It is recommended to boot the nodes time-delayed.
      </para>
     </important>
    </step>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface on the &admserv;,
      for example <literal>http://192.168.124.10/</literal>. Log in as user
      <systemitem class="username">crowbar</systemitem>. The password is
      <literal>crowbar</literal> by default, if you have not changed it.
     </para>
     <para>
      Click <menuchoice> <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu>
      </menuchoice> to open the <guimenu>Node Dashboard</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Each node that has successfully booted will be listed as being in state
      <literal>Discovered</literal>, indicated by a yellow bullet. The nodes
      will be listed with their MAC address as a name. Wait until all nodes are
      listed as being <literal>Discovered</literal> before proceeding. In case
      a node does not report as being <literal>Discovered</literal>, it may
      need to be rebooted manually.
     </para>
     <figure>
      <title>Discovered Nodes</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_dashboard_initial_nodes.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Although this step is optional, it is recommended to properly group your
      nodes at this stage, since it lets you clearly arrange all nodes.
      Grouping the nodes by role would be one option, for example monitor nodes
      and OSD nodes.
     </para>
     <substeps performance="required">
      <step>
       <para>
        Enter the name of a new group into the <guimenu>New Group</guimenu>
        text box and click <guimenu>Add Group</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Drag and drop a node onto the title of the newly created group. Repeat
        this step for each node you want to put into the group.
       </para>
       <figure>
        <title>Grouping Nodes</title>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="depl_node_dashboard_groups_initial.png" width="100%" format="png"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="depl_node_dashboard_groups_initial.png" width="75%" format="png"/>
         </imageobject>
        </mediaobject>
       </figure>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      To allocate all nodes, click <menuchoice><guimenu>Nodes</guimenu>
      <guimenu>Bulk Edit</guimenu></menuchoice>. To allocate a single node,
      click the name of a node, then click <guimenu>Edit</guimenu>.
     </para>
     <figure>
      <title>Editing a Single Node</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_edit.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_edit.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
     <important>
      <title>Limit the Number of Concurrent Node Deployments</title>
      <para>
       Deploying many nodes in bulk mode will cause heavy load on the
       &admserv;. The subsequent concurrent &chef; client runs triggered by the
       nodes will require a lot of RAM on the &admserv;.
      </para>
      <para>
       Therefore it is recommended to limit the number of concurrent
       <quote>Allocations</quote> in bulk mode. The maximum number depends on
       the amount of RAM on the &admserv;&mdash;limiting concurrent deployments
       to five up to ten is recommended.
      </para>
     </important>
    </step>
    <step>
     <para>
      In single node editing mode, you can also specify the <guimenu>Filesystem
      Type</guimenu> for the node. By default, it is set to
      <literal>ext4</literal> for all nodes. It is recommended to keep this
      default.
     </para>
    </step>
    <step>
     <para>
      Provide a meaningful <guimenu>Alias</guimenu>, <guimenu>Public
      Name</guimenu> and a <guimenu>Description</guimenu> for each node and
      check the <guimenu>Allocate</guimenu> box. You can also specify the
      <guimenu>Intended Role</guimenu> for the node. This optional setting is
      used to make reasonable proposals for the &barcl;s.
     </para>
     <para>
      By default <guimenu>Target Platform</guimenu> is set to <guimenu>SLES 12
      SP2</guimenu>.
     </para>
     <tip>
      <title>Alias Names</title>
      <para>
       Providing an alias name will change the default node names (MAC address)
       to the name you provided, making it easier to identify the node.
       Furthermore, this alias will also be used as a DNS
       <literal>CNAME</literal> for the node in the admin network. As a result,
       you can access the node via this alias when, for example, logging in via
       SSH.
      </para>
     </tip>
     <figure>
      <title>Bulk Editing Nodes</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_bulk_edit_allocate.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_bulk_edit_allocate.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      When you have filled in the data for all nodes, click
      <guimenu>Save</guimenu>. The nodes will reboot and commence the
      &ay;-based &sls; installation (or installation of other target platforms,
      if selected) via a second boot using PXE. Click <menuchoice>
      <guimenu>Nodes</guimenu> <guimenu>&dash;</guimenu> </menuchoice> to
      return to the <guimenu>Node Dashboard</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Nodes that are being installed are listed with the status
      <literal>Installing</literal> (yellow/green bullet). When the
      installation of a node has finished, it is listed as being
      <literal>Ready</literal>, indicated by a green bullet. Wait until all
      nodes are listed as being <literal>Ready</literal> before proceeding.
     </para>
     <figure>
      <title>All Nodes Have Been Installed</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="depl_node_dashboard_groups_installed.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="depl_node_dashboard_groups_installed.png" width="75%" format="png"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.depl.ceph.barclamps">
   <title>&Barcl;s</title>
   <para>
    The &ceph; service is automatically installed on the nodes by using
    so-called &barcl;s&mdash;a set of recipes, templates, and installation
    instructions. A &barcl; is configured via a so-called proposal. A proposal
    contains the configuration of the service(s) associated with the &barcl;
    and a list of machines onto which the &barcl; should be deployed.
   </para>
   <para>
    All existing &barcl;s can be accessed from the &crow; Web interface by
    clicking <guimenu>Barclamps</guimenu>. To create or edit &barcl; proposals
    and deploy them, proceed as follows:
   </para>
   <procedure>
    <step>
     <para>
      Open a browser and point it to the &crow; Web interface available on the
      &admserv;, for example <literal>http://192.168.124.10/</literal>. Log in
      as user <systemitem class="username">crowbar</systemitem>. The password
      is <literal>crowbar</literal> by default, if you have not changed it.
     </para>
     <para>
      Click <guimenu>Barclamps</guimenu> to open the <guimenu>All
      Barclamps</guimenu> menu. Alternatively you may filter the list to
      <guimenu>Crowbar</guimenu> or <guimenu>SUSE Enterprise Storage</guimenu>
      &barcl;s by choosing the respective option from
      <guimenu>Barclamps</guimenu>. The <guimenu>Crowbar</guimenu> &barcl;s
      contain general recipes for setting up and configuring all nodes, while
      the <guimenu>SUSE Enterprise Storage</guimenu> &barcl;s are dedicated to
      &ceph; service deployment and configuration.
     </para>
    </step>
    <step>
     <para>
      You can either <guimenu>Create</guimenu> a proposal or
      <guimenu>Edit</guimenu> an existing one.
     </para>
     <para>
      Most &ceph; &barcl;s consist of two sections: the
      <guimenu>Attributes</guimenu> section lets you change the configuration,
      and the <guimenu>Node Deployment</guimenu> section lets you choose onto
      which nodes to deploy the &barcl;.
     </para>
    </step>
    <step>
     <para>
      To edit the <guimenu>Attributes</guimenu> section, change the values via
      the Web form. Alternatively you can directly edit the configuration file
      by clicking <guimenu>Raw</guimenu>.
     </para>
     <warning>
      <title>Raw Mode</title>
      <para>
       If you switch between <guimenu>Raw</guimenu> mode and Web form
       (<guimenu>Custom</guimenu> mode), make sure to <guimenu>Save</guimenu>
       your changes before switching, otherwise they will be lost.
      </para>
     </warning>
    </step>
    <step>
     <para>
      To assign nodes to a role, use the <guimenu>Deployment</guimenu> section
      of the &barcl;. It shows the <guimenu>Available Nodes</guimenu> that you
      can assign to the roles belonging to the &barcl;.
     </para>
     <para>
      One or more nodes are usually automatically pre-selected for available
      roles. If this pre-selection does not meet your requirements, click the
      <guimenu>Remove</guimenu> icon next to the role to remove the assignment.
      Assign a node or cluster of your choice by selecting the respective entry
      from the list of <guimenu>Available Nodes</guimenu>, <guimenu>Available
      Clusters</guimenu>, or <guimenu>Available Clusters with Remote
      Nodes</guimenu>. Drag it to the desired role and drop it onto the
      <emphasis>role name</emphasis>. Do <emphasis>not</emphasis> drop a node
      or cluster onto the text box&mdash;this is used to filter the list of
      available nodes or clusters!
     </para>
    </step>
    <step>
     <para>
      To save and deploy your edits, click <guimenu>Apply</guimenu>. To save
      your changes without deploying them, click <guimenu>Save</guimenu>. To
      remove the complete proposal, click <guimenu>Delete</guimenu>. A proposal
      that already has been deployed can only be deleted manually, see
      <xref linkend="sec.depl.ceph.barclamps.delete"/> for details.
     </para>
     <para>
      If you deploy a proposal onto a node where a previous one is still
      active, the new proposal will overwrite the old one.
     </para>
     <note>
      <title>Wait Until a Proposal has been Deployed</title>
      <para>
       Deploying a proposal might take some time (up to several minutes). It is
       strongly recommended to always wait until you see the note
       <quote>Successfully applied the proposal</quote> before proceeding on to
       the next proposal.
      </para>
     </note>
    </step>
   </procedure>
   <sect3 xml:id="sec.depl.ceph.barclamps.delete">
    <title>Delete a Proposal That Already Has Been Deployed</title>
    <para>
     To delete a proposal that already has been deployed, you first need to
     <guimenu>Deactivate</guimenu> it in the &crow; Web interface. Deactivating
     a proposal removes the chef role from the nodes, so the routine that
     installed and set up the services is not executed anymore. After a
     proposal has been deactivated, you can <guimenu>Delete</guimenu> it in the
     &crow; Web interface to remove the barclamp configuration data from the
     server.
    </para>
    <para>
     Deactivating and deleting a barclamp that already had been deployed does
     <emphasis>not</emphasis> remove packages installed when the &barcl; was
     deployed. Nor does it stop any services that were started during the
     &barcl; deployment. To undo the deployment on the affected node, you need
     to stop (<command>systemctl stop
     <replaceable>service</replaceable></command>) the respective services and
     disable (<command>systemctl disable
     <replaceable>service</replaceable></command>) them. Uninstalling packages
     should not be necessary.
    </para>
   </sect3>
   <sect3 xml:id="sec.depl.ceph.barclamps.queues">
    <title>Queuing/Dequeuing Proposals</title>
    <para>
     When a proposal is applied to one or more nodes that are nor yet available
     for deployment (for example because they are rebooting or have not been
     fully installed, yet), the proposal will be put in a queue. A message like
    </para>
<screen>Successfully queued the proposal until the following become ready: d52-54-00-6c-25-44</screen>
    <para>
     will be shown when having applied the proposal. A new button
     <guimenu>Dequeue</guimenu> will also become available. Use it to cancel
     the deployment of the proposal by removing it from the queue.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.depl.ceph.ceph">
   <title>Deploying &ceph;</title>
   <para>
    For &ceph; at least four nodes are required. If deploying the optional
    Calamari server for &ceph; management and monitoring, an additional node is
    required.
   </para>
   <para>
    The &ceph; &barcl; has the following configuration options:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>Disk Selection Method</guimenu>
     </term>
     <listitem>
      <para>
       Choose whether to only use the first available disk or all available
       disks. <quote>Available disks</quote> are all disks currently not used
       by the system. Note that one disk (usually
       <filename>/dev/sda</filename>) of every block storage node is already
       used for the operating system and is not available for &ceph;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Number of Replicas of an Object</guimenu>
     </term>
     <listitem>
      <para>
       For data security, stored objects are not only stored once, but
       redundantly. Specify the number of copies that should be stored for each
       object with this setting. The number includes the object itself. If you
       for example want the object plus two copies, specify 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>SSL Support for RadosGW</guimenu>
     </term>
     <listitem>
      <para>
       Choose whether to encrypt public communication
       (<guimenu>HTTPS</guimenu>) or not (<guimenu>HTTP</guimenu>). If choosing
       <guimenu>HTTPS</guimenu>, you need to specify the locations for the
       certificate key pair files.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>Calamari Credentials</guimenu>
     </term>
     <listitem>
      <para>
       Calamari is a Web front-end for managing and analyzing the &ceph;
       cluster. Provide administrator credentials (user name, password, e-mail
       address) in this section. When &ceph; has bee deployed you can log in to
       Calamari with these credentials. Deploying Calamari is
       optional&mdash;leave these text boxes empty when not deploying Calamari.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>The &ceph; &Barcl;</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_ceph.png" width="100%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_ceph.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    The &ceph; service consists of the following different roles:
   </para>
   <variablelist>
    <varlistentry>
     <term><guimenu>ceph-osd</guimenu>
     </term>
     <listitem>
      <para>
       The virtual block storage service. Install this role on all dedicated
       &ceph; &stornode;s (at least three), but not on any other node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-mon</guimenu>
     </term>
     <listitem>
      <para>
       Cluster monitor daemon for the &ceph; distributed file system.
       <guimenu>ceph-mon</guimenu> needs to be installed on three or five
       dedicated nodes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-calamari</guimenu>
     </term>
     <listitem>
      <para>
       Sets up the Calamari Web interface which lets you manage the &ceph;
       cluster. Deploying it is optional. The Web interface can be accessed via
       http://<replaceable>IP-ADDRESS</replaceable>/ (where
       <replaceable>IP-ADDRESS</replaceable> is the address of the machine
       where <guimenu>ceph-calamari</guimenu> is deployed on).
       <guimenu>ceph-calamari</guimenu> needs to be installed on a dedicated
       node&mdash;it is <emphasis>not</emphasis> possible to install it on a
       nodes running other services.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-radosgw</guimenu>
     </term>
     <listitem>
      <para>
       The HTTP REST gateway for &ceph;. Install it on a dedicated node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><guimenu>ceph-mds</guimenu>
     </term>
     <listitem>
      <para>
       The metadata server daemon for the &cephfs;. Install it on a dedicated
       node. For more information on &cephfs; refer to
       <xref linkend="cha.ceph.cephfs"/>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <figure>
    <title>The &ceph; &Barcl;: Node Deployment Example</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="100%" format="png"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="depl_barclamp_ceph_node_deployment.png" width="75%" format="png"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
</chapter>
