<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-ceph-examples">
<!-- ============================================================== -->
 <title>&ceph; examples</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="ceph-examples">
  <title>&ceph; examples</title>

  <para>
   Configuration for &rook; and &ceph; can be configured in multiple ways to
   provide block devices, shared file system volumes, or object storage in a
   &kube; namespace. We have provided several examples to simplify storage
   setup, but remember there are many tunables and you will need to decide what
   settings work for your use case and environment.
  </para>

  <sect2 xml:id="common-resources">
   <title>Creating common resources</title>
   <para>
    The first step to deploy &rook; is to create the common resources. The
    configuration for these resources will be the same for most deployments.
    The <filename>common.yaml</filename> sets these resources up.
   </para>
<screen>&prompt.kuberuser;kubectl create -f common.yaml</screen>
   <para>
    The examples all assume the operator and all &ceph; daemons will be started
    in the same namespace. If you want to deploy the operator in a separate
    namespace, see the comments throughout <filename>common.yaml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="operator">
   <title>Creating the operator</title>
   <para>
    After the common resources are created, the next step is to create the
    Operator deployment.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <filename>operator.yaml</filename>: The most common settings for
      production deployments
     </para>
<screen>&prompt.kuberuser;kubectl create -f operator.yaml</screen>
    </listitem>
    <listitem>
     <para>
      <filename>operator-openshift.yaml</filename>: Includes all of the
      operator settings for running a basic &rook; cluster in an OpenShift
      environment.
     </para>
<screen>&prompt.kuberuser;oc create -f operator-openshift.yaml</screen>
    </listitem>
   </itemizedlist>
   <para>
    Settings for the operator are configured through environment variables on
    the operator deployment. The individual settings are documented in the
    <filename>common.yaml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="cluster-crd">
   <title>Creating the cluster CRD</title>
   <para>
    Now that your operator is running, create your &ceph; storage cluster. This
    CR contains the most critical settings that will influence how the operator
    configures the storage. It is important to understand the various ways to
    configure the cluster. These examples represent a very small set of the
    different ways to configure the storage.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <filename>cluster.yaml</filename>: This file contains common settings for
      a production storage cluster. Requires at least three nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>cluster-test.yaml</filename>: Settings for a test cluster where
      redundancy is not configured. Requires only a single node.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>cluster-on-pvc.yaml</filename>: This file contains common
      settings for backing the &ceph; MONs and OSDs by PVs. Useful when running
      in cloud environments or where local PVs have been created for &ceph; to
      consume.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>cluster-with-drive-groups.yaml</filename>: This file contains
      example configurations for creating advanced OSD layouts on nodes using
      &ceph; Drive Groups.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>cluster-external</literal>: Connect to an external &ceph;
      cluster with minimal access to monitor the health of the cluster and
      connect to the storage.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>cluster-external-management</literal>: Connect to an external
      &ceph; cluster with the admin key of the external cluster to enable
      remote creation of pools and configure services such as an &objstore; or
      Shared file system.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="setting-up-consumable-storage">
   <title>Setting up consumable storage</title>
   <para>
    Now we are ready to setup block, shared file system or object storage in
    the &rook; &ceph; cluster. These kinds of storage are respectively referred
    to as <literal>CephBlockPool</literal>, <literal>Cephfilesystem</literal>
    and <literal>CephObjectStore</literal> in the spec files.
   </para>
   <sect3 xml:id="block-devices">
    <title>Provisioning block devices</title>
    <para>
     &ceph; can provide raw block device volumes to pods. Each example below
     sets up a storage class which can then be used to provision a block device
     in &kube; pods.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>storageclass.yaml</filename>: This example illustrates
       replication of three for production scenarios and requires at least
       three nodes. Your data is replicated on three different &kube; worker
       nodes and intermittent or long-lasting single node failures will not
       result in data unavailability or loss.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>storageclass-ec.yaml</filename>: Configures erasure coding for
       data durability rather than replication.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>storageclass-test.yaml</filename>: Replication of one for test
       scenarios and it requires only a single node. Do not use this for
       applications that store valuable data or have high-availability storage
       requirements, since a single node failure can result in data loss.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The storage classes are found in different sub-directories depending on
     the driver:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>csi/rbd</literal>: The CSI driver for block devices.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="shared-filesystem">
    <title>Shared file system</title>
    <para>
     &cephfs; (CephFS) allows the user to mount a shared POSIX-compliant folder
     into one or more hosts (pods in the container world). This storage is
     similar to NFS shared storage or CIFS shared folders.
    </para>
    <para>
     File storage contains multiple pools that can be configured for different
     scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>filesystem.yaml</filename>: Replication of three for
       production scenarios. Requires at least three nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>filesystem-ec.yaml</filename>: Erasure coding for production
       scenarios. Requires at least three nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>filesystem-test.yaml</filename>: Replication of one for test
       scenarios. Requires only a single node.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Dynamic provisioning is possible with the CSI driver. The storage class
     for shared file systems is found in the <filename>csi/cephfs</filename>
     directory.
    </para>
   </sect3>
   <sect3 xml:id="ceph-examples-object-storage">
    <title>&objstore;</title>
    <para>
     &ceph; supports storing blobs of data called objects that support
     HTTP[S]-type get/put/post and delete semantics.
    </para>
    <para>
     &objstore; contains multiple pools that can be configured for different
     scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>object.yaml</filename>: Replication of three for production
       scenarios. Requires at least three nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>object-openshift.yaml</filename>: Replication of three with
       &ogw; in a port range valid for OpenShift. Requires at least three
       nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>object-ec.yaml</filename>: Erasure coding rather than
       replication for production scenarios. Requires at least three nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>object-test.yaml</filename>: Replication of one for test
       scenarios. Requires only a single node.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="object-storage-user">
    <title>&objstore; user</title>
    <itemizedlist>
     <listitem>
      <para>
       <filename>object-user.yaml</filename>: Creates a simple object storage
       user and generates credentials for the S3 API.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="object-storage-buckets">
    <title>&objstore; buckets</title>
    <para>
     The &ceph; operator also runs an object store bucket provisioner which can
     grant access to existing buckets or dynamically provision new buckets.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>object-bucket-claim-retain.yaml</filename>: Creates a request
       for a new bucket by referencing a StorageClass which saves the bucket
       when the initiating OBC is deleted.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>object-bucket-claim-delete.yaml</filename>: Creates a request
       for a new bucket by referencing a StorageClass which deletes the bucket
       when the initiating OBC is deleted.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>storageclass-bucket-retain.yaml</filename>: Creates a new
       StorageClass which defines the &ceph; Object Store, a region, and
       retains the bucket after the initiating OBC is deleted.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>storageclass-bucket-delete.yaml</filename> Creates a new
       StorageClass which defines the &ceph; Object Store, a region, and
       deletes the bucket after the initiating OBC is deleted.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
</chapter>
