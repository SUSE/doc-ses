<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE glossary
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<glossary xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="gloss-storage-glossary">
<!-- === CEPH ====================================================== -->
 <title>Glossary</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <glossdiv xml:id="gloss-storage-general">
  <title>General</title>
  <glossentry xml:id="gloss-storage-adminode"><glossterm>Admin node</glossterm>
   <glossdef>
    <para>
     The node from which you run the <command>ceph-deploy</command> utility to
     deploy &ceph; on OSD nodes.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-bucket"><glossterm>Bucket</glossterm>
   <glossdef>
    <para>
     A point which aggregates other nodes into a hierarchy of physical
     locations.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-crush"><glossterm>CRUSH, &crushmap;</glossterm>
   <glossdef>
    <para>
     An algorithm that determines how to store and retrieve data by computing
     data storage locations. CRUSH requires a map of the cluster to
     pseudo-randomly store and retrieve data in OSDs with a uniform
     distribution of data across the cluster.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-mon"><glossterm>Monitor node, MON</glossterm>
   <glossdef>
    <para>
     A cluster node that maintains maps of cluster state, including the monitor
     map, or the OSD map.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-osd"><glossterm>OSD node</glossterm>
   <glossdef>
    <para>
     A cluster node that stores data, handles data replication, recovery,
     backfilling, rebalancing, and provides some monitoring information to
     &ceph; monitors by checking other &ceph; OSD daemons.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-node"><glossterm>Node</glossterm>
   <glossdef>
    <para>
     Any single machine or server in a Ceph cluster.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Pool</glossterm>
   <glossdef>
    <para>
     Logical partitions for storing objects such as disk images.
    </para>
   </glossdef>
  </glossentry>
  <glossentry><glossterm>Rule Set</glossterm>
   <glossdef>
    <para>
     Rules to determine data placement for a pool.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>
<!-- === CEPH ====================================================== -->
 <glossdiv xml:id="gloss-storage-ceph">
  <title>&ceph; Specific Terms</title>
  <glossentry><glossterm>Calamari</glossterm>
   <glossdef>
    <para>
     A management and monitoring system for &ceph; storage cluster. It provides
     a Web user interface that makes &ceph; cluster monitoring simple.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-ceph-storage-cluster"><glossterm>&ceph; Storage Cluster</glossterm>
   <glossdef>
    <para>
     The core set of storage software which stores the userâ€™s data. Such a
     set consists of &ceph; monitors and OSDs.
    </para>
    <para>
     AKA <quote>&ceph; Object Store</quote>.
    </para>
   </glossdef>
  </glossentry>
  <glossentry xml:id="gloss-storage-rgw"><glossterm>&rgw;</glossterm>
   <glossdef>
    <para>
     The S3/Swift gateway component for &ceph; Object Store.
    </para>
   </glossdef>
  </glossentry>
 </glossdiv>
</glossary>
