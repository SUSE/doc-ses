<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-storage-about">
 <title>&productname; &productnumber; and &ceph;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &productname; &productnumber; is a distributed storage system designed for
  scalability, reliability and performance which is based on the &ceph;
  technology. A &ceph; cluster can be run on commodity servers in a common
  network like Ethernet. The cluster scales up well to thousands of servers
  (later on referred to as nodes) and into the petabyte range. As opposed to
  conventional systems which have allocation tables to store and fetch data,
  &ceph; uses a deterministic algorithm to allocate storage for data and has no
  centralized information structure. &ceph; assumes that in storage clusters
  the addition or removal of hardware is the rule, not the exception. The
  &ceph; cluster automates management tasks such as data distribution and
  redistribution, data replication, failure detection and recovery. &ceph; is
  both self-healing and self-managing which results in a reduction of
  administrative and budget overhead.
 </para>
 <para>
  This chapter provides a high level overview of &productname; &productnumber;
  and briefly describes the most important components.
 </para>
 <tip>
  <para>
   Since &productname; 5, the only cluster deployment method is &deepsea;.
   Refer to <xref linkend="ceph-install-saltstack"/> for details about the
   deployment process.
  </para>
 </tip>
 <sect1 xml:id="storage-intro-features">
  <title>&ceph; Features</title>

  <para>
   The &ceph; environment has the following features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Scalability</term>
    <listitem>
     <para>
      &ceph; can scale to thousands of nodes and manage storage in the range of
      petabytes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Commodity Hardware</term>
    <listitem>
     <para>
      No special hardware is required to run a &ceph; cluster. For details, see
      <xref linkend="storage-bp-hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Self-managing</term>
    <listitem>
     <para>
      The &ceph; cluster is self-managing. When nodes are added, removed or
      fail, the cluster automatically redistributes the data. It is also aware
      of overloaded disks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>No Single Point of Failure</term>
    <listitem>
     <para>
      No node in a cluster stores important information alone. The number of
      redundancies can be configured.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Open Source Software</term>
    <listitem>
     <para>
      &ceph; is an open source software solution and independent of specific
      hardware or vendors.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-intro-core">
  <title>Core Components</title>

  <para>
   To make full use of &ceph;'s power, it is necessary to understand some of
   the basic components and concepts. This section introduces some parts of
   &ceph; that are often referenced in other chapters.
  </para>

  <sect2 xml:id="storage-intro-core-rados">
   <title>RADOS</title>
   <para>
    The basic component of &ceph; is called <emphasis>RADOS</emphasis>
    <emphasis>(Reliable Autonomic Distributed Object Store)</emphasis>. It is
    responsible for managing the data stored in the cluster. Data in &ceph; is
    usually stored as objects. Each object consists of an identifier and the
    data.
   </para>
   <para>
    RADOS provides the following access methods to the stored objects that
    cover many use cases:
   </para>
   <variablelist>
    <varlistentry>
     <term>&rgw;</term>
     <listitem>
      <para>
       &rgw; is an HTTP REST gateway for the RADOS object store. It enables
       direct access to objects stored in the &ceph; cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RADOS Block Device</term>
     <listitem>
      <para>
       RADOS Block Devices (RBD) can be accessed like any other block device.
       These can be used for example in combination with &libvirt; for
       virtualization purposes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&cephfs;</term>
     <listitem>
      <para>
       The &ceph; File System is a POSIX-compliant file system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem></term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> is a library that can
       be used with many programming languages to create an application capable
       of directly interacting with the storage cluster.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> is used by &rgw; and RBD
    while &cephfs; directly interfaces with RADOS
    <xref linkend="storage-intro-core-rados-figure"/>.
   </para>
   <figure xml:id="storage-intro-core-rados-figure">
    <title>Interfaces to the &ceph; Object Store</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage-intro-core-crush">
   <title>CRUSH</title>
   <para>
    At the core of a &ceph; cluster is the <emphasis>CRUSH</emphasis>
    algorithm. CRUSH is the acronym for <emphasis>Controlled Replication Under
    Scalable Hashing</emphasis>. CRUSH is a function that handles the storage
    allocation and needs comparably few parameters. That means only a small
    amount of information is necessary to calculate the storage position of an
    object. The parameters are a current map of the cluster including the
    health state, some administrator-defined placement rules and the name of
    the object that needs to be stored or retrieved. With this information, all
    nodes in the &ceph; cluster are able to calculate where an object and its
    replicas are stored. This makes writing or reading data very efficient.
    CRUSH tries to evenly distribute data over all nodes in the cluster.
   </para>
   <para>
    The <emphasis>CRUSH map</emphasis> contains all storage nodes and
    administrator-defined placement rules for storing objects in the cluster.
    It defines a hierarchical structure that usually corresponds to the
    physical structure of the cluster. For example, the data-containing disks
    are in hosts, hosts are in racks, racks in rows and rows in data centers.
    This structure can be used to define <emphasis>failure domains</emphasis>.
    &ceph; then ensures that replications are stored on different branches of a
    specific failure domain.
   </para>
   <para>
    If the failure domain is set to rack, replications of objects are
    distributed over different racks. This can mitigate outages caused by a
    failed switch in a rack. If one power distribution unit supplies a row of
    racks, the failure domain can be set to row. When the power distribution
    unit fails, the replicated data is still available on other rows.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-core-nodes">
   <title>&ceph; Nodes and Daemons</title>
   <para>
    In &ceph;, nodes are servers working for the cluster. They can run several
    different types of daemons. We recommend to run only one type of
    daemon on each node, except for &mgr; daemons which can be collocated with
    &mon;s. Each cluster requires at least &mon;, &mgr;, and &osd; daemons:
   </para>
   <variablelist>
    <varlistentry>
     <term>&adm;</term>
     <listitem>
      <para>
       <emphasis>&adm;</emphasis> is a &ceph; cluster node where the &smaster;
       service is running. The &adm; is a central point of the &ceph; cluster
       because it manages the rest of the cluster nodes by querying and
       instructing their &sminion; services.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&ceph; Monitor</term>
     <listitem>
      <para>
       <emphasis>&mon;</emphasis> (often abbreviated as
       <emphasis>MON</emphasis>) nodes maintain information about the cluster
       health state, a map of all nodes and data distribution rules (see
       <xref linkend="storage-intro-core-crush"/>).
      </para>
      <para>
       If failures or conflicts occur, the &mon; nodes in the cluster decide by
       majority which information is correct. To form a qualified majority, it
       is recommended to have an odd number of &mon; nodes, and at least three
       of them.
      </para>
      <para>
       If more than one site is used, the &mon; nodes should be distributed
       over an odd number of sites. The number of &mon; nodes per site should
       be such that more than 50% of the &mon; nodes remain functional if one
       site fails.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&mgr;</term>
     <listitem>
      <para>
       The &mgr; collects the state information from the whole
       cluster. The &mgr; daemon runs alongside the monitor daemons.
       It provides additional monitoring, and interfaces the external
       monitoring and management systems. It includes other services
       as well, for example the &dashboard; Web UI.
       The &ceph; Dashboard Web UI runs on the same node as the &mgr;.
      </para>
      <para>
       The &ceph; manager requires no additional configuration, beyond ensuring
       it is running. You can deploy it as a separate role using &deepsea;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&osd;</term>
     <listitem>
      <para>
       A <emphasis>&osd;</emphasis> is a daemon handling <emphasis>Object
       Storage Devices</emphasis> which are a physical or logical storage units
       (hard disks or partitions). Object Storage Devices can be physical
       disks/partitions or logical volumes. The daemon additionally takes care
       of data replication and rebalancing in case of added or removed nodes.
      </para>
      <para>
       &ceph; OSD daemons communicate with monitor daemons and provide them
       with the state of the other OSD daemons.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    To use &cephfs;, &rgw;, &ganesha;, or &igw;, additional nodes are required:
   </para>
   <variablelist>
    <varlistentry>
     <term>&mds; (MDS)</term>
     <listitem>
      <para>
       The &mds;s store metadata for the &cephfs;. By using an MDS you can
       execute basic file system commands such as <command>ls</command> without
       overloading the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&ogw;</term>
     <listitem>
      <para>
       The &ogw; is an HTTP REST gateway for the RADOS object store. It is
       compatible with OpenStack Swift and Amazon S3 and has its own user
       management.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&ganesha;</term>
     <listitem>
      <para>
       &ganesha; provides an NFS access to either the &ogw; or the &cephfs;. It
       runs in the user instead of the kernel space and directly interacts with
       the &ogw; or &cephfs;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&igw;</term>
     <listitem>
      <para>
       iSCSI is a storage network protocol that allows clients to send SCSI
       commands to SCSI storage devices (targets) on remote servers.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&sgw;</term>
     <listitem>
      <para>
       The &sgw; provides a SAMBA access to data stored on &cephfs;.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-intro-structure">
  <title>Storage Structure</title>

  <sect2 xml:id="storage-intro-structure-pool">
   <title>Pool</title>
   <para>
    Objects that are stored in a &ceph; cluster are put into
    <emphasis>pools</emphasis>. Pools represent logical partitions of the
    cluster to the outside world. For each pool a set of rules can be defined,
    for example, how many replications of each object must exist. The standard
    configuration of pools is called <emphasis>replicated pool</emphasis>.
   </para>
   <para>
    Pools usually contain objects but can also be configured to act similar to
    a RAID 5. In this configuration, objects are stored in chunks along with
    additional coding chunks. The coding chunks contain the redundant
    information. The number of data and coding chunks can be defined by the
    administrator. In this configuration, pools are referred to as
    <emphasis>erasure coded pools</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-pg">
   <title>Placement Group</title>
   <para>
    <emphasis>Placement Groups</emphasis> (PGs) are used for the distribution
    of data within a pool. When creating a pool, a certain number of placement
    groups is set. The placement groups are used internally to group objects
    and are an important factor for the performance of a &ceph; cluster. The PG
    for an object is determined by the object's name.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-example">
   <title>Example</title>
   <para>
    This section provides a simplified example of how &ceph; manages data (see
    <xref linkend="storage-intro-structure-example-figure"/>). This example
    does not represent a recommended configuration for a &ceph; cluster. The
    hardware setup consists of three storage nodes or &ceph; OSDs
    (<literal>Host 1</literal>, <literal>Host 2</literal>, <literal>Host
    3</literal>). Each node has three hard disks which are used as OSDs
    (<literal>osd.1</literal> to <literal>osd.9</literal>). The &mon; nodes are
    neglected in this example.
   </para>
   <note>
    <title>Difference between &ceph; OSD and OSD</title>
    <para>
     While <emphasis>&ceph; OSD</emphasis> or <emphasis>&ceph; OSD
     daemon</emphasis> refers to a daemon that is run on a node, the word
     <emphasis>OSD</emphasis> refers to the logical disk that the daemon
     interacts with.
    </para>
   </note>
   <para>
    The cluster has two pools, <literal>Pool A</literal> and <literal>Pool
    B</literal>. While Pool A replicates objects only two times, resilience for
    Pool B is more important and it has three replications for each object.
   </para>
   <para>
    When an application puts an object into a pool, for example via the REST
    API, a Placement Group (<literal>PG1</literal> to <literal>PG4</literal>)
    is selected based on the pool and the object name. The CRUSH algorithm then
    calculates on which OSDs the object is stored, based on the Placement Group
    that contains the object.
   </para>
   <para>
    In this example the failure domain is set to host. This ensures that
    replications of objects are stored on different hosts. Depending on the
    replication level set for a pool, the object is stored on two or three OSDs
    that are used by the Placement Group.
   </para>
   <para>
    An application that writes an object only interacts with one &ceph; OSD,
    the primary &ceph; OSD. The primary &ceph; OSD takes care of replication
    and confirms the completion of the write process after all other OSDs have
    stored the object.
   </para>
   <para>
    If <literal>osd.5</literal> fails, all object in <literal>PG1</literal> are
    still available on <literal>osd.1</literal>. As soon as the cluster
    recognizes that an OSD has failed, another OSD takes over. In this example
    <literal>osd.4</literal> is used as a replacement for
    <literal>osd.5</literal>. The objects stored on <literal>osd.1</literal>
    are then replicated to <literal>osd.4</literal> to restore the replication
    level.
   </para>
   <figure xml:id="storage-intro-structure-example-figure">
    <title>Small Scale Ceph Example</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    If a new node with new OSDs is added to the cluster, the cluster map is
    going to change. The CRUSH function then returns different locations for
    objects. Objects that receive new locations will be relocated. This process
    results in a balanced usage of all OSDs.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about-bluestore">
  <title>&bluestore;</title>

  <para>
   &bluestore; is a new default storage back-end for &ceph; from &productname;
   5. It has better performance than &filestore;, full data check-summing, and
   built-in compression.
  </para>

  <para>
   &bluestore; manages either one, two, or three storage devices. In the
   simplest case, &bluestore; consumes a single primary storage device. The
   storage device is normally partitioned into two parts:
  </para>

  <orderedlist>
   <listitem>
    <para>
     A small partition named BlueFS that implements file system-like
     functionalities required by RocksDB.
    </para>
   </listitem>
   <listitem>
    <para>
     The rest of the device is normally a large partition occupying the rest of
     the device. It is managed directly by &bluestore; and contains all of the
     actual data. This primary device is normally identified by a block
     symbolic link in the data directory.
    </para>
   </listitem>
  </orderedlist>

  <para>
   It is also possible to deploy &bluestore; across two additional devices:
  </para>

  <para>
   A <emphasis>WAL device</emphasis> can be used for &bluestore;’s internal
   journal or write-ahead log. It is identified by the
   <literal>block.wal</literal> symbolic link in the data directory. It is only
   useful to use a separate WAL device if the device is faster than the primary
   device or the DB device, for example when:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The WAL device is an NVMe, and the DB device is an SSD, and the data
     device is either SSD or HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Both the WAL and DB devices are separate SSDs, and the data device is an
     SSD or HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A <emphasis>DB device</emphasis> can be used for storing &bluestore;’s
   internal metadata. &bluestore; (or rather, the embedded RocksDB) will put as
   much metadata as it can on the DB device to improve performance. Again, it
   is only helpful to provision a shared DB device if it is faster than the
   primary device.
  </para>

  <tip>
   <title>Plan for the DB Size</title>
   <para>
    Plan thoroughly to ensure sufficient size of the DB device. If the DB device
    fills up, metadata will spill over to the primary device, which badly
    degrades the OSD's performance.
   </para>
   <para>
    You can check if a WAL/DB partition is getting full and spilling over with
    the <command>ceph daemon osd<replaceable>.ID</replaceable> perf
    dump</command> command. The <option>slow_used_bytes</option> value shows
    the amount of data being spilled out:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage-moreinfo">
  <title>Additional Information</title>

  <itemizedlist>
   <listitem>
    <para>
     &ceph; as a community project has its own extensive online documentation.
     For topics not found in this manual, refer to
     <link xlink:href="http://docs.ceph.com/docs/master/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The original publication <emphasis>CRUSH: Controlled, Scalable,
     Decentralized Placement of Replicated Data</emphasis> by <emphasis>S.A.
     Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> provides helpful
     insight into the inner workings of &ceph;. Especially when deploying large
     scale clusters it is a recommended reading. The publication can be found
     at <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
   <listitem>
     <para>
       &productname; can be used with non-SUSE &ostack; distributions.
       The &ceph; clients need to be at a level that is compatible with &productname;.
     </para>
     <note>
       <para>
         SUSE supports the server component of the &ceph; deployment
         and the client is supported by the &ostack; distribution vendor.
       </para>
     </note>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
