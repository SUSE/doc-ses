<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.about">
 <title>About &productname;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES5</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="storage.intro">
  <title>Introduction Into &ceph;</title>

  <para>
   &storage; is a distributed storage system designed for scalability, reliability and
   performance which is based on the &ceph; technology. A &ceph; cluster can be run on
   commodity hardware and scales up well to thousands of nodes and into the petabyte range.
   It assumes that in clusters the addition or removal of hardware is the rule, not the exception.
   As opposed to conventional systems which have allocation tables to store and
   fetch data, &ceph; uses a deterministic function to allocate storage for data
   and has no centralized information structure. The &ceph; cluster automates 
   management tasks such as data distribution and redistribution, data replication, failure detection and recovery. &ceph;
   is both self-healing and self-managing which results in a reduction of
   administrative and budget overhead. 
  </para>
  <para>
   For managing the nodes &storage; can be deployed with either &deepsea;
   and &salt; or with Crowbar.
  </para>
  <para>
   This chapter provides a high level overview of &storage; and briefly
   describes the most important components.
  </para>


  <sect2 xml:id="storage.intro.features">
   <title>&ceph; Features</title>
   <para>
    The &ceph; environment has the following features:
   </para>
   <variablelist>
    <varlistentry>
     <term>Scalability</term>
     <listitem>
      <para>
       &ceph; easily scales to thousands of nodes and can manage storage in
       the range of petabytes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Commodity Hardware</term>
     <listitem>
      <para>
       No special hardware is required to run a &ceph; cluster. However
       depending on the configuration, some considerations have to be taken
       into account when buying hardware.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Self-managing</term>
     <listitem>
      <para>
       The &ceph; cluster is self-managing. When nodes are added, removed or
       fail, the cluster is automatically redistributing the data. It is
       also aware of overloaded OSDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>No Single Point of Failure</term>
     <listitem>
      <para>
       No node in a cluster stores important information alone. The number
       of redundancies can be set in configuration files. With &ceph; being
       highly redundant, the physical infrastructure consisting of power
       supply, network, fire protection and so on becomes the more important
       factor.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Open Source Software</term>
     <listitem>
      <para>
       &ceph; is an Open Source software solution and independent of specific
       hardware or vendors.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>


  <sect2 xml:id="storage.intro.core">
   <title>Core Components</title>

   <sect3 xml:id="storage.intro.core.rados">
    <title>RADOS</title>
    <para>
     The basic component of &ceph; is called <emphasis>RADOS</emphasis>
     <emphasis>(Reliable Autonomic Distributed Object Store)</emphasis>.
     It is responsible for managing the data stored in the cluster. Data 
     in &ceph; is usually stored as object. Each object consists of an
     identifier and the data.
    </para>
    <para>
     RADOS provides 4 access methods to the stored objects, covering many
     use cases:
    </para>
    <variablelist>
     <varlistentry>
       <term>radosgw</term>
       <listitem>
        <para>
         radosgw is a HTTP REST gateway for the RADOS object store. It enables
         direct access to objects stored in the &ceph; cluster.
        </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term>RADOS Block Device</term>
       <listitem>
        <para>
         RADOS Block Devices (RBD) can be accessed like any other block device.
         These can be used for instance in combination with <emphasis>libvirt</emphasis> for
         virtualization purposes.
        </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term>Ceph FS</term>
       <listitem>
        <para>
         The Ceph Filesystem (Ceph FS) is a POSIX-compliant filesystem.
        </para>
       </listitem>
     </varlistentry>
     <varlistentry>
       <term>librados</term>
       <listitem>
        <para>
         librados is a library that can be used with many programming languages 
         to create a program capable of directly interacting with the storage
         cluster.
        </para>
       </listitem>
     </varlistentry>
    </variablelist>
    <para>
     librados is used by radosgw and RBD while Ceph FS directly interfaces with
     RADOS <xref linkend="storage.intro.core.rados.figure" />.
    </para>
    
    <figure xml:id="storage.intro.core.rados.figure">
     <title>Interfaces to the &ceph; Object Store</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
   <sect3 xml:id="storage.intro.core.crush">
    <title>CRUSH</title>
    <para>
     At the core of a &ceph; cluster is the <emphasis>CRUSH</emphasis> algorithm. 
     CRUSH is the acronym for <emphasis>Controlled, Scalable, Decentralized
     Placement of Replicated Data</emphasis>.
     CRUSH is a function that handles the storage allocation and needs comparably
     few parameters. That means only a small amount of information is necessary to
     calculate the storage position of an object. The parameters are a
     current map of the cluster including the health state, some administrator-
     defined placement rules, and the name of the object that has to be stored
     or retrieved. With this information, all nodes in the &ceph; cluster are able
     to calculate where an object and its replicas are stored. This makes writing
     or reading data very efficient. Data is evenly distributed over all nodes in
     the cluster.
    </para>
    <para>
     The map of the cluster contains:
         The set of maps comprising the monitor map, OSD map, PG map, MDS map and CRUSH map. See Cluster Map for details.
    </para>
   </sect3>
   <sect3 xml:id="storage.intro.core.nodes">
    <title>&ceph; Nodes</title>
    <para>
     In &ceph; nodes are computers working for the cluster. They can run 4
     different types of daemons. It is recommended to run only one type of
     daemon on each node. Each cluster requires at least two different types
     of daemons and therefore nodes:
    </para>
    <variablelist>
     <varlistentry>
      <term>&ceph; Monitor</term>
      <listitem>
       <para>
        <emphasis>&ceph; Monitor</emphasis> (often abbreviated <emphasis>MON</emphasis>)
        nodes maintain information about cluster health state and a
        map of the cluster and placement rules. The placement rules are defined
        by the administrator and are used to improve the resilience of the cluster.
        For example the resilience can be increased by distributing object
        replications to different server racks or data centers.
       </para>
       <para>
        If conflicts occur, the monitor nodes in the cluster decide by majority which
        information is correct. To be able to form a majority, at least 3 monitor
        nodes are needed. 
       </para>
       <para>
        Monitor nodes also keep history of changes performed to the cluster. The
        history is necessary information when nodes are added or removed and data
        needs to be redistributed.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&ceph; OSD</term>
      <listitem>
       <para>
        A <emphasis>&ceph; OSD</emphasis> is a daemon handling one or more
        object-based storage devices (OSDs). Object-based storage devices can be
        physical disks/partitions or logical volumes. The daemon additionally
        takes care of data replication and rebalancing in case of added or
        removed nodes.
       </para>
       <para>
        &ceph; OSD daemons communicate with monitor nodes and provide them with the
        state of the other OSD daemons.
       </para>
       <para>
        The acronym <emphasis>OSD</emphasis> refers to <emphasis>Object Storage
        Device</emphasis> which is a physical or logical storage unit
        (LUN, hard disk or partition).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <note>
     <title>Each Node Type on a Separate Server</title>
     <para>
      We strongly recommend to install only one node type on a single server.
     </para>
    </note>
    <para>
      To use <emphasis>Ceph FS</emphasis> or <emphasis>radosgw</emphasis>
      (see <xref linkend="storage.intro.core.rados" />), additional nodes
      are required:
    </para>
    <variablelist>
     <varlistentry>
      <term>Metadata Server (MDS)</term>
      <listitem>
       <para>
        The metadata servers store metadata for the &ceph; file system. By using
        MDS you can execute basic file system commands such as
        <command>ls</command> without overloading the cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&rgw;</term>
      <listitem>
       <para>
        &rgw; is an HTTP REST gateway for the RADOS object store. You can use
        this node type also when using the &ceph; file system.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
  <sect2 xml:id="storage.intro.structure">
   <title>Storage Structure</title>
   <sect3 xml:id="storage.intro.structure.pool">
    <title>Pool</title>
    <para>
     Objects that are stored in a &ceph; cluster are put into <emphasis>pools</emphasis>. 
     Pools represent logical partitions of the cluster to the outside world.
     The access to data is 
     For each pool a set of rules can be defined, for example how many replications
     of each object must exist. This standard configuration of pools is called
     <emphasis>replicated pool</emphasis>.
    </para>
    <para>
     Pools usually contain objects but can also be configured to act more like a
     RAID 5. In this configuration, data is stored in chunks and a additional
     coding chunks. The coding chunks contain the redundant information. The
     number of coding chunks can be defined by the administrator. In this
     configuration, pools are referred to as <emphasis>erasure pools</emphasis>.
    </para>
   </sect3>
   <sect3 xml:id="storage.intro.structure.pg">
    <title>Placement Group</title>
    <para>
     <emphasis>Placement Groups</emphasis> (PG) are used for the distribution
     of data within the cluster. When creating a pool, a certain number of
     placement groups is set. The placement groups are used internally to track
     objects and are an important factor for the performance of a &ceph; cluster.
    </para>
   </sect3>
   <sect3 xml:id="storage.intro.structure.example">
    <title>Example</title>
    <para>
     This section provides a simple example of how &ceph; manages data (see figure
     <xref linkend="storage.intro.structure.example.figure" />). The hardware
     setup consists of 3 storage nodes or &ceph; OSDs (<literal>Host 1</literal>, 
     <literal>Host 2</literal>, <literal>Host 3</literal>). Each node has 3
     hard disks which are used as OSDs (<literal>osd.0</literal> to 
     <literal>osd.9</literal>). The monitor nodes are neglected in this example.
    </para>
    <note>
     <title>Difference Between &ceph; OSD and OSD</title>
     <para>
      While <emphasis>&ceph; OSD</emphasis> or <emphasis>&ceph; OSD daemon</emphasis>
      referes to a daemon that is run on a node, the word
      <emphasis>OSD</emphasis> refers to the logical disk that the daemon interacts with.
     </para>
    </note>
    <para>
     The cluster has two pools, <literal>Pool A</literal> and <literal>Pool B</literal>.
     While Pool A replicates objects only 2 times, resilience for Pool B is
     more important and it has 3 replications for each object.
    </para>
    <para>
     When a program puts an object into a defined pool, for example via
     the REST API, a Placement Group (<literal>PG1</literal> to
     <literal>PG4</literal>) is selected based on the pool and
     the object ID. With the Placement Group the CRUSH algorithm calculates
     on which OSDs the object is stored.
    </para>
    <para>
     In this example the failure domain is set to host. This ensures that
     replications of objects are stored on different hosts. Depending on 
     the replication level set for a pool, the object is stored on 2 or 3
     OSDs that are associated with the Placement Group.
    </para>
    <para>
     The program that writes an object only interacts with one &ceph; OSD,
     the primary &ceph; OSD. The primary &ceph; OSD takes care of replication
     and confirms the completion of the write process after all other OSDs
     have stored the object.
    </para>
    <para>
     If <literal>osd.5</literal> fails, all object in <literal>PG 1</literal>
     are still available on <literal>osd.1</literal>. As soon as the cluster recognizes that an
     OSD has failed, another OSD takes over. In this example <literal>osd.4</literal>
     is used as a replacement for <literal>osd.5</literal>. The objects stored
     on <literal>osd.1</literal> are then replicated to <literal>osd.4</literal>.
    </para>
    <figure xml:id="storage.intro.structure.example.figure">
     <title>Small Scale Ceph Example</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     If a new node with new OSDs is added to the cluster, the cluster map
     is going to change. The CRUSH function then returns different locations
     for objects. Objects that receive new locations will be relocated. This
     process results in a rebalancing of the usage of all OSDs.
    </para>
   </sect3>
  </sect2>
  <sect2 xml:id="storage.moreinfo">
   <title>Additional Information</title>
   <itemizedlist>
    <listitem>
     <para>
      &ceph; as a community project has its own extensive online documentation.
      For topics not found in this manual refer to
      <link xlink:href="http://ceph.com/docs/master/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The original publication <emphasis>CRUSH: Controlled, Scalable, Decentralized
      Placement of Replicated Data</emphasis> by <emphasis>SA Weil, SA Brandt, EL Miller, C Maltzahn</emphasis>
      provides helpful insight into the inner workings of &ceph;. Especially
      when deploying large scale clusters it is a recommended reading. The publication
      can be found at <link xlink:href="www.ssrc.ucsc.edu/papers/weil-sc06.pd"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 
 
 <sect1 xml:id="storage.management">
  <title>Managing the Cluster</title>
  <remark role="fixme">Comparison of Crowbar vs Salt</remark>
  <sect2 xml:id="storage.management.salt">
   <title>&deepsea; and &salt;</title>
   <para>
    <emphasis>&salt;</emphasis> is an Open Source tool for deploying and managing a large amounts
    of computers. From a central machine &salt; can install and configure
    software on its managed minions. <emphasis>&deepsea;</emphasis> is a set of &salt; configuration
    files for deploying, managing and automating a &ceph; cluster.
   </para>
   <para>
    &salt; provides great flexibility for configuring the &ceph; cluster
    with all aspects. However, most parts of the installation have to be
    executed with a command line interface.
   </para>
  </sect2>
  <sect2 xml:id="storage.management.crowbar">
   <title>Crowbar</title>
   <para>
    <emphasis>Crowbar</emphasis> is a web &ceph; management interface. First a Crowbar server
    has to be set up. It then provides the web interface that allows easy
    configuration and deployment of all cluster nodes. The nodes can boot
    over PXE and are then automatically discovered by the crowbar server.
   </para>
   <para>
    While Crowbar is easier to use than &salt;, it is less flexible in terms
    of configuration options.
   </para>
  </sect2>
  <sect2 xml:id="storage.management.ceph-deploy">
   <title>ceph-deploy</title>
   <para>
    <emphasis>ceph-deploy</emphasis> is a tool for small scale deployments of 
    &ceph;. Beginning with &productname; 5 ceph-deploy is no longer supported.
   </para>
  </sect2>
 </sect1>

</chapter>
