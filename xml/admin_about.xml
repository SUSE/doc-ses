<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.about">
 <title>About &productname;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="storage.intro">
  <title>Introduction</title>

  <para>
   &storage; is a distributed storage designed for scalability, reliability and
   performance based on the &ceph; technology. As opposed to conventional
   systems which have allocation tables to store and fetch data, &ceph; uses a
   pseudo-random data distribution function to store data, which reduces the
   number of look-ups required in storage. Data is stored on intelligent object
   storage devices (OSDs) by using daemons, which automates data management
   tasks such as data distribution, data replication, failure detection and
   recovery. &ceph; is both self-healing and self-managing which results in
   reduction of administrative and budget overhead.
  </para>

  <para>
   The &ceph; storage cluster uses two mandatory types of nodes&mdash;monitors
   and OSD daemons:
  </para>

  <variablelist>
   <varlistentry>
    <term>Monitor</term>
    <listitem>
     <para>
      Monitoring nodes maintain information about cluster health state, a map
      of other monitoring nodes and a CRUSH map.
     </para>
     <para>
      Monitor nodes also keep history of changes performed to the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD Daemon</term>
    <listitem>
     <para>
      An OSD daemon stores data and manages the data replication and
      rebalancing processes. Each OSD daemon handles one or more OSDs, which
      can be physical disks/partitions or logical volumes.
     </para>
     <para>
      OSD daemons also communicate with monitor nodes and provide them with the
      state of the other OSD daemons.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The &ceph; storage cluster can use the following optional node types:
  </para>

  <variablelist>
   <varlistentry>
    <term>Metadata Server (MDS)</term>
    <listitem>
     <para>
      The metadata servers store metadata for the &ceph; file system. By using
      MDS you can execute basic file system commands such as
      <command>ls</command> without overloading the cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&rgw;</term>
    <listitem>
     <para>
      &rgw; is an HTTP REST gateway for the RADOS object store. You can use
      this node type also when using the &ceph; file system.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <note>
   <title>Each Node Type on a Separate Server</title>
   <para>
    We strongly recommend to install only one node type on a single server.
   </para>
  </note>

  <para>
   The &ceph; environment has the following features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Controlled, Scalable, Decentralized Placement of replicated Data using CRUSH</term>
    <listitem>
     <para>
      The &ceph; system uses a unique map called CRUSH (Controlled Replication
      Under Scalable Hashing) to assign data to OSDs in an efficient manner.
      Data assignment offsets are generated as opposed to being looked up in
      tables. This does away with disk look-ups which come with conventional
      allocation table based systems, reducing the communication between the
      storage and the client. The client armed with the CRUSH map and the
      metadata such as object name and byte offset knows where it can find the
      data or which OSD it needs to place the data.
     </para>
     <para>
      CRUSH maintains a hierarchy of devices and the replica placement policy.
      As new devices are added, data from existing nodes is moved to the new
      device to improve distribution with regard to workload and resilience. As
      a part of the replica placement policy, it can add weights to the devices
      so some devices are more favored as opposed to others. This could be used
      to give more weights to Solid State Devices (SSDs) and lower weights to
      conventional rotational hard disks to get overall better performance.
     </para>
     <para>
      CRUSH is designed to optimally distribute data to make use of available
      devices efficiently. CRUSH supports different ways of data distribution
      such as the following:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        n-way replication (mirroring)
       </para>
      </listitem>
      <listitem>
       <para>
        RAID parity schemes
       </para>
      </listitem>
      <listitem>
       <para>
        Erasure Coding
       </para>
      </listitem>
      <listitem>
       <para>
        Hybrid approaches such as RAID-10
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Reliable Autonomic Distributed Object Storage (&rados;)</term>
    <listitem>
     <para>
      The intelligence in the OSD Daemons allows tasks such as data replication
      and migration for self-management and self-healing automatically. By
      default, data written to &ceph; storage is replicated within the OSDs.
      The level and type of replication is configurable. In case of failures,
      the CRUSH map is updated and data is written to new (replicated) OSDs.
     </para>
     <para>
      The intelligence of OSD Daemons enables to handle data replication, data
      migration, failure detection and recovery. These tasks are automatically
      and autonomously managed. This also allows the creation of various pools
      for different sorts of I/O.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replicated Monitor Servers</term>
    <listitem>
     <para>
      The monitor servers keep track of all the devices in the system. They
      manage the CRUSH map which is used to determine where the data needs to
      be placed. In case of failures of any of the OSDs, the CRUSH map is
      re-generated and re-distributed to the rest of the system. At a given
      time, it is recommended that a system contains multiple monitor servers
      to add redundancy and improve resilience.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
   	<term>
   	Configuration and management framework for your cluster - &deepsea;
   	</term>
   	<listitem>
   		<para>
   		&deepsea; is a collection of &salt; states, runners and modules for deploying and managing &ceph;.
   		</para>
   	</listitem>
   </varlistentry>
  </variablelist>

  <para>
   Currently the &ceph; storage cluster can provide the following services:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&ceph; object storage</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>&ceph; file system</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>RADOS block device</emphasis>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage.moreinfo">
  <title>Additional Information</title>

  <para>
   &ceph; as a community project has its own extensive online documentation.
   For topics not found in this manual refer to
   <link xlink:href="http://ceph.com/docs/master/"/>.
  </para>
 </sect1>
</chapter>
