<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.about">
 <title>About &productname;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES5</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="storage.intro">
  <title>Introduction Into &ceph;</title>

  <para>
   &storage; is a distributed storage designed for scalability, reliability and
   performance based on the &ceph; technology. A &ceph; cluster can be run on
   commodity hardware and scales well up to thousands of nodes in the petabyte range.
   As opposed to conventional systems, which have allocation tables to store and
   fetch data, &ceph; uses a deterministic function to allocate storage for data.
   Circumventing a centralized infrastructure, &ceph; does not have a single
   point of failure. The &ceph; cluster automates data management tasks such as
   data distribution, data replication, failure detection and recovery. &ceph;
   is both self-healing and self-managing which results in reduction of
   administrative and budget overhead. 
  </para>
  <para>
   For managing the nodes &storage; can be deployed with either &deepsea;
   and &salt; or Crowbar.
  </para>
  <para>
   This chapter provides a high level overview of &storage; and
   briefly describes the most important components. Details can be found
   in other parts of the &storage; documentation.
  </para>
  <sect2 xml:id="storage.intro.core">
   <title>Core Components</title>
   <para>
    At the core of a &ceph; cluster is the CRUSH algorithm and RADOS software. 
    CRUSH is a function that handles the storage allocation and needs only
    a few parameters. The parameters basically are a current map of the cluster, some
    administrator-defined placement rules, and the name of the object that has 
    to be stored or retrieved. With this information all nodes in the &ceph;
    cluster are able to calculate where an object and its replicas are stored.
    This makes writing or reading data very efficient and prevents single points
    of failure. To evenly distribute data over all nodes, the distribution function
    returns pseudo-random locations.
   </para>

   <para>
    The &ceph; storage cluster uses two mandatory types of nodes&mdash;monitors
    and OSDs:
   </para>

   <variablelist>
    <varlistentry>
     <term>&ceph; Monitor</term>
     <listitem>
      <para>
       Monitoring (MON) nodes maintain information about cluster health state, a
       map of the cluster and placement rules. The placement rules are used to
       improve the resilience of the cluster, for example by distributing object
       replications to different server racks or data centers.
      </para>
      <para>
       If conflicts occur, the monitor nodes in cluster decide by majority which
       information is correct. To be able to form a majority, at least 3 monitor
       nodes are needed. Monitor nodes also keep history of changes performed
       to the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&ceph; OSD</term>
     <listitem>
      <para>
       A &ceph; OSD is a daemon handling one or more object-based storage devices.
       Object-based storage devices can be physical disks/partitions or logical
       volumes. The daemon additionally takes care of data replication and 
       rebalancing in case of added or removed nodes. 
      </para>
      <para>
       OSD daemons communicate with monitor nodes and provide them with the
       state of the other OSD daemons.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>Each Node Type on a Separate Server</title>
    <para>
     We strongly recommend to install only one node type on a single server.
    </para>
   </note>
   <para>
    The software that provides all required object store functions is called 
    <emphasis>RADOS</emphasis> (Reliable Autonomic Distributed Object Store).
   </para>
  </sect2>
  <sect2 xml:id="storage.intro.access">
   <title>Accessing Data</title>
   <para>
    &ceph; provides 4 methods of accessing data. The access methods
    cover many different use cases. The underlying software providing
    the functionality is RADOS. The 4 methods are:
   </para>
   <variablelist>
    <varlistentry>
      <term>radosgw</term>
      <listitem>
       <para>
        radosgw is a HTTP REST gateway for the RADOS object store. It enables
        direct access to objects stored in the &ceph; cluster.
       </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>RADOS Block Device</term>
      <listitem>
       <para>
        RADOS Block Devices (RBD) can be accessed as any other block device.
        These can be used for instance in combination with <emphasis>libvirt</emphasis> for
        virtualization purposes.
       </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>Ceph FS</term>
      <listitem>
       <para>
        The Ceph Filesystem (Ceph FS) is a POSIX-compliant filesystem.
       </para>
      </listitem>
    </varlistentry>
    <varlistentry>
      <term>librados</term>
      <listitem>
       <para>
        librados is a library that can be used with many programming
        languages to make a program capable of directly interacting with 
        the storage cluster.
       </para>
      </listitem>
    </varlistentry>
   </variablelist>
   <para>
    librados is used by radosgw and RBD while Ceph FS directly interfaces with
    RADOS <xref linkend="storage.intro.access.figure" />.
   </para>
   <figure xml:id="storage.intro.access.figure">
    <title>Interfaces to RADOS</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
     To use <emphasis>Ceph FS</emphasis> or <emphasis>radosgw</emphasis>,
     additional nodes are required:
   </para>
   <variablelist>
    <varlistentry>
     <term>Metadata Server (MDS)</term>
     <listitem>
      <para>
       The metadata servers store metadata for the &ceph; file system. By using
       MDS you can execute basic file system commands such as
       <command>ls</command> without overloading the cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>&rgw;</term>
     <listitem>
      <para>
       &rgw; is an HTTP REST gateway for the RADOS object store. You can use
       this node type also when using the &ceph; file system.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="storage.intro.structure">
   <title>Storage Structure</title>
   <sect3 xml:id="storage.intro.structure.pool">
    <title>Pool</title>
    <para>
     Objects that are stored in a &ceph; cluster are put into <emphasis>pools</emphasis>. 
     Pools represent logical partitions of the cluster to the outside world. 
     For each pool a set of rules can be defined, for example how many replications
     of each object exist. This standard configuration of pools is called
     <emphasis>replicated pool</emphasis>.
    </para>
    <para>
     Pools usually contain objects but can also be configured to act more like a
     RAID 5. In this configuration data is stored in chunks and, depending on 
     the configuration, a number of additional coding chunks.
     In this configuration, pools are referred to as <emphasis>erasure pools</emphasis>.
    </para>
   </sect3>
   <sect3 xml:id="storage.intro.structure.pg">
    <title>Placement Group</title>
    <para>
     <emphasis>Placement Groups</emphasis> (PG) are important for the distribution
     of data within the cluster. When creating a pool, a certain number of
     placement groups is set. The placement groups are used internally to track
     objects and are an important factor for the performance of a &ceph; cluster.
    </para>
   </sect3>
  </sect2>
  <sect2 xml:id="storage.intro.features">
   <title>&ceph; Features</title>
   <para>
    The &ceph; environment has the following features:
   </para>
   <variablelist>
    <varlistentry>
     <term>Scalability</term>
     <listitem>
      <para>
       &ceph; easily scales to thousands of nodes and can manage storage in
       the range of petabytes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Commodity Hardware</term>
     <listitem>
      <para>
       No special hardware is required to run a &ceph; cluster. However
       depending on the configuration, some considerations have to be taken
       into account when buying hardware.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Self-managing</term>
     <listitem>
      <para>
       The &ceph; cluster is self-managing. When nodes are added, removed or
       fail, the cluster ist automatically redistributing the data. It is
       also aware of overloaded OSDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>No Single Point of Failure</term>
     <listitem>
      <para>
       No node in a cluster stores important information alone. The number
       of redundancies can be set in configuration files. With &ceph; being
       highly redundant, the physical infrastructure consisting of power
       supply, network, fire protection and so on becomes the more important
       factor.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Open Source Software</term>
     <listitem>
      <para>
       &ceph; is an Open Source software solution and independent of specific
       hardware or hardware vendor.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
  <sect2 xml:id="storage.moreinfo">
   <title>Additional Information</title>
   <para>
    &ceph; as a community project has its own extensive online documentation.
    For topics not found in this manual refer to
    <link xlink:href="http://ceph.com/docs/master/"/>.
   </para>
   <para>
    The original publication <emphasis>CRUSH: Controlled, Scalable, Decentralized
    Placement of Replicated Data</emphasis> by <emphasis>SA Weil, SA Brandt, EL Miller, C Maltzahn</emphasis>
    provides helpful insight into the inner workings of &ceph;. Especially
    when deploying large scale clusters it is a recommended reading. The publication
    can be found at <link xlink:href="www.ssrc.ucsc.edu/papers/weil-sc06.pd"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.management">
  <title>Managing the Cluster</title>
  <remark role="fixme">Comparison of Crowbar vs Salt</remark>
  <sect2 xml:id="storage.management.salt">
   <title>&deepsea; and &salt;</title>
   <para>
    &salt; is an Open Source tool for deploying and managing a large amounts
    of computers. From a central machine &salt; can install and configure
    software on its managed minions. &deepsea; is a set of &salt; configuration
    files for deploying, managing and automating a &ceph; cluster.
   </para>
  </sect2>
  <sect2 xml:id="storage.management.crowbar">
   <title>Crowbar</title>
   <para>
    
   </para>
  </sect2>
  <sect2 xml:id="storage.management.ceph-deploy">
   <title>ceph-deploy</title>
   <para>
    <emphasis>ceph-deploy</emphasis> is a tool for small scale deployments of 
    &ceph;. Beginning with &productname; 5 ceph-deploy is no longer supported.
   </para>
  </sect2>
 </sect1>

</chapter>
