<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.about">
 <title>About &productname;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="storage.intro">
  <title>Introduction</title>

  <para>
   &storage; is a distributed storage designed for scalability, reliability and
   performance based on the &ceph; technology. As opposed to conventional systems which have allocation
   tables to store and fetch data, &ceph; uses a pseudo-random data
   distribution function to store data, which reduces the number of look-ups
   required in storage. Data is stored on intelligent object storage devices
   (OSDs) by using daemons, which automates data management tasks such as data distribution,
   data replication, failure detection and recovery. &ceph; is both
   self-healing and self-managing which results in reduction of
   administrative and budget overhead.
  </para>
  
 
  
  <para>
  The &ceph; storage cluster uses two mandatory types of nodes&emdash;monitors and OSD Daemons:  
  </para>
  <variablelist>
  	<varlistentry>  	
  		<term>Monitor</term>
  		<listitem>
  			<para>
  			Monitoring nodes maintan information about cluster health state, a map of other monitoring nodes and a CRUSH map.
  			</para>
  			<para>
  			Monitor nodes also keep history of changes performed to the cluster.
  			</para>
  		</listitem>
  	</varlistentry>
  	<varlistentry>
  		<term>OSD Daemon</term>
  		<listitem>
  			<para>
  			An OSD daemon stores data and manages the data replication and rebalancing processes. Each OSD daemon handles several OSD, which can be physical disks/paritions or a logical volume.   
  			</para>
  			<para>
  			OSD daemons also communicates with monitor nodes and provide them with state of other OSD daemons.
  			</para>
  		</listitem>
  	</varlistentry>
  </variablelist>
  <para>
  The &ceph; storage cluster can use the following optional node types:
  </para>
  <variablelist>
  	<varlistentry>
  		<term>Metadata Server</term>
  		<listitem>
  			<para>
  			The metadata servers store metadata for the &ceph; filesystem. By using MDS you can execute basic commands like <command>ls</command>, etc. without overloading the cluster.
  			</para>
  		</listitem>
  	</varlistentry>
  	<varlistentry>
  		<term>&rgw;</term>
  		<listitem>
  			<para>
  			&rgw; is a HTTP REST gateway for the RADOS object store. You can use this node type also when using the &ceph; filesystem.
  			</para>
  		</listitem>
  	</varlistentry>
  	
  </variablelist>
	
<note>
	<title>Each Node Type on a Separate Server</title>
	<para>
	We strongly recommend to install only one node on a single server.
	</para>
</note>
  <para>
   The &ceph; system has the following features:
  </para>

  <variablelist>
   <varlistentry>
    <term>Controlled, Scalable, Decentralized Placement of replicated Data using CRUSH</term>
    <listitem>
     <para>
      The &ceph; system uses a unique map called CRUSH (Controlled
      Replication Under Scalable Hashing) to assign data to OSDs in an
      efficient manner. Data assignment offsets are generated as opposed to
      being looked up in tables. This does away with disk look-ups which
      come with conventional allocation table based systems, reducing the
      communication between the storage and the client. The client armed
      with the CRUSH map and the metadata such as object name and byte
      offset knows where it can find the data or which OSD it needs to place
      the data.
     </para>
     <para>
      CRUSH maintains a hierarchy of devices and the replica placement
      policy. As new devices are added, data from existing nodes is moved to
      the new device to improve distribution with regard to workload and
      resilience. As a part of the replica placement policy, it can add
      weights to the devices so some devices are more favored as opposed to
      others. This could be used to give more weights to Solid State Devices
      (SSDs) and lower weights to conventional rotational hard disks to get
      overall better performance.
     </para>
     <para>
      CRUSH is designed to optimally distribute data to make use of
      available devices efficiently. CRUSH supports different ways of data
      distribution such as the following:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        n-way replication (mirroring)
       </para>
      </listitem>
      <listitem>
       <para>
        RAID parity schemes
       </para>
      </listitem>
      <listitem>
       <para>
        Erasure Coding
       </para>
      </listitem>
      <listitem>
       <para>
        Hybrid approaches such as RAID-10
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Reliable Autonomic Distributed Object Storage (&rados;)</term>
    <listitem>
     <para>
      The intelligence in the OSD Daemons allows tasks such as data replication and
      migration for self-management and self-healing automatically. By
      default, data written to &ceph; storage is replicated within the OSDs.
      The level and type of replication is configurable. In case of
      failures, the CRUSH map is updated and data is written to new
      (replicated) OSDs.
     </para>
     <para>
      The intelligence of OSD Daemons enables to handle data replication, data
      migration, failure detection and recovery. These tasks are
      automatically and autonomously managed. This also allows the creation
      of various pools for different sorts of I/O.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replicated Monitor Servers</term>
    <listitem>
     <para>
      The monitor servers keep track of all the devices in the system. They
      manage the CRUSH map which is used to determine where the data needs
      to be placed. In case of failures of any of the OSDs, the CRUSH map is
      re-generated and re-distributed to the rest of the system. At a given
      time, it is recommended that a system contains multiple monitor
      servers to add redundancy and improve resilience.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  
   <para>
  Currently the &ceph; storage cluster can provide the following services:
  </para>
  
  <itemizedlist>
  	<listitem>
  		<para>
  		<emphasis>&ceph; Object Storage</emphasis>
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis>&ceph; Filesystem</emphasis>
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis>RADOS Block Device</emphasis>
  		</para>
  	</listitem>
  </itemizedlist>
  
 </sect1>
 
 <sect1 xml:id="storage.moreinfo">
  <title>Additional Information</title>

  <para>
   &ceph; as a community project has its own extensive online documentation.
   For topics not found in this manual refer to
   <link xlink:href="http://ceph.com/docs/master/"/>.
  </para>
 </sect1>
</chapter>
