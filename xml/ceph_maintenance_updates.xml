<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
 <title>&ceph; Maintenance Updates Based on Upstream '&cephname;' Point Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Several key packages in &productname; &productnumber; are based on the
  &cephname; release series of &ceph;. When the &ceph; project
  (<link xlink:href="https://github.com/ceph/ceph"/>) publishes new point
  releases in the &cephname; series, &productname; &productnumber; is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </para>
 <para>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been&mdash;or is planned to be&mdash;included in the
  product.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.5 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Health warnings are now issued if daemons have
    recently crashed.</emphasis> &ceph; will now issue health warnings if
    daemons have recently crashed. &ceph; has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </para>
<screen>&prompt.cephuser;ceph crash ls-new</screen>
   <para>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </para>
<screen>
&prompt.cephuser;ceph crash archive <replaceable>CRASH-ID</replaceable>
&prompt.cephuser;ceph crash archive-all
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold"><option>pg_num</option> must be a power of two,
    otherwise <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph;
    will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> pg_num <replaceable>NEW-PG-NUM</replaceable>
</screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Pool size needs to be greater than 1 otherwise
    <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph; will issue a
    health warning if a &rados; poolâ€™s size is set to 1 or if the pool is
    configured with no redundancy. &ceph; will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> size <replaceable>NUM-REPLICAS</replaceable>
</screen>
   <para>
    You can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</emphasis> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_ratio</option>,
    specifies a percentage of <option>osd_heartbeat_grace</option> to determine
    the threshold. A value of zero disables the warning.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_time</option>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </para>
   <para>
    A new command <command>ceph daemon
    mgr.<replaceable>MGR-NUMBER</replaceable> dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </para>
   <para>
    A new command <command>ceph daemon osd.# dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Changes in the telemetry MGR module.</emphasis>
   </para>
   <para>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <literal>telemetry.ceph.com</literal> in
    order to build and improve device failure prediction algorithms.
   </para>
   <para>
    Telemetry reports information about &cephfs; file systems, including:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      How many MDS daemons (in total and per file system).
     </para>
    </listitem>
    <listitem>
     <para>
      Which features are (or have been) enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many data pools.
     </para>
    </listitem>
    <listitem>
     <para>
      Approximate file system age (year and the month of creation).
     </para>
    </listitem>
    <listitem>
     <para>
      How many files, bytes, and snapshots.
     </para>
    </listitem>
    <listitem>
     <para>
      How much metadata is being cached.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Other miscellaneous information:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Which &ceph; release the monitors are running.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether &rados; cache tiering is enabled (and the mode).
     </para>
    </listitem>
    <listitem>
     <para>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether a separate OSD cluster network is being used.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      Aggregate stats about the &crushmap;, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
   <para>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/telemetry/channel_device false
&prompt.cephuser;ceph telemetry on
</screen>
   <para>
    You can view exactly what information will be reported first with:
   </para>
<screen>
&prompt.cephuser;ceph telemetry show        # see everything
&prompt.cephuser;ceph telemetry show device # just the device info
&prompt.cephuser;ceph telemetry show basic  # basic cluster info
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_recovery_reservations</command></emphasis>. It reveals the
    recovery locks held (<option>in_progress</option>) and waiting in priority
    queues. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_recovery_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_scrub_reservations</command>. </emphasis> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_scrub_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports S3 Object Lock set of
    APIs.</emphasis> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports List Objects V2.</emphasis> RGW now
    supports List Objects V2 as specified at
    <link xlink:href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html"/>.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.4 Point Release</bridgehead>
 <para>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect &productname; customers
  because we did not ship a version based on 14.2.3.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.3 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    Fixed a denial of service vulnerability where an unauthenticated client of
    &cogw; could trigger a crash from an uncaught exception.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephname;-based librbd clients can now open images on Jewel clusters.
   </para>
  </listitem>
  <listitem>
   <para>
    The &ogw; <option>num_rados_handles</option> has been removed. If you were
    using a value of <option>num_rados_handles</option> greater than 1,
    multiply your current <option>objecter_inflight_ops</option> and
    <option>objecter_inflight_op_bytes</option> parameters by the old
    <option>num_rados_handles</option> to get the same throttle behavior.
   </para>
  </listitem>
  <listitem>
   <para>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </para>
  </listitem>
  <listitem>
   <para>
    <option>osd_deep_scrub_large_omap_object_key_threshold</option> has been
    lowered to detect an object with a large number of omap keys more easily.
   </para>
  </listitem>
  <listitem>
   <para>
    The &dashboard; now supports silencing &prometheus; notifications.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.2 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    The <literal>no{up,down,in,out}</literal> related commands have been
    revamped. There are now two ways to set the
    <literal>no{up,down,in,out}</literal> flags: the old command
   </para>
<screen>ceph osd [un]set <replaceable>FLAG</replaceable></screen>
   <para>
    which sets cluster-wide flags; and the new command
   </para>
<screen>ceph osd [un]set-group <replaceable>FLAGS</replaceable> <replaceable>WHO</replaceable></screen>
   <para>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>radosgw-admin</command> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of &ogw;. One subcommand lists such objects and
    the other deletes them.
   </para>
  </listitem>
  <listitem>
   <para>
    Earlier &cephname; releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new &cephname; &bluestore; OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-&cephname;) breaks the pool
    utilization statistics reported by <command>ceph df</command>. Until all
    OSDs have been reprovisioned or updated (via <command>ceph-bluestore-tool
    repair</command>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <emphasis>all</emphasis> OSDs are 14.2.2 or later, are &blockstore;, and
    have been updated via the repair function if they were created prior to
    &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    The default value for <option>mon_crush_min_required_version</option> has
    been changed from <literal>firefly</literal> to <literal>hammer</literal>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will move around by making the switch to Hammer tunables.
   </para>
   <para>
    If possible, we recommend that you set the oldest allowed client to
    <literal>hammer</literal> or later. To display what the current oldest
    allowed client is, run:
   </para>
<screen>&prompt.cephuser;ceph osd dump | grep min_compat_client</screen>
   <para>
    If the current value is older than <literal>hammer</literal>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </para>
<screen>&prompt.cephuser;ceph features</screen>
   <para>
    The newer <literal>straw2</literal> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <literal>straw2</literal> buckets to be used,
    including the <literal>crush-compat</literal> mode for the Balancer
    (<xref linkend="mgr-modules-balancer" />).
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Find detailed information about the patch at
  <link xlink:href="https://download.suse.com/Download?buildid=D38A7mekBz4~"/>
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.1 Point Release</bridgehead>
 <para>
  This was the first point release following the original &cephname; release
  (14.2.0). The original ('General Availability' or 'GA') version of
  &productname; &productnumber; was based on this point release.
 </para>
</appendix>
