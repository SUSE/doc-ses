<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
 <title>&ceph; Maintenance Updates Based on Upstream '&cephname;' Point Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Several key packages in &productname; &productnumber; are based on the
  &cephname; release series of &ceph;. When the &ceph; project
  (<link xlink:href="https://github.com/ceph/ceph"/>) publishes new point
  releases in the &cephname; series, &productname; &productnumber; is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </para>
 <para>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been&mdash;or is planned to be&mdash;included in the
  product.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.20 Point Release</bridgehead>
 <para>
  This release includes a security fix that ensures the
  <option>global_id</option> value (a numeric value that should be unique for
  every authenticated client or daemon in the cluster) is reclaimed after a
  network disconnect or ticket renewal in a secure fashion. Two new health
  alerts may appear during the upgrade indicating that there are clients or
  daemons that are not yet patched with the appropriate fix.
 </para>
 <para>
  To temporarily mute the health alerts around insecure clients for the
  duration of the upgrade, you may want to run:
 </para>
<screen>
&prompt.cephuser;ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM 1h
&prompt.cephuser;ceph health mute AUTH_INSECURE_GLOBAL_ID_RECLAIM_ALLOWED 1h
</screen>
 <para>
  When all clients are updated, enable the new secure behavior, not allowing
  old insecure clients to join the cluster:
 </para>
<screen>&prompt.cephuser;ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
 <para>
  For more details, refer ro
  <link
  xlink:href="https://docs.ceph.com/en/latest/security/CVE-2021-20288/"/>.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.18 Point Release</bridgehead>
 <para>
  This release fixes a regression introduced in 14.2.17 in which the manager
  module tries to use a couple of Python modules that do not exist in some
  environments.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    This release fixes issues loading the dashboard and volumes manager modules
    in some environments.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.17 Point Release</bridgehead>
 <para>
  This release includes the following fixes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <varname>$pid</varname> expansion in configuration paths such as
    <literal>admin_socket</literal> will now properly expand to the daemon PID
    for commands like <command>ceph-mds</command> or
    <command>ceph-osd</command>. Previously, only <command>ceph-fuse</command>
    and <command>rbd-nbd</command> expanded <varname>$pid</varname> with the
    actual daemon PID.
   </para>
  </listitem>
  <listitem>
   <para>
    RADOS: PG removal has been optimized.
   </para>
  </listitem>
  <listitem>
   <para>
    RADOS: Memory allocations are tracked in finer detail in &bluestore; and
    displayed as a part of the <command>dump_mempools</command> command.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephfs;: clients which acquire capabilities too quickly are throttled to
    prevent instability. See new config option
    <option>mds_session_cap_acquisition_throttle</option> to control this
    behavior.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.16 Point Release</bridgehead>
 <para>
  This release fixes a security flaw in &cephfs;.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-27781 : OpenStack Manila use of
    <command>ceph_volume_client.py</command> library allowed tenant access to
    any &ceph; credentials' secret.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.15 Point Release</bridgehead>
 <para>
  This release fixes a ceph-volume regression introduced in v14.2.13 and
  includes few other fixes.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    ceph-volume: Fixes <command>lvm batch –auto</command>, which breaks
    backward compatibility when using non rotational devices only (SSD and/or
    NVMe).
   </para>
  </listitem>
  <listitem>
   <para>
    &bluestore;: Fixes a bug in <literal>collection_list_legacy</literal> which
    makes PGs inconsistent during scrub when running OSDs older than 14.2.12
    with newer ones.
   </para>
  </listitem>
  <listitem>
   <para>
    MGR: progress module can now be turned on or off, using the commands
    <command>ceph progress on</command> and <command>ceph progress
    off</command>.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.14 Point Release</bridgehead>
 <para>
  This releases fixes a security flaw affecting Messenger V2 for Octopus and
  Nautilus, among other fixes across components.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE 2020-25660: Fix a regression in Messenger V2 replay attacks.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.13 Point Release</bridgehead>
 <para>
  This release fixes a regression introduced in v14.2.12, and a few ceph-volume
  amd RGW fixes.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Fixed a regression that caused breakage in clusters that referred to
    ceph-mon hosts using dns names instead of IP addresses in the
    <option>mon_host</option> parameter in <filename>ceph.conf</filename>.
   </para>
  </listitem>
  <listitem>
   <para>
    ceph-volume: the <command>lvm batch</command> subcommand received a major
    rewrite.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.12 Point Release</bridgehead>
 <para>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The <command>ceph df command</command> now lists the number of PGs in each
    pool.
   </para>
  </listitem>
  <listitem>
   <para>
    MONs now have a config option <option>mon_osd_warn_num_repaired</option>,
    10 by default. If any OSD has repaired more than this many I/O errors in
    stored data, a <literal>OSD_TOO_MANY_REPAIRS</literal> health warning is
    generated. In order to allow clearing of the warning, a new command
    <command>ceph tell osd.<replaceable>SERVICE_ID</replaceable>
    clear_shards_repaired <replaceable>COUNT</replaceable></command> has been
    added. By default, it will set the repair count to 0. If you want to be
    warned again if additional repairs are performed, you can provide a value
    to the command and specify the value of
    <option>mon_osd_warn_num_repaired</option>. This command will be replaced
    in future releases by the health mute/unmute feature.
   </para>
  </listitem>
  <listitem>
   <para>
    It is now possible to specify the initial MON to contact for &ceph; tools
    and daemons using the <option>mon_host_override config</option> option or
    <option>--mon-host-override <replaceable>IP</replaceable></option>
    command-line switch. This generally should only be used for debugging and
    only affects initial communication with &ceph;’s MON cluster.
   </para>
  </listitem>
  <listitem>
   <para>
    Fix an issue with osdmaps not being trimmed in a healthy cluster.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.11 Point Release</bridgehead>
 <para>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    RGW: The <command>radosgw-admin</command> sub-commands dealing with orphans
    – <command>radosgw-admin orphans find</command>, <command>radosgw-admin
    orphans finish</command>, <command>radosgw-admin orphans
    list-jobs</command> – have been deprecated. They have not been actively
    maintained and they store intermediate results on the cluster, which could
    fill a nearly-full cluster. They have been replaced by a tool, currently
    considered experimental, <command>rgw-orphan-list</command>.
   </para>
  </listitem>
  <listitem>
   <para>
    Now, when <option>noscrub</option> and/or <option>nodeep-scrub</option>
    flags are set globally or per pool, scheduled scrubs of the type disabled
    will be aborted. All user initiated scrubs are <emphasis>not</emphasis>
    interrupted.
   </para>
  </listitem>
  <listitem>
   <para>
    Fixed a ceph-osd crash in committed OSD maps when there is a failure to
    encode the first incremental map.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.10 Point Release</bridgehead>
 <para>
  This upstream release patched one security flaw:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s
    ExposeHeader
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In addition to security flaws, this major upstream release brought a number
  of notable changes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The pool parameter <option>target_size_ratio</option>, used by the PG
    autoscaler, has changed meaning. It is now normalized across pools, rather
    than specifying an absolute ratio. If you have set target size ratios on
    any pools, you may want to set these pools to autoscale
    <literal>warn</literal> mode to avoid data movement during the upgrade:
   </para>
<screen>ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_autoscale_mode warn</screen>
  </listitem>
  <listitem>
   <para>
    The behaviour of the <option>-o</option> argument to the RADOS tool has
    been reverted to its original behaviour of indicating an output file. This
    reverts it to a more consistent behaviour when compared to other tools.
    Specifying object size is now accomplished by using an upper case O
    <option>-O</option>.
   </para>
  </listitem>
  <listitem>
   <para>
    The format of MDSs in <command>ceph fs dump</command> has changed.
   </para>
  </listitem>
  <listitem>
   <para>
    &ceph; will issue a health warning if a RADOS pool’s
    <literal>size</literal> is set to 1 or, in other words, the pool is
    configured with no redundancy. This can be fixed by setting the pool size
    to the minimum recommended value with:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>pool-name</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    The warning can be silenced with:
   </para>
<screen>&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false</screen>
  </listitem>
  <listitem>
   <para>
    RGW: bucket listing performance on sharded bucket indexes has been notably
    improved by heuristically – and significantly, in many cases – reducing
    the number of entries requested from each bucket index shard.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.9 Point Release</bridgehead>
 <para>
  This upstream release patched two security flaws:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </para>
  </listitem>
  <listitem>
   <para>
    CVE-2020-1760: Fixed XSS due to RGW GetObject header-splitting
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In SES 6, these flaws were patched in &ceph; version 14.2.5.389+gb0f23ac248.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.8 Point Release</bridgehead>
 <para>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The default value of <option>bluestore_min_alloc_size_ssd</option> has been
    changed to 4K to improve performance across all workloads.
   </para>
  </listitem>
  <listitem>
   <para>
    The following OSD memory config options related to &bluestore; cache
    autotuning can now be configured during runtime:
   </para>
<screen>
osd_memory_base (default: 768 MB)
osd_memory_cache_min (default: 128 MB)
osd_memory_expected_fragmentation (default: 0.15)
osd_memory_target (default: 4 GB)
</screen>
   <para>
    You can set the above options by running:
   </para>
<screen>&prompt.cephuser;ceph config set osd <replaceable>OPTION</replaceable> <replaceable>VALUE</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    The &mgr; now accepts <literal>profile rbd</literal> and <literal>profile
    rbd-read-only</literal> user capabilities. You can use these capabilities
    to provide users access to MGR-based RBD functionality such as <literal>rbd
    perf image iostat</literal> and <literal>rbd perf image iotop</literal>.
   </para>
  </listitem>
  <listitem>
   <para>
    The configuration value <option>osd_calc_pg_upmaps_max_stddev</option> used
    for upmap balancing has been removed. Instead, use the &mgr; balancer
    configuration option <option>upmap_max_deviation</option> which now is an
    integer number of PGs of deviation from the target PGs per OSD. You can set
    it with a following command:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/balancer/upmap_max_deviation 2</screen>
   <para>
    The default <option>upmap_max_deviation</option> is 5. There are situations
    where crush rules would not allow a pool to ever have completely balanced
    PGs. For example, if crush requires 1 replica on each of 3 racks, but there
    are fewer OSDs in 1 of the racks. In those cases, the configuration value
    can be increased.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephfs;: multiple active &mds; forward scrub is now rejected. Scrub is
    currently only permitted on a file system with a single rank. Reduce the
    ranks to one via <command>ceph fs set <replaceable>FS_NAME</replaceable>
    max_mds 1</command>.
   </para>
  </listitem>
  <listitem>
   <para>
    &ceph; will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. This can be fixed
    by adjusting the pool to an adjacent power of two:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_num <replaceable>NEW_PG_NUM</replaceable></screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</screen>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.7 Point Release</bridgehead>
 <para>
  This upstream release patched two security flaws:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-1699: a path traversal flaw in &dashboard; that could allow for
    potential information disclosure.
   </para>
  </listitem>
  <listitem>
   <para>
    CVE-2020-1700: a flaw in the RGW beast front-end that could lead to denial
    of service from an unauthenticated client.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In SES 6, these flaws were patched in &ceph; version 14.2.5.382+g8881d33957b.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.6 Point Release</bridgehead>
 <para>
  This release fixed a &mgr; bug that caused MGRs becoming unresponsive on
  larger clusters. SES users were never exposed to the bug.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.5 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Health warnings are now issued if daemons have
    recently crashed.</emphasis> &ceph; will now issue health warnings if
    daemons have recently crashed. &ceph; has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </para>
<screen>&prompt.cephuser;ceph crash ls-new</screen>
   <para>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </para>
<screen>
&prompt.cephuser;ceph crash archive <replaceable>CRASH-ID</replaceable>
&prompt.cephuser;ceph crash archive-all
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold"><option>pg_num</option> must be a power of two,
    otherwise <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph;
    will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> pg_num <replaceable>NEW-PG-NUM</replaceable>
</screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Pool size needs to be greater than 1 otherwise
    <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph; will issue a
    health warning if a &rados; pool’s size is set to 1 or if the pool is
    configured with no redundancy. &ceph; will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> size <replaceable>NUM-REPLICAS</replaceable>
</screen>
   <para>
    You can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</emphasis> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_ratio</option>,
    specifies a percentage of <option>osd_heartbeat_grace</option> to determine
    the threshold. A value of zero disables the warning.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_time</option>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </para>
   <para>
    A new command <command>ceph daemon
    mgr.<replaceable>MGR-NUMBER</replaceable> dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </para>
   <para>
    A new command <command>ceph daemon osd.# dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Changes in the telemetry MGR module.</emphasis>
   </para>
   <para>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <literal>telemetry.ceph.com</literal> in
    order to build and improve device failure prediction algorithms.
   </para>
   <para>
    Telemetry reports information about &cephfs; file systems, including:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      How many MDS daemons (in total and per file system).
     </para>
    </listitem>
    <listitem>
     <para>
      Which features are (or have been) enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many data pools.
     </para>
    </listitem>
    <listitem>
     <para>
      Approximate file system age (year and the month of creation).
     </para>
    </listitem>
    <listitem>
     <para>
      How many files, bytes, and snapshots.
     </para>
    </listitem>
    <listitem>
     <para>
      How much metadata is being cached.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Other miscellaneous information:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Which &ceph; release the monitors are running.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether &rados; cache tiering is enabled (and the mode).
     </para>
    </listitem>
    <listitem>
     <para>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether a separate OSD cluster network is being used.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      Aggregate stats about the &crushmap;, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
   <para>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/telemetry/channel_device false
&prompt.cephuser;ceph telemetry on
</screen>
   <para>
    You can view exactly what information will be reported first with:
   </para>
<screen>
&prompt.cephuser;ceph telemetry show        # see everything
&prompt.cephuser;ceph telemetry show device # just the device info
&prompt.cephuser;ceph telemetry show basic  # basic cluster info
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_recovery_reservations</command></emphasis>. It reveals the
    recovery locks held (<option>in_progress</option>) and waiting in priority
    queues. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_recovery_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_scrub_reservations</command>. </emphasis> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_scrub_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports S3 Object Lock set of
    APIs.</emphasis> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports List Objects V2.</emphasis> RGW now
    supports List Objects V2 as specified at
    <link xlink:href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html"/>.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.4 Point Release</bridgehead>
 <para>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect &productname; customers
  because we did not ship a version based on 14.2.3.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.3 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    Fixed a denial of service vulnerability where an unauthenticated client of
    &cogw; could trigger a crash from an uncaught exception.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephname;-based librbd clients can now open images on Jewel clusters.
   </para>
  </listitem>
  <listitem>
   <para>
    The &ogw; <option>num_rados_handles</option> has been removed. If you were
    using a value of <option>num_rados_handles</option> greater than 1,
    multiply your current <option>objecter_inflight_ops</option> and
    <option>objecter_inflight_op_bytes</option> parameters by the old
    <option>num_rados_handles</option> to get the same throttle behavior.
   </para>
  </listitem>
  <listitem>
   <para>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </para>
  </listitem>
  <listitem>
   <para>
    <option>osd_deep_scrub_large_omap_object_key_threshold</option> has been
    lowered to detect an object with a large number of omap keys more easily.
   </para>
  </listitem>
  <listitem>
   <para>
    The &dashboard; now supports silencing &prometheus; notifications.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.2 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    The <literal>no{up,down,in,out}</literal> related commands have been
    revamped. There are now two ways to set the
    <literal>no{up,down,in,out}</literal> flags: the old command
   </para>
<screen>ceph osd [un]set <replaceable>FLAG</replaceable></screen>
   <para>
    which sets cluster-wide flags; and the new command
   </para>
<screen>ceph osd [un]set-group <replaceable>FLAGS</replaceable> <replaceable>WHO</replaceable></screen>
   <para>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>radosgw-admin</command> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of &ogw;. Expire-stale objects are expired
    objects that should have been automatically erased but still exist and need
    to be listed and removed manually. One subcommand lists such objects and
    the other deletes them.
   </para>
  </listitem>
  <listitem>
   <para>
    Earlier &cephname; releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new &cephname; &bluestore; OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-&cephname;) breaks the pool
    utilization statistics reported by <command>ceph df</command>. Until all
    OSDs have been reprovisioned or updated (via <command>ceph-bluestore-tool
    repair</command>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <emphasis>all</emphasis> OSDs are 14.2.2 or later, are &blockstore;, and
    have been updated via the repair function if they were created prior to
    &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    The default value for <option>mon_crush_min_required_version</option> has
    been changed from <literal>firefly</literal> to <literal>hammer</literal>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </para>
   <para>
    If possible, we recommend that you set the oldest allowed client to
    <literal>hammer</literal> or later. To display what the current oldest
    allowed client is, run:
   </para>
<screen>&prompt.cephuser;ceph osd dump | grep min_compat_client</screen>
   <para>
    If the current value is older than <literal>hammer</literal>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </para>
<screen>&prompt.cephuser;ceph features</screen>
   <para>
    The newer <literal>straw2</literal> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <literal>straw2</literal> buckets to be used,
    including the <literal>crush-compat</literal> mode for the Balancer
    (<xref linkend="mgr-modules-balancer" />).
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Find detailed information about the patch at
  <link xlink:href="https://download.suse.com/Download?buildid=D38A7mekBz4~"/>
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.1 Point Release</bridgehead>
 <para>
  This was the first point release following the original &cephname; release
  (14.2.0). The original ('General Availability' or 'GA') version of
  &productname; &productnumber; was based on this point release.
 </para>
</appendix>
