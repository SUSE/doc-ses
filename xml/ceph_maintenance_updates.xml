<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
 <title>&ceph; Maintenance Updates Based on Upstream '&cephname;' Point Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Several key packages in &productname; &productnumber; are based on the
  &cephname; release series of &ceph;. When the &ceph; project
  (<link xlink:href="https://github.com/ceph/ceph"/>) publishes new point
  releases in the &cephname; series, &productname; &productnumber; is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </para>
 <para>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been&mdash;or is planned to be&mdash;included in the
  product.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.12 Point Release</bridgehead>
 <para>
   In addition to bug fixes, this major upstream release brought a number of
   notable changes:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       The <command>ceph df command</command> now lists the number of PGs in each pool.
     </para>
   </listitem>
   <listitem>
     <para>
       MONs now have a config option <option>mon_osd_warn_num_repaired</option>,
       10 by default. If any OSD has repaired more than this many I/O errors in
       stored data, a <literal>OSD_TOO_MANY_REPAIRS</literal> health warning is
       generated. In order to allow clearing of the warning, a new command
       <command>ceph tell osd.<replaceable>SERVICE_ID</replaceable> clear_shards_repaired <replaceable>COUNT</replaceable></command> has been
       added. By default, it will set the repair count to 0. If you want to
       be warned again if additional repairs are performed, you can provide a
       value to the command and specify the value of
       <option>mon_osd_warn_num_repaired</option>. This command will be replaced
       in future releases by the health mute/unmute feature.
     </para>
   </listitem>
   <listitem>
     <para>
       It is now possible to specify the initial MON to contact for &ceph;
       tools and daemons using the <option>mon_host_override config</option> option
       or <option>--mon-host-override <replaceable>IP</replaceable></option>
       command-line switch. This generally should only be used for debugging
       and only affects initial communication with &ceph;’s MON cluster.
     </para>
   </listitem>
   <listitem>
     <para>
       Fix an issue with osdmaps not being trimmed in a healthy cluster.
     </para>
   </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.11 Point Release</bridgehead>
 <para>
   In addition to bug fixes, this major upstream release brought a number of
   notable changes:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       RGW: The <command>radosgw-admin</command> sub-commands dealing with
       orphans – <command>radosgw-admin orphans find</command>, <command>radosgw-admin orphans finish</command>,
       <command>radosgw-admin orphans list-jobs</command> – have been deprecated.
       They have not been actively maintained and they store intermediate
       results on the cluster, which could fill a nearly-full cluster. They have
       been replaced by a tool, currently considered experimental,
       <command>rgw-orphan-list</command>.
     </para>
   </listitem>
   <listitem>
     <para>
       Now, when <option>noscrub</option> and/or <option>nodeep-scrub</option> flags
       are set globally or per pool, scheduled scrubs of the type disabled will
       be aborted. All user initiated scrubs are <emphasis>not</emphasis> interrupted.
     </para>
   </listitem>
   <listitem>
     <para>
       Fixed a ceph-osd crash in committed OSD maps when there is a failure
       to encode the first incremental map.
     </para>
   </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.10 Point Release</bridgehead>
 <para>
  This upstream release patched one security flaw:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-10753: rgw: sanitize newlines in s3 CORSConfiguration’s ExposeHeader
   </para>
  </listitem>
</itemizedlist>
 <para>
   In addition to security flaws, this major upstream release brought a number of
   notable changes:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       RGW: Bucket notifications now support Kafka endpoints. This requires
       librdkafka of version 0.9.2 and up. Note that Ubuntu 16.04.6 LTS
       (Xenial Xerus) has an older version of librdkafka, and would require
       an update to the library.
     </para>
   </listitem>
   <listitem>
     <para>
       The pool parameter <option>target_size_ratio</option>, used by the PG
       autoscaler, has changed meaning. It is now normalized across pools,
       rather than specifying an absolute ratio. If you have set target size
       ratios on any pools, you may want to set these pools to autoscale <literal>warn</literal>
       mode to avoid data movement during the upgrade:
     </para>
<screen>ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_autoscale_mode warn</screen>
   </listitem>
   <listitem>
     <para>
       The behaviour of the <option>-o</option> argument to the RADOS tool has
       been reverted to its orignal behaviour of indicating an output file.
       This reverts it to a more consistent behaviour when compared to other
       tools. Specifying object size is now accomplished by using an upper
       case O <option>-O</option>.
     </para>
   </listitem>
   <listitem>
     <para>
       The format of MDSs in <command>ceph fs dump</command> has changed.
     </para>
   </listitem>
   <listitem>
     <para>
       &ceph; will issue a health warning if a RADOS pool’s <literal>size</literal> is set to 1 or,
       in other words, the pool is configured with no redundancy. This can be
       fixed by setting the pool size to the minimum recommended value with:
     </para>
 <screen>&prompt.cephuser;ceph osd pool set <replaceable>pool-name</replaceable> size <replaceable>num-replicas</replaceable></screen>
     <para>
       The warning can be silenced with:
     </para>
 <screen>&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false</screen>
   </listitem>
   <listitem>
     <para>
       RGW: bucket listing performance on sharded bucket indexes has been notably
       improved by heuristically – and significantly, in many cases – reducing
       the number of entries requested from each bucket index shard.
     </para>
   </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.9 Point Release</bridgehead>
 <para>
  This upstream release patched two security flaws:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-1759: Fixed nonce reuse in msgr V2 secure mode
   </para>
  </listitem>
  <listitem>
   <para>
    CVE-2020-1760: Fixed XSS due to RGW GetObject header-splitting
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In SES 6, these flaws were patched in &ceph; version 14.2.5.389+gb0f23ac248.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.8 Point Release</bridgehead>
 <para>
  In addition to bug fixes, this major upstream release brought a number of
  notable changes:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The default value of <option>bluestore_min_alloc_size_ssd</option> has been
    changed to 4K to improve performance across all workloads.
   </para>
  </listitem>
  <listitem>
   <para>
    The following OSD memory config options related to &bluestore; cache
    autotuning can now be configured during runtime:
   </para>
<screen>
osd_memory_base (default: 768 MB)
osd_memory_cache_min (default: 128 MB)
osd_memory_expected_fragmentation (default: 0.15)
osd_memory_target (default: 4 GB)
</screen>
   <para>
    You can set the above options by running:
   </para>
<screen>&prompt.cephuser;ceph config set osd <replaceable>OPTION</replaceable> <replaceable>VALUE</replaceable></screen>
  </listitem>
  <listitem>
   <para>
    The &mgr; now accepts <literal>profile rbd</literal> and <literal>profile
    rbd-read-only</literal> user capabilities. You can use these capabilities
    to provide users access to MGR-based RBD functionality such as <literal>rbd
    perf image iostat</literal> and <literal>rbd perf image iotop</literal>.
   </para>
  </listitem>
  <listitem>
   <para>
    The configuration value <option>osd_calc_pg_upmaps_max_stddev</option> used
    for upmap balancing has been removed. Instead, use the &mgr; balancer
    configuration option <option>upmap_max_deviation</option> which now is an
    integer number of PGs of deviation from the target PGs per OSD. You can set
    it with a following command:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/balancer/upmap_max_deviation 2</screen>
   <para>
    The default <option>upmap_max_deviation</option> is 5. There are situations
    where crush rules would not allow a pool to ever have completely balanced
    PGs. For example, if crush requires 1 replica on each of 3 racks, but there
    are fewer OSDs in 1 of the racks. In those cases, the configuration value
    can be increased.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephfs;: multiple active &mds; forward scrub is now rejected. Scrub is
    currently only permitted on a file system with a single rank. Reduce the
    ranks to one via <command>ceph fs set <replaceable>FS_NAME</replaceable>
    max_mds 1</command>.
   </para>
  </listitem>
  <listitem>
   <para>
    &ceph; will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. This can be fixed
    by adjusting the pool to an adjacent power of two:
   </para>
<screen>&prompt.cephuser;ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_num <replaceable>NEW_PG_NUM</replaceable></screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false</screen>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.7 Point Release</bridgehead>
 <para>
  This upstream release patched two security flaws:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    CVE-2020-1699: a path traversal flaw in &dashboard; that could allow for
    potential information disclosure.
   </para>
  </listitem>
  <listitem>
   <para>
    CVE-2020-1700: a flaw in the RGW beast front-end that could lead to denial
    of service from an unauthenticated client.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In SES 6, these flaws were patched in &ceph; version 14.2.5.382+g8881d33957b.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.6 Point Release</bridgehead>
 <para>
  This release fixed a &mgr; bug that caused MGRs becoming unresponsive on
  larger clusters. SES users were never exposed to the bug.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.5 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Health warnings are now issued if daemons have
    recently crashed.</emphasis> &ceph; will now issue health warnings if
    daemons have recently crashed. &ceph; has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </para>
<screen>&prompt.cephuser;ceph crash ls-new</screen>
   <para>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </para>
<screen>
&prompt.cephuser;ceph crash archive <replaceable>CRASH-ID</replaceable>
&prompt.cephuser;ceph crash archive-all
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold"><option>pg_num</option> must be a power of two,
    otherwise <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph;
    will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> pg_num <replaceable>NEW-PG-NUM</replaceable>
</screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Pool size needs to be greater than 1 otherwise
    <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph; will issue a
    health warning if a &rados; pool’s size is set to 1 or if the pool is
    configured with no redundancy. &ceph; will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> size <replaceable>NUM-REPLICAS</replaceable>
</screen>
   <para>
    You can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</emphasis> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_ratio</option>,
    specifies a percentage of <option>osd_heartbeat_grace</option> to determine
    the threshold. A value of zero disables the warning.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_time</option>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </para>
   <para>
    A new command <command>ceph daemon
    mgr.<replaceable>MGR-NUMBER</replaceable> dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </para>
   <para>
    A new command <command>ceph daemon osd.# dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Changes in the telemetry MGR module.</emphasis>
   </para>
   <para>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <literal>telemetry.ceph.com</literal> in
    order to build and improve device failure prediction algorithms.
   </para>
   <para>
    Telemetry reports information about &cephfs; file systems, including:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      How many MDS daemons (in total and per file system).
     </para>
    </listitem>
    <listitem>
     <para>
      Which features are (or have been) enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many data pools.
     </para>
    </listitem>
    <listitem>
     <para>
      Approximate file system age (year and the month of creation).
     </para>
    </listitem>
    <listitem>
     <para>
      How many files, bytes, and snapshots.
     </para>
    </listitem>
    <listitem>
     <para>
      How much metadata is being cached.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Other miscellaneous information:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Which &ceph; release the monitors are running.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether &rados; cache tiering is enabled (and the mode).
     </para>
    </listitem>
    <listitem>
     <para>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether a separate OSD cluster network is being used.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      Aggregate stats about the &crushmap;, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
   <para>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/telemetry/channel_device false
&prompt.cephuser;ceph telemetry on
</screen>
   <para>
    You can view exactly what information will be reported first with:
   </para>
<screen>
&prompt.cephuser;ceph telemetry show        # see everything
&prompt.cephuser;ceph telemetry show device # just the device info
&prompt.cephuser;ceph telemetry show basic  # basic cluster info
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_recovery_reservations</command></emphasis>. It reveals the
    recovery locks held (<option>in_progress</option>) and waiting in priority
    queues. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_recovery_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_scrub_reservations</command>. </emphasis> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_scrub_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports S3 Object Lock set of
    APIs.</emphasis> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports List Objects V2.</emphasis> RGW now
    supports List Objects V2 as specified at
    <link xlink:href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html"/>.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.4 Point Release</bridgehead>
 <para>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect &productname; customers
  because we did not ship a version based on 14.2.3.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.3 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    Fixed a denial of service vulnerability where an unauthenticated client of
    &cogw; could trigger a crash from an uncaught exception.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephname;-based librbd clients can now open images on Jewel clusters.
   </para>
  </listitem>
  <listitem>
   <para>
    The &ogw; <option>num_rados_handles</option> has been removed. If you were
    using a value of <option>num_rados_handles</option> greater than 1,
    multiply your current <option>objecter_inflight_ops</option> and
    <option>objecter_inflight_op_bytes</option> parameters by the old
    <option>num_rados_handles</option> to get the same throttle behavior.
   </para>
  </listitem>
  <listitem>
   <para>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </para>
  </listitem>
  <listitem>
   <para>
    <option>osd_deep_scrub_large_omap_object_key_threshold</option> has been
    lowered to detect an object with a large number of omap keys more easily.
   </para>
  </listitem>
  <listitem>
   <para>
    The &dashboard; now supports silencing &prometheus; notifications.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.2 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    The <literal>no{up,down,in,out}</literal> related commands have been
    revamped. There are now two ways to set the
    <literal>no{up,down,in,out}</literal> flags: the old command
   </para>
<screen>ceph osd [un]set <replaceable>FLAG</replaceable></screen>
   <para>
    which sets cluster-wide flags; and the new command
   </para>
<screen>ceph osd [un]set-group <replaceable>FLAGS</replaceable> <replaceable>WHO</replaceable></screen>
   <para>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>radosgw-admin</command> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of &ogw;. Expire-stale objects are expired
    objects that should have been automatically erased but still exist and need
    to be listed and removed manually. One subcommand lists such objects and
    the other deletes them.
   </para>
  </listitem>
  <listitem>
   <para>
    Earlier &cephname; releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new &cephname; &bluestore; OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-&cephname;) breaks the pool
    utilization statistics reported by <command>ceph df</command>. Until all
    OSDs have been reprovisioned or updated (via <command>ceph-bluestore-tool
    repair</command>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <emphasis>all</emphasis> OSDs are 14.2.2 or later, are &blockstore;, and
    have been updated via the repair function if they were created prior to
    &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    The default value for <option>mon_crush_min_required_version</option> has
    been changed from <literal>firefly</literal> to <literal>hammer</literal>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </para>
   <para>
    If possible, we recommend that you set the oldest allowed client to
    <literal>hammer</literal> or later. To display what the current oldest
    allowed client is, run:
   </para>
<screen>&prompt.cephuser;ceph osd dump | grep min_compat_client</screen>
   <para>
    If the current value is older than <literal>hammer</literal>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </para>
<screen>&prompt.cephuser;ceph features</screen>
   <para>
    The newer <literal>straw2</literal> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <literal>straw2</literal> buckets to be used,
    including the <literal>crush-compat</literal> mode for the Balancer
    (<xref linkend="mgr-modules-balancer" />).
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Find detailed information about the patch at
  <link xlink:href="https://download.suse.com/Download?buildid=D38A7mekBz4~"/>
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.1 Point Release</bridgehead>
 <para>
  This was the first point release following the original &cephname; release
  (14.2.0). The original ('General Availability' or 'GA') version of
  &productname; &productnumber; was based on this point release.
 </para>
</appendix>
