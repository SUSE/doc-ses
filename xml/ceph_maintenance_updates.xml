<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
 <title>&ceph; Maintenance Updates Based on Upstream '&cephname;' Point Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Several key packages in &productname; &productnumber; are based on the
  &cephname; release series of &ceph;. When the &ceph; project
  (<link xlink:href="https://github.com/ceph/ceph"/>) publishes new point
  releases in the &cephname; series, &productname; &productnumber; is updated
  to ensure that the product benefits from the latest upstream bugfixes and
  feature backports.
 </para>
 <para>
  This chapter contains summaries of notable changes contained in each upstream
  point release that has been&mdash;or is planned to be&mdash;included in the
  product.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.5 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Health warnings are now issued if daemons have
    recently crashed.</emphasis> &ceph; will now issue health warnings if
    daemons have recently crashed. &ceph; has been collecting crash reports
    since the initial Nautilus release, but the health alerts are new. To view
    new crashes (or all crashes, if you have just upgraded), run:
   </para>
<screen>&prompt.cephuser;ceph crash ls-new</screen>
   <para>
    To acknowledge a particular crash (or all crashes) and silence the health
    warning, run:
   </para>
<screen>
&prompt.cephuser;ceph crash archive <replaceable>CRASH-ID</replaceable>
&prompt.cephuser;ceph crash archive-all
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold"><option>pg_num</option> must be a power of two,
    otherwise <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph;
    will now issue a health warning if a &rados; pool has a
    <option>pg_num</option> value that is not a power of two. You can fix this
    by adjusting the pool to a nearby power of two:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> pg_num <replaceable>NEW-PG-NUM</replaceable>
</screen>
   <para>
    Alternatively, you can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_pg_num_not_power_of_two false
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Pool size needs to be greater than 1 otherwise
    <literal>HEALTH_WARN</literal> is reported.</emphasis> &ceph; will issue a
    health warning if a &rados; poolâ€™s size is set to 1 or if the pool is
    configured with no redundancy. &ceph; will stop issuing the warning if the
    pool size is set to the minimum recommended value:
   </para>
<screen>
&prompt.cephuser;ceph osd pool set <replaceable>POOL-NAME</replaceable> size <replaceable>NUM-REPLICAS</replaceable>
</screen>
   <para>
    You can silence the warning with:
   </para>
<screen>
&prompt.cephuser;ceph config set global mon_warn_on_pool_no_redundancy false
</screen>
   <warning>
    <title>Additional Replicas Require Extra Space</title>
    <para>
     Ensure there is sufficent free space <emphasis>before</emphasis> adding
     replicas. You can check using the commands: 
    </para>
<screen>&prompt.cephuser;ceph df</screen>
    <para>
     and
    </para>
<screen>&prompt.cephuser;ceph osd df</screen>
    <para>
     Each replica requires enough free space to hold another copy of the
     cluster's existing contents. For example, if there were 100&nbsp;GB of data
     and only 50&nbsp;GB of free space, then increasing the number of replicas
     from 1 to 2 will cause the cluster to run out of space. 
    </para>
   </warning>
   
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Health warning is reported if average OSD heartbeat
    ping time exceeds the threshold.</emphasis> A health warning is now
    generated if the average OSD heartbeat ping time exceeds a configurable
    threshold for any of the intervals computed. The OSD computes 1 minute, 5
    minute and 15 minute intervals with average, minimum, and maximum values.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_ratio</option>,
    specifies a percentage of <option>osd_heartbeat_grace</option> to determine
    the threshold. A value of zero disables the warning.
   </para>
   <para>
    A new configuration option, <option>mon_warn_on_slow_ping_time</option>,
    specified in milliseconds, overrides the computed value and causes a
    warning when OSD heartbeat pings take longer than the specified amount.
   </para>
   <para>
    A new command <command>ceph daemon
    mgr.<replaceable>MGR-NUMBER</replaceable> dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> lists all connections with a
    ping time longer than the specified threshold or value determined by the
    configuration options, for the average for any of the 3 intervals.
   </para>
   <para>
    A new command <command>ceph daemon osd.# dump_osd_network
    <replaceable>THRESHOLD</replaceable></command> will do the same as the
    previous one but only including heartbeats initiated by the specified OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Changes in the telemetry MGR module.</emphasis>
   </para>
   <para>
    A new 'device' channel (enabled by default) will report anonymized hard
    disk and SSD health metrics to <literal>telemetry.ceph.com</literal> in
    order to build and improve device failure prediction algorithms.
   </para>
   <para>
    Telemetry reports information about &cephfs; file systems, including:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      How many MDS daemons (in total and per file system).
     </para>
    </listitem>
    <listitem>
     <para>
      Which features are (or have been) enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many data pools.
     </para>
    </listitem>
    <listitem>
     <para>
      Approximate file system age (year and the month of creation).
     </para>
    </listitem>
    <listitem>
     <para>
      How many files, bytes, and snapshots.
     </para>
    </listitem>
    <listitem>
     <para>
      How much metadata is being cached.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Other miscellaneous information:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Which &ceph; release the monitors are running.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether msgr v1 or v2 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether IPv4 or IPv6 addresses are used for the monitors.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether &rados; cache tiering is enabled (and the mode).
     </para>
    </listitem>
    <listitem>
     <para>
      Whether pools are replicated or erasure coded, and which erasure code
      profile plug-in and parameters are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      How many hosts are in the cluster, and how many hosts have each type of
      daemon.
     </para>
    </listitem>
    <listitem>
     <para>
      Whether a separate OSD cluster network is being used.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RBD pools and images are in the cluster, and how many pools have
      RBD mirroring enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      How many RGW daemons, zones, and zonegroups are present and which RGW
      frontends are in use.
     </para>
    </listitem>
    <listitem>
     <para>
      Aggregate stats about the &crushmap;, such as which algorithms are used,
      how big buckets are, how many rules are defined, and what tunables are in
      use.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If you had telemetry enabled before 14.2.5, you will need to re-opt-in
    with:
   </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
   <para>
    If you are not comfortable sharing device metrics, you can disable that
    channel first before re-opting-in:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/telemetry/channel_device false
&prompt.cephuser;ceph telemetry on
</screen>
   <para>
    You can view exactly what information will be reported first with:
   </para>
<screen>
&prompt.cephuser;ceph telemetry show        # see everything
&prompt.cephuser;ceph telemetry show device # just the device info
&prompt.cephuser;ceph telemetry show basic  # basic cluster info
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_recovery_reservations</command></emphasis>. It reveals the
    recovery locks held (<option>in_progress</option>) and waiting in priority
    queues. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_recovery_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">New OSD daemon command
    <command>dump_scrub_reservations</command>. </emphasis> It reveals the
    scrub reservations that are held for local (primary) and remote (replica)
    PGs. Usage:
   </para>
<screen>
&prompt.cephuser;ceph daemon osd.<replaceable>ID</replaceable> dump_scrub_reservations
</screen>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports S3 Object Lock set of
    APIs.</emphasis> RGW now supports S3 Object Lock set of APIs allowing for a
    WORM model for storing objects. 6 new APIs have been added PUT/GET bucket
    object lock, PUT/GET object retention, PUT/GET object legal hold.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">RGW now supports List Objects V2.</emphasis> RGW now
    supports List Objects V2 as specified at
    <link xlink:href="https://docs.aws.amazon.com/AmazonS3/latest/API/API_ListObjectsV2.html"/>.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.4 Point Release</bridgehead>
 <para>
  This point release fixes a serious regression that found its way into the
  14.2.3 point release. This regression did not affect &productname; customers
  because we did not ship a version based on 14.2.3.
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.3 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    Fixed a denial of service vulnerability where an unauthenticated client of
    &cogw; could trigger a crash from an uncaught exception.
   </para>
  </listitem>
  <listitem>
   <para>
    &cephname;-based librbd clients can now open images on Jewel clusters.
   </para>
  </listitem>
  <listitem>
   <para>
    The &ogw; <option>num_rados_handles</option> has been removed. If you were
    using a value of <option>num_rados_handles</option> greater than 1,
    multiply your current <option>objecter_inflight_ops</option> and
    <option>objecter_inflight_op_bytes</option> parameters by the old
    <option>num_rados_handles</option> to get the same throttle behavior.
   </para>
  </listitem>
  <listitem>
   <para>
    The secure mode of Messenger v2 protocol is no longer experimental with
    this release. This mode is now the preferred mode of connection for
    monitors.
   </para>
  </listitem>
  <listitem>
   <para>
    <option>osd_deep_scrub_large_omap_object_key_threshold</option> has been
    lowered to detect an object with a large number of omap keys more easily.
   </para>
  </listitem>
  <listitem>
   <para>
    The &dashboard; now supports silencing &prometheus; notifications.
   </para>
  </listitem>
 </itemizedlist>
 <bridgehead renderas="sect1">&cephname; 14.2.2 Point Release</bridgehead>
 <itemizedlist>
  <listitem>
   <para>
    The <literal>no{up,down,in,out}</literal> related commands have been
    revamped. There are now two ways to set the
    <literal>no{up,down,in,out}</literal> flags: the old command
   </para>
<screen>ceph osd [un]set <replaceable>FLAG</replaceable></screen>
   <para>
    which sets cluster-wide flags; and the new command
   </para>
<screen>ceph osd [un]set-group <replaceable>FLAGS</replaceable> <replaceable>WHO</replaceable></screen>
   <para>
    which sets flags in batch at the granularity of any crush node or device
    class.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>radosgw-admin</command> introduces two subcommands that allow the
    managing of expire-stale objects that might be left behind after a bucket
    reshard in earlier versions of &ogw;. Expire-stale objects are expired
    objects that should have been automatically erased but
    still exist and need to be listed and removed manually. One subcommand
    lists such objects and the other deletes them.
   </para>
  </listitem>
  <listitem>
   <para>
    Earlier &cephname; releases (14.2.1 and 14.2.0) have an issue where
    deploying a single new &cephname; &bluestore; OSD on an upgraded cluster
    (i.e. one that was originally deployed pre-&cephname;) breaks the pool
    utilization statistics reported by <command>ceph df</command>. Until all
    OSDs have been reprovisioned or updated (via <command>ceph-bluestore-tool
    repair</command>), the pool statistics will show values that are lower than
    the true value. This is resolved in 14.2.2, such that the cluster only
    switches to using the more accurate per-pool stats after
    <emphasis>all</emphasis> OSDs are 14.2.2 or later, are &blockstore;, and
    have been updated via the repair function if they were created prior to
    &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    The default value for <option>mon_crush_min_required_version</option> has
    been changed from <literal>firefly</literal> to <literal>hammer</literal>,
    which means the cluster will issue a health warning if your CRUSH tunables
    are older than Hammer. There is generally a small (but non-zero) amount of
    data that will be re-balanced after making the switch to Hammer tunables.
   </para>
   <para>
    If possible, we recommend that you set the oldest allowed client to
    <literal>hammer</literal> or later. To display what the current oldest
    allowed client is, run:
   </para>
<screen>&prompt.cephuser;ceph osd dump | grep min_compat_client</screen>
   <para>
    If the current value is older than <literal>hammer</literal>, run the
    following command to determine whether it is safe to make this change by
    verifying that there are no clients older than Hammer currently connected
    to the cluster:
   </para>
<screen>&prompt.cephuser;ceph features</screen>
   <para>
    The newer <literal>straw2</literal> CRUSH bucket type was introduced in
    Hammer. If you verify that all clients are Hammer or newer, it allows new
    features only supported for <literal>straw2</literal> buckets to be used,
    including the <literal>crush-compat</literal> mode for the Balancer
    (<xref linkend="mgr-modules-balancer" />).
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Find detailed information about the patch at
  <link xlink:href="https://download.suse.com/Download?buildid=D38A7mekBz4~"/>
 </para>
 <bridgehead renderas="sect1">&cephname; 14.2.1 Point Release</bridgehead>
 <para>
  This was the first point release following the original &cephname; release
  (14.2.0). The original ('General Availability' or 'GA') version of
  &productname; &productnumber; was based on this point release.
 </para>
</appendix>
