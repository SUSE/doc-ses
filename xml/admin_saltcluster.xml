<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-salt-cluster">
 <title>Operational Tasks</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modifying Cluster Configuration</title>
  <para>
   To modify the configuration of an existing &ceph; cluster,
   follow these steps:
  </para>
  <procedure>
   <step>
    <para>
     Export the current configuration of the cluster to a file:
    </para>
<screen>&prompt.cephuser;ceph orch ls --export --format yaml > cluster.yaml</screen>
   </step>
   <step>
    <para>
     Edit the file with the configuration and update the relevant lines.  Find
     specification examples in <xref linkend="deploy-cephadm-day2"/> and <xref
     linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Apply the new configuration:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Adding Nodes</title>

  <para>
   To add a new node to a &ceph; cluster, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Install &sls; and &productname; on the new host. Refer to
     <xref linkend="deploy-os"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Configure the host as a &sminion; of an already existing &smaster;. Refer
     to <xref linkend="deploy-salt"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Add the new host to &cephsalt; and make &cephadm; aware of it, for
     example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions add ses-min5.example.com
&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Refer to <xref linkend="deploy-cephadm-configure-minions"/> for more
     information.
    </para>
   </step>
   <step>
    <para>
     Verify that the node was added to &cephsalt;:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Apply the configuration to the new cluster host:
    </para>
<screen>
&prompt.smaster;ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verify that the newly added host now belongs to the &cephadm; environment:
    </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removing Nodes</title>

  <tip>
   <title>Removing OSDs</title>
   <para>
    If the node that you are going to remove runs OSDs, remove the OSDs from it
    first and check that no OSDs are running on that node. Refer to
    <xref
     linkend="removing-node-osds"/> for more details on removing
    OSDs.
   </para>
  </tip>

  <para>
   To remove a node from a cluster, do the following:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     For all &ceph; service types except for <literal>node-exporter</literal>
     and <literal>crash</literal>, remove the node's host name from the cluster
     placement specification file (for example,
     <filename>cluster.yml</filename>). Refer to
     <xref linkend="cephadm-service-and-placement-specs"/> for more details.
     For example, if you are removing the host named
     <literal>ses-min2</literal>, remove all occurrences of <literal>-
     ses-min2</literal> from all <literal>placement:</literal> sections:
    </para>
    <para>
     Update
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     to
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
     <para>
       Apply your changes to the configuration file:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Remove the node from &cephadm;'s environment:
    </para>
<screen>&prompt.cephuser;ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     If the node is running <literal>crash.osd.1</literal> and
     <literal>crash.osd.2</literal> services, remove them by running the following
     command on the host:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Remove all the roles from the minion you want to delete:
    </para>
<screen>&prompt.cephuser;ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
&prompt.cephuser;ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
&prompt.cephuser;ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
&prompt.cephuser;ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
      If the minion you want to remove is the bootstrap minion, you also need
      to remove the bootstrap role:
    </para>
<screen>&prompt.cephuser;ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
     <para>
       You can now remove the minion from the cluster:
     </para>
<screen>&prompt.cephuser;ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSD Management</title>

  <para>
   This section describes how to add, erase, or remove OSDs in a &ceph;
   cluster.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Listing Disk Devices</title>
   <para>
    To identify used and unused disk devices on all cluster nodes, list them by
    running the following command:
   </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Erasing Disk Devices</title>
   <para>
    To re-use a disk device, you need to erase (or <emphasis>zap</emphasis>) it
    first:
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     If you previously deployed OSDs by using &drvgrps; or the
     <option>--all-available-devices</option> option while the
     <literal>unmanaged</literal> flag was not set, &cephadm; will deploy these
     OSDs automatically after you erase them.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Adding OSDs using &drvgrps; Specification</title>
   <para>
    <emphasis>&drvgrps;</emphasis> specify the layouts of OSDs in the &ceph;
    cluster. They are defined in a single YAML file. In this section, we will
    use <filename>drive_groups.yml</filename> as an example.
   </para>
   <para>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on a mixture of HDDs and SDDs)
    or share identical deployment options (for example, the same object store,
    same encryption option, stand-alone OSDs). To avoid explicitly listing
    devices, &drvgrps; use a list of filter items that correspond to a few
    selected fields of <command>ceph-volume</command>'s inventory reports.
    &cephadm; will provide code that translates these &drvgrps; into actual
    device lists for inspection by the user.
   </para>
   <para>
    To apply OSD specification to your cluster, run
   </para>
<screen>&prompt.cephuser;ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <tip>
    <para>
     You can use the <option>--dry-run</option> option together with the
     <command>ceph orch apply osd</command> command to see a preview of actions
     that will actually happen when running the command without the
     <option>--dry-run</option> option.
    </para>
   </tip>
  <sect3>
   <title>Unmanaged OSDs</title>
   <para>
    All available clean disk devices that match the &drvgrps; specification
    will be used as OSDs automatically after you add them to the cluster. This
    behavior is called a <emphasis>managed</emphasis> mode.
   </para>
   <para>
    To disable the <emphasis>managed</emphasis> mode, add the
    <literal>unmanaged: true</literal> line to the relevant specifications,
    for example:
   </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      To change already deployed OSDs from the <emphasis>managed</emphasis> to
      <emphasis>unmanaged</emphasis> mode, add the <literal>unmanaged:
       true</literal> lines where applicable during the procedure described in
      <xref linkend="modifying-cluster-configuration"/>.
       </para>
    </tip>
  </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Specification</title>
    <para>
     Following is an example &drvgrps; specification file:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
   </sect3>
   <sect3>
    <title>Matching Disk Devices</title>
    <para>
     You can describe the specification using the following filters:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       By a disk model:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       By a disk vendor:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Always enter the <replaceable>DISK_VENDOR_STRING</replaceable> in lower
        case.
       </para>
      </tip>
      <para>
       To obtain details about disk model and vendor, examine the output of the
       following command:
      </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Whether a disk is rotational or not. SSDs and NVMe drives are not
       rotational.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Deploy a node using <emphasis>all</emphasis> available drives for OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Additionally, by limiting the number of matching disks:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Filtering Devices by Size</title>
    <para>
     You can filter disk devices by their size&mdash;either by an exact size,
     or a size range. The <option>size:</option> parameter accepts arguments in
     the following form:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - Includes disks of an exact size.
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - Includes disks whose size is within the range.
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - Includes disks less than or equal to 10&nbsp;GB in size.
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - Includes disks equal to or greater than 40&nbsp;GB in size.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Matching by Disk Size</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>Quotes Required</title>
     <para>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </para>
    </note>
    <tip>
     <title>Unit Shortcuts</title>
     <para>
      Instead of Gigabytes (G), you can specify the sizes in Megabytes (M) or
      Terabytes (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Examples</title>
    <para>
     This section includes examples of different OSD setups.
    </para>
    <example>
     <title>Simple Setup</title>
     <para>
      This example describes two nodes with the same setup:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The corresponding <filename>drive_groups.yml</filename> file will be as
      follows:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </para>
     <para>
      If you know that drives with more than 2&nbsp;TB will always be the
      slower data devices, you can filter by size:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Advanced Setup</title>
     <para>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Such a setup can be defined with two layouts as follows:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Advanced Setup with Non-Uniform Nodes</title>
     <para>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </para>
     <para>
      Nodes 1-5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodes 6-10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      You can use the 'target' key in the layout to target specific nodes.
      &salt; target notation helps to keep things simple:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      followed by
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Expert Setup</title>
     <para>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Complex (and Unlikely) Setup</title>
     <para>
      In the following setup, we are trying to define:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs backed by 1 SSD(db) and 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSDs stand-alone (encrypted)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD is spare and should not be deployed
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The summary of used drives is as follows:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The &drvgrps; definition will be the following:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      One HDD will remain as the file is being parsed from top to bottom.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Removing OSDs</title>
   <para>
    Before removing an OSD node from the cluster, verify that the cluster has
    more free disk space than the OSD disk you are going to remove. Be aware
    that removing an OSD results in rebalancing of the whole cluster.
   </para>
   <procedure>
    <step>
     <para>
      Identify which OSD to remove by getting its ID:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd
NAME   HOST          STATUS
osd.0  ses-min2  running (21h)
osd.1  ses-min3  running (5d)
osd.2  ses-min4  running (5d)
[...]
</screen>
    </step>
    <step>
     <para>
      Remove one or more OSDs from the cluster:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      For example:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      You can query the state of the removal operation:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Stopping OSD Removal</title>
    <para>
     After you have scheduled an OSD removal, you can stop the removal if
     needed. The following command will reset the initial state of the OSD and
     remove it from the queue:
    </para>
<screen>&prompt.cephuser;ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Replacing OSDs</title>
   <para>
    To replace an OSD while preserving its ID, run:
   </para>
<screen>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch osd rm 4 --replace</screen>
   <para>
    Replacing an OSD is identical to removing an OSD (see
    <xref linkend="removing-node-osds"/> for more details) with the exception
    that the OSD is not permanently removed from the CRUSH hierarchy and is
    assigned a <literal>destroyed</literal> flag instead.
   </para>
   <para>
    The <literal>destroyed</literal> flag is used to determined OSD IDs that
    will be reused during the next OSD deployment. Newly added disks that match
    the &drvgrps; specification (see <xref linkend="drive-groups"/> for more
    details) will be assigned OSD IDs of their replaced counterpart.
   </para>
   <tip>
    <para>
     Appending the <option>--dry-run</option> option will not execute the
     actual replacement, but will preview the steps that would normally happen.
    </para>
   </tip>
  </sect2>
 </sect1>

 <sect1 xml:id="moving-saltmaster">
  <title>Moving the &smaster; to a New Node</title>

  <para>
   If you need to replace the &smaster; host with a new one, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Export the cluster configuration and back up the exported JSON file. Find
     more details in <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     If the old &smaster; is also the only administration node in the cluster,
     then manually move <filename>/etc/ceph/ceph.client.admin.keyring</filename>
     and <filename>/etc/ceph/ceph.conf</filename> to the new &smaster;.
    </para>
    </step>
    <step>
      <para>
        Stop and disable the &smaster; &systemd; service on the old &smaster; node:
      </para>
<screen>
&prompt.smaster;systemctl stop salt-master.service
&prompt.smaster;systemctl disable salt-master.service
</screen>
   </step>
   <step>
     <para>
       If the old &smaster; node is no longer in the cluster, also stop and
       disable the &sminion; &systemd; service:
     </para>
<screen>
&prompt.smaster;systemctl stop salt-minion.service
&prompt.smaster;systemctl disable salt-minion.service
</screen>
    <warning>
      <para>
        Do not stop or disable the <literal>salt-minion.service</literal> if
        the old &smaster; node has any &ceph; daemons (MON, MGR, OSD, MDS,
        gateway, monitoring) running on it.
      </para>
    </warning>
   </step>
   <step>
    <para>
     Install &cephos; on the new &smaster; following the procedure described in
     <xref linkend="deploy-os"/>.
    </para>
    <tip>
     <title>Transition of Salt Minions</title>
     <para>
      To ease the transition of &sminion;s to the new &smaster;, remove the
      original &smaster;'s public key from each of them:
     </para>
<screen>
&prompt.sminion;rm /etc/salt/pki/minion/minion_master.pub
&prompt.sminion;systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
     <para>
       Install the <package>salt-master</package> package and, if applicable, the
       <package>salt-minion</package> package on the new &smaster;.
     </para>
   </step>
   <step>
     <para>
      Install &cephsalt; on the new &smaster; node:
     </para>
<screen>
&prompt.smaster;zypper install ceph-salt
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt '*' saltutil.sync_all
</screen>
     <important>
       <para>
         Make sure to run all three commands before continuing. The commands are
         idempotent, so it does not matter if they get repeated.
       </para>
     </important>
   </step>
   <step>
      <para>
       Include the new &smaster; in the cluster as described in <xref linkend="deploy-cephadm-cephsalt"/>,
       <xref linkend="deploy-cephadm-configure-minions"/> and
       <xref linkend="deploy-cephadm-configure-admin"/>.
      </para>
   </step>
   <step>
     <para>
       Import the backed up cluster configuration and apply it:
     </para>
<screen>
&prompt.smaster;ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
&prompt.smaster;ceph-salt apply
</screen>
     <important>
       <para>
         Rename the &smaster;'s <literal>minion id</literal> in the exported
         <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename>
         file before importing it.
       </para>
     </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   Keep the &ceph; cluster nodes up-to-date by applying rolling updates
   regularly.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Software Repositories</title>
   <para>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <xref linkend="verify-previous-upgrade-patch-repos-repos"/> for a complete list of the
    required repositories.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Repository Staging</title>
   <para>
    If you use a staging tool&mdash;for example, &susemgr;, &smtool;, or
    &rmt;&mdash;that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for &sls; and &productname; are
    created at the same point in time.
   </para>
   <para>
    We strongly recommend to use a staging tool to apply patches which have
    <literal>frozen</literal> or <literal>staged</literal> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </para>
  </sect2>

  <sect2>
   <title>Downtime of &ceph; Services</title>
   <para>
    Depending on the configuration, cluster nodes may be rebooted during the
    update. If there is a single point of failure for services such as &ogw;,
    &sgw;, &ganesha;, or &iscsi;, the client machines may be temporarily
    disconnected from services whose nodes are being rebooted.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Running the Update</title>
   <para>
    To update the software packages on all cluster nodes to the latest version,
    run the following command:
   </para>
<screen>&prompt.smaster;ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">

  <title>Updating &ceph;</title>

  <para>
   You can instruct &cephadm; to update &ceph; from one bugfix release to
   another. The automated update of &ceph; services respects the recommended
   order&mdash;it starts with &mgr;s, &mon;s, and then continues on to other
   services such as &osd;s, &mds;s, and &ogw;s. Each daemon is restarted only
   after &ceph; indicates that the cluster will remain available.
  </para>

   <note>
     <para>
       The following update procedure uses the <command>ceph orch upgrade</command>
       command. Keep in mind that the following instructions detail how to update
       your &ceph; cluster with a product version (for example, a maintenance update),
       and <emphasis>does not</emphasis> provide instructions
       on how to upgrade your cluster from one product version to another.
     </para>
   </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Starting the Update</title>
   <para>
    Before you start the update, verify that all nodes are currently online
    and your cluster is healthy:
   </para>
<screen>&prompt.cephuser;cephadm shell -- ceph -s</screen>
   <para>
    To update to a specific &ceph; release:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Monitoring the Update</title>
   <para>
    Run the following command to determine whether an update is in progress:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade status</screen>
   <para>
    While the update is in progress, you will see a progress bar in the &ceph;
    status output:
   </para>
<screen>&prompt.cephuser;ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    You can also watch the &cephadm; log:
   </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Cancelling an Update</title>
   <para>
    You can stop the update process at any time:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Removing an Entire &ceph; Cluster</title>

  <para>
   The <command>ceph-salt purge</command> runner removes the entire &ceph;
   cluster. If there are more &ceph; clusters deployed, the one reported by
   <command>ceph -s</command> is purged. This way you can clean the cluster
   environment when testing different setups.
  </para>

  <para>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the &ceph;
   cluster by running:
  </para>

<screen>
&prompt.smaster;ceph-salt disengage-safety
&prompt.smaster;ceph-salt purge
</screen>
 </sect1>
</chapter>
