<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-salt-cluster">
 <title>&salt; Cluster Administration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  After you deploy a &ceph; cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </para>
 <sect1 xml:id="salt-adding-nodes">
  <title>Adding New Cluster Nodes</title>

  <para>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <xref linkend="ceph-install-saltstack"/>:
  </para>

  <tip>
   <title>Prevent Rebalancing</title>
   <para>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, add all OSDs you intend to add at the same time.
   </para>
   <para>
    An additional way is to set the <option>osd crush initial weight =
    0</option> option in the <filename>ceph.conf</filename> file before adding
    the OSDs:
   </para>
   <procedure>
    <step>
     <para>
      Add <option>osd crush initial weight = 0</option> to
      <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      Create the new configuration on the &smaster; node:
     </para>
<screen>
&prompt.smaster;salt '<replaceable>SALT_MASTER_NODE</replaceable>' state.apply ceph.configuration.create
</screen>
    </step>
    <step>
     <para>
      Apply the new configuration to the targeted OSD minions:
     </para>
<screen>
&prompt.smaster;salt '<replaceable>OSD_MINIONS</replaceable>' state.apply ceph.configuration
</screen>
    </step>
    <step>
     <para>
      After the new OSDs are added, adjust their weights as required with the
      <command>ceph osd crush reweight</command> command.
     </para>
    </step>
   </procedure>
  </tip>

  <procedure>
   <step>
    <para>
     Install &cephos; on the new node and configure its network setting so that
     it resolves the &smaster; host name correctly. Verify that it has a proper
     connection to both public and cluster networks, and that time
     synchronization is correctly configured. Then install the
     <systemitem>salt-minion</systemitem> package:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     If the &smaster;'s host name is different from <literal>salt</literal>,
     edit <filename>/etc/salt/minion</filename> and add the following:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the <systemitem>salt.minion</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     On the &smaster;, accept the salt key of the new node:
    </para>
<screen>&prompt.smaster;salt-key --accept <replaceable>NEW_NODE_KEY</replaceable></screen>
   </step>
   <step>
    <para>
     Verify that <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>
     targets the new &sminion; and/or set the proper &deepsea; grain. Refer to
     <xref linkend="ds-minion-targeting-name"/> or
     <xref linkend="ds-depl-stages"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information &deepsea; expects.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <important>
     <title>Possible Restart of &deepsea; stage 0</title>
     <para>
      If the &smaster; rebooted after its kernel update, you need to restart
      &deepsea; stage 0.
     </para>
    </important>
   </step>
   <step>
    <para>
     Run the discovery stage. It will write new file entries in the
     <filename>/srv/pillar/ceph/proposals</filename> directory, where you can
     edit relevant .yml files:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Optionally, change
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <xref linkend="policy-configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Run the configuration stage. It reads everything under
     <filename>/srv/pillar/ceph</filename> and updates the pillar accordingly:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillar stores data which you can access with the following command:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.items</screen>
    <tip>
     <title>Modifying OSD's Layout</title>
     <para>
      If you want to modify the default OSD's layout and change the drive
      groups configuration, follow the procedure described in
      <xref linkend="ds-drive-groups" />.
     </para>
    </tip>
   </step>
   <step>
    <para>
     The configuration and deployment stages include newly added nodes:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-adding-services">
  <title>Adding New Roles to Nodes</title>

  <para>
   You can deploy all types of supported roles with &deepsea;. See
   <xref linkend="policy-role-assignment"/> for more information on supported
   role types and examples of matching them.
  </para>

  <para>
   To add a new service to an existing node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Adapt <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> to match
     the existing host with a new role. For more details, refer to
     <xref linkend="policy-configuration"/>. For example, if you need to run an
     &ogw; on a MON node, the line is similar to:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Run stage 2 to update the pillar:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Run stage 3 to deploy core services, or stage 4 to deploy optional
     services. Running both stages does not hurt.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removing and Reinstalling Cluster Nodes</title>

  <tip>
   <title>Removing a Cluster Node Temporarily</title>
   <para>
    The &smaster; expects all minions to be present in the cluster and
    responsive. If a minion breaks and is not responsive anymore, it causes
    problems to the &salt; infrastructure, mainly to &deepsea; and &dashboard;.
   </para>
   <para>
    Before you fix the minion, delete its key from the &smaster; temporarily:
   </para>
<screen>
&prompt.smaster;salt-key -d <replaceable>MINION_HOST_NAME</replaceable>
</screen>
   <para>
    After the minion is fixed, add its key to the &smaster; again:
   </para>
<screen>
&prompt.smaster;salt-key -a <replaceable>MINION_HOST_NAME</replaceable>
</screen>
  </tip>

  <para>
   To remove a role from a cluster, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and remove the
   corresponding line(s). Then run stages 2 and 5 as described in
   <xref linkend="ceph-install-stack"/>.
  </para>

  <note>
   <title>Removing OSDs from Cluster</title>
   <para>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </para>
   <para>
    Before running stage 5 to do the actual removal, always check which OSDs
    are going to be removed by &deepsea;:
   </para>
<screen>&prompt.smaster;salt-run rescinded.ids</screen>
  </note>

  <para>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </para>

  <para>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </para>

  <note>
   <title>Preserving Partitions Created by Other Methods</title>
   <para>
    Disk drives previously configured by other methods, such as
    <command>ceph-deploy</command>, may still contain partitions. &deepsea;
    will not automatically destroy these. The administrator must reclaim these
    drives manually.
   </para>
  </note>

  <example xml:id="ex-ds-rmnode">
   <title>Removing a &sminion; from the Cluster</title>
   <para>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <filename>policy.cfg</filename> are similar to the following:
   </para>
<screen>[...]
# Hardware Profile
role-storage/cluster/data*.sls
[...]</screen>
   <para>
    Then to remove the &sminion; 'data2.ceph', change the lines to the
    following:
   </para>
<screen>
[...]
# Hardware Profile
role-storage/cluster/data[1,3-6]*.sls
[...]</screen>
   <para>
    Also keep in mind to adapt your drive_groups.yml file to match the new
    targets.
   </para>
<screen>
    [...]
    drive_group_name:
      target: 'data[1,3-6]*'
    [...]</screen>
   <para>
    Then run stage 2, check which OSDs are going to be removed, and finish by
    running stage 5:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run rescinded.ids
&prompt.smaster;salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex-ds-mignode">
   <title>Migrating Nodes</title>
   <para>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    &ogw; while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </para>
   <para>
    After running stages 0 and 1 (see <xref linkend="ds-depl-stages"/>) for the
    new hardware, you named the new gateway <literal>rgw1</literal>. If the
    node <literal>data8</literal> needs the &ogw; role removed and the storage
    role added, and the current <filename>policy.cfg</filename> looks like
    this:
   </para>
<screen># Hardware Profile
role-storage/cluster/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Then change it to:
   </para>
<screen># Hardware Profile
role-storage/cluster/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Run stages 2 to 4, check which OSDs are going to be possibly removed, and
    finish by running stage 5. Stage 3 will add <literal>data8</literal> as a
    storage node. For a moment, <literal>data8</literal> will have both roles.
    Stage 4 will add the &ogw; role to <literal>rgw1</literal> and stage 5 will
    remove the &ogw; role from <literal>data8</literal>:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
&prompt.smaster;salt-run rescinded.ids
&prompt.smaster;salt-run state.orch ceph.stage.5</screen>
  </example>
  <example xml:id="ex-failed-node">
   <title>Removal of a Failed Node</title>
   <para>
     If the &sminion; is not responding and the administrator is unable to
     resolve the issue, we recommend removing the &salt; key:
   </para>
<screen>
&prompt.smaster;salt-key -d <replaceable>MINION_ID</replaceable>
</screen>
   </example>
   <example xml:id="ex-failed-storage-node">
   <title>Removal of a Failed Storage Node</title>
     <para>When a server fails (due to network, power, or other issues), it means
       that all the OSDs are dead. Issue the following commands for <emphasis>each</emphasis>
       OSD on the failed storage node:
     </para>
<screen>
&prompt.cephuser;ceph osd purge-actual $id --yes-i-really-mean-it
&prompt.cephuser;ceph auth del osd.$id
</screen>
     <para>
       Running the <command>ceph osd purge-actual</command> command is
       equivalent to the following:
     </para>
<screen>
&prompt.cephuser;ceph destroy $id
&prompt.cephuser;ceph osd rm $id
&prompt.cephuser;ceph osd crush remove osd.$id
</screen>
 </example>
 </sect1>
 <sect1 xml:id="ds-mon">
  <title>Redeploying Monitor Nodes</title>

  <para>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </para>

  <important>
   <title>The Minimum Is Three Monitor Nodes</title>
   <para>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only two monitor nodes, you need to
    temporarily assign the monitor role to other cluster nodes before you
    redeploy the failed monitor nodes. After you redeploy the failed monitor
    nodes, you can uninstall the temporary monitor roles.
   </para>
   <para>
    For more information on adding new nodes/roles to the &ceph; cluster, see
    <xref linkend="salt-adding-nodes"/> and
    <xref linkend="salt-adding-services"/>.
   </para>
   <para>
    For more information on removing cluster nodes, refer to
    <xref linkend="salt-node-removing"/>.
   </para>
  </important>

  <para>
   There are two basic degrees of a &ceph; node failure:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &sminion; host is broken either physically or on the OS level, and
     does not respond to the <command>salt
     '<replaceable>minion_name</replaceable>' test.ping</command> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <xref linkend="ceph-install-stack"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor related services failed and refuse to recover, but the host
     responds to the <command>salt '<replaceable>minion_name</replaceable>'
     test.ping</command> call. In such case, follow these steps:
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Edit <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> on the
     &smaster;, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes. For
     example:
    </para>
<screen>
[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; stages 2 to 5 to apply the changes:
    </para>
<screen>
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.4
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-add-disk">
  <title>Adding an OSD Disk to a Node</title>

  <para>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <xref linkend="deploy-wiping-disk"/> in
   <xref linkend="ceph-install-stack"/> for more details. Adapt
   <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>
   accordingly (refer to <xref linkend="ds-drive-groups" /> for details). After
   saving the file, run &deepsea;'s stage 3:
  </para>

<screen>&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3</screen>
 </sect1>
 <sect1 xml:id="salt-removing-osd">
  <title>Removing an OSD</title>

  <para>
   You can remove a &osd; from the cluster by running the following command:
  </para>

<screen>&prompt.smaster;salt-run osd.remove <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> needs to be a number of the OSD without
   the <literal>osd.</literal> prefix. For example, from
   <literal>osd.3</literal> only use the digit <literal>3</literal>.
  </para>

  <sect2 xml:id="osd-removal-multiple">
   <title>Removing Multiple OSDs</title>
   <para>
    Use the same procedure as mentioned in <xref linkend="salt-removing-osd"/>
    but simply supply multiple OSD IDs:
   </para>
<screen>
&prompt.smaster;salt-run osd.remove 2 6 11 15
Removing osd 2 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.2 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 6 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.6 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 11 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.11 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 15 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.15 is safe to destroy
Purging from the crushmap
Zapping the device


2:
True
6:
True
11:
True
15:
True

</screen>
  </sect2>

  <sect2 xml:id="remove-all-osds-per-host">
   <title>Removing All OSDs on a Host</title>
   <para>
    To remove all OSDs on a specific host, run the following command:
   </para>
<screen>&prompt.smaster;salt-run osd.remove <replaceable>OSD_HOST_NAME</replaceable></screen>
  </sect2>

  <sect2 xml:id="osd-forced-removal">
   <title>Removing Broken OSDs Forcefully</title>
   <para>
    There are cases when removing an OSD gracefully (see
    <xref linkend="salt-removing-osd"/>) fails. This may happen, for example,
    if the OSD or its journal, WAL or DB are broken, when it suffers from
    hanging I/O operations, or when the OSD disk fails to unmount.
   </para>
<screen>&prompt.smaster;salt-run osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <tip>
    <title>Hanging Mounts</title>
    <para>
     If a partition is still mounted on the disk being removed, the command
     will exit with the 'Unmount failed - check for processes on
     <replaceable>DEVICE</replaceable>' message. You can then list all
     processes that access the file system with the <command>fuser -m
     <replaceable>DEVICE</replaceable></command>. If <command>fuser</command>
     returns nothing, try manual <command>unmount
     <replaceable>DEVICE</replaceable></command> and watch the output of
     <command>dmesg</command> or <command>journalctl</command> commands.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="validate-osd-lvm">
   <title>Validating OSD LVM Metadata</title>
   <para>
    After removing an OSD using the <command>salt-run osd.remove
    <replaceable>ID</replaceable></command> or through other ceph commands, LVM
    metadata may not be completely removed. This means that if you want to
    re-deploy a new OSD, old LVM metadata would be used.
   </para>
   <procedure>
    <step>
     <para>
      First, check if the OSD has been removed:
     </para>
<screen>&prompt.cephuser.osd;ceph-volume lvm list</screen>
     <para>
      Even if one of the OSD's has been removed successfully, it can still be
      listed. For example, if you removed <literal>osd.2</literal>, the
      following would be the output:
     </para>
<screen>
  ====== osd.2 =======

  [block] /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380

  block device /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380
  block uuid kH9aNy-vnCT-ExmQ-cAsI-H7Gw-LupE-cvSJO9
  cephx lockbox secret
  cluster fsid 6b6bbac4-eb11-45cc-b325-637e3ff9fa0c
  cluster name ceph
  crush device class None
  encrypted 0
  osd fsid aac51485-131c-442b-a243-47c9186067db
  osd id 2
  type block
  vdo 0
  devices /dev/sda
</screen>
     <para>
      In this example, you can see that <literal>osd.2</literal> is still
      located in <filename>/dev/sda</filename>.
     </para>
    </step>
    <step>
     <para>
      Validate the LVM metadata on the OSD node:
     </para>
<screen>&prompt.cephuser.osd;ceph-volume inventory</screen>
     <para>
      The output from running <command>ceph-volume inventory</command> marks
      the <filename>/dev/sda</filename> availablity as
      <literal>False</literal>. For example:
     </para>
<screen>
  Device Path Size rotates available Model name
  /dev/sda 40.00 GB True False QEMU HARDDISK
  /dev/sdb 40.00 GB True False QEMU HARDDISK
  /dev/sdc 40.00 GB True False QEMU HARDDISK
  /dev/sdd 40.00 GB True False QEMU HARDDISK
  /dev/sde 40.00 GB True False QEMU HARDDISK
  /dev/sdf 40.00 GB True False QEMU HARDDISK
  /dev/vda 25.00 GB True False
</screen>
    </step>
    <step>
     <para>
      Run the following command on the OSD node to remove the LVM metadata
      completely:
     </para>
<screen>&prompt.cephuser.osd;ceph-volume lvm zap --osd-id <replaceable>ID</replaceable> --destroy </screen>
    </step>
    <step>
     <para>
      Run the <command>inventory</command> command again to verify that the
      <filename>/dev/sda</filename> availability returns
      <literal>True</literal>. For example:
     </para>
<screen>&prompt.cephuser.osd;ceph-volume inventory
Device Path Size rotates available Model name
/dev/sda 40.00 GB True True QEMU HARDDISK
/dev/sdb 40.00 GB True False QEMU HARDDISK
/dev/sdc 40.00 GB True False QEMU HARDDISK
/dev/sdd 40.00 GB True False QEMU HARDDISK
/dev/sde 40.00 GB True False QEMU HARDDISK
/dev/sdf 40.00 GB True False QEMU HARDDISK
/dev/vda 25.00 GB True False</screen>
     <para>
      LVM metadata has been removed. You can safely run the
      <command>dd</command> command on the device.
     </para>
    </step>
    <step>
     <para>
      The OSD can now be re-deployed without needing to reboot the OSD node:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ds-osd-replace">
  <title>Replacing an OSD Disk</title>

  <para>
   There are several reasons why you may need to replace an OSD disk, for
   example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The OSD disk failed or is soon going to fail based on SMART information,
     and can no longer be used to store data safely.
    </para>
   </listitem>
   <listitem>
    <para>
     You need to upgrade the OSD disk, for example to increase its size.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The replacement procedure is the same for both cases. It is also valid for
   both default and customized &crushmap;s.
  </para>

  <procedure>
   <step>
    <para>
     Suppose that, for example, '5' is the ID of the OSD whose disk needs to be
     replaced. The following command marks it as
     <emphasis role="bold">destroyed</emphasis> in the &crushmap; but leaves
     its original ID:
    </para>
<screen>
&prompt.smaster;salt-run osd.replace 5
</screen>
    <tip>
     <title><command>osd.replace</command> and <command>osd.remove</command></title>
     <para>
      The &salt;'s <command>osd.replace</command> and
      <command>osd.remove</command> (see <xref linkend="salt-removing-osd"/>)
      commands are identical except that <command>osd.replace</command> leaves
      the OSD as 'destroyed' in the &crushmap; while
      <command>osd.remove</command> removes all traces from the &crushmap;.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Manually replace the failed/upgraded OSD drive.
    </para>
   </step>
   <step>
    <para>
     If you want to modify the default OSD's layout and change the &drvgrps;
     configuration, follow the procedure described in
     <xref linkend="ds-drive-groups" />.
    </para>
   </step>
   <step>
    <para>
     Run the deployment stage 3 to deploy the replaced OSD disk:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds-osd-recover">
  <title>Recovering a Reinstalled OSD Node</title>

  <para>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </para>

  <procedure>
   <step>
    <para>
     Reinstall the base &sle; operating system on the node where the OS broke.
     Install the <package>salt-minion</package> packages on the OSD node,
     delete the old &sminion; key on the &smaster;, and register the new
     &sminion;'s key with the &smaster;. For more information on the initial
     deployment, see <xref linkend="ceph-install-stack"/>.
    </para>
   </step>
   <step>
    <para>
     Instead of running the whole of stage 0, run the following parts:
    </para>
<screen>
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; stages 1 to 5:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
&prompt.smaster;salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; stage 0:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </para>
   </step>
   <step>
    <para>
     Get &prometheus;' node exporter installed/running:
    </para>
<screen>&prompt.smaster;salt '<replaceable>RECOVERED_MINION</replaceable>' \
 state.apply ceph.monitoring.prometheus.exporters.node_exporter</screen>
   </step>
   <step>
    <para>
     Remove unnecessary &salt; grains (best after all OSDs have been migrated to LVM):
    </para>
<screen>&prompt.smaster;salt -I roles:storage grains.delkey ceph</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Moving the &adm; to a New Server</title>

  <para>
   If you need to replace the &adm; host with a new one, you need to move the
   &smaster; and &deepsea; files. Use your favorite synchronization tool for
   transferring the files. In this procedure, we use <command>rsync</command>
   because it is a standard tool available in &cephos; software repositories.
  </para>

  <procedure>
   <step>
    <para>
     Stop <systemitem class="daemon">salt-master</systemitem> and
     <systemitem class="daemon">salt-minion</systemitem> services on the old
     &adm;:
    </para>
<screen>
&prompt.smaster;systemctl stop salt-master.service
&prompt.smaster;systemctl stop salt-minion.service
</screen>
   </step>
   <step>
    <para>
     Configure &salt; on the new &adm; so that the &smaster; and &sminion;s
     communicate. Find more details in <xref linkend="ceph-install-stack"/>.
    </para>
    <tip>
     <title>Transition of Salt Minions</title>
     <para>
      To ease the transition of &sminion;s to the new &adm;, remove the
      original &smaster;'s public key from each of them:
     </para>
<screen>
&prompt.sminion;rm /etc/salt/pki/minion/minion_master.pub
&prompt.sminion;systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Verify that the <package>deepsea</package> package is installed and
     install it if required.
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Customize the <filename>policy.cfg</filename> file by changing the
     <literal>role-master</literal> line. Find more details in
     <xref linkend="policy-configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Synchronize <filename>/srv/pillar</filename> and
     <filename>/srv/salt</filename> directories from the old &adm; to the new
     one.
    </para>
    <tip>
     <title><command>rsync</command> Dry Run and Symbolic Links</title>
     <para>
      If possible, try synchronizing the files in a dry run first to see which
      files will be transferred (<command>rsync</command>'s option
      <option>-n</option>). Also, include symbolic links
      (<command>rsync</command>'s option <option>-a</option>). For
      <command>rsync</command>, the synchronize command will look as follows:
     </para>
<screen>&prompt.smaster;rsync -avn /srv/pillar/ <replaceable>NEW-ADMIN-HOSTNAME:</replaceable>/srv/pillar</screen>
    </tip>
   </step>
   <step>
    <para>
     If you made custom changes to files outside of
     <filename>/srv/pillar</filename> and <filename>/srv/salt</filename>, for
     example in <filename>/etc/salt/master</filename> or
     <filename>/etc/salt/master.d</filename>, synchronize them as well.
    </para>
   </step>
   <step>
    <para>
     Now you can run &deepsea; stages from the new &adm;. Refer to
     <xref linkend="deepsea-description"/> for their detailed description.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-automated-installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. There is a
    real risk of putting your &smaster; into a death spiral.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom <filename>global.yml</filename> placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, stage 0 will begin. A lock prevents
   multiple instances. When all minions complete stage 0, stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, edit the file
  </para>

<screen>/etc/salt/master.d/reactor.conf</screen>

  <para>
   and replace the following line
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   with
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>

  <para>
   Verify that the line is not commented out.
  </para>
 </sect1>
 <sect1 xml:id="deepsea-rolling-updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   Keep the &ceph; cluster nodes up-to-date by applying rolling updates
   regularly.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Software Repositories</title>
   <para>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <xref linkend="upgrade-one-node-manual"/> for a complete list of the
    required repositories.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Repository Staging</title>
   <para>
    If you use a staging tool&mdash;for example, &susemgr;, &smtool;, or
    &rmt;&mdash;that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for &sls; and &productname; are
    created at the same point in time.
   </para>
   <para>
    We strongly recommend to use a staging tool to apply patches which have
    <literal>frozen</literal> or <literal>staged</literal> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-patch-or-dup">
   <title><command>zypper patch</command> or <command>zypper dup</command></title>
   <para>
    By default, cluster nodes are upgraded using the <command>zypper
    dup</command> command. If you prefer to update the system using
    <command>zypper patch</command> instead, edit
    <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
    following line:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </sect2>

  <sect2 xml:id="rolling-updates-reboots">
   <title>Cluster Node Reboots</title>
   <para>
    During the update, cluster nodes may be optionally rebooted if their kernel
    was upgraded by the update. If you want to eliminate the possibility of a
    forced reboot of potentially all nodes, either verify that the latest
    kernel is installed and running on &ceph; nodes, or disable automatic node
    reboots as described in <xref linkend="ds-disable-reboots"/>.
   </para>
  </sect2>

  <sect2>
   <title>Downtime of &ceph; Services</title>
   <para>
    Depending on the configuration, cluster nodes may be rebooted during the
    update as described in <xref linkend="rolling-updates-reboots"/>. If there
    is a single point of failure for services such as &ogw;, &sgw;, &ganesha;,
    or &iscsi;, the client machines may be temporarily disconnected from
    services whose nodes are being rebooted.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Running the Update</title>
   <para>
    To update the software packages on all cluster nodes to the latest version,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Update the <package>deepsea</package>, <package>salt-master</package>,
      and <package>salt-minion</package> packages and restart relevant services
      on the &smaster;:
     </para>
<screen>&prompt.smaster;salt -I 'roles:master' state.apply ceph.updates.master</screen>
    </step>
    <step>
     <para>
      Update and restart the <package>salt-minion</package> package on all
      cluster nodes:
     </para>
<screen>&prompt.smaster;salt -I 'cluster:ceph' state.apply ceph.updates.salt</screen>
    </step>
    <step>
     <para>
      Update all other software packages on the cluster:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      Restart &ceph; related services:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
    </step>

   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds-custom-cephconf">
  <title>Adjusting <filename>ceph.conf</filename> with Custom Settings</title>

  <para>
   If you need to put custom settings into the <filename>ceph.conf</filename>
   file, you can do so by modifying the configuration files in the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>
   directory:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Unique <filename>rgw.conf</filename></title>
   <para>
    The &ogw; offers a lot of flexibility and is unique compared to the other
    <filename>ceph.conf</filename> sections. All other &ceph; components have
    static headers such as <literal>[mon]</literal> or
    <literal>[osd]</literal>. The &ogw; has unique headers such as
    <literal>[client.rgw.rgw1]</literal>. This means that the
    <filename>rgw.conf</filename> file needs a header entry. For examples, see
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw.conf</filename>
</screen>
   <para>
    or
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw-ssl.conf</filename>
</screen>
   <para>
     See <xref linkend="ceph-rgw-https"/> for more examples.
   </para>
  </note>

  <important>
   <title>Run stage 3</title>
   <para>
    After you make custom changes to the above mentioned configuration files,
    run stages 3 and 4 to apply these changes to the cluster nodes:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
  </important>

  <para>
   These files are included from the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
   template file, and correspond to the different sections that the &ceph;
   configuration file accepts. Putting a configuration snippet in the correct
   file enables &deepsea; to place it into the correct section. You do not need
   to add any of the section headers.
  </para>

  <tip>
   <para>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <literal>[osd.1]</literal>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </para>
  </tip>

  <sect2>
   <title>Overriding the Defaults</title>
   <para>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
    file:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
   <para>
    When redefining the default values, &ceph; related tools such as
    <command>rados</command> may issue warnings that specific values from the
    <filename>ceph.conf.j2</filename> were redefined in
    <filename>global.conf</filename>. These warnings are caused by one
    parameter assigned twice in the resulting <filename>ceph.conf</filename>.
   </para>
   <para>
    As a workaround for this specific case, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Change the current directory to
      <filename>/srv/salt/ceph/configuration/create</filename>:
     </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/configuration/create
</screen>
    </step>
    <step>
     <para>
      Copy <filename>default.sls</filename> to <filename>custom.sls</filename>:
     </para>
<screen>
&prompt.smaster;cp default.sls custom.sls
</screen>
    </step>
    <step>
     <para>
      Edit <filename>custom.sls</filename> and change
      <option>ceph.conf.j2</option> to <option>custom-ceph.conf.j2</option>.
     </para>
    </step>
    <step>
     <para>
      Change current directory to
      <filename>/srv/salt/ceph/configuration/files</filename>:
     </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/configuration/files
</screen>
    </step>
    <step>
     <para>
      Copy <filename>ceph.conf.j2</filename> to
      <filename>custom-ceph.conf.j2</filename>:
     </para>
<screen>
&prompt.smaster;cp ceph.conf.j2 custom-ceph.conf.j2
</screen>
    </step>
    <step>
     <para>
      Edit <filename>custom-ceph.conf.j2</filename> and delete the following
      line:
     </para>
<screen>
{% include "ceph/configuration/files/rbd.conf" %}
</screen>
     <para>
      Edit <filename>global.yml</filename> and add the following line:
     </para>
<screen>
configuration_create: custom
</screen>
    </step>
    <step>
     <para>
      Refresh the pillar:
     </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> saltutil.pillar_refresh
</screen>
    </step>
    <step>
     <para>
      Run stage 3:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    </step>
   </procedure>
   <para>
    Now you should have only one entry for each value definition. To re-create
    the configuration, run:
   </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.configuration.create
</screen>
   <para>
    and then verify the contents of
    <filename>/srv/salt/ceph/configuration/cache/ceph.conf</filename>.
   </para>
  </sect2>

  <sect2>
   <title>Including Configuration Files</title>
   <para>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <filename>osd.conf</filename> file:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    In the previous example, the <filename>osd1.conf</filename>,
    <filename>osd2.conf</filename>, <filename>osd3.conf</filename>, and
    <filename>osd4.conf</filename> files contain the configuration options
    specific to the related OSD.
   </para>
   <tip>
    <title>Runtime Configuration</title>
    <para>
     Changes made to &ceph; configuration files take effect after the related
     &ceph; daemons restart. See <xref linkend="ceph-config-runtime"/> for more
     information on changing the &ceph; runtime configuration.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="admin-apparmor">
  <title>Enabling &aa; Profiles</title>

  <para>
   &aa; is a security solution that confines programs by a specific profile.
   For more details, refer to
   <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#part-apparmor"/>.
  </para>

  <para>
   &deepsea; provides three states for &aa; profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular &aa; state, run:
  </para>

<screen>
salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<replaceable>STATE</replaceable>
</screen>

  <para>
   To put the &aa; profiles in an 'enforce' state:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce
</screen>

  <para>
   To put the &aa; profiles in a 'complain' state:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain
</screen>

  <para>
   To disable the &aa; profiles:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable
</screen>

  <tip>
   <title>Enabling the &aa; Service</title>
   <para>
    Each of these three calls verifies if &aa; is installed and installs it if
    not, and starts and enables the related &systemd; service. &deepsea; will
    warn you if &aa; was installed and started/enabled in another way and
    therefore runs without &deepsea; profiles.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="deactivate-tuned-profiles">
  <title>Deactivating Tuned Profiles</title>

  <para>
   By default, &deepsea; deploys &ceph; clusters with tuned profiles active on
   &mon;, &mgr;, and &osd; nodes. In some cases, you may need to permanently
   deactivate tuned profiles. To do so, put the following lines in
   <filename>/srv/pillar/ceph/stack/global.yml</filename> and re-run stage 3:
  </para>

<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>

<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
 </sect1>
 <sect1 xml:id="deepsea-ceph-purge">
  <title>Removing an Entire &ceph; Cluster</title>

  <para>
   The <command>ceph.purge</command> runner removes the entire &ceph; cluster.
   This way you can clean the cluster environment when testing different
   setups. After the <command>ceph.purge</command> completes, the &salt;
   cluster is reverted back to the state at the end of &deepsea; stage 1. You
   can then either change the <filename>policy.cfg</filename> (see
   <xref linkend="policy-configuration"/>), or proceed to &deepsea; stage 2
   with the same setup.
  </para>

  <para>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the &ceph;
   cluster by running:
  </para>

<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.purge
</screen>

  <tip>
   <title>Disabling &ceph; Cluster Removal</title>
   <para>
    If you want to prevent anyone from running the
    <command>ceph.purge</command> runner, create a file named
    <filename>disabled.sls</filename> in the
    <filename>/srv/salt/ceph/purge</filename> directory and insert the
    following line in the
    <filename>/srv/pillar/ceph/stack/global.yml</filename> file:
   </para>
<screen>purge_init: disabled</screen>
  </tip>

  <important>
   <title>Rescind Custom Roles</title>
   <para>
    If you previously created custom roles for &dashboard; (refer to
    <xref linkend="dashboard-adding-roles"/> and
    <xref linkend="dashboard-permissions"/> for detailed information), you need
    to take manual steps to purge them before running the
    <command>ceph.purge</command> runner. For example, if the custom role for
    &ogw; is named 'us-east-1', then follow these steps:
   </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/rescind
&prompt.smaster;rsync -a rgw/ us-east-1
&prompt.smaster;sed -i 's!rgw!us-east-1!' us-east-1/*.sls
</screen>
  </important>
 </sect1>
</chapter>
