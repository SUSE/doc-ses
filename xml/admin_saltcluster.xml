<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-salt-cluster">
 <title>Operational Tasks</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="adding-node">
  <title>Adding Nodes</title>

  <para>
   To add a new node to a &ceph; cluster, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Install &sls; and &productname; on the new host. Refer to
     <xref linkend="deploy-os"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Configure the host as a &sminion; of an already existing &smaster;. Refer
     to <xref linkend="deploy-salt"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Add the new host to &cephsalt; and make &cephadm; aware of it, for
     example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions add ses-min5.example.com
&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Refer to <xref linkend="deploy-cephadm-configure-minions"/> for more
     information.
    </para>
   </step>
   <step>
    <para>
     Verify that the node was added to &cephsalt;:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Apply the configuration to the new cluster host:
    </para>
<screen>
&prompt.smaster;ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verify that the newly added host now belongs to the &cephadm; environment:
    </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removing Nodes</title>

  <tip>
   <title>Removing OSDs</title>
   <para>
    If the node that you are going to remove runs OSDs, remove the OSDs from it
    first and check that no OSDs are running on that node. Refer to
    <xref
     linkend="removing-node-osds"/> for more details on removing
    OSDs.
   </para>
  </tip>

  <para>
   To remove a node from a cluster, do the following:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     For all &ceph; service types except for <literal>node-exporter</literal>
     and <literal>crash</literal>, remove the node's host name from the cluster
     placement specification file (for example,
     <filename>cluster.yml</filename>). Refer to
     <xref linkend="cephadm-service-and-placement-specs"/> for more details.
     For example, if you are removing the host named
     <literal>ses-min2</literal>, remove all occurrences of <literal>-
     ses-min2</literal> from all <literal>placement:</literal> sections:
    </para>
    <para>
     Update
    </para>
<screen>
service_type: rgw
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     to
    </para>
<screen>
service_type: rgw
placement:
  hosts:
  - ses-min3
</screen>
   </step>
   <step>
    <para>
     Remove the node from &cephadm;'s environment:
    </para>
<screen>&prompt.cephuser;ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     If the node is running <literal>node-exporter</literal> and
     <literal>crash</literal> services, remove them by running the following
     command on the host:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name node-exporter
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name crash
</screen>
   </step>
   <step>
    <para>
     Remove the minion from the cluster:
    </para>
<screen>&prompt.cephuser;ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSD Management</title>

  <para>
   This section describes how to add, erase, or remove OSDs in a &ceph;
   cluster.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Listing Disk Devices</title>
   <para>
    To identify used and unused disk devices on all cluster nodes, list them by
    running the following command:
   </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Erasing Disk Devices</title>
   <para>
    To re-use a disk device, you need to erase (or <emphasis>zap</emphasis>) it
    first:
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     If you previously deployed OSDs by using &drvgrps; or the
     <option>--all-available-devices</option> option while the
     <literal>unmanaged</literal> flag was not set, &cephadm; will deploy these
     OSDs automatically after you erase them.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Adding OSDs using &drvgrps; Specification</title>
   <para>
    <emphasis>&drvgrps;</emphasis> specify the layouts of OSDs in the &ceph;
    cluster. They are defined in a single YAML file. In this section, we will
    use <filename>drive_groups.yml</filename> as an example.
   </para>
   <para>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on a mixture of HDDs and SDDs)
    or share identical deployment options (for example, the same object store,
    same encryption option, stand-alone OSDs). To avoid explicitly listing
    devices, &drvgrps; use a list of filter items that correspond to a few
    selected fields of <command>ceph-volume</command>'s inventory reports.
    &cephadm; will provide code that translates these &drvgrps; into actual
    device lists for inspection by the user.
   </para>
   <para>
    To apply OSD specification to your cluster, run
   </para>
<screen>&prompt.cephuser;ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <tip>
    <para>
     You can use the <option>--dry-run</option> option together with the
     <command>ceph orch apply osd</command> command to see a preview of actions
     that will actually happen when running the command without the
     <option>--dry-run</option> option.
    </para>
   </tip>
   <important>
    <para>
     All available clean disk devices that match the &drvgrps; specification
     will be used as OSDs automatically after you add them to the cluster. If
     you need to avoid this behavior, add the <option>--unmanaged=true</option>
     option when creating OSDs:
    </para>
<screen>&prompt.cephuser;ceph orch apply osd -i <filename>drive_groups.yml</filename> --unmanaged=true</screen>
    <para>
     Alternatively, if you have already created OSDs without the
     <option>--unmanaged=true</option>, you can disable the automatic OSD
     creation using the following command:
    </para>
<screen>&prompt.cephuser;ceph orch osd spec --service-name  osd.<replaceable>OSD_SERVICE_ID</replaceable> --unmanaged</screen>
   </important>
   <sect3 xml:id="drive-groups-specs">
    <title>Specification</title>
    <para>
     Following is an example &drvgrps; specification file:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
<!-- osds_per_device: 1   # number of osd daemons per device -->
encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
   </sect3>
   <sect3>
    <title>Matching Disk Devices</title>
    <para>
     You can describe the specification using the following filters:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       By a disk model:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       By a disk vendor:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>Lowercase Vendor String</title>
       <para>
        Always enter the <replaceable>DISK_VENDOR_STRING</replaceable> in lower
        case.
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       Whether a disk is rotational or not. SSDs and NVMe drives are not
       rotational.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Deploy a node using <emphasis>all</emphasis> available drives for OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Additionally, by limiting the number of matching disks:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Filtering Devices by Size</title>
    <para>
     You can filter disk devices by their size&mdash;either by an exact size,
     or a size range. The <option>size:</option> parameter accepts arguments in
     the following form:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - Includes disks of an exact size.
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - Includes disks whose size is within the range.
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - Includes disks less than or equal to 10&nbsp;GB in size.
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - Includes disks equal to or greater than 40&nbsp;GB in size.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Matching by Disk Size</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>Quotes Required</title>
     <para>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </para>
    </note>
    <tip>
     <title>Unit Shortcuts</title>
     <para>
      Instead of Gigabytes (G), you can specify the sizes in Megabytes (M) or
      Terabytes (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Examples</title>
    <para>
     This section includes examples of different OSD setups.
    </para>
    <example>
     <title>Simple Setup</title>
     <para>
      This example describes two nodes with the same setup:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The corresponding <filename>drive_groups.yml</filename> file will be as
      follows:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </para>
     <para>
      If you know that drives with more than 2&nbsp;TB will always be the
      slower data devices, you can filter by size:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Advanced Setup</title>
     <para>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Such a setup can be defined with two layouts as follows:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Advanced Setup with Non-Uniform Nodes</title>
     <para>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </para>
     <para>
      Nodes 1-5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodes 6-10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      You can use the 'target' key in the layout to target specific nodes.
      &salt; target notation helps to keep things simple:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      followed by
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Expert Setup</title>
     <para>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Complex (and Unlikely) Setup</title>
     <para>
      In the following setup, we are trying to define:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs backed by 1 SSD(db) and 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSDs stand-alone (encrypted)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD is spare and should not be deployed
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The summary of used drives is as follows:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The &drvgrps; definition will be the following:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encryption: True
</screen>
     <para>
      One HDD will remain as the file is being parsed from top to bottom.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Removing OSDs</title>
   <para>
    Before removing an OSD node from the cluster, verify that the cluster has
    more free disk space than the OSD disk you are going to remove. Be aware
    that removing an OSD results in rebalancing of the whole cluster.
   </para>
   <procedure>
    <step>
     <para>
      Identify which OSD to remove by getting its ID:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd
NAME   HOST          STATUS
osd.0  ses-min2  running (21h)
osd.1  ses-min3  running (5d)
osd.2  ses-min4  running (5d)
[...]
</screen>
    </step>
    <step>
     <para>
      Remove one or more OSDs from the cluster:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      For example:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      You can query the state of the removal operation:
     </para>
<screen>
&prompt.cephuser;ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Stopping OSD Removal</title>
    <para>
     After you have scheduled an OSD removal, you can stop the removal if
     needed. The following command will reset the initial state of the OSD and
     remove it from the queue:
    </para>
<screen>&prompt.cephuser;ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Replacing OSDs</title>
   <para>
    To replace an OSD while preserving its ID, run:
   </para>
<screen>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch osd rm 4 --replace</screen>
   <para>
    Replacing an OSD is identical to removing an OSD (see
    <xref linkend="removing-node-osds"/> for more details) with the exception
    that the OSD is not permanently removed from the CRUSH hierarchy and is
    assigned a <literal>destroyed</literal> flag instead.
   </para>
   <para>
    The <literal>destroyed</literal> flag is used to determined OSD IDs that
    will be reused during the next OSD deployment. Newly added disks that match
    the &drvgrps; specification (see <xref linkend="drive-groups"/> for more
    details) will be assigned OSD IDs of their replaced counterpart.
   </para>
   <tip>
    <para>
     Appending the <option>--dry-run</option> option will not execute the
     actual replacement, but will preview the steps that would normally happen.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Moving the &adm; to a New Server</title>

  <para>
   If you need to replace the &adm; host with a new one, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Export the cluster configuration and back up the exported JSON file. Find
     more details in <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     If the &adm; is a &smaster; at the same time, stop
     <systemitem class="daemon">salt-master</systemitem>
     <systemitem class="daemon">salt-minion</systemitem> services on the old
     &adm;:
    </para>
<screen>
&prompt.smaster;systemctl stop salt-master.service
&prompt.smaster;systemctl stop salt-minion.service
</screen>
   </step>
   <step>
    <para>
     Install &cephos; on the new &adm; following the procedure described in
     <xref linkend="deploy-os"/>.
    </para>
   </step>
   <step>
    <para>
     If the &adm; is a &smaster; at the same time, install the
     <package>salt-master</package> (and possibly
     <package>salt-minion</package>) package. If the &adm; is
     <emphasis>not</emphasis> a &smaster;, install the
     <package>salt-minion</package> package only. Configure &salt; so that the
     &smaster; and &sminion;s communicate. Follow the procedure described in
     <xref linkend="deploy-salt"/>.
    </para>
    <tip>
     <title>Transition of Salt Minions</title>
     <para>
      To ease the transition of &sminion;s to the new &adm;, remove the
      original &smaster;'s public key from each of them:
     </para>
<screen>
&prompt.sminion;rm /etc/salt/pki/minion/minion_master.pub
&prompt.sminion;systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     On the &smaster;, install the <package>ceph-salt</package> package if it
     is not already installed (see <xref linkend="deploy-cephadm-cephsalt"/>),
     and include the new &adm; in the cluster as described in
     <xref linkend="deploy-cephadm-configure-cephadm"/> and
     <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Import the backed up cluster configuration and apply it:
    </para>
<screen>
&prompt.smaster;ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
&prompt.smaster;ceph-salt apply
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-rolling-updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   Keep the &ceph; cluster nodes up-to-date by applying rolling updates
   regularly.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Software Repositories</title>
   <para>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <xref linkend="upgrade-one-node-manual"/> for a complete list of the
    required repositories.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Repository Staging</title>
   <para>
    If you use a staging tool&mdash;for example, &susemgr;, &smtool;, or
    &rmt;&mdash;that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for &sls; and &productname; are
    created at the same point in time.
   </para>
   <para>
    We strongly recommend to use a staging tool to apply patches which have
    <literal>frozen</literal> or <literal>staged</literal> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </para>
  </sect2>

  <sect2>
   <title>Downtime of &ceph; Services</title>
   <para>
    Depending on the configuration, cluster nodes may be rebooted during the
    update. If there is a single point of failure for services such as &ogw;,
    &sgw;, &ganesha;, or &iscsi;, the client machines may be temporarily
    disconnected from services whose nodes are being rebooted.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Running the Update</title>
   <para>
    To update the software packages on all cluster nodes to the latest version,
    run the following command:
   </para>
<screen>&prompt.smaster;ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupgrade">
<!-- https://docs.ceph.com/docs/master/cephadm/upgrade/ -->

  <title>Upgrading &ceph;</title>

  <para>
   You can instruct &cephadm; to upgrade &ceph; from one bugfix release to
   another. The automated upgrade of &ceph; services respects the recommended
   order&mdash;it starts with &mgr;s, &mon;s, and then continues on to other
   services such as &osd;s, &mds;s, and &ogw;s. Each daemon is restarted only
   after &ceph; indicates that the cluster will remain available.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade-start">
   <title>Starting the Upgrade</title>
   <para>
    Before you start the upgrade, verify that all nodes are currently online
    and your cluster is healthy:
   </para>
<screen>&prompt.cephuser;cephadm shell -- ceph -s</screen>
   <para>
    To upgrade (or downgrade) to a specific &ceph; release:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version <replaceable>VERSION</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version 15.2.1</screen>
   <para>
    If the new container is available, but the &ceph; version remains the same,
    execute the following:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade start registry.suse.de/devel/storage/7.0/containers/ses/7/ceph/ceph</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade-monitor">
   <title>Monitoring the Upgrade</title>
   <para>
    Run the following command to determine whether an upgrade is in progress:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade status</screen>
   <para>
    While the upgrade is in progress, you will see a progress bar in the &ceph;
    status output:
   </para>
<screen>&prompt.cephuser;ceph -s
[...]
  progress:
    Upgrade to docker.io/ceph/ceph:v15.2.1 (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    You can also watch the &cephadm; log:
   </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade-stop">
   <title>Cancelling an Upgrade</title>
   <para>
    You can stop the upgrade process at any time:
   </para>
<screen>&prompt.cephuser;ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Removing an Entire &ceph; Cluster</title>

  <para>
   The <command>ceph.purge</command> runner removes the entire &ceph; cluster.
   This way you can clean the cluster environment when testing different
   setups.
  </para>

  <para>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the &ceph;
   cluster by running:
  </para>

<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.purge
</screen>

  <tip>
   <title>Disabling &ceph; Cluster Removal</title>
   <para>
    If you want to prevent anyone from running the
    <command>ceph.purge</command> runner, create a file named
    <filename>disabled.sls</filename> in the
    <filename>/srv/salt/ceph/purge</filename> directory and insert the
    following line in the
    <filename>/srv/pillar/ceph/stack/global.yml</filename> file:
   </para>
<screen>purge_init: disabled</screen>
  </tip>

  <important>
   <title>Rescind Custom Roles</title>
   <para>
    If you previously created custom roles for &dashboard; (refer to
    <xref linkend="dashboard-adding-roles"/> and
    <xref linkend="dashboard-permissions"/> for detailed information), you need
    to take manual steps to purge them before running the
    <command>ceph.purge</command> runner. For example, if the custom role for
    &ogw; is named 'us-east-1', then follow these steps:
   </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/rescind
&prompt.smaster;rsync -a rgw/ us-east-1
&prompt.smaster;sed -i 's!rgw!us-east-1!' us-east-1/*.sls
</screen>
  </important>
 </sect1>
</chapter>
