<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.salt.cluster">
 <title>&salt; Cluster Administration</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  After you deploy &ceph; cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>Adding New Cluster Nodes</title>

  <para>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <xref linkend="ceph.install.saltstack"/>:
  </para>

  <procedure>
   <step>
    <para>
     Install &cephos; on the new node, configure its network setting so that it
     resolves the &smaster; host name correctly, and install the
     <systemitem>salt-minion</systemitem> package:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     If the &smaster;'s host name is different from <literal>salt</literal>,
     edit <filename>/etc/salt/minion</filename> and add the following:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the <systemitem>salt.minion</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>
     targets the new &sminion; as well. Refer to <xref
      linkend="ds.minion.targeting.name"/> of <xref linkend="ds.depl.stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information &deepsea; expects:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     Run the discovery stage. It will write new file entries in the
     <filename>/srv/pillar/ceph/proposals</filename> directory, where you can
     edit relevant .yml files:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Optionally, change
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> if the newly
     added host does not match the existing naming scheme.
    </para>
   </step>
   <step>
    <para>
     Run the configuration stage. It reads everything under
     <filename>/srv/pillar/ceph</filename> and updates the pillar accordingly:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillar stores data which you can access with the following command:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     The configuration and deployment stages include newly added nodes:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>Adding New Roles to Nodes</title>

  <para>
   You can deploy all types of supported roles with &deepsea;. See
   <xref linkend="policy.role.assignment"/> for more information on
   supported role types and examples of matching them.
  </para>

  <tip>
   <title>Required and Optional Roles and Stages</title>
   <para>
    Generally, we recommend running all deployment Stages 0 to 5 when adding a
    new role to a cluster node. To save some time, you can skip Stages 3 or 4,
    depending on which type of role you intend to deploy. Whereas OSD and MON
    roles include core services and are required by &ceph;, other roles such as
    &rgw; are optional. &deepsea; deployment stages are hierarchical: whereas
    Stage 3 deploys core services, Stage 4 deploys the optional ones.
   </para>
   <para>
    Therefore, you need to run Stage 3 when deploying core roles, such as MON
    on an existing OSD node, and you may skip Stage 4.
   </para>
   <para>
    Similarly, you can skip Stage 3 when deploying optional services such as
    &rgw;, but you need to run Stage 4 in that case.
   </para>
  </tip>

  <para>
   To add a new service to an existing node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Adapt <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> to match
     the existing host with a new role. For example, if you need to run a &rgw;
     on a MON node, the line is similar to:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Run Stage 2 to update the pillar:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Run Stage 3 to deploy core services, or Stage 4 to deploy optional
     services. Running both stages does not hurt.
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, we recommend adding all the OSDs you intend to add at the same
    time.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>Removing and Reinstalling Cluster Nodes</title>

  <para>
   To remove a role from a cluster, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and remove the
   corresponding line(s).
  </para>

  <para>
   If you are removing roles only, run Stages 2 and 5 as described in
   <xref linkend="ceph.install.stack"/>.
  </para>

  <para>
   If you are removing <emphasis>and</emphasis> adding roles at the same time,
   run Stages 2 to 5 as described in <xref linkend="ceph.install.stack"/>.
  </para>

  <note>
   <title>Removing OSDs from Your Cluster</title>
   <para>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </para>
  </note>

  <para>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </para>

  <para>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </para>

  <note>
   <title>Preserving Partitions Created by Other Methods</title>
   <para>
    Disk drives previously configured by other methods, such as
    <command>ceph-deploy</command>, may still contain partitions. &deepsea;
    will not automatically destroy these. The administrator must reclaim these
    drives.
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>Removing a &sminion; from the Cluster</title>
   <para>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <filename>policy.cfg</filename> are similar to the following:
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    Then to remove the &sminion; 'data2.ceph', change the lines to the
    following:
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    Then run Stages 2 and 5:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>Migrating Nodes</title>
   <para>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    &ogw; while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </para>
   <para>
    After running Stages 0 and 1 (see <xref linkend="ds.depl.stages"/>) for the
    new hardware, you named the new gateway <literal>rgw1</literal>. If the
    node <literal>data8</literal> needs the &ogw; role removed and the storage
    role added, and the current <filename>policy.cfg</filename> looks like
    this:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Then change it to:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Run Stages 2 to 5. Stage 3 will add <literal>data8</literal> as a storage
    node. For a moment, <literal>data8</literal> will have both roles. Stage 4
    will add the &ogw; role to <literal>rgw1</literal> and Stage 5 will remove
    the &ogw; role from <literal>data8</literal>.
   </para>
  </example>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>Add OSD to Node</title>
  <para>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <xref linkend="deploy.wiping.disk"/> in <xref
    linkend="ceph.install.stack"/> for more details.  After the disk is empty,
   add the disk to the YAML file of the node. The path to the file is
   <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>node_name</replaceable>.yml</filename>.
   After saving the file, run &deepsea; stages 2 and 3:
  </para>
  <screen>&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3</screen>
<tip>
 <title>Updated Profiles Automatically</title>
  <para>
   Instead of manually editing the YAML file, &deepsea; can create new profiles.
   To let &deepsea; create new profiles, the existing profiles have to be
   moved:
  </para>
  <screen>&prompt.smaster;<command>old</command> /srv/pillar/ceph/proposals/profile-default/
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.1
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.2
&prompt.smaster;<command>deepsea</command> stage run ceph.stage.3</screen>
</tip>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>Removing OSD</title>

  <para>
   A &osd; can be removed from the cluster by running the following command:
  </para>
<screen>&prompt.smaster;<command>salt-run</command> disengage.safety
&prompt.smaster;<command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

<para>
 <replaceable>OSD_ID</replaceable> has to be the number of the OSD without
 the term <literal>osd</literal>. For example, from <literal>osd.3</literal>
 only use the digit <literal>3</literal>.
</para>


 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. There is a
    real risk of putting your &smaster; into a death spiral.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, change the last line in the
   <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   to
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="Deepsea.restart">
  <title>Restarting &ceph; Services using &deepsea;</title>

  <para>
   After applying updates to the cluster nodes, you need to restart the
   assigned services to make use of the recently installed version.
  </para>

  <note>
   <title>Watching the Restart</title>
   <para>
    The process of restarting the cluster may take some time. You can watch the
    events by using the &salt; event bus by running:
   </para>
<screen>&prompt.smaster;salt-run state.event pretty=True</screen>
  </note>

  <sect2 xml:id="deepsea.restart.all">
   <title>Restarting All Services</title>
   <para>
    To restart <emphasis>all</emphasis> services on the cluster, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
   <para>
    The script iterates over all roles you have configured in the following
    order: MON, MGR, OSD, MDS, &rgw;, &igw;, &ganesha;. To keep the downtime
    low and to find potential issues as early as possible, nodes are restarted
    sequentially. For example, only one monitoring node is restarted at a time.
    The command also waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </para>
  </sect2>

  <sect2 xml:id="deepsea.restart.specific">
   <title>Restarting Specific Services</title>
   <para>
    To restart a specific service on the cluster, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.<replaceable>service_name</replaceable></screen>
   <para>
    For example, to restart all &rgw;s, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
   <para>
    You can use the following targets:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mon</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mgr</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.osd</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mds</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.igw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.ganesha</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   It is a good idea to apply rolling updates to your cluster's nodes
   regularly. To apply the updates, run Stage 0:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>

  <para>
   If &deepsea; detects a running &ceph; cluster, it applies updates and
   restarts nodes sequentially. &deepsea; follows &ceph;'s official
   recommendation of first updating the monitors, then the OSDs, and lastly
   additional services, such as MDS, &rgw;, &igw;, or &ganesha;. &deepsea;
   stops the update process if it detects an issue in the cluster. A trigger
   for that can be:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &ceph; reports 'HEALTH_ERR' for longer then 300 seconds.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s are queried for their assigned services to be still up and
     running after an update. The update fails if the services are down for
     more than 900 seconds.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Making these arrangements ensures that even with corrupted or failing
   updates, the &ceph; cluster is still operational.
  </para>

  <para>
   &deepsea; Stage 0 updates the system via <command>zypper update</command>
   and reboots the system if the kernel is updated. If you want to eliminate
   the possibility of a forced reboot of potentially all nodes, make sure that
   the latest kernel is installed and running before initiating &deepsea; Stage
   0.
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    If you prefer to update the system using the <command>zypper
    patch</command> command, edit
    <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
    following line:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   You can change the default reboot behavior of &deepsea; Stage 0 by adding
   the following lines to the
   <filename>/srv/pillar/ceph/stack/global.yml</filename>:
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal> sets the Stage 0 behavior of the
   &smaster;, and <literal>stage_prep_minion</literal> sets the behavior of all
   minions. All available parameters are:
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      Install updates and reboot after updating.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      Install updates without rebooting.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      Reboots without installing updates.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      Do not install updates or reboot.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.root;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.root;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>Custom <filename>ceph.conf</filename> File</title>

  <para>
   If you need to put custom settings into the <filename>ceph.conf</filename>
   file, you can do so by modifying the configuration files in the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>
   directory:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
    </listitem>
    <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>Run Stage 3</title>
   <para>
    After you make custom changes to the above mentioned configuration files,
    run Stage 3 to apply these changes to the cluster nodes:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   These files are included from the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
   template file, and correspond to the different sections that the &ceph;
   configuration file accepts. Putting a configuration snippet in the correct
   file enables &deepsea; to place it into the correct section. You do not need
   to add any of the section headers.
  </para>

  <tip>
   <para>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <literal>[osd.1]</literal>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </para>
  </tip>

  <sect2>
   <title>Overriding the Defaults</title>
   <para>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
    file:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>Including Configuration Files</title>
   <para>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <filename>osd.conf</filename> file:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    In the previous example, the <filename>osd1.conf</filename>,
    <filename>osd2.conf</filename>, <filename>osd3.conf</filename>, and
    <filename>osd4.conf</filename> files contain the configuration options
    specific to the related OSD.
   </para>
   <tip>
    <title>Runtime Configuration</title>
    <para>
     Changes made to &ceph; configuration files take effect after the related
     &ceph; daemons restart. See <xref linkend="ceph.config.runtime"/> for more
     information on changing the &ceph; runtime configuration.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.config.runtime">
  <title>Runtime &ceph; Configuration</title>

  <para>
   <xref linkend="ds.custom.cephconf"/> describes how to make changes to the
   &ceph; configuration file <filename>ceph.conf</filename>. However, the
   actual cluster behavior is determined not by the current state of the
   <filename>ceph.conf</filename> file but by the configuration of the running
   &ceph; daemons, which is stored in memory.
  </para>

  <para>
   If you want to query an individual &ceph; daemon for a particular
   configuration setting, you can use the <emphasis>admin socket</emphasis> on
   the node where the daemon is running. For example, the following command
   gets the value of the <option>osd_max_write_size</option> configuration
   parameter from daemon named <literal>osd.0</literal>:
  </para>

<screen>&prompt.root;ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
 config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   When a &ceph; daemon starts, it parses the <filename>ceph.conf</filename>
   file to obtain its initial configuration. One way to ensure that
   <filename>ceph.conf</filename> changes take effect is to reboot the entire
   cluster (all cluster nodes). This ensures that all &ceph; daemons are
   restarted.
  </para>

  <para>
   In a production cluster, it is not desirable to reboot all the nodes or even
   to restart all the daemons to read a configuration change. In such a case,
   it is possible to tell the daemons to change their in-memory settings. For
   example, the following command changes the
   <option>osd_max_write_size</option> parameter to '50' for all OSDs in the
   cluster:
  </para>

<screen>&prompt.root;ceph tell osd.* config set osd_max_write_size 50</screen>

  <para>
   This way you can change the configuration of the running daemons without
   rebooting any nodes or restarting any daemons.
  </para>
 </sect1>
</chapter>
