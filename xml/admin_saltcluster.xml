<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-salt-cluster">
 <title>Operational Tasks</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="adding-node">
  <title>Adding New Hosts</title>

  <para>
   To add a new host to a &ceph; cluster, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Install &sls; and &productname; on the new host. Refer to
     <xref linkend="deploy-os"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Configure the host as a &sminion; of an already existing &smaster;. Refer
     to <xref linkend="deploy-salt"/> for more information.
    </para>
   </step>
   <step>
    <para>
     Add the new host to &cephsalt; and make &cephadm; aware of it, for
     example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions add ses-min5.example.com
&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Refer to <xref linkend="deploy-cephadm-configure-minions"/> for more
     information.
    </para>
   </step>
   <step>
    <para>
     Verify that the node was added to &cephsalt;:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Apply the configuration to the new cluster host:
    </para>
<screen>
&prompt.smaster;ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verify that the newly added host now belongs to the &cephadm; environment:
    </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removing Hosts</title>

  <para>
   To remove a host from a cluster, do the following:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     For all &ceph; service types except for <literal>node-exporter</literal>
     and <literal>crash</literal>, remove the host's name from cluster's
     placement specification file (for example,
     <filename>cluster.yml</filename>). Refer to
     <xref linkend="cephadm-service-and-placement-specs"/> for more details.
     For example, if you are removing the host named
     <literal>ses-min2</literal>, remove all occurrences of <literal>-
     ses-min2</literal> from all <literal>placement:</literal> sections:
    </para>
    <para>
     Update
    </para>
<screen>
service_type: rgw
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     to
    </para>
<screen>
service_type: rgw
placement:
  hosts:
  - ses-min3
</screen>
   </step>
   <step>
    <para>
     Remove the host from &cephadm;'s environment:
    </para>
<screen>&prompt.cephuser;ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     If the host is running <literal>node-exporter</literal> and
     <literal>crash</literal> services, remove them by running the following
     command on the host:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name node-exporter
&prompt.sminion;cephadm rm-daemon --fsid b4b30c6e... --name crash
</screen>
   </step>
   <step>
    <para>
     Remove the minion from the cluster:
    </para>
<screen>&prompt.cephuser;ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="removing-node-osds">
  <title>Removing OSDs</title>

  <procedure>
   <step>
    <para>
     Identify which OSD to remove by getting its ID:
    </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd
NAME   HOST          STATUS
osd.0  ses-min2  running (21h)
osd.1  ses-min3  running (5d)
osd.2  ses-min4  running (5d)
[...]
</screen>
   </step>
   <step>
    <para>
     Remove one or more OSDs from the cluster:
    </para>
<screen>
&prompt.cephuser;ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
    <para>
     For example:
    </para>
<screen>
&prompt.cephuser;ceph orch osd rm 1 2
</screen>
   </step>
   <step>
    <para>
     You can query the state of the removal operation:
    </para>
<screen>
&prompt.cephuser;ceph orch osd rm status
NAME  HOST  PGS STARTED_AT
osd.1 node1 55 2020-04-22 19:28:38.785761
osd.1 node3 3 2020-04-22 19:28:34.201685
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="replacing-node-osds">
  <title>Replacing OSDs</title>

  <para>
   Replacing an OSD follows the same procedure as described in
   <xref linkend="removing-node-osds"/>. The only exception is that the OSD is
   not permanently removed from the CRUSH hierarchy, but is assigned a
   <literal>destroyed</literal> flag:
  </para>

<screen>
&prompt.cephuser;ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ... --replace
</screen>

  <sect2 xml:id="replacing-node-osds-preserve">
   <title>Preserving OSD IDs</title>
   <para>
    The <literal>destroyed</literal> flag that was set when replacing the OSD
    is used to identify OSD IDs that will be reused in the next OSD deployment.
   </para>
   <para>
    Because you use OSD specification to deploy OSDs (refer to
    <xref linkend="drive-groups"/> for more details), your newly added disks
    will be assigned with the OSD IDs of their replaced counterparts (if they
    still match the same specification).
<!-- 2020-06-22 tbazant: not settled yet...
    To verify such behavior, you can
    obtain the name of the OSD specification by running <command>ceph orch
    ls</command> and using the <emphasis>preview</emphasis> feature:
-->
   </para>
<!-- 2020-06-22 tbazant: not settled yet...
<screen>
&prompt.cephuser;ceph orch apply osd \-\-service-name <replaceable>NAME_OF_OSD_SPEC</replaceable> \-\-preview
</screen>
   <para>
    Alternatively, you can use the OSD specification file:
   </para>
<screen>
&prompt.cephuser;ceph orch apply osd -i <replaceable>OSD_SPEC_FILE</replaceable> \-\-preview
</screen>
-->
  </sect2>
 </sect1>
 <sect1 xml:id="ds-osd-recover">
  <title>Recovering a Reinstalled OSD Node</title>

  <para>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </para>

  <procedure>
   <step>
    <para>
     Reinstall the base &sle; operating system on the node where the OS broke.
     Install the <package>salt-minion</package> packages on the OSD node,
     delete the old &sminion; key on the &smaster;, and register the new
     &sminion;'s key with the &smaster;. For more information on the initial
     deployment, see DEAD LINK.
    </para>
   </step>
   <step>
    <para>
     Instead of running the whole of stage 0, run the following parts:
    </para>
<screen>
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; stages 1 to 5:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
&prompt.smaster;salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; stage 0:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </para>
   </step>
   <step>
    <para>
     Get &prometheus;' node exporter installed/running:
    </para>
<screen>&prompt.smaster;salt '<replaceable>RECOVERED_MINION</replaceable>' \
 state.apply ceph.monitoring.prometheus.exporters.node_exporter</screen>
   </step>
   <step>
    <para>
     Remove unnecessary &salt; grains (preferably after all OSDs have been migrated
     to LVM):
    </para>
<screen>&prompt.smaster;salt -I roles:storage grains.delkey ceph</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Moving the &adm; to a New Server</title>

  <para>
   If you need to replace the &adm; host with a new one, you need to move the
   &smaster; and &deepsea; files. Use your favorite synchronization tool for
   transferring the files. In this procedure, we use <command>rsync</command>
   because it is a standard tool available in &cephos; software repositories.
  </para>

  <procedure>
   <step>
    <para>
     Stop <systemitem class="daemon">salt-master</systemitem> and
     <systemitem class="daemon">salt-minion</systemitem> services on the old
     &adm;:
    </para>
<screen>
&prompt.smaster;systemctl stop salt-master.service
&prompt.smaster;systemctl stop salt-minion.service
</screen>
   </step>
   <step>
    <para>
     Configure &salt; on the new &adm; so that the &smaster; and &sminion;s
     communicate. Find more details in DEAD LINK.
    </para>
    <tip>
     <title>Transition of Salt Minions</title>
     <para>
      To ease the transition of &sminion;s to the new &adm;, remove the
      original &smaster;'s public key from each of them:
     </para>
<screen>
&prompt.sminion;rm /etc/salt/pki/minion/minion_master.pub
&prompt.sminion;systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Verify that the <package>deepsea</package> package is installed and
     install it if required.
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Customize the <filename>policy.cfg</filename> file by changing the
     <literal>role-master</literal> line. Find more details in DEAD LINK
    </para>
   </step>
   <step>
    <para>
     Synchronize <filename>/srv/pillar</filename> and
     <filename>/srv/salt</filename> directories from the old &adm; to the new
     one.
    </para>
    <tip>
     <title><command>rsync</command> Dry Run and Symbolic Links</title>
     <para>
      If possible, try synchronizing the files in a dry run first to see which
      files will be transferred (<command>rsync</command>'s option
      <option>-n</option>). Also, include symbolic links
      (<command>rsync</command>'s option <option>-a</option>). For
      <command>rsync</command>, the synchronize command will look as follows:
     </para>
<screen>&prompt.smaster;rsync -avn /srv/pillar/ <replaceable>NEW-ADMIN-HOSTNAME:</replaceable>/srv/pillar</screen>
    </tip>
   </step>
   <step>
    <para>
     If you made custom changes to files outside of
     <filename>/srv/pillar</filename> and <filename>/srv/salt</filename>, for
     example in <filename>/etc/salt/master</filename> or
     <filename>/etc/salt/master.d</filename>, synchronize them as well.
    </para>
   </step>
   <step>
    <para>
     Now you can run &deepsea; stages from the new &adm;. Refer to DEAD LINK
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-rolling-updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   Keep the &ceph; cluster nodes up-to-date by applying rolling updates
   regularly.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Software Repositories</title>
   <para>
    Before patching the cluster with the latest software packages, verify that
    all the cluster's nodes have access to the relevant repositories. Refer to
    <xref linkend="upgrade-one-node-manual"/> for a complete list of the
    required repositories.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Repository Staging</title>
   <para>
    If you use a staging tool&mdash;for example, &susemgr;, &smtool;, or
    &rmt;&mdash;that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for &sls; and &productname; are
    created at the same point in time.
   </para>
   <para>
    We strongly recommend to use a staging tool to apply patches which have
    <literal>frozen</literal> or <literal>staged</literal> patch levels. This
    ensures that new nodes joining the cluster have the same patch level as the
    nodes already running in the cluster. This way you avoid the need to apply
    the latest patches to all the cluster's nodes before new nodes can join the
    cluster.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-patch-or-dup">
   <title><command>zypper patch</command> or <command>zypper dup</command></title>
   <para>
    By default, cluster nodes are upgraded using the <command>zypper
    dup</command> command. If you prefer to update the system using
    <command>zypper patch</command> instead, edit
    <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
    following line:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </sect2>

  <sect2 xml:id="rolling-updates-reboots">
   <title>Cluster Node Reboots</title>
   <para>
    During the update, cluster nodes may be optionally rebooted if their kernel
    was upgraded by the update. If you want to eliminate the possibility of a
    forced reboot of potentially all nodes, either verify that the latest
    kernel is installed and running on &ceph; nodes, or disable automatic node
    reboots as described in
<!-- DEAD_LINK -->
    .
   </para>
  </sect2>

  <sect2>
   <title>Downtime of &ceph; Services</title>
   <para>
    Depending on the configuration, cluster nodes may be rebooted during the
    update as described in <xref linkend="rolling-updates-reboots"/>. If there
    is a single point of failure for services such as &ogw;, &sgw;, &ganesha;,
    or &iscsi;, the client machines may be temporarily disconnected from
    services whose nodes are being rebooted.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Running the Update</title>
   <para>
    To update the software packages on all cluster nodes to the latest version,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Update the <package>deepsea</package>, <package>salt-master</package>,
      and <package>salt-minion</package> packages and restart relevant services
      on the &smaster;:
     </para>
<screen>&prompt.smaster;salt -I 'roles:master' state.apply ceph.updates.master</screen>
    </step>
    <step>
     <para>
      Update and restart the <package>salt-minion</package> package on all
      cluster nodes:
     </para>
<screen>&prompt.smaster;salt -I 'cluster:ceph' state.apply ceph.updates.salt</screen>
    </step>
    <step>
     <para>
      Update all other software packages on the cluster:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      Restart &ceph; related services:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &ogw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="admin-apparmor">
  <title>Enabling &aa; Profiles</title>

  <para>
   &aa; is a security solution that confines programs by a specific profile.
   For more details, refer to
   <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-security/#part-apparmor"/>.
  </para>

  <para>
   &deepsea; provides three states for &aa; profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular &aa; state, run:
  </para>

<screen>
salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<replaceable>STATE</replaceable>
</screen>

  <para>
   To put the &aa; profiles in an 'enforce' state:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce
</screen>

  <para>
   To put the &aa; profiles in a 'complain' state:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain
</screen>

  <para>
   To disable the &aa; profiles:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable
</screen>

  <tip>
   <title>Enabling the &aa; Service</title>
   <para>
    Each of these three calls verifies if &aa; is installed and installs it if
    not, and starts and enables the related &systemd; service. &deepsea; will
    warn you if &aa; was installed and started/enabled in another way and
    therefore runs without &deepsea; profiles.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="deepsea-ceph-purge">
  <title>Removing an Entire &ceph; Cluster</title>

  <para>
   The <command>ceph.purge</command> runner removes the entire &ceph; cluster.
   This way you can clean the cluster environment when testing different
   setups. After the <command>ceph.purge</command> completes, the &salt;
   cluster is reverted back to the state at the end of &deepsea; stage 1. You
   can then either change the <filename>policy.cfg</filename> (see DEAD LINK
   with the same setup.
  </para>

  <para>
   To prevent accidental deletion, the orchestration checks if the safety is
   disengaged. You can disengage the safety measures and remove the &ceph;
   cluster by running:
  </para>

<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.purge
</screen>

  <tip>
   <title>Disabling &ceph; Cluster Removal</title>
   <para>
    If you want to prevent anyone from running the
    <command>ceph.purge</command> runner, create a file named
    <filename>disabled.sls</filename> in the
    <filename>/srv/salt/ceph/purge</filename> directory and insert the
    following line in the
    <filename>/srv/pillar/ceph/stack/global.yml</filename> file:
   </para>
<screen>purge_init: disabled</screen>
  </tip>

  <important>
   <title>Rescind Custom Roles</title>
   <para>
    If you previously created custom roles for &dashboard; (refer to
    <xref linkend="dashboard-adding-roles"/> and
    <xref linkend="dashboard-permissions"/> for detailed information), you need
    to take manual steps to purge them before running the
    <command>ceph.purge</command> runner. For example, if the custom role for
    &ogw; is named 'us-east-1', then follow these steps:
   </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/rescind
&prompt.smaster;rsync -a rgw/ us-east-1
&prompt.smaster;sed -i 's!rgw!us-east-1!' us-east-1/*.sls
</screen>
  </important>
 </sect1>
</chapter>
