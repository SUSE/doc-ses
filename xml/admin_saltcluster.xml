<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.salt.cluster">
 <title>&salt; Cluster Administration</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  After you deploy a &ceph; cluster, you will probably need to perform several
  modifications to it occasionally. These include adding or removing new nodes,
  disks, or services. This chapter describes how you can achieve these
  administration tasks.
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>Adding New Cluster Nodes</title>

  <para>
   The procedure of adding new nodes to the cluster is almost identical to the
   initial cluster node deployment described in
   <xref linkend="ceph.install.saltstack"/>:
  </para>

  <tip>
   <title>Prevent Rebalancing</title>
   <para>
    When adding an OSD to the existing cluster, bear in mind that the cluster
    will be rebalancing for some time afterward. To minimize the rebalancing
    periods, add all OSDs you intend to add at the same time.
   </para>
   <para>
    Additional way is to set the <option>osd crush initial weight = 0</option>
    option in the <filename>ceph.conf</filename> file before adding the OSDs:
   </para>
   <procedure>
    <step>
     <para>
      Add <option>osd crush initial weight = 0</option> to
      <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>.
     </para>
    </step>
    <step>
     <para>
      Create the new configuration:
     </para>
<screen>
&prompt.smaster;salt <replaceable>TARGET</replaceable> state.apply ceph.configuration.create
</screen>
    </step>
    <step>
     <para>
      Apply the new configuration:
     </para>
<screen>
&prompt.smaster;salt <replaceable>TARGET</replaceable> state.apply ceph.configuration
</screen>
    </step>
    <step>
     <para>
      After the new OSDs are added, adjust their weights as required with the
      <command>ceph osd reweight</command> command.
     </para>
    </step>
   </procedure>
  </tip>

  <procedure>
   <step>
    <para>
     Install &cephos; on the new node and configure its network setting so that
     it resolves the &smaster; host name correctly. Verify that it has a proper
     connection to both public and cluster networks, and that time
     synchronization is correctly configured. Then install the
     <systemitem>salt-minion</systemitem> package:
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     If the &smaster;'s host name is different from <literal>salt</literal>,
     edit <filename>/etc/salt/minion</filename> and add the following:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the <systemitem>salt.minion</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     On the &smaster;, accept the salt key of the new node:
    </para>
<screen>&prompt.smaster;salt-key --a <replaceable>NEW_NODE_KEY</replaceable></screen>
   </step>
   <step>
    <para>
     Verify that <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>
     targets the new &sminion; and/or set the proper &deepsea; grain. Refer to
     <xref linkend="ds.minion.targeting.name"/> of
     <xref linkend="ds.depl.stages"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Run the preparation stage. It synchronizes modules and grains so that the
     new minion can provide all the information &deepsea; expects.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <important>
     <title>Possible Restart of &deepsea; Stage 0</title>
     <para>
      If the &smaster; rebooted after its kernel update, you need to restart
      &deepsea; Stage 0.
     </para>
    </important>
   </step>
   <step>
    <para>
     Run the discovery stage. It will write new file entries in the
     <filename>/srv/pillar/ceph/proposals</filename> directory, where you can
     edit relevant .yml files:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Optionally, change
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> if the newly
     added host does not match the existing naming scheme. For details, refer
     to <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Run the configuration stage. It reads everything under
     <filename>/srv/pillar/ceph</filename> and updates the pillar accordingly:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillar stores data which you can access with the following command:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     The configuration and deployment stages include newly added nodes:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>Adding New Roles to Nodes</title>

  <para>
   You can deploy all types of supported roles with &deepsea;. See
   <xref linkend="policy.role.assignment"/> for more information on supported
   role types and examples of matching them.
  </para>

  <para>
   To add a new service to an existing node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Adapt <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> to match
     the existing host with a new role. For more details, refer to
     <xref linkend="policy.configuration"/>. For example, if you need to run an
     &ogw; on a MON node, the line is similar to:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Run Stage 2 to update the pillar:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Run Stage 3 to deploy core services, or Stage 4 to deploy optional
     services. Running both stages does not hurt.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>Removing and Reinstalling Cluster Nodes</title>

  <tip>
   <title>Removing a Cluster Node Temporarily</title>
   <para>
    The &smaster; expects all minions to be present in the cluster and
    responsive. If a minion breaks and is not responsive any more, it causes
    problems to the &salt; infrastructure, mainly to &deepsea; and &oa;.
   </para>
   <para>
    Before you fix the minion, delete its key from the &smaster; temporarily:
   </para>
<screen>
&prompt.smaster;salt-key -d <replaceable>MINION_HOST_NAME</replaceable>
</screen>
   <para>
    After the minions is fixed, add its key to the &smaster; again:
   </para>
<screen>
&prompt.smaster;salt-key -a <replaceable>MINION_HOST_NAME</replaceable>
</screen>
  </tip>

  <para>
   To remove a role from a cluster, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and remove the
   corresponding line(s). Then run Stages 2 and 5 as described in
   <xref linkend="ceph.install.stack"/>.
  </para>

  <note>
   <title>Removing OSDs from Your Cluster</title>
   <para>
    In case you need to remove a particular OSD node from your cluster, ensure
    that your cluster has more free disk space than the disk you intend to
    remove. Bear in mind that removing an OSD results in rebalancing of the
    whole cluster.
   </para>
  </note>

  <para>
   When a role is removed from a minion, the objective is to undo all changes
   related to that role. For most of the roles, the task is simple, but there
   may be problems with package dependencies. If a package is uninstalled, its
   dependencies are not.
  </para>

  <para>
   Removed OSDs appear as blank drives. The related tasks overwrite the
   beginning of the file systems and remove backup partitions in addition to
   wiping the partition tables.
  </para>

  <note>
   <title>Preserving Partitions Created by Other Methods</title>
   <para>
    Disk drives previously configured by other methods, such as
    <command>ceph-deploy</command>, may still contain partitions. &deepsea;
    will not automatically destroy these. The administrator must reclaim these
    drives manually.
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>Removing a &sminion; from the Cluster</title>
   <para>
    If your storage minions are named, for example, 'data1.ceph', 'data2.ceph'
    ... 'data6.ceph', and the related lines in your
    <filename>policy.cfg</filename> are similar to the following:
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    Then to remove the &sminion; 'data2.ceph', change the lines to the
    following:
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    Then run Stages 2 and 5:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>Migrating Nodes</title>
   <para>
    Assume the following situation: during the fresh cluster installation, you
    (the administrator) allocated one of the storage nodes as a stand-alone
    &ogw; while waiting for the gateway's hardware to arrive. Now the permanent
    hardware has arrived for the gateway and you can finally assign the
    intended role to the backup storage node and have the gateway role removed.
   </para>
   <para>
    After running Stages 0 and 1 (see <xref linkend="ds.depl.stages"/>) for the
    new hardware, you named the new gateway <literal>rgw1</literal>. If the
    node <literal>data8</literal> needs the &ogw; role removed and the storage
    role added, and the current <filename>policy.cfg</filename> looks like
    this:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Then change it to:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Run Stages 2 to 5. Stage 3 will add <literal>data8</literal> as a storage
    node. For a moment, <literal>data8</literal> will have both roles. Stage 4
    will add the &ogw; role to <literal>rgw1</literal> and Stage 5 will remove
    the &ogw; role from <literal>data8</literal>.
   </para>
  </example>
 </sect1>
 <sect1 xml:id="ds.mon">
  <title>Redeploying Monitor Nodes</title>

  <para>
   When one or more of your monitor nodes fail and are not responding, you need
   to remove the failed monitors from the cluster and possibly then re-add them
   back in the cluster.
  </para>

  <important>
   <title>The Minimum is Three Monitor Nodes</title>
   <para>
    The number of monitor nodes must not be less than three. If a monitor node
    fails, and as a result your cluster has only two monitor nodes only, you
    need to temporarily assign the monitor role to other cluster nodes before
    you redeploy the failed monitor nodes. After you redeploy the failed
    monitor nodes, you can uninstall the temporary monitor roles.
   </para>
   <para>
    For more information on adding new nodes/roles to the &ceph; cluster, see
    <xref linkend="salt.adding.nodes"/> and
    <xref linkend="salt.adding.services"/>.
   </para>
   <para>
    For more information on removing cluster nodes, refer to
    <xref linkend="salt.node.removing"/>.
   </para>
  </important>

  <para>
   There are two basic degrees of a &ceph; node failure:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &sminion; host is broken either physically or on the OS level, and
     does not respond to the <command>salt
     '<replaceable>minion_name</replaceable>' test.ping</command> call. In such
     case you need to redeploy the server completely by following the relevant
     instructions in <xref linkend="ceph.install.stack"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     The monitor related services failed and refuse to recover, but the host
     responds to the <command>salt '<replaceable>minion_name</replaceable>'
     test.ping</command> call. In such case, follow these steps:
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Edit <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> on the
     &smaster;, and remove or update the lines that correspond to the failed
     monitor nodes so that they now point to the working monitor nodes. For
     example:
    </para>
<screen>
[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; Stages 2 to 5 to apply the changes:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
&prompt.smaster;salt-run state.orch ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>Adding an OSD Disk to a Node</title>

  <para>
   To add a disk to an existing OSD node, verify that any partition on the disk
   was removed and wiped. Refer to <xref linkend="deploy.wiping.disk"/> in
   <xref linkend="ceph.install.stack"/> for more details. After the disk is
   empty, add the disk to the YAML file of the node. The path to the file is
   <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>node_name</replaceable>.yml</filename>.
   After saving the file, run &deepsea; stages 2 and 3:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3</screen>

  <tip>
   <title>Updated Profiles Automatically</title>
   <para>
    Instead of manually editing the YAML file, &deepsea; can create new
    profiles. To let &deepsea; create new profiles, the existing profiles need
    to be moved:
   </para>
<screen>&prompt.smaster;<command>old</command> /srv/pillar/ceph/proposals/profile-default/
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
   <para>
    We recommend verifying the suggested proposals before deploying the
    changes. Refer to <xref linkend="policy.profile.assignment" /> for more
    details on viewing proposals.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>Removing an OSD</title>

  <para>
   You can remove an &osd; from the cluster by running the following command:
  </para>

<screen>&prompt.smaster;<command>salt-run</command> disengage.safety
&prompt.smaster;<command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> needs to be a number of the OSD without
   the <literal>osd.</literal> prefix. For example, from
   <literal>osd.3</literal> only use the digit <literal>3</literal>.
  </para>

  <sect2 xml:id="osd.removal.multiple">
   <title>Removing Multiple OSDs</title>
   <para>
    Use the same procedure as mentioned in <xref linkend="salt.removing.osd"/>
    but simply supply multiple OSD IDs:
   </para>
<screen>
&prompt.smaster;salt-run disengage.safety
safety is now disabled for cluster ceph

&prompt.smaster;salt-run remove.osd 1 13 20
Removing osds 1, 13, 20 from minions
Press Ctrl-C to abort
Removing osd 1 from minion data4.ceph
Removing osd 13 from minion data4.ceph
Removing osd 20 from minion data4.ceph
Removing osd 1 from Ceph
Removing osd 13 from Ceph
Removing osd 20 from Ceph
</screen>
   <important xml:id="imp.removed_osd_in_grains">
    <title>Removed OSD ID Still Present in grains</title>
    <para>
     After the <command>remove.osd</command> command finishes, the ID of the
     removed OSD is still part of &salt; grains and you can see it after
     running <command>salt <replaceable>target</replaceable>
     osd.list</command>. The reason is that if the
     <command>remove.osd</command> command partially fails on removing the data
     disk, the only reference to related partitions on the shared devices is in
     the grains. If we updated the grains immediately, then those partitions
     would be orphaned.
    </para>
    <para>
     To update the grains manually, run <command>salt
     <replaceable>target</replaceable> osd.retain</command>. It is part of
     &deepsea; Stage 3, therefore if you are going to run Stage 3 after the OSD
     removal, the grains get updated automatically.
    </para>
   </important>
   <tip>
    <title>Automatic Retries</title>
    <para>
     You can append the <option>timeout</option> parameter (in seconds) after
     which &salt; retries the OSD removal:
    </para>
<screen>
&prompt.smaster;salt-run remove.osd 20 timeout=6
Removing osd 20 from minion data4.ceph
  Timeout expired - OSD 20 has 22 PGs remaining
Retrying...
Removing osd 20 from Ceph
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="osd.forced.removal">
   <title>Removing Broken OSDs Forcefully</title>
   <para>
    There are cases when removing an OSD gracefully (see
    <xref linkend="salt.removing.osd"/>) fails. This may happen for example if
    the OSD or its journal, Wall or DB are broken, when it suffers from hanging
    I/O operations, or when the OSD disk fails to unmount. In such case, you
    need to force the OSD removal. The following command removes both the data
    partition, and the journal or WAL/DB partitions:
   </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <tip>
    <title>Hanging Mounts</title>
    <para>
     If a partition is still mounted on the disk being removed, the command
     will exit with the 'Unmount failed - check for processes on
     <replaceable>DEVICE</replaceable>' message. You can then list all
     processes that access the file system with the <command>fuser -m
     <replaceable>DEVICE</replaceable></command>. If <command>fuser</command>
     returns nothing, try manual <command>unmount
     <replaceable>DEVICE</replaceable></command> and watch the output of
     <command>dmesg</command> or <command>journalctl</command> commands.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ds.osd.replace">
  <title>Replacing an OSD Disk</title>

  <para>
   There are several reasons why you may need to replace an OSD disk, for
   example:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The OSD disk failed or is soon going to fail based on SMART information,
     and can no longer be used to store data safely.
    </para>
   </listitem>
   <listitem>
    <para>
     You need to upgrade the OSD disk, for example to increase its size.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The replacement procedure is the same for both cases. It is also valid for
   both default and customized &crushmap;s.
  </para>

  <warning>
   <title>The Number of Free Disks</title>
   <para>
    When doing an automated OSDs replacement, the number of free disks needs to
    be the same as the number of disks you need to replace. If there are more
    free disks available in the system, it is impossible to guess which free
    disks to replace. Therefore the automated replacement will not be
    performed.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Turn off safety limitations temporarily:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
   </step>
   <step>
    <para>
     Suppose that for example '5' is the ID of the OSD whose disk needs to be
     replaced. The following command marks it as
     <emphasis role="bold">destroyed</emphasis> in the &crushmap; but leaves
     its original ID:
    </para>
<screen>
&prompt.smaster;salt-run replace.osd 5
</screen>
    <tip>
     <title><command>replace.osd</command> and <command>remove.osd</command></title>
     <para>
      The &salt;'s <command>replace.osd</command> and
      <command>remove.osd</command> (see <xref linkend="salt.removing.osd"/>)
      commands are identical except that <command>replace.osd</command> leaves
      the OSD as 'destroyed' in the &crushmap; while
      <command>remove.osd</command> removes all traces from the &crushmap;.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Manually replace the failed/upgraded OSD drive.
    </para>
   </step>
   <step>
    <para>
     After replacing the physical drive, you need to modify the configuration
     of the related &sminion;. You can do so either manually or in an automated
     way.
    </para>
    <para>
     To manually change a &sminion;'s configuration, see
     <xref linkend="osd.replace.manual"/>.
    </para>
    <para>
     To change a &sminion;'s configuration in an automated way, see
     <xref linkend="osd.replace.auto"/>.
    </para>
   </step>
   <step>
    <para>
     After you finish either manual or automated configuration of the
     &sminion;, run &deepsea; Stage 2 to update the &salt; configuration. It
     prints out a summary about the differences between the storage
     configuration and the current setup:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
deepsea_minions          : valid
yaml_syntax              : valid
profiles_populated       : valid
public network           : 172.16.21.0/24
cluster network          : 172.16.22.0/24

These devices will be deployed
data1.ceph: /dev/sdb, /dev/sdc, /dev/sdd, /dev/sde, /dev/sdf, /dev/sdg
</screen>
    <tip>
     <title>Run <command>salt-run advise.osds</command></title>
     <para>
      To summarize the steps that will be taken when the actual replacement is
      deployed, you can run the following command:
     </para>
<screen>
&prompt.smaster;salt-run advise.osds
These devices will be deployed

data1.ceph:
  /dev/disk/by-id/cciss-3600508b1001c7c24c537bdec8f3a698f:

Run 'salt-run state.orch ceph.stage.3'
</screen>
    </tip>
   </step>
   <step>
    <para>
     Run the deployment Stage 3 to deploy the replaced OSD disk:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
  </procedure>

  <sect2 xml:id="osd.replace.manual">
   <title>Manual Configuration</title>
   <procedure>
    <step>
     <para>
      Find the renamed YAML file for the &sminion;. For example, the file for
      the minion named 'data1.ceph' is
     </para>
<screen>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/stack/default/ceph/minions/data1.ceph.yml-replace</screen>
    </step>
    <step>
     <para>
      Rename the file to its original name (without the
      <literal>-replace</literal> suffix), edit it, and replace the old device
      with the new device name.
     </para>
     <tip>
      <title>salt osd.report</title>
      <para>
       Consider using <command>salt '<replaceable>MINION_NAME</replaceable>'
       osd.report</command> to identify the device that has been removed.
      </para>
     </tip>
     <para>
      For example, if the <filename>data1.ceph.yml</filename> file contains
     </para>
<screen>
ceph:
  storage:
    osds:
      [...]
      /dev/disk/by-id/cciss-3600508b1001c93595b70bd0fb700ad38:
        format: bluestore
      [...]
</screen>
     <para>
      replace the corresponding device path with
     </para>
<screen>
ceph:
  storage:
    osds:
      [...]
      /dev/disk/by-id/cciss-3600508b1001c7c24c537bdec8f3a698f:
        format: bluestore
        replace: True
      [...]
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="osd.replace.auto">
   <title>Automated Configuration</title>
   <para>
    While the default profile for Stage 1 may work for the simplest setups,
    this stage can be optionally customized:
   </para>
   <procedure>
    <step>
     <para>
      Set the <option>stage_discovery:
      <replaceable>CUSTOM_STAGE_NAME</replaceable></option> option in
      <filename>/srv/pillar/ceph/stack/global.yml</filename>.
     </para>
    </step>
    <step>
     <para>
      Create the corresponding file
      <filename>/srv/salt/ceph/stage/1/<replaceable>CUSTOM_STAGE_NAME</replaceable>.sls</filename>
      and customize it to reflect your specific requirements for Stage 1. See
      <xref linkend="app.stage1.custom"/> for an example.
     </para>
     <tip>
      <title>Inspect <filename>init.sls</filename></title>
      <para>
       Inspect the <filename>/srv/salt/ceph/stage/1/init.sls</filename> file to
       see what variables you can use in your custom Stage 1 .sls file.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Refresh the Pillar:
     </para>
<screen>
&prompt.smaster;salt '*' saltutil.pillar_refresh
</screen>
    </step>
    <step>
     <para>
      Run Stage 1 to generate the new configuration file:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.1
</screen>
    </step>
   </procedure>
   <tip>
    <title>Custom Options</title>
    <para>
     To list all available options, inspect the output of the <command>salt-run
     proposal.help</command> command.
    </para>
    <para>
     If you customized the cluster deployment with a specific command
    </para>
<screen>
salt-run proposal.populate <replaceable>OPTION</replaceable>=<replaceable>VALUE</replaceable>
</screen>
    <para>
     use the same configuration when doing the automated configuration.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ds.osd.recover">
  <title>Recovering a Reinstalled OSD Node</title>

  <para>
   If the operating system breaks and is not recoverable on one of your OSD
   nodes, follow these steps to recover it and redeploy its OSD role with
   cluster data untouched:
  </para>

  <procedure>
   <step>
    <para>
     Reinstall the base &sle; operating system on the node where the OS broke.
     Install the <package>salt-minion</package> packages on the OSD node,
     delete the old &sminion; key on the &smaster;, and register the new
     &sminion;'s key it with the &smaster;. For more information on the initial
     deployment, see <xref linkend="ceph.install.stack"/>.
    </para>
   </step>
   <step>
    <para>
     Instead of running the whole of Stage 0, run the following parts:
    </para>
<screen>
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
&prompt.smaster;salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; Stages 1 to 5:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
&prompt.smaster;salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     Run &deepsea; Stage 0:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     Reboot the relevant OSD node. All OSD disks will be rediscovered and
     reused.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. There is a
    real risk of putting your &smaster; into a death spiral.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, edit the file
  </para>

<screen>/etc/salt/master.d/reactor.conf</screen>

  <para>
   and replace the following line
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   with
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>

  <para>
   Verify that the line is not commented out.
  </para>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>Updating the Cluster Nodes</title>

  <para>
   It is a good idea to apply rolling updates to your cluster's nodes
   regularly.
  </para>

  <important>
   <title>Access to Software Repositories</title>
   <para>
    Before patching the cluster with latest software packages, verify that all
    its nodes have access to &sls; repositories that match your version of
    &productname;. For &productname; &productnumber;, the following
    repositories are required:
   </para>
<screen>
&prompt.root;zypper lr -E
#  | Alias   | Name                              | Enabled | GPG Check | Refresh
---+---------+-----------------------------------+---------+-----------+--------
 4 | [...]   | SUSE-Enterprise-Storage-5-Pool    | Yes     | (r ) Yes  | No
 6 | [...]   | SUSE-Enterprise-Storage-5-Updates | Yes     | (r ) Yes  | Yes
 9 | [...]   | SLES12-SP3-Pool                   | Yes     | (r ) Yes  | No
11 | [...]   | SLES12-SP3-Updates                | Yes     | (r ) Yes  | Yes
</screen>
  </important>

  <tip>
   <title>Repository Staging</title>
   <para>
    If you use a staging tool&mdash;for example &susemgr;, &smtool;, or
    &rmt;&mdash;that serves software repositories to the cluster nodes, verify
    that stages for both 'Updates' repositories for &sls; and &productname; are
    created at the same point in time.
   </para>
   <para>
    We strongly recommend to use a staging tool for patches with
    <emphasis role="bold">frozen / staged patch levels</emphasis>. It ensures
    that new nodes joining the cluster have the same patch level as the nodes
    already running in the cluster. This way you avoid the need to apply the
    latest patches to all cluster's nodes before new nodes can join the
    cluster.
   </para>
  </tip>

  <para>
   To apply the updates, run Stage 0:
  </para>

<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>

  <note>
   <title>Possible Downtime of &ceph; Services</title>
   <para>
    When applying updates to &ceph; cluster nodes, &ceph; services may be
    restarted. If there is a single point of failure for services such as
    &ogw;, &ganesha;, or &iscsi;, the client machines may be temporarily
    disconnected from related services.
   </para>
  </note>

  <para>
   If &deepsea; detects a running &ceph; cluster, it applies available updates,
   restarts running &ceph; services, and optionally restarts nodes sequentially
   if a kernel update was installed. &deepsea; follows &ceph;'s official
   recommendation of first updating the monitors, then the OSDs, and lastly
   additional services, such as &mds;, &rgw;, &igw;, or &ganesha;. &deepsea;
   stops the update process if it detects an issue in the cluster. A trigger
   for that can be:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &ceph; reports 'HEALTH_ERR' for longer then 300 seconds.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s are queried for their assigned services to be still up and
     running after an update. The update fails if the services are down for
     more than 900 seconds.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Making these arrangements ensures that even with corrupted or failing
   updates, the &ceph; cluster is still operational.
  </para>

  <para>
   &deepsea; Stage 0 updates the system via <command>zypper update</command>
   and optionally reboots the system if the kernel is updated. If you want to
   eliminate the possibility of a forced reboot of potentially all nodes,
   either make sure that the latest kernel is installed and running before
   initiating &deepsea; Stage 0, or disable automatic node reboots as described
   in <xref linkend="ds.disable.reboots"/>.
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    If you prefer to update the system using the <command>zypper
    patch</command> command, edit
    <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
    following line:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   You can change the default update/reboot behavior of &deepsea; Stage 0 by
   adding/changing the <option>stage_prep_master</option> and
   <option>stage_prep_minion</option> options. For more information, see
   <xref linkend="ds.disable.reboots"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>Halting or Rebooting Cluster</title>

  <para>
   In some cases it may be necessary to halt or reboot the whole cluster. We
   recommended carefully checking for dependencies of running services. The
   following steps provide an outline for stopping and starting the cluster:
  </para>

  <procedure>
   <step>
    <para>
     Tell the &ceph; cluster not to mark OSDs as out:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stop daemons and nodes in the following order:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     If required, perform maintenance tasks.
    </para>
   </step>
   <step>
    <para>
     Start the nodes and servers in the reverse order of the shutdown process:
    </para>
    <orderedlist>
     <listitem>
      <para>
       &mon;
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, for example &ganesha; or &rgw;
      </para>
     </listitem>
     <listitem>
      <para>
       Storage clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remove the noout flag:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>Adjusting <filename>ceph.conf</filename> with Custom Settings</title>

  <para>
   If you need to put custom settings into the <filename>ceph.conf</filename>
   file, you can do so by modifying the configuration files in the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>
   directory:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Unique <filename>rgw.conf</filename></title>
   <para>
    The &ogw; offers a lot flexibility and is unique compared to the other
    <filename>ceph.conf</filename> sections. All other &ceph; components have
    static headers such as <literal>[mon]</literal> or
    <literal>[osd]</literal>. The &ogw; has unique headers such as
    <literal>[client.rgw.rgw1]</literal>. This means that the
    <filename>rgw.conf</filename> file needs a header entry. For examples, see
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw.conf</filename>
</screen>
   <para>
    or
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw-ssl.conf</filename>
</screen>
  </note>

  <important>
   <title>Run Stage 3</title>
   <para>
    After you make custom changes to the above mentioned configuration files,
    run Stages 3 and 4 to apply these changes to the cluster nodes:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
  </important>

  <para>
   These files are included from the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
   template file, and correspond to the different sections that the &ceph;
   configuration file accepts. Putting a configuration snippet in the correct
   file enables &deepsea; to place it into the correct section. You do not need
   to add any of the section headers.
  </para>

  <tip>
   <para>
    To apply any configuration options only to specific instances of a daemon,
    add a header such as <literal>[osd.1]</literal>. The following
    configuration options will only be applied to the OSD daemon with the ID 1.
   </para>
  </tip>

  <sect2>
   <title>Overriding the Defaults</title>
   <para>
    Later statements in a section overwrite earlier ones. Therefore it is
    possible to override the default configuration as specified in the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
    template. For example, to turn off cephx authentication, add the following
    three lines to the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
    file:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
   <para>
    When redefining the default values, &ceph; related tools such as
    <command>rados</command> may issue warnings that specific values from the
    <filename>ceph.conf.j2</filename> were redefined in
    <filename>global.conf</filename>. These warnings are caused by one
    parameter assigned twice in the resulting <filename>ceph.conf</filename>.
   </para>
   <para>
    As a workaround for this specific case, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Change the current directory to
      <filename>/srv/salt/ceph/configuration/create</filename>:
     </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/configuration/create
</screen>
    </step>
    <step>
     <para>
      Copy <filename>default.sls</filename> to <filename>custom.sls</filename>:
     </para>
<screen>
&prompt.smaster;cp default.sls custom.sls
</screen>
    </step>
    <step>
     <para>
      Edit <filename>custom.sls</filename> and change
      <option>ceph.conf.j2</option> to <option>custom-ceph.conf.j2</option>.
     </para>
    </step>
    <step>
     <para>
      Change current directory to
      <filename>/srv/salt/ceph/configuration/files</filename>:
     </para>
<screen>
&prompt.smaster;cd /srv/salt/ceph/configuration/files
</screen>
    </step>
    <step>
     <para>
      Copy <filename>ceph.conf.j2</filename> to
      <filename>custom-ceph.conf.j2</filename>:
     </para>
<screen>
&prompt.smaster;cp ceph.conf.j2 custom-ceph.conf.j2
</screen>
    </step>
    <step>
     <para>
      Edit <filename>custom-ceph.conf.j2</filename> and delete the following
      line:
     </para>
<screen>
{% include "ceph/configuration/files/rbd.conf" %}
</screen>
     <para>
      Edit <filename>global.yml</filename> and add the following line:
     </para>
<screen>
configuration_create: custom
</screen>
    </step>
    <step>
     <para>
      Refresh the pillar:
     </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> saltutil.pillar_refresh
</screen>
    </step>
    <step>
     <para>
      Run Stage 3:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    </step>
   </procedure>
   <para>
    Now you should have only one entry for each value definition. To re-create
    the configuration, run:
   </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.configuration.create
</screen>
   <para>
    and then verify the contents of
    <filename>/srv/salt/ceph/configuration/cache/ceph.conf</filename>.
   </para>
  </sect2>

  <sect2>
   <title>Including Configuration Files</title>
   <para>
    If you need to apply a lot of custom configurations, use the following
    include statements within the custom configuration files to make file
    management easier. Following is an example of the
    <filename>osd.conf</filename> file:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    In the previous example, the <filename>osd1.conf</filename>,
    <filename>osd2.conf</filename>, <filename>osd3.conf</filename>, and
    <filename>osd4.conf</filename> files contain the configuration options
    specific to the related OSD.
   </para>
   <tip>
    <title>Runtime Configuration</title>
    <para>
     Changes made to &ceph; configuration files take effect after the related
     &ceph; daemons restart. See <xref linkend="ceph.config.runtime"/> for more
     information on changing the &ceph; runtime configuration.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="admin.apparmor">
  <title>Enabling &aa; Profiles</title>

  <para>
   &aa; is a security solution that confines programs by a specific profile.
   For more details, refer to
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_security/data/part_apparmor.html"/>.
  </para>

  <para>
   &deepsea; provides three states for &aa; profiles: 'enforce', 'complain',
   and 'disable'. To activate a particular &aa; state, run:
  </para>

<screen>
salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<replaceable>STATE</replaceable>
</screen>

  <para>
   To put the &aa; profiles in an 'enforce' state:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce
</screen>

  <para>
   To put the &aa; profiles in a 'complain' status:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain
</screen>

  <para>
   To disable the &aa; profiles:
  </para>

<screen>
&prompt.smaster;salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable
</screen>

  <tip>
   <title>Enabling the &aa; Service</title>
   <para>
    Each of these three calls verifies if &aa; is installed and installs it if
    not, and starts and enables the related &systemd; service. &deepsea; will
    warn you if &aa; was installed and started/enabled in another way and
    therefore runs without &deepsea; profiles.
   </para>
  </tip>
 </sect1>
</chapter>
