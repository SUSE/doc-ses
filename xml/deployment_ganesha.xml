<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-as-ganesha">
<!-- ============================================================== -->
 <title>Installation of &ganesha;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; provides NFS access to either the &rgw; or the &cephfs;. In
  &productname; &productnumber;, NFS versions 3 and 4 are supported. &ganesha;
  runs in the user space instead of the kernel space and directly interacts
  with the &rgw; or &cephfs;.
 </para>
 <warning>
  <title>Cross Protocol Access</title>
  <para>
   Native &cephfs; and NFS clients are not restricted by file locks obtained
   via Samba, and vice versa. Applications that rely on cross protocol file
   locking may experience data corruption if &cephfs; backed Samba share paths
   are accessed via other means.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparation</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>General Information</title>
   <para>
    To successfully deploy &ganesha;, you need to add a
    <literal>role-ganesha</literal> to your
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. For details,
    see <xref linkend="policy-configuration"/>. &ganesha; also needs either a
    <literal>role-rgw</literal> or a <literal>role-mds</literal> present in the
    <filename>policy.cfg</filename>.
   </para>
   <para>
    Although it is possible to install and run the &ganesha; server on an
    already existing &ceph; node, we recommend running it on a dedicated host
    with access to the &ceph; cluster. The client hosts are typically not part
    of the cluster, but they need to have network access to the &ganesha;
    server.
   </para>
   <para>
    To enable the &ganesha; server at any point after the initial installation,
    add the <literal>role-ganesha</literal> to the
    <filename>policy.cfg</filename> and re-run at least &deepsea; stages 2 and
    4. For details, see <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    &ganesha; is configured via the file
    <filename>/etc/ganesha/ganesha.conf</filename> that exists on the &ganesha;
    node. However, this file is overwritten each time &deepsea; stage 4 is
    executed. Therefore we recommend to edit the template used by &salt;, which
    is the file
    <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> on the
    &smaster;. For details about the configuration file, see
    <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Summary of Requirements</title>
   <para>
    The following requirements need to be met before &deepsea; stages 2 and 4
    can be executed to install &ganesha;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      At least one node needs to be assigned the
      <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      You can define only one <literal>role-ganesha</literal> per minion.
     </para>
    </listitem>
    <listitem>
     <para>
      &ganesha; needs either an &ogw; or &cephfs; to work.
     </para>
    </listitem>
    <listitem>
     <para>
      The kernel based NFS needs to be disabled on minions with the
      <literal>role-ganesha</literal> role.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Example Installation</title>

  <para>
   This procedure provides an example installation that uses both the &rgw; and
   &cephfs; File System Abstraction Layers (FSAL) of &ganesha;.
  </para>

  <procedure>
   <step>
    <para>
     If you have not done so, execute &deepsea; stages 0 and 1 before
     continuing with this procedure.
    </para>
<screen>&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     After having executed stage 1 of &deepsea;, edit the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and add the
     line
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Replace <replaceable>NODENAME</replaceable> with the name of a node in
     your cluster.
    </para>
    <para>
     Also make sure that a <literal>role-mds</literal> and a
     <literal>role-rgw</literal> are assigned.
    </para>
   </step>
   <step>
    <para>
     Execute at least stages 2 and 4 of &deepsea;. Running stage 3 in between
     is recommended.
    </para>
<screen>&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.2
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verify that &ganesha; is working by checking that the NFS Ganesha service
     is running on the minion node:
    </para>
<screen>&prompt.smaster;<command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>High Availability Active-Passive Configuration</title>

  <para>
   This section provides an example of how to set up a two-node active-passive
   configuration of &ganesha; servers. The setup requires the &sle; &hasi;. The
   two nodes are called &si.earth; and &si.mars;.
  </para>

  <important>
   <title>Co-location of Services</title>
   <para>
    Services that have their own fault tolerance and their own load balancing
    should not be running on cluster nodes that get fenced for failover
    services. Therefore, do not run &mon;, &mds;, &iscsi;, or &osd; services on
    &ha; setups.
   </para>
  </important>

  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Basic Installation</title>
   <para>
    In this setup &si.earth; has the IP address
    <systemitem class="ipaddress">192.168.1.1</systemitem> and &si.mars; has
    the address <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Additionally, two floating virtual IP addresses are used, allowing clients
    to connect to the service independent of which physical node it is running
    on. <systemitem class="ipaddress">192.168.1.10</systemitem> is used for
    cluster administration with Hawk2 and
    <systemitem class="ipaddress">192.168.2.1</systemitem> is used exclusively
    for the NFS exports. This makes it easier to apply security restrictions
    later.
   </para>
   <para>
    The following procedure describes the example installation. More details
    can be found at
    <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-install-quick/"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Prepare the &ganesha; nodes on the &smaster;:
     </para>
     <substeps>
      <step>
       <para>
        Run &deepsea; stages 0 and 1.
       </para>
<screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assign the nodes &si.earth; and &si.mars; the
        <literal>role-ganesha</literal> in the
        <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Run &deepsea; stages 2 to 4.
       </para>
<screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.2
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Register the &sle; &hasi; on &si.earth; and &si.mars;.
     </para>
<screen>
&prompt.root;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Install <package>ha-cluster-bootstrap</package> on both nodes:
     </para>
<screen>&prompt.root;<command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialize the cluster on &si.earth;:
       </para>
<screen>&prompt.earth;<command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Let &si.mars; join the cluster:
       </para>
<screen>&prompt.mars;<command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Check the status of the cluster. You should see two nodes added to the
      cluster:
     </para>
<screen>&prompt.earth;<command>crm</command> status</screen>
    </step>
    <step>
     <para>
      On both nodes, disable the automatic start of the &ganesha; service at
      boot time:
     </para>
<screen>&prompt.root;<command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Start the <command>crm</command> shell on &si.earth;:
     </para>
<screen>&prompt.earth;<command>crm</command> configure</screen>
     <para>
      The next commands are executed in the crm shell.
     </para>
    </step>
    <step>
     <para>
      On &si.earth;, run the crm shell to execute the following commands to
      configure the resource for &ganesha; daemons as clone of systemd resource
      type:
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>commit
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Create a primitive IPAddr2 with the crm shell:
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt xmlns="http://docbook.org/ns/docbook">crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      To set up a relationship between the &ganesha; server and the floating
      Virtual IP, we use collocation and ordering.
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Use the <command>mount</command> command from the client to ensure that
      cluster setup is complete:
     </para>
<screen>&prompt.root;<command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Clean Up Resources</title>
   <para>
    In the event of an &ganesha; failure at one of the node, for example
    &si.earth;, fix the issue and clean up the resource. Only after the
    resource is cleaned up can the resource fail back to &si.earth; in case
    &ganesha; fails at &si.mars;.
   </para>
   <para>
    To clean up the resource:
   </para>
<screen>&prompt.earth;<command>crm</command> resource cleanup nfs-ganesha-clone earth
&prompt.earth;<command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Setting Up Ping Resource</title>
   <para>
    It may happen that the server is unable to reach the client because of a
    network issue. A ping resource can detect and mitigate this problem.
    Configuring this resource is optional.
   </para>
   <procedure>
    <step>
     <para>
      Define the ping resource:
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> is a list of IP addresses separated by space
      characters. The IP addresses will be pinged regularly to check for
      network outages. If a client must always have access to the NFS server,
      add it to <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Create a clone:
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      The following command creates a constraint for the &ganesha; service. It
      forces the service to move to another node when
      <literal>host_list</literal> is unreachable.
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="setup-portblock-resource">
    <title>Setting Up PortBlock Resource</title>
    <para>When a service goes down, the TCP connection that is in use by
      &ganesha; is required to be closed otherwise it
      continues to run until a system-specific timeout occurs. This timeout
      can take upwards of 3 minutes.</para>
    <para>To shorten the timeout time, the TCP connection needs to be reset.
      We recommend configuring <literal>portblock</literal> to reset stale TCP
      connections.</para>
    <para>You can choose to use portblock with or without the
      <literal>tickle_dir</literal> parameters that could unblock and
      reconnect clients to the new service faster. We recommend to
      have <literal>tickle_dir</literal> as the shared &cephfs; mount
      between two HA nodes (where &ganesha; services are running).</para>
      <procedure>
        <note>
          <para>Configuring the following resource is optional.</para>
        </note>
        <step>
          <para>On &si.earth;, run the crm shell to execute the following
            commands to configure the resource for &ganesha; daemons:</para>
<screen>&prompt.earth;<command>crm</command> configure</screen>
        </step>
        <step>
          <para>Configure the <literal>block</literal> action for
          <literal>portblock</literal> and omit the <literal>tickle_dir</literal>
          option if you have not configured a shared directory:</para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>
primitive nfs-ganesha-block ocf:portblock protocol=tcp portno=2049 action=block
ip=192.168.2.1 op monitor depth="0" timeout="10" interval="10" tickle_dir="/tmp/ganesha/tickle/"</screen>
        </step>
        <step>
          <para>Configure the <literal>unblock</literal> action for
          <literal>portblock</literal> and omit the <literal>reset_local_on_unblock_stop</literal>
          option if you have not configured a shared directory:</para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> colocation
ganesha-portblock inf: ganesha-ip nfs-ganesha-block nfs-ganesha-unblock
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> edit ganesha-ip-after-nfs-ganesha-server
</screen>
        </step>
        <step>
          <para>Configure the <literal>IPAddr2</literal> resource with <literal>portblock</literal>:</para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> colocation
ganesha-portblock inf: ganesha-ip nfs-ganesha-block nfs-ganesha-unblock
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> edit
ganesha-ip-after-nfs-ganesha-server order ganesha-ip-after-nfs-ganesha-server
Mandatory: nfs-ganesha-block nfs-ganesha-clone ganesha-ip nfs-ganesha-unblock
</screen>
        </step>
        <step>
          <para>Save your changes:</para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> commit</screen>
        </step>
        <step>
          <para>Your configuration should look like this:</para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt> show</screen>
<screen>
"
node 1084782956: nfs1
node 1084783048: nfs2
primitive ganesha-ip IPaddr2 \
        params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
        op monitor interval=10 timeout=20
primitive nfs-ganesha-block portblock \
        params protocol=tcp portno=2049 action=block ip=192.168.2.1 \
        tickle_dir="/tmp/ganesha/tickle/" op monitor timeout=10 interval=10 depth=0
primitive nfs-ganesha-server systemd:nfs-ganesha \
        op monitor interval=30s
primitive nfs-ganesha-unblock portblock \
        params protocol=tcp portno=2049 action=unblock ip=192.168.2.1 \
        reset_local_on_unblock_stop=true tickle_dir="/tmp/ganesha/tickle/" \
        op monitor timeout=10 interval=10 depth=0
clone nfs-ganesha-clone nfs-ganesha-server \
        meta interleave=true
location cli-prefer-ganesha-ip ganesha-ip role=Started inf: nfs1
order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-block nfs-ganesha-clone ganesha-ip nfs-ganesha-unblock
colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
colocation ganesha-portblock inf: ganesha-ip nfs-ganesha-block nfs-ganesha-unblock
property cib-bootstrap-options: \
        have-watchdog=false \
        dc-version=1.1.16-6.5.1-77ea74d \
        cluster-infrastructure=corosync \
        cluster-name=hacluster \
        stonith-enabled=false \
        placement-strategy=balanced \
        last-lrm-refresh=1544793779
rsc_defaults rsc-options: \
        resource-stickiness=1 \
        migration-threshold=3
op_defaults op-options: \
        timeout=600 \
        record-pending=true
"
</screen>
        <para>In this example <filename>/tmp/ganesha/</filename> is the &cephfs;
        mount on both nodes (nfs1 and nfs2):</para>
<screen>172.16.1.11:6789:/ganesha on /tmp/ganesha type ceph (rw,relatime,name=admin,secret=...hidden...,acl,wsize=16777216)</screen>
        <para>The <literal>tickle</literal> directory has been initially
        created.</para>
      </step>
    </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>&ganesha; HA and &deepsea;</title>
   <para>
    &deepsea; does not support configuring &ganesha; HA. To prevent &deepsea;
    from failing after &ganesha; HA is configured, exclude starting and
    stopping the &ganesha; service from &deepsea; stage 4:
   </para>
   <procedure>
    <step>
     <para>
      Copy <filename>/srv/salt/ceph/ganesha/default.sls</filename> to
      <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Remove the <literal>.service</literal> entry from
      <filename>/srv/salt/ceph/ganesha/ha.sls</filename> so that it looks as
      follows:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Add the following line to
      <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Active-Active Configuration</title>

  <para>
   This section provides an example of simple active-active &ganesha; setup.
   The aim is to deploy two &ganesha; servers layered on top of the same
   existing &cephfs;. The servers will be two &ceph; cluster nodes with
   separate addresses. The clients need to be distributed between them
   manually. <quote>Failover</quote> in this configuration means manually
   unmounting and remounting the other server on the client.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Prerequisites</title>
   <para>
    For our example configuration, you need the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Running &ceph; cluster. See <xref linkend="ceph-install-stack" /> for
      details on deploying and configuring &ceph; cluster by using &deepsea;.
     </para>
    </listitem>
    <listitem>
     <para>
      At least one configured &cephfs;. See
      <xref linkend="cha-ceph-as-cephfs" /> for more details on deploying and
      configuring &cephfs;.
     </para>
    </listitem>
    <listitem>
     <para>
      Two &ceph; cluster nodes with &ganesha; deployed. See
      <xref linkend="cha-as-ganesha" /> for more details on deploying
      &ganesha;.
     </para>
     <tip>
      <title>Use Dedicated Servers</title>
      <para>
       Although &ganesha; nodes can share resources with other &ceph; related
       services, we recommend to use dedicated servers to improve performance.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    After you deploy the &ganesha; nodes, verify that the cluster is
    operational and the default &cephfs; pools are there:
   </para>
<screen>
&prompt.cephuser;rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Configure &ganesha;</title>
   <para>
    Check that both &ganesha; nodes have the file
    <filename>/etc/ganesha/ganesha.conf</filename> installed. Add the
    following blocks, if they do not exist yet, to the configuration file in
    order to enable RADOS as the recovery backend of &ganesha;.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   You can find out the values for <replaceable>rados_pool</replaceable> and
   <replaceable>pool_namespace</replaceable> by checking the already existing line in
   the configuration of the form:</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   The value for <replaceable>nodeid</replaceable> option corresponds to the FQDN of
   the machine, and <replaceable>UserId</replaceable> and <replaceable>Ceph_Conf</replaceable>
   options value can be found in the already existing
   <replaceable>RADOS_URLS</replaceable> block.
   </para>
   <para>
    Because legacy versions of NFS prevent us from lifting the grace period
    early and therefore prolong a server restart, we disable options for NFS
    prior to version 4.2. We also disable most of the &ganesha; caching as
    &ceph; libraries do aggressive caching already.
   </para>
   <para>
    The 'rados_cluster' recovery back-end stores its info in &rados; objects.
    Although it is not a lot of data, we want it highly available. We use the
    &cephfs; metadata pool for this purpose, and declare a new 'ganesha'
    namespace in it to keep it distinct from &cephfs; objects.
   </para>
   <note>
    <title>Cluster Node IDs</title>
    <para>
     Most of the configuration is identical between the two hosts, however the
     <option>nodeid</option> option in the 'RADOS_KV' block needs to be a
     unique string for each node. By default, &ganesha; sets
     <option>nodeid</option> to the host name of the node.
    </para>
    <para>
     If you need to use different fixed values other than host names, you can
     for example set <option>nodeid = 'a'</option> on one node and
     <option>nodeid = 'b'</option> on the other one.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Populate the Cluster Grace Database</title>
   <para>
    We need to verify that all of the nodes in the cluster know about each
    other. This done via a &rados; object that is shared between the hosts.
    &ganesha; uses this object to communicate the current state with regard to
    a grace period.
   </para>
   <para>
    The <package>nfs-ganesha-rados-grace</package> package contains a command
    line tool for querying and manipulating this database. If the package is
    not installed on at least one of the nodes, install it with
   </para>
<screen>
&prompt.root;zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    We will use the command to create the DB and add both
    <option>nodeid</option>s. In our example, the two &ganesha; nodes are
    named <literal>ses6min1.example.com</literal> and
    <literal>ses6min2.example.com</literal> On one of the &ganesha; hosts, run
   </para>
<screen>
&prompt.cephuser;ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
&prompt.cephuser;ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
&prompt.cephuser;ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    This creates the grace database and adds both 'ses6min1.example.com' and
    'ses6min2.example.com' to it. The last command dumps the current state.
    Newly added hosts are always considered to be enforcing the grace period so
    they both have the 'E' flag set. The 'cur' and 'rec' values show the
    current and recovery epochs, which is how we keep track of what hosts are
    allowed to perform recovery and when.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Restart &ganesha; Services</title>
   <para>
    On both &ganesha; nodes, restart the related services:
   </para>
<screen>
&prompt.root;systemctl restart nfs-ganesha.service
</screen>
   <para>
    After the services are restarted, check the grace database:
   </para>
<screen>
&prompt.cephuser;ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>Cleared the 'E' Flag</title>
    <para>
     Note that both nodes have cleared their 'E' flags, indicating that they
     are no longer enforcing the grace period and are now in normal operation
     mode.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Conclusion</title>
   <para>
    After you complete all the preceding steps, you can mount the exported NFS
    from either of the two &ganesha; servers, and perform normal NFS operations
    against them.
   </para>
   <para>
    Our example configuration assumes that if one of the two &ganesha; servers
    goes down, you will restart it manually within 5 minutes. After 5 minutes,
    the &mds; may cancel the session that the &ganesha; client held and all of
    the state associated with it. If the session’s capabilities get cancelled
    before the rest of the cluster goes into the grace period, the server’s
    clients may not be able to recover all of their state.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>More Information</title>

  <para>
   More information can be found in <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
