<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.as.ganesha">
<!-- ============================================================== -->
 <title>Installation of &ganesha;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; provides an NFS access to either the &rgw; or the &cephfs;.
  In &productname; 5, NFS versions 3 and 4 are supported. &ganesha;
  runs in the user instead of the kernel space and directly interacts
  with the &rgw; or &cephfs;.
 </para>
 <sect1 xml:id="sec.as.ganesha.preparation">
  <title>Preparation</title>
  <sect2 xml:id="sec.as.ganesha.preparation.general">
   <title>General Information</title>
   <para>
    To successfully deploy &ganesha;, you need to add a
    <literal>role-ganesha</literal> to your
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. For details,
    see <xref linkend="policy.configuration" />. &ganesha; also
    needs either a <literal>role-rgw</literal> or a <literal>role-mds</literal>
    present in the <filename>policy.cfg</filename>.
   </para>
   <para>
    Although it is possible to install and run the &ganesha; server on an
    already existing &ceph; node, we recommend running it on a dedicated host
    with access to the &ceph; cluster. The client hosts are typically not
    part of the cluster, but they need to have network access to the &ganesha;
    server.
   </para>
   <para>
    To enable the &ganesha; server at any point after the initial installation,
    add the <literal>role-ganesha</literal> to the <filename>policy.cfg</filename>
    and re-run at least &deepsea; stages 2 and 4. For details,
    see <xref linkend="ceph.install.stack" />.
   </para>
   <para>
    &ganesha; is configured via the file <filename>/etc/ganesha/ganesha.conf</filename>
    that exists on the &ganesha; node. However, this file is overwritten each time
    &deepsea; stage 4 is executed. Therefore we recommend to edit the template
    used by &salt; which is the file
    <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>
    on the &smaster;. For details about the configuration file, see
    <xref linkend="ceph.nfsganesha.config" />.
   </para>
  </sect2>
  <sect2 xml:id="sec.as.ganesha.preparation.requirements">
   <title>Summary of Requirements</title>
   <para>
    The following requirements have to be met before &deepsea; stages 2
    and 4 can be executed to install &ganesha;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      At least one node needs to be assigned the <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      You can define only one <literal>role-ganesha</literal> per minion.
     </para>
    </listitem>
    <listitem>
     <para>
      &ganesha; needs either a &rgw; or &cephfs; to work.
     </para>
    </listitem>
    <listitem>
     <para>
      If &ganesha; is supposed to use the &rgw; to interface with the cluster,
      the <filename>/srv/pillar/ceph/rgw.sls</filename> on the &smaster;
      has to be populated.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.basic_example">
  <title>Example Installation</title>
  <para>
   This procedure provides an example installation that uses both the
   &rgw; and &cephfs; File System Abstraction Layers (FSAL) of &ganesha;.
  </para>
  <procedure>
   <step>
    <para>
     If you have not done so, execute &deepsea; stages 0 and 1 before
     continuing with this procedure.
    </para>
    <screen>&prompt.root;<command>salt-run</command> state.orch ceph.stage.0
&prompt.root;<command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     After having executed stage 1 of &deepsea;, edit the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and add
     the line
    </para>
    <screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Replace <replaceable>NODENAME</replaceable> with the name of a node
     in your cluster.
    </para>
    <para>
     Also make sure that a <literal>role-mds</literal> and a <literal>role-rgw</literal>
     is assigned.
    </para>
   </step>
   <step>
    <para>
     Create the file <filename>/srv/pillar/ceph/rgw.sls</filename> and insert
     the following content:
    </para>
    <screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     These users are later created as &rgw; users and API keys are generated.
     On the &rgw; node, you can later run <command>radosgw-admin user ls</command> to
     list all created users and <command>radosgw-admin user info --uid=demo</command>
     to obtain details about single users.
    </para>
    <para>
     &deepsea; makes sure that &rgw; and &ganesha; both receive the
     credentials of all users listed in the <literal>rgw</literal>
     section of the <filename>rgw.sls</filename>.
    </para>
    <para>
     The exported NFS uses these user names on the first level of the file
     system, in this example the paths <filename>/demo</filename> and
     <filename>/demo1</filename> would be exported.
    </para>
   </step>
   <step>
    <para>
     Execute at least stages 2 and 4 of &deepsea;. Running stage 3
     in between is recommended.
    </para>
<screen>&prompt.root;<command>salt-run</command> state.orch ceph.stage.2
&prompt.root;<command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
&prompt.root;<command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verify that &ganesha; is working by mounting the NFS share from a client
     node:
    </para>
    <screen>&prompt.root;<command>mount</command> -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
&prompt.root;<command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> should contain all exported paths. Folders
     for &cephfs; and both &rgw; users should exist. For each bucket a user
     owns, a path <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>
     would be exported.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.ha-ap">
  <title>High Availability Active-Passive Configuration</title>
  <para>
   This section provides an example of how to set up a 2 node active-passive configuration
   of &ganesha; servers. The setup uses requires the &sle; &hasi;. The
   2 nodes are called <systemitem class="domainname">earth</systemitem> and
   <systemitem class="domainname">mars</systemitem>.
  </para>
  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://www.suse.com/documentation/sle-ha-12/" />.
  </para>
  <para>
   In this setup <systemitem class="domainname">earth</systemitem> has the
   IP address <systemitem class="ipaddress">192.168.1.1</systemitem> and
   <systemitem class="domainname">mars</systemitem> has the address
   <systemitem class="ipaddress">192.168.1.2</systemitem>.
  </para>
  <para>
   Additionally 2 floating virtual IP addresses are used, allowing clients
   to connect to the service independent of which physical node it is running on.
   <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
   administration with Hawk2 and <systemitem class="ipaddress">192.168.2.1</systemitem>
   is used exclusively for the NFS exports.
  </para>
  <para>
   The following procedure describes the example installation. More details
   can be found at <link xlink:href=
   "https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html" />.
  </para>
  <procedure xml:id="proc.as.ganesha.ha-ap">
   <step>
    <para>
     Register the &sle; &hasi; on <systemitem class="domainname">earth</systemitem>
     and <systemitem class="domainname">mars</systemitem>.
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
    </screen>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
    </screen>
   </step>
   <step>
    <para>
     Run &deepsea; stages 0 and 1 on the &smaster;.
    </para>
    <screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1
    </screen>
   </step>
   <step>
    <para>
     Assign the nodes <systemitem class="domainname">earth</systemitem> and
     <systemitem class="domainname">mars</systemitem> the <literal>role-ganesha</literal>
     in the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
    </para>
    <screen>
role-ganesha/cluster/earth*.sls
role-ganesha/cluster/bob*.sls
    </screen>
   </step>
   <step>
    <para>
     Run &deepsea; stages 3 and 4 on the &smaster;.
    </para>
    <screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4
    </screen>
   </step>
   <step>
    <para>
     Install <package>ha-cluster-bootstrap</package> on both nodes:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap
    </screen>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap
    </screen>
   </step>
   <step>
    <para>
     Init the cluster on <systemitem class="domainname">earth</systemitem>
     and let <systemitem class="domainname">mars</systemitem> join:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap
    </screen>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@mars # </prompt><command>zypper</command> in ha-cluster-join -c earth
    </screen>
   </step>
   <step>
    <para>
     Check the status of cluster. You should see two nodes added in cluster.
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>crm</command> status
    </screen>
   </step>
   <step>
    <para>
     Disable the automatic start of the &ganesha; service at boot time on both nodes:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>systemctl</command> disable nfs-ganesha
    </screen>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@mars # </prompt><command>systemctl</command> disable nfs-ganesha
    </screen>
   </step>
   <step>
    <para>
     Start the <command>crm</command> shell on <systemitem class="domainname">earth</systemitem>:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>crm</command> configure
    </screen>
    <para>
     The next commands are executed in the crm shell.
    </para>
   </step>
   <step>
    <para>
     On the crm shell on <systemitem class="domainname">earth</systemitem>
     execute the following commands to configure the resource for &ganesha;
     daemons as clone of systemd resource type.
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>commit
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>status
    2 nodes configured 
    2 resources configured

    Online: [ earth mars ]
              
    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]
    </screen>
   </step>
   <step>
    <para>
     The initial installation creates an administrative virtual IP address
     for Hawk2. Although you could use this IP address for your NFS exports
     too, create another one exclusively for NFS exports. This makes it easier
     to apply security restrictions later. Use the following commands in the
     crm shell: 
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)# </prompt>status
Online: [ alice bob  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ alice bob ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started alice
    </screen>
   </step>
   <step>
    <para>
     To set up relation between &ganesha; server and floating Virtual IP, we
     use colocation and ordering. 
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
    </screen>
   </step>
   <step>
    <para>
     You can use the <command>mount</command> command from the client to ensure
     that cluster setup is complete:
    </para>
    <screen>
&prompt.root;<command>mount</command> -t nfs -v -o nfsvers=4 192.168.2.1:/ /mnt
    </screen>
   </step>
  </procedure>
  <sect2 xml:id="sec.as.ganesha.ha-ap.cleanup">
   <title>Cleanup Resources</title>
   <para>
    In event of &ganesha; failure at one of the node, for example
    <systemitem class="domainname">earth</systemitem>, fix the issue and
    cleanup the resource. Only when the resource is cleaned up, the resource
    can fail back to <systemitem class="domainname">earth</systemitem> in case
    &ganesha; fails at <systemitem class="domainname">mars</systemitem>. 
   </para>
   <para>
    To clean up the resource:
   </para>
   <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt xmlns='http://docbook.org/ns/docbook'>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth
   </screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.info">
  <title>More Information</title>
  <para>
   More information can be found in <xref linkend="cha.ceph.nfsganesha" />.
  </para>
 </sect1>
</chapter>
