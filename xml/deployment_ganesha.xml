<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-as-ganesha">
<!-- ============================================================== -->
 <title>Installation of &ganesha;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; provides NFS access to either the &rgw; or the &cephfs;. In
  &productname; &productnumber;, NFS versions 3 and 4 are supported. &ganesha; runs in the
  user space instead of the kernel space and directly interacts with the &rgw;
  or &cephfs;.
 </para>
 <warning>
  <title>Cross Protocol Access</title>
  <para>
   Native &cephfs; and NFS clients are not restricted by file locks obtained
   via Samba, and vice-versa. Applications that rely on cross protocol file
   locking may experience data corruption if &cephfs; backed Samba share paths
   are accessed via other means.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparation</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>General Information</title>
   <para>
    To successfully deploy &ganesha;, you need to add a
    <literal>role-ganesha</literal> to your
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. For details,
    see <xref linkend="policy-configuration"/>. &ganesha; also needs either a
    <literal>role-rgw</literal> or a <literal>role-mds</literal> present in the
    <filename>policy.cfg</filename>.
   </para>
   <para>
    Although it is possible to install and run the &ganesha; server on an
    already existing &ceph; node, we recommend running it on a dedicated host
    with access to the &ceph; cluster. The client hosts are typically not part
    of the cluster, but they need to have network access to the &ganesha;
    server.
   </para>
   <para>
    To enable the &ganesha; server at any point after the initial installation,
    add the <literal>role-ganesha</literal> to the
    <filename>policy.cfg</filename> and re-run at least &deepsea; stages 2 and
    4. For details, see <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    &ganesha; is configured via the file
    <filename>/etc/ganesha/ganesha.conf</filename> that exists on the &ganesha;
    node. However, this file is overwritten each time &deepsea; stage 4 is
    executed. Therefore we recommend to edit the template used by &salt;, which
    is the file
    <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> on the
    &smaster;. For details about the configuration file, see
    <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Summary of Requirements</title>
   <para>
    The following requirements need to be met before &deepsea; stages 2 and 4
    can be executed to install &ganesha;:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      At least one node needs to be assigned the
      <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      You can define only one <literal>role-ganesha</literal> per minion.
     </para>
    </listitem>
    <listitem>
     <para>
      &ganesha; needs either an &ogw; or &cephfs; to work.
     </para>
    </listitem>
    <listitem>
     <para>
      If &ganesha; is supposed to use the &rgw; to interface with the cluster,
      the <filename>/srv/pillar/ceph/rgw.sls</filename> on the &smaster; needs
      to be populated.
     </para>
    </listitem>
    <listitem>
     <para>
      The kernel based NFS needs to be disabled on minions with the
      <literal>role-ganesha</literal> role.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Example Installation</title>

  <para>
   This procedure provides an example installation that uses both the &rgw; and
   &cephfs; File System Abstraction Layers (FSAL) of &ganesha;.
  </para>

  <procedure>
   <step>
    <para>
     If you have not done so, execute &deepsea; stages 0 and 1 before
     continuing with this procedure.
    </para>
<screen>&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     After having executed stage 1 of &deepsea;, edit the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and add the
     line
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Replace <replaceable>NODENAME</replaceable> with the name of a node in
     your cluster.
    </para>
    <para>
     Also make sure that a <literal>role-mds</literal> and a
     <literal>role-rgw</literal> are assigned.
    </para>
   </step>
   <step>
    <para>
     Create a file with '.yml' extension in the
     <filename>/srv/salt/ceph/rgw/users/users.d</filename> directory and insert
     the following content:
    </para>
<screen>
- { uid: "demo", name: "Demo", email: "demo@demo.nil" }
- { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }
</screen>
    <para>
     These users are later created as &rgw; users, and API keys are generated.
     On the &rgw; node, you can later run <command>radosgw-admin user
     list</command> to list all created users and <command>radosgw-admin user
     info --uid=demo</command> to obtain details about single users.
    </para>
    <para>
     &deepsea; makes sure that &rgw; and &ganesha; both receive the credentials
     of all users listed in the <literal>rgw</literal> section of the
     <filename>rgw.sls</filename>.
    </para>
    <para>
     The exported NFS uses these user names on the first level of the file
     system, in this example the paths <filename>/demo</filename> and
     <filename>/demo1</filename> would be exported.
    </para>
   </step>
   <step>
    <para>
     Execute at least stages 2 and 4 of &deepsea;. Running stage 3 in between
     is recommended.
    </para>
<screen>&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.2
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verify that &ganesha; is working by mounting the NFS share from a client
     node:
    </para>
<screen>&prompt.root;<command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
&prompt.root;<command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> should contain all exported paths. Directories
     for &cephfs; and both &rgw; users should exist. For each bucket a user
     owns, a path
     <filename>/mnt/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>
     would be exported.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>High Availability Active-Passive Configuration</title>

  <para>
   This section provides an example of how to set up a two-node active-passive
   configuration of &ganesha; servers. The setup requires the &sle; &hasi;. The
   two nodes are called &si.earth; and &si.mars;.
  </para>

  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Basic Installation</title>
   <para>
    In this setup &si.earth; has the IP address
    <systemitem class="ipaddress">192.168.1.1</systemitem> and &si.mars; has
    the address <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Additionally, two floating virtual IP addresses are used, allowing clients
    to connect to the service independent of which physical node it is running
    on. <systemitem class="ipaddress">192.168.1.10</systemitem> is used for
    cluster administration with Hawk2 and
    <systemitem class="ipaddress">192.168.2.1</systemitem> is used exclusively
    for the NFS exports. This makes it easier to apply security restrictions
    later.
   </para>
   <para>
    The following procedure describes the example installation. More details
    can be found at
    <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Prepare the &ganesha; nodes on the &smaster;:
     </para>
     <substeps>
      <step>
       <para>
        Run &deepsea; stages 0 and 1.
       </para>
<screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assign the nodes &si.earth; and &si.mars; the
        <literal>role-ganesha</literal> in the
        <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Run &deepsea; stages 2 to 4.
       </para>
<screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.2
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Register the &sle; &hasi; on &si.earth; and &si.mars;.
     </para>
<screen>
&prompt.root;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Install <package>ha-cluster-bootstrap</package> on both nodes:
     </para>
<screen>&prompt.root;<command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialize the cluster on &si.earth;:
       </para>
<screen>&prompt.earth;<command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Let &si.mars; join the cluster:
       </para>
<screen>&prompt.mars;<command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Check the status of the cluster. You should see two nodes added to the
      cluster:
     </para>
<screen>&prompt.earth;<command>crm</command> status</screen>
    </step>
    <step>
     <para>
      On both nodes, disable the automatic start of the &ganesha; service at
      boot time:
     </para>
<screen>&prompt.root;<command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Start the <command>crm</command> shell on &si.earth;:
     </para>
<screen>&prompt.earth;<command>crm</command> configure</screen>
     <para>
      The next commands are executed in the crm shell.
     </para>
    </step>
    <step>
     <para>
      On &si.earth;, run the crm shell to execute the following commands to
      configure the resource for &ganesha; daemons as clone of systemd resource
      type:
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>commit
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Create a primitive IPAddr2 with the crm shell:
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt xmlns="http://docbook.org/ns/docbook">crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      To set up a relationship between the &ganesha; server and the floating
      Virtual IP, we use collocation and ordering.
     </para>
<screen>
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Use the <command>mount</command> command from the client to ensure that
      cluster setup is complete:
     </para>
<screen>&prompt.root;<command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Clean Up Resources</title>
   <para>
    In the event of an &ganesha; failure at one of the node, for example
    &si.earth;, fix the issue and clean up the resource. Only after the
    resource is cleaned up can the resource fail back to &si.earth; in case
    &ganesha; fails at &si.mars;.
   </para>
   <para>
    To clean up the resource:
   </para>
<screen>&prompt.earth;<command>crm</command> resource cleanup nfs-ganesha-clone earth
&prompt.earth;<command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Setting Up Ping Resource</title>
   <para>
    It may happen that the server is unable to reach the client because of a
    network issue. A ping resource can detect and mitigate this problem.
    Configuring this resource is optional.
   </para>
   <procedure>
    <step>
     <para>
      Define the ping resource:
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> is a list of IP addresses separated by space
      characters. The IP addresses will be pinged regularly to check for
      network outages. If a client must always have access to the NFS server,
      add it to <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Create a clone:
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      The following command creates a constraint for the &ganesha; service. It
      forces the service to move to another node when
      <literal>host_list</literal> is unreachable.
     </para>
<screen><prompt xmlns="http://docbook.org/ns/docbook">crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>&ganesha; HA and &deepsea;</title>
   <para>
    &deepsea; does not support configuring &ganesha; HA. To prevent &deepsea;
    from failing after &ganesha; HA was configured, exclude starting and
    stopping the &ganesha; service from &deepsea; Stage 4:
   </para>
   <procedure>
    <step>
     <para>
      Copy <filename>/srv/salt/ceph/ganesha/default.sls</filename> to
      <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Remove the <literal>.service</literal> entry from
      <filename>/srv/salt/ceph/ganesha/ha.sls</filename> so that it looks as
      follows:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Add the following line to
      <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>More Information</title>

  <para>
   More information can be found in <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
