<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.tiered">
<!-- ============================================================== -->
 <title>Cache Tiering</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES3</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In the context of &ceph;, <quote>tiered storage</quote> is implemented as a
  cache tier.
 </para>
 <para>
  A <emphasis>cache tier</emphasis> provides &ceph; clients with better I/O
  performance for a subset of the data stored in a backing storage tier. Cache
  tiering involves creating a pool of relatively fast/expensive storage devices
  (for example SSD drives) configured to act as a cache tier, and a backing
  pool of either erasure-coded and/or relatively slower and cheaper devices
  configured to act as an storage tier. The &ceph; objecter handles where to
  place the objects and the tiering agent determines when to flush objects from
  the cache to the backing storage tier.
 </para>
 <sect1>
  <title>When to Use Cache Tiering</title>

  <para>
   Consider using cache tiering in the following cases:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     You need to access erasure coded pools via RADOS block device (RBD).
    </para>
   </listitem>
   <listitem>
    <para>
     You need to access erasure coded pools via iSCSI as it inherits the
     limitations of RBD. For more information on iSCSI, refer to
     <xref linkend="cha.ceph.iscsi"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     You have a limited number of high performance storage and a large
     collection of low performance storage, and need to access the stored data
     faster.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.ceph.tiered.cachemodes">
  <title>Cache Modes</title>

  <para>
   The cache tiering agent handles the migration of data between the cache tier
   and the backing storage tier. Administrators have the ability to configure
   how this migration takes place. There are two main scenarios:
  </para>

  <variablelist>
   <varlistentry>
    <term>Write-back Mode</term>
    <listitem>
     <para>
      When administrators configure tiers with write-back mode, &ceph; clients
      write data to the cache tier and receive an ACK from the cache tier. In
      time, the data written to the cache tier migrates to the storage tier and
      gets flushed from the cache tier. Conceptually, the cache tier is
      overlaid <quote>in front</quote> of the backing storage tier. When a
      &ceph; client needs data that resides in the storage tier, the cache
      tiering agent migrates the data to the cache tier on read, then it is
      sent to the &ceph; client. Thereafter, the &ceph; client can perform I/O
      using the cache tier, until the data becomes inactive. This is ideal for
      mutable data such as photo or video editing, transactional data, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Read-only Mode</term>
    <listitem>
     <para>
      When administrators configure tiers with read-only mode, &ceph; clients
      write data to the backing tier. On read, &ceph; copies the requested
      objects from the backing tier to the cache tier. Stale objects get
      removed from the cache tier based on the defined policy. This approach is
      ideal for immutable data such as presenting pictures or videos on a
      social network, DNA data, X-ray imaging, etc., because reading data from
      a cache pool that might contain out-of-date data provides weak
      consistency. Do not use read-only mode for mutable data.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.ceph.tiered.caution">
   <title>A Word of Caution</title>
   <para>
    Cache tiering may <emphasis>degrade</emphasis> the cluster performance for
    specific workloads. The following points show some of its aspects you need
    to consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Workload dependent</emphasis>: Whether a cache will improve
      performance is dependent on the workload. Because there is a cost
      associated with moving objects into or out of the cache, it can be more
      effective when most of the requests touch a small number of objects. The
      cache pool should be large enough to capture the working set for your
      workload to avoid thrashing.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Difficult to benchmark</emphasis>: Most performance benchmarks
      may show low performance with cache tiering. The reason is that they
      request a big set of objects, and it takes a long time for the cache to
      'warm up'.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Usually slower</emphasis>: For workloads that are not suitable
      for cache tiering, performance is often slower than a normal replicated
      pool without cache tiering enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis><systemitem>librados</systemitem> object
      enumeration</emphasis>: If your application is using
      <systemitem>librados</systemitem> directly and relies on object
      enumeration, cache tiering may not work as expected. (This is not a
      problem for &rgw;, RBD, or &cephfs;.)
     </para>
    </listitem>
   </itemizedlist>
   <sect3>
    <title>Known Good Workloads</title>
    <para>
     <emphasis>RGW time-skewed</emphasis>: If the &rgw; workload is such that
     almost all read operations are directed at recently written objects, a
     simple cache tiering configuration that destages recently written objects
     from the cache to the base tier after a configurable period can work well.
    </para>
   </sect3>
   <sect3>
    <title>Known Bad Workloads</title>
    <para>
     The following configurations are known to work poorly with cache tiering.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>RBD with replicated cache and erasure-coded base</emphasis>:
       This is a common request, but usually does not perform well. Even
       workloads with many requests to a small set of objects still send some
       small writes to cold objects, and because small writes are not yet
       supported by the erasure-coded pool, entire (usually 4 MB) objects must
       be migrated into the cache in order to satisfy a small (often 4 KB)
       write.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>RBD with replicated cache and base</emphasis>: RBD with a
       replicated base tier does better than when the base is erasure coded,
       but it is still highly dependent on the character and amount of requests
       in the workload. The user will need to have a good understanding of
       their workload and will need to tune the cache tiering parameters
       carefully.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
 <sect1>
  <title>&ceph; Pools in the Context of Tiered Storage</title>

  <para>
   For general information on pools, see <xref linkend="ceph.pools"/>.
  </para>

  <sect2>
   <title>The Backing Storage Pool</title>
   <para>
    Either a standard storage that stores several copies of an object in the
    &ceph; Storage Cluster or storage with erasure coding which is a way to
    store data much more efficiently with a small performance trade-off. For
    erasure coding, see <xref linkend="cha.ceph.erasure"/>.
   </para>
   <para>
    In the context of tiered storage this backing storage pool will also be
    referred to as <quote>cold-storage</quote>.
   </para>
  </sect2>

  <sect2>
   <title>The Cache Pool</title>
   <para>
    For the cache pool you configure standard storage, but you use high
    performance drives in dedicated servers with their own ruleset. See
    <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/crush-map/#placing-different-pools-on-different-osds"/>.
   </para>
   <para>
    In the context of tiered storage this cache pool will also be referred to
    as <emphasis>hot-storage</emphasis>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ses.tiered.storage">
  <title>Setting Up an Example Tiered Storage</title>

  <para>
   This section illustrates how to set up a fast SSD cache tier (hot-storage)
   in front of a standard HDD (cold-storage).
  </para>

  <tip>
   <para>
    The following example is for illustration purposes only and includes a
    setup with one root and one rule for the SSD part residing on a single
    &ceph; node.
   </para>
   <para>
    In the production environment, the cluster setup typically includes more
    root and rule entries for the hot storage, and also mixed nodes with both
    SSDs and SATA disks.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Prepare a host machine with fast drives, such as SSDs. This cluster node
     will act as a fast cache tier.
    </para>
   </step>
   <step>
    <para>
     Turn the machine into a &ceph; node. Install the software and configure
     the host machine as described in
     <xref linkend="ceph.install.ceph-deploy.eachnode"/>. Let us assume that
     its name is <replaceable>node-4</replaceable>.
    </para>
   </step>
   <step>
    <para>
     You need to create 4 OSDs nodes. For this purpose run
     <command>ceph-deploy</command> from the admin server (refer to
     <xref linkend="ceph.install.ceph-deploy.cephdeploy"/>). Remember to
     replace <replaceable>node-4</replaceable> with the actual node name and
     device with the actual device name:
    </para>
<screen>&prompt.cephuser;for d in a b c d; do
  ceph-deploy osd create node-4:device${d}
done</screen>
    <para>
     This may result in an entry like this in the CRUSH map:
    </para>
<screen>[...]
host node-4 {
        id -5  # do not change unnecessarily
        # weight 0.012
        alg straw
        hash 0  # rjenkins1
        item osd.6 weight 0.003
        item osd.7 weight 0.003
        item osd.8 weight 0.003
        item osd.9 weight 0.003
}
[...]</screen>
   </step>
   <step>
    <para>
     Edit the CRUSH map for the hot-storage pool mapped to the OSDs backed by
     the fast SSD drives. Define a second hierarchy with a root node for the
     SSDs (as <quote>root ssd</quote>). Additionally, change the weight and a
     CRUSH rule for the SSDs. For more information on CRUSH map, see
     <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/crush-map/"/>.
    </para>
    <para>
     Edit the CRUSH map directly with command line tools such as
     <command>getcrushmap</command> and <command>crushtool</command>:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Retrieve the current map and save it as <filename>c.map</filename>:
      </para>
<screen>sudo ceph osd getcrushmap -o c.map</screen>
     </step>
     <step>
      <para>
       Decompile <filename>c.map</filename> and save it as
       <filename>c.txt</filename>:
      </para>
<screen>&prompt.cephuser;crushtool -d c.map -o c.txt</screen>
     </step>
     <step>
      <para>
       Edit <filename>c.txt</filename>:
      </para>
<screen>[...]
host node-4 {
        id -5  # do not change unnecessarily
        # weight 4.000
        alg straw
        hash 0  # rjenkins1
        item osd.6 weight 1.000
        item osd.7 weight 1.000
        item osd.8 weight 1.000
        item osd.9 weight 1.000
}
root ssd {    # newly added root for the SSD hot-storage
        id -6
        alg straw
        hash 0
        item node-4 weight 4.00
}
rule ssd {
        ruleset 4
        type replicated
        min_size 0
        max_size 4
        step take ssd
        step chooseleaf firstn 0 type host
        step emit
}
[...]</screen>
     </step>
     <step>
      <para>
       Compile the edited <filename>c.txt</filename> file and save it as
       <filename>ssd.map</filename>:
      </para>
<screen>&prompt.cephuser;crushtool -c c.txt -o ssd.map</screen>
     </step>
     <step>
      <para>
       Finally install <filename>ssd.map</filename> as the new CRUSH map:
      </para>
<screen>sudo ceph osd setcrushmap -i ssd.map</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Create the hot-storage pool to be used for cache tiering. Use the new
     'ssd' rule for it:
    </para>
<screen>sudo ceph osd pool create hot-storage 100 100 replicated ssd</screen>
   </step>
   <step>
    <para>
     Create the cold-storage pool using the default 'replicated_ruleset' rule:
    </para>
<screen>sudo ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</screen>
   </step>
   <step>
    <para>
     Then setting up a cache tier involves associating a backing storage pool
     with a cache pool, in this case cold-storage (= storage pool) with
     hot-storage (= cache pool):
    </para>
<screen>sudo ceph osd tier add cold-storage hot-storage</screen>
   </step>
   <step>
    <para>
     To set the cache mode to <quote>writeback</quote>, execute the following:
    </para>
<screen>sudo ceph osd tier cache-mode hot-storage writeback</screen>
    <para>
     For more information about cache modes, see
     <xref linkend="sec.ceph.tiered.cachemodes"/>.
    </para>
    <para>
     Writeback cache tiers overlay the backing storage tier, so they require
     one additional step: you must direct all client traffic from the storage
     pool to the cache pool. To direct client traffic directly to the cache
     pool, execute the following for example:
    </para>
<screen>sudo ceph osd tier set-overlay cold-storage hot-storage</screen>
   </step>
  </procedure>

  <sect2 xml:id="cache.tier.configure">
   <title>Configuring a Cache Tier</title>
   <para>
    There are several options you can use to configure cache tiers. Use the
    following syntax:
   </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <sect3>
    <title>Target Size and Type</title>
    <para>
     &ceph;'s production cache tiers use a Bloom Filter for the
     <option>hit_set_type</option>:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_type bloom</screen>
    <para>
     The <option>hit_set_count</option> and <option>hit_set_period</option>
     define how much time each HitSet should cover, and how many such HitSets
     to store.
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_count 12
sudo ceph osd pool set <replaceable>cachepool</replaceable> hit_set_period 14400
sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes 1000000000000</screen>
    <note>
     <para>
      A larger <option>hit_set_count</option> results in more RAM consumed by
      the <systemitem class="process">ceph-osd</systemitem> process.
     </para>
    </note>
    <para>
     The <option>min_read_recency_for_promote</option> defines how many HitSets
     to check for the existence of an object when handling a read operation.
     The checking result is used to decide whether to promote the object
     asynchronously. Its value should be between 0 and
     <option>hit_set_count</option>. If set to 0, the object is always
     promoted. If set to 1, the current HitSet is checked. And if this object
     is in the current HitSet, it is promoted, otherwise not. For the other
     values, the exact number of archive HitSets are checked. The object is
     promoted if the object is found in any of the most recent
     <option>min_read_recency_for_promote</option> HitSets.
    </para>
    <para>
     You can set a similar parameter
     <option>min_write_recency_for_promote</option> for the write operation:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> min_read_recency_for_promote 2
sudo ceph osd pool set <replaceable>cachepool</replaceable> min_write_recency_for_promote 2</screen>
    <note>
     <para>
      The longer the period and the higher the
      <option>min_read_recency_for_promote</option> and
      <option>min_write_recency_for_promote</option> values, the more RAM the
      <systemitem class="process">ceph-osd</systemitem> daemon consumes. In
      particular, when the agent is active to flush or evict cache objects, all
      <option>hit_set_count</option> HitSets are loaded into RAM.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cache Sizing</title>
    <para>
     The cache tiering agent performs two main functions:
    </para>
    <variablelist>
     <varlistentry>
      <term>Flushing</term>
      <listitem>
       <para>
        The agent identifies modified (or dirty) objects and forwards them to
        the storage pool for long-term storage.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Evicting</term>
      <listitem>
       <para>
        The agent identifies objects that have not been modified (or clean) and
        evicts the least recently used among them from the cache.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <sect4 xml:id="cache.tier.config.absizing">
     <title>Absolute Sizing</title>
     <para>
      The cache tiering agent can flush or evict objects based upon the total
      number of bytes or the total number of objects. To specify a maximum
      number of bytes, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes <replaceable>num_of_bytes</replaceable></screen>
     <para>
      To specify the maximum number of objects, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> target_max_objects <replaceable>num_of_bytes</replaceable></screen>
     <note>
      <para>
       &ceph; is not able to determine the size of a cache pool automatically,
       so the configuration on the absolute size is required here, otherwise
       the flush/evict will not work. If you specify both limits, the cache
       tiering agent will begin flushing or evicting when either threshold is
       triggered.
      </para>
     </note>
     <note>
      <para>
       All client requests will be blocked only when
       <option>target_max_bytes</option> or <option>target_max_objects</option>
       reached.
      </para>
     </note>
    </sect4>
    <sect4 xml:id="cache.tier.config.relsizing">
     <title>Relative Sizing</title>
     <para>
      The cache tiering agent can flush or evict objects relative to the size
      of the cache pool (specified by <option>target_max_bytes</option> /
      <option>target_max_objects</option> in
      <xref linkend="cache.tier.config.absizing"/>). When the cache pool
      consists of a certain percentage of modified (or dirty) objects, the
      cache tiering agent will flush them to the storage pool. To set the
      <option>cache_target_dirty_ratio</option>, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_ratio <replaceable>0.0...1.0</replaceable></screen>
     <para>
      For example, setting the value to 0.4 will begin flushing modified
      (dirty) objects when they reach 40% of the cache pool's capacity:
     </para>
<screen>sudo ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</screen>
     <para>
      When the dirty objects reach a certain percentage of its capacity, flush
      dirty objects with a higher speed. Use
      <option>cache_target_dirty_high_ratio</option>:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_high_ratio <replaceable>0.0..1.0</replaceable></screen>
     <para>
      When the cache pool reaches a certain percentage of its capacity, the
      cache tiering agent will evict objects to maintain free capacity. To set
      the <option>cache_target_full_ratio</option>, execute the following:
     </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_target_full_ratio <replaceable>0.0..1.0</replaceable></screen>
    </sect4>
   </sect3>
   <sect3>
    <title>Cache Age</title>
    <para>
     You can specify the minimum age of an object before the cache tiering
     agent flushes a recently modified (or dirty) object to the backing storage
     pool:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_min_flush_age <replaceable>num_of_seconds</replaceable></screen>
    <para>
     You can specify the minimum age of an object before it will be evicted
     from the cache tier:
    </para>
<screen>sudo ceph osd pool set <replaceable>cachepool</replaceable> cache_min_evict_age <replaceable>num_of_seconds</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
