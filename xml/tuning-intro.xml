<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="tuning-intro">
 <title>Introduction to Tuning &productname; Clusters </title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   Tuning a distributed cluster is a foray into the use of the
   scientific method, backed with iterative testing. By taking a holistic
   look at the cluster and then delving into all the components, it is
   possible to achieve dramatic improvements. Over the
   course of the work that contributed to the authoring of this guide,
   the authors have seen performance more than double in some specific cases.
 </para>
 <para>
   This guide is intended to assist the reader in understanding the what
   and how of tuning a &productname; cluster. There are topics
   that are beyond the scope of this guide, and it is expected that
   there are further tweaks that may be performed to an individual cluster
   in order to achieve optimum performance in a particular end user environment.
 </para>
 <para>
   This reference guide is targeted at architects and administrators who need
   to tune their &productname; cluster for optimal performance.
   Familiarity with Linux and &ceph; are assumed.
 </para>
 <sect1 xml:id="tuning-philosophy">
   <title>Philosophy of Tuning</title>
   <para>
     Tuning requires looking at the entire system being tuned and approaching
     the process with scientific rigor. An exhaustive approach requires
     taking an initial baseline and altering a single variable at a time,
     measuring the result, and then reverting it back to default while
     moving on to the next tuning parameter. At the end of that process,
     it is then possible to examine the results, whether it be increased
     throughput, reduced latency, reduced CPU consumption, and then
     decide which are likely candidates for combining for additive benefit.
     This second phase should also be iterated through in the same fashion
     as the first. This general process would be continued until all possible
     combinations were tried and the optimal settings discovered.
   </para>
   <para>
     Unfortunately, few have the time to perform such an exhaustive
     effort. This being reality, it is possible to utilize some knowledge
     and begin an iterative process of combining and measuring well-known
     candidates for performance improvements and measuring the resulting
     changes. That is the process followed during the research that produced
     this work.
   </para>
 </sect1>
 <sect1 xml:id="tuning-process">
   <title>The Process</title>
   <para>
     A proper process is required for effective tuning to occur.
     The tenets of this process are:
   </para>
   <variablelist>
    <varlistentry>
     <term>Measure</term>
     <listitem>
      <para>
        Start with a baseline and measure the same way after each iteration.
        Make sure you are measuring all relevant dimensions. Discovering
        that you are CPU-bound only after working through multiple iterations
        invalidates all the time spent on the iterations.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Document</term>
     <listitem>
      <para>
        Document all results for future analysis. Patterns may not be evident
        until later review.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Discuss</term>
     <listitem>
      <para>
        When possible, discuss the results you are seeing with others for
        their insights and feedback.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Repeat</term>
     <listitem>
      <para>
        A single measurement is not a guarantee of repeatability.
        Performing the same test multiple times helps to better establish an outcome.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Isolate variables</term>
     <listitem>
      <para>
        Having only a single change affect the environment at a time may cause
        the process to be longer, but it also helps validate the particular
        adjustment being tested.
      </para>
     </listitem>
    </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="tuning-hardware-software">
   <title>Hardware and Software</title>
   <para>
     This work leveraged for &productname; was performed on two models of servers.
     Any results referenced in this guide are from this specific hardware
     environment. Variations of the environment can and will have an
     effect on the results.
   </para>
   <para>
     Storage nodes:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         2U Server
       </para>
       <itemizedlist>
         <listitem>
           <para>
             1x Intel Skylake 6124
           </para>
         </listitem>
         <listitem>
           <para>
             96&nbsp;GB RAM
           </para>
         </listitem>
         <listitem>
           <para>
             Mellanox Dual Port ConnectX-4 100&nbsp;GbE
           </para>
         </listitem>
         <listitem>
           <para>
             12x Intel SSD D3-S4510 960&nbsp;GB
           </para>
         </listitem>
         <listitem>
           <para>
             RAID-1 480&nbsp;GB M.2 Boot Device
           </para>
         </listitem>
       </itemizedlist>
     </listitem>
   </itemizedlist>
   <para>
     Admin, monitor, and protocol gateways:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         1U Server
       </para>
       <itemizedlist>
         <listitem>
           <para>
             1x Intel Skylake 4112
           </para>
         </listitem>
         <listitem>
           <para>
             32&nbsp;GB RAM
           </para>
         </listitem>
         <listitem>
           <para>
             Mellanox Dual Port ConnectX-4 100&nbsp;GbE
           </para>
         </listitem>
         <listitem>
           <para>
             RAID-1 480&nbsp;GB M.2 Boot Device
           </para>
         </listitem>
       </itemizedlist>
     </listitem>
   </itemizedlist>
   <para>
     Switches:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         2x 32-port 100&nbsp;GbE
       </para>
     </listitem>
   </itemizedlist>
   <para>
     Software:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         &productname; &prevproductnumber;
       </para>
     </listitem>
     <listitem>
       <para>
         &sls; 15 SP1
       </para>
     </listitem>
   </itemizedlist>
   <note>
     <para>
       Limited-use subscriptions are provided with &productname;
       as part of the subscription entitlement.
     </para>
   </note>
   <sect2>
     <title>Performance Metrics</title>
     <para>
       The performance of storage is measured on two different, but related
       axes: <literal>latency</literal> and <literal>throughput</literal>. In
       some cases, one is more important than the other. An example of
       throughput being more
       important is that of backup use cases, where throughput is the most
       critical measurement and maximum performance is achieved with larger
       transfer sizes. Conversely, for a high-performance database, latency
       is the most important measurement.
     </para>
     <para>
       <emphasis>Latency</emphasis> is the time from when the request was made to when it was
       completed. This is usually measured in terms of milliseconds. This
       performance can be directly tied to CPU clock speed, the system bus,
       and device performance.
       <emphasis>Throughput</emphasis> is the measure of an amount of data that can be written
       or retrieved within a particular time period. The measure is usually
       in MB/s and GB/s, or perhaps MiB/s and GiB/s.
     </para>
     <para>
       A third measurement that is often referred to is <literal>IOPS</literal>.
       This stands for Input/output Operations Per Second. This
       measure is somewhat ambiguous as the result is reliant on the size of
       the I/O operation, what type (read/write), and details about the I/O
       pattern: fresh write, overwrite, random write, mix of reads and writes.
       While ambiguous, it is still a valid tool to use when measuring
       the changes in the performance of your storage environment. For
       example, it is possible to make adjustments that may not affect the
       latency of a single 4K sequential write operation, but would allow
       many more of the operations to happen in parallel, resulting in a
       change in throughput and IOPS.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-measurement">
   <title>Determining What to Measure</title>
   <para>
     When tuning an environment, it is important to understand the I/O that
     is being tuned for. By properly understanding the I/O pattern, it is
     then possible to match the tests to the environment, resulting in a
     close simulation of the environment.
   </para>
   <para>
     Tools that can be useful for understanding the I/O patterns are:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         <command>iostat</command>
       </para>
     </listitem>
     <listitem>
       <para>
         <command>blktrace</command>, <command>blkparse</command>
       </para>
     </listitem>
     <listitem>
       <para>
         <command>systemtap (stap)</command>
       </para>
     </listitem>
     <listitem>
       <para>
         <command>dtrace</command>
       </para>
     </listitem>
   </itemizedlist>
   <para>
     While discussing these tools is beyond the scope of this document,
     the information they can provide may be very helpful in understanding
     the I/O profile that needs to be tuned.
   </para>
   <sect2>
     <title>Single Thread vs Aggregate IO</title>
     <para>
       Understanding the requirements of the workload in relation to
       whether it needs scale-up or scale-out performance is often a key to
       proper tuning as well, particularly when it comes to tuning the
       hardware architecture and to creating test scenarios that provide
       valid information regarding tuning for the application(s) in question.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-testing-tools-protocol">
   <title>Testing Tools and Protocol</title>
   <para>
     Proper testing involves selection of the right tools. For most performance
     test cases, use of <command>fio</command> is recommended, as it provides a vast array of
     options for constructing test cases. For some use cases, such as S3,
     it may be necessary to use alternative tools to test all phases of I/O.
   </para>
   <para>
     Tools that are commonly used to simulate I/O are:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         <command>fio</command>
       </para>
     </listitem>
     <listitem>
       <para>
         <command>iometer</command>
       </para>
     </listitem>
   </itemizedlist>
   <para>
     When performing testing, it is imperative that sound practices be
     utilized. This involves:
   </para>
   <itemizedlist>
     <listitem>
       <para>
         Pre-conditioning the media.
       </para>
     </listitem>
     <listitem>
       <para>
         Ensuring what you are measuring is what you intend to measure.
       </para>
     </listitem>
     <listitem>
       <para>
         Validate that the results you see make sense.
       </para>
     </listitem>
     <listitem>
       <para>
         Test each component individually and then in aggregate, one
         layer at a time.
       </para>
     </listitem>
   </itemizedlist>
 </sect1>
</chapter>
