<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="tuning-intro">
 <title>Introduction to tuning &productname; clusters</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Tuning a distributed cluster is a foray into the use of the scientific
  method, backed with iterative testing. By taking a holistic look at the
  cluster and then delving into all the components, it is possible to achieve
  dramatic improvements. Over the course of the work that contributed to the
  authoring of this guide, the authors have seen performance more than double
  in some specific cases.
 </para>
 <para>
  This guide is intended to assist the reader in understanding the what and how
  of tuning a &productname; cluster. There are topics that are beyond the scope
  of this guide, and it is expected that there are further tweaks that may be
  performed to an individual cluster in order to achieve optimum performance in
  a particular end user environment.
 </para>
 <para>
  This reference guide is targeted at architects and administrators who need to
  tune their &productname; cluster for optimal performance. Familiarity with
  Linux and &ceph; are assumed.
 </para>
 <sect1 xml:id="tuning-philosophy">
  <title>Philosophy of tuning</title>

  <para>
   Tuning requires looking at the entire system being tuned and approaching the
   process with scientific rigor. An exhaustive approach requires taking an
   initial baseline and altering a single variable at a time, measuring the
   result, and then reverting it back to default while moving on to the next
   tuning parameter. At the end of that process, it is then possible to examine
   the results, whether it be increased throughput, reduced latency, reduced
   CPU consumption, and then decide which are likely candidates for combining
   for additive benefit. This second phase should also be iterated through in
   the same fashion as the first. This general process would be continued until
   all possible combinations were tried and the optimal settings discovered.
  </para>

  <para>
   Unfortunately, few have the time to perform such an exhaustive effort. This
   being reality, it is possible to utilize some knowledge and begin an
   iterative process of combining and measuring well-known candidates for
   performance improvements and measuring the resulting changes. That is the
   process followed during the research that produced this work.
  </para>
 </sect1>
 <sect1 xml:id="tuning-process">
  <title>Tuning process</title>

  <para>
   A proper process is required for effective tuning to occur. The tenets of
   this process are:
  </para>

  <variablelist>
   <varlistentry>
    <term>Measure</term>
    <listitem>
     <para>
      Start with a baseline and measure the same way after each iteration. Make
      sure you are measuring all relevant dimensions. Discovering that you are
      CPU-bound only after working through multiple iterations invalidates all
      the time spent on the iterations.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Document</term>
    <listitem>
     <para>
      Document all results for future analysis. Patterns may not be evident
      until later review.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Discuss</term>
    <listitem>
     <para>
      When possible, discuss the results you are seeing with others for their
      insights and feedback.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Repeat</term>
    <listitem>
     <para>
      A single measurement is not a guarantee of repeatability. Performing the
      same test multiple times helps to better establish an outcome.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Isolate variables</term>
    <listitem>
     <para>
      Having only a single change affect the environment at a time may cause
      the process to be longer, but it also helps validate the particular
      adjustment being tested.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="tuning-hardware-software">
  <title>Hardware and software</title>

  <para>
   This work leveraged for &productname; was performed on two models of
   servers. Any results referenced in this guide are from this specific
   hardware environment. Variations of the environment can and will have an
   effect on the results.
  </para>

  <para>
   Storage nodes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     2U Server
    </para>
    <itemizedlist>
     <listitem>
      <para>
       1x Intel Skylake 6124
      </para>
     </listitem>
     <listitem>
      <para>
       96&nbsp;GB RAM
      </para>
     </listitem>
     <listitem>
      <para>
       Mellanox Dual Port ConnectX-4 100&nbsp;GbE
      </para>
     </listitem>
     <listitem>
      <para>
       12x Intel SSD D3-S4510 960&nbsp;GB
      </para>
     </listitem>
     <listitem>
      <para>
       RAID-1 480&nbsp;GB M.2 Boot Device
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   Admin, monitor, and protocol gateways:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     1U Server
    </para>
    <itemizedlist>
     <listitem>
      <para>
       1x Intel Skylake 4112
      </para>
     </listitem>
     <listitem>
      <para>
       32&nbsp;GB RAM
      </para>
     </listitem>
     <listitem>
      <para>
       Mellanox Dual Port ConnectX-4 100&nbsp;GbE
      </para>
     </listitem>
     <listitem>
      <para>
       RAID-1 480&nbsp;GB M.2 Boot Device
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   Switches:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     2x 32-port 100&nbsp;GbE
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Software:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &productname; &prevproductnumber;
    </para>
   </listitem>
   <listitem>
    <para>
     &sls; 15 SP1
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Limited-use subscriptions are provided with &productname; as part of the
    subscription entitlement.
   </para>
  </note>

  <sect2 xml:id="tuning-performance-metrics">
   <title>Performance metrics</title>
   <para>
    The performance of storage is measured on two different, but related axes:
    <literal>latency</literal> and <literal>throughput</literal>. In some
    cases, one is more important than the other. An example of throughput being
    more important is that of backup use cases, where throughput is the most
    critical measurement and maximum performance is achieved with larger
    transfer sizes. Conversely, for a high-performance database, latency is the
    most important measurement.
   </para>
   <para>
    <emphasis>Latency</emphasis> is the time from when the request was made to
    when it was completed. This is usually measured in terms of milliseconds.
    This performance can be directly tied to CPU clock speed, the system bus,
    and device performance. <emphasis>Throughput</emphasis> is the measure of
    an amount of data that can be written or retrieved within a particular time
    period. The measure is usually in MB/s and GB/s, or perhaps MiB/s and
    GiB/s.
   </para>
   <para>
    A third measurement that is often referred to is <literal>IOPS</literal>.
    This stands for Input/output Operations Per Second. This measure is
    somewhat ambiguous as the result is reliant on the size of the I/O
    operation, what type (read/write), and details about the I/O pattern: fresh
    write, overwrite, random write, mix of reads and writes. While ambiguous,
    it is still a valid tool to use when measuring the changes in the
    performance of your storage environment. For example, it is possible to
    make adjustments that may not affect the latency of a single 4K sequential
    write operation, but would allow many more of the operations to happen in
    parallel, resulting in a change in throughput and IOPS.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-measurement">
  <title>Determining what to measure</title>

  <para>
   When tuning an environment, it is important to understand the I/O that is
   being tuned for. By properly understanding the I/O pattern, it is then
   possible to match the tests to the environment, resulting in a close
   simulation of the environment.
  </para>

  <para>
   Tools that can be useful for understanding the I/O patterns are:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>iostat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>blktrace</command>, <command>blkparse</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>systemtap (stap)</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>dtrace</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   While discussing these tools is beyond the scope of this document, the
   information they can provide may be very helpful in understanding the I/O
   profile that needs to be tuned.
  </para>

  <sect2 xml:id="tuning-single-thread-aggregate-io">
   <title>Single thread vs aggregate IO</title>
   <para>
    Understanding the requirements of the workload in relation to whether it
    needs scale-up or scale-out performance is often a key to proper tuning as
    well, particularly when it comes to tuning the hardware architecture and to
    creating test scenarios that provide valid information regarding tuning for
    the application(s) in question.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-testing-tools-protocol">
  <title>Testing tools and protocol</title>

  <para>
   Proper testing involves selection of the right tools. For most performance
   test cases, use of <command>fio</command> is recommended, as it provides a
   vast array of options for constructing test cases. For some use cases, such
   as S3, it may be necessary to use alternative tools to test all phases of
   I/O.
  </para>

  <para>
   Tools that are commonly used to simulate I/O are:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>fio</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>iometer</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   When performing testing, it is imperative that sound practices be utilized.
   This involves:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Pre-conditioning the media.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensuring what you are measuring is what you intend to measure.
    </para>
   </listitem>
   <listitem>
    <para>
     Validate that the results you see make sense.
    </para>
   </listitem>
   <listitem>
    <para>
     Test each component individually and then in aggregate, one layer at a
     time.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
