<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.inst">
 <title>Cluster Administration</title>
 <para>
  The chapter describes some useful operations that can be performed after the
  cluster is completely deployed and running, like adding nodes, disks.
 </para>
 <sect1 xml:id="storage.bp.inst.cephdeploy_usage">
  <title>Using <command>ceph-deploy</command> on an Already Setup Server</title>

  <para>
   <command>ceph-deploy</command> is a command line utility to easily deploy a
   &ceph; cluster (see <xref linkend="ceph.install.ceph-deploy"/>). After the
   cluster is deployed, you can use <command>ceph-deploy</command> to
   administer the clusters' nodes. You can add OSD nodes, monitor nodes, gather
   authentication keys, or purge a running cluster.
   <command>ceph-deploy</command> has the following general syntax:
  </para>

<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>

  <para>
   A list of selected <command>ceph-deploy</command> subcommands with short
   descriptions follow.
  </para>

  <tip>
   <para>
    Administer &ceph; nodes with <command>ceph-deploy</command> from the admin
    node. Before administering them, always create a new temporary directory
    and <command>cd</command> into it. Then choose one monitor node and gather
    the authentication keys with the <command>gatherkeys</command> subcommand
    from it, and copy the <filename>/etc/ceph/ceph.conf</filename> file from
    the monitor node into the current local directory.
   </para>
<screen>&prompt.cephuser; mkdir ceph_tmp
&prompt.cephuser; cd ceph_tmp
&prompt.cephuser; ceph-deploy gatherkeys ceph_mon_host
&prompt.cephuser; scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
  </tip>

  <variablelist>
   <varlistentry>
    <term>gatherkeys</term>
    <listitem>
     <para>
      Gather authentication keys for provisioning new nodes. It takes host
      names as arguments. It checks for and fetches <literal>client.admin
      keyring</literal>, monitor keyring and
      <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor host.
      These authentication keys are used when new
      <literal>monitors/OSDs/MDS</literal> are added to the cluster.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
     <para>
      <replaceable>hostname</replaceable> is the host name of the monitor from
      where keys are to be pulled.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon add</term>
    <listitem>
     <para>
      Adds a monitor to an existing cluster. It first detects the platform and
      distribution for the target host, and checks if the host name is
      compatible for deployment. It then uses the monitor keyring, ensures
      configuration for new monitor host and adds the monitor to the cluster.
      If the section for the monitor exists, it can define the monitor address
      by the <literal>mon addr</literal> option, otherwise it will fall back by
      resolving the host name to an IP. If <option>--address</option> is used,
      it will override all other options. After adding the monitor to the
      cluster, it gives it some time to start. It then looks for any monitor
      errors, and checks monitor status. Monitor errors arise if the monitor is
      not added in the <option>mon initial members</option> option, if it does
      not exist in <option>monmap</option>, or if neither
      <option>public_addr</option> nor <option>public_network</option> keys
      were defined for monitors. Under such conditions, monitors may not be
      able to form quorum. Monitor status tells if the monitor is up and
      running normally. The status is checked by running <command>ceph daemon
      mon.hostname mon_status</command> on remote end which provides the output
      and returns a Boolean status of what is going on.
      <literal>False</literal> means a monitor that is not fine even if it is
      up and running, while <literal>True</literal> means the monitor is up and
      running correctly.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy mon add <replaceable>host</replaceable>
ceph-deploy mon add <replaceable>host</replaceable> <option>--address <replaceable>IP</replaceable></option></screen>
     <para>
      <replaceable>host</replaceable> is the host name and
      <replaceable>IP</replaceable> is the IP address of the desired monitor
      node.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>osd prepare</term>
    <listitem>
     <para>
      Prepares a directory, disk or drive for a &ceph; OSD. It first checks
      against multiple OSDs getting created and warns about the possibility of
      more than the recommended which would cause issues with max allowed PIDs
      in a system. It then reads the bootstrap-osd key for the cluster or
      writes the bootstrap key if not found. It then uses
      <command>ceph-disk</command> utility’s <command>prepare</command>
      subcommand to prepare the disk and journal and deploy the OSD on the
      desired host. It gives some time to the OSD to settle and checks for any
      possible errors and if found, reports them to the user.
     </para>
     <para>
      Usage:
     </para>
<screen>ceph-deploy osd prepare <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>osd activate</term>
    <listitem>
     <para>
      Activates the OSD prepared using the <command>prepare</command>
      subcommand. It actually uses <command>ceph-disk</command> utility’s
      <command>activate</command> subcommand to activate the OSD with the
      appropriate initialization type based on the distribution. When
      activated, it gives some time to the OSD to start and checks for any
      possible errors and if found, reports to the user. It checks the status
      of the prepared OSD, checks the OSD tree and makes sure the OSDs are up
      and in.
     </para>
     <tip>
      <para>
       <command>osd activate</command> is usually not needed as
       <systemitem>udev</systemitem> rules explicitly trigger "activate" after
       a disk is prepared after <command>osd prepare</command>.
      </para>
     </tip>
     <para>
      Usage:
     </para>
<screen>ceph-deploy osd activate <replaceable>host</replaceable>:<replaceable>disk</replaceable>[<replaceable>journal</replaceable>] ...</screen>
     <tip>
      <para>
       You can use <command>ceph-deploy osd create</command> to join
       <command>prepare</command> and <command>activate</command> functionality
       into one command.
      </para>
     </tip>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw prepare/activate/create</term>
    <listitem>
     <para>
      Find more information in <xref linkend="storage.bp.inst.rgw"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>purge, purgedata, forgetkeys</term>
    <listitem>
     <para>
      You can use the subcommands to completely purge the &ceph; cluster (or
      some of its nodes) as if &ceph; was never installed on the cluster
      servers. They are typically used when &ceph; installation fails and you
      want to start with a clean environment. Or, you can purge one or more
      nodes because you want to remove them from the cluster as their
      life-cycle ends.
     </para>
     <para>
      For more information on purging the cluster or its nodes, see
      <xref linkend="ceph.install.ceph-deploy.purge"/>.
     </para>
     <tip>
      <para>
       If you do not intend to purge the whole cluster, do not use the
       <command>forgetkeys</command> subcommand, as the keys will remain in
       place for the remaining cluster infrastructure.
      </para>
     </tip>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_osd_cephdisk">
  <title>Adding OSDs with <command>ceph-disk</command></title>

  <para>
   <command>ceph-disk</command> is a utility that can prepare and activate a
   disk, partition or directory as a &ceph; OSD. It automates the multiple
   steps involved in manual creation and start of an OSD into two steps of
   preparing and activating the OSD by using the subcommands
   <command>prepare</command> and <command>activate</command>.
  </para>

  <variablelist>
   <varlistentry>
    <term><command>prepare</command>
    </term>
    <listitem>
     <para>
      Prepares a directory, disk or drive for a &ceph; OSD. It creates a GPT
      partition, marks the partition with &ceph; type uuid, creates a file
      system, marks the file system as ready for &ceph; consumption, uses
      entire partition and adds a new partition to the journal disk.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>activate</command>
    </term>
    <listitem>
     <para>
      Activates the &ceph; OSD. It mounts the volume in a temporary location,
      allocates an OSD ID (if needed), remounts in the correct location
      <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></filename>
      and starts <command>ceph-osd</command>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following example shows steps for adding an OSD with
   <command>ceph-osd</command>.
  </para>

  <procedure>
   <step>
    <para>
     Make sure a new disk is physically present on the node where you want to
     add the OSD. In our example, it is <emphasis>node1</emphasis> belonging to
     cluster <emphasis>ceph</emphasis>.
    </para>
   </step>
   <step>
    <para>
     <command>ssh</command> to node1.
    </para>
   </step>
   <step>
    <para>
     Generate a unique identification for the new OSD:
    </para>
<screen>uuidgen
c70c032a-6e88-4962-8376-4aa119cb52ee</screen>
   </step>
   <step>
    <para>
     Prepare the disk:
    </para>
<screen>sudo ceph-disk prepare --cluster ceph \
--cluster-uuid c70c032a-6e88-4962-8376-4aa119cb52ee --fs-type xfs /dev/hdd1</screen>
   </step>
   <step>
    <para>
     Activate the OSD:
    </para>
<screen>sudo ceph-disk activate /dev/hdd1</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_osd_cephdeploy">
  <title>Adding OSDs with <command>ceph-deploy</command></title>

  <para>
   <command>ceph-deploy</command> is a command line utility to simplify the
   installation and configuration of a &ceph; cluster. It can be used to add or
   remove OSDs as well. To add a new OSD to a node <literal>node2</literal>
   with <command>ceph-deploy</command>, follow these steps:
  </para>

  <tip>
   <para>
    <command>ceph-deploy</command> is usually run from the administration node,
    from which you installed the cluster.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     List available disks on a node:
    </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/vdc other, unknown</screen>
    <para>
     <filename>/dev/vdc</filename> seems to be unused, so let us focus on
     adding it as an OSD.
    </para>
   </step>
   <step>
    <para>
     Zap the disk. Zapping deletes the disk's partition table.
    </para>
<screen>ceph-deploy disk zap node2:vdc</screen>
    <warning>
     <para>
      Zapping deletes all data from the disk
     </para>
    </warning>
   </step>
   <step>
    <para>
     Prepare the OSD. The <command>prepare</command> command expects you to
     specify the disk for data, and optionally the disk for its journal. We
     recommend storing the journal on a separate drive to maximize throughput.
    </para>
<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
   </step>
<!-- 2015-11-05 tbazant: not needed, so commenting out
    <step>
     <para>
      Activate the OSD. The <command>activate</command> command will cause
      your OSD to come <literal>up</literal> and be placed
      <literal>in</literal> the cluster. The <command>activate</command>
      command uses the path to the partition created when running the
      <command>prepare</command> command.
     </para>
<screen>ceph-deploy osd activate node2:/dev/vdc1:/dev/ssd1</screen>
    </step>
    -->
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.inst.add_rm_monitor">
  <title>Adding and Removing Monitors</title>

  <para>
   With <command>ceph-deploy</command>, adding and removing monitors is a
   simple task. Also, take into account the following
   restrictions/recommendation.
  </para>

  <important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <command>ceph-deploy</command> restricts you to only install one monitor
      per host.
     </para>
    </listitem>
    <listitem>
     <para>
      We do not recommend mixing monitors and OSDs on the same host.
     </para>
    </listitem>
    <listitem>
     <para>
      For high availability, you should run a production &ceph; cluster with
      <emphasis>at least</emphasis> three monitors.
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <sect2 xml:id="storage.bp.inst.add_rm_monitor.addmon">
   <title>Adding a Monitor</title>
   <para>
    After you create a cluster and install &ceph; packages to the monitor
    host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
    information), you may deploy the monitors to the monitor hosts. You may
    specify more monitor host names in the same command.
   </para>
<screen>ceph-deploy mon create <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     When adding a monitor on a host that was not in hosts initially defined
     with the <command>ceph-deploy new</command> command, a <option>public
     network</option> statement needs to be added to the
     <filename>ceph.conf</filename> file.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="storage.bp.inst.add_rm_monitor.rmmon">
   <title>Removing a Monitor</title>
   <para>
    If you have a monitor in your cluster that you want to remove, you may use
    the destroy option. You may specify more monitor host names in the same
    command.
   </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     Ensure that if you remove a monitor, the remaining monitors will be able
     to establish a consensus. If that is not possible, consider adding a
     monitor before removing the monitor you want to take offline.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw">
  <title>Usage of <command>ceph-deploy rgw</command></title>

  <para>
   The <command>ceph-deploy</command> script includes the
   <command>rgw</command> component that helps you manage &rgw; instances. Its
   general form follows this pattern:
  </para>

<screen>ceph-deploy rgw <replaceable>subcommand</replaceable> <replaceable>rgw-host</replaceable>:<replaceable>rgw-instance</replaceable>:<replaceable>fqdn</replaceable>:<replaceable>port</replaceable>:<replaceable>redirect</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term>subcommand</term>
    <listitem>
     <para>
      One of <command>list</command>, <command>prepare</command>,
      <command>activate</command>, <command>create</command> (=
      <command>prepare</command> + <command>activate</command>), or
      <command>delete</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-host</term>
    <listitem>
     <para>
      Host name where you want to operate the &rgw;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-instance</term>
    <listitem>
     <para>
      &ceph; instance name. Default is 'rgw-host'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>fqdn</term>
    <listitem>
     <para>
      Virtual host to listen to. Default is 'None'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>port</term>
    <listitem>
     <para>
      Port to listen to. Default is 80.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>redirect</term>
    <listitem>
     <para>
      The URL redirect. Default is '^/(.*)'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For example:
  </para>

<screen>ceph-deploy rgw prepare example_host2:gateway1</screen>

  <para>
   or
  </para>

<screen>ceph-deploy activate example_host1:gateway1:virtual_srv2:81</screen>

  <tip>
   <title>Specifying Multiple &rgw; Instances</title>
   <para>
    You can specify more <option>rgw_hostname:rgw_instance</option> pairs on
    the same command line if you separate them with a comma:
   </para>
<screen>ceph-deploy rgw create hostname1:rgw,hostname2:rgw,hostname3:rgw</screen>
  </tip>

  <para>
   For a practical example of setting &rgw; with
   <command>ceph-deploy</command>, see <xref linkend="ses.rgw.config"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw_client">
  <title>&rgw; Client Usage</title>

  <para>
   To use &rgw; REST interfaces, you need to create a user for the S3
   interface, then a subuser for the Swift interface. Find more information on
   creating &rgw; users in <xref linkend="storage.bp.account.swiftadd"/> and
   <xref linkend="storage.bp.account.s3add"/>.
  </para>

  <sect2>
   <title>S3 Interface Access</title>
   <para>
    To access the S3 interface, you need to write a Python script. The script
    will connect to &rgw;, create a new bucket, and list all buckets. The
    values for <option>aws_access_key_id</option> and
    <option>aws_secret_access_key</option> are taken from the values of
    <option>access_key</option> and <option>secret_key</option> returned by the
    <command>radosgw_admin</command> command from
    <xref linkend="storage.bp.account.s3add"/>.
   </para>
   <procedure>
    <step>
     <para>
      Install the <systemitem>python-boto</systemitem> package:
     </para>
<screen>sudo zypper in python-boto</screen>
    </step>
    <step>
     <para>
      Create a new Python script called <filename>s3test.py</filename> with the
      following content:
     </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
     <para>
      Replace <literal>{hostname}</literal> with the host name of the host
      where you configured &rgw; service, for example
      <literal>gateway_host</literal>.
     </para>
    </step>
    <step>
     <para>
      Run the script:
     </para>
<screen>python s3test.py</screen>
     <para>
      The script outputs something like the following:
     </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Swift Interface Access</title>
   <para>
    To access &rgw; via Swift interface, you need the <command>swift</command>
    command line client. Its manual page <command>man 1 swift</command> tells
    you more about its command line options.
   </para>
   <para>
    To install <command>swift</command>, run the following:
   </para>
<screen>sudo zypper in python-swiftclient</screen>
   <para>
    The swift access uses the following syntax:
   </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
   <para>
    Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
    gateway server, and <replaceable>swift_secret_key</replaceable> with its
    value from the output of the <command>radosgw-admin key create</command>
    command executed for the <systemitem>swift</systemitem> user in
    <xref linkend="storage.bp.account.swiftadd"/>.
   </para>
   <para>
    For example:
   </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
   <para>
    The output is:
   </para>
<screen>my-new-bucket</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Automated Installation via &salt;</title>

  <para>
   The installation can be automated by using the &salt; reactor. For virtual
   environments or consistent hardware environments, this configuration will
   allow the creation of a &ceph; cluster with the specified behavior.
  </para>

  <warning>
   <para>
    &salt; cannot perform dependency checks based on reactor events. Putting
    your &salt; master into a death spiral is a real risk.
   </para>
  </warning>

  <para>
   The automated installation requires the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A properly created
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>.
    </para>
   </listitem>
   <listitem>
    <para>
     Prepared custom configuration, placed to the
     <filename>/srv/pillar/ceph/stack</filename> directory.
    </para>
   </listitem>
   <listitem>
    <para>
     The example reactor file
     <filename>/usr/share/doc/packages/deepsea/reactor.conf</filename> must be
     copied to <filename>/etc/salt/master.d/reactor.conf</filename>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The default reactor configuration will only run Stages 0 and 1. This allows
   testing of the reactor without waiting for subsequent stages to complete.
  </para>

  <para>
   When the first salt-minion starts, Stage 0 will begin. A lock prevents
   multiple instances. When all minions complete Stage 0, Stage 1 will begin.
  </para>

  <para>
   If the operation is performed properly, change the last line in the
   <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   to
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="bp.node.management">
  <title>Node Management</title>

  <para>
   After you set up a complete cluster you may need to perform additional
   changes to the cluster like adding or removing monitoring nodes or
   adding/removing &ceph;&nbsp; OSD nodes. Adding and removing of cluster nodes
   can be done without shutting down the whole cluster, but it might increase
   replication traffic.
  </para>

  <note>
   <title>Limitations</title>
   <para>
    The procedures described in sections: <xref linkend="Adding.OSD.Nodes"/>
    and <xref linkend="Removing.OSD.Nodes"/> can be performed only with the
    default CRUSH map. The default CRUSH map must have been created by using
    <literal>&ceph;-deploy</literal> or <emphasis>DeepSea</emphasis>.
   </para>
  </note>

  <sect2 xml:id="Adding.OSD.Nodes">
   <title>Adding &ceph; OSD Nodes</title>
   <para>
    The procedure below describes adding of a &ceph; OSD node to your cluster.
   </para>
   <procedure xml:id="proc.Adding.Ceph.Node">
    <title>Adding a &ceph; OSD Node</title>
    <step>
     <para>
      List all &ceph; OSD nodes and then choose a proper name for the new
      node/s
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      Inspect your CRUSH map to find out the bucket type, for a procedure refer
      to <xref linkend="op.crush"/>. Typically the bucket type is
      <emphasis>host</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Create a record for the new node in your CRUSH map.
     </para>
<screen>ceph osd crush add-bucket <replaceable>{bucket name} {bucket type}</replaceable></screen>
     <para>
      for example:
     </para>
<screen>ceph osd crush add-bucket ses4-4 host</screen>
    </step>
    <step>
     <para>
      Add all OSD that the new node should use. For a procedure refer to
      <xref linkend="storage.bp.inst.add_osd_cephdeploy"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="Removing.OSD.Nodes">
   <title>Removing &ceph; OSD Nodes</title>
   <para>
    To remove a &ceph; OSD node follow the procedure:
   </para>
   <procedure xml:id="proc.Removing.Ceph.Node">
    <title>Removing a &ceph; OSD Node</title>
    <step>
     <para>
      Remove all OSD on the node you want to delete as described in
      <xref linkend="storage.bp.disk.del"/>.
     </para>
    </step>
    <step>
     <para>
      Verify that all OSDs have been removed:
     </para>
<screen>ceph osd tree</screen>
     <para>
      The OSD to be removed must not have any OSD.
     </para>
    </step>
    <step>
     <para>
      Remove the node from the cluster:
     </para>
<screen>ceph osd crush remove <replaceable>{bucket name}</replaceable></screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="salt.node.removing">
   <title>Removing and Reinstalling &salt; Cluster Nodes</title>
   <para>
    You may want remove a role from your minion, to do so use the Stage 5
    command:
   </para>
<screen>&prompt.root;salt-run state.orch ceph.stage.5</screen>
   <para>
    When a role is removed from a minion, the objective is to undo all changes
    related to that role. For most of the roles, the task is simple, but there
    may be problems with package dependencies. If a package is uninstalled, its
    dependencies are not.
   </para>
   <para>
    Removed OSDs appear as blank drives. The related tasks overwrite the
    beginning of the file systems and remove backup partitions in addition to
    wiping the partition tables.
   </para>
   <note>
    <title>Preserving Partitions Created by Other Methods</title>
    <para>
     Disk drives previously configured by other methods, such as
     <command>ceph-deploy</command>, may still contain partitions. &deepsea;
     will not automatically destroy these. Currently, the administrator must
     reclaim these drives.
    </para>
   </note>
  </sect2>
 </sect1>
</chapter>
