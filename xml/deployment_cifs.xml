<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ses-cifs">
<!-- ============================================================== -->
 <title>Exporting &ceph; Data via &samba;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter describes how to export data stored in a &ceph; cluster via a
  &samba;/CIFS share so that you can easily access them from Windows* client
  machines. It also includes information that will help you configure a &ceph;
  &samba; gateway to join &ad; in the Windows* domain to authenticate and
  authorize users.
 </para>
 <note>
  <title>&samba; Gateway Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &cephfs;
   via a &samba; Gateway may significantly reduce application performance when
   compared to native &ceph; clients.
  </para>
 </note>
 <sect1 xml:id="cephfs-samba">
  <title>Export &cephfs; via Samba Share</title>

  <warning>
   <title>Cross Protocol Access</title>
   <para>
    Native &cephfs; and NFS clients are not restricted by file locks obtained
    via Samba, and vice versa. Applications that rely on cross protocol file
    locking may experience data corruption if &cephfs; backed Samba share paths
    are accessed via other means.
   </para>
  </warning>

  <sect2 xml:id="cephfs-samba-packages">
   <title>&samba; Related Packages Installation</title>
   <para>
    To configure and export a &samba; share, the following packages need to be
    installed: <package>samba-ceph</package> and
    <package>samba-winbind</package>. If these packages are not installed,
    install them:
   </para>
<screen>
&prompt.cephuser.smb;zypper install samba-ceph samba-winbind
</screen>
  </sect2>

  <sect2 xml:id="sec-ses-cifs-example">
   <title>Single Gateway Example</title>
   <para>
    In preparation for exporting a Samba share, choose an appropriate node to
    act as a &sgw;. The node needs to have access to the &ceph; client network,
    as well as sufficient CPU, memory, and networking resources.
   </para>
   <para>
    Failover functionality can be provided with CTDB and the &sle; &hasi;.
    Refer to <xref linkend="sec-ses-cifs-ha"/> for more information on HA
    setup.
   </para>
   <procedure>
    <step>
     <para>
      Make sure that a working &cephfs; already exists in your cluster. For
      details, see DEAD_LINK <!-- <xref linkend="cha-ceph-as-cephfs"/> -->.
     </para>
    </step>
    <step>
     <para>
      Create a &sgw; specific keyring on the &ceph; admin node and copy it to
      both &sgw; nodes:
     </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
     <para>
      Replace <replaceable>SAMBA_NODE</replaceable> with the name of the Samba
      gateway node.
     </para>
    </step>
    <step>
     <para>
      The following steps are executed on the &sgw; node. Install Samba
      together with the &ceph; integration package:
     </para>
<screen>&prompt.cephuser.smb;sudo zypper in samba samba-ceph</screen>
    </step>
    <step>
     <para>
      Replace the default contents of the
      <filename>/etc/samba/smb.conf</filename> file with the following:
     </para>
<screen>
[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<replaceable>SHARE_NAME</replaceable>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
 </screen>
     <para>
      The above share configuration uses the <systemitem>vfs_ceph</systemitem>
      module for &samba; to communicate with the &ceph; cluster. As a
      higher-performing but less-flexible alternative, &samba; can instead be
      configured to use the Linux kernel &cephfs; client, as shown below:
     </para>
<screen>
[<replaceable>SHARE_NAME</replaceable>]
  path = <replaceable>CEPHFS_MOUNT</replaceable>
  # "vfs objects = ceph" and "ceph: config_file / user_id" are dropped
  read only = no
  oplocks = no
  kernel share modes = no
</screen>
     <para>
      The <replaceable>CEPHFS_MOUNT</replaceable> path above must be mounted
      prior to starting &samba; with a kernel &cephfs; share configuration. See
      <xref linkend="ceph-cephfs-cephfs-fstab"/>.
     </para>
     <tip>
      <title>Oplocks and Share Modes</title>
      <para>
       <option>oplocks</option> (also known as SMB2+ leases) allow for improved
       performance through aggressive client caching, but are currently unsafe
       when &samba; is deployed together with other &cephfs; clients, such as
       kernel <literal>mount.ceph</literal>, FUSE, or &ganesha;.
      </para>
      <para>
       If all &cephfs; file system path access is exclusively handled by
       &samba;, then the <option>oplocks</option> parameter can be safely
       enabled.
      </para>
      <para>
       Currently <option>kernel share modes</option> needs to be disabled in a
       share running with the &cephfs; vfs module for file serving to work
       properly.
      </para>
     </tip>
     <important>
      <title>Permitting Access</title>
      <para>
       Samba maps SMB users and groups to local accounts. Local users can be
       assigned a password for Samba share access via:
      </para>
<screen>
&prompt.root;smbpasswd -a <replaceable>USERNAME</replaceable>
</screen>
      <para>
       For successful I/O, the share path's access control list (ACL) needs to
       permit access to the user connected via &samba;. You can
       modify the ACL by temporarily mounting via the &cephfs; kernel client
       and using the <command>chmod</command>, <command>chown</command>, or
       <command>setfacl</command> utilities against the share path. For
       example, to permit access for all users, run:
      </para>
<screen>
&prompt.root;chmod 777 <replaceable>MOUNTED_SHARE_PATH</replaceable>
</screen>
     </important>
    </step>
    <step>
     <para>
      Start and enable the Samba daemon:
     </para>
<screen>
&prompt.cephuser.smb;sudo systemctl start smb.service
&prompt.cephuser.smb;sudo systemctl enable smb.service
&prompt.cephuser.smb;sudo systemctl start nmb.service
&prompt.cephuser.smb;sudo systemctl enable nmb.service
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ses-cifs-ha">
   <title>High Availability Configuration</title>
   <important>
    <title>Transparent Failover Not Supported</title>
    <para>
     Although a multi-node Samba + CTDB deployment is more highly available
     compared to the single node (see <xref linkend="cha-ses-cifs"/>),
     client-side transparent failover is not supported. Applications will
     likely experience a short outage on &sgw; node failure.
    </para>
   </important>
   <para>
    This section provides an example of how to set up a two-node high
    availability configuration of Samba servers. The setup requires the &sle;
    &hasi;. The two nodes are called &si.earth;
    (<systemitem class="ipaddress">192.168.1.1</systemitem>) and &si.mars;
    (<systemitem class="ipaddress">192.168.1.2</systemitem>).
   </para>
   <para>
    For details about &sle; &hasi;, see
    <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/"/>.
   </para>
   <para>
    Additionally, two floating virtual IP addresses allow clients to connect to
    the service no matter which physical node it is running on.
    <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
    administration with Hawk2 and
    <systemitem class="ipaddress">192.168.2.1</systemitem> is used exclusively
    for the CIFS exports. This makes it easier to apply security restrictions
    later.
   </para>
   <para>
    The following procedure describes the example installation. More details
    can be found at
    <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-install-quick/"/>.
   </para>
   <procedure xml:id="proc-sec-ses-cifs-ha">
    <step>
     <para>
      Create a &sgw; specific keyring on the &adm; and copy it to both nodes:
     </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring &si.earth;:/etc/ceph/
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring &si.mars;:/etc/ceph/</screen>
    </step>
    <step>
     <para>
      SLE-HA setup requires a fencing device to avoid a <emphasis>split
      brain</emphasis> situation when active cluster nodes become
      unsynchronized. For this purpose, you can use a &ceph; RBD image with
      Stonith Block Device (SBD). Refer to
      <link
       xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-storage-protect-fencing-setup"/>
      for more details.
     </para>
     <para>
      If it does not yet exist, create an RBD pool called
      <literal>rbd</literal> (see
      <xref linkend="ceph-pools-operate-add-pool"
       />) and associate it
      with <literal>rbd</literal> (see
      <xref
       linkend="ceph-pools-associate" />). Then create a related
      RBD image called <literal>sbd01</literal>:
     </para>
<screen>
&prompt.cephuser;ceph osd pool create rbd <replaceable>PG_NUM</replaceable> <replaceable>PGP_NUM</replaceable> replicated
&prompt.cephuser;ceph osd pool application enable rbd rbd
&prompt.cephuser;rbd -p rbd create sbd01 --size 64M --image-shared
</screen>
    </step>
    <step>
     <para>
      Prepare &si.earth; and &si.mars; to host the Samba service:
     </para>
     <substeps>
      <step>
       <para>
        Make sure the following packages are installed before you proceed:
        <package>ctdb</package>, <package>tdb-tools</package>, and
        <package>samba</package> (needed for
        <systemitem
         class="daemon">smb.service</systemitem> and
        <systemitem
         class="daemon">nmb.service</systemitem>).
       </para>
<screen>&prompt.cephuser.smb;<command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
      </step>
      <step>
       <para>
        Make sure the services <literal>ctdb</literal>, <literal>smb</literal>,
        and <literal>nmb</literal> are stopped and disabled:
       </para>
<screen>&prompt.cephuser.smb;sudo systemctl disable ctdb
&prompt.cephuser.smb;sudo systemctl disable smb
&prompt.cephuser.smb;sudo systemctl disable nmb
&prompt.cephuser.smb;sudo systemctl stop smb
&prompt.cephuser.smb;sudo systemctl stop nmb</screen>
      </step>
      <step>
       <para>
        Open port <literal>4379</literal> of your firewall on all nodes. This
        is needed for CTDB to communicate with other cluster nodes.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      On &si.earth;, create the configuration files for Samba. They will later
      automatically synchronize to &si.mars;.
     </para>
     <substeps>
      <step>
       <para>
        Insert a list of private IP addresses of &sgw; nodes in the
        <filename>/etc/ctdb/nodes</filename> file. Find more details in the
        ctdb manual page (<command>man 7 ctdb</command>).
       </para>
<screen>&subnetI;.1
&subnetI;.2</screen>
      </step>
      <step>
       <para>
        Configure Samba. Add the following lines in the
        <literal>[global]</literal> section of
        <filename>/etc/samba/smb.conf</filename>. Use the host name of your
        choice in place of <replaceable>CTDB-SERVER</replaceable> (all nodes in
        the cluster will appear as one big node with this name). Add a share
        definition as well, consider <replaceable>SHARE_NAME</replaceable> as
        an example:
       </para>
<screen>
[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
</screen>
       <para>
        Note that the <filename>/etc/ctdb/nodes</filename> and
        <filename>/etc/samba/smb.conf</filename> files need to match on all
        &sgw; nodes.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Install and bootstrap the &sle; &ha; cluster.
     </para>
     <substeps>
      <step>
       <para>
        Register the &sle; &hasi; on &si.earth; and &si.mars;:
       </para>
<screen>&prompt.earth;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen>&prompt.mars;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
      </step>
      <step>
       <para>
        Install <package>ha-cluster-bootstrap</package> on both nodes:
       </para>
<screen>&prompt.earth;<command>zypper</command> in ha-cluster-bootstrap</screen>
<screen>&prompt.mars;<command>zypper</command> in ha-cluster-bootstrap</screen>
      </step>
      <step>
       <para>
        Map the RBD image <literal>sbd01</literal> on both &sgw;s via
        <systemitem class="daemon">rbdmap.service</systemitem>.
       </para>
       <para>
        Edit <filename>/etc/ceph/rbdmap</filename> and add an entry for the SBD
        image:
       </para>
<screen>rbd/sbd01 id=samba.gw,keyring=/etc/ceph/ceph.client.samba.gw.keyring</screen>
       <para>
        Enable and start
        <systemitem class="daemon">rbdmap.service</systemitem>:
       </para>
<screen>
&prompt.earth;systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
&prompt.mars;systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
</screen>
       <para>
        The <filename>/dev/rbd/rbd/sbd01</filename> device should be available
        on both &sgw;s.
       </para>
      </step>
      <step>
       <para>
        Initialize the cluster on &si.earth; and let &si.mars; join it.
       </para>
<screen>&prompt.earth;<command>ha-cluster-init</command></screen>
<screen>&prompt.mars;<command>ha-cluster-join</command> -c earth</screen>
       <important>
        <para>
         During the process of initialization and joining the cluster, you will
         be interactively asked whether to use SBD. Confirm with
         <option>y</option> and then specify
         <filename>/dev/rbd/rbd/sbd01</filename> as a path to the storage
         device.
        </para>
       </important>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Check the status of the cluster. You should see two nodes added in the
      cluster:
     </para>
<screen>&prompt.earth;<command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
    </step>
    <step>
     <para>
      Execute the following commands on &si.earth; to configure the CTDB
      resource:
     </para>
<!--taroth 2012-02-07: fixed bnc#745334, default timeout for start/stop-->
<screen>&prompt.earth;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
&prompt.crm.conf;<command>primitive</command> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>primitive</command> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>group</command> g-ctdb ctdb nmb smb
&prompt.crm.conf;<command>clone</command> cl-ctdb g-ctdb meta interleave="true"
&prompt.crm.conf;<command>commit</command></screen>
     <para>
      The binary
      <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> in the
      configuration option <literal>ctdb_recovery_lock</literal> has the
      parameters <replaceable>CLUSTER_NAME</replaceable>,
      <replaceable>CEPHX_USER</replaceable>,
      <replaceable>RADOS_POOL</replaceable>, and
      <replaceable>RADOS_OBJECT</replaceable>, in this order.
     </para>
     <para>
      An extra lock-timeout parameter can be appended to override the default
      value used (10 seconds). A higher value will increase the CTDB recovery
      master failover time, whereas a lower value may result in the recovery
      master being incorrectly detected as down, triggering flapping failovers.
     </para>
    </step>
    <step>
     <para>
      Add a clustered IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip ocf:heartbeat:IPaddr2<!--
    --> params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
&prompt.crm.conf;<command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
&prompt.crm.conf;<command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>commit</command></screen>
     <para>
      If <literal>unique_clone_address</literal> is set to
      <literal>true</literal>, the IPaddr2 resource agent adds a clone ID to
      the specified address, leading to three different IP addresses. These are
      usually not needed, but help with load balancing. For further information
      about this topic, see
      <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-lb"/>.
     </para>
    </step>
    <step>
     <para>
      Check the result:
     </para>
<screen>&prompt.earth;<command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
    </step>
    <step>
     <para>
      Test from a client machine. On a Linux client, run the following command
      to see if you can copy files from and to the system:
     </para>
<screen>&prompt.root;<command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-ad">
  <title>&sgw; Joining &ad;</title>

  <para>
   You can configure the &ceph; &samba; gateway to become a member of &samba;
   domain with &ad; (AD) support. As a &samba; domain member, you can use
   domain users and groups in local access lists (ACLs) on files and
   directories from the exported &cephfs;.
  </para>

  <sect2 xml:id="cephfs-ad-preparation">
   <title>Preparation</title>
   <para>
    This section introduces preparatory steps that you need to take care of
    before configuring the &samba; itself. Starting with a clean environment
    helps you prevent confusion and verifies that no files from the previous
    &samba; installation are mixed with the new domain member installation.
   </para>
   <tip>
    <title>Synchronize Clocks</title>
    <para>
     All &sgw; nodes' clocks need to be synchronized with the &ad; Domain
     controller. Clock skew may result in authentication failures.
    </para>
   </tip>
   <para>
    Verify that no &samba; or name caching processes are running:
   </para>
<screen>
&prompt.cephuser.smb;ps ax | egrep "samba|smbd|nmbd|winbindd|nscd"
</screen>
   <para>
    If the output lists any <literal>samba</literal>, <literal>smbd</literal>,
    <literal>nmbd</literal>, <literal>winbindd</literal>, or
    <literal>nscd</literal> processes, stop them.
   </para>
   <para>
    If you have previously run a &samba; installation on this host, remove the
    <filename>/etc/samba/smb.conf</filename> file. Also remove all &samba;
    database files, such as <filename>*.tdb</filename> and
    <filename>*.ldb</filename> files. To list directories containing &samba;
    databases, run:
   </para>
<screen>
&prompt.cephuser.smb;smbd -b | egrep "LOCKDIR|STATEDIR|CACHEDIR|PRIVATE_DIR"
</screen>
  </sect2>

  <sect2 xml:id="cephfs-ad-dns">
   <title>Verify DNS</title>
   <para>
    &ad; (AD) uses DNS to locate other domain controllers (DCs) and services,
    such as Kerberos. Therefore AD domain members and servers need to be able
    to resolve the AD DNS zones.
   </para>
   <para>
    Verify that DNS is correctly configured and that both forward and reverse
    lookup resolve correctly, for example:
   </para>
<screen>
&prompt.cephuser;nslookup DC1.domain.example.com
Server:         10.99.0.1
Address:        10.99.0.1#53

Name:   DC1.domain.example.com
Address: 10.99.0.1
</screen>
<screen>
&prompt.cephuser;10.99.0.1
Server:        10.99.0.1
Address:	10.99.0.1#53

1.0.99.10.in-addr.arpa	name = DC1.domain.example.com.
</screen>
  </sect2>

  <sect2 xml:id="cephfs-ad-srv">
   <title>Resolving SRV Records</title>
   <para>
    AD uses SRV records to locate services, such as Kerberos and LDAP. To
    verify that SRV records are resolved correctly, use the
    <command>nslookup</command> interactive shell, for example:
   </para>
<screen>
&prompt.cephuser;nslookup
Default Server:  10.99.0.1
Address:  10.99.0.1

> set type=SRV
> _ldap._tcp.domain.example.com.
Server:  UnKnown
Address:  10.99.0.1

_ldap._tcp.domain.example.com   SRV service location:
          priority       = 0
          weight         = 100
          port           = 389
          svr hostname   = dc1.domain.example.com
domain.example.com      nameserver = dc1.domain.example.com
dc1.domain.example.com  internet address = 10.99.0.1
</screen>
  </sect2>

  <sect2 xml:id="cephfs-ad-kerberos">
   <title>Configuring Kerberos</title>
   <para>
    &samba; supports Heimdal and MIT Kerberos back-ends. To configure Kerberos
    on the domain member, set the following in your
    <filename>/etc/krb5.conf</filename> file:
   </para>
<screen>
[libdefaults]
	default_realm = DOMAIN.EXAMPLE.COM
	dns_lookup_realm = false
	dns_lookup_kdc = true
</screen>
   <para>
    The previous example configures Kerberos for the DOMAIN.EXAMPLE.COM realm.
    We do not recommend to set any further parameters in the
    <filename>/etc/krb5.conf</filename> file. If your
    <filename>/etc/krb5.conf</filename> contains an <literal>include</literal>
    line it will not work&mdash;you <emphasis role="bold">must</emphasis>
    remove this line.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-ad-local-resolution">
   <title>Local Host Name Resolution</title>
   <para>
    When you join a host to the domain, &samba; tries to register the host name
    in the AD DNS zone. For this, the <command>net</command> utility needs to
    be able to resolve the host name using DNS or using a correct entry in the
    <filename>/etc/hosts</filename> file.
   </para>
   <para>
    To verify that your host name resolves correctly, use the <command>getent
    hosts</command> command:
   </para>
<screen>
&prompt.cephuser;getent hosts example-host
10.99.0.5      example-host.domain.example.com    example-host
</screen>
   <para>
    The host name and FQDN must not resolve to the 127.0.0.1 IP address or any
    IP address other than the one used on the LAN interface of the domain
    member. If no output is displayed or the host is resolved to the wrong IP
    address and you are not using DHCP, set the correct entry in the
    <filename>/etc/hosts</filename> file:
   </para>
<screen>
127.0.0.1      localhost
10.99.0.5      example-host.samdom.example.com    example-host
</screen>
   <tip>
    <title>DHCP and <filename>/etc/hosts</filename></title>
    <para>
     If you are using DHCP, check that <filename>/etc/hosts</filename> only
     contains the '127.0.0.1' line. If you continue to have problems, contact
     the administrator of your DHCP server.
    </para>
    <para>
     If you need to add aliases to the machine host name, add them to the end
     of the line that starts with the machine's IP address, not to the
     '127.0.0.1' line.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephfs-ad-smb-conf">
   <title>Configuring &samba;</title>
   <para>
    This section introduces information about specific configuration options
    that you need to include in the &samba; configuration file
    <filename>/etc/samba/smb.conf</filename>.
   </para>
   <sect3>
    <title>Choose Back-end for ID Mapping in <systemitem>winbindd</systemitem></title>
    <para>
     If you need your users to have different login shells and/or Unix home
     directory paths, or you want them to have the same ID everywhere, you will
     need to use the winbind 'ad' back-end and add RFC2307 attributes to AD.
    </para>
    <important>
     <title>RFC2307 Attributes and ID Numbers</title>
     <para>
      The RFC2307 attributes are not added automatically when users or groups
      are created.
     </para>
     <para>
      The ID numbers found on a DC (numbers in the 3000000 range) are
      <emphasis>not</emphasis> RFC2307 attributes and will not be used on Unix
      Domain Members. If you need to have the same ID numbers everywhere, add
      <literal>uidNumber</literal> and <literal>gidNumber</literal> attributes
      to AD and use the winbind 'ad' back-end on Unix Domain Members. If you do
      decide to add <literal>uidNumber</literal> and
      <literal>gidNumber</literal> attributes to AD, do not use numbers in the
      3000000 range.
     </para>
    </important>
    <para>
     If your users will only use the &samba; AD DC for authentication and will
     not store data on it or log in to it, you can use the winbind 'rid'
     back-end. This calculates the user and group IDs from the Windows* RID. If
     you use the same <literal>[global]</literal> section of the
     <filename>smb.conf</filename> on every Unix domain member, you will get
     the same IDs. If you use the 'rid' back-end, you do not need to add
     anything to AD and RFC2307 attributes will be ignored. When using the
     'rid' back-end, set the <option>template shell</option> and
     <option>template homedir</option> parameters in
     <filename>smb.conf</filename>. These settings are global and everyone gets
     the same login shell and Unix home directory path (unlike the RFC2307
     attributes where you can set individual Unix home directory paths and
     shells).
    </para>
    <para>
     There is another way of setting up &samba;&mdash;when you require your
     users and groups to have the same ID everywhere, but only need your users
     to have the same login shell and use the same Unix home directory path.
     You can do this by using the winbind 'ad' back-end and using the template
     lines in <filename>smb.conf</filename>. This way you only need to add
     <literal>uidNumber</literal> and <literal>gidNumber</literal> attributes
     to AD.
    </para>
    <tip>
     <title>More Information about Back-ends for ID Mapping</title>
     <para>
      Find more detailed information about available ID mapping back-ends in
      the related manual pages: <command>man 8 idmap_ad</command>, <command>man
      8 idmap_rid</command>, and <command>man 8 idmap_autorid</command>.
     </para>
    </tip>
   </sect3>
   <sect3>
    <title>Setting User and Group ID Ranges</title>
    <para>
     After you decide which winbind back-end to use, you need to specify the
     ranges to use with the <option>idmap config</option> option in
     <filename>smb.conf</filename>. By default, there are multiple blocks of
     user and group IDs reserved on a Unix domain member:
    </para>
    <table>
     <title>Default Users and Group ID Blocks</title>
<?dbhtml table-width="50%" ?>
<?dbfo table-width="50%" ?>
     <tgroup cols="2">
      <thead>
       <row>
        <entry>IDs</entry>
        <entry>Range</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>0-999</entry>
        <entry>Local system users and groups.</entry>
       </row>
       <row>
        <entry>Starting at 1000</entry>
        <entry>Local Unix users and groups.</entry>
       </row>
       <row>
        <entry>Starting at 10000</entry>
        <entry>DOMAIN users and groups.</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     As you can see from the above ranges, you should not set either the '*' or
     'DOMAIN' ranges to start at 999 or less, as they would interfere with the
     local system users and groups. You also should leave a space for any local
     Unix users and groups, so starting the <option>idmap config</option>
     ranges at 3000 seems to be a good compromise.
    </para>
    <para>
     You need to decide how large your 'DOMAIN' is likely to grow and if you
     plan to have any trusted domains. Then you can set the <option>idmap
     config</option> ranges as follows:
    </para>
    <table>
     <title>ID Ranges</title>
<?dbhtml table-width="50%" ?>
<?dbfo table-width="50%" ?>
     <tgroup cols="2">
      <thead>
       <row>
        <entry>Domain</entry>
        <entry>Range</entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>*</entry>
        <entry>3000-7999</entry>
       </row>
       <row>
        <entry>DOMAIN</entry>
        <entry>10000-999999</entry>
       </row>
       <row>
        <entry>TRUSTED</entry>
        <entry>1000000-9999999</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </sect3>
   <sect3>
    <title>Mapping the Domain Administrator Account to the Local &rootuser; User</title>
    <para>
     Samba enables you to map domain accounts to a local account. Use this
     feature to execute file operations on the domain member's file system as a
     different user than the account that requested the operation on the
     client.
    </para>
    <tip>
     <title>Mapping the Domain Administrator (Optional)</title>
     <para>
      Mapping the domain administrator to the local &rootuser; account is
      optional. Only configure the mapping if the domain administrator needs to
      be able to execute file operations on the domain member using &rootuser;
      permissions. Be aware that mapping Administrator to the &rootuser;
      account does not allow you to log in to Unix domain members as
      'Administrator'.
     </para>
    </tip>
    <para>
     To map the domain administrator to the local &rootuser; account, follow
     these steps:
    </para>
    <procedure>
     <step>
      <para>
       Add the following parameter to the <literal>[global]</literal> section
       of your <filename>smb.conf</filename> file:
      </para>
<screen>
username map = /etc/samba/user.map
</screen>
     </step>
     <step>
      <para>
       Create the <filename>/etc/samba/user.map</filename> file with the
       following content:
      </para>
<screen>
!root = <replaceable>DOMAIN</replaceable>\Administrator
</screen>
     </step>
    </procedure>
    <important>
     <para>
      When using the 'ad' ID mapping back-end, do not set the
      <option>uidNumber</option> attribute for the domain administrator
      account. If the account has the attribute set, the value overrides the
      local UID '0' of the &rootuser; user, and therefore the mapping fails.
     </para>
    </important>
    <para>
     For more details, see the <option>username map</option> parameter in the
     <filename>smb.conf</filename> manual page (<command>man 5
     smb.conf</command>).
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="cephfs-ad-joining">
   <title>Joining the &ad; Domain</title>
   <para>
    To join the host to an &ad;, run:
   </para>
<screen>
&prompt.cephuser.smb;net ads join -U administrator
Enter administrator's password: <replaceable>PASSWORD</replaceable>
Using short domain name -- <replaceable>DOMAIN</replaceable>
Joined <replaceable>EXAMPLE-HOST</replaceable> to dns domain <replaceable>'DOMAIN</replaceable>.example.com'
</screen>
  </sect2>

  <sect2 xml:id="cephfs-ad-nss">
   <title>Configuring the Name Service Switch</title>
   <para>
    To make domain users and groups available to the local system, you need to
    enable the name service switch (NSS) library. Append the
    <option>winbind</option> entry to the following databases in the
    <filename>/etc/nsswitch.conf</filename> file:
   </para>
<screen>
passwd: files winbind
group:  files winbind
</screen>
   <important>
    <title>Points to Consider</title>
    <itemizedlist>
     <listitem>
      <para>
       Keep the <option>files</option> entry as the first source for both
       databases. This enables NSS to look up domain users and groups from the
       <filename>/etc/passwd</filename> and <filename>/etc/group</filename>
       files before querying the
       <systemitem class="daemon">winbind</systemitem> service.
      </para>
     </listitem>
     <listitem>
      <para>
       Do not add the <option>winbind</option> entry to the NSS
       <literal>shadow</literal> database. This can cause the
       <command>wbinfo</command> utility to fail.
      </para>
     </listitem>
     <listitem>
      <para>
       Do not use the same user names in the local
       <filename>/etc/passwd</filename> file as in the domain.
      </para>
     </listitem>
    </itemizedlist>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-ad-services">
   <title>Starting the Services</title>
   <para>
    There are three services that you need to enable and start to have a fully
    functioning Unix domain member:
    <systemitem class="daemon">smbd</systemitem>,
    <systemitem class="daemon">nmbd</systemitem>, and
    <systemitem class="daemon">winbindd</systemitem>. Enable and start them by
    using the following commands:
   </para>
<screen>
&prompt.cephuser.smb;sudo systemctl enable smbd.service &amp;&amp; systemctl start smbd.service
&prompt.cephuser.smb;sudo systemctl enable nmbd.service &amp;&amp; systemctl start nmbd.service
&prompt.cephuser.smb;sudo systemctl enable winbindd.service &amp;&amp; systemctl start winbindd.service
</screen>
   <tip>
    <title>Optional <systemitem class="daemon">nmbd</systemitem></title>
    <para>
     If you do not require network browsing, you do not need to enable and
     start the <systemitem class="daemon">nmbd</systemitem> service on a Unix
     domain member.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephfs-ad-testing">
   <title>Testing the <systemitem class="daemon">winbindd</systemitem> Connectivity</title>
   <sect3>
    <title>Sending a <systemitem class="daemon">winbindd</systemitem> Ping</title>
    <para>
     To verify if the <systemitem class="daemon">winbindd</systemitem> service
     is able to connect to AD Domain Controllers (DC) or a primary domain
     controller (PDC), enter:
    </para>
<screen>
&prompt.cephuser.smb;wbinfo --ping-dc
checking the NETLOGON for domain[<replaceable>DOMAIN</replaceable>] dc connection to "DC.DOMAIN.EXAMPLE.COM" succeeded
</screen>
    <para>
     If the previous command fails, verify that the
     <systemitem class="daemon">winbindd</systemitem> service is running and
     that the <filename>smb.conf</filename> file is set up correctly.
    </para>
   </sect3>
   <sect3>
    <title>Looking Up Domain Users and Groups</title>
    <para>
     The <systemitem>libnss_winbind</systemitem> library enables you to look up
     domain users and groups. For example, to look up the domain user
     'DOMAIN\demo01':
    </para>
<screen>
&prompt.cephuser.smb;getent passwd DOMAIN\\demo01
DOMAIN\demo01:*:10000:10000:demo01:/home/demo01:/bin/bash
</screen>
    <para>
     To look up the domain group 'Domain Users':
    </para>
<screen>
&prompt.cephuser.smb;getent group "DOMAIN\\Domain Users"
DOMAIN\domain users:x:10000:
</screen>
   </sect3>
   <sect3>
    <title>Assigning File Permissions to Domain Users and Groups</title>
    <para>
     The name service switch (NSS) library enables you to use domain user
     accounts and groups in commands. For example to set the owner of a file to
     the 'demo01' domain user and the group to the 'Domain Users' domain group,
     enter:
    </para>
<screen>
&prompt.cephuser.smb;chown "DOMAIN\\demo01:DOMAIN\\domain users" file.txt
</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
