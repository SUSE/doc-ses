<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ses-cifs">
<!-- ============================================================== -->
 <title>Exporting &ceph; Data via &samba;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter describes how to export data stored in a &ceph; cluster via a
  &samba;/CIFS share so that you can easily access them from Windows* client
  machines. It also includes information that will help you configure a &ceph;
  &samba; gateway to join &ad; in the Windows* domain to authenticate and
  authorize users.
 </para>
 <note>
  <title>&samba; Gateway Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &cephfs;
   via a &samba; Gateway may significantly reduce application performance when
   compared to native &ceph; clients.
  </para>
 </note>
 <sect1 xml:id="cephfs-samba">
  <title>Export &cephfs; via Samba Share</title>

  <warning>
   <title>Cross Protocol Access</title>
   <para>
    Native &cephfs; and NFS clients are not restricted by file locks obtained
    via Samba, and vice versa. Applications that rely on cross protocol file
    locking may experience data corruption if &cephfs; backed Samba share paths
    are accessed via other means.
   </para>
  </warning>

  <sect2 xml:id="cephfs-samba-packages">
   <title>&samba; Related Packages Installation</title>
   <para>
    To configure and export a &samba; share, the following packages need to be
    installed: <package>samba-ceph</package> and
    <package>samba-winbind</package>. If these packages are not installed,
    install them:
   </para>
<screen>
&prompt.cephuser.smb;zypper install samba-ceph samba-winbind
</screen>
  </sect2>

  <sect2 xml:id="sec-ses-cifs-example">
   <title>Single Gateway Example</title>
   <para>
    In preparation for exporting a Samba share, choose an appropriate node to
    act as a &sgw;. The node needs to have access to the &ceph; client network,
    as well as sufficient CPU, memory, and networking resources.
   </para>
   <para>
    Failover functionality can be provided with CTDB and the &sle; &hasi;.
    Refer to <xref linkend="sec-ses-cifs-ha"/> for more information on HA
    setup.
   </para>
   <procedure>
    <step>
     <para>
      Make sure that a working &cephfs; already exists in your cluster. For
      details, see <xref linkend="cha-ceph-as-cephfs"/>.
     </para>
    </step>
    <step>
     <para>
      Create a &sgw; specific keyring on the &ceph; admin node and copy it to
      both &sgw; nodes:
     </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
     <para>
      Replace <replaceable>SAMBA_NODE</replaceable> with the name of the Samba
      gateway node.
     </para>
    </step>
    <step>
     <para>
      The following steps are executed on the &sgw; node. Install Samba
      together with the &ceph; integration package:
     </para>
<screen>&prompt.cephuser.smb;sudo zypper in samba samba-ceph</screen>
    </step>
    <step>
     <para>
      Replace the default contents of the
      <filename>/etc/samba/smb.conf</filename> file with the following:
     </para>
<screen>
[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<replaceable>SHARE_NAME</replaceable>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
 </screen>
     <tip>
      <title>Oplocks and Share Modes</title>
      <para>
       <option>oplocks</option> (also known as SMB2+ leases) allow for improved
       performance through aggressive client caching, but are currently unsafe
       when Samba is deployed together with other &cephfs; clients, such as
       kernel <literal>mount.ceph</literal>, FUSE, or &ganesha;.
      </para>
      <para>
       Currently <option>kernel share modes</option> needs to be disabled in a
       share running with the &cephfs; vfs module for file serving to work
       properly.
      </para>
     </tip>
     <important>
      <title>Permitting Access</title>
      <para>
       Since <systemitem>vfs_ceph</systemitem> does not require a file system
       mount, the share path is interpreted as an absolute path within the
       &ceph; file system on the attached &ceph; cluster. For successful share
       I/O, the path's access control list (ACL) needs to permit access from
       the mapped user for the given Samba client. You can modify the ACL by
       temporarily mounting via the &cephfs; kernel client and using the
       <command>chmod</command>, <command>chown</command> or
       <command>setfacl</command> utilities against the share path. For
       example, to permit access for all users, run:
      </para>
<screen>
&prompt.root;chmod 777 <replaceable>MOUNTED_SHARE_PATH</replaceable>
</screen>
     </important>
    </step>
    <step>
     <para>
      Start and enable the Samba daemon:
     </para>
<screen>
&prompt.cephuser.smb;sudo systemctl start smb.service
&prompt.cephuser.smb;sudo systemctl enable smb.service
&prompt.cephuser.smb;sudo systemctl start nmb.service
&prompt.cephuser.smb;sudo systemctl enable nmb.service
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-ses-cifs-ha">
   <title>High Availability Configuration</title>
   <important>
    <title>Transparent Failover Not Supported</title>
    <para>
     Although a multi-node Samba + CTDB deployment is more highly available
     compared to the single node (see <xref linkend="cha-ses-cifs"/>),
     client-side transparent failover is not supported. Applications will
     likely experience a short outage on &sgw; node failure.
    </para>
   </important>
   <para>
    This section provides an example of how to set up a two-node high
    availability configuration of Samba servers. The setup requires the &sle;
    &hasi;. The two nodes are called &si.earth;
    (<systemitem class="ipaddress">192.168.1.1</systemitem>) and &si.mars;
    (<systemitem class="ipaddress">192.168.1.2</systemitem>).
   </para>
   <para>
    For details about &sle; &hasi;, see
    <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/"/>.
   </para>
   <para>
    Additionally, two floating virtual IP addresses allow clients to connect to
    the service no matter which physical node it is running on.
    <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
    administration with Hawk2 and
    <systemitem class="ipaddress">192.168.2.1</systemitem> is used exclusively
    for the CIFS exports. This makes it easier to apply security restrictions
    later.
   </para>
   <para>
    The following procedure describes the example installation. More details
    can be found at
    <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/"/>.
   </para>
   <procedure xml:id="proc-sec-ses-cifs-ha">
    <step>
     <para>
      Create a &sgw; specific keyring on the &adm; and copy it to both nodes:
     </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring &si.earth;:/etc/ceph/
&prompt.cephuser;<command>scp</command> ceph.client.samba.gw.keyring &si.mars;:/etc/ceph/</screen>
    </step>
    <step>
     <para>
      SLE-HA setup requires a fencing device to avoid a <emphasis>split
      brain</emphasis> situation when active cluster nodes become
      unsynchronized. For this purpose, you can use a &ceph; RBD image with
      Stonith Block Device (SBD). Refer to
      <link
       xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#sec-ha-storage-protect-fencing-setup"/>
      for more details.
     </para>
     <para>
      If it does not yet exist, create an RBD pool called
      <literal>rbd</literal> (see
      <xref linkend="ceph-pools-operate-add-pool"
       />) and associate it
      with <literal>rbd</literal> (see
      <xref
       linkend="ceph-pools-associate" />). Then create a related
      RBD image called <literal>sbd01</literal>:
     </para>
<screen>
&prompt.cephuser;ceph osd pool create rbd <replaceable>PG_NUM</replaceable> <replaceable>PGP_NUM</replaceable> replicated
&prompt.cephuser;ceph osd pool application enable rbd rbd
&prompt.cephuser;rbd -p rbd create sbd01 --size 64M --image-shared
</screen>
    </step>
    <step>
     <para>
      Prepare &si.earth; and &si.mars; to host the Samba service:
     </para>
     <substeps>
      <step>
       <para>
        Make sure the following packages are installed before you proceed:
        <package>ctdb</package>, <package>tdb-tools</package>, and
        <package>samba</package> (needed for
        <systemitem
         class="daemon">smb.service</systemitem> and
        <systemitem
         class="daemon">nmb.service</systemitem>).
       </para>
<screen>&prompt.cephuser.smb;<command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
      </step>
      <step>
       <para>
        Make sure the services <literal>ctdb</literal>, <literal>smb</literal>,
        and <literal>nmb</literal> are stopped and disabled:
       </para>
<screen>&prompt.cephuser.smb;sudo systemctl disable ctdb
&prompt.cephuser.smb;sudo systemctl disable smb
&prompt.cephuser.smb;sudo systemctl disable nmb
&prompt.cephuser.smb;sudo systemctl stop smb
&prompt.cephuser.smb;sudo systemctl stop nmb</screen>
      </step>
      <step>
       <para>
        Open port <literal>4379</literal> of your firewall on all nodes. This
        is needed for CTDB to communicate with other cluster nodes.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      On &si.earth;, create the configuration files for Samba. They will later
      automatically synchronize to &si.mars;.
     </para>
     <substeps>
      <step>
       <para>
        Insert a list of private IP addresses of &sgw; nodes in the
        <filename>/etc/ctdb/nodes</filename> file. Find more details in the
        ctdb manual page (<command>man 7 ctdb</command>).
       </para>
<screen>&subnetI;.1
&subnetI;.2</screen>
      </step>
      <step>
       <para>
        Configure Samba. Add the following lines in the
        <literal>[global]</literal> section of
        <filename>/etc/samba/smb.conf</filename>. Use the host name of your
        choice in place of <replaceable>CTDB-SERVER</replaceable> (all nodes in
        the cluster will appear as one big node with this name). Add a share
        definition as well, consider <replaceable>SHARE_NAME</replaceable> as
        an example:
       </para>
<screen>
[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
</screen>
       <para>
        Note that the <filename>/etc/ctdb/nodes</filename> and
        <filename>/etc/samba/smb.conf</filename> files need to match on all
        &sgw; nodes.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Install and bootstrap the &sle; &ha; cluster.
     </para>
     <substeps>
      <step>
       <para>
        Register the &sle; &hasi; on &si.earth; and &si.mars;:
       </para>
<screen>&prompt.earth;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen>&prompt.mars;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
      </step>
      <step>
       <para>
        Install <package>ha-cluster-bootstrap</package> on both nodes:
       </para>
<screen>&prompt.earth;<command>zypper</command> in ha-cluster-bootstrap</screen>
<screen>&prompt.mars;<command>zypper</command> in ha-cluster-bootstrap</screen>
      </step>
      <step>
       <para>
        Map the RBD image <literal>sbd01</literal> on both &sgw;s via
        <systemitem class="daemon">rbdmap.service</systemitem>.
       </para>
       <para>
        Edit <filename>/etc/ceph/rbdmap</filename> and add an entry for the SBD
        image:
       </para>
<screen>rbd/sbd01 id=samba.gw,keyring=/etc/ceph/ceph.client.samba.gw.keyring</screen>
       <para>
        Enable and start
        <systemitem class="daemon">rbdmap.service</systemitem>:
       </para>
<screen>
&prompt.earth;systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
&prompt.mars;systemctl enable rbdmap.service &amp;&amp; systemctl start rbdmap.service
</screen>
       <para>
        The <filename>/dev/rbd/rbd/sbd01</filename> device should be available
        on both &sgw;s.
       </para>
      </step>
      <step>
       <para>
        Initialize the cluster on &si.earth; and let &si.mars; join it.
       </para>
<screen>&prompt.earth;<command>ha-cluster-init</command></screen>
<screen>&prompt.mars;<command>ha-cluster-join</command> -c earth</screen>
       <important>
        <para>
         During the process of initialization and joining the cluster, you will
         be interactively asked whether to use SBD. Confirm with
         <option>y</option> and then specify
         <filename>/dev/rbd/rbd/sbd01</filename> as a path to the storage
         device.
        </para>
       </important>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Check the status of the cluster. You should see two nodes added in the
      cluster:
     </para>
<screen>&prompt.earth;<command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
    </step>
    <step>
     <para>
      Execute the following commands on &si.earth; to configure the CTDB
      resource:
     </para>
<!--taroth 2012-02-07: fixed bnc#745334, default timeout for start/stop-->
<screen>&prompt.earth;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
&prompt.crm.conf;<command>primitive</command> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>primitive</command> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>group</command> g-ctdb ctdb nmb smb
&prompt.crm.conf;<command>clone</command> cl-ctdb g-ctdb meta interleave="true"
&prompt.crm.conf;<command>commit</command></screen>
     <para>
      The binary
      <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> in the
      configuration option <literal>ctdb_recovery_lock</literal> has the
      parameters <replaceable>CLUSTER_NAME</replaceable>,
      <replaceable>CEPHX_USER</replaceable>,
      <replaceable>RADOS_POOL</replaceable>, and
      <replaceable>RADOS_OBJECT</replaceable>, in this order.
     </para>
     <para>
      An extra lock-timeout parameter can be appended to override the default
      value used (10 seconds). A higher value will increase the CTDB recovery
      master failover time, whereas a lower value may result in the recovery
      master being incorrectly detected as down, triggering flapping failovers.
     </para>
    </step>
    <step>
     <para>
      Add a clustered IP address:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip ocf:heartbeat:IPaddr2<!--
    --> params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
&prompt.crm.conf;<command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
&prompt.crm.conf;<command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>commit</command></screen>
     <para>
      If <literal>unique_clone_address</literal> is set to
      <literal>true</literal>, the IPaddr2 resource agent adds a clone ID to
      the specified address, leading to three different IP addresses. These are
      usually not needed, but help with load balancing. For further information
      about this topic, see
      <link xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#cha-ha-lb"/>.
     </para>
    </step>
    <step>
     <para>
      Check the result:
     </para>
<screen>&prompt.earth;<command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
    </step>
    <step>
     <para>
      Test from a client machine. On a Linux client, run the following command
      to see if you can copy files from and to the system:
     </para>
<screen>&prompt.root;<command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
