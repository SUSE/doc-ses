<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ses-cifs">
<!-- ============================================================== -->
 <title>Exporting &ceph; Data via &samba;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section describes how to export &cephfs; via a Samba/CIFS share. Samba
  shares can be used with Windows* clients.
 </para>
 <note>
  <title>&samba; Gateway Performance</title>
  <para>
   Due to increased protocol overhead and additional latency caused by extra
   network hops between the client and the storage, accessing &cephfs; via a
   &samba; Gateway may significantly reduce application performance when
   compared to native &ceph; clients.
  </para>
 </note>
 <warning>
  <title>Cross Protocol Access</title>
  <para>
   Native &cephfs; and NFS clients are not restricted by file locks obtained
   via Samba, and vice-versa. Applications that rely on cross protocol file
   locking may experience data corruption if &cephfs; backed Samba share paths
   are accessed via other means.
  </para>
 </warning>
 <sect1 xml:id="sec-ses-cifs-example">
  <title>Single Gateway Example</title>

  <para>
   In preparation for exporting a Samba share, choose an appropriate node to
   act as a Samba gateway. The node needs to have access to the &ceph; client
   network, as well as sufficient CPU, memory and networking resources.
  </para>

  <para>
   Failover functionality can be provided with CTDB and the &sle; &hasi;. Refer
   to <xref linkend="sec-ses-cifs-ha"/> for more information on HA setup.
  </para>

  <procedure>
   <step>
    <para>
     Make sure that a working &cephfs; already exists in your cluster. For
     details, see <xref linkend="cha-ceph-as-cephfs"/>.
    </para>
   </step>
   <step>
    <para>
     Create a Samba Gateway specific keyring on the &ceph; admin node and copy
     it to the Samba gateway node:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
 osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.smaster;<command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     Replace <replaceable>SAMBA_NODE</replaceable> with the name of the Samba
     gateway node.
    </para>
   </step>
   <step>
    <para>
     The following steps are executed on the Samba gateway node. Install Samba
     together with the &ceph; integration package:
    </para>
<screen>&prompt.root;<command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     Replace the default contents of the
     <filename>/etc/samba/smb.conf</filename> file with the following:
    </para>
<screen>
[global]
  netbios name = SAMBA-GW
  clustering = no
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[<replaceable>SHARE_NAME</replaceable>]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
 </screen>
    <tip>
     <title>Oplocks and Share Modes</title>
     <para>
      <option>oplocks</option> (also known as SMB2+ leases) allow for improved
      performance through aggressive client caching, but are currently unsafe
      when Samba is deployed together with other &cephfs; clients, such as
      kernel <literal>mount.cephfs</literal>, FUSE, or &ganesha;.
     </para>
     <para>
      Currently <option>kernel share modes</option> needs to be disabled in a
      share running with the &cephfs; vfs module for file serving to work
      properly.
     </para>
    </tip>
    <important>
     <title>Permitting Access</title>
     <para>
      Since <systemitem>vfs_ceph</systemitem> does not require a file system
      mount, the share path is interpreted as an absolute path within the
      &ceph; file system on the attached &ceph; cluster. For successful share
      I/O, the path's access control list (ACL) needs to permit access from the
      mapped user for the given Samba client. You can modify the ACL by
      temporarily mounting via the &cephfs; kernel client and using the
      <command>chmod</command>, <command>chown</command> or
      <command>setfacl</command> utilities against the share path. For example,
      to permit access for all users, run:
     </para>
<screen>
&prompt.root;chmod 777 <replaceable>MOUNTED_SHARE_PATH</replaceable>
</screen>
    </important>
   </step>
   <step>
    <para>
     Start and enable the Samba daemon:
    </para>
<screen>
&prompt.root;<command>systemctl</command> start smb.service
&prompt.root;<command>systemctl</command> enable smb.service
&prompt.root;<command>systemctl</command> start nmb.service
&prompt.root;<command>systemctl</command> enable nmb.service
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-ses-cifs-ha">
  <title>High Availability Configuration</title>

  <important>
   <title>Transparent Failover Not Supported</title>
   <para>
    Although a multi-node Samba + CTDB deployment is more highly available
    compared to the single node (see <xref linkend="cha-ses-cifs"/>),
    client-side transparent failover is not supported. Applications will likely
    experience a short outage on Samba gateway node failure.
   </para>
  </important>

  <para>
   This section provides an example of how to set up a two-node high
   availability configuration of Samba servers. The setup requires the &sle;
   &hasi;. The two nodes are called &si.earth;
   (<systemitem class="ipaddress">192.168.1.1</systemitem>) and &si.mars;
   (<systemitem class="ipaddress">192.168.1.2</systemitem>).
  </para>

  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/"/>.
  </para>

  <para>
   Additionally, two floating virtual IP addresses allow clients to connect to
   the service no matter which physical node it is running on.
   <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
   administration with Hawk2 and
   <systemitem class="ipaddress">192.168.2.1</systemitem> is used exclusively
   for the CIFS exports. This makes it easier to apply security restrictions
   later.
  </para>

  <para>
   The following procedure describes the example installation. More details can
   be found at
   <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/"/>.
  </para>

  <procedure xml:id="proc-sec-ses-cifs-ha">
   <step>
    <para>
     Create a Samba Gateway specific keyring on the &smaster; and copy it to
     both nodes:
    </para>
<screen>&prompt.cephuser;<command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
&prompt.smaster;<command>scp</command> ceph.client.samba.gw.keyring &si.earth;:/etc/ceph/
&prompt.smaster;<command>scp</command> ceph.client.samba.gw.keyring &si.mars;:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     Prepare &si.earth; and &si.mars; to host the Samba service:
    </para>
    <substeps>
     <step>
      <para>
       Make sure the following packages are installed before you proceed:
       <package>ctdb</package>, <package>tdb-tools</package>, and
       <package>samba</package> (needed for smb and nmb resources).
      </para>
<screen>&prompt.root;<command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       Make sure the services <literal>ctdb</literal>, <literal>smb</literal>,
       and <literal>nmb</literal> are stopped and disabled:
      </para>
<screen>&prompt.root;<command>systemctl</command> disable ctdb
&prompt.root;<command>systemctl</command> disable smb
&prompt.root;<command>systemctl</command> disable nmb
&prompt.root;<command>systemctl</command> stop smb
&prompt.root;<command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       Open port <literal>4379</literal> of your firewall on all nodes. This is
       needed for CTDB to communicate with other cluster nodes.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     On &si.earth; create the configuration files for Samba. They will later
     automatically be synchronized to &si.mars;.
    </para>
    <substeps>
     <step>
      <para>
       In <filename>/etc/ctdb/nodes</filename> insert all nodes which contain
       all private IP addresses of each node in the cluster:
      </para>
<screen>&subnetI;.1
&subnetI;.2</screen>
     </step>
     <step>
      <para>
       Configure Samba. Add the following lines in the
       <literal>[global]</literal> section of
       <filename>/etc/samba/smb.conf</filename>. Use the host name of your
       choice in place of "CTDB-SERVER" (all nodes in the cluster will appear
       as one big node with this name, effectively). Add a share definition as
       well, consider <replaceable>SHARE_NAME</replaceable> as an example:
      </para>
<screen>
[global]
  netbios name = SAMBA-HA-GW
  clustering = yes
  idmap config * : backend = tdb2
  passdb backend = tdbsam
  ctdbd socket = /var/lib/ctdb/ctdb.socket
  # disable print server
  load printers = no
  smbd: backgroundqueue = no

[SHARE_NAME]
  path = /
  vfs objects = ceph
  ceph: config_file = /etc/ceph/ceph.conf
  ceph: user_id = samba.gw
  read only = no
  oplocks = no
  kernel share modes = no
</screen>
      <para>
       Note that the <filename>/etc/ctdb/nodes</filename> and
       <filename>/etc/samba/smb.conf</filename> files need to match on all
       Samba gateway nodes.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Install and bootstrap the &sle; &ha; cluster.
    </para>
    <substeps>
     <step>
      <para>
       Register the &sle; &hasi; on &si.earth; and &si.mars;.
      </para>
<screen>&prompt.earth;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen>&prompt.mars;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       Install <package>ha-cluster-bootstrap</package> on both nodes:
      </para>
<screen>&prompt.earth;<command>zypper</command> in ha-cluster-bootstrap</screen>
<screen>&prompt.mars;<command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       Initialize the cluster on &si.earth;:
      </para>
<screen>
&prompt.earth;<command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       Let &si.mars; join the cluster:
      </para>
<screen>
&prompt.mars;<command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Check the status of the cluster. You should see two nodes added in
     cluster:
    </para>
<screen>&prompt.earth;<command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     Execute the following commands on &si.earth; to configure the CTDB
     resource:
    </para>
<!--taroth 2012-02-07: fixed bnc#745334, default timeout for start/stop-->
<screen>&prompt.earth;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="200" \
        op stop interval="0" timeout="100"
&prompt.crm.conf;<command>primitive</command> nmb systemd:nmb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>primitive</command> smb systemd:smb \
    op start timeout="100" interval="0" \
    op stop timeout="100" interval="0" \
    op monitor interval="60" timeout="100"
&prompt.crm.conf;<command>group</command> g-ctdb ctdb nmb smb
&prompt.crm.conf;<command>clone</command> cl-ctdb g-ctdb meta interleave="true"
&prompt.crm.conf;<command>commit</command></screen>
    <para>
     The binary <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command>
     in the configuration option <literal>ctdb_recovery_lock</literal> has the
     parameters <replaceable>CLUSTER_NAME</replaceable>
     <replaceable>CEPHX_USER</replaceable>
     <replaceable>RADOS_POOL</replaceable>
     <replaceable>RADOS_OBJECT</replaceable> in this order.
    </para>
    <para>
     An extra lock-timeout parameter can be appended to override the default
     value used (10 seconds). A higher value will increase the CTDB recovery
     master failover time, a lower value may result in the recovery master
     being incorrectly detected as down, triggering flapping failovers.
    </para>
   </step>
   <step>
    <para>
     Add a clustered IP address:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip ocf:heartbeat:IPaddr2<!--
    --> params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
&prompt.crm.conf;<command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
&prompt.crm.conf;<command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
&prompt.crm.conf;<command>commit</command></screen>
    <para>
     If <literal>unique_clone_address</literal> is set to
     <literal>true</literal>, the IPaddr2 resource agent adds a clone ID to the
     specified address, leading to three different IP addresses. These are
     usually not needed, but help with load balancing. For further information
     about this topic, see
     <link xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-ha-lb"/>.
    </para>
   </step>
   <step>
    <para>
     Check the result:
    </para>
<screen>&prompt.earth;<command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     Test from a client machine. On a Linux client, run the following command
     to see if you can copy files from and to the system:
    </para>
<screen>&prompt.root;<command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
