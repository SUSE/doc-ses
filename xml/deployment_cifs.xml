<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha.ses.cifs">
<!-- ============================================================== -->
 <title>Export &cephfs; via Samba</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section describes how to export &cephfs; via Samba share. Samba shares
  can be used with Windows* clients.
 </para>
 <warning>
  <title>Technology Preview</title>
  <para>
   As of &storage; 5, <literal>CIFS Export</literal> is considered a technology
   preview and is not supported.
  </para>
 </warning>
 <sect1 xml:id="sec.ses.cifs.example">
  <title>Example Installation</title>
  <para>
   Exporting &cephfs; is not a technology preview and not supported. To enable
   CIFS, you have to manually install Samba on one cluster node and configure it.
   Failover functionality can be provided with CTDB and the &sle; &hasi;.
  </para>
  <para>
   First install the Samba daemon on one node:
  </para>
<screen>&prompt.root;<command>zypper</command> in samba</screen>
  <para>
   Then edit the <filename>/etc/smb/smb.conf</filename> and add the following
   section:
  </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
ceph:config_file = /etc/ceph/ceph.conf</screen>
  <para>
   Start and enable the Samba daemon:
  </para>
<screen>&prompt.root;<command>systemctl</command> start smb.service
&prompt.root;<command>systemctl</command> enable smb.service</screen>
 </sect1>
 <sect1 xml:id="sec.ses.cifs.ha">
  <title>High Availability Active-Passive Configuration</title>
  <para>
   This section provides an example of how to set up a two-ode active-passive
   configuration of &ganesha; servers. The setup requires the &sle; &hasi;.
   The 2 nodes are called &si.earth;
   and &si.mars;.
  </para>
  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://www.suse.com/documentation/sle-ha-12/" />.
  </para>
  <para>
   In this setup &si.earth; has the
   IP address <systemitem class="ipaddress">192.168.1.1</systemitem> and
   &si.mars; has the address
   <systemitem class="ipaddress">192.168.1.2</systemitem>.
  </para>
  <para>
   Additionally 2 floating virtual IP addresses are used, allowing clients
   to connect to the service independent of which physical node it is running on.
   <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
   administration with Hawk2 and <systemitem class="ipaddress">192.168.2.1</systemitem>
   is used exclusively for the NFS exports. This makes it easier
   to apply security restrictions later.
  </para>
  <para>
   The following procedure describes the example installation. More details
   can be found at <link xlink:href=
   "https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html" />.
  </para>
  <procedure xml:id="proc.sec.ses.cifs.ha">
   <step>
    <para>
     Prepare the &ganesha; nodes on the &smaster;:
    </para>
    <substeps>
     <step>
      <para>
       Run &deepsea; stages 0 and 1 on the &smaster;.
      </para>
      <screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1
      </screen>
     </step>
     <step>
      <para>
       Assign the nodes &si.earth; and
       &si.mars; the <literal>role-ganesha</literal>
       in the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
      </para>
      <screen>
role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls
      </screen>
     </step>
     <step>
      <para>
       Run &deepsea; stages 3 and 4 on the &smaster;.
      </para>
      <screen>
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Register the &sle; &hasi; on &si.earth;
     and &si.mars;.
    </para>
    <screen>
&prompt.root;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
    </screen>
   </step>
   <step>
    <para>
     Install <package>ha-cluster-bootstrap</package> on both nodes:
    </para>
    <screen>
&prompt.root;<command>zypper</command> in ha-cluster-bootstrap
    </screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Init the cluster on &si.earth;:
      </para>
      <screen>
&prompt.earth;<command>ha-cluster-bootstrap</command>
      </screen>
     </step>
     <step>
      <para>
       Let &si.mars; join the cluster:
      </para>
      <screen>
&prompt.mars;<command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Check the status of cluster. You should see two nodes added in cluster:
    </para>
    <screen>
&prompt.earth;<command>crm</command> status
    </screen>
   </step>
   <step>
    <para>
     On both nodes, disable the automatic start of the &ganesha; service at boot time:
    </para>
    <screen>
&prompt.root;<command>systemctl</command> disable nfs-ganesha
    </screen>
   </step>
   <step>
    <para>
     Start the <command>crm</command> shell on &si.earth;:
    </para>
    <screen>
&prompt.earth;<command>crm</command> configure
    </screen>
    <para>
     The next commands are executed in the crm shell.
    </para>
   </step>
   <step>
    <para>
     On &si.earth;, run the crm shell
     to execute the following commands to configure
     the resource for &ganesha; daemons as clone of systemd resource type:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>commit
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>status
    2 nodes configured 
    2 resources configured

    Online: [ earth mars ]
              
    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]
    </screen>
   </step>
   <step>
    <para>
     Create a primitive IPAddr2 with the crm shell:
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth
    </screen>
   </step>
   <step>
    <para>
     To set up relation between the &ganesha; server and the floating Virtual IP,
     we use colocation and ordering. 
    </para>
    <screen>
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt xmlns='http://docbook.org/ns/docbook'>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
    </screen>
   </step>
   <step>
    <para>
     Use the <command>mount</command> command from the client to ensure
     that cluster setup is complete:
    </para>
    <screen>
&prompt.root;<command>mount</command> -t nfs -v -o nfsvers=4 192.168.2.1:/ /mnt
    </screen>
   </step>
  </procedure>
  <sect2 xml:id="sec.ses.cifs.ha.cleanup">
   <title>Cleanup Resources</title>
   <para>
    In event of &ganesha; failure at one of the node, for example &si.earth;,
    fix the issue and  cleanup the resource. Only when the resource is cleaned
    up, the resource can fail back to &si.earth; in case &ganesha; fails at
    &si.mars;.
   </para>
   <para>
    To clean up the resource:
   </para>
   <screen>
&prompt.earth;<command>crm</command> resource cleanup nfs-ganesha-clone earth
&prompt.earth;<command>crm</command> resource cleanup ganesha-ip earth
   </screen>
  </sect2>
 </sect1>
</chapter>
