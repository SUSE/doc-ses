<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha.ses.cifs">
<!-- ============================================================== -->
 <title>Export &cephfs; via Samba</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This section describes how to export &cephfs; via Samba share. Samba shares
  can be used with Windows* clients.
 </para>
 <warning>
  <title>Technology Preview</title>
  <para>
   As of &storage; 5, <literal>CIFS Export</literal> is considered a technology
   preview and is not supported.
  </para>
 </warning>
 <sect1 xml:id="sec.ses.cifs.example">
  <title>Example Installation</title>
  <para>
   Exporting &cephfs; is a technology preview and not supported. To enable
   CIFS, you have to manually install Samba on one cluster node and configure it.
   Failover functionality can be provided with CTDB and the &sle; &hasi;.
  </para>
  <para>
   First install the Samba daemon on one node:
  </para>
<screen>&prompt.root;<command>zypper</command> in samba</screen>
  <para>
   Then edit the <filename>/etc/smb/smb.conf</filename> and add the following
   section:
  </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
ceph:config_file = /etc/ceph/ceph.conf</screen>
  <para>
   Start and enable the Samba daemon:
  </para>
<screen>&prompt.root;<command>systemctl</command> start smb.service
&prompt.root;<command>systemctl</command> enable smb.service</screen>
 </sect1>
 <sect1 xml:id="sec.ses.cifs.ha">
  <title>High Availability Active-Passive Configuration</title>
  <para>
   This section provides an example of how to set up a two-node active-passive
   configuration of Samba servers. The setup requires the &sle; &hasi;.
   The 2 nodes are called &si.earth; and &si.mars;.
  </para>
  <para>
   For details about &sle; &hasi;, see
   <link xlink:href="https://www.suse.com/documentation/sle-ha-12/" />.
  </para>
  <para>
   In this setup &si.earth; has the
   IP address <systemitem class="ipaddress">192.168.1.1</systemitem> and
   &si.mars; has the address
   <systemitem class="ipaddress">192.168.1.2</systemitem>.
  </para>
  <para>
   Additionally 2 floating virtual IP addresses are used, allowing clients
   to connect to the service independent of which physical node it is running on.
   <systemitem class="ipaddress">192.168.1.10</systemitem> is used for cluster
   administration with Hawk2 and <systemitem class="ipaddress">192.168.2.1</systemitem>
   is used exclusively for the CIFS exports. This makes it easier to apply
   security restrictions later.
  </para>
  <para>
   The following procedure describes the example installation. More details
   can be found at <link xlink:href=
   "https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html" />.
  </para>
  <procedure xml:id="proc.sec.ses.cifs.ha">
   <step>
    <para>
     Prepare the nodes used to host the Samba service:
    </para>
    <substeps>
     <step>
      <para>
       Make sure the following packages are installed on &si.earth; and &si.mars;
       before you proceed: <package>ctdb</package>,
       <package>tdb-tools</package>, and <package>samba</package>
       (needed for smb and nmb resources). 
      </para>
<screen>&prompt.root;<command>zypper</command> in ctdb tdb-tools samba</screen>
     </step>
     <step>
      <para>
       Make sure the services <literal>ctdb</literal>, <literal>smb</literal>,
       and <literal>nmb</literal> are disabled: 
      </para>
<screen>&prompt.root;<command>systemctl</command> disable ctdb
&prompt.root;<command>systemctl</command> disable smb
&prompt.root;<command>systemctl</command> disable nmb</screen>
     </step>
     <step>
      <para>
       Open port <literal>4379</literal> of your firewall on all nodes. This
       is needed for CTDB to communicate with other cluster nodes.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Create a directory for the CTDB lock on the shared file system:
    </para>
<screen>&prompt.root;<command>mkdir</command> -p /srv/samba/</screen>
   </step>
   <step>
    <para>
     In <filename>/etc/ctdb/nodes</filename> insert all nodes which contain
     all private IP addresses of each node in the cluster:
    </para>
<screen>&subnetI;.1
&subnetI;.2</screen>
   </step>
   <step>
    <para>
     Configure Samba. Add the following lines in the
     <literal>[global]</literal> section of
     <filename>/etc/samba/smb.conf</filename> on &si.earth; and &si.mars;.
     Use the host name of your choice in place of "CTDB-SERVER" (all nodes
     in the cluster will appear as one big node with this name, effectively):
    </para>
<screen>[global]
    # ...
    # settings applicable for all CTDB deployments
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket
    # settings necessary for CTDB on OCFS2
    fileid:algorithm = fsid
    vfs objects = fileid
    # ...</screen>
    <para>
     Alternatively, <command>csync2</command> can be used to synchronize
     config files. For details, see
     <link xlink:href=
     "https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"
     />.
    </para>
   </step>
   <step>
    <para>
     Register the &sle; &hasi; on &si.earth; and &si.mars;.
    </para>
    <screen>
&prompt.root;<command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
    </screen>
   </step>
   <step>
    <para>
     Install <package>ha-cluster-bootstrap</package> on both nodes:
    </para>
    <screen>
&prompt.root;<command>zypper</command> in ha-cluster-bootstrap
    </screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Init the cluster on &si.earth;:
      </para>
      <screen>
&prompt.earth;<command>ha-cluster-bootstrap</command>
      </screen>
     </step>
     <step>
      <para>
       Let &si.mars; join the cluster:
      </para>
      <screen>
&prompt.mars;<command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Check the status of cluster. You should see two nodes added in cluster:
    </para>
    <screen>
&prompt.earth;<command>crm</command> status
    </screen>
   </step>
   <step>
    <para>
     Start the <command>crm</command> shell on &si.earth;:
    </para>
    <screen>
&prompt.earth;<command>crm</command> configure
    </screen>
   </step>
   <step>
    <para>
     Execute the following commands to configure the the CTDB resource:
    </para>
<!--taroth 2012-02-07: fixed bnc#745334, default timeout for start/stop-->
<screen>&prompt.earth;<command>crm</command> configure
&prompt.crm.conf;<command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \ 
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="/srv/samba/ctdb.lock" \
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \ 
      op monitor interval="10" timeout="20" \
      op start interval="0" timeout="90" \
      op stop interval="0" timeout="100"
&prompt.crm.conf;<command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
&prompt.crm.conf;<command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
&prompt.crm.conf;<command>group</command> ctdb-group ctdb nmb smb
&prompt.crm.conf;<command>clone</command> ctdb-clone ctdb-group meta interleave="true"
&prompt.crm.conf;<command>colocation</command> ctdb-with-cephfs inf: ctdb-clone c-cephfs
&prompt.crm.conf;<command>order</command> cephfs-then-ctdb inf: c-cephfs ctdb-clone
&prompt.crm.conf;<command>commit</command></screen>
   </step>
   <step>
    <para>
     Add a clustered IP address:
    </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip ocf:heartbeat:IPaddr2<!-- 
    --> params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
&prompt.crm.conf;<command>clone</command> ip-clone ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
&prompt.crm.conf;<command>colocation</command> col-with-ctdb 0: ip-clone ctdb-clone
&prompt.crm.conf;<command>order</command> o-with-ctdb 0: ip-clone ctdb-clone
&prompt.crm.conf;<command>commit</command></screen>
    <para>
     If <literal>unique_clone_address</literal> is set to
     <literal>true</literal>, the IPaddr2 resource agent adds a clone ID to
     the specified address, leading to three different IP addresses. These
     are usually not needed, but help with load balancing. For further
     information about this topic, see <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"
     />.
    </para>
   </step>
   <step>
    <para>
     Commit your change:
    </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
   </step>
   <step>
    <para>
     Check the result:
    </para>
<screen>&prompt.earth;<command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
Clone Set: c-cephfs [cephfs]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: ctdb-clone [ctdb-group]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: ip-clone [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     Test from a client machine. On a Linux client, run the following
     command to see if you can copy files from and to the system:
    </para>
<screen>&prompt.root;<command>smbclient</command> <option>//&subnetII;.222/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
