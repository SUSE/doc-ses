<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.calamari">
 <title>Calamari</title>
 <para>
  Calamari is a management and monitoring system for &ceph; storage cluster. It
  provides a Web user interface that makes &ceph; cluster monitoring very
  simple and handy.
 </para>
 <para>
  The Calamari installation procedure differs according to the used deployment
  procedure. If you deployed your &ceph; by using
  <command>ceph-deploy</command>, refer to
  <xref linkend="ceph-deploy.calamari.installation"/>. If you deployed your
  cluster by using &crow;, refer to
  <xref linkend="crowbar.calamari.installation"/>.
 </para>
 <sect1 xml:id="ceph-deploy.calamari.installation">
  <title>Installing Calamari with <command>ceph-deploy</command></title>

  <para>
   To install Calamari, do the following:
  </para>

  <procedure>
   <step>
    <para>
     Install the client part of Calamari:
    </para>
<screen>sudo zypper in romana</screen>
   </step>
   <step>
    <para>
     Initialize Calamari installation. You will be asked for superuser user
     name and password. These will be needed when logging in to the Web
     interface after the setup is complete.
    </para>
<screen>sudo calamari-ctl initialize
[INFO] Loading configuration..
[INFO] Starting/enabling salt...
[INFO] Starting/enabling postgres...
[INFO] Initializing database...
[INFO] Initializing web interface...
[INFO] You will now be prompted for login details for the administrative user
account.  This is the account you will use to log into the web interface once
setup is complete.
Username (leave blank to use 'root'):
Email address:
Password:
Password (again):
Superuser created successfully.
[INFO] Starting/enabling services...
[INFO] Restarting services...
[INFO] Complete.</screen>
   </step>
   <step>
    <para>
     Check the firewall status
    </para>
<screen>sudo /sbin/SuSEfirewall2 status</screen>
    <para>
     and if it is off, check its configuration and turn it on with
    </para>
<screen>sudo /sbin/SuSEfirewall2 on</screen>
    <para>
     You can find detailed information in
     <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </step>
   <step>
    <tip>
     <para>
      In order for Calamari to work correctly, the admin keyring needs to be
      installed on each monitor node:
     </para>
<screen>&prompt.cephuser;ceph-deploy admin mon1 mon2 mon3</screen>
     <para>
      where <replaceable>mon1</replaceable>, <replaceable>mon2</replaceable>,
      or <replaceable>mon3</replaceable> are the host names of the monitors.
     </para>
    </tip>
    <para>
     Now open your Web browser and point it to the host name/IP address of the
     server where you installed Calamari. Log in with the credentials you
     entered when installing the Calamari client. A welcome screen appears,
     instructing you to enter the <command>ceph-deploy calamari
     connect</command> command. Switch to the terminal on the Calamari host and
     enter the following command. Note that the <option>--master</option>
     option specifies the host name of the Calamari server to which all the
     cluster nodes connect to:
    </para>
<screen>&prompt.cephuser;ceph-deploy calamari connect --master <replaceable>master_host</replaceable> <replaceable>node1</replaceable> <replaceable>node2</replaceable> <replaceable>...</replaceable></screen>
    <para>
     After the command is successfully finished, reload the Web browser. Now
     you can monitor your &ceph; cluster, OSDs, pools, etc.
    </para>
    <tip>
     <title>Empty Usage Graphs</title>
     <para>
      If, after having installed Calamari initially, the usage graphs are empty/blank,
      it is possible that the diamond metrics collector was not automatically
      installed. To fix this, run <command>salt '*' state.highstate</command> on
      the Calamari host.
     </para>
    </tip>
    <important>
     <para>
      The Calamari dashboard screen shows the current status of the cluster.
      This updates regularly, so any change to the cluster state&mdash;for
      example if a node goes offline&mdash;should be reflected in Calamari
      within a few seconds. The <guimenu>Health</guimenu> panel includes a
      timer to indicate how long it has been since Calamari last saw heartbeat
      information from the cluster. Normally, this will not be more than one
      minute old, but in certain failure cases, for example when a network
      outage occurs or if the cluster loses quorum (that is if more than half
      of the monitor nodes are down), Calamari will no longer be able to
      determine cluster state. In this case, the <guimenu>Health</guimenu>
      panel will indicate that the last update was more than one minute ago.
      After too long time with no updates, Calamari displays a warning at the
      top of the screen "Cluster Updates Are Stale. The Cluster is not updating
      Calamari." If this occurs, the other status information Calamari presents
      will not be correct so you should investigate further to check the status
      of your storage nodes and network.
     </para>
    </important>
    <tip>
     <para>
      They may be leftovers of the previous Calamari setup on the system. If
      after logging in to the Calamari application some nodes are already
      joined or registered, run the following on the Calamari host to trigger a
      re-run of salt on all &ceph; nodes, which should clear up any odd state
      or missing bits and pieces.
     </para>
     <note>
      <title>Salt Installed by Default with Calamari</title>
      <para>
       Even though you deployed your &ceph; cluster by using
       <command>ceph-deploy</command>, salt is installed along with Calamari.
       The <command>salt</command> command is thus installed even though you
       did not install salt manually.
      </para>
     </note>
<screen>salt '*' state.highstate</screen>
     <para>
      We also recommend to remove files from the previous Calamari setup, such
      as state files, configuration files, or PostgreSQL database files. At
      minimum, remove the files in the following directories:
     </para>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        <filename>/etc/calamari/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/etc/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/*/salt/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/graphite/</filename>
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/pgsql/</filename>
       </para>
      </listitem>
     </itemizedlist>
    </tip>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="crowbar.calamari.installation">
  <title>Installing Calamari Using &crow;</title>

  <note>
   <title>Conflicts in Combination with Deployment using &crow;</title>
   <para>
    If you used &crow; to install &storage;, install Calamari on a different
    server than &crow; as &crow; uses the same port as Calamari (port 80).
   </para>
  </note>

  <para>
   Use the &crow; UI to deploy Calamari as described in
   <xref linkend="sec.depl.ceph.ceph"/>.
  </para>
 </sect1>
</chapter>
