<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-admin-ha">
 <title>&ceph; Admin Node HA Setup</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  <emphasis>&ceph; admin node</emphasis> is a &ceph; cluster node where the
  &smaster; service is running. The admin node is a central point of the &ceph;
  cluster because it manages the rest of the cluster nodes by querying and
  instructing their &sminion; services. It usually includes other services as
  well, for example the &oa; Web UI with the <emphasis>&grafana;</emphasis>
  dashboard backed by the <emphasis>&prometheus;</emphasis> monitoring toolkit.
 </para>
 <para>
  In case of the &ceph; admin node failure, you usually need to provide a new
  working hardware for the node and restore the complete cluster configuration
  stack from a recent backup. Such method is time consuming and causes cluster
  outage.
 </para>
 <para>
  To prevent the &ceph; cluster performance downtime caused by the admin node
  failure, we recommend to make use of the &ha; (HA) cluster for the &ceph;
  admin node.
 </para>
 <sect1 xml:id="admin-ha-architecture">
  <title>Outline of the HA Cluster for &ceph; Admin Node</title>

  <para>
   The idea of an HA cluster is that in case of one cluster node failure, the
   other node automatically takes over its role including the virtualized
   &ceph; admin node. This way other &ceph; cluster nodes do not notice that
   the &ceph; admin node failed.
  </para>

  <para>
   The minimal HA solution for the &ceph; admin node requires the following
   hardware:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Two bare metal servers able to run &sle; with the &ha; extension and
     virtualize the &ceph; admin node.
    </para>
   </listitem>
   <listitem>
    <para>
     Two or more redundant network communication paths, for example via Network
     Device Bonding.
    </para>
   </listitem>
   <listitem>
    <para>
     Shared storage to host the disk image(s) of the &ceph; admin node virtual
     machine. The shared storage needs to be accessible form both servers. It
     can be for example an NFS export, a Samba share, or &iscsi; target.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Find more details on the cluster requirements at
   <link
    xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#sec-ha-inst-quick-req"/>.
  </para>
  <figure>
   <title>2-Node HA Cluster for &ceph; Admin Node</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_admin_ha1.png" width="60%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_admin_ha1.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="admin-ha-cluster">
  <title>Building HA Cluster with &ceph; Admin Node</title>

  <para>
   The following procedure summarizes the most important steps of building the
   HA cluster for virtualizing the &ceph; admin node. For details, refer to the
   indicated links.
  </para>

  <procedure>
   <step>
    <para>
     Set up a basic 2-node HA cluster with shared storage as described in
     <link
      xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-install-quick/#art-sleha-install-quick"/>.
    </para>
   </step>
   <step>
    <para>
     On both cluster nodes, install all packages required for running the &kvm;
     hypervisor and the &libvirt; toolkit as described in
     <link
      xlink:href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-vt-installation-kvm"/>.
    </para>
   </step>
   <step>
    <para>
     On the first cluster node, create a new &kvm; virtual machine (VM) making
     use of &libvirt; as described in
     <link
      xlink:href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-virtualization/#sec-libvirt-inst-virt-install"/>.
     Use the preconfigured shared storage to store the disk images of the VM.
    </para>
   </step>
   <step>
    <para>
     After the VM setup is complete, export its configuration to an XML file on
     the shared storage. Use the following syntax:
    </para>
<screen>
&prompt.root;virsh dumpxml <replaceable>VM_NAME</replaceable> &gt; /path/to/shared/vm_name.xml
</screen>
   </step>
   <step>
    <para>
     Create a resource for the &adm; VM. Refer to
     <link
      xlink:href="https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-guide/#cha-conf-hawk2"/>
     for general info on creating HA resources. Detailed info on creating
     resource for a &kvm; virtual machine is described in
     <link xlink:href="http://www.linux-ha.org/wiki/VirtualDomain_%28resource_agent%29"/>.
    </para>
   </step>
   <step>
    <para>
     On the newly created VM guest, deploy the &ceph; admin node including the
     additional services you need there. Follow relevant steps in
     <xref linkend="ceph-install-stack"/>. At the same time, deploy the
     remaining &ceph; cluster nodes on the non-HA cluster servers.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
