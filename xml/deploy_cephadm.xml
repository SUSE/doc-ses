<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-cephadm">
 <title>Deploy with &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &productname; &productnumber; uses the &salt;-based &cephsalt; tool to
  prepare the operating system on each participating cluster node for
  deployment via &cephadm;. &cephadm; deploys and manages a &ceph; cluster by
  connecting to hosts from the &mgr; daemon via SSH. &cephadm; manages the full
  life-cycle of a &ceph; cluster. It starts by bootstrapping a tiny cluster on
  a single node (one MON and MGR service) and then uses the orchestration
  interface to expand the cluster to include all hosts and to provision all
  &ceph; services. You can perform this via the &ceph; command line interface
  (CLI) or partially via &dashboard; (GUI).
 </para>
 <important>
  <para>
   Note that the &ceph; community documentation uses the <command>cephadm
   bootstrap</command> command during initial deployment. The &cephsalt; calls
   the <command>cephadm bootstrap</command> command and should not be run
   directly. Any &ceph; cluster deployment manually using the <command>cephadm
   bootstrap</command> will be unsupported.
  </para>
 </important>
 <para>
  To deploy a &ceph; cluster by using &cephadm;, you need to complete the
  following tasks:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Install and do basic configuration of the underlying operating
    system&mdash;&cephos;&mdash;on all cluster nodes.
   </para>
  </listitem>
  <listitem>
   <para>
    Deploy the &salt; infrastructure on all cluster nodes for performing the
    initial deployment preparations via &cephsalt;.
   </para>
  </listitem>
  <listitem>
   <para>
    Configure the basic properties of the cluster via &cephsalt; and deploy it.
   </para>
  </listitem>
  <listitem>
   <para>
    Add new nodes and roles to the cluster and deploy services to them using
    &cephadm;.
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="deploy-os">
  <title>Installing and configuring &sls;</title>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on each cluster node. During installation of
     &productname;, access to the update repositories is required, therefore
     registration is mandatory. Include at least the following modules:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Basesystem Module
      </para>
     </listitem>
     <listitem>
      <para>
       Server Applications Module
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Find more details on how to install &sls; in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-install.html"/>.
    </para>
   </step>
   <step>
    <para>
     Install the <emphasis>&productname; &productnumber;</emphasis> extension
     on each cluster node.
    </para>
    <tip>
     <title>Install &productname; together with &sls;</title>
     <para>
      You can either install the &productname; &productnumber; extension
      separately after you have installed &cephos;, or you can add it during
      the &cephos; installation procedure.
     </para>
    </tip>
    <para>
     Find more details on how to install extensions in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html"/>.
    </para>
   </step>
   <step>
    <para>
     Configure network settings including proper DNS name resolution on each
     node. For more information on configuring a network, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#sec-network-yast"/>
     For more information on configuring a DNS server, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-dns"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-salt">
  <title>Deploying &salt;</title>

  <para>
   &productname; uses &salt; and &cephsalt; for the initial cluster
   preparation. &salt; helps you configure and run commands on multiple cluster
   nodes simultaneously from one dedicated host called the
   <emphasis>&smaster;</emphasis>. Before deploying &salt;, consider the
   following important points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by a dedicated
     node called &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     If the &smaster; host should be part of the &ceph; cluster, it needs to
     run its own &sminion;, but this is not a requirement.
    </para>
    <tip>
     <title>Sharing multiple roles per server</title>
     <para>
      You will get the best performance from your &ceph; cluster when each role
      is deployed on a separate node. But real deployments sometimes require
      sharing one node for multiple roles. To avoid trouble with performance
      and the upgrade procedure, do not deploy the &osd;, &mds;, or &mon; role
      to the &adm;.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name, but you can specify any other network-reachable host name in the
     <filename>/etc/salt/minion</filename> file.
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Install the <literal>salt-master</literal> on the &smaster; node:
    </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     If you intend to use the firewall, verify that the &smaster; node has
     ports 4505 and 4506 open to all &sminion; nodes. If the ports are closed,
     you can open them using the <command>yast2 firewall</command> command by
     allowing the <guimenu>salt-master</guimenu> service for the appropriate
     zone. For example, <literal>public</literal>.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
   </step>
   <step>
    <para>
     Edit <filename>/etc/salt/minion</filename> and uncomment the following
     line:
    </para>
<screen>#log_level_logfile: warning</screen>
    <para>
     Change the <literal>warning</literal> log level to
     <literal>info</literal>.
    </para>
    <note>
     <title><option>log_level_logfile</option> and <option>log_level</option></title>
     <para>
      While <option>log_level</option> controls which log messages will be
      displayed on the screen, <option>log_level_logfile</option> controls
      which log messages will be written to
      <filename>/var/log/salt/minion</filename>.
     </para>
    </note>
    <note>
     <para>
      Ensure you change the log level on <emphasis>all</emphasis> cluster
      (minion) nodes.
     </para>
    </note>
   </step>
   <step>
    <para>
     Make sure that the <emphasis>fully qualified domain name</emphasis> of
     each node can be resolved to an IP address on the public cluster network
     by all the other nodes.
    </para>
   </step>
   <step>
    <para>
     Configure all minions to connect to the master. If your &smaster; is not
     reachable by the host name <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all related &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started on all nodes. Enable and start it if needed:
    </para>
<screen>&prompt.root;systemctl enable salt-minion.service
&prompt.root;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verify each &sminion;'s fingerprint and accept all salt keys on the
     &smaster; if the fingerprints match.
    </para>
    <note>
     <para>
      If the &sminion; fingerprint comes back empty, make sure the &sminion;
      has a &smaster; configuration and that it can communicate with the
      &smaster;.
     </para>
    </note>
    <para>
     View each minion's fingerprint:
    </para>
<screen>&prompt.sminion;salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     After gathering fingerprints of all the &sminion;s, list fingerprints of
     all unaccepted minion keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     If the minions' fingerprints match, accept them:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Test whether all &sminion;s respond:
    </para>
<screen>&prompt.smaster;salt-run manage.status</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day1">
  <title>Deploying the &ceph; cluster</title>

  <para>
   This section guides you through the process of deploying a basic &ceph;
   cluster. Read the following subsections carefully and execute the included
   commands in the given order.
  </para>

  <sect2 xml:id="deploy-cephadm-cephsalt">
   <title>Installing &cephsalt;</title>
   <para>
    &cephsalt; provides tools for deploying &ceph; clusters managed by
    &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
    management&mdash;for example, software updates or time
    synchronization&mdash;and defining roles for &sminion;s.
   </para>
   <para>
    On the &smaster;, install the <package>ceph-salt</package> package:
   </para>
<screen>&prompt.smaster;zypper install ceph-salt</screen>
   <para>
    The above command installed <package>ceph-salt-formula</package> as a
    dependency which modified the &smaster; configuration by inserting
    additional files in the <filename>/etc/salt/master.d</filename> directory.
    To apply the changes, restart
    <systemitem class="daemon">salt-master.service</systemitem> and synchronize
    &salt; modules:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure">
   <title>Configuring cluster properties</title>
   <para>
    Use the <command>ceph-salt config</command> command to configure the basic
    properties of the cluster.
   </para>
   <important>
    <para>
     The <filename>/etc/ceph/ceph.conf</filename> file is managed by &cephadm;
     and users <emphasis>should not</emphasis> edit it. &ceph; configuration
     parameters should be set using the new <command>ceph config</command>
     command. See <xref linkend="cha-ceph-configuration-db"/> for more
     information.
    </para>
   </important>
   <sect3 xml:id="deploy-cephadm-configure-shell">
    <title>Using the &cephsalt; shell</title>
    <para>
     If you run <command>ceph-salt config</command> without any path or
     subcommand, you will enter an interactive &cephsalt; shell. The shell is
     convenient if you need to configure multiple properties in one batch and
     do not want type the full command syntax.
    </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
    <para>
     As you can see from the output of &cephsalt;'s <command>ls</command>
     command, the cluster configuration is organized in a tree structure. To
     configure a specific property of the cluster in the &cephsalt; shell, you
     have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Run the command from the current position and enter the absolute path to
       the property as the first argument:
      </para>
<screen>
<prompt>/></prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/> /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
     </listitem>
     <listitem>
      <para>
       Change to the path whose property you need to configure and run the
       command:
      </para>
<screen>
<prompt>/></prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions></prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Autocompletion of configuration snippets</title>
     <para>
      While in a &cephsalt; shell, you can use the autocompletion feature
      similar to a normal Linux shell (Bash) autocompletion. It completes
      configuration paths, subcommands, or &sminion; names. When autocompleting
      a configuration path, you have two options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To let the shell finish a path relative to your current position,press
        the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
      <listitem>
       <para>
        To let the shell finish an absolute path, enter <keycap>/</keycap> and
        press the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
     </itemizedlist>
    </tip>
    <tip>
     <title>Navigating with the cursor keys</title>
     <para>
      If you enter <command>cd</command> from the &cephsalt; shell without any
      path, the command will print a tree structure of the cluster
      configuration with the line of the current path active. You can use the
      up and down cursor keys to navigate through individual lines. After you
      confirm with <keycap function="enter"></keycap>, the configuration path
      will change to the last active one.
     </para>
    </tip>
    <important>
     <title>Convention</title>
     <para>
      To keep the documentation consistent, we will use a single command syntax
      without entering the &cephsalt; shell. For example, you can list the
      cluster configuration tree by using the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-minions">
    <title>Adding &sminion;s</title>
    <para>
     Include all or a subset of &sminion;s that we deployed and accepted in
     <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
     either specify the &sminion;s by their full names, or use a glob
     expressions '*' and '?' to include multiple &sminion;s at once. Use the
     <command>add</command> subcommand under the
     <literal>/ceph_cluster/minions</literal> path. The following command
     includes all accepted &sminion;s:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
    <para>
     Verify that the specified &sminion;s were added:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-cephadm">
    <title>Specifying &sminion;s managed by &cephadm;</title>
    <para>
     Specify which nodes will belong to the &ceph; cluster and will be managed
     by &cephadm;. Include all nodes that will run &ceph; services as well as
     the &adm;:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-admin">
    <title>Specifying &adm;</title>
    <para>
     The &adm; is the node where the <filename>ceph.conf</filename>
     configuration file and the &ceph; admin keyring is installed. You usually
     run &ceph; related commands on the &adm;.
    </para>
    <tip>
     <title>&smaster; and &adm; on the Same Node</title>
     <para>
      In a homogeneous environment where all or most hosts belong to
      &productname;, we recommend having the &adm; on the same host as the
      &smaster;.
     </para>
     <para>
      In a heterogeneous environment where one &salt; infrastructure hosts more
      than one cluster, for example, &productname; together with &susemgr;, do
      <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
     </para>
    </tip>
    <para>
     To specify the &adm;, run the following command:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
    <tip>
     <title>Install <filename>ceph.conf</filename> and the admin keyring on multiple nodes</title>
     <para>
      You can install the &ceph; configuration file and admin keyring on
      multiple nodes if your deployment requires it. For security reasons,
      avoid installing them on all the cluster's nodes.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-mon">
    <title>Specifying first MON/MGR node</title>
    <para>
     You need to specify which of the cluster's &sminion;s will bootstrap the
     cluster. This minion will become the first one running &mon; and &mgr;
     services.
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
    <para>
     Additionally, you need to specify the bootstrap MON's IP address on the
     public network to ensure that the <option>public_network</option>
     parameter is set correctly, for example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-tuned-profiles">
    <title>Specifying tuned profiles</title>
    <para>
     You need to specify which of the cluster's minions have actively tuned
     profiles. To do so, add these roles explicitly with the following
     commands:
    </para>
    <note>
     <para>
      One minion cannot have both the <literal>latency</literal> and
      <literal>throughput</literal> roles.
     </para>
    </note>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ssh">
    <title>Generating an SSH key pair</title>
    <para>
     &cephadm; uses the SSH protocol to communicate with cluster nodes. A user
     account named <literal>cephadm</literal> is automatically created and used
     for SSH communication.
    </para>
    <para>
     You need to generate the private and public part of the SSH key pair:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ntp">
    <title>Configuring the time server</title>
    <para>
     All cluster nodes need to have their time synchronized with a reliable
     time source. There are several scenarios to approach time synchronization:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If all cluster nodes are already configured to synchronize their time
       using an NTP service of choice, disable time server handling completely:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server disable
</screen>
     </listitem>
     <listitem>
      <para>
       If your site already has a single source of time, specify the host name
       of the time source:
      </para>
<screen>
 &prompt.smaster;ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Alternatively, &cephsalt; has the ability to configure one of the
       &sminion; to serve as the time server for the rest of the cluster. This
       is sometimes referred to as an "internal time server". In this scenario,
       &cephsalt; will configure the internal time server (which should be one
       of the &sminion;) to synchronize its time with an external time server,
       such as <literal>pool.ntp.org</literal>, and configure all the other
       minions to get their time from the internal time server. This can be
       achieved as follows:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/servers add ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
      <para>
       The <option>/time_server/subnet</option> option specifies the subnet
       from which NTP clients are allowed to access the NTP server. It is
       automatically set when you specify
       <option>/time_server/servers</option>. If you need to change it or
       specify it manually, run:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
     </listitem>
    </itemizedlist>
    <para>
     Check the time server settings:
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
    <para>
     Find more information on setting up time synchronization in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
    </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-dashboardlogin">
    <title>Configuring the &dashboard; login credentials</title>
    <para>
     &dashboard; will be available after the basic cluster is deployed. To
     access it, you need to set a valid user name and password, for example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/username set admin
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
    <tip>
     <title>Forcing password update</title>
     <para>
      By default, the first dashboard user will be forced to change their
      password on first login to the dashboard. To disable this feature, run
      the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-registry">
    <title>Using the container registry</title>
    <para>
     The &ceph; cluster needs to have access to a container registry so that it
     can download and deploy containerized &ceph; services. There are two ways
     to access the registry:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If your cluster can access the default registry at
       <literal>registry.suse.com</literal> (directly or via proxy), you can
       point &cephsalt; directly to this URL without creating a local registry.
       Continue by following the steps in
       <xref linkend="deploy-cephadm-configure-imagepath"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       If your cluster cannot access the default registry&mdash;for example,
       for air-gapped deployment&mdash;you need to configure a local container
       registry. After the local registry is created and configured, you need
       to point &cephsalt; to it.
      </para>
     </listitem>
    </itemizedlist>
    <sect4 xml:id="updating-ceph-local-registry">
     <title>Creating and configuring the local registry (optional)</title>
     <tip>
      <title>Placement and port usage</title>
      <para>
       Deploy the registry on a machine accessible by all nodes in the cluster.
       We recommend the &adm;. By default, the registry listens on port 5000.
      </para>
      <para>
       On the registry node, use the following command to ensure that the port
       is free:
      </para>
<screen>ss -tulpn | grep :5000</screen>
      <para>
       If other processes (such as <literal>iscsi-tcmu</literal>) are already
       listening on port 5000, determine another free port which can be used to
       map to port 5000 in the registry container.
      </para>
     </tip>
     <procedure>
      <title>Creating the local registry</title>
      <step>
       <para>
        Verify that the <package>Containers Module</package> extension is
        enabled:
       </para>
<screen>
&prompt.user;SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP2 x86_64 (Activated)
</screen>
      </step>
      <step>
       <para>
        Verify that the following packages are installed:
        <package>apache2-utils</package> (if enabling a secure registry),
        <package>cni</package>, <package>cni-plugins</package>,
        <package>podman</package>, <package>podman-cni-config</package>, and
        <package>skopeo</package>.
       </para>
      </step>
      <step>
       <para>
        Gather the following information:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fully qualified domain name of the registry host
          (<option>REG_HOST_FQDN</option>).
         </para>
        </listitem>
        <listitem>
         <para>
          An available port number used to map to the registry container port
          of 5000 (<option>REG_HOST_PORT</option>).
         </para>
        </listitem>
        <listitem>
         <para>
          Whether the registry will be secure or insecure
          (<option>insecure=[true|false]</option>).
         </para>
        </listitem>
       </itemizedlist>
      </step>
      <step>
       <para>
        Configure the registry by using &cephsalt;:
       </para>
<screen>
&prompt.cephuser;ceph-salt config containers/registries_conf enable
&prompt.cephuser;ceph-salt config containers/registries_conf/registries \
 add prefix=<option>REG_HOST_FQDN</option> insecure=[true|false] \
 location=<option>REG_HOST_PORT</option>:5000
&prompt.cephuser;ceph-salt apply --non-interactive
&prompt.cephuser;salt '*' cmd.shell "systemctl restart podman"
</screen>
      </step>
      <step>
       <para>
        To start an insecure local registry, create the necessary directory
        (for example, <filename>/var/lib/registry</filename> and start the
        registry with the <command>podman</command> command:
       </para>
<screen>
&prompt.root;mkdir -p /var/lib/registry
&prompt.root;podman run --privileged -d --name registry \
 -p <option>REG_HOST_PORT</option>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
      </step>
      <step>
       <para>
        To start a secure registry, follow these steps:
       </para>
       <substeps>
        <step>
         <para>
          Create the necessary directories:
         </para>
<screen>&prompt.root;mkdir -p /var/lib/registry/{auth,certs}</screen>
        </step>
        <step>
         <para>
          Generate an SSL certificate:
         </para>
<screen>
&prompt.root;openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt
</screen>
         <note>
          <para>
           Set the <literal>CN=[value]</literal> value to the fully qualified
           domain name of the host ([<option>REG_HOST_FQDN</option>]).
          </para>
         </note>
        </step>
        <step>
         <para>
          Copy the certificate to all cluster nodes and refresh the certificate
          cache:
         </para>
<screen>
&prompt.root;salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
&prompt.root;salt '*' cmd.shell "update-ca-certificates"
</screen>
        </step>
        <step>
         <para>
          Generate a username and password combination for authentication to
          the registry:
         </para>
<screen>
&prompt.root;htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <option>REG_USERNAME</option> <option>REG_PASSWORD</option>
</screen>
        </step>
        <step>
         <para>
          Start the secure registry:
         </para>
<screen>
podman run --name myregistry -p <option>REG_HOST_PORT</option>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
        </step>
        <step>
         <para>
          Test secure access to the registry:
         </para>
<screen>
&prompt.user;curl https://<option>REG_HOST_FQDN</option>:<option>REG_HOST_PORT</option>/v2/_catalog \
 -u <option>REG_USERNAME</option>:<option>REG_PASSWORD</option>
</screen>
        </step>
       </substeps>
      </step>
     </procedure>
     <procedure>
      <title>Configure the local registry and access credentials</title>
      <step>
       <para>
        Configure the URL of the local registry:
       </para>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/registry set <replaceable>REG_HOST_URL</replaceable></screen>
      </step>
      <step>
       <para>
        Configure the user name and password to access the local registry:
       </para>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable></screen>
<screen>&prompt.cephuser;ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable></screen>
      </step>
      <step>
       <para>
        Run <command>ceph-salt apply</command> to update the &salt; pillar on
        all minions.
       </para>
      </step>
     </procedure>
     <tip>
      <title>Registry cache</title>
      <para>
       To avoid re-syncing the local registry when new updated containers
       appear, you can configure a <emphasis>registry cache</emphasis>.
      </para>
     </tip>
     <para>
      Cloud Native Application Development and Delivery methods require a
      registry and a CI/CD (Continuous Integration/Delivery) instance for the
      development and production of container images. You can use a private
      registry in this instance.
     </para>
    </sect4>
    <sect4 xml:id="deploy-cephadm-configure-imagepath">
     <title>Configuring the path to container images</title>
     <tip>
      <title>Configuring HTTPS proxy</title>
      <para>
       If you need to use a proxy to communicate with the container registry
       server, perform the following configuration steps on the &adm;:
      </para>
      <procedure>
       <step>
        <para>
         Copy the configuration file for containers:
        </para>
<screen>&prompt.sudo;cp /usr/share/containers/containers.conf /etc/containers/containers.conf</screen>
       </step>
       <step>
        <para>
         Edit the newly-copied file and add the <option>http_proxy</option>
         setting to its <literal>[engine]</literal> section, for example:
        </para>
<screen>&prompt.user;cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]
 </screen>
       </step>
      </procedure>
     </tip>
     <para>
      &cephadm; needs to know a valid URI path to container images. Verify the
      default setting by executing
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
     <para>
      If you do not need an alternative path or to create a local container
      registry, you can leave the default or set it when the setting has no
      default path:
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7/ceph/ceph</screen>
     <para>
      If your deployment requires a specific path, for example, a path to a
      local registry, configure it as follows:
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
     <note>
      <para>
       Note that those container images will not be used by &cephsalt; for the
       deployment. It is a preparation for a later step where &cephadm; will be
       used for the deployment or migration of monitoring components.
      </para>
      <para>
       For more information about the images used by the monitoring stack and
       how to customize them, visit the
       <xref linkend="monitoring-custom-images"/> page.
      </para>
     </note>
    </sect4>
   </sect3>
   <sect3 xml:id="deploy-cephadm-inflight-encryption">
    <title>Enabling data in-flight encryption (msgr2)</title>
    <para>
     The Messenger v2 protocol (MSGR2) is &ceph;'s on-wire protocol. It
     provides a security mode that encrypts all data passing over the network,
     encapsulation of authentication payloads, and the enabling of future
     integration of new authentication modes (such as Kerberos).
    </para>
    <important>
     <para>
      msgr2 is not currently supported by Linux kernel Ceph clients, such as
      &cephfs; and &rbd;.
     </para>
    </important>
    <para>
     &ceph; daemons can bind to multiple ports, allowing both legacy &ceph;
     clients and new v2-capable clients to connect to the same cluster. By
     default, MONs now bind to the new IANA-assigned port 3300 (CE4h or 0xCE4)
     for the new v2 protocol, while also binding to the old default port 6789
     for the legacy v1 protocol.
    </para>
    <para>
     The v2 protocol (MSGR2) supports two connection modes:
    </para>
    <variablelist>
     <varlistentry>
      <term>crc mode</term>
      <listitem>
       <para>
        A strong initial authentication when the connection is established and
        a CRC32C integrity check.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>secure mode</term>
      <listitem>
       <para>
        A strong initial authentication when the connection is established and
        full encryption of all post-authentication traffic, including a
        cryptographic integrity check.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     For most connections, there are options that control which modes are used:
    </para>
    <variablelist>
     <varlistentry>
      <term>ms_cluster_mode</term>
      <listitem>
       <para>
        The connection mode (or permitted modes) used for intra-cluster
        communication between &ceph; daemons. If multiple modes are listed, the
        modes listed first are preferred.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>ms_service_mode</term>
      <listitem>
       <para>
        A list of permitted modes for clients to use when connecting to the
        cluster.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>ms_client_mode</term>
      <listitem>
       <para>
        A list of connection modes, in order of preference, for clients to use
        (or allow) when talking to a &ceph; cluster.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     There are a parallel set of options that apply specifically to monitors,
     allowing administrators to set different (usually more secure)
     requirements on communication with the monitors.
    </para>
    <variablelist>
     <varlistentry>
      <term>ms_mon_cluster_mode</term>
      <listitem>
       <para>
        The connection mode (or permitted modes) to use between monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>ms_mon_service_mode</term>
      <listitem>
       <para>
        A list of permitted modes for clients or other &ceph; daemons to use
        when connecting to monitors.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>ms_mon_client_mode</term>
      <listitem>
       <para>
        A list of connection modes, in order of preference, for clients or
        non-monitor daemons to use when connecting to monitors.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     In order to enable MSGR2 encryption mode during the deployment, you need
     to add some configuration options to the &cephsalt; configuration before
     running <command>ceph-salt apply</command>.
    </para>
    <para>
     To use <literal>secure</literal> mode, run the following commands.
    </para>
    <para>
     Add the global section to <filename>ceph_conf</filename> in the &cephsalt;
     configuration tool:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
    <para>
     Set the following options:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
    <note>
     <para>
      Ensure <literal>secure</literal> precedes <literal>crc</literal>.
     </para>
    </note>
    <para>
     To <emphasis>force</emphasis> <literal>secure</literal> mode, run the
     following commands:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
    <tip xml:id="update-inflight-encryption-settings">
     <title>Updating settings</title>
     <para>
      If you want to change any of the above settings, set the configuration
      changes in the monitor configuration store. This is achieved using the
      <command>ceph config set</command> command.
     </para>
<screen>&prompt.smaster;ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
     <para>
      For example:
     </para>
<screen>&prompt.smaster;ceph config set global ms_cluster_mode "secure crc"</screen>
     <para>
      If you want to check the current value, including default value, run the
      following command:
     </para>
<screen>&prompt.smaster;ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
     <para>
      For example, to get the <literal>ms_cluster_mode</literal> for OSD's,
      run:
     </para>
<screen>&prompt.smaster;ceph config get osd ms_cluster_mode</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-enable-network">
    <title>Configuring the cluster network</title>
    <para>
     Optionally, if you are running a separate cluster network, you may need to
     set the cluster network IP address followed by the subnet mask part after
     the slash sign, for example <literal>192.168.10.22/24</literal>.
    </para>
    <para>
     Run the following commands to enable <literal>cluster_network</literal>:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-verify">
    <title>Verifying the cluster configuration</title>
    <para>
     The minimal cluster configuration is finished. Inspect it for obvious
     errors:
    </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path ............... [registry.suse.com/ses/7/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
    <tip>
     <title>Status of cluster configuration</title>
     <para>
      You can check if the configuration of the cluster is valid by running the
      following command:
     </para>
<screen>
&prompt.smaster;ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-export">
    <title>Exporting cluster configurations</title>
    <para>
     After you have configured the basic cluster and its configuration is
     valid, it is a good idea to export its configuration to a file:
    </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
    <warning>
     <para>
      The output of the <command>ceph-salt export</command> includes the SSH
      private key. If you are concerned about the security implications, do not
      execute this command without taking appropriate precautions.
     </para>
    </warning>
    <para>
     In case you break the cluster configuration and need to revert to a backup
     state, run:
    </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-deploy">
   <title>Updating nodes and bootstrap minimal cluster</title>
   <para>
    Before you deploy the cluster, update all software packages on all nodes:
   </para>
<screen>&prompt.smaster;ceph-salt update</screen>
   <para>
    If a node reports <literal>Reboot is needed</literal> during the update,
    important OS packages&mdash;such as the kernel&mdash;were updated to a
    newer version and you need to reboot the node to apply the changes.
   </para>
   <para>
    To reboot all nodes that require rebooting, either append the
    <option>--reboot</option> option
   </para>
<screen>&prompt.smaster;ceph-salt update --reboot</screen>
   <para>
    Or, reboot them in a separate step:
   </para>
<screen>&prompt.smaster;ceph-salt reboot</screen>
   <important>
    <para>
     The &smaster; is never rebooted by <command>ceph-salt update
     --reboot</command> or <command>ceph-salt reboot</command> commands. If the
     &smaster; needs rebooting, you need to reboot it manually.
    </para>
   </important>
   <para>
    After the nodes are updated, bootstrap the minimal cluster:
   </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   <note>
    <para>
     When bootstrapping is complete, the cluster will have one &mon; and one
     &mgr;.
    </para>
   </note>
   <para>
    The above command will open an interactive user interface that shows the
    current progress of each minion.
   </para>
   <figure>
    <title>Deployment of a minimal cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <title>Non-interactive mode</title>
    <para>
     If you need to apply the configuration from a script, there is also a
     non-interactive mode of deployment. This is also useful when deploying the
     cluster from a remote machine because constant updating of the progress
     information on the screen over the network may become distracting:
    </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-min-cluster-final-steps">
   <title>Reviewing final steps</title>
   <para>
    After the <command>ceph-salt apply</command> command has completed, you
    should have one &mon; and one &mgr;. You should be able to run the
    <command>ceph status</command> command successfully on any of the minions
    that were given the <literal>admin</literal> role as
    <literal>root</literal> or the <literal>cephadm</literal> user using
    <literal>sudo</literal>.
   </para>
   <para>
    The next steps involve using the &cephadm; to deploy additional &mon;,
    &mgr;, OSDs, the Monitoring Stack, and Gateways.
   </para>
   <para>
    Before you continue, review your new cluster's network settings. At this
    point, the <literal>public_network</literal> setting has been populated
    based on what was entered for <literal>/cephadm_bootstrap/mon_ip</literal>
    in the <literal>ceph-salt</literal> configuration. However, this setting
    was only applied to &mon;. You can review this setting with the following
    command:
   </para>
<screen>&prompt.smaster;ceph config get mon public_network</screen>
   <para>
    This is the minimum that &ceph; requires to work, but we recommend making
    this <literal>public_network</literal> setting <literal>global</literal>,
    which means it will apply to all types of &ceph; daemons, and not only to
    MONs:
   </para>
<screen>&prompt.smaster;ceph config set global public_network "$(ceph config get mon public_network)"</screen>
   <note>
    <para>
     This step is not required. However, if you do not use this setting, the
     &ceph; OSDs and other daemons (except &mon;) will listen on <emphasis>all
     addresses</emphasis>.
    </para>
    <para>
     If you want your OSDs to communicate amongst themselves using a completely
     separate network, run the following command:
    </para>
<screen>&prompt.smaster;ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
    <para>
     Executing this command will ensure that the OSDs created in your
     deployment will use the intended cluster network from the start.
    </para>
   </note>
   <para>
    If your cluster is set to have dense nodes (greater than 62 OSDs per host),
    make sure to assign sufficient ports for &ceph; OSDs. The default range
    (6800-7300) currently allows for no more than 62 OSDs per host. For a
    cluster with dense nodes, adjust the setting
    <literal>ms_bind_port_max</literal> to a suitable value. Each OSD will
    consume eight additional ports. For example, given a host that is set to
    run 96 OSDs, 768 ports will be needed. <literal>ms_bind_port_max</literal>
    should be set at least to 7568 by running the following command:
   </para>
<screen>&prompt.smaster;ceph config set osd.* ms_bind_port_max 7568</screen>
   <para>
    You will need to adjust your firewall settings accordingly for this to
    work. See <xref linkend="storage-bp-net-firewall"/> for more information.
   </para>
  </sect2>

  <sect2 xml:id="deploy-min-cluster-disable-insecure">
   <title>Disable insecure clients</title>
   <para>
    Since &cephname; v15.2.11, a new health warning was introduced that informs
    you that insecure clients are allowed to join the cluster. This warning is
    <emphasis>on</emphasis> by default. The &dashboard; will show the cluster
    in the <literal>HEALTH_WARN</literal> status and verifying the cluster
    status on the command line informs you as follows:
   </para>
<screen>
&prompt.cephuser;ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>
   <para>
    This warning means that the &mon;s are still allowing old, unpatched
    clients to connect to the cluster. This ensures existing clients can still
    connect while the cluster is being upgraded, but warns you that there is a
    problem that needs to be addressed. When the cluster and all clients are
    upgraded to the latest version of &ceph;, disallow unpatched clients by
    running the following command:
   </para>
<screen>&prompt.cephuser;ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2">
  <title>Deploying services and gateways</title>

  <para>
   After deploying the basic &ceph; cluster, deploy core services to more
   cluster nodes. To make the cluster data accessible to clients, deploy
   additional services as well.
  </para>

  <para>
   Currently, we support deployment of &ceph; services on the command line by
   using the &ceph; orchestrator (<command>ceph orch</command> subcommands).
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch">
   <title>The <command>ceph orch</command> command</title>
   <para>
    The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
    an interface to the &cephadm; module&mdash;will take care of listing
    cluster components and deploying &ceph; services on new cluster nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-orch-status">
    <title>Displaying the orchestrator status</title>
    <para>
     The following command shows the current mode and status of the &ceph;
     orchestrator.
    </para>
<screen>&prompt.cephuser;ceph orch status</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-orch-list">
    <title>Listing devices, services, and daemons</title>
    <para>
     To list all disk devices, run the following:
    </para>
<screen>
&prompt.cephuser;ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
    <tip>
     <title>Services and daemons</title>
     <para>
      <emphasis>Service</emphasis> is a general term for a &ceph; service of a
      specific type, for example &mgr;.
     </para>
     <para>
      <emphasis>Daemon</emphasis> is a specific instance of a service, for
      example a process <literal>mgr.ses-min1.gdlcik</literal> running on a
      node called <literal>ses-min1</literal>.
     </para>
    </tip>
    <para>
     To list all services known to &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
    <tip>
     <para>
      You can limit the list to services on a particular node with the optional
      <option>-–host</option> parameter, and services of a particular type with
      the optional <option>--service-type</option> parameter (acceptable types
      are <literal>mon</literal>, <literal>osd</literal>,
      <literal>mgr</literal>, <literal>mds</literal>, and
      <literal>rgw</literal>).
     </para>
    </tip>
    <para>
     To list all running daemons deployed by &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
    <tip>
     <para>
      To query the status of a particular daemon, use
      <option>--daemon_type</option> and <option>--daemon_id</option>. For
      OSDs, the ID is the numeric OSD ID. For MDS, the ID is the file system
      name:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="cephadm-service-and-placement-specs">
   <title>Service and placement specification</title>
   <para>
    The recommended way to specify the deployment of &ceph; services is to
    create a YAML-formatted file with the specification of the services that
    you intend to deploy.
   </para>
   <sect3 xml:id="cephadm-service-spec">
    <title>Creating service specifications</title>
    <para>
     You can create a separate specification file for each type of service, for
     example:
    </para>
<screen>
&prompt.smaster;cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
    <para>
     Alternatively, you can specify multiple (or all) service types in one
     file&mdash;for example, <filename>cluster.yml</filename>&mdash;that
     describes which nodes will run specific services. Remember to separate
     individual service types with three dashes (<literal>---</literal>):
    </para>
<screen>
&prompt.cephuser;cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
    <para>
     The aforementioned properties have the following meaning:
    </para>
    <variablelist>
     <varlistentry>
      <term><literal>service_type</literal></term>
      <listitem>
       <para>
        The type of the service. It can be either a &ceph; service
        (<literal>mon</literal>, <literal>mgr</literal>,
        <literal>mds</literal>, <literal>crash</literal>,
        <literal>osd</literal>, or <literal>rbd-mirror</literal>), a gateway
        (<literal>nfs</literal> or <literal>rgw</literal>), or part of the
        monitoring stack (<literal>alertmanager</literal>,
        <literal>grafana</literal>, <literal>node-exporter</literal>, or
        <literal>prometheus</literal>).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>service_id</literal></term>
      <listitem>
       <para>
        The name of the service. Specifications of type <literal>mon</literal>,
        <literal>mgr</literal>, <literal>alertmanager</literal>,
        <literal>grafana</literal>, <literal>node-exporter</literal>, and
        <literal>prometheus</literal> do not require the
        <literal>service_id</literal> property.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>placement</literal></term>
      <listitem>
       <para>
        Specifies which nodes will be running the service. Refer to
        <xref linkend="cephadm-placement-specs"/> for more details.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><literal>spec</literal></term>
      <listitem>
       <para>
        Additional specification relevant for the service type.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <tip>
     <title>Applying specific services</title>
     <para>
      &ceph; cluster services have usually a number of properties specific to
      them. For examples and details of individual services' specification,
      refer to <xref linkend="deploy-cephadm-day2-services"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="cephadm-placement-specs">
    <title>Creating placement specification</title>
    <para>
     To deploy &ceph; services, &cephadm; needs to know on which nodes to
     deploy them. Use the <literal>placement</literal> property and list the
     short host names of the nodes that the service applies to:
    </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
   </sect3>
   <sect3 xml:id="cephadm-apply-cluster-specs">
    <title>Applying cluster specification</title>
    <para>
     After you have created a full <filename>cluster.yml</filename> file with
     specifications of all services and their placement, you can apply the
     cluster by running the following command:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yml</screen>
    <para>
     To view the status of the cluster, run the <command>ceph orch
     status</command> command. For more details, see
     <xref linkend="deploy-cephadm-day2-orch-status"/>.
    </para>
   </sect3>
   <sect3 xml:id="cephadm-apply-cluster-specs-">
    <title>Exporting the specification of a running cluster</title>
    <para>
     Although you deployed services to the &ceph; cluster by using the
     specification files as described in
     <xref
      linkend="cephadm-service-and-placement-specs" />, the
     configuration of the cluster may diverge from the original specification
     during its operation. Also, you may have removed the specification files
     accidentally.
    </para>
    <para>
     To retrieve a complete specification of a running cluster, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
    <tip>
     <para>
      You can append the <option>--format</option> option to change the default
      <literal>yaml</literal> output format. You can select from
      <literal>json</literal>, <literal>json-pretty</literal>, or
      <literal>yaml</literal>. For example:
     </para>
<screen>ceph orch ls --export --format json</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-services">
   <title>Deploy &ceph; services</title>
   <para>
    After the basic cluster is running, you can deploy &ceph; services to
    additional nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-service-mon">
    <title>Deploying &mon;s and &mgr;s</title>
    <para>
     &ceph; cluster has three or five MONs deployed across different nodes. If
     there are five or more nodes in the cluster, we recommend deploying five
     MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
    </para>
    <important>
     <title>Include Bootstrap MON</title>
     <para>
      When deploying MONs and MGRs, remember to include the first MON that you
      added when configuring the basic cluster in
      <xref linkend="deploy-cephadm-configure-mon"/>.
     </para>
    </important>
    <para>
     To deploy MONs, apply the following specification:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <note>
     <para>
      If you need to add another node, append the host name to the same YAML
      list. For example:
     </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
    </note>
    <para>
     Similarly, to deploy MGRs, apply the following specification:
    </para>
    <important>
     <para>
      Ensure your deployment has at least three &mgr;s in each deployment.
     </para>
    </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <tip>
     <para>
      If MONs or MGRs are <emphasis>not</emphasis> on the same subnet, you need
      to append the subnet addresses. For example:
     </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-osd">
    <title>Deploying &osd;s</title>
    <important>
     <title>When Storage Device is Available</title>
     <para>
      A storage device is considered <emphasis>available</emphasis> if all of
      the following conditions are met:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The device has no partitions.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not have any LVM state.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is not be mounted.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a file system.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a &bluestore; OSD.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is larger than 5&nbsp;GB.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If the above conditions are not met, &ceph; refuses to provision such
      OSDs.
     </para>
    </important>
    <para>
     There are two ways you can deploy OSDs:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Tell &ceph; to consume all available and unused storage devices:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
     </listitem>
     <listitem>
      <para>
       Use &drvgrps; (see <xref linkend="drive-groups"/>) to create OSD
       specification describing devices that will be deployed based on their
       properties, such as device type (SSD or HDD), device model names, size,
       or the nodes on which the devices exist. Then apply the specification by
       running the following command:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-mds">
    <title>Deploying &mds;s</title>
    <para>
     &cephfs; requires one or more &mds; (MDS) services. To create a &cephfs;,
     first create MDS servers by applying the following specification:
    </para>
    <note>
     <para>
      Ensure you have at least two pools, one for &cephfs; data and one for
      &cephfs; metadata, created before applying the following specification.
     </para>
    </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <para>
     After MDSs are functional, create the &cephfs;:
    </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-ogw">
    <title>Deploying &ogw;s</title>
    <para>
     &cephadm; deploys an &ogw; as a collection of daemons that manage a
     particular <emphasis>realm</emphasis> and <emphasis>zone</emphasis>.
    </para>
    <para>
     You can either relate an &ogw; service to already existing realm and zone,
     (refer to <xref linkend="ceph-rgw-fed"/> for more details), or you can
     specify a non-existing <replaceable>REALM_NAME</replaceable> and
     <replaceable>ZONE_NAME</replaceable> and they will be created
     automatically after you apply the following configuration:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
    <sect4 xml:id="cephadm-deploy-using-secure-ssl-access">
     <title>Using secure SSL access</title>
     <para>
      To use a secure SSL connection to the &ogw;, you need a pair of valid SSL
      certificate and key files (see <xref linkend="ceph-rgw-https"/> for more
      details). You need to enable SSL, specify a port number for SSL
      connections, and the SSL certificate and key files.
     </para>
     <para>
      To enable SSL and specify the port number, include the following in your
      specification:
     </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
     <para>
      To specify the SSL certificate and key, you can paste their contents
      directly into the YAML specification file. The pipe sign
      (<literal>|</literal>) at the end of line tells the parser to expect a
      multi-line string as a value. For example:
     </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
     <tip>
      <para>
       Instead of pasting the content of SSL certificate and key files, you can
       omit the <literal>rgw_frontend_ssl_certificate:</literal> and
       <literal>rgw_frontend_ssl_key:</literal> keywords and upload them to the
       configuration database:
      </para>
<screen>
&prompt.cephuser;ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
&prompt.cephuser;ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
     </tip>
    </sect4>
    <sect4 xml:id="cephadm-deploy-with-subcluster">
     <title>Deploying with a subcluster</title>
     <para>
      <emphasis>Subclusters</emphasis> help you organize the nodes in your
      clusters to isolate workloads and make elastic scaling easier. If you are
      deploying with a subcluster, apply the following configuration:
     </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
    </sect4>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-igw">
    <title>Deploying &igw;s</title>
    <para>
     &cephadm; deploys an &igw; which is a storage area network (SAN) protocol
     that allows clients (called initiators) to send SCSI commands to SCSI
     storage devices (targets) on remote servers.
    </para>
    <para>
     Apply the following configuration to deploy. Ensure
     <literal>trusted_ip_list</literal> contains the IP addresses of all &igw;
     and &mgr; nodes (see the example output below).
    </para>
    <note>
     <para>
      Ensure the pool is created before applying the following specification.
     </para>
    </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
    <note>
     <para>
      Ensure the IPs listed for <literal>trusted_ip_list</literal> do
      <emphasis>not</emphasis> have a space after the comma separation.
     </para>
    </note>
    <sect4>
     <title>Secure SSL configuration</title>
     <para>
      To use a secure SSL connection between the &dashboard; and the &iscsi;
      target API, you need a pair of valid SSL certificate and key files. These
      can be either CA-issued or self-signed (see
      <xref
      linkend="self-sign-certificates"/>). To enable SSL, include
      the <literal>api_secure: true</literal> setting in your specification
      file:
     </para>
<screen>
spec:
  api_secure: true
</screen>
     <para>
      To specify the SSL certificate and key, you can paste the content
      directly into the YAML specification file. The pipe sign
      (<literal>|</literal>) at the end of line tells the parser to expect a
      multi-line string as a value. For example:
     </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
    </sect4>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-nfs">
    <title>Deploying &ganesha;</title>
    &ganesha_support;
    <para>
     &cephadm; deploys &ganesha; using a pre-defined &rados; pool and an
     optional name-space. To deploy &ganesha;, apply the following
     specification:
    </para>
    <note>
     <para>
      You need to have a pre-defined &rados; pool otherwise the <command>ceph
      orch apply</command> operation will fail. For more information on
      creating a pool, see <xref linkend="ceph-pools-operate-add-pool"/>.
     </para>
    </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NFS</replaceable> with an arbitrary string that
       identifies the NFS export.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_POOL</replaceable> with the name of the pool where
       the &ganesha; RADOS configuration object will be stored.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NAMESPACE</replaceable> (optional) with the desired
       &ogw; NFS namespace (for example, <literal>ganesha</literal>).
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-rbdmirror">
    <title>Deploying &rbdmirror;</title>
    <para>
     The &rbdmirror; service takes care of synchronizing &rbd; images between
     two &ceph; clusters (for more details, see
     <xref
     linkend="ceph-rbd-mirror"/>). To deploy &rbdmirror;, use the
     following specification:
    </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-monitoring">
    <title>Deploying the monitoring stack</title>
    <para>
     The monitoring stack consists of &prometheus;, &prometheus; exporters,
     &prometheus; &alertmanager;, and &grafana;. &dashboard; makes use of these
     components to store and visualize detailed metrics on cluster usage and
     performance.
    </para>
    <tip>
     <para>
      If your deployment requires custom or locally served container images of
      the monitoring stack services, refer to
      <xref
       linkend="monitoring-custom-images"/>.
     </para>
    </tip>
    <para>
     To deploy the monitoring stack, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Enable the <literal>prometheus</literal> module in the &mgr; daemon.
       This exposes the internal &ceph; metrics so that &prometheus; can read
       them:
      </para>
<screen>&prompt.cephuser;ceph mgr module enable prometheus</screen>
      <note>
       <para>
        Ensure this command is run before &prometheus; is deployed. If the
        command was not run before the deployment, you must redeploy
        &prometheus; to update &prometheus;' configuration:
       </para>
<screen>&prompt.cephuser;ceph orch redeploy prometheus</screen>
      </note>
     </step>
     <step>
      <para>
       Create a specification file (for example
       <filename>monitoring.yaml</filename>) with a content similar to the
       following:
      </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
     </step>
     <step>
      <para>
       Apply monitoring services by running:
      </para>
<screen>&prompt.cephuser;ceph orch apply -i monitoring.yaml</screen>
      <para>
       It may take a minute or two for the monitoring services to be deployed.
      </para>
     </step>
    </procedure>
    <important>
     <para>
      &prometheus;, &grafana;, and the &dashboard; are all automatically
      configured to talk to each other, resulting in a fully functional
      &grafana; integration in the &dashboard; when deployed as described
      above.
     </para>
     <para>
      The only exception to this rule is monitoring with RBD images. See
      <xref linkend="monitoring-rbd-image"/> for more information.
     </para>
    </important>
   </sect3>
  </sect2>
 </sect1>
</chapter>
