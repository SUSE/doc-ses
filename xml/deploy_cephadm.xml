<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-cephadm">
 <title>Deploying with &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  To deploy a &ceph; cluster by using &cephadm;, you need to complete the
  following tasks:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Install and do basic configuration of the underlying operating
    system&mdash;&cephos;&mdash;on all cluster nodes.
   </para>
  </listitem>
  <listitem>
   <para>
    Deploy the &salt; infrastructure over all cluster nodes so that you can
    orchestrate them effectively.
   </para>
  </listitem>
  <listitem>
   <para>
    Configure the basic properties of the cluster and deploy it.
   </para>
  </listitem>
  <listitem>
   <para>
    Add new nodes to the cluster and deploy services to them.
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="deploy-os">
  <title>Install and Configure &sls;</title>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on each cluster node. Include the following
     modules:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Basesystem Module
      </para>
     </listitem>
     <listitem>
      <para>
       Server Applications Module
      </para>
     </listitem>
     <listitem>
      <para>
       Containers Module
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Find more details on how to install &sls; in
     <link xlink:href="https://documentation.suse.com/sles/15-SP22/html/SLES-all/cha-install.html"/>.
    </para>
   </step>
   <step>
    <para>
     Install the <emphasis>&productname; &productnumber;</emphasis> extension
     on each cluster node.
    </para>
    <tip>
     <title>Install &productname; Together with &sls;</title>
     <para>
      You can either install the &productname; &productnumber; extension
      separately after you have installed &cephos;, or you can add it during
      the &cephos; installation procedure.
     </para>
    </tip>
    <para>
     Find more details on how to install extensions in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html"/>.
    </para>
   </step>
   <step>
    <para>
     Configure network settings including proper DNS name resolution on each
     node. For more information on configuring a network, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#sec-network-yast"/>
     For more information on configuring a DNS server, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-dns"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-salt">
  <title>Deploy &salt;</title>

  <para>
   &productname; uses &salt; as a cluster orchestrator. &salt; helps you
   configure and run commands on multiple cluster nodes simultaneously from one
   dedicated host called the <emphasis>&smaster;</emphasis>. Before deploying
   &salt;, consider the following important points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by a dedicated
     node called &smaster;. &sminion;s have roles, for example &osd;, &mon;,
     &mgr;, &rgw;, &igw;, or &ganesha;.
    </para>
   </listitem>
   <listitem>
    <para>
     A &smaster; runs its own &sminion;. It is required for running privileged
     tasks&mdash;for example creating, authorizing, and copying keys to
     minions&mdash;so that remote minions never need to run privileged tasks.
    </para>
    <tip>
     <title>Sharing Multiple Roles per Server</title>
     <para>
      You will get the best performance from your &ceph; cluster when each role
      is deployed on a separate node. But real deployments sometimes require
      sharing one node for multiple roles. To avoid trouble with performance
      and the upgrade procedure, do not deploy the &osd;, &mds;, or &mon; role
      to the &adm;.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name, but you can specify any other network-reachable host name in the
     <filename>/etc/salt/minion</filename> file.
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Install the <literal>salt-master</literal> and
     <literal>salt-minion</literal> packages on the &smaster; node:
    </para>
<screen>&prompt.smaster;zypper in salt-master salt-minion</screen>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     If you intend to use the firewall, verify that the &smaster; node has
     ports 4505 and 4506 open to all &sminion; nodes. If the ports are closed,
     you can open them using the <command>yast2 firewall</command> command by
     allowing the <guimenu>SaltStack</guimenu> service.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Make sure that the <emphasis>fully qualified domain name</emphasis> of
     each node can be resolved to an IP address on the public cluster network
     by all the other nodes.
    </para>
   </step>
   <step>
    <para>
     Configure all minions (including the master minion) to connect to the
     master. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all related &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started on all nodes. Enable and start it if needed:
    </para>
<screen>&prompt.root;systemctl enable salt-minion.service
&prompt.root;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verify each &sminion;'s fingerprint and accept all salt keys on the
     &smaster; if the fingerprints match.
    </para>
    <note>
     <para>
      If the &sminion; fingerprint comes back empty, make sure the &sminion;
      has a &smaster; configuration and that it can communicate with the
      &smaster;.
     </para>
    </note>
    <para>
     View each minion's fingerprint:
    </para>
<screen>&prompt.sminion;salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     After gathering fingerprints of all the &sminion;s, list fingerprints of
     all unaccepted minion keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     If the minions' fingerprints match, accept them:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Test whether all &sminion;s respond:
    </para>
<screen>&prompt.smaster;salt '*' test.ping</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day1">
  <title>Deploy Basic Cluster (Day 1)</title>

  <para>
   This section guides you through the process of deploying a basic &ceph;
   cluster. Read the following subsections carefully and execute the included
   commands in the given order.
  </para>

  <sect2 xml:id="deploy-cephadm-cephsalt">
   <title>Install &cephsalt;</title>
   <para>
    &cephsalt; provides tools for deploying &ceph; clusters managed by
    &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
    management&mdash;for example, software updates or time
    synchronization&mdash;and defining roles for &sminion;s.
   </para>
   <para>
    On the &smaster;, install the <package>ceph-salt</package> package:
   </para>
<screen>&prompt.smaster;zypper install ceph-salt</screen>
   <para>
    The above command installed <package>ceph-salt-formula</package> as a
    dependency which modified the &smaster; configuration by inserting
    additional files in the <filename>/etc/salt/master.d</filename> directory.
    To apply the changes, restart
    <systemitem class="daemon">salt-master.service</systemitem> and synchronize
    &salt; modules:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure">
   <title>Configure Cluster Properties</title>
   <para>
    Use the <command>ceph-salt config</command> command to configure the basic
    properties of the cluster.
   </para>
   <sect3 xml:id="deploy-cephadm-configure-shell">
    <title>&cephsalt; Shell</title>
    <para>
     If you run <command>ceph-salt config</command> without any path or
     subcommand, you will enter an interactive &cephsalt; shell. The shell is
     convenient if you need to configure multiple properties in one batch and
     do not want type the full command syntax.
    </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  o- cephadm_bootstrap ......................................... [enabled]
  | o- ceph_conf ................................................... [...]
  | o- dashboard ................................................... [...]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  o- containers .................................................... [...]
  | o- images ...................................................... [...]
  |   o- ceph ............................................ [no image path]
  | o- registries ................................................ [empty]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- system_update ................................................. [...]
  | o- packages ................................................ [enabled]
  | o- reboot .................................................. [enabled]
  o- time_server ............................................... [enabled]
    o- external_servers .......................................... [empty]
    o- server_hostname ......................................... [not set]
    o- subnet .................................................. [not set]
</screen>
    <para>
     As you can see from the output of &cephsalt;'s <command>ls</command>
     command, the cluster configuration is organized in a tree structure. To
     configure a specific property of the cluster in the &cephsalt; shell, you
     have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Change to the path whose property you need to configure and run the
       command:
      </para>
<screen>
<prompt>/></prompt> cd /ceph_cluster/minions/
<prompt>/ceph_cluster/minions></prompt> ls
o- minions .................................................. [Minions: 5]
  o- ses-master.example.com ................................... [no roles]
  o- ses-min1.example.com ..................................... [no roles]
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Run the command from the current position and enter the absolute path to
       the property as the first argument:
      </para>
<screen>
<prompt>/></prompt> /ceph_cluster/minions/ ls
o- minions .................................................. [Minions: 5]
  o- ses-master.example.com ................................... [no roles]
  o- ses-min1.example.com ..................................... [no roles]
[...]
</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Autocompletion of Configuration Snippets</title>
     <para>
      While in a &cephsalt; shell, you can use the autocompletion feature
      similar to a normal Linux shell (Bash) autocompletion. It completes
      configuration paths, subcommands, or &sminion; names. When autocompleting
      a configuration path, you have two options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To let the shell finish a path relative to your current position, hit
        the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
      <listitem>
       <para>
        To let the shell finish an absolute path, enter <keycap>/</keycap> and
        hit the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
     </itemizedlist>
    </tip>
    <tip>
     <title>Navigating with the Cursor Keys</title>
     <para>
      If you enter <command>cd</command> from the &cephsalt; shell without any
      path, the command will print a tree structure of the cluster
      configuration with the line of the current path active. You can use the
      up and down cursor keys to navigate through individual lines. After you
      confirm with <keycap function="enter"></keycap>, the configuration path
      will change to the last active one.
     </para>
    </tip>
    <important>
     <title>Convention</title>
     <para>
      To keep the documentation consistent, we will use a single command syntax
      without entering the &cephsalt; shell. For example, you can list the
      cluster configuration tree by using the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-minions">
    <title>Add &sminion;s</title>
    <para>
     Include all or a subset of &sminion;s that we deployed and accepted in
     <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
     either specify the &sminion;s by their full names, or use a glob
     expressions '*' and '?' to include multiple &sminion;s at once. Use the
     <command>add</command> subcommand under the
     <literal>/ceph_cluster/minions</literal> path. The following command
     includes all accepted &sminion;s:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
    <para>
     Verify that the specified &sminion;s were added:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-admin">
    <title>Specify &adm;</title>
    <para>
     The &adm; is the node where the <filename>ceph.conf</filename>
     configuration file and the &ceph; admin keyring is installed. You usually
     run &ceph; related commands on the &adm;.
    </para>
    <tip>
     <title>&smaster; and &adm; on the Same Node</title>
     <para>
      In a homogeneous environment where all or most hosts belong to
      &productname;, we recommend having the &adm; on the same host as the
      &smaster;.
     </para>
     <para>
      In a heterogeneous environment where one &salt; infrastructure hosts more
      than one cluster, for example, &productname; together with &susemgr;, do
      <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
     </para>
    </tip>
    <para>
     To specify the &adm;, run the following command:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ............................ [no other roles]
</screen>
    <tip>
     <title>Install <filename>ceph.conf</filename> and the Admin Keyring on Multiple Nodes</title>
     <para>
      You can install the &ceph; configuration file and admin keyring on
      multiple nodes if you deployment requires it. For security reasons, avoid
      installing them on all the cluster's nodes.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-mon">
    <title>Specify First MON/MGR Node</title>
    <para>
     You need to specify which of the cluster's &sminion;s will bootstrap the
     cluster. This minion will become the first one running &mon; and &mgr;
     services.
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
    <important>
     <para>
      The minion that will bootstrap the cluster needs to have the admin
      keyring as well:
     </para>
<screen>
&smaster;ceph-salt config /ceph_cluster/roles/admin add ses-min1.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ............................ [no other roles]
  o- ses-min1.suse.cz .......................... [other roles: bootstrap]
</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ssh">
    <title>Generate SSH Key Pair</title>
    <para>
     &cephadm; uses the SSH protocol to communicate with cluster nodes. You
     need to generate the private and public part of the SSH key pair:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh ................................................... [Key Pair set]
  o- private_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ....... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ntp">
    <title>Configure Time Server</title>
    <para>
     Select one of the &sminion;s to be a time server for the rest of the
     cluster, and configure it to synchronize its time with a reliable time
     source outside of the cluster.
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/server_hostname set ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
    <para>
     The <option>/time_server/subnet</option> option specifies the subnet from
     which NTP clients are allowed to access the NTP server. It is
     automatically set when you specify
     <option>/time_server/server_hostname</option>. If you need to change it or
     specify it manually, run:
    </para>
<screen>
&prompt.cephuser;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
    <para>
     For more details, refer to the <command>man 5 chrony.conf</command> manual
     page and search for the <literal>allow</literal> directive.
    </para>
    <para>
     Check the time server settings:
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- server_hostname ........................... [ses-master.example.com]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
    <para>
     Find more information on setting up time synchronization in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
    </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configure Path to Container Images</title>
    <para>
     &cephadm; needs to know a valid URI path to container images that will be
     used during the deployment step. Verify whether the default path is set:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph ls</screen>
    <para>
     If there is no default path set or your deployment requires a specific
     path, add it as follows:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph set registry.suse.com/ses/7/ceph/ceph</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-verify">
    <title>Verify Cluster Configuration</title>
    <para>
     The minimal cluster configuration is finished. Inspect it for obvious
     errors:
    </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / .................................................................. [...]
  o- ceph_cluster ..................................................... [...]
  | o- minions ................................................. [Minions: 5]
  | | o- ses-master.example.com ..................................... [admin]
  | | o- ses-min1.example.com ................................... [bootstrap]
  | | o- ses-min2.example.com .................................... [no roles]
  | | o- ses-min3.example.com .................................... [no roles]
  | | o- ses-min4.example.com .................................... [no roles]
  | o- roles .......................................................... [...]
  |   o- admin ................................................. [Minions: 1]
  |   | o- ses-master.example.com .......................... [no other roles]
  |   o- bootstrap ................................... [ses-min1.example.com]
  o- cephadm_bootstrap ............................................ [enabled]
  | o- ceph_conf ...................................................... [...]
  | o- dashboard ...................................................... [...]
  |   o- password ...................................... [randomly generated]
  |   o- username ................................................... [admin]
  o- containers ....................................................... [...]
  | o- images ......................................................... [...]
  |   o- ceph ........................... [registry.suse.com/ses/7/ceph/ceph]
  | o- registries ................................................... [empty]
  o- ssh ..................................................... [Key Pair set]
  | o- private_key ........ [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ......... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- system_update .................................................... [...]
  | o- packages ................................................... [enabled]
  | o- reboot ..................................................... [enabled]
  o- time_server .................................................. [enabled]
    o- external_servers ................................................. [1]
    | o- 0.pt.pool.ntp.org ............................................ [...]
    o- server_hostname ............................. [ses-master.example.com]
    o- subnet ................................................ [10.20.6.0/24]
</screen>
    <tip>
     <title>Status of Cluster Configuration</title>
     <para>
      You can check if the configuration of the cluster is valid by running the
      following command:
     </para>
<screen>
&prompt.smaster;ceph-salt status
hosts:  0/5 deployed
config: OK
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-export">
    <title>Export Cluster Configuration</title>
    <para>
     After you have configured the basic cluster and its configuration is
     valid, it is a good idea to export its configuration to a file:
    </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
    <para>
     In case you break the cluster configuration and need to revert to a backup
     state, run:
    </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-deploy">
   <title>Deploy Cluster</title>
   <para>
    Deploy the previously configured minimal &ceph; cluster by running the
    following command:
   </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   <para>
    The above command will open an interactive user interface that shows the
    current progress of each minion.
   </para>
   <figure>
    <title>Deployment of Minimal Cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <title>Non-interactive Mode</title>
    <para>
     If you need to apply the configuration from a script, there is also a
     non-interactive mode of deployment. This is also useful when deploying the
     cluster from a remote machine because constant updating of the progress
     information on the screen over the network may become distracting:
    </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2">
  <title>Further Deployment and Administration (Day 2)</title>

  <para>
   After you have deployed the basic &ceph; cluster, you need to deploy core
   services to more cluster nodes. To make the cluster data accessible to
   clients, deploy additional services.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch">
<!-- https://docs.ceph.com/docs/master/mgr/orchestrator/ -->
   <title>The <command>ceph orch</command> Command</title>
   <para>
    The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
    an interface to the &cephadm; module&mdash;will take care of listing
    cluster components and deploying &ceph; services on new cluster nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-orch-status">
    <title>Displaying the Orchestrator Status</title>
    <para>
     The following command shows the current mode and status of the &ceph;
     orchestrator.
    </para>
<screen>&prompt.cephuser;ceph orch status</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-orch-list">
    <title>Listing Devices, Services, and Daemons</title>
    <para>
     To run <command>ceph-volume</command> on all nodes and list all disk
     devices, run:
    </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST        PATH      TYPE   SIZE  DEVICE  AVAIL  REJECT REASONS
ses-master  /dev/vda  hdd   16.0G          False  locked
ses-min1    /dev/vdb  hdd   20.0G          True
ses-min1    /dev/vda  hdd   16.0G          False  locked
ses-min2    /dev/vdb  hdd   20.0G          True
[...]
</screen>
    <tip>
     <title>Services and Daemons</title>
     <para>
      <emphasis>Service</emphasis> is a general term for a &ceph; service of a
      specific type, for example &mgr;.
     </para>
     <para>
      <emphasis>Daemon</emphasis> is a specific instance of a service, for
      example a process <literal>mgr.ses-min1.gdlcik</literal> running on a
      node called <literal>ses-min1</literal>.
     </para>
    </tip>
    <para>
     To list all services known to &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
    <tip>
     <para>
      You can limit the list to services on a particular node with the optional
      <option>–host</option> parameter and services of a particular type with
      the optional <option>–type</option> parameter (accepts
      <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>,
      <literal>mds</literal>, and <literal>rgw</literal>).
     </para>
    </tip>
    <para>
     To list all running daemons deployed by &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
    <tip>
     <para>
      To query the status of a particular daemon, use
      <option>--daemon_type</option> and <option>--daemon_id</option>. For
      OSDs, the ID is the numeric OSD ID. For MDS, the ID is the file system
      name:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-addnode">
   <title>Adding a New Node</title>
   <para>
    To add a new node to a &ceph; cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Install &sls; and &productname; on the new node. Refer to
      <xref linkend="deploy-os"/> for more information.
     </para>
    </step>
    <step>
     <para>
      Configure the node as a &sminion; of an already existing &smaster;. Refer
      to <xref linkend="deploy-salt"/> for more information.
     </para>
    </step>
    <step>
     <para>
      Add the new minion to &cephsalt;, for example:
     </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions add ses-min5.example.com
&prompt.smaster;ceph-salt deploy ses-min5.example.com
</screen>
     <para>
      Refer to <xref linkend="deploy-cephadm-configure-minions"/> for more
      information.
     </para>
    </step>
    <step>
     <para>
      Verify that the node was added:
     </para>
<screen>
&prompt.cephuser;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-services">
   <title>Deploying Services to Nodes</title>
   <para>
    After you have added a node to the &cephsalt; environment as described in
    <xref linkend="deploy-cephadm-day2-addnode"/>, you can deploy &ceph;
    service(s) to it.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-service-mon">
    <title>Deploy &mon;s and &mgr;s</title>
    <para>
     &ceph; cluster has three or five MONs deployed across different nodes. If
     there are five or more nodes in the cluster, we recommend deploying five
     MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
    </para>
    <important>
     <title>Include Bootstrap MON</title>
     <para>
      When deploying MONs and MGRs, remember to include the first MON that you
      added when configuring the basic cluster in
      <xref linkend="deploy-cephadm-configure-mon"/>.
     </para>
    </important>
    <para>
     To deploy new MONs, run:
    </para>
<screen>&prompt.cephuser;ceph orch apply mon <replaceable>HOST_SPECIFICATION</replaceable></screen>
    <para>
     To deploy new MGRs, run:
    </para>
<screen>&prompt.cephuser;ceph orch apply mgr <replaceable>HOST_SPECIFICATION</replaceable></screen>
    <para>
     Depending on whether all MONs or MGRs are on the same subnet, you need to
     adjust the <replaceable>HOST_SPECIFICATION</replaceable>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If all MONs or MGRs are on the same subnet, specify the short host names
       of the MONs or MGRs you need to add:
      </para>
<screen>&prompt.cephuser;ceph orch apply mon ses-min1 ses-min2 ses-min3</screen>
      <para>
       and
      </para>
<screen>&prompt.cephuser;ceph orch apply mgr ses-min1 ses-min2 ses-min3</screen>
     </listitem>
     <listitem>
      <para>
       If MONs and MGRs are on different subnets, you need to replace the
       <replaceable>HOST_SPECIFICATION</replaceable> with the following
       pattern:
      </para>
<screen><replaceable>SHORT_HOST_NAME</replaceable>:<replaceable>MON_SUBNET_IP</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&prompt.cephuser;ceph orch apply mon ses-min1:10.1.2.0/24 ses-min2:10.1.5.0/24</screen>
      <para>
       and
      </para>
<screen>&prompt.cephuser;ceph orch apply mgr ses-min1:10.1.2.0/24 ses-min2:10.1.5.0/24</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Labels for Nodes of a Specific Service</title>
     <para>
      You can mark the nodes that will be used as MONs or MGRs with the
      <literal>mon</literal> or <literal>mgr</literal> label, for example:
     </para>
<screen>
&prompt.cephuser;ceph orch host label add ses-min1 mon
&prompt.cephuser;ceph orch host label add ses-min2 mon
&prompt.cephuser;ceph orch host label add ses-min3 mon
</screen>
<screen>
&prompt.cephuser;ceph orch host label add ses-min1 mgr
&prompt.cephuser;ceph orch host label add ses-min2 mgr
&prompt.cephuser;ceph orch host label add ses-min3 mgr
</screen>
     <para>
      Check the hosts and their labels:
     </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST      ADDR   LABELS  STATUS
ses-min1         mon,mgr
ses-min2         mon,mgr
ses-min3         mon,mgr
ses-min4
</screen>
     <para>
      Then do the actual deployment:
     </para>
<screen>&prompt.cephuser;ceph orch apply mon</screen>
     <para>
      and
     </para>
<screen>&prompt.cephuser;ceph orch apply mgr</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-osd">
    <title>Deploy &osd;s</title>
    <important>
     <title>When Storage Device is Available</title>
     <para>
      A storage device is considered <emphasis>available</emphasis> if all of
      the following conditions are met:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The device has no partitions.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not have any LVM state.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is not be mounted.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a file system.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a &bluestore; OSD.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is larger than 5&nbsp;GB.
``
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If the above conditions are not met, &ceph; refuses to provision such
      OSDs.
     </para>
    </important>
    <para>
     There are several ways you can deploy new OSDs:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Tell &ceph; to consume all available and unused storage devices:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
     </listitem>
     <listitem>
      <para>
       Create an OSD from a specific device on a specific host:
      </para>
<screen>&prompt.cephuser;ceph orch daemon add osd <replaceable>HOST_NAME</replaceable>:<replaceable>DEVICE_PATH</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&prompt.cephuser;ceph orch daemon add osd ses-min1:/dev/sdb</screen>
     </listitem>
     <listitem>
      <para>
       Use &drvgrps; (see <xref linkend="drive-groups"/>) to describe devices
       that will be deployed based on their properties, such as device type
       (SSD or HDD), device model names, size, or the hosts on which the
       devices exist:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade">
<!-- https://docs.ceph.com/docs/master/cephadm/upgrade/ -->
   <title>Upgrading &ceph;</title>
   <para>
    You can instruct &cephadm; to upgrade &ceph; from one bugfix release to
    another. The automated upgrade of &ceph; services respects the recommended
    order&mdash;it starts with &mgr;s, &mon;s, and then continues on other
    services such as &osd;s, &mds;s, and &ogw;s. Each daemon is restarted only
    after &ceph; indicates that the cluster will remain available.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-start">
    <title>Starting the Upgrade</title>
    <para>
     Before you start the upgrade, verify that all nodes are currently online
     and your cluster is healthy:
    </para>
<screen>&prompt.cephuser;cephadm shell -- ceph -s</screen>
    <para>
     To upgrade (or downgrade) to a specific &ceph; release:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version <replaceable>VERSION</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version 15.2.1</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-monitor">
    <title>Monitoring the Upgrade</title>
    <para>
     Run the following command to determine whether an upgrade is in progress:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade status</screen>
    <para>
     While the upgrade is in progress, you will see a progress bar in the
     &ceph; status output:
    </para>
<screen>&prompt.cephuser;ceph -s
[...]
  progress:
    Upgrade to docker.io/ceph/ceph:v15.2.1 (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
    <para>
     You can also watch the &cephadm; log:
    </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-stop">
    <title>Cancelling an Upgrade</title>
    <para>
     You can stop the upgrade process at any time:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade stop</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>&drvgrps;</title>
   <para>
    <emphasis>&drvgrps;</emphasis> specify the layouts of OSDs in the &ceph;
    cluster. They are defined in a single YAML file. In this section, we will
    use <filename>drive_groups.yml</filename> as an example.
   </para>
   <para>
    An administrator should manually specify a group of OSDs that are
    interrelated (hybrid OSDs that are deployed on a mixture of HDDs and SDDs) or
    share the same deployment options (identical, for example same object
    store, same encryption option, stand-alone OSDs). To avoid explicitly
    listing devices, &drvgrps; use a list of filter items that correspond to a
    few selected fields of <command>ceph-volume</command>'s inventory reports.
    In the simplest case this could be the 'rotational' flag (all solid-state
    drives are to be db_devices, all rotating ones data devices) or something
    more involved such as 'model' strings, or sizes. &deepsea; will provide
    code that translates these &drvgrps; into actual device lists for
    inspection by the user.
   </para>
<!-- 2020-04-30 tbazant: seems to be DeepSea-related
   <para>
    Following is a simple procedure that demonstrates the basic workflow when
    configuring &drvgrps;:
   </para>
   <procedure>
    <step>
     <para>
      Inspect your disks' properties as seen by the
      <command>ceph-volume</command> command. Only these properties are
      accepted by &drvgrps;:
     </para>
<screen>
&prompt.smaster;salt-run disks.details
</screen>
    </step>
    <step>
     <para>
      Open a YAML file where you will store the &drvgrps; specification, for
      example <filename>drive_groups.yml</filename> and adjust to your needs.
      Refer to <xref linkend="drive-groups-specs" />. Remember to use spaces
      instead of tabs. Find more advanced examples in
      <xref linkend="drive-groups-examples" />. The following example includes
      all drives available to &ceph; as OSDs:
     </para>
<screen>
default_drive_group_name:
  target: '*'
  data_devices:
    all: true
</screen>
    </step>
    <step>
     <para>
      Verify new layouts:
     </para>
<screen>
&prompt.smaster;salt-run disks.list
</screen>
     <para>
      This runner returns you a structure of matching disks based on your
      &drvgrps;. If you are not happy with the result, repeat the previous
      step.
     </para>
     <tip>
      <title>Detailed Report</title>
      <para>
       In addition to the <command>disks.list</command> runner, there is a
       <command>disks.report</command> runner that prints out a detailed
       report.
      </para>
<screen>
&prompt.smaster;salt-run disks.report
</screen>
     </tip>
    </step>
   </procedure>
-->
   <sect3 xml:id="drive-groups-specs">
    <title>Specification</title>
    <para>
     The &drvgrps; specification file&mdash;for example,
     <filename>drive_groups.yml</filename>&mdash;can take one of two basic
     forms, depending on whether &bluestore; or &filestore; is to be used. For
     &bluestore; setups, <filename>drive_groups.yml</filename> can be as
     follows:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  db_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  wal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    <para>
     For &filestore; setups, <filename>drive_groups.yml</filename> can be as
     follows:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  journal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  format: filestore
  encryption: True
</screen>
   </sect3>
   <sect3>
    <title>Matching Disk Devices</title>
    <para>
     You can describe the specification using the following filters:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       By a disk model:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       By a disk vendor:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>Lowercase Vendor String</title>
       <para>
        Always enter the <replaceable>DISK_VENDOR_STRING</replaceable> in lower case.
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       Whether a disk is rotational or not. SSDs and NVMe drives are not
       rotational.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Deploy a node using <emphasis>all</emphasis> available drives for OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Additionally, by limiting the number of matching disks:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Filtering Devices by Size</title>
    <para>
     You can filter disk devices by their size&mdash;either by an exact size,
     or a size range. The <option>size:</option> parameter accepts arguments in
     the following form:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - Includes disks of an exact size.
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - Includes disks whose size is within the range.
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - Includes disks less than or equal to 10&nbsp;GB in size.
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - Includes disks equal to or greater than 40&nbsp;GB in size.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Matching by Disk Size</title>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <note>
     <title>Quotes Required</title>
     <para>
      When using the ':' delimiter, you need to enclose the size in quotes,
      otherwise the ':' sign will be interpreted as a new configuration hash.
     </para>
    </note>
    <tip>
     <title>Unit Shortcuts</title>
     <para>
      Instead of (G)igabytes, you can specify the sizes in (M)egabytes or
      (T)erabytes as well.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Examples</title>
    <para>
     This section includes examples of different OSD setups.
    </para>
    <example>
     <title>Simple Setup</title>
     <para>
      This example describes two nodes with the same setup:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The corresponding <filename>drive_groups.yml</filename> file will be as
      follows:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ
   </screen>
     <para>
      Such a configuration is simple and valid. The problem is that an
      administrator may add disks from different vendors in the future, and
      these will not be included. You can improve it by reducing the filters on
      core properties of the drives:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
   </screen>
     <para>
      In the previous example, we are enforcing all rotating devices to be
      declared as 'data devices' and all non-rotating devices will be used as
      'shared devices' (wal, db).
     </para>
     <para>
      If you know that drives with more than 2&nbsp;TB will always be the
      slower data devices, you can filter by size:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <example>
     <title>Advanced Setup</title>
     <para>
      This example describes two distinct setups: 20 HDDs should share 2 SSDs,
      while 10 SSDs should share 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Such a setup can be defined with two layouts as follows:
     </para>
<screen>
drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
</screen>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB
</screen>
    </example>
    <example>
     <title>Advanced Setup with Non-uniform Nodes</title>
     <para>
      The previous examples assumed that all nodes have the same drives.
      However, that is not always the case:
     </para>
     <para>
      Nodes 1-5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodes 6-10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      You can use the 'target' key in the layout to target specific nodes.
      &salt; target notation helps to keep things simple:
     </para>
<screen>
drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
</screen>
     <para>
      followed by
     </para>
<screen>
drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Expert Setup</title>
     <para>
      All previous cases assumed that the WALs and DBs use the same device. It
      is however possible to deploy the WAL on a dedicated device as well:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Complex (and Unlikely) Setup</title>
     <para>
      In the following setup, we are trying to define:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs backed by 1 SSD(db) and 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs backed by 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSDs stand-alone (encrypted)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD is spare and should not be deployed
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The summary of used drives follows:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Model: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 4&nbsp;TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Model: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 512&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Vendor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Model: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Size: 256&nbsp;GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      The &drvgrps; definition will be the following:
     </para>
<screen>
drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True
 </screen>
     <para>
      One HDD will remain as the file is being parsed from top to bottom.
     </para>
    </example>
   </sect3>
  </sect2>
 </sect1>
</chapter>
