<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-cephadm">
 <title>Deploying with &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &productname; &productnumber; uses the &salt;-based &cephsalt; tool to
  prepare the operating system on each participating cluster node for the
  deployment via &cephadm;. &cephadm; deploys and manages a &ceph; cluster by
  connecting to hosts from the &mgr; daemon via SSH. &cephadm; manages the full
  lifecycle of a &ceph; cluster. It starts by bootstrapping a tiny cluster on a
  single node (one MON and MGR service) and then uses the orchestration
  interface to expand the cluster to include all hosts and to provision all
  &ceph; services. You can perform this via the &ceph; command line interface
  (CLI) or partially via &dashboard; (GUI).
 </para>
 <para>
  To deploy a &ceph; cluster by using &cephadm;, you need to complete the
  following tasks:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Install and do basic configuration of the underlying operating
    system&mdash;&cephos;&mdash;on all cluster nodes.
   </para>
  </listitem>
  <listitem>
   <para>
    Deploy the &salt; infrastructure on all cluster nodes for performing the
    initial deployment preparations via &cephsalt;.
   </para>
  </listitem>
  <listitem>
   <para>
    Configure the basic properties of the cluster via &cephsalt; and deploy it.
   </para>
  </listitem>
  <listitem>
   <para>
    Add new nodes and roles to the cluster and deploy services to them using
    &cephadm;.
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="deploy-os">
  <title>Install and Configure &sls;</title>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on each cluster node. Include at least the
     following modules:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Basesystem Module
      </para>
     </listitem>
     <listitem>
      <para>
       Server Applications Module
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Find more details on how to install &sls; in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-install.html"/>.
    </para>
   </step>
   <step>
    <para>
     Install the <emphasis>&productname; &productnumber;</emphasis> extension
     on each cluster node.
    </para>
    <tip>
     <title>Install &productname; Together with &sls;</title>
     <para>
      You can either install the &productname; &productnumber; extension
      separately after you have installed &cephos;, or you can add it during
      the &cephos; installation procedure.
     </para>
    </tip>
    <para>
     Find more details on how to install extensions in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html"/>.
    </para>
   </step>
   <step>
    <para>
     Configure network settings including proper DNS name resolution on each
     node. For more information on configuring a network, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#sec-network-yast"/>
     For more information on configuring a DNS server, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-dns"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-salt">
  <title>Deploy &salt;</title>

  <para>
   &productname; uses &salt; and &cephsalt; for the initial cluster
   preparation. &salt; helps you configure and run commands on multiple cluster
   nodes simultaneously from one dedicated host called the
   <emphasis>&smaster;</emphasis>. Before deploying &salt;, consider the
   following important points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by a dedicated
     node called &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     If the &smaster; host should be part of the &ceph; cluster, it needs to
     run its own &sminion;, but this is not a requirement.
    </para>
    <tip>
     <title>Sharing Multiple Roles per Server</title>
     <para>
      You will get the best performance from your &ceph; cluster when each role
      is deployed on a separate node. But real deployments sometimes require
      sharing one node for multiple roles. To avoid trouble with performance
      and the upgrade procedure, do not deploy the &osd;, &mds;, or &mon; role
      to the &adm;.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name, but you can specify any other network-reachable host name in the
     <filename>/etc/salt/minion</filename> file.
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Install the <literal>salt-master</literal> on the &smaster; node:
    </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     If you intend to use the firewall, verify that the &smaster; node has
     ports 4505 and 4506 open to all &sminion; nodes. If the ports are closed,
     you can open them using the <command>yast2 firewall</command> command by
     allowing the <guimenu>salt-master</guimenu> service for the appropriate zone. For example, <literal>public</literal>.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Make sure that the <emphasis>fully qualified domain name</emphasis> of
     each node can be resolved to an IP address on the public cluster network
     by all the other nodes.
    </para>
   </step>
   <step>
    <para>
     Configure all minions to connect to the master. If your &smaster; is not
     reachable by the host name <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all related &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started on all nodes. Enable and start it if needed:
    </para>
<screen>&prompt.root;systemctl enable salt-minion.service
&prompt.root;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verify each &sminion;'s fingerprint and accept all salt keys on the
     &smaster; if the fingerprints match.
    </para>
    <note>
     <para>
      If the &sminion; fingerprint comes back empty, make sure the &sminion;
      has a &smaster; configuration and that it can communicate with the
      &smaster;.
     </para>
    </note>
    <para>
     View each minion's fingerprint:
    </para>
<screen>&prompt.sminion;salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     After gathering fingerprints of all the &sminion;s, list fingerprints of
     all unaccepted minion keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     If the minions' fingerprints match, accept them:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Test whether all &sminion;s respond:
    </para>
<screen>&prompt.smaster;salt-run manage.status</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day1">
  <title>Deploy Basic Cluster (Day 1)</title>

  <para>
   This section guides you through the process of deploying a basic &ceph;
   cluster. Read the following subsections carefully and execute the included
   commands in the given order.
  </para>

  <sect2 xml:id="deploy-cephadm-cephsalt">
   <title>Install &cephsalt;</title>
   <para>
    &cephsalt; provides tools for deploying &ceph; clusters managed by
    &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
    management&mdash;for example, software updates or time
    synchronization&mdash;and defining roles for &sminion;s.
   </para>
   <para>
    On the &smaster;, install the <package>ceph-salt</package> package:
   </para>
<screen>&prompt.smaster;zypper install ceph-salt</screen>
   <para>
    The above command installed <package>ceph-salt-formula</package> as a
    dependency which modified the &smaster; configuration by inserting
    additional files in the <filename>/etc/salt/master.d</filename> directory.
    To apply the changes, restart
    <systemitem class="daemon">salt-master.service</systemitem> and synchronize
    &salt; modules:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure">
   <title>Configure Cluster Properties</title>
   <para>
    Use the <command>ceph-salt config</command> command to configure the basic
    properties of the cluster.
   </para>
   <sect3 xml:id="deploy-cephadm-configure-shell">
    <title>&cephsalt; Shell</title>
    <para>
     If you run <command>ceph-salt config</command> without any path or
     subcommand, you will enter an interactive &cephsalt; shell. The shell is
     convenient if you need to configure multiple properties in one batch and
     do not want type the full command syntax.
    </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- auth ........................................................ [...]
  | | o- registries .............................................. [empty]
  | o- images ...................................................... [...]
  | | o- ceph ............................................ [no image path]
  | o- registries_conf ......................................... [enabled]
  |   o- registries .............................................. [empty]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- system_update ................................................. [...]
  | o- packages ................................................ [enabled]
  | o- reboot .................................................. [enabled]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- server_hostname ......................................... [not set]
    o- subnet .................................................. [not set]
</screen>
    <para>
     As you can see from the output of &cephsalt;'s <command>ls</command>
     command, the cluster configuration is organized in a tree structure. To
     configure a specific property of the cluster in the &cephsalt; shell, you
     have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Run the command from the current position and enter the absolute path to
       the property as the first argument:
      </para>
<screen>
<prompt>/></prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/> /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
     </listitem>
     <listitem>
      <para>
       Change to the path whose property you need to configure and run the
       command:
      </para>
<screen>
<prompt>/></prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions></prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Autocompletion of Configuration Snippets</title>
     <para>
      While in a &cephsalt; shell, you can use the autocompletion feature
      similar to a normal Linux shell (Bash) autocompletion. It completes
      configuration paths, subcommands, or &sminion; names. When autocompleting
      a configuration path, you have two options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To let the shell finish a path relative to your current position, hit
        the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
      <listitem>
       <para>
        To let the shell finish an absolute path, enter <keycap>/</keycap> and
        hit the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
     </itemizedlist>
    </tip>
    <tip>
     <title>Navigating with the Cursor Keys</title>
     <para>
      If you enter <command>cd</command> from the &cephsalt; shell without any
      path, the command will print a tree structure of the cluster
      configuration with the line of the current path active. You can use the
      up and down cursor keys to navigate through individual lines. After you
      confirm with <keycap function="enter"></keycap>, the configuration path
      will change to the last active one.
     </para>
    </tip>
    <important>
     <title>Convention</title>
     <para>
      To keep the documentation consistent, we will use a single command syntax
      without entering the &cephsalt; shell. For example, you can list the
      cluster configuration tree by using the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-minions">
    <title>Add &sminion;s</title>
    <para>
     Include all or a subset of &sminion;s that we deployed and accepted in
     <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
     either specify the &sminion;s by their full names, or use a glob
     expressions '*' and '?' to include multiple &sminion;s at once. Use the
     <command>add</command> subcommand under the
     <literal>/ceph_cluster/minions</literal> path. The following command
     includes all accepted &sminion;s:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
    <para>
     Verify that the specified &sminion;s were added:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-cephadm">
    <title>Specify &sminion;s Managed by &cephadm;</title>
    <para>
     Specify which nodes will belong to the &ceph; cluster and will be managed
     by &cephadm;. Include all nodes that will run &ceph; services as well as
     the &adm;:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-admin">
    <title>Specify &adm;</title>
    <para>
     The &adm; is the node where the <filename>ceph.conf</filename>
     configuration file and the &ceph; admin keyring is installed. You usually
     run &ceph; related commands on the &adm;.
    </para>
    <tip>
     <title>&smaster; and &adm; on the Same Node</title>
     <para>
      In a homogeneous environment where all or most hosts belong to
      &productname;, we recommend having the &adm; on the same host as the
      &smaster;.
     </para>
     <para>
      In a heterogeneous environment where one &salt; infrastructure hosts more
      than one cluster, for example, &productname; together with &susemgr;, do
      <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
     </para>
    </tip>
    <para>
     To specify the &adm;, run the following command:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
    <tip>
     <title>Install <filename>ceph.conf</filename> and the Admin Keyring on Multiple Nodes</title>
     <para>
      You can install the &ceph; configuration file and admin keyring on
      multiple nodes if you deployment requires it. For security reasons, avoid
      installing them on all the cluster's nodes.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-mon">
    <title>Specify First MON/MGR Node</title>
    <para>
     You need to specify which of the cluster's &sminion;s will bootstrap the
     cluster. This minion will become the first one running &mon; and &mgr;
     services.
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
    <important>
     <para>
      The minion that will bootstrap the cluster needs to have the admin
      keyring as well:
     </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-min1.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 2]
  o- ses-master.example.com ............................ [Other roles: cephadm]
  o- ses-min1.example.com ...................... [Other roles: cephadm, bootstrap]
</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ssh">
    <title>Generate SSH Key Pair</title>
    <para>
     &cephadm; uses the SSH protocol to communicate with cluster nodes. You
     need to generate the private and public part of the SSH key pair:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh ................................................... [Key Pair set]
  o- private_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ....... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ntp">
    <title>Configure Time Server</title>
    <para>
     All cluster nodes need to have their time synchronized with a reliable
     time source. There are several scenarios to approach time synchronization:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If all cluster nodes are already configured to synchronize their time
       using a NTP service of choice, disable time server handling completely:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server disable
</screen>
     </listitem>
     <listitem>
      <para>
       If your site already has a single source of time, specify the host name
       of the time source:
      </para>
<screen>
 &prompt.smaster;ceph-salt config /time_server/server_hostname set <replaceable>time-server.example.com</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       To have the time synchronized by &cephsalt;, select one of the
       &sminion;s to be a time server for the rest of the cluster, and
       configure it to synchronize its time with a reliable time source outside
       of the cluster.
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/server_hostname set ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
      <para>
       The <option>/time_server/subnet</option> option specifies the subnet
       from which NTP clients are allowed to access the NTP server. It is
       automatically set when you specify
       <option>/time_server/server_hostname</option>. If you need to change it
       or specify it manually, run:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
     </listitem>
    </itemizedlist>
    <para>
     For more details, refer to the <command>man 5 chrony.conf</command> manual
     page and search for the <literal>allow</literal> directive.
    </para>
    <para>
     Check the time server settings:
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- server_hostname ........................... [ses-master.example.com]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
    <para>
     Find more information on setting up time synchronization in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
    </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-dashboardlogin">
    <title>Configure &dashboard; Login Credentials</title>
    <para>
     &dashboard; will be available after the basic cluster is deployed. To
     access it, you need to set a valid user name and password, for example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/username set admin
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
    <tip>
     <title>Forcing Password Update</title>
     <para>
      By default, the first dashboard user will be forced to change their
      password on first login to the dashboard. To disable this feature, run
      the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configure Path to Container Images</title>
    <para>
     &cephadm; needs to know a valid URI path to container images that will be
     used during the deployment step. Verify whether the default path is set:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph ls</screen>
    <para>
     If there is no default path set or your deployment requires a specific
     path, add it as follows:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph set registry.suse.com/ses/7/ceph/ceph</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-registry">
    <title>Configure Container Registry</title>
    <para>
     Optionally, you can set a local container registry. This will serve as a
     mirror of the <literal>registry.suse.com</literal> registry. Remember you
     need to re-sync the local registry whenever there are new updated
     containers available from <systemitem class="systemname">registry.suse.com</systemitem>.  Find more
     information about creating a container registry in <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-registry-installation.html"/>
    </para>
    <para>
     Creating a local registry is useful in the following scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       You have a lot of cluster nodes and want to save download time and
       bandwidth by creating a local mirror of container images.
      </para>
     </listitem>
     <listitem>
      <para>
       Your cluster has no access to the online registry (an air-gapped
       deployment) and you need a local mirror to pull the container images
       from.
      </para>
     </listitem>
     <listitem>
      <para>
       If configuration or network issues prevent your cluster from accessing
       remote registries across a secure link, so you need a local, unencrypted
       registry instead.
      </para>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Registry Cache</title>
     <para>
      To avoid re-syncing the local registry when new updated containers
      appear, you can configure a <emphasis>registry cache</emphasis>. Find
      more information about containers and air-gapped scenarios in
      <link
       xlink:href="https://documentation.suse.com/suse-caasp/4.1/html/caasp-deployment/_deployment_scenarios.html#_airgapped_deployment"/>.
     </para>
    </tip>
    <tip>
     <title>Container Tools</title>
     <para>
      The following procedure uses <command>podman</command> to create a
      container registry and <command>skopeo</command> to mirror container
      images. To install <package>podman</package>, you need to add the
      <literal>Containers Module</literal> extension. For more information,
      refer to their manual pages <command>man 1 podman</command> and
      <command>man 1 skopeo</command>.
     </para>
    </tip>
    <procedure>
     <step>
      <para>
       Install <command>podman</command> and <command>skopeo</command>:
      </para>
<screen>&prompt.smaster;zypper install podman skopeo</screen>
     </step>
    </procedure>
    <para>
     To configure a local container registry, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Create a local container registry accessible but outside of the &ceph;
       cluster, for example:
      </para>
<screen>
&prompt.root;podman run -d --restart=always --net=host --name registry -p 5000:5000 registry:2
</screen>
     </step>
     <step>
      <para>
       Mirror SES &productnumber; related container images to the local
       registry:
      </para>
<screen>
&prompt.root;skopeo copy \
 --dest-tls-verify=false \
 docker://registry.suse.com/ses/7/ceph/ceph \
 docker://<replaceable>LOCAL_REGISTRY_HOST_IP</replaceable>:5000/registry.suse.com/ses/7/ceph/ceph
</screen>
     </step>
     <step>
      <para>
       On the &smaster;, add the local repository to the &cephsalt;
       configuration:
      </para>
<screen>
&prompt.smaster;ceph-salt config /containers/registries_conf/registries \
 add prefix=registry.suse.com \
 location=<replaceable>LOCAL_REGISTRY_HOST_IP</replaceable>:5000/registry.suse.com
</screen>
      <para>
       If your deployment requires insecure access, add the
       <option>insecure=true</option> option:
      </para>
<screen>
&prompt.smaster;ceph-salt config /containers/registries_conf/registries \
 add prefix=registry.suse.com \
 location=<replaceable>LOCAL_REGISTRY_HOST_IP</replaceable>:5000/registry.suse.com insecure=true
</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-reboots">
    <title>Configure Cluster Update and Reboot Behavior</title>
    <para>
     You can configure whether software packages will be updated on cluster
     nodes during the deployment (see <xref linkend="deploy-cephadm-deploy"/>),
     and whether nodes will automatically reboot when the update requires it.
    </para>
    <para>
     By default, package updates and reboots are enabled.
    </para>
    <important>
     <title>&smaster; Reboot</title>
     <para>
      If the &smaster; is part of the cluster (it is its own minion at the same
      time) and needs to reboot during the deployment, do such reboot manually.
      After the &smaster; reboots, run <command>ceph-salt apply</command> again
      to continue the deployment.
     </para>
    </important>
    <para>
     In rare cases, you may want to disable the automatic update of software
     packages on cluster nodes. For example, if you verified that the system is
     operating optimally and therefore want to keep the packages at the current
     version. To disable software updates, change the
     <option>/system_update/packages</option> configuration:
    </para>
<screen>&prompt.smaster;ceph-salt config /system_update/packages disable</screen>
    <para>
     Preventing nodes from automatic reboots is useful if you run cluster
     services that should not be interrupted during system updates. To disable
     automatic node reboots, change the <option>/system_update/reboot</option>
     configuration:
    </para>
<screen>&prompt.smaster;ceph-salt config /system_update/reboot disable</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-verify">
    <title>Verify Cluster Configuration</title>
    <para>
     The minimal cluster configuration is finished. Inspect it for obvious
     errors:
    </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ..................................................... [None]
  o- containers .................................................... [...]
  | o- images ...................................................... [...]
  |   o- ceph ........................ [registry.suse.com/ses/7/ceph/ceph]
  | o- registries ................................................ [empty]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- system_update ................................................. [...]
  | o- packages ................................................ [enabled]
  | o- reboot .................................................. [enabled]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- server_hostname .......................... [ses-master.example.com]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
    <tip>
     <title>Status of Cluster Configuration</title>
     <para>
      You can check if the configuration of the cluster is valid by running the
      following command:
     </para>
<screen>
&prompt.smaster;ceph-salt status
hosts:  0/5 managed by cephadm
config: OK
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-export">
    <title>Export Cluster Configuration</title>
    <para>
     After you have configured the basic cluster and its configuration is
     valid, it is a good idea to export its configuration to a file:
    </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
    <warning>
     <para>
      The output of the <command>ceph-salt export</command> includes the SSH
      private key. If you are concerned about the security implications, do not
      execute this command without taking appropriate precautions.
     </para>
    </warning>
    <para>
     In case you break the cluster configuration and need to revert to a backup
     state, run:
    </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-deploy">
   <title>Deploy Cluster</title>
   <para>
    Deploy the previously configured minimal &ceph; cluster by running the
    following command:
   </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   <para>
    The above command will open an interactive user interface that shows the
    current progress of each minion.
   </para>
   <figure>
    <title>Deployment of Minimal Cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <title>Non-interactive Mode</title>
    <para>
     If you need to apply the configuration from a script, there is also a
     non-interactive mode of deployment. This is also useful when deploying the
     cluster from a remote machine because constant updating of the progress
     information on the screen over the network may become distracting:
    </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
   </tip>
  </sect2>

  <sect2 xml:id="day2-deployment">
   <title>Further Deployment (Day 2)</title>
   <para>
    After you have deployed the basic &ceph; cluster, you need to deploy core
    services to more cluster nodes. To make the cluster data accessible to
    clients, deploy additional services as well.
   </para>
   <para>
    There are two ways to deploy additional &ceph; services:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      By using the &dashboard;'s graphical Web UI. Find more details in
      <xref linkend="deploy-dashboard-day2"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      By using the <command>ceph orch</command> subcommands on the command
      line. Find more details in <xref linkend="deploy-cephadm-day2"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-dashboard-day2">
  <title>Further Deployment (Day 2) Using the &dashboard;</title>

  <para>
   ADDME
  </para>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2">
  <title>Further Deployment (Day 2) Using the Command Line</title>

  <sect2 xml:id="deploy-cephadm-day2-orch">
<!-- https://docs.ceph.com/docs/master/mgr/orchestrator/ -->
   <title>The <command>ceph orch</command> Command</title>
   <para>
    The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
    an interface to the &cephadm; module&mdash;will take care of listing
    cluster components and deploying &ceph; services on new cluster nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-orch-status">
    <title>Displaying the Orchestrator Status</title>
    <para>
     The following command shows the current mode and status of the &ceph;
     orchestrator.
    </para>
<screen>&prompt.cephuser;ceph orch status</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-orch-list">
    <title>Listing Devices, Services, and Daemons</title>
    <para>
     To run <command>ceph-volume</command> on all nodes and list all disk
     devices, run:
    </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST        PATH      TYPE   SIZE  DEVICE  AVAIL  REJECT REASONS
ses-master  /dev/vda  hdd   16.0G          False  locked
ses-min1    /dev/vdb  hdd   20.0G          True
ses-min1    /dev/vda  hdd   16.0G          False  locked
ses-min2    /dev/vdb  hdd   20.0G          True
[...]
</screen>
    <tip>
     <title>Services and Daemons</title>
     <para>
      <emphasis>Service</emphasis> is a general term for a &ceph; service of a
      specific type, for example &mgr;.
     </para>
     <para>
      <emphasis>Daemon</emphasis> is a specific instance of a service, for
      example a process <literal>mgr.ses-min1.gdlcik</literal> running on a
      node called <literal>ses-min1</literal>.
     </para>
    </tip>
    <para>
     To list all services known to &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
    <tip>
     <para>
      You can limit the list to services on a particular node with the optional
      <option>-–host</option> parameter and services of a particular type
      with the optional <option>-–type</option> parameter (acceptable types
      are <literal>mon</literal>, <literal>osd</literal>,
      <literal>mgr</literal>, <literal>mds</literal>, and
      <literal>rgw</literal>).
     </para>
    </tip>
    <para>
     To list all running daemons deployed by &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
    <tip>
     <para>
      To query the status of a particular daemon, use
      <option>--daemon_type</option> and <option>--daemon_id</option>. For
      OSDs, the ID is the numeric OSD ID. For MDS, the ID is the file system
      name:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="cephadm-service-and-placement-specs">
   <title>Service and Placement Specification</title>
   <para>
    The recommended way to specify the deployment of &ceph; services is to
    create a YAML-formatted file with specification of services that you intend
    to deploy.
   </para>
   <para>
    You can create a separate specification file for each type of service, for
    example:
   </para>
<screen>
&prompt.smaster;cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
 spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Alternatively, you can specify multiple (or all) service types in one
    file&mdash;for example, <filename>cluster.yml</filename>&mdash;that
    describes which nodes will run specific services. Remember to separate
    individual service types with three dashes (<literal>---</literal>):
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
 spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    The aforementioned properties have the following meaning:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       The type of the service. It can be either a &ceph; service
       (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>,
       <literal>crash</literal>, <literal>osd</literal>, or
       <literal>rbd-mirror</literal>), a gateway (<literal>nfs</literal> or
       <literal>rgw</literal>), or part of the monitoring stack
       (<literal>alertmanager</literal>, <literal>grafana</literal>,
       <literal>node-exporter</literal>, or <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       The name of the service. Specifications of type <literal>mon</literal>,
       <literal>mgr</literal>, <literal>alertmanager</literal>,
       <literal>grafana</literal>, <literal>node-exporter</literal>, and
       <literal>prometheus</literal> do not require the
       <literal>service_id</literal> property.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Specifies which nodes will be running the service. Refer to
       <xref linkend="cephadm-placement-specs"/> for more details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Additional specification relevant for the service type.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Applying Specific Services</title>
    <para>
     &ceph; cluster services have usually a number of properties specific to
     them. For examples and details of individual services' specification,
     refer to <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
   <sect3 xml:id="cephadm-placement-specs">
    <title>Placement Specification</title>
    <para>
     To deploy &ceph; services, &cephadm; needs to know on which nodes to
     deploy them. Use the <literal>placement</literal> property and list the
     short host names of the nodes that the service applies to:
    </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
 placement:
  hosts:
   - host1
   - host2
   - host3
[...]
</screen>
   </sect3>
   <sect3 xml:id="drive-groups">
    <title>OSD Specification</title>
    <para>
     <emphasis>&drvgrps;</emphasis> specify the layouts of OSDs in the &ceph;
     cluster. They are defined in a single YAML file. In this section, we will
     use <filename>drive_groups.yml</filename> as an example.
    </para>
    <para>
     An administrator should manually specify a group of OSDs that are
     interrelated (hybrid OSDs that are deployed on a mixture of HDDs and SDDs)
     or share identical deployment options (for example, the same object store,
     same encryption option, stand-alone OSDs). To avoid explicitly listing
     devices, &drvgrps; use a list of filter items that correspond to a few
     selected fields of <command>ceph-volume</command>'s inventory reports.
     &cephadm; will provide code that translates these &drvgrps; into actual
     device lists for inspection by the user.
    </para>
    <para>
     To apply OSD specification to your cluster, run
    </para>
<screen>&prompt.cephuser;ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
    <sect4 xml:id="drive-groups-specs">
     <title>Specification</title>
     <para>
      Following is an example &drvgrps; specification file:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
osds_per_device: 1   # number of osd daemons per device
encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    </sect4>
    <sect4>
     <title>Matching Disk Devices</title>
     <para>
      You can describe the specification using the following filters:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        By a disk model:
       </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
      </listitem>
      <listitem>
       <para>
        By a disk vendor:
       </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
       <tip>
        <title>Lowercase Vendor String</title>
        <para>
         Always enter the <replaceable>DISK_VENDOR_STRING</replaceable> in
         lower case.
        </para>
       </tip>
      </listitem>
      <listitem>
       <para>
        Whether a disk is rotational or not. SSDs and NVMe drives are not
        rotational.
       </para>
<screen>
rotational: 0
</screen>
      </listitem>
      <listitem>
       <para>
        Deploy a node using <emphasis>all</emphasis> available drives for OSDs:
       </para>
<screen>
data_devices:
  all: true
</screen>
      </listitem>
      <listitem>
       <para>
        Additionally, by limiting the number of matching disks:
       </para>
<screen>
limit: 10
</screen>
      </listitem>
     </itemizedlist>
    </sect4>
    <sect4>
     <title>Filtering Devices by Size</title>
     <para>
      You can filter disk devices by their size&mdash;either by an exact size,
      or a size range. The <option>size:</option> parameter accepts arguments
      in the following form:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        '10G' - Includes disks of an exact size.
       </para>
      </listitem>
      <listitem>
       <para>
        '10G:40G' - Includes disks whose size is within the range.
       </para>
      </listitem>
      <listitem>
       <para>
        ':10G' - Includes disks less than or equal to 10&nbsp;GB in size.
       </para>
      </listitem>
      <listitem>
       <para>
        '40G:' - Includes disks equal to or greater than 40&nbsp;GB in size.
       </para>
      </listitem>
     </itemizedlist>
     <example>
      <title>Matching by Disk Size</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
     </example>
     <note>
      <title>Quotes Required</title>
      <para>
       When using the ':' delimiter, you need to enclose the size in quotes,
       otherwise the ':' sign will be interpreted as a new configuration hash.
      </para>
     </note>
     <tip>
      <title>Unit Shortcuts</title>
      <para>
       Instead of Gigabytes (G), you can specify the sizes in Megabytes (M) or
       Terabytes (T).
      </para>
     </tip>
    </sect4>
    <sect4 xml:id="ds-drive-groups-examples">
     <title>Examples</title>
     <para>
      This section includes examples of different OSD setups.
     </para>
     <example>
      <title>Simple Setup</title>
      <para>
       This example describes two nodes with the same setup:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         20 HDDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         2 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       The corresponding <filename>drive_groups.yml</filename> file will be as
       follows:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
   </screen>
      <para>
       Such a configuration is simple and valid. The problem is that an
       administrator may add disks from different vendors in the future, and
       these will not be included. You can improve it by reducing the filters
       on core properties of the drives:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
   </screen>
      <para>
       In the previous example, we are enforcing all rotating devices to be
       declared as 'data devices' and all non-rotating devices will be used as
       'shared devices' (wal, db).
      </para>
      <para>
       If you know that drives with more than 2&nbsp;TB will always be the
       slower data devices, you can filter by size:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
     </example>
     <example>
      <title>Advanced Setup</title>
      <para>
       This example describes two distinct setups: 20 HDDs should share 2 SSDs,
       while 10 SSDs should share 2 NVMes.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         20 HDDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         12 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         2 NVMes
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Samsung
          </para>
         </listitem>
         <listitem>
          <para>
           Model: NVME-QQQQ-987
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 256&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       Such a setup can be defined with two layouts as follows:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
 host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
     </example>
     <example>
      <title>Advanced Setup with Non-Uniform Nodes</title>
      <para>
       The previous examples assumed that all nodes have the same drives.
       However, that is not always the case:
      </para>
      <para>
       Nodes 1-5:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         20 HDDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         2 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       Nodes 6-10:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         5 NVMes
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         20 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       You can use the 'target' key in the layout to target specific nodes.
       &salt; target notation helps to keep things simple:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
 host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
      <para>
       followed by
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
 host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
     </example>
     <example>
      <title>Expert Setup</title>
      <para>
       All previous cases assumed that the WALs and DBs use the same device. It
       is however possible to deploy the WAL on a dedicated device as well:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         20 HDDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         2 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         2 NVMes
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Samsung
          </para>
         </listitem>
         <listitem>
          <para>
           Model: NVME-QQQQ-987
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 256&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
     </example>
     <example>
      <title>Complex (and Unlikely) Setup</title>
      <para>
       In the following setup, we are trying to define:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         20 HDDs backed by 1 NVMe
        </para>
       </listitem>
       <listitem>
        <para>
         2 HDDs backed by 1 SSD(db) and 1 NVMe (wal)
        </para>
       </listitem>
       <listitem>
        <para>
         8 SSDs backed by 1 NVMe
        </para>
       </listitem>
       <listitem>
        <para>
         2 SSDs stand-alone (encrypted)
        </para>
       </listitem>
       <listitem>
        <para>
         1 HDD is spare and should not be deployed
        </para>
       </listitem>
      </itemizedlist>
      <para>
       The summary of used drives is as follows:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         23 HDDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Intel
          </para>
         </listitem>
         <listitem>
          <para>
           Model: SSD-123-foo
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 4&nbsp;TB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         10 SSDs
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Micron
          </para>
         </listitem>
         <listitem>
          <para>
           Model: MC-55-44-ZX
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 512&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
       <listitem>
        <para>
         1 NVMe
        </para>
        <itemizedlist>
         <listitem>
          <para>
           Vendor: Samsung
          </para>
         </listitem>
         <listitem>
          <para>
           Model: NVME-QQQQ-987
          </para>
         </listitem>
         <listitem>
          <para>
           Size: 256&nbsp;GB
          </para>
         </listitem>
        </itemizedlist>
       </listitem>
      </itemizedlist>
      <para>
       The &drvgrps; definition will be the following:
      </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
 host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
 </screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
 host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
 </screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
 host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
 </screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
 host_pattern: '*'
data_devices:
  model: SSD-123-foo
encryption: True
 </screen>
      <para>
       One HDD will remain as the file is being parsed from top to bottom.
      </para>
     </example>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-apply-cluster-specs">
    <title>Applying Cluster Specification</title>
    <para>
     After you have created a full <filename>cluster.yml</filename> file with
     specifications of all services and their placement, you can apply the
     cluster by running the following command:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yml</screen>
    <para>
     To view the status of the cluster, run the <command>ceph orch
     status</command> command. For more details, see
     <xref linkend="deploy-cephadm-day2-orch-status"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-services">
   <title>Deploying Services to Nodes</title>
   <para>
    After the basic cluster is running, you can deploy &ceph; services to
    additional nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-service-mon">
    <title>Deploy &mon;s and &mgr;s</title>
    <para>
     &ceph; cluster has three or five MONs deployed across different nodes. If
     there are five or more nodes in the cluster, we recommend deploying five
     MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
    </para>
    <important>
     <title>Include Bootstrap MON</title>
     <para>
      When deploying MONs and MGRs, remember to include the first MON that you
      added when configuring the basic cluster in
      <xref linkend="deploy-cephadm-configure-mon"/>.
     </para>
    </important>
    <para>
     To deploy MONs, apply the following specification:
    </para>
<screen>
service_type: mon
placement:
 hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <para>
     Similarly, to deploy MGRs, apply the following specification:
    </para>
<screen>
service_type: mgr
placement:
 hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <tip>
     <para>
      If MONs or MGRs are <emphasis>not</emphasis> on the same subnet, you need
      to append the subnet addresses. For example:
     </para>
<screen>
service_type: mon
placement:
 hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-osd">
    <title>Deploy &osd;s</title>
    <important>
     <title>When Storage Device is Available</title>
     <para>
      A storage device is considered <emphasis>available</emphasis> if all of
      the following conditions are met:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The device has no partitions.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not have any LVM state.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is not be mounted.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a file system.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a &bluestore; OSD.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is larger than 5&nbsp;GB.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If the above conditions are not met, &ceph; refuses to provision such
      OSDs.
     </para>
    </important>
    <para>
     There are two ways you can deploy OSDs:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Tell &ceph; to consume all available and unused storage devices:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
     </listitem>
     <listitem>
      <para>
       Use &drvgrps; (see <xref linkend="drive-groups"/>) to create OSD
       specification describing devices that will be deployed based on their
       properties, such as device type (SSD or HDD), device model names, size,
       or the nodes on which the devices exist. Then apply the specification by
       running the following command:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-mds">
    <title>Deploy &mds;s</title>
    <para>
     &cephfs; requires one or more &mds; (MDS) services. These are
     automatically deployed when you create the &cephfs;. To create a &cephfs;,
     first create MDS servers by applying the following specification:
    </para>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
  - ses-min3
 </screen>
    <para>
     After MDSs are functional, create the &cephfs;:
    </para>
<screen>&prompt.cephuser;ceph fs volume create <replaceable>CEPHFS_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-ogw">
    <title>Deploy &ogw;s</title>
    <para>
     &cephadm; deploys an &ogw; as a collection of daemons that manage a
     particular <emphasis>realm</emphasis> and <emphasis>zone</emphasis> (refer
     to <xref linkend="ceph-rgw-fed"/> for more details).
    </para>
    <procedure>
     <step>
      <para>
       If a realm has not been created yet, create it:
      </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
     </step>
     <step>
      <para>
       Create a new &zgroup;:
      </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup create --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>  \
 --master --default</screen>
     </step>
     <step>
      <para>
       Create a zone:
      </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> --master --default
</screen>
     </step>
     <step>
      <para>
       Finally, apply the following specification to deploy a set of &ogw;
       daemons for a particular realm and zone:
      </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-nfs">
    <title>Deploy &ganesha;</title>
    <para>
     &cephadm; deploys &ganesha; using a pre-defined &rados; pool and an
     optional namespace. To deploy &ganesha;, apply the following
     specification:
    </para>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
 hosts:
  - ses-min1
  - ses-min2
 spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
    <para>
     Replace
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NFS</replaceable> with an arbitrary string that
       identifies the NFS export.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_POOL</replaceable> with the name of the pool where
       the &ganesha; RADOS configuration object will be stored.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NAMESPACE</replaceable> (optional) with the desired
       NFS namespace (for example, <literal>nfs</literal>). For more
       information on NFS namespaces, see
       <link xlink:href="https://docs.ceph.com/docs/master/radosgw/nfs/#namespace-conventions
"/>
<!-- 2020-06-10 tbazant: possibly import that upstream section into
       Admin Guide? -->
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade">
<!-- https://docs.ceph.com/docs/master/cephadm/upgrade/ -->
   <title>Upgrading &ceph;</title>
   <para>
    You can instruct &cephadm; to upgrade &ceph; from one bugfix release to
    another. The automated upgrade of &ceph; services respects the recommended
    order&mdash;it starts with &mgr;s, &mon;s, and then continues on other
    services such as &osd;s, &mds;s, and &ogw;s. Each daemon is restarted only
    after &ceph; indicates that the cluster will remain available.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-start">
    <title>Starting the Upgrade</title>
    <para>
     Before you start the upgrade, verify that all nodes are currently online
     and your cluster is healthy:
    </para>
<screen>&prompt.cephuser;cephadm shell -- ceph -s</screen>
    <para>
     To upgrade (or downgrade) to a specific &ceph; release:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version <replaceable>VERSION</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version 15.2.1</screen>
    <para>
     If the new container is available, but the &ceph; version remains the
     same, execute the following:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start <replaceable>REGISTRY_URL</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start registry.suse.de/devel/storage/7.0/containers/ses/7/ceph/ceph</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-monitor">
    <title>Monitoring the Upgrade</title>
    <para>
     Run the following command to determine whether an upgrade is in progress:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade status</screen>
    <para>
     While the upgrade is in progress, you will see a progress bar in the
     &ceph; status output:
    </para>
<screen>&prompt.cephuser;ceph -s
[...]
  progress:
    Upgrade to docker.io/ceph/ceph:v15.2.1 (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
    <para>
     You can also watch the &cephadm; log:
    </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-stop">
    <title>Cancelling an Upgrade</title>
    <para>
     You can stop the upgrade process at any time:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade stop</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
