<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-cephadm">
 <title>Deploying with &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  To deploy a &ceph; cluster by using &cephadm;, you need to complete the
  following tasks:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Install and do basic configuration of the underlying operating
    system&mdash;&cephos;&mdash;on all cluster nodes.
   </para>
  </listitem>
  <listitem>
   <para>
    Deploy the &salt; infrastructure over all cluster nodes so that you can
    orchestrate them effectively.
   </para>
  </listitem>
  <listitem>
   <para>
    Configure the basic properties of the cluster and deploy it.
   </para>
  </listitem>
  <listitem>
   <para>
    Add new nodes to the cluster and deploy services to them.
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="deploy-os">
  <title>Install and Configure &sls;</title>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on each cluster node. Include the following
     modules:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Basesystem Module
      </para>
     </listitem>
     <listitem>
      <para>
       Server Applications Module
      </para>
     </listitem>
     <listitem>
      <para>
       Containers Module
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Find more details on how to install &sls; in
     <link xlink:href="https://documentation.suse.com/sles/15-SP22/html/SLES-all/cha-install.html"/>.
    </para>
   </step>
   <step>
    <para>
     Install the <emphasis>&productname; &productnumber;</emphasis> extension
     on each cluster node.
    </para>
    <tip>
     <title>Install &productname; Together with &sls;</title>
     <para>
      You can either install the &productname; &productnumber; extension
      separately after you have installed &cephos;, or you can add it during
      the &cephos; installation procedure.
     </para>
    </tip>
    <para>
     Find more details on how to install extensions in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html"/>.
    </para>
   </step>
   <step>
    <para>
     Configure network settings including proper DNS name resolution on each
     node. For more information on configuring a network, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#sec-network-yast"/>
     For more information on configuring a DNS server, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-dns"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-salt">
  <title>Deploy &salt;</title>

  <para>
   &productname; uses &salt; as a cluster orchestrator. &salt; helps you
   configure and run commands on multiple cluster nodes simultaneously from one
   dedicated host called the <emphasis>&smaster;</emphasis>. Before deploying
   &salt;, consider the following important points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by a dedicated
     node called &smaster;. &sminion;s have roles, for example &osd;, &mon;,
     &mgr;, &rgw;, &igw;, or &ganesha;.
    </para>
   </listitem>
   <listitem>
    <para>
     A &smaster; runs its own &sminion;. It is required for running privileged
     tasks&mdash;for example creating, authorizing, and copying keys to
     minions&mdash;so that remote minions never need to run privileged tasks.
    </para>
    <tip>
     <title>Sharing Multiple Roles per Server</title>
     <para>
      You will get the best performance from your &ceph; cluster when each role
      is deployed on a separate node. But real deployments sometimes require
      sharing one node for multiple roles. To avoid trouble with performance
      and the upgrade procedure, do not deploy the &osd;, &mds;, or &mon; role
      to the &adm;.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name, but you can specify any other network-reachable host name in the
     <filename>/etc/salt/minion</filename> file.
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Install the <literal>salt-master</literal> and
     <literal>salt-minion</literal> packages on the &smaster; node:
    </para>
<screen>&prompt.smaster;zypper in salt-master salt-minion</screen>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     If you intend to use the firewall, verify that the &smaster; node has
     ports 4505 and 4506 open to all &sminion; nodes. If the ports are closed,
     you can open them using the <command>yast2 firewall</command> command by
     allowing the <guimenu>SaltStack</guimenu> service.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Make sure that the <emphasis>fully qualified domain name</emphasis> of
     each node can be resolved to an IP address on the public cluster network
     by all the other nodes.
    </para>
   </step>
   <step>
    <para>
     Configure all minions (including the master minion) to connect to the
     master. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all related &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started on all nodes. Enable and start it if needed:
    </para>
<screen>&prompt.root;systemctl enable salt-minion.service
&prompt.root;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verify each &sminion;'s fingerprint and accept all salt keys on the
     &smaster; if the fingerprints match.
    </para>
    <note>
     <para>
      If the &sminion; fingerprint comes back empty, make sure the &sminion;
      has a &smaster; configuration and that it can communicate with the
      &smaster;.
     </para>
    </note>
    <para>
     View each minion's fingerprint:
    </para>
<screen>&prompt.smaster;salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     After gathering fingerprints of all the &sminion;s, list fingerprints of
     all unaccepted minion keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     If the minions' fingerprints match, accept them:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Test whether all &sminion;s respond:
    </para>
<screen>&prompt.smaster;salt '*' test.ping</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day1">
  <title>Deploy Basic Cluster (Day 1)</title>

  <para>
   This section guides you through the process of deploying a basic &ceph;
   cluster. Read the following subsections carefully and execute the included
   commands in the given order.
  </para>

  <sect2 xml:id="deploy-cephadm-cephsalt">
   <title>Install &cephsalt;</title>
   <para>
    &cephsalt; provides tools for deploying &ceph; clusters managed by
    &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
    management&mdash;for example, software updates or time
    synchronization&mdash;and defining roles for &sminion;s.
   </para>
   <para>
    On the &smaster;, install the <package>ceph-salt</package> package:
   </para>
<screen>&prompt.smaster;zypper install ceph-salt</screen>
   <para>
    The above command installed <package>ceph-salt-formula</package> as a
    dependency which modified the &smaster; configuration by inserting
    additional files in the <filename>/etc/salt/master.d</filename> directory.
    To apply the changes, restart
    <systemitem class="daemon">salt-master.service</systemitem> and synchronize
    &salt; modules:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure">
   <title>Configure Cluster Properties</title>
   <para>
    Use the <command>ceph-salt config</command> command to configure the basic
    properties of the cluster.
   </para>
   <sect3 xml:id="deploy-cephadm-configure-shell">
    <title>&cephsalt; Shell</title>
    <para>
     If you run <command>ceph-salt config</command> without any path or
     subcommand, you will enter an interactive &cephsalt; shell. The shell is
     convenient if you need to configure multiple properties in one batch and
     do not want type the full command syntax.
    </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  o- cephadm_bootstrap ......................................... [enabled]
  | o- ceph_conf ................................................... [...]
  | o- dashboard ................................................... [...]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  o- containers .................................................... [...]
  | o- images ...................................................... [...]
  |   o- ceph ............................................ [no image path]
  | o- registries ................................................ [empty]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- system_update ................................................. [...]
  | o- packages ................................................ [enabled]
  | o- reboot .................................................. [enabled]
  o- time_server ............................................... [enabled]
    o- external_servers .......................................... [empty]
    o- server_hostname ......................................... [not set]
    o- subnet .................................................. [not set]
</screen>
    <para>
     As you can see from the output of &cephsalt;'s <command>ls</command>
     command, the cluster configuration is organized in a tree structure. To
     configure a specific property of the cluster in the &cephsalt; shell, you
     have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Change to the path whose property you need to configure and run the
       command:
      </para>
<screen>
<prompt>/></prompt> cd /ceph_cluster/minions/
<prompt>/ceph_cluster/minions></prompt> ls
o- minions .................................................. [Minions: 5]
  o- ses-master.example.com ................................... [no roles]
  o- ses-min1.example.com ..................................... [no roles]
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Run the command from the current position and enter the absolute path to
       the property as the first argument:
      </para>
<screen>
<prompt>/></prompt> /ceph_cluster/minions/ ls
o- minions .................................................. [Minions: 5]
  o- ses-master.example.com ................................... [no roles]
  o- ses-min1.example.com ..................................... [no roles]
[...]
</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Autocompletion of Configuration Snippets</title>
     <para>
      While in a &cephsalt; shell, you can use the autocompletion feature
      similar to a normal Linux shell (Bash) autocompletion. It completes
      configuration paths, subcommands, or &sminion; names. When autocompleting
      a configuration path, you have two options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To let the shell finish a path relative to your current position, hit
        the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
      <listitem>
       <para>
        To let the shell finish an absolute path, enter <keycap>/</keycap> and
        hit the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
     </itemizedlist>
    </tip>
    <tip>
     <title>Navigating with the Cursor Keys</title>
     <para>
      If you enter <command>cd</command> from the &cephsalt; shell without any
      path, the command will print a tree structure of the cluster
      configuration with the line of the current path active. You can use the
      up and down cursor keys to navigate through individual lines. After you
      confirm with <keycap function="enter"></keycap>, the configuration path
      will change to the last active one.
     </para>
    </tip>
    <important>
     <title>Convention</title>
     <para>
      To keep the documentation consistent, we will use a single command syntax
      without entering the &cephsalt; shell. For example, you can list the
      cluster configuration tree by using the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-minions">
    <title>Add &sminion;s</title>
    <para>
     Include all or a subset of &sminion;s that we deployed and accepted in
     <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
     either specify the &sminion;s by their full names, or use a glob
     expressions '*' and '?' to include multiple &sminion;s at once. Use the
     <command>add</command> subcommand under the
     <literal>/ceph_cluster/minions</literal> path. The following command
     includes all accepted &sminion;s:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
    <para>
     Verify that the specified &sminion;s were added:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-admin">
    <title>Specify &adm;</title>
    <para>
     The &adm; is the node where the <filename>ceph.conf</filename>
     configuration file and the &ceph; admin keyring is installed. You usually
     run &ceph; related commands on the &adm;.
    </para>
    <tip>
     <title>&smaster; and &adm; on the Same Node</title>
     <para>
      In a homogeneous environment where all or most hosts belong to
      &productname;, we recommend having the &adm; on the same host as the
      &smaster;.
     </para>
     <para>
      In a heterogeneous environment where one &salt; infrastructure hosts more
      than one cluster, for example, &productname; together with &susemgr;, do
      <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
     </para>
    </tip>
    <para>
     To specify the &adm;, run the following command:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ............................ [no other roles]
</screen>
    <tip>
     <title>Install <filename>ceph.conf</filename> and the Admin Keyring on Multiple Nodes</title>
     <para>
      You can install the &ceph; configuration file and admin keyring on
      multiple nodes if you deployment requires it. For security reasons, avoid
      installing them on all the cluster's nodes.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-mon">
    <title>Specify First MON/MGR Node</title>
    <para>
     You need to specify which of the cluster's &sminion;s will bootstrap the
     cluster. This minion will become the first one running &mon; and &mgr;
     services.
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
    <important>
     <para>
      The minion that will bootstrap the cluster needs to have the admin
      keyring as well:
     </para>
<screen>
&smaster;ceph-salt config /ceph_cluster/roles/admin add ses-min1.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ............................ [no other roles]
  o- ses-min1.suse.cz .......................... [other roles: bootstrap]
</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ssh">
    <title>Generate SSH Key Pair</title>
    <para>
     &cephadm; uses the SSH protocol to communicate with cluster nodes. You
     need to generate the private and public part of the SSH key pair:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh ................................................... [Key Pair set]
  o- private_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ....... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ntp">
    <title>Configure Time Server</title>
    <para>
     Select one of the &sminion;s to be a time server for the rest of the
     cluster, and configure it to synchronize its time with a reliable time
     source(s). Verify that the time synchronization service is enabled on each
     cluster node on system start-up.
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/server_hostname set ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add 0.pt.pool.ntp.org
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- 0.pt.pool.ntp.org .......................................... [...]
  o- server_hostname ........................... [ses-master.example.com]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
    <para>
     Find more information on setting up time synchronization in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
    </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configure Path to Container Images</title>
    <para>
     &cephadm; needs to know a valid URI path to container images that will be
     used during the deployment step. Verify whether the default path is set:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph ls</screen>
    <para>
     If there is no default path set or your deployment requires a specific
     path, add it as follows:
    </para>
<screen>&prompt.smaster;ceph-salt config /containers/images/ceph set registry.suse.com/ses/7/ceph/ceph</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-verify">
    <title>Verify Cluster Configuration</title>
    <para>
     The minimal cluster configuration is finished. Inspect it for obvious
     errors:
    </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / .................................................................. [...]
  o- ceph_cluster ..................................................... [...]
  | o- minions ................................................. [Minions: 5]
  | | o- ses-master.example.com ..................................... [admin]
  | | o- ses-min1.example.com ................................... [bootstrap]
  | | o- ses-min2.example.com .................................... [no roles]
  | | o- ses-min3.example.com .................................... [no roles]
  | | o- ses-min4.example.com .................................... [no roles]
  | o- roles .......................................................... [...]
  |   o- admin ................................................. [Minions: 1]
  |   | o- ses-master.example.com .......................... [no other roles]
  |   o- bootstrap ................................... [ses-min1.example.com]
  o- cephadm_bootstrap ............................................ [enabled]
  | o- ceph_conf ...................................................... [...]
  | o- dashboard ...................................................... [...]
  |   o- password ...................................... [randomly generated]
  |   o- username ................................................... [admin]
  o- containers ....................................................... [...]
  | o- images ......................................................... [...]
  |   o- ceph ........................... [registry.suse.com/ses/7/ceph/ceph]
  | o- registries ................................................... [empty]
  o- ssh ..................................................... [Key Pair set]
  | o- private_key ........ [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ......... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- system_update .................................................... [...]
  | o- packages ................................................... [enabled]
  | o- reboot ..................................................... [enabled]
  o- time_server .................................................. [enabled]
    o- external_servers ................................................. [1]
    | o- 0.pt.pool.ntp.org ............................................ [...]
    o- server_hostname ............................. [ses-master.example.com]
    o- subnet ................................................ [10.20.6.0/24]
</screen>
    <tip>
     <title>Status of Cluster Configuration</title>
     <para>
      You can check if the configuration of the cluster is valid by running the
      following command:
     </para>
<screen>
&prompt.smaster;ceph-salt status
hosts:  0/5 deployed
config: OK
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-export">
    <title>Export Cluster Configuration</title>
    <para>
     After you have configured the basic cluster and its configuration is
     valid, it is a good idea to export its configuration to a file:
    </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
    <para>
     In case you break the cluster configuration and need to revert to a backup
     state, run:
    </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-deploy">
   <title>Deploy Cluster</title>
   <para>
    Deploy the previously configured minimal &ceph; cluster by running the
    following command:
   </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   <para>
    The above command will open an interactive user interface that shows the
    current progress of each minion.
   </para>
   <figure>
    <title>Deployment of Minimal Cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <title>Non-interactive Mode</title>
    <para>
     If you need to apply the configuration from a script, there is also a
     non-interactive mode:
    </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2">
  <title>Further Deployment and Administration (Day 2)</title>

  <para>
   After you have deployed the basic &ceph; cluster, you need to deploy core
   services to more cluster nodes. To make the cluster data accessible to
   clients, deploy additional services.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch">
<!-- https://docs.ceph.com/docs/master/mgr/orchestrator/ -->
   <title>The <command>ceph orch</command> Command</title>
   <para>
    The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
    an interface to the &cephadm; module&mdash;will take care of listing
    cluster components and deploying &ceph; services on new cluster nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-orch-status">
    <title>Displaying the Orchestrator Status</title>
    <para>
     The following command shows the current mode and status of the &ceph;
     orchestrator.
    </para>
<screen>&prompt.cephuser;ceph orch status</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-orch-list">
    <title>Listing Devices, Services, and Daemons</title>
    <para>
     To run <command>ceph-volume</command> on all nodes and list all disk
     devices, run:
    </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST        PATH      TYPE   SIZE  DEVICE  AVAIL  REJECT REASONS
ses-master  /dev/vda  hdd   16.0G          False  locked
ses-min1    /dev/vdb  hdd   20.0G          True
ses-min1    /dev/vda  hdd   16.0G          False  locked
ses-min2    /dev/vdb  hdd   20.0G          True
[...]
</screen>
    <tip>
     <title>Services and Daemons</title>
     <para>
      <emphasis>Service</emphasis> is a general term for a &ceph; service of a
      specific type, for example &mgr;.
     </para>
     <para>
      <emphasis>Daemon</emphasis> is a specific instance of a service, for
      example a process <literal>mgr.ses-min1.gdlcik</literal> running on a
      node called <literal>ses-min1</literal>.
     </para>
    </tip>
    <para>
     To list all services known to &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
    <tip>
     <para>
      You can limit the list to services on a particular node with the optional
      <option>–host</option> parameter and services of a particular type with
      the optional <option>–type</option> parameter (accepts
      <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>,
      <literal>mds</literal>, and <literal>rgw</literal>).
     </para>
    </tip>
    <para>
     To list all running daemons deployed by &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
    <tip>
     <para>
      To query the status of a particular daemon, use
      <option>--daemon_type</option> and <option>--daemon_id</option>. For
      OSDs, the ID is the numeric OSD ID. For MDS, the ID is the file system
      name:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-addnode">
   <title>Adding a New Node</title>
   <para>
    To add a new node to a &ceph; cluster, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Install &sls; and &productname; on the new node. Refer to
      <xref linkend="deploy-os"/> for more information.
     </para>
    </step>
    <step>
     <para>
      Configure the node as a &sminion; of an already existing &smaster;. Refer
      to <xref linkend="deploy-salt"/> for more information.
     </para>
    </step>
    <step>
     <para>
      Add the new minion to &cephsalt;, for example:
     </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions add ses-min5.example.com
&prompt.smaster;ceph-salt deploy ses-min5.example.com
</screen>
     <para>
      Refer to <xref linkend="deploy-cephadm-configure-minions"/> for more
      information.
     </para>
    </step>
    <step>
     <para>
      Verify that the node was added:
     </para>
<screen>
&prompt.cephuser;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-services">
   <title>Deploying Services to Nodes</title>
   <para>
    After you have added a node to the &cephsalt; environment as described in
    <xref linkend="deploy-cephadm-day2-addnode"/>, you can deploy &ceph;
    service(s) to it.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-service-mon">
    <title>Deploy &mon;s and &mgr;s</title>
    <para>
     &ceph; cluster has three or five MONs deployed across different nodes. If
     there are five or more nodes in the cluster, we recommend deploying five
     MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
    </para>
    <important>
     <title>Include Bootstrap MON</title>
     <para>
      When deploying MONs and MGRs, remember to include the first MON that you
      added when configuring the basic cluster in
      <xref linkend="deploy-cephadm-configure-mon"/>.
     </para>
    </important>
    <para>
     To deploy new MONs, run:
    </para>
<screen>&prompt.cephuser;ceph orch apply mon <replaceable>HOST_SPECIFICATION</replaceable></screen>
    <para>
     To deploy new MGRs, run:
    </para>
<screen>&prompt.cephuser;ceph orch apply mgr <replaceable>HOST_SPECIFICATION</replaceable></screen>
    <para>
     Depending on whether all MONs or MGRs are on the same subnet, you need to
     adjust the <replaceable>HOST_SPECIFICATION</replaceable>:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If all MONs or MGRs are on the same subnet, specify the short host names
       of the MONs or MGRs you need to add:
      </para>
<screen>&prompt.cephuser;ceph orch apply mon ses-min1 ses-min2 ses-min3</screen>
      <para>
       and
      </para>
<screen>&prompt.cephuser;ceph orch apply mgr ses-min1 ses-min2 ses-min3</screen>
     </listitem>
     <listitem>
      <para>
       If MONs and MGRs are on different subnets, you need to replace the
       <replaceable>HOST_SPECIFICATION</replaceable> with the following
       pattern:
      </para>
<screen><replaceable>SHORT_HOST_NAME</replaceable>:<replaceable>MON_SUBNET_IP</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&prompt.cephuser;ceph orch apply mon ses-min1:10.1.2.0/24 ses-min2:10.1.5.0/24</screen>
      <para>
       and
      </para>
<screen>&prompt.cephuser;ceph orch apply mgr ses-min1:10.1.2.0/24 ses-min2:10.1.5.0/24</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Labels for Nodes of a Specific Service</title>
     <para>
      You can mark the nodes that will be used as MONs or MGRs with the
      <literal>mon</literal> or <literal>mgr</literal> label, for example:
     </para>
<screen>
&prompt.cephuser;ceph orch host label add ses-min1 mon
&prompt.cephuser;ceph orch host label add ses-min2 mon
&prompt.cephuser;ceph orch host label add ses-min3 mon
</screen>
<screen>
&prompt.cephuser;ceph orch host label add ses-min1 mgr
&prompt.cephuser;ceph orch host label add ses-min2 mgr
&prompt.cephuser;ceph orch host label add ses-min3 mgr
</screen>
     <para>
      Check the hosts and their labels:
     </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST      ADDR   LABELS  STATUS
ses-min1         mon,mgr
ses-min2         mon,mgr
ses-min3         mon,mgr
ses-min4
</screen>
     <para>
      Then do the actual deployment:
     </para>
<screen>&prompt.cephuser;ceph orch apply mon</screen>
     <para>
      and
     </para>
<screen>&prompt.cephuser;ceph orch apply mgr</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupgrade">
<!-- https://docs.ceph.com/docs/master/cephadm/upgrade/ -->
   <title>Upgrading &ceph;</title>
   <para>
    You can instruct &cephadm; to upgrade &ceph; from one bugfix release to
    another. The automated upgrade of &ceph; services respects the recommended
    order&mdash;it starts with &mgr;s, &mon;s, and then continues on other
    services such as &osd;s, &mds;s, and &ogw;s. Each daemon is restarted only
    after &ceph; indicates that the cluster will remain available.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-start">
    <title>Starting the Upgrade</title>
    <para>
     Before you start the upgrade, verify that all nodes are currently online
     and your cluster is healthy:
    </para>
<screen>&prompt.cephuser;ceph -s</screen>
    <para>
     To upgrade (or downgrade) to a specific &ceph; release:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version <replaceable>VERSION</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade start --ceph-version 15.2.1</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-monitor">
    <title>Monitoring the Upgrade</title>
    <para>
     Run the following command to determine whether an upgrade is in progress:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade status</screen>
    <para>
     While the upgrade is in progress, you will see a progress bar in the
     &ceph; status output:
    </para>
<screen>&prompt.cephuser;ceph -s
[...]
  progress:
    Upgrade to docker.io/ceph/ceph:v15.2.1 (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
    <para>
     You can also watch the &cephadm; log:
    </para>
<screen>&prompt.cephuser;ceph -W cephadm</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-cephupgrade-stop">
    <title>Cancelling an Upgrade</title>
    <para>
     You can stop the upgrade process at any time:
    </para>
<screen>&prompt.cephuser;ceph orch upgrade stop</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
