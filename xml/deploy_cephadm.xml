<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-cephadm">
 <title>Deploying with &cephadm;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &productname; &productnumber; uses the &salt;-based &cephsalt; tool to
  prepare the operating system on each participating cluster node for
  deployment via &cephadm;. &cephadm; deploys and manages a &ceph; cluster by
  connecting to hosts from the &mgr; daemon via SSH. &cephadm; manages the full
  lifecycle of a &ceph; cluster. It starts by bootstrapping a tiny cluster on a
  single node (one MON and MGR service) and then uses the orchestration
  interface to expand the cluster to include all hosts and to provision all
  &ceph; services. You can perform this via the &ceph; command line interface
  (CLI) or partially via &dashboard; (GUI).
 </para>
 <para>
  To deploy a &ceph; cluster by using &cephadm;, you need to complete the
  following tasks:
 </para>
 <orderedlist>
  <listitem>
   <para>
    Install and do basic configuration of the underlying operating
    system&mdash;&cephos;&mdash;on all cluster nodes.
   </para>
  </listitem>
  <listitem>
   <para>
    Deploy the &salt; infrastructure on all cluster nodes for performing the
    initial deployment preparations via &cephsalt;.
   </para>
  </listitem>
  <listitem>
   <para>
    Configure the basic properties of the cluster via &cephsalt; and deploy it.
   </para>
  </listitem>
  <listitem>
   <para>
    Add new nodes and roles to the cluster and deploy services to them using
    &cephadm;.
   </para>
  </listitem>
 </orderedlist>
 <sect1 xml:id="deploy-os">
  <title>Install and Configure &sls;</title>

  <procedure>
   <step>
    <para>
     Install and register &cephos; on each cluster node. Include at least the
     following modules:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Basesystem Module
      </para>
     </listitem>
     <listitem>
      <para>
       Server Applications Module
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Find more details on how to install &sls; in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-install.html"/>.
    </para>
   </step>
   <step>
    <para>
     Install the <emphasis>&productname; &productnumber;</emphasis> extension
     on each cluster node.
    </para>
    <tip>
     <title>Install &productname; Together with &sls;</title>
     <para>
      You can either install the &productname; &productnumber; extension
      separately after you have installed &cephos;, or you can add it during
      the &cephos; installation procedure.
     </para>
    </tip>
    <para>
     Find more details on how to install extensions in
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-register-sle.html"/>.
    </para>
   </step>
   <step>
    <para>
     Configure network settings including proper DNS name resolution on each
     node. For more information on configuring a network, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#sec-network-yast"/>
     For more information on configuring a DNS server, see
     <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-dns"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-salt">
  <title>Deploy &salt;</title>

  <para>
   &productname; uses &salt; and &cephsalt; for the initial cluster
   preparation. &salt; helps you configure and run commands on multiple cluster
   nodes simultaneously from one dedicated host called the
   <emphasis>&smaster;</emphasis>. Before deploying &salt;, consider the
   following important points:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by a dedicated
     node called &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     If the &smaster; host should be part of the &ceph; cluster, it needs to
     run its own &sminion;, but this is not a requirement.
    </para>
    <tip>
     <title>Sharing Multiple Roles per Server</title>
     <para>
      You will get the best performance from your &ceph; cluster when each role
      is deployed on a separate node. But real deployments sometimes require
      sharing one node for multiple roles. To avoid trouble with performance
      and the upgrade procedure, do not deploy the &osd;, &mds;, or &mon; role
      to the &adm;.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name, but you can specify any other network-reachable host name in the
     <filename>/etc/salt/minion</filename> file.
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Install the <literal>salt-master</literal> on the &smaster; node:
    </para>
<screen>&prompt.smaster;zypper in salt-master</screen>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     If you intend to use the firewall, verify that the &smaster; node has
     ports 4505 and 4506 open to all &sminion; nodes. If the ports are closed,
     you can open them using the <command>yast2 firewall</command> command by
     allowing the <guimenu>salt-master</guimenu> service for the appropriate
     zone. For example, <literal>public</literal>.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Make sure that the <emphasis>fully qualified domain name</emphasis> of
     each node can be resolved to an IP address on the public cluster network
     by all the other nodes.
    </para>
   </step>
   <step>
    <para>
     Configure all minions to connect to the master. If your &smaster; is not
     reachable by the host name <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all related &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started on all nodes. Enable and start it if needed:
    </para>
<screen>&prompt.root;systemctl enable salt-minion.service
&prompt.root;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verify each &sminion;'s fingerprint and accept all salt keys on the
     &smaster; if the fingerprints match.
    </para>
    <note>
     <para>
      If the &sminion; fingerprint comes back empty, make sure the &sminion;
      has a &smaster; configuration and that it can communicate with the
      &smaster;.
     </para>
    </note>
    <para>
     View each minion's fingerprint:
    </para>
<screen>&prompt.sminion;salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     After gathering fingerprints of all the &sminion;s, list fingerprints of
     all unaccepted minion keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     If the minions' fingerprints match, accept them:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Test whether all &sminion;s respond:
    </para>
<screen>&prompt.smaster;salt-run manage.status</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day1">
  <title>Deploy Basic Cluster (Day 1)</title>

  <para>
   This section guides you through the process of deploying a basic &ceph;
   cluster. Read the following subsections carefully and execute the included
   commands in the given order.
  </para>

  <sect2 xml:id="deploy-cephadm-cephsalt">
   <title>Install &cephsalt;</title>
   <para>
    &cephsalt; provides tools for deploying &ceph; clusters managed by
    &cephadm;. &cephsalt; uses the &salt; infrastructure to perform OS
    management&mdash;for example, software updates or time
    synchronization&mdash;and defining roles for &sminion;s.
   </para>
   <para>
    On the &smaster;, install the <package>ceph-salt</package> package:
   </para>
<screen>&prompt.smaster;zypper install ceph-salt</screen>
   <para>
    The above command installed <package>ceph-salt-formula</package> as a
    dependency which modified the &smaster; configuration by inserting
    additional files in the <filename>/etc/salt/master.d</filename> directory.
    To apply the changes, restart
    <systemitem class="daemon">salt-master.service</systemitem> and synchronize
    &salt; modules:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure">
   <title>Configure Cluster Properties</title>
   <para>
    Use the <command>ceph-salt config</command> command to configure the basic
    properties of the cluster.
   </para>
   <sect3 xml:id="deploy-cephadm-configure-shell">
    <title>&cephsalt; Shell</title>
    <para>
     If you run <command>ceph-salt config</command> without any path or
     subcommand, you will enter an interactive &cephsalt; shell. The shell is
     convenient if you need to configure multiple properties in one batch and
     do not want type the full command syntax.
    </para>
<screen>
&prompt.smaster;ceph-salt config
<prompt>/></prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- server_hostname ......................................... [not set]
    o- subnet .................................................. [not set]
</screen>
    <para>
     As you can see from the output of &cephsalt;'s <command>ls</command>
     command, the cluster configuration is organized in a tree structure. To
     configure a specific property of the cluster in the &cephsalt; shell, you
     have two options:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Run the command from the current position and enter the absolute path to
       the property as the first argument:
      </para>
<screen>
<prompt>/></prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/> /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
     </listitem>
     <listitem>
      <para>
       Change to the path whose property you need to configure and run the
       command:
      </para>
<screen>
<prompt>/></prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions></prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Autocompletion of Configuration Snippets</title>
     <para>
      While in a &cephsalt; shell, you can use the autocompletion feature
      similar to a normal Linux shell (Bash) autocompletion. It completes
      configuration paths, subcommands, or &sminion; names. When autocompleting
      a configuration path, you have two options:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        To let the shell finish a path relative to your current position, hit
        the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
      <listitem>
       <para>
        To let the shell finish an absolute path, enter <keycap>/</keycap> and
        hit the TAB key <keycap function="tab"></keycap> twice.
       </para>
      </listitem>
     </itemizedlist>
    </tip>
    <tip>
     <title>Navigating with the Cursor Keys</title>
     <para>
      If you enter <command>cd</command> from the &cephsalt; shell without any
      path, the command will print a tree structure of the cluster
      configuration with the line of the current path active. You can use the
      up and down cursor keys to navigate through individual lines. After you
      confirm with <keycap function="enter"></keycap>, the configuration path
      will change to the last active one.
     </para>
    </tip>
    <important>
     <title>Convention</title>
     <para>
      To keep the documentation consistent, we will use a single command syntax
      without entering the &cephsalt; shell. For example, you can list the
      cluster configuration tree by using the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
    </important>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-minions">
    <title>Add &sminion;s</title>
    <para>
     Include all or a subset of &sminion;s that we deployed and accepted in
     <xref linkend="deploy-salt"/> to the &ceph; cluster configuration. You can
     either specify the &sminion;s by their full names, or use a glob
     expressions '*' and '?' to include multiple &sminion;s at once. Use the
     <command>add</command> subcommand under the
     <literal>/ceph_cluster/minions</literal> path. The following command
     includes all accepted &sminion;s:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/minions add '*'</screen>
    <para>
     Verify that the specified &sminion;s were added:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-cephadm">
    <title>Specify &sminion;s Managed by &cephadm;</title>
    <para>
     Specify which nodes will belong to the &ceph; cluster and will be managed
     by &cephadm;. Include all nodes that will run &ceph; services as well as
     the &adm;:
    </para>
<screen>&prompt.smaster;ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-admin">
    <title>Specify &adm;</title>
    <para>
     The &adm; is the node where the <filename>ceph.conf</filename>
     configuration file and the &ceph; admin keyring is installed. You usually
     run &ceph; related commands on the &adm;.
    </para>
    <tip>
     <title>&smaster; and &adm; on the Same Node</title>
     <para>
      In a homogeneous environment where all or most hosts belong to
      &productname;, we recommend having the &adm; on the same host as the
      &smaster;.
     </para>
     <para>
      In a heterogeneous environment where one &salt; infrastructure hosts more
      than one cluster, for example, &productname; together with &susemgr;, do
      <emphasis>not</emphasis> place the &adm; on the same host as &smaster;.
     </para>
    </tip>
    <para>
     To specify the &adm;, run the following command:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
    <tip>
     <title>Install <filename>ceph.conf</filename> and the Admin Keyring on Multiple Nodes</title>
     <para>
      You can install the &ceph; configuration file and admin keyring on
      multiple nodes if your deployment requires it. For security reasons,
      avoid installing them on all the cluster's nodes.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-mon">
    <title>Specify First MON/MGR Node</title>
    <para>
     You need to specify which of the cluster's &sminion;s will bootstrap the
     cluster. This minion will become the first one running &mon; and &mgr;
     services.
    </para>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-tuned-profiles">
    <title>Specify Tuned Profiles</title>
    <para>
     You need to specify which of the cluster's minions have actively tuned
     profiles. To do so, add these roles explicitly with the following
     commands:
    </para>
    <note>
     <para>
      One minion cannot have both the <literal>latency</literal> and
      <literal>throughput</literal> roles.
     </para>
    </note>
<screen>
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
&prompt.smaster;ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ssh">
    <title>Generate SSH Key Pair</title>
    <para>
     &cephadm; uses the SSH protocol to communicate with cluster nodes. A user
     account named <literal>cephadm</literal> is automatically created and used
     for SSH communication.
    </para>
    <para>
     You need to generate the private and public part of the SSH key pair:
    </para>
<screen>
&prompt.smaster;ceph-salt config /ssh generate
Key pair generated.
&prompt.smaster;ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-ntp">
    <title>Configure Time Server</title>
    <para>
     All cluster nodes need to have their time synchronized with a reliable
     time source. There are several scenarios to approach time synchronization:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If all cluster nodes are already configured to synchronize their time
       using a NTP service of choice, disable time server handling completely:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server disable
</screen>
     </listitem>
     <listitem>
      <para>
       If your site already has a single source of time, specify the host name
       of the time source:
      </para>
<screen>
 &prompt.smaster;ceph-salt config /time_server/server_hostname set <replaceable>time-server.example.com</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Alternatively, &cephsalt; has the ability to configure one of the
       &sminion; to serve as the time server for the rest of the cluster. This
       is sometimes referred to as an "internal time server". In this scenario,
       &cephsalt; will configure the internal time server (which should be one
       of the &sminion;) to sync its time with an external time server, such as
       <literal>pool.ntp.org</literal>, and configure all the other minions to
       get their time from the internal time server. This can be achieved as
       follows:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/server_hostname set ses-master.example.com
&prompt.smaster;ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
      <para>
       The <option>/time_server/subnet</option> option specifies the subnet
       from which NTP clients are allowed to access the NTP server. It is
       automatically set when you specify
       <option>/time_server/server_hostname</option>. If you need to change it
       or specify it manually, run:
      </para>
<screen>
&prompt.smaster;ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
     </listitem>
    </itemizedlist>
    <para>
     Check the time server settings:
    </para>
<screen>
&prompt.smaster;ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- server_hostname ........................... [ses-master.example.com]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
    <para>
     Find more information on setting up time synchronization in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
    </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-dashboardlogin">
    <title>Configure &dashboard; Login Credentials</title>
    <para>
     &dashboard; will be available after the basic cluster is deployed. To
     access it, you need to set a valid user name and password, for example:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/username set admin
&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
    <tip>
     <title>Forcing Password Update</title>
     <para>
      By default, the first dashboard user will be forced to change their
      password on first login to the dashboard. To disable this feature, run
      the following command:
     </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configure Path to Container Images</title>
    <para>
     &cephadm; needs to know a valid URI path to container images that will be
     used during the deployment step. Verify whether the default path is set:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
    <para>
     If there is no default path set or your deployment requires a specific
     path, add it as follows:
    </para>
<screen>&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7/ceph/ceph</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-registry">
    <title>Configure Container Registry</title>
    <para>
     Optionally, you can set a local container registry. This will serve as a
     mirror of the <literal>registry.suse.com</literal> registry. Remember you
     need to re-sync the local registry whenever there are new updated
     containers available from
     <systemitem class="systemname">registry.suse.com</systemitem>. Find more
     information about creating a container registry in
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-registry-installation.html"/>
    </para>
    <para>
     Creating a local registry is useful in the following scenarios:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       You have a lot of cluster nodes and want to save download time and
       bandwidth by creating a local mirror of container images.
      </para>
     </listitem>
     <listitem>
      <para>
       Your cluster has no access to the online registry (an air-gapped
       deployment) and you need a local mirror to pull the container images
       from.
      </para>
     </listitem>
     <listitem>
      <para>
       If configuration or network issues prevent your cluster from accessing
       remote registries across a secure link, so you need a local, unencrypted
       registry instead.
      </para>
     </listitem>
    </itemizedlist>
    <tip>
     <title>Registry Cache</title>
     <para>
      To avoid re-syncing the local registry when new updated containers
      appear, you can configure a <emphasis>registry cache</emphasis>. Find
      more information about containers and air-gapped scenarios in
      <link
       xlink:href="https://documentation.suse.com/suse-caasp/4.5/single-html/caasp-airgap/"/>.
     </para>
    </tip>
    <tip>
     <title>Container Tools</title>
     <para>
      The following procedure uses <command>podman</command> to create a
      container registry and <command>skopeo</command> to mirror container
      images. To install <package>podman</package>, you need to add the
      <literal>Containers Module</literal> extension. For more information,
      refer to their manual pages <command>man 1 podman</command> and
      <command>man 1 skopeo</command>.
     </para>
    </tip>
    <procedure>
     <step>
      <para>
       Install <command>podman</command> and <command>skopeo</command>:
      </para>
<screen>&prompt.smaster;zypper install podman skopeo</screen>
     </step>
    </procedure>
    <para>
     To configure a local container registry, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Create a local container registry accessible but outside of the &ceph;
       cluster, for example:
      </para>
<screen>
&prompt.root;podman run -d --restart=always --net=host --name registry -p 5000:5000 registry:2
</screen>
     </step>
     <step>
      <para>
       Mirror SES &productnumber; related container images to the local
       registry:
      </para>
<screen>
&prompt.root;skopeo copy \
 --dest-tls-verify=false \
 docker://registry.suse.com/ses/7/ceph/ceph \
 docker://<replaceable>LOCAL_REGISTRY_HOST_IP</replaceable>:5000/registry.suse.com/ses/7/ceph/ceph
</screen>
     </step>
     <step>
      <para>
       On the &smaster;, add the local repository to the &cephsalt;
       configuration:
      </para>
<screen>
&prompt.smaster;ceph-salt config /containers/registries_conf/registries \
 add prefix=registry.suse.com \
 location=<replaceable>LOCAL_REGISTRY_HOST_IP</replaceable>:5000/registry.suse.com
</screen>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="deploy-cephadm-enable-network">
    <title>Configure the Cluster Network</title>
    <para>
     Optionally, if you are running a separate cluster network, you may need to
     set the cluster network IP address followed by the subnet mask part after
     the slash sign, for example <literal>192.168.10.22/24</literal>.
    </para>
    <para>
     Run the following commands to enable <literal>cluster_network</literal>:
    </para>
<screen>
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf add global
&prompt.smaster;ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-verify">
    <title>Verify Cluster Configuration</title>
    <para>
     The minimal cluster configuration is finished. Inspect it for obvious
     errors:
    </para>
<screen>
&prompt.smaster;ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path ............... [registry.suse.com/ses/7/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ..................................................... [None]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- server_hostname .......................... [ses-master.example.com]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
    <tip>
     <title>Status of Cluster Configuration</title>
     <para>
      You can check if the configuration of the cluster is valid by running the
      following command:
     </para>
<screen>
&prompt.smaster;ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-export">
    <title>Export Cluster Configuration</title>
    <para>
     After you have configured the basic cluster and its configuration is
     valid, it is a good idea to export its configuration to a file:
    </para>
<screen>&prompt.smaster;ceph-salt export > cluster.json</screen>
    <warning>
     <para>
      The output of the <command>ceph-salt export</command> includes the SSH
      private key. If you are concerned about the security implications, do not
      execute this command without taking appropriate precautions.
     </para>
    </warning>
    <para>
     In case you break the cluster configuration and need to revert to a backup
     state, run:
    </para>
<screen>&prompt.smaster;ceph-salt import cluster.json</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-deploy">
   <title>Update Nodes and Deploy Cluster</title>
   <para>
    Before you deploy the cluster, update all software packages on all nodes:
   </para>
<screen>&prompt.smaster;ceph-salt update</screen>
   <para>
    If a node reports <literal>Reboot is needed</literal> during the update,
    important OS packages&mdash;such as the kernel&mdash;were updated to a
    newer version and you need to reboot the node to apply the changes.
   </para>
   <para>
    To reboot all nodes that require rebooting, either append the
    <option>--reboot</option> option
   </para>
<screen>&prompt.smaster;ceph-salt update --reboot</screen>
   <para>
    or reboot them in a separate step:
   </para>
<screen>&prompt.smaster;ceph-salt reboot</screen>
   <important>
    <para>
     The &smaster; is never rebooted by <command>ceph-salt update
     --reboot</command> or <command>ceph-salt reboot</command> commands. If the
     &smaster; needs rebooting, you need to reboot it manually.
    </para>
   </important>
   <para>
    After the nodes are updated, deploy the minimal cluster:
   </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   <para>
    The above command will open an interactive user interface that shows the
    current progress of each minion.
   </para>
   <figure>
    <title>Deployment of Minimal Cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="cephadm_deploy.png" width="75%"/>
     </imageobject>
    </mediaobject>
   </figure>
   <tip>
    <title>Non-interactive Mode</title>
    <para>
     If you need to apply the configuration from a script, there is also a
     non-interactive mode of deployment. This is also useful when deploying the
     cluster from a remote machine because constant updating of the progress
     information on the screen over the network may become distracting:
    </para>
<screen>&prompt.smaster;ceph-salt apply --non-interactive</screen>
   </tip>
  </sect2>

  <sect2 xml:id="day2-deployment">
   <title>Further Deployment (Day 2)</title>
   <para>
    After you have deployed the basic &ceph; cluster, you need to deploy core
    services to more cluster nodes. To make the cluster data accessible to
    clients, deploy additional services as well.
   </para>
   <para>
    Currently, we support deployment of &ceph; services on the command line by
    using the &ceph; orchestrator (<command>ceph orch</command> subcommands).
   </para>
<!-- 2020-09-15 tbazant: dashboard deployment not ready for SES7
   <para>
    There are two ways to deploy additional &ceph; services:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      By using the &dashboard;'s graphical Web UI. Find more details in
      <xref linkend="deploy-dashboard-day2"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      By using the <command>ceph orch</command> subcommands on the command
      line. Find more details in <xref linkend="deploy-cephadm-day2"/>.
     </para>
    </listitem>
   </itemizedlist>
-->
  </sect2>
 </sect1>
<!-- 2020-09-15 tbazant: dashboard deployment not ready for SES7
 <sect1 xml:id="deploy-dashboard-day2">
  <title>Further Deployment (Day 2) Using the &dashboard;</title>

  <para>
   ADDME
  </para>
 </sect1>
-->
 <sect1 xml:id="deploy-cephadm-day2">
  <title>Further Deployment (Day 2) Using the Command Line</title>

  <sect2 xml:id="deploy-cephadm-day2-orch">
<!-- https://docs.ceph.com/docs/master/mgr/orchestrator/ -->
   <title>The <command>ceph orch</command> Command</title>
   <para>
    The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
    an interface to the &cephadm; module&mdash;will take care of listing
    cluster components and deploying &ceph; services on new cluster nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-orch-status">
    <title>Displaying the Orchestrator Status</title>
    <para>
     The following command shows the current mode and status of the &ceph;
     orchestrator.
    </para>
<screen>&prompt.cephuser;ceph orch status</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-orch-list">
    <title>Listing Devices, Services, and Daemons</title>
    <para>
     To run <command>ceph-volume</command> on all nodes and list all disk
     devices, run:
    </para>
<screen>
&prompt.cephuser;ceph orch device ls
HOST        PATH      TYPE   SIZE  DEVICE  AVAIL  REJECT REASONS
ses-master  /dev/vda  hdd   16.0G          False  locked
ses-min1    /dev/vdb  hdd   20.0G          True
ses-min1    /dev/vda  hdd   16.0G          False  locked
ses-min2    /dev/vdb  hdd   20.0G          True
[...]
</screen>
    <tip>
     <title>Services and Daemons</title>
     <para>
      <emphasis>Service</emphasis> is a general term for a &ceph; service of a
      specific type, for example &mgr;.
     </para>
     <para>
      <emphasis>Daemon</emphasis> is a specific instance of a service, for
      example a process <literal>mgr.ses-min1.gdlcik</literal> running on a
      node called <literal>ses-min1</literal>.
     </para>
    </tip>
    <para>
     To list all services known to &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
    <tip>
     <para>
      You can limit the list to services on a particular node with the optional
      <option>-â€“host</option> parameter and services of a particular type
      with the optional <option>--service-type</option> parameter (acceptable
      types are <literal>mon</literal>, <literal>osd</literal>,
      <literal>mgr</literal>, <literal>mds</literal>, and
      <literal>rgw</literal>).
     </para>
    </tip>
    <para>
     To list all running daemons deployed by &cephadm;, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
    <tip>
     <para>
      To query the status of a particular daemon, use
      <option>--daemon_type</option> and <option>--daemon_id</option>. For
      OSDs, the ID is the numeric OSD ID. For MDS, the ID is the file system
      name:
     </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="cephadm-service-and-placement-specs">
   <title>Service and Placement Specification</title>
   <para>
    The recommended way to specify the deployment of &ceph; services is to
    create a YAML-formatted file with the specification of the services that
    you intend to deploy.
   </para>
   <para>
    You can create a separate specification file for each type of service, for
    example:
   </para>
<screen>
&prompt.smaster;cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Alternatively, you can specify multiple (or all) service types in one
    file&mdash;for example, <filename>cluster.yml</filename>&mdash;that
    describes which nodes will run specific services. Remember to separate
    individual service types with three dashes (<literal>---</literal>):
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    The aforementioned properties have the following meaning:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       The type of the service. It can be either a &ceph; service
       (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>,
       <literal>crash</literal>, <literal>osd</literal>, or
       <literal>rbd-mirror</literal>), a gateway (<literal>nfs</literal> or
       <literal>rgw</literal>), or part of the monitoring stack
       (<literal>alertmanager</literal>, <literal>grafana</literal>,
       <literal>node-exporter</literal>, or <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       The name of the service. Specifications of type <literal>mon</literal>,
       <literal>mgr</literal>, <literal>alertmanager</literal>,
       <literal>grafana</literal>, <literal>node-exporter</literal>, and
       <literal>prometheus</literal> do not require the
       <literal>service_id</literal> property.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Specifies which nodes will be running the service. Refer to
       <xref linkend="cephadm-placement-specs"/> for more details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Additional specification relevant for the service type.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Applying Specific Services</title>
    <para>
     &ceph; cluster services have usually a number of properties specific to
     them. For examples and details of individual services' specification,
     refer to <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
   <sect3 xml:id="cephadm-placement-specs">
    <title>Placement Specification</title>
    <para>
     To deploy &ceph; services, &cephadm; needs to know on which nodes to
     deploy them. Use the <literal>placement</literal> property and list the
     short host names of the nodes that the service applies to:
    </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
   </sect3>
   <sect3 xml:id="cephadm-apply-cluster-specs">
    <title>Applying Cluster Specification</title>
    <para>
     After you have created a full <filename>cluster.yml</filename> file with
     specifications of all services and their placement, you can apply the
     cluster by running the following command:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yml</screen>
    <para>
     To view the status of the cluster, run the <command>ceph orch
     status</command> command. For more details, see
     <xref linkend="deploy-cephadm-day2-orch-status"/>.
    </para>
   </sect3>
   <sect3 xml:id="cephadm-apply-cluster-specs-">
    <title>Exporting the Specification of a Running Cluster</title>
    <para>
     Although you deployed services to the &ceph; cluster by using the
     specification files as described in
     <xref
      linkend="cephadm-service-and-placement-specs" />, the
     configuration of the cluster may diverge from the original specification
     during its operation. Also, you may have removed the specification files
     accidentally.
    </para>
    <para>
     To retrieve a complete specification of a running cluster, run:
    </para>
<screen>
&prompt.cephuser;ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
    <tip>
     <para>
      You can append the <option>--format</option> option to change the default
      <literal>yaml</literal> output format. You can choose from
      <literal>json</literal>, <literal>json-pretty</literal>, or
      <literal>yaml</literal>. For example:
     </para>
<screen>ceph orch ls --export --format json</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-services">
   <title>Deploying Services to Nodes</title>
   <para>
    After the basic cluster is running, you can deploy &ceph; services to
    additional nodes.
   </para>
   <sect3 xml:id="deploy-cephadm-day2-service-mon">
    <title>Deploy &mon;s and &mgr;s</title>
    <para>
     &ceph; cluster has three or five MONs deployed across different nodes. If
     there are five or more nodes in the cluster, we recommend deploying five
     MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
    </para>
    <important>
     <title>Include Bootstrap MON</title>
     <para>
      When deploying MONs and MGRs, remember to include the first MON that you
      added when configuring the basic cluster in
      <xref linkend="deploy-cephadm-configure-mon"/>.
     </para>
    </important>
    <para>
     To deploy MONs, apply the following specification:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <note>
     <para>
      If you need to add another node, append the host name to the same YAML
      list. For example:
     </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
    </note>
    <para>
     Similarly, to deploy MGRs, apply the following specification:
    </para>
    <important>
     <para>
      Ensure your deployment has at least three &mgr;s in each deployment.
     </para>
    </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <tip>
     <para>
      If MONs or MGRs are <emphasis>not</emphasis> on the same subnet, you need
      to append the subnet addresses. For example:
     </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-osd">
    <title>Deploy &osd;s</title>
    <important>
     <title>When Storage Device is Available</title>
     <para>
      A storage device is considered <emphasis>available</emphasis> if all of
      the following conditions are met:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        The device has no partitions.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not have any LVM state.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is not be mounted.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a file system.
       </para>
      </listitem>
      <listitem>
       <para>
        The device does not contain a &bluestore; OSD.
       </para>
      </listitem>
      <listitem>
       <para>
        The device is larger than 5&nbsp;GB.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If the above conditions are not met, &ceph; refuses to provision such
      OSDs.
     </para>
    </important>
    <para>
     There are two ways you can deploy OSDs:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Tell &ceph; to consume all available and unused storage devices:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
     </listitem>
     <listitem>
      <para>
       Use &drvgrps; (see <xref linkend="drive-groups"/>) to create OSD
       specification describing devices that will be deployed based on their
       properties, such as device type (SSD or HDD), device model names, size,
       or the nodes on which the devices exist. Then apply the specification by
       running the following command:
      </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-mds">
    <title>Deploy &mds;s</title>
    <para>
     &cephfs; requires one or more &mds; (MDS) services. These are
     automatically deployed when you create the &cephfs;. To create a &cephfs;,
     first create MDS servers by applying the following specification:
    </para>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    <para>
     After MDSs are functional, create the &cephfs;:
    </para>
<screen>&prompt.cephuser;ceph fs volume create <replaceable>CEPHFS_NAME</replaceable> <replaceable>PLACEMENT_STRING</replaceable></screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-ogw">
    <title>Deploy &ogw;s</title>
    <para>
     &cephadm; deploys an &ogw; as a collection of daemons that manage a
     particular <emphasis>realm</emphasis> and <emphasis>zone</emphasis>.
    </para>
    <para>
     You can either relate an &ogw; service to already existing realm and zone,
     (refer to <xref linkend="ceph-rgw-fed"/> for more details), or you you can
     specify a non-existing <replaceable>REALM_NAME</replaceable> and
     <replaceable>ZONE_NAME</replaceable> and they will be created
     automatically after you apply the following configuration:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-igw">
    <title>Deploy &igw;s</title>
    <para>
      &cephadm; deploys an &igw; which is a storage area
      network (SAN) protocol that allows clients (called
      initiators) to send SCSI commands to SCSI storage devices
      (targets) on remote servers.
    </para>
    <para>
      Apply the following configuration to deploy. To use the &dashboard;
      to manage the &igw;, ensure <literal>trusted_ip_list</literal>
      contains the IP addresses of all &igw; and &mgr; nodes (see the
      example output below.
    </para>
    <note>
      <para>
        Ensure the pool is created before applying the following specification.
      </para>
    </note>
<screen>
service_type: iscsi
service_id:
placement:
  count: 1
spec:
  pool: iscsi-pool
  api_user: iscsi-user
  api_password: password
  trusted_ip_list: "<replaceable>IP_ADDRESS</replaceable>, <replaceable>IP_ADDRESS</replaceable>"
</screen>
     <para>
       Placeholder for content on which IP addresses to apply
     </para>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-nfs">
    <title>Deploy &ganesha;</title>
    <para>
     &cephadm; deploys &ganesha; using a pre-defined &rados; pool and an
     optional namespace. To deploy &ganesha;, apply the following
     specification:
    </para>
<screen>
    service_type: nfs
    service_id: <replaceable>EXAMPLE_NFS</replaceable>
    placement:
      hosts:
      - ses-min1
      - ses-min2
    spec:
      pool: <replaceable>EXAMPLE_POOL</replaceable>
      namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NFS</replaceable> with an arbitrary string that
       identifies the NFS export.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_POOL</replaceable> with the name of the pool where
       the &ganesha; RADOS configuration object will be stored.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>EXAMPLE_NAMESPACE</replaceable> (optional) with the desired
       &ogw; NFS namespace (for example, <literal>nfs</literal>).
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="deploy-cephadm-day2-service-monitoring">
    <title>Deploying Monitoring Stack</title>
    <para>
     The monitoring stack consists of &prometheus;, &prometheus; exporters,
     &prometheus; &alertmanager;, and &grafana;. &dashboard; makes use of these
     components to store and visualize detailed metrics on cluster utilization
     and performance.
    </para>
    <para>
     To deploy the monitoring stack, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Enable the <literal>prometheus</literal> module in the &mgr; daemon.
       This exposes the internal &ceph; metrics so that &prometheus; can read
       them:
      </para>
<screen>&prompt.cephuser;ceph mgr module enable prometheus</screen>
     </step>
     <step>
      <para>
       Create a specification file (for example
       <filename>monitoring.yaml</filename>) with a content similar to the
       following:
      </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
     </step>
     <step>
      <para>
       Apply monitoring services by running:
      </para>
<screen>&prompt.cephuser;ceph orch apply -i monitoring.yaml</screen>
      <para>
       It may take a minute or two for the monitoring services to be deployed.
      </para>
     </step>
    </procedure>
    <important>
     <para>
      &prometheus;, &grafana;, and the &dashboard; are all automatically
      configured to talk to each other, resulting in a fully functional
      &grafana; integration in the &dashboard; when deployed as described
      above.
     </para>
     <para>
      The only exception to this rule is monitoring with RBD images. See
      <xref linkend="monitoring-rbd-image"/> for more information.
     </para>
    </important>
   </sect3>
  </sect2>
 </sect1>
</chapter>
