<!-- ZAPPING OSD DISKS -->
<step xml:id="deploy-wiping-disk">
 <para>
  Prior to deploying &productname; &productnumber;, manually zero all the
  disks. Remember to replace 'X' with the correct disk letter:
 </para>
 <substeps>
  <step>
   <para>
    Stop all processes that are using the specific disk.
   </para>
  </step>
  <step>
   <para>
    Verify whether any partition on the disk is mounted, and unmount if
    needed.
   </para>
  </step>
  <step>
   <para>
    If the disk is managed by LVM, deactivate and delete the whole LVM
    infrastructure. Refer to
    <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-lvm.html"/>
    for more details.
   </para>
  </step>
  <step>
   <para>
    If the disk is part of an <literal>mdraid</literal>, deactivate the array. Refer to
    <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/part-software-raid.html"/>
    for more details.
   </para>
  </step>
  <step>
   <tip>
    <title>Rebooting the Server</title>
    <para>
     If you get error messages such as 'partition in use' or 'kernel cannot
     be updated with the new partition table' during the following steps,
     reboot the server.
    </para>
   </tip>
   <para>
    Wipe the beginning of each partition (as &rootuser;):
   </para>
<screen>for partition in /dev/sdX[0-9]*
do
dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
  </step>
  <step>
   <para>
    Wipe the beginning of the drive:
   </para>
<screen>
&prompt.root;dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
</screen>
  </step>
  <step>
   <para>
    Wipe the end of the drive:
   </para>
<screen>
&prompt.root;dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
</screen>
  </step>
  <step>
   <para>
    Verify that the drive is empty (with no GPT structures) using:
   </para>
<screen>
&prompt.root;parted -s /dev/sdX print free
</screen>
   <para>
    or
   </para>
<screen>
&prompt.root;dd if=/dev/sdX bs=512 count=34 | hexdump -C
&prompt.root;dd if=/dev/sdX bs=512 count=33 \
skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C
</screen>
  </step>
 </substeps>
</step>
<!-- / ZAPPING OSD DISKS -->
<!-- MARKING MINIONS -->
<screen>ceph-salt config /Ceph_Cluster/Minions add doc-ses-*suse.cz</screen>
<!-- / MARKING MINIONS -->

<!-- 2020-08-11 move vvvvvvvvvvvvvvvvvvvvvv to the upgrade chapter -->
  <sect2 xml:id="ceph-rgw-single-site-multi">
   <title>Migrating a Single Site System to Multi-Site</title>
   <para>
    To migrate from a single site system with a default &zgroup; and zone to a
    multi-site system, use the following steps:
   </para>
   <procedure>
    <step>
     <para>
      Create a realm. Replace <replaceable>NAME</replaceable> with the realm
      name.
     </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Create a system user. Replace <replaceable>USER-ID</replaceable> with the
      username. Replace <replaceable>DISPLAY-NAME</replaceable> with a display
      name. Only the display name may contain spaces.
     </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=<replaceable>USER-ID</replaceable> --display-name="<replaceable>DISPLAY-NAME</replaceable>"\
                            --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET-KEY</replaceable> --system</screen>
    </step>
    <step>
     <para>
      Rename the default zone and &zgroup;. Replace
      <replaceable>NAME</replaceable> with the &zgroup; or zone name.
     </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup rename --rgw-zonegroup default --zonegroup-new-name=<replaceable>NAME</replaceable>
&prompt.cephuser;radosgw-admin zone rename --rgw-zone default --zone-new-name us-east-1 --rgw-zonegroup=<replaceable>NAME</replaceable></screen>
    </step>
    <step>
     <para>
      Configure the master &zgroup;. Replace <replaceable>NAME</replaceable>
      with the realm or &zgroup; name. Replace <replaceable>FQDN</replaceable>
      with the fully qualified domain name(s) in the &zgroup;.
     </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup modify --rgw-realm=<replaceable>NAME</replaceable> --rgw-zonegroup=<replaceable>NAME</replaceable> --endpoints http://<replaceable>FQDN</replaceable>:80 --master --default</screen>
    </step>
    <step>
     <para>
      Configure the master zone. Replace <replaceable>NAME</replaceable> with
      the realm, &zgroup; or zone name. Replace <replaceable>FQDN</replaceable>
      with the fully qualified domain name(s) in the &zgroup;.
     </para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-realm=<replaceable>NAME</replaceable> --rgw-zonegroup=<replaceable>NAME</replaceable> \
                            --rgw-zone=<replaceable>NAME</replaceable> --endpoints http://<replaceable>FQDN</replaceable>:80 \
                            --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET-KEY</replaceable> \
                            --master --default</screen>
    </step>
    <step>
     <para>
      Commit the updated configuration.
     </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
    </step>
    <step>
     <para>
      Restart the &cogw;.
     </para>
<screen>&prompt.cephuser.ogw;systemctl restart ceph-radosgw@rgw.`hostname -s`</screen>
    </step>
   </procedure>
   <para>
    After completing this procedure, configure a secondary zone to create a
    secondary zone in the master &zgroup;.
   </para>
  </sect2>
<!-- 2020-08-11 move ^^^^^^^^^^^^^^^^^^^^^^^^^^^ to the upgrade chapter -->
