<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.monitoring">
 <title>Monitoring</title>
 <para/>
 <sect1 xml:id="storage.bp.monitoring.calamari_usage_graphs">
  <title>Usage Graphs on Calamari</title>

  <para>
   Calamari&mdash;a &ceph;'s Web front-end for managing and monitoring the
   cluster&mdash;includes several graphs on the cluster's usage.
  </para>

  <para>
   At the bottom of the <guimenu>Dashboard</guimenu>&mdash;the home page of
   Calamari&mdash;there are two usage related boxes. While
   <guimenu>IOPS</guimenu> shows the cluster's overall number of input/output
   operations per second, the <guimenu>Usage</guimenu> graph shows the number
   of the cluster's total/used disk space.
  </para>

  <figure>
   <title>IOPS and Usage Graphs on Calamari Dashboard</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_iops_usage.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_iops_usage.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   You can find more detailed and interactive graphs after clicking the
   <guimenu>Charts</guimenu> menu item. It shows the cluster's overall
   input/output operations per second and free disk space by default. Select
   <guimenu>Pool IOPS</guimenu> from the top drop-down box to detail the view
   by existing pools.
  </para>

  <figure>
   <title>Pool IOPS Detailed View</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_mypool_iops.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_mypool_iops.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   By moving the slider in the <guimenu>Time Axis</guimenu> pane, you can
   change the displayed time interval in the graph. By moving the mouse over
   the graph, the time/read/write information changes accordingly. By clicking
   and dragging the mouse horizontally across the graph, the specified time
   interval gets zoomed. You can see more help by moving the mouse over the
   little question mark in the top right corner of the graph.
  </para>

  <para>
   If you select the host name of a specific &ceph; server, Calamari displays
   detailed information about CPU, average load, and memory related to the
   specified host.
  </para>

  <figure>
   <title>&ceph; Host Average Load</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="calamari_host_avgload.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="calamari_host_avgload.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 
 <!-- Moved to the Admin guide
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Checking for Full OSDs</title>

  <para>
   &ceph; prevents you from writing to a full OSD so that you do not lose data.
   In an operational cluster, you should receive a warning when your cluster is
   getting near its full ratio. The <command>mon osd full ratio</command>
   defaults to 0.95, or 95% of capacity before it stops clients from writing
   data. The <command>mon osd nearfull ratio</command> defaults to 0.85, or 85%
   of capacity, when it generates a health warning.
  </para>

  <para>
   Full OSD nodes will be reported by <command>ceph health</command>:
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   or
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   The best way to deal with a full cluster is to add new OSD nodes allowing
   the cluster to redistribute data to the newly available storage.
  </para>

  <para>
   If you cannot start an OSD because it is full, you may delete some data by
   deleting some placement group directories in the full OSD.
  </para>

  <tip>
   <title>Preventing Full OSDs</title>
   <para>
    After an OSD becomes full&mdash;it uses 100% of its disk space&mdash;it
    will normally crash quickly without warning. Following are a few tips to
    remember when administering OSD nodes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Each OSD's disk space (usually mounted under
      <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) needs to be placed on
      a dedicated underlying disk or partition.
     </para>
    </listitem>
    <listitem>
     <para>
      Check the &ceph; configuration files and make sure that &ceph; does not
      store its log file to the disks/partitions dedicated for use by OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Make sure that no other process writes to the disks/partitions dedicated
      for use by OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 
 -->
 
 <sect1 xml:id="storage.bp.monitoring.osd">
  <title>Checking if OSD Daemons are Running on a Node</title>

  <para>
   To check the status of OSD services on a specific node, log in to the node,
   and run the following:
  </para>

<screen>sudo systemctl status ceph-osd*
 ceph-osd@0.service - Ceph object storage daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-osd@.service; enabled)
    Active: active (running) since Fri 2015-02-20 11:13:18 CET; 2 days ago
  Main PID: 1822 (ceph-osd)
    CGroup: /system.slice/system-ceph\x2dosd.slice/ceph-osd@0.service
            └─1822 /usr/bin/ceph-osd -f --cluster ceph --id 0</screen>

  <para>
   For more information, see <xref linkend="ceph.operating.services"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.mon">
  <title>Checking if Monitor Daemons are Running on a Node</title>

  <para>
   To check the status of monitor services on a specific node, log in to the
   node, and run the following:
  </para>

<screen>sudo systemctl status ceph-mon*
 ceph-mon@doc-ceph1.service - Ceph cluster monitor daemon
    Loaded: loaded (/usr/lib/systemd/system/ceph-mon@.service; enabled)
    Active: active (running) since Wed 2015-02-18 16:57:17 CET; 4 days ago
  Main PID: 1203 (ceph-mon)
    CGroup: /system.slice/system-ceph\x2dmon.slice/ceph-mon@doc-ceph1.service
            └─1203 /usr/bin/ceph-mon -f --cluster ceph --id doc-ceph1</screen>

  <para>
   For more information, see <xref linkend="ceph.operating.services"/>.
  </para>
 </sect1>
 <!-- Moved to FAQs
 <sect1 xml:id="storage.bp.monitoring.diskfails">
  <title>What Happens When a Disk Fails?</title>

  <para>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     The related OSD crashed and is automatically removed from the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     Then you should remove the disk from the cluster &crushmap;, and
     physically from the host hardware.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 
 <sect1 xml:id="storage.bp.monitoring.journalfails">
  <title>What Happens When a Journal Disk Fails?</title>

  <para>
   &ceph; OSDs use journaling file systems (see
   <link xlink:href="http://en.wikipedia.org/wiki/Journaling_file_system"/> for
   more information) to store data. When a disk dedicated to a journal fails,
   the related OSD(s) fail as well (see
   <xref linkend="storage.bp.monitoring.diskfails"/>).
  </para>

  <warning>
   <title>Hosting Multiple Journals on One Disk</title>
   <para>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </para>
  </warning>
 </sect1>
 -->
</chapter>
