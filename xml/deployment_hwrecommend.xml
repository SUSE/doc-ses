<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.hwreq">
 <title>Hardware Recommendations</title>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>Hardware Recommendations for the OSD Node</title>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Minimum Disk Size for an OSD Node</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal, and the space for the stored data. The minimum (and default)
    value for the journal is 6GB. The minimum space for data is 5GB as
    partitions smaller than 5GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11GB, we do not recommend
    a disk smaller than 20GB, even for testing purposes.
   </para>
  </sect2>

  <sect2 xml:id="ses.bp.numofdisks">
   <title>Maximum Count of Disks per OSD Node</title>
   <para>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> For optimum performance, reserve at least
      2GB of RAM per terabyte of disk space installed.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server between the other nodes in the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Using SSD for OSD Journals</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk. The <option>osd
    journal</option> configuration setting defaults to
    <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable>/journal</filename>.
    You can mount this path to an SSD or to an SSD partition so that it is not
    merely a file on the same disk as the object data.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively small space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than 6 journals on the same SSD disk.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ses.bp.ram">
   <title>Minimum RAM Size for an OSD Node</title>
   <para>
    The recommended minimum is 2GB per OSD. Note that during recovery, 1 or
    even 2GB of RAM per terabyte of OSD disk space is optimal.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.hwreq.replicas">
  <title>Reducing Data Replication</title>

  <para>
   &ceph; stores data within pools. Pools are logical groups for storing
   objects. Data objects within a pool are replicated so that they can be
   recovered when OSDs fail. New pools are created with the default of three
   replicas. This number includes the 'original' data object itself. Three
   replicas then mean the data object and two its copies for a total of three
   instances.
  </para>

  <para>
   You can manually change the number of pool replicas (see
   <xref linkend="ceph.pools.options.num_of_replicas"/>). Setting a pool to two
   replicas means that there is only <emphasis>one</emphasis> copy of the data
   object besides the object itself, so if you lose one object instance, you
   need to trust that the other copy has not been corrupted for example since
   the last
   <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">scrubbing</link>
   during recovery.
  </para>

  <para>
   Setting a pool to one replica means that there is exactly
   <emphasis>one</emphasis> instance of the data object in the pool. If the OSD
   fails, you lose the data. A possible usage for a pool with one replica is
   storing temporary data for a short time.
  </para>

  <para>
   Setting more than three replicas for a pool means only a small increase in
   reliability, but may be suitable in rare cases. Remember that the more
   replicas, the more disk space needed for storing the object copies. If you
   need the ultimate data security, we recommend using erasure coded pools. For
   more information, see <xref linkend="cha.ceph.erasure"/>.
  </para>

  <warning>
   <para>
    We strongly encourage you to either leave the number of replicas for a pool
    at the default value of 3, or use higher value if suitable. Setting the
    number of replicas to a smaller number is dangerous and may cause the loss
    of data stored in the cluster.
   </para>
  </warning>
 </sect1>
 <sect1 xml:id="storage.bp.hwreq.ec">
  <title>Reducing Redundancy Similar to RAID 6 Arrays</title>

  <para>
   When creating a new pool, &ceph; uses the replica type by default, which
   replicates objects across multiple disks to be able to recover from an OSD
   failure. While this type of pool is safe, it uses a lot of disk space to
   store objects.
  </para>

  <para>
   To reduce the disk space needed, &ceph; implements <emphasis>erasure
   coded</emphasis> pools. This method adds extra chunks of data to detect
   errors in a data stream. Erasure coded pools exhibit similar performance,
   reliability, and storage saved as RAID 6 arrays.
  </para>

  <para>
   As erasure coding is a complex topic, you need to study it properly to be
   able to deploy it for optimum performance. For more information, see
   <xref linkend="cha.ceph.erasure"/>.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run OSDs and monitor nodes on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance&mdash;the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another aspect is whether to share disks between an OSD, a monitor node, and
   the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>
</chapter>
