<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-bp-hwreq">
 <title>Hardware Requirements and Recommendations</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The hardware requirements of &ceph; are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </para>
 <para>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </para>
 <sect1 xml:id="multi-architecture">
  <title>Multiple Architecture Configurations</title>

  <para>
   &productname; supports both &x86; and &arm; architectures. When considering
   each architecture, it is important to note that from a cores per OSD,
   frequency, and RAM perspective, there is no real difference between CPU
   architectures for sizing.
  </para>
  <para>
   As with smaller &x86; processors (non-server), lower-performance
   &arm;-based cores may not provide an optimal experience, especially when
   used for erasure coded pools.
  </para>
  <note>
    <para>
      Throughout the documentation, <replaceable>SYSTEM-ARCH</replaceable>
      is used in place of &x86; or &arm;.
    </para>
  </note>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>Minimum Cluster Configuration</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least four OSD nodes, with eight OSD disks each, are required.
    </para>
   </listitem>
   <listitem>
    <para>
     Three &mon; nodes (requires SSD for dedicated OS drive).
    </para>
   </listitem>
   <listitem>
    <para>
     &igw;s, &rgw;s, and &mds;s require incremental 4 GB RAM and four cores.
    </para>
   </listitem>
   <listitem>
    <para>
     &mon;s, &rgw;s, and &mds;s' nodes require redundant deployment.
    </para>
   </listitem>
   <listitem>
    <para>
     Separate &adm; with 4 GB RAM, four cores, 1 TB capacity. This is typically
     the &smaster; node. &ceph; services and gateways, such as &mon;, &mgr;,
     &mds;, &osd;, &ogw;, or &ganesha; are not supported on the &adm;.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>&osn;s</title>

  <sect2 xml:id="sysreq-osd">
   <title>Minimum Requirements</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      CPU recommendations:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1x 2GHz CPU Thread per spinner
       </para>
      </listitem>
      <listitem>
       <para>
        2x 2GHz CPU Thread per SSD
       </para>
      </listitem>
      <listitem>
       <para>
        4x 2GHz CPU Thread per NVMe
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Separate 10 GbE networks (public/client and back-end), required 4x 10
      GbE, recommended 2x 25 GbE.
     </para>
    </listitem>
    <listitem>
     <para>
      Total RAM required = number of OSDs x (1 GB +
      <option>osd_memory_target</option>) + 16 GB
     </para>
     <para>
      Refer to <xref linkend="config-auto-cache-sizing"/> for more details on
      <option>osd_memory_target</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks in JBOD configurations or individual RAID-0 configurations.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD journal can reside on OSD disk.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks should be exclusively used by &productname;.
     </para>
    </listitem>
    <listitem>
     <para>
      Dedicated disk/SSD for the operating system, preferably in a RAID 1
      configuration.
     </para>
    </listitem>
    <listitem>
     <para>
      If this OSD host will host part of a cache pool used for cache tiering,
      allocate at least an additional 4 GB of RAM.
     </para>
    </listitem>
    <listitem>
     <para>
      &mon;s, gateway and &mds;s can reside on &osn;s.
     </para>
    </listitem>
    <listitem>
     <para>
      For disk performance reasons, we recommend using bare metal for OSD nodes
      and not virtual machines.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Minimum Disk Size</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal (for &filestore;) or WAL/DB device (for &bluestore;), and the
    primary space for the stored data. The minimum (and default) value for the
    journal/WAL/DB is 6 GB. The minimum space for data is 5 GB, as partitions
    smaller than 5 GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Recommended Size for the &bluestore;'s WAL and DB Device</title>
   <tip>
    <title>More Information</title>
    <para>
     Refer to <xref linkend="about-bluestore"/> for more information on
     &bluestore;.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      We recommend reserving 4 GB for the WAL device. The recommended size for
      DB is 64 GB for most workloads.
     </para>
    </listitem>
    <listitem>
     <para>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows &ceph; to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as &ceph; uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not used fully then
      its space is not wasted but used for DB operation.
     </para>
     <para>
      To share the DB device with the WAL, do <emphasis>not</emphasis> specify
      the WAL device, and specify only the DB device.
     </para>
     <para>
      Find more information about specifying an OSD layout in
      <xref linkend="ds-drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>Using SSD for OSD Journals</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively little space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than six journals on the same SSD disk and 12 on
     NVMe disks.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Maximum Recommended Number of Disks</title>
   <para>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> RAM above 2&nbsp;GB is used for the &bluestore;
      cache. With the default <option>osd_memory_target</option> of 4&nbsp;GB, the
      system has a reasonable starting cache size for spinning media. If using
      SSD or NVME, consider increasing the cache size and RAM allocation per
      OSD to maximize performance.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Monitor Nodes</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least three &mon; nodes are required. The number of monitors should
     always be odd (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB of RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processor with four logical cores.
    </para>
   </listitem>
   <listitem>
    <para>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <filename>/var/lib/ceph</filename> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </para>
   </listitem>
   <listitem>
    <para>
     There must only be one monitor process per node.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or &rgw; nodes is only supported if sufficient
     hardware resources are available. That means that the requirements for all
     services need to be added up.
    </para>
   </listitem>
   <listitem>
    <para>
     Two network interfaces bonded to multiple switches.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>&rgw; Nodes</title>

  <para>
   &rgw; nodes should have six to eight CPU cores and 32 GB of RAM (64 GB
   recommended). When other processes are co-located on the same machine, their
   requirements need to be added up.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>&mds; Nodes</title>

  <para>
   Proper sizing of the &mds; nodes depends on the specific use case.
   Generally, the more open files the &mds; is to handle, the more CPU and RAM
   it needs. The following are the minimum requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3&nbsp;GB of RAM for each &mds; daemon.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interface.
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU with at least 2 cores.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   At least 4 GB of RAM and a quad-core CPU are required. This includes
   running the &dashboard; on the &adm;. For large clusters with hundreds of
   nodes, 6 GB of RAM is suggested.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>iSCSI Nodes</title>

  <para>
   iSCSI nodes should have six to eight CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>Network Recommendations</title>

  <para>
   For the &ceph; cluster network environment, we recommend two bonded 25 GbE
   network interfaces. The network interfaces should be split into a public
   client part, and a trusted internal part using VLANs. Use the 802.3ad
   bonding mode to provide maximum bandwidth and resiliency. Find more details
   about configuring bonded network interfaces in
   <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-network.html#sec-network-iface-bonding"/>
  </para>

  <para>
   The public VLAN serves to provide the service to the customers, while the
   internal part provides for the authenticated &ceph; network communication.
   The main reason for this is that although &ceph; provides authentication and
   protection against attacks once secret keys are in place, the messages used
   to configure these keys may be transferred openly and are vulnerable.
  </para>

  <important>
   <title>Administration Network not Supported</title>
   <para>
    Additional administration network setup&mdash;that enables for example
    separating SSH, &salt;, or DNS networking&mdash;is neither tested nor
    supported.
   </para>
  </important>

  <tip>
   <title>Nodes Configured via DHCP</title>
   <para>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    &ceph; daemons start. If this happens, the &ceph; MONs and OSDs will not
    start correctly (running <command>systemctl status ceph\*</command> will
    result in "unable to bind" errors) To avoid this issue, we recommend
    increasing the DHCP client timeout to at least 30 seconds on each node in
    your storage cluster. This can be done by changing the following settings
    on each node:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename>, set
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename>, set
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>Adding a Private Network to a Running Cluster</title>
   <para>
    If you do not specify a cluster network during &ceph; deployment, it
    assumes a single public network environment. While &ceph; operates fine
    with a public network, its performance and security improves when you set a
    second private cluster network. To support two networks, each &ceph; node
    needs to have at least two network cards.
   </para>
   <para>
    You need to apply the following changes to each &ceph; node. It is
    relatively quick to do for a small cluster, but can be very time consuming
    if you have a cluster consisting of hundreds or thousands of nodes.
   </para>
   <procedure>
    <step>
     <para>
      Stop &ceph; related services on each cluster node.
     </para>
     <para>
      Add a line to <filename>/etc/ceph/ceph.conf</filename> to define the
      cluster network, for example:
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      If you need to specifically assign static IP addresses or override
      <option>cluster network</option> settings, you can do so with the
      optional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Check that the private cluster network works as expected on the OS level.
     </para>
    </step>
    <step>
     <para>
      Start &ceph; related services on each cluster node.
     </para>
<screen>&prompt.root;systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>Monitor Nodes on Different Subnets</title>
   <para>
    If the monitor nodes are on multiple subnets, for example they are located
    in different rooms and served by different switches, you need to adjust the
    <filename>ceph.conf</filename> file accordingly. For example, if the nodes
    have IP addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12, add the
    following lines to their <literal>global</literal> section:
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    Additionally, if you need to specify a per-monitor public address or
    network, you need to add a
    <literal>[mon.<replaceable>X</replaceable>]</literal> section for each
    monitor:
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Naming Limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run &ceph; OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance&mdash;the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another consideration is whether to share disks between an OSD, a monitor
   node, and the operating system on the server. The answer is simple: if
   possible, dedicate a separate disk to OSD, and a separate server to a
   monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>Recommended Production Cluster Configuration</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Seven &osn;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       No single node exceeds ~15% of total storage
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb Ethernet (four physical networks bonded to multiple switches)
      </para>
     </listitem>
     <listitem>
      <para>
       56+ OSDs per storage cluster
      </para>
     </listitem>
     <listitem>
      <para>
       RAID 1 OS disks for each OSD storage node
      </para>
     </listitem>
     <listitem>
      <para>
       SSDs for Journal with 6:1 ratio SSD journal to OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1.5 GB of RAM per TB of raw OSD capacity for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz per OSD for each &osn;
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Dedicated physical infrastructure nodes
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Three &mon; nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk
      </para>
     </listitem>
     <listitem>
      <para>
       One SES management node: 4 GB RAM, 4 core processor, RAID 1 SSDs for
       disk
      </para>
     </listitem>
     <listitem>
      <para>
       Redundant physical deployment of gateway or &mds; nodes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         &rgw; nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &igw; nodes: 16 GB RAM, 4 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &mds; nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
         RAID 1 SSDs for disk
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>&productname; &productnumber; and Other &suse; Products</title>

  <para>
   This section contains important information about integrating &productname;
   &productnumber; with other &suse; products.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>&susemgr;</title>
   <para>
    &susemgr; and &productname; are not integrated, therefore &susemgr; cannot
    currently manage a &productname; cluster.
   </para>
  </sect2>
 </sect1>
</chapter>
