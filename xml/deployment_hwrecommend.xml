<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.hwreq">
 <title>Hardware Requirements and Recommendations</title>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>&osn;s</title>
  <para>
   The hardware requirements of &ceph; are heavily dependent on the IO
   workload. The following hardware requirements and recommendations
   should be considered as a starting point for detailed planning.
  </para>
  <sect2 xml:id="sysreq.osd">
  <title>Minimal Requirements</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least 4 OSD nodes with 8 OSD disks each are required.
    </para>
   </listitem>
   <listitem>
    <para>
     1 GB RAM per TB raw OSD capacity for each OSD storage node,
     1.5 GB of RAM per each Terabyte of OSD are recommended.
     Note that during recovery, 1 or even 2GB of RAM per Terabyte of OSD
     disk space is optimal.
    </para>
   </listitem>
   <listitem>
    <para>
     1.5 GHz of a logical CPU core per OSD, 2 GHz recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     10 Gb Ethernet (2 networks bonded to multiple switches).
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks in JBOD configurations.
    </para>
   </listitem>
   <listitem>
    <para>
     OSD disks should be exclusively used by &storage;.
    </para>
   </listitem>
   <listitem>
    <para>
     Dedicated disk/SSD for the operating system, preferably in a RAID1
     configuration.
    </para>
   </listitem>
   <listitem>
    <para>
     Additional 4 GB of RAM if cache tiering is used.
    </para>
   </listitem>
  </itemizedlist>
 </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Minimum Disk Size</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal, and the space for the stored data. The minimum (and default)
    value for the journal is 6GB. The minimum space for data is 5GB as
    partitions smaller than 5GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11GB, we do not recommend
    a disk smaller than 20GB, even for testing purposes.
   </para>
  </sect2>



  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Using SSD for OSD Journals</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk. The <option>osd
    journal</option> configuration setting defaults to
    <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable>/journal</filename>.
    You can mount this path to an SSD or to an SSD partition so that it is not
    merely a file on the same disk as the object data.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively small space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than 6 journals on the same SSD disk.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>Maximum Recommended Count of Disks</title>

  <para>
   You can have as many disks in one server as it allows. There are a few
   things to consider when planning the number of disks per server:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>Network bandwidth.</emphasis> The more disks you have in a
     server, the more data must be transferred via the network card(s) for the
     disk write operations.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Memory.</emphasis> For optimum performance, reserve at least 2GB
     of RAM per Terabyte of disk space installed.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
     more disks it has, the more OSDs the cluster temporarily loses. Moreover,
     to keep the replication rules running, you need to copy all the data from
     the failed server between the other nodes in the cluster.
    </para>
   </listitem>
  </itemizedlist>
  </sect2>
 </sect1>

 <sect1 xml:id="sysreq.mon">
  <title>Monitor Nodes</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least 3 Monitor Nodes are recommended. The number of Monitors should
     always be odd (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB of RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processor with 4 logical cores.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD or fast hard disk in a RAID1 configuration
    </para>
   </listitem>
   <listitem>
    <para>
     On installations with fewer than seven nodes, these can be hosted on the
     system disk of the OSD nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     Nodes should be bare metal, not virtualized, for performance reasons.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or &rgw; nodes with the actual workload is not
     supported. No other load generating services other than OSDs, monitors or
     &rgw;s daemons are supported on the same host.
    </para>
   </listitem>
   <listitem>
    <para>
     Configurations may vary from, and frequently exceed, these recommendations
     depending on individual sizing and performance needs.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interfaces for redundancy.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sysreq.rgw">
  <title>&rgw; Nodes</title>
  <para>
   &rgw; nodes should have 6-8 CPU cores and 32 GB of RAM (64 GB recommended).
  </para>
 </sect1>

 <sect1 xml:id="sysreq.iscsi">
  <title>iSCSI Nodes</title>
  <para>
   iSCSI nodes should have 6-8 CPU cores and 16 GB of RAM.
  </para>
  </sect1>

  <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>Network Recommendations</title>

  <para>
   The network environment where you intend to run &ceph; should ideally be a
   bonded set of at least two network interfaces that is logically split into a
   public part and trusted internal part using VLANs. The bonding mode is
   recommended to be 802.3ad when possible to provide maximum bandwidth and
   resiliency.
  </para>

  <para>
   The public VLAN serves for providing the service to the customers, the
   internal part provides for the authenticated &ceph; network communication.
   The main reason is that although &ceph; authentication and protection
   against attacks once secret keys are in place, the messages used to
   configure these keys may be transferred open and are vulnerable.
  </para>

  <tip>
   <title>Nodes Configured via DHCP</title>
   <para>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    &ceph; daemons start. If this happens, the &ceph; MONs and OSDs will not
    start correctly (running <command>systemctl status ceph\*</command> will
    result in "unable to bind" errors) To avoid this issue, we recommend
    increasing the DHCP client timeout to at least 30 seconds on each node in
    your storage cluster. This can be done by changing the following settings on
    each node:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename> set
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename> set
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>
 <sect2 xml:id="storage.bp.net.private">
  <title>Adding a Private Network to a Running Cluster</title>

  <para>
   If you do not specify a cluster network during &ceph; deployment, it assumes
   a single public network environment. While &ceph; operates fine with a
   public network, its performance and security improves when you set a second
   private cluster network.
   To support two networks,
   each &ceph; node needs to have at least two network cards.
  </para>

  <para>
   You need to apply the following changes to each &ceph; node. It is
   comfortable for a small cluster, but can be very time demanding if you have
   a cluster consisting of hundreds or thousands of nodes.
  </para>

  <procedure>
   <step>
    <para>
     Stop &ceph; related services on each cluster node.
    </para>
    <para>
     Add a line to <filename>/etc/ceph/ceph.conf</filename> to define the cluster
     network, for example:
    </para>
    <screen>cluster network = 10.0.0.0/24</screen>
    <para>
      If you need to specifically assign static IP addresses or override
     <option>cluster network</option> settings, you can do so with the optional
     <option>cluster addr</option>.
    </para>
   </step>
   <step>
    <para>
     Check that the private cluster network works as expected on the OS level.
    </para>
   </step>
   <step>
    <para>
     Start &ceph; related services on each cluster node.
    </para>
<screen>sudo rcceph start</screen>
   </step>
  </procedure>
 </sect2>
 <sect2 xml:id="storage.bp.net.subnets">
  <title>Monitor Nodes on Different Subnets</title>
  <para>
   If the monitor nodes are not in the same subnet, you need to modify the
   ceph.conf in the current directory. For example if the nodes have IP
   addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12,
  </para>
 </sect2>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Naming Limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>

 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run &ceph; OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance&mdash;the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another aspect is whether to share disks between an OSD, a monitor node, and
   the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>

 <sect1 xml:id="ses.bp.minimum_cluster">
  <title>Minimum Cluster Configuration</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     4 &osn;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10 Gb Ethernet (2 networks bonded to multiple switches)
      </para>
     </listitem>
     <listitem>
      <para>
       32 OSDs per storage cluster
      </para>
     </listitem>
     <listitem>
      <para>
       OSD journal can reside on OSD disk
      </para>
     </listitem>
     <listitem>
      <para>
       Dedicated OS disk per &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       1 GB RAM per TB raw OSD capacity for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       1.5 GHz per OSD for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;s, gateway and &mds;s can reside on &osn;s
      </para>
      <itemizedlist>
       <listitem>
        <para>
         3 Monitor nodes (requires SSD for dedicated OS drive)
        </para>
       </listitem>
       <listitem>
        <para>
         &mon;s, &rgw;s and &mds;s nodes require redundant deployment
        </para>
       </listitem>
       <listitem>
        <para>
         &igw;s, &rgw;s and &mds;s require incremental 4 GB RAM and 4 Cores
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Separate management node with 4 GB RAM, 4 Cores, 1 TB capacity
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="ses.bp.production_cluster">
  <title>Recommended Production Cluster Configuration</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     7 &osn;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       No single node exceeds ~15% of total storage
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb Ethernet (4 physical networks bonded to multiple switches)
      </para>
     </listitem>
     <listitem>
      <para>
       56+ OSDs per storage cluster
      </para>
     </listitem>
     <listitem>
      <para>
       RAID 1 OS disks for each OSD storage node
      </para>
     </listitem>
     <listitem>
      <para>
       SSDs for Journal with 6:1 ratio SSD journal to OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1.5 GB RAM per TB raw OSD capacity for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz per OSD for each &osn;
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Dedicated physical infrastructure nodes
    </para>
    <itemizedlist>
     <listitem>
      <para>
       3 &mon; nodes: 4 GB RAM , 4 core processor, RAID 1 SSDs for disk
      </para>
     </listitem>
     <listitem>
      <para>
       1 SES management node: 4GB RAM, 4 core processor, RAID 1 SSDs for disk
      </para>
     </listitem>
     <listitem>
      <para>
       Redundant physical deployment of gateway or &mds; nodes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         &rgw; nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &igw; nodes: 16 GB RAM, 4 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &mds; nodes (one active/one hot standby); 32 GB RAM, 8 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 </chapter>
