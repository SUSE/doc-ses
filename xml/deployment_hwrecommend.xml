<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-bp-hwreq">
 <title>Hardware requirements and recommendations</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The hardware requirements of &ceph; are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </para>
 <para>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </para>
 <sect1 xml:id="network-overview">
  <title>Network overview</title>

  <para>
   &ceph; has several logical networks:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A front-end network called the <literal>public network</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     A trusted internal network, the back-end network, called the
     <literal>cluster network</literal>. This is optional.
    </para>
   </listitem>
   <listitem>
    <para>
     One or more client networks for gateways. This is optional and beyond the
     scope of this chapter.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The public network is the network over which &ceph; daemons communicate with
   each other and with their clients. This means that all &ceph; cluster
   traffic goes over this network except in the case when a cluster network is
   configured.
  </para>

  <para>
   The cluster network is the back-end network between the OSD nodes, for
   replication, re-balancing, and recovery. If configured, this optional
   network would ideally provide twice the bandwidth of the public network with
   default three-way replication, since the primary OSD sends two copies to
   other OSDs via this network. The public network is between clients and
   gateways on the one side to talk to monitors, managers, MDS nodes, OSD
   nodes. It is also used by monitors, managers, and MDS nodes to talk with OSD
   nodes.
  </para>

  <figure xml:id="network-overview-figure">
   <title>Network overview</title>
   <mediaobject>
    <imageobject role="html">
     <imagedata fileref="network-overview-diagram.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="ceph-install-ceph-deploy-network">
   <title>Network recommendations</title>
   <para>
    We recommend a single fault-tolerant network with enough bandwidth to
    fulfil your requirements. For the &ceph; public network environment, we
    recommend two bonded 25&nbsp;GbE (or faster) network interfaces bonded
    using 802.3ad (LACP). This is considered the minimal setup for &ceph;. If
    you are also using a cluster network, we recommend four bonded 25&nbsp;GbE
    network interfaces. Bonding two or more network interfaces provides better
    throughput via link aggregation and, given redundant links and switches,
    improved fault tolerance and maintainability.
   </para>
   <para>
    You can also create VLANs to isolate different types of traffic over a
    bond. For example, you can create a bond to provide two VLAN interfaces,
    one for the public network, and the second for the cluster network.
    However, this is <emphasis>not</emphasis> required when setting up &ceph;
    networking. Details on bonding the interfaces can be found in
    <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-iface-bonding"/>.
   </para>
   <para>
    Fault tolerance can be enhanced through isolating the components into
    failure domains. To improve fault tolerance of the network, bonding one
    interface from two separate Network Interface Cards (NIC) offers protection
    against failure of a single NIC. Similarly, creating a bond across two
    switches protects against failure of a switch. We recommend consulting with
    the network equipment vendor in order to architect the level of fault
    tolerance required.
   </para>
   <important>
    <title>Administration network not supported</title>
    <para>
     Additional administration network setup&mdash;that enables for example
     separating SSH, &salt;, or DNS networking&mdash;is neither tested nor
     supported.
    </para>
   </important>
   <tip>
    <title>Nodes configured via DHCP</title>
    <para>
     If your storage nodes are configured via DHCP, the default timeouts may
     not be sufficient for the network to be configured correctly before the
     various &ceph; daemons start. If this happens, the &ceph; MONs and OSDs
     will not start correctly (running <command>systemctl status
     ceph\*</command> will result in "unable to bind" errors). To avoid this
     issue, we recommend increasing the DHCP client timeout to at least 30
     seconds on each node in your storage cluster. This can be done by changing
     the following settings on each node:
    </para>
    <para>
     In <filename>/etc/sysconfig/network/dhcp</filename>, set
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     In <filename>/etc/sysconfig/network/config</filename>, set
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
   <sect3 xml:id="storage-bp-net-private">
    <title>Adding a private network to a running cluster</title>
    <para>
     If you do not specify a cluster network during &ceph; deployment, it
     assumes a single public network environment. While &ceph; operates fine
     with a public network, its performance and security improves when you set
     a second private cluster network. To support two networks, each &ceph;
     node needs to have at least two network cards.
    </para>
    <para>
     You need to apply the following changes to each &ceph; node. It is
     relatively quick to do for a small cluster, but can be very time consuming
     if you have a cluster consisting of hundreds or thousands of nodes.
    </para>
    <procedure>
     <step>
      <para>
       Set the cluster network using the following command:
      </para>
<screen>&prompt.root;ceph config set global cluster_network <replaceable>MY_NETWORK</replaceable></screen>
      <para>
       Restart the OSDs to bind to the specified cluster network:
      </para>
<screen>&prompt.root;systemctl restart ceph-*@osd.*.service</screen>
     </step>
     <step>
      <para>
       Check that the private cluster network works as expected on the OS
       level.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="storage-bp-net-subnets">
    <title>Monitoring nodes on different subnets</title>
    <para>
     If the monitor nodes are on multiple subnets, for example they are located
     in different rooms and served by different switches, you need to specify
     their public network address in CIDR notation:
    </para>
<screen>&prompt.cephuser;ceph config set mon public_network "<replaceable>MON_NETWORK_1</replaceable>, <replaceable>MON_NETWORK_2</replaceable>, <replaceable>MON_NETWORK_N</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;ceph config set mon public_network "192.168.1.0/24, 10.10.0.0/16"</screen>
    <warning>
     <para>
      If you do specify more than one network segment for the public (or
      cluster) network as described in this section, each of these subnets must
      be capable of routing to all the others - otherwise, the MONs and other
      &ceph; daemons on different network segments will not be able to
      communicate and a split cluster will ensue. Additionally, if you are
      using a firewall, make sure you include each IP address or subnet in your
      iptables and open ports for them on all nodes as necessary.
     </para>
    </warning>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="multi-architecture">
  <title>Multiple architecture configurations</title>

  <para>
   &productname; supports both &x86; and &arm; architectures. When considering
   each architecture, it is important to note that from a cores per OSD,
   frequency, and RAM perspective, there is no real difference between CPU
   architectures for sizing.
  </para>

  <para>
   As with smaller &x86; processors (non-server), lower-performance &arm;-based
   cores may not provide an optimal experience, especially when used for
   erasure coded pools.
  </para>

  <note>
   <para>
    Throughout the documentation, <replaceable>SYSTEM-ARCH</replaceable> is
    used in place of &x86; or &arm;.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="ses-hardware-config">
  <title>Hardware configuration</title>

  <para>
   For the best product experience, we recommend to start with the recommended
   cluster configuration. For a test cluster or a cluster with less performance
   requirements, we document a minimal supported cluster configuration.
  </para>

  <sect2 xml:id="ses-bp-minimum-cluster">
   <title>Minimum cluster configuration</title>
   <para>
    A minimal product cluster configuration consists of:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      At least four physical nodes (OSD nodes) with co-location of services
     </para>
    </listitem>
    <listitem>
     <para>
      Dual-10 Gb Ethernet as a bonded network
     </para>
    </listitem>
    <listitem>
     <para>
      A separate &adm; (can be virtualized on an external node)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    A detailed configuration is:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Separate &adm; with 4 GB RAM, four cores, 1 TB storage capacity. This is
      typically the &smaster; node. &ceph; services and gateways, such as
      &mon;, &mds;, &osd;, &ogw;, or &ganesha; are not supported on the &adm;
      as it needs to orchestrate the cluster update and upgrade processes
      independently.
     </para>
    </listitem>
    <listitem>
     <para>
      At least four physical OSD nodes, with eight OSD disks each, see
      <xref linkend="sysreq-osd"/> for requirements.
     </para>
     <para>
      The total capacity of the cluster should be sized so that even with one
      node unavailable, the total used capacity (including redundancy) does not
      exceed 80%.
     </para>
    </listitem>
    <listitem>
     <para>
      Three &mon; instances. Monitors need to be run from SSD/NVMe storage, not
      HDDs, for latency reasons.
     </para>
    </listitem>
    <listitem>
     <para>
      Monitors, &mds;, and gateways can be co-located on the OSD nodes, see
      <xref linkend="ses-bp-diskshare"/> for monitor co-location. If you
      co-locate services, the memory and CPU requirements need to be added up.
     </para>
    </listitem>
    <listitem>
     <para>
      &igw;, &ogw;, and &mds; require at least incremental 4 GB RAM and four
      cores.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are using &cephfs;, S3/Swift, &iscsi;, at least two instances of
      the respective roles (&mds;, &ogw;, &iscsi;) are required for redundancy
      and availability.
     </para>
    </listitem>
    <listitem>
     <para>
      The nodes are to be dedicated to &productname; and must not be used for
      any other physical, containerized, or virtualized workload.
     </para>
    </listitem>
    <listitem>
     <para>
      If any of the gateways (&iscsi;, &ogw;, &ganesha;, &mds;, ...) are
      deployed within VMs, these VMs must not be hosted on the physical
      machines serving other cluster roles. (This is unnecessary, as they are
      supported as collocated services.)
     </para>
    </listitem>
    <listitem>
     <para>
      When deploying services as VMs on hypervisors outside the core physical
      cluster, failure domains must be respected to ensure redundancy.
     </para>
     <para>
      For example, do not deploy multiple roles of the same type on the same
      hypervisor, such as multiple MONs or MDSs instances.
     </para>
    </listitem>
    <listitem>
     <para>
      When deploying inside VMs, it is particularly crucial to ensure that the
      nodes have strong network connectivity and well working time
      synchronization.
     </para>
    </listitem>
    <listitem>
     <para>
      The hypervisor nodes must be adequately sized to avoid interference by
      other workloads consuming CPU, RAM, network, and storage resources.
     </para>
    </listitem>
   </itemizedlist>
   <figure>
    <title>Minimum cluster configuration</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="ses-bp-production-cluster">
   <title>Recommended production cluster configuration</title>
   <para>
    Once you grow your cluster, we recommend relocating &mon;s, &mds;s, and
    Gateways to separate nodes for better fault tolerance.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Seven &osn;s
     </para>
     <itemizedlist>
      <listitem>
       <para>
        No single node exceeds ~15% of total storage.
       </para>
      </listitem>
      <listitem>
       <para>
        The total capacity of the cluster should be sized so that even with one
        node unavailable, the total used capacity (including redundancy) does
        not exceed 80%.
       </para>
      </listitem>
      <listitem>
       <para>
        25 Gb Ethernet or better, bonded for internal cluster and external
        public network each.
       </para>
      </listitem>
      <listitem>
       <para>
        56+ OSDs per storage cluster.
       </para>
      </listitem>
      <listitem>
       <para>
        See <xref linkend="sysreq-osd"/> for further recommendation.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Dedicated physical infrastructure nodes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Three &mon; nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </para>
       <para>
        See <xref linkend="sysreq-mon"/> for further recommendation.
       </para>
      </listitem>
      <listitem>
       <para>
        &ogw; nodes: 32&nbsp;GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </para>
       <para>
        See <xref linkend="sysreq-rgw"/> for further recommendation.
       </para>
      </listitem>
      <listitem>
       <para>
        &igw; nodes: 16 GB RAM, 8 core processor, RAID 1 SSDs for disk.
       </para>
       <para>
        See <xref linkend="sysreq-iscsi"/> for further recommendation.
       </para>
      </listitem>
      <listitem>
       <para>
        &mds; nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
        RAID 1 SSDs for disk.
       </para>
       <para>
        See <xref linkend="sysreq-mds"/> for further recommendation.
       </para>
      </listitem>
      <listitem>
       <para>
        One SES &adm;: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deployment-hw-multipath">
   <title>Multipath configuration</title>
   <para>
    If you want to use multipath hardware, ensure that LVM sees
    <literal>multipath_component_detection = 1</literal> in the configuration
    file under the <literal>devices</literal> section. This can be checked via
    the <command>lvm config</command> command.
   </para>
   <para>
    Alternatively, ensure that LVM filters a device's mpath components via the
    LVM filter configuration. This will be host specific.
   </para>
   <note>
    <para>
     This is not recommended and should only ever be considered if
     <literal>multipath_component_detection = 1</literal> cannot be set.
    </para>
   </note>
   <para>
    For more information on multipath configuration, see
    <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-multipath.html#sec-multipath-lvm"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>&osn;s</title>

  <sect2 xml:id="sysreq-osd">
   <title>Minimum requirements</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      The following CPU recommendations account for devices independent of
      usage by &ceph;:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1x 2GHz CPU Thread per spinner.
       </para>
      </listitem>
      <listitem>
       <para>
        2x 2GHz CPU Thread per SSD.
       </para>
      </listitem>
      <listitem>
       <para>
        4x 2GHz CPU Thread per NVMe.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Separate 10 GbE networks (public/client and internal), required 4x 10
      GbE, recommended 2x 25 GbE.
     </para>
    </listitem>
    <listitem>
     <para>
      Total RAM required = number of OSDs x (1 GB +
      <option>osd_memory_target</option>) + 16 GB
     </para>
     <para>
      Refer to <xref linkend="config-auto-cache-sizing"/> for more details on
      <option>osd_memory_target</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks in JBOD configurations or individual RAID-0 configurations.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD journal can reside on OSD disk.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks should be exclusively used by &productname;.
     </para>
    </listitem>
    <listitem>
     <para>
      Dedicated disk and SSD for the operating system, preferably in a RAID 1
      configuration.
     </para>
    </listitem>
    <listitem>
     <para>
      Allocate at least an additional 4 GB of RAM if this OSD host will host
      part of a cache pool used for cache tiering.
     </para>
    </listitem>
    <listitem>
     <para>
      &mon;s, gateway and &mds;s can reside on &osn;s.
     </para>
    </listitem>
    <listitem>
     <para>
      For disk performance reasons, OSD nodes are bare metal nodes. No other
      workloads should run on an OSD node unless it is a minimal setup of
      &mon;s and &mgr;s.
     </para>
    </listitem>
    <listitem>
     <para>
      SSDs for Journal with 6:1 ratio SSD journal to OSD.
     </para>
    </listitem>
   </itemizedlist>
     <note>
       <para>
         Ensure that OSD nodes do not have any networked block devices mapped,
         such as &iscsi; or &rbd; images.
       </para>
     </note>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Minimum disk size</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    WAL/DB device, and the primary space for the stored data. The minimum (and
    default) value for the WAL/DB is 6 GB. The minimum space for data is 5 GB,
    as partitions smaller than 5 GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Recommended size for the &bluestore;'s WAL and DB device</title>
   <tip>
    <title>More Information</title>
    <para>
     Refer to <xref linkend="about-bluestore"/> for more information on
     &bluestore;.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      We recommend reserving 4 GB for the WAL device. The recommended size for
      DB is 64 GB for most workloads.
     </para>
     <important>
      <para>
       We recommend larger DB volumes for high-load deployments, especially if
       there is high RGW or &cephfs; usage. Reserve some capacity (slots) to
       install more hardware for more DB space if required.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows &ceph; to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as &ceph; uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not used fully then its
      space is not wasted but used for DB operation.
     </para>
     <para>
      To share the DB device with the WAL, do <emphasis>not</emphasis> specify
      the WAL device, and specify only the DB device.
     </para>
     <para>
      Find more information about specifying an OSD layout in
      <xref linkend="drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>SSD for WAL/DB partitions</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their WAL/DB
    partitions on an SSD and the object data on a separate hard disk.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple WAL/DB Partitions</title>
    <para>
     As WAL/DB partitions occupy relatively little space, you can share one SSD
     disk with multiple WAL/DB partitions. Keep in mind that with each WAL/DB
     partition, the performance of the SSD disk degrades. We do not recommend
     sharing more than six WAL/DB partitions on the same SSD disk and 12 on
     NVMe disks.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Maximum recommended number of disks</title>
   <para>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> RAM above 2&nbsp;GB is used for the
      &bluestore; cache. With the default <option>osd_memory_target</option> of
      4&nbsp;GB, the system has a reasonable starting cache size for spinning
      media. If using SSD or NVME, consider increasing the cache size and RAM
      allocation per OSD to maximize performance.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Monitor nodes</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least three MON nodes are required. The number of monitors should
     always be odd (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB of RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processor with four logical cores.
    </para>
   </listitem>
   <listitem>
    <para>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <filename>/var/lib/ceph</filename> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </para>
   </listitem>
   <listitem>
    <para>
     There must only be one monitor process per node.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, MON, or &ogw; nodes is only supported if sufficient hardware
     resources are available. That means that the requirements for all services
     need to be added up.
    </para>
   </listitem>
   <listitem>
    <para>
     Two network interfaces bonded to multiple switches.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>&ogw; nodes</title>

  <para>
   &ogw; nodes should have at least six CPU cores and 32 GB of RAM. When other
   processes are co-located on the same machine, their requirements need to be
   added up.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>&mds; nodes</title>

  <para>
   Proper sizing of the &mds; nodes depends on the specific use case.
   Generally, the more open files the &mds; is to handle, the more CPU and RAM
   it needs. The following are the minimum requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     4&nbsp;GB of RAM for each &mds; daemon.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interface.
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU with at least 2 cores.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-admin-node">
  <title>&adm;</title>

  <para>
   At least 4 GB of RAM and a quad-core CPU are required. This includes running
   the &smaster; on the &adm;. For large clusters with hundreds of nodes, 6 GB
   of RAM is suggested.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>&igw; nodes</title>

  <para>
   &igw; nodes should have at least six CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SES and other &suse; products</title>

  <para>
   This section contains important information about integrating SES with other
   &suse; products.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>&susemgr;</title>
   <para>
    &susemgr; and &productname; are not integrated, therefore &susemgr; cannot
    currently manage an SES cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Name limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>OSD and monitor sharing one server</title>

  <para>
   Although it is technically possible to run OSDs and MONs on the same server
   in test environments, we strongly recommend having a separate server for
   each monitor node in production. The main reason is performance&mdash;the
   more OSDs the cluster has, the more I/O operations the MON nodes need to
   perform. And when one server is shared between a MON node and OSD(s), the
   OSD I/O operations are a limiting factor for the monitor node.
  </para>

  <para>
   Another consideration is whether to share disks between an OSD, a MON node,
   and the operating system on the server. The answer is simple: if possible,
   dedicate a separate disk to OSD, and a separate server to a monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and MON node on
    the same server, run MON on a separate disk by mounting the disk to the
    <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>
</chapter>
