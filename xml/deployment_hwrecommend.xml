<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.hwreq">
 <title>Hardware Requirements and Recommendations</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>yes</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  The hardware requirements of &ceph; are heavily dependent on the IO workload.
  The following hardware requirements and recommendations should be considered
  as a starting point for detailed planning.
 </para>
 <para>
  In general, the recommendations given in this section are on a per-process
  basis. If several processes are located on the same machine, the CPU, RAM,
  disk and network requirements need to be added up.
 </para>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>&osn;s</title>

  <sect2 xml:id="sysreq.osd">
   <title>Minimum Requirements</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      At least 4 OSD nodes, with 8 OSD disks each, are required.
     </para>
    </listitem>
    <listitem>
     <para>
      For OSDs that do <emphasis>not</emphasis> use &bluestore;, 1 GB of RAM
      per terabyte of raw OSD capacity is minimally required for each OSD
      storage node. 1.5 GB of RAM per terabyte of raw OSD capacity is
      recommended. During recovery, 2 GB of RAM per terabyte of raw OSD
      capacity may be optimal.
     </para>
     <para>
      For OSDs that <emphasis>use</emphasis> &bluestore;, first calculate the
      size of RAM that is recommended for OSDs that do not use &bluestore;,
      then calculate 2 GB plus the size of the &bluestore; cache of RAM is
      recommended for each OSD process, and choose the bigger value of RAM of
      the two results. Note that the default &bluestore; cache is 1 GB for HDD
      and 3 GB for SSD drives by default. In summary, pick the greater of:
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      or
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      1.5 GHz of a logical CPU core per OSD is minimally required for each OSD
      daemon process. 2 GHz per OSD daemon process is recommended. Note that
      Ceph runs one OSD daemon process per storage disk; do not count disks
      reserved solely for use as OSD journals, WAL journals, omap metadata, or
      any combination of these three cases.
     </para>
    </listitem>
    <listitem>
     <para>
      10 Gb Ethernet (two network interfaces bonded to multiple switches).
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks in JBOD configurations.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD disks should be exclusively used by &storage;.
     </para>
    </listitem>
    <listitem>
     <para>
      Dedicated disk/SSD for the operating system, preferably in a RAID 1
      configuration.
     </para>
    </listitem>
    <listitem>
     <para>
      If this OSD host will host part of a cache pool used for cache tiering,
      allocate at least an additional 4 GB of RAM.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD nodes should be bare metal, not virtualized, for disk performance
      reasons.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Minimum Disk Size</title>
   <para>
    There are two types of disk space needed to run on OSD: the space for the
    disk journal (for &filestore;) or WAL/DB device (for &bluestore;), and the
    primary space for the stored data. The minimum (and default) value for the
    journal/WAL/DB is 6 GB. The minimum space for data is 5 GB, as partitions
    smaller than 5 GB are automatically assigned the weight of 0.
   </para>
   <para>
    So although the minimum disk space for an OSD is 11 GB, we do not recommend
    a disk smaller than 20 GB, even for testing purposes.
   </para>
  </sect2>

  <sect2 xml:id="rec.waldb.size">
   <title>Recommended Size for the &bluestore;'s WAL and DB Device</title>
   <tip>
    <title>More Information</title>
    <para>
     Refer to <xref linkend="about.bluestore"/> for more information on
     &bluestore;.
    </para>
   </tip>
   <para>
    Following are several rules for WAL/DB device sizing. When using &deepsea;
    to deploy OSDs with &bluestore;, it applies the recommended rules
    automatically and notifies the administrator about the fact.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      10GB of the DB device for each Terabyte of the OSD capacity (1/100th of
      the OSD).
     </para>
    </listitem>
    <listitem>
     <para>
      Between 500MB and 2GB for the WAL device. The WAL size depends on the
      data traffic and workload, not on the OSD size. If you know that an OSD
      is physically able to handle small writes and overwrites at a very high
      throughput, more WAL is preferred rather than less WAL. 1GB WAL device is
      a good compromise that fulfills most deployments.
     </para>
    </listitem>
    <listitem>
     <para>
      If you intend to put the WAL and DB device on the same disk, then we
      recommend using a single partition for both devices, rather than having a
      separate partition for each. This allows &ceph; to use the DB device for
      the WAL operation as well. Management of the disk space is therefore more
      effective as &ceph; uses the DB partition for the WAL only if there is a
      need for it. Another advantage is that the probability that the WAL
      partition gets full is very small, and when it is not entirely used then
      its space is not wasted but used for DB operation.
     </para>
     <para>
      To share the DB device with the WAL, do <emphasis>not</emphasis> specify
      the WAL device, and specify only the DB device:
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      Alternatively, you can put the WAL on its own separate device. In such
      case, we recommend the fastest device for the WAL operation.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Using SSD for OSD Journals</title>
   <para>
    Solid-state drives (SSD) have no moving parts. This reduces random access
    time and read latency while accelerating data throughput. Because their
    price per 1MB is significantly higher than the price of spinning hard
    disks, SSDs are only suitable for smaller storage.
   </para>
   <para>
    OSDs may see a significant performance improvement by storing their journal
    on an SSD and the object data on a separate hard disk.
   </para>
   <tip>
    <title>Sharing an SSD for Multiple Journals</title>
    <para>
     As journal data occupies relatively little space, you can mount several
     journal directories to a single SSD disk. Keep in mind that with each
     shared journal, the performance of the SSD disk degrades. We do not
     recommend sharing more than six journals on the same SSD disk and 12 on
     NVMe disks.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>Maximum Recommended Number of Disks</title>
   <para>
    You can have as many disks in one server as it allows. There are a few
    things to consider when planning the number of disks per server:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Network bandwidth.</emphasis> The more disks you have in a
      server, the more data must be transferred via the network card(s) for the
      disk write operations.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Memory.</emphasis> For optimum performance, reserve at least 2
      GB of RAM per terabyte of disk space installed.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fault tolerance.</emphasis> If the complete server fails, the
      more disks it has, the more OSDs the cluster temporarily loses. Moreover,
      to keep the replication rules running, you need to copy all the data from
      the failed server among the other nodes in the cluster.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.mon">
  <title>Monitor Nodes</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     At least three &mon; nodes are required. The number of monitors should
     always be odd (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB of RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processor with four logical cores.
    </para>
   </listitem>
   <listitem>
    <para>
     An SSD or other sufficiently fast storage type is highly recommended for
     monitors, specifically for the <filename>/var/lib/ceph</filename> path on
     each monitor node, as quorum may be unstable with high disk latencies. Two
     disks in RAID 1 configuration is recommended for redundancy. It is
     recommended that separate disks or at least separate disk partitions are
     used for the monitor processes to protect the monitor's available disk
     space from things like log file creep.
    </para>
   </listitem>
   <listitem>
    <para>
     There must only be one monitor process per node.
    </para>
   </listitem>
   <listitem>
    <para>
     Mixing OSD, monitor, or &rgw; nodes is only supported if sufficient
     hardware resources are available. That means that the requirements for all
     services need to be added up.
    </para>
   </listitem>
   <listitem>
    <para>
     Two network interfaces bonded to multiple switches.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>&rgw; Nodes</title>

  <para>
   &rgw; nodes should have six to eight CPU cores and 32 GB of RAM (64 GB
   recommended). When other processes are co-located on the same machine, their
   requirements need to be added up.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.mds">
  <title>&mds; Nodes</title>

  <para>
   Proper sizing of the &mds; nodes depends on the specific use case.
   Generally, the more open files the &mds; is to handle, the more CPU and RAM
   it needs. Following are the minimal requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3G of RAM per one &mds; daemon.
    </para>
   </listitem>
   <listitem>
    <para>
     Bonded network interface.
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU with at least 2 cores.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.smaster">
  <title>Salt Master</title>

  <para>
   At least 4 GB of RAM and a quad-core CPU are required. This is includes
   running &oa; on the &smaster;. For large clusters with hundreds of nodes, 6
   GB of RAM is suggested.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>iSCSI Nodes</title>

  <para>
   iSCSI nodes should have six to eight CPU cores and 16 GB of RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>Network Recommendations</title>

  <para>
   The network environment where you intend to run &ceph; should ideally be a
   bonded set of at least two network interfaces that is logically split into a
   public part and a trusted internal part using VLANs. The bonding mode is
   recommended to be 802.3ad if possible to provide maximum bandwidth and
   resiliency.
  </para>

  <para>
   The public VLAN serves to provide the service to the customers, while the
   internal part provides for the authenticated &ceph; network communication.
   The main reason for this is that although &ceph; provides authentication and
   protection against attacks once secret keys are in place, the messages used
   to configure these keys may be transferred openly and are vulnerable.
  </para>

  <tip>
   <title>Nodes Configured via DHCP</title>
   <para>
    If your storage nodes are configured via DHCP, the default timeouts may not
    be sufficient for the network to be configured correctly before the various
    &ceph; daemons start. If this happens, the &ceph; MONs and OSDs will not
    start correctly (running <command>systemctl status ceph\*</command> will
    result in "unable to bind" errors) To avoid this issue, we recommend
    increasing the DHCP client timeout to at least 30 seconds on each node in
    your storage cluster. This can be done by changing the following settings
    on each node:
   </para>
   <para>
    In <filename>/etc/sysconfig/network/dhcp</filename>, set
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    In <filename>/etc/sysconfig/network/config</filename>, set
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage.bp.net.private">
   <title>Adding a Private Network to a Running Cluster</title>
   <para>
    If you do not specify a cluster network during &ceph; deployment, it
    assumes a single public network environment. While &ceph; operates fine
    with a public network, its performance and security improves when you set a
    second private cluster network. To support two networks, each &ceph; node
    needs to have at least two network cards.
   </para>
   <para>
    You need to apply the following changes to each &ceph; node. It is
    relatively quick to do for a small cluster, but can be very time consuming
    if you have a cluster consisting of hundreds or thousands of nodes.
   </para>
   <procedure>
    <step>
     <para>
      Stop &ceph; related services on each cluster node.
     </para>
     <para>
      Add a line to <filename>/etc/ceph/ceph.conf</filename> to define the
      cluster network, for example:
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      If you need to specifically assign static IP addresses or override
      <option>cluster network</option> settings, you can do so with the
      optional <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Check that the private cluster network works as expected on the OS level.
     </para>
    </step>
    <step>
     <para>
      Start &ceph; related services on each cluster node.
     </para>
<screen>&prompt.root;systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.net.subnets">
   <title>Monitor Nodes on Different Subnets</title>
   <para>
    If the monitor nodes are on multiple subnets, for example they are located
    in different rooms and served by different switches, you need to adjust the
    <filename>ceph.conf</filename> file accordingly. For example if the nodes
    have IP addresses 192.168.123.12, 1.2.3.4, and 242.12.33.12, add the
    following lines to its global section:
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    Additionally, if you need to specify a per-monitor public address or
    network, you need to add a
    <literal>[mon.<replaceable>X</replaceable>]</literal> section per each
    monitor:
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Naming Limitations</title>

  <para>
   &ceph; does not generally support non-ASCII characters in configuration
   files, pool names, user names and so forth. When configuring a &ceph;
   cluster we recommend using only simple alphanumeric characters (A-Z, a-z,
   0-9) and minimal punctuation ('.', '-', '_') in all &ceph;
   object/configuration names.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>OSD and Monitor Sharing One Server</title>

  <para>
   Although it is technically possible to run &ceph; OSDs and Monitors on the
   same server in test environments, we strongly recommend having a separate
   server for each monitor node in production. The main reason is
   performance&mdash;the more OSDs the cluster has, the more I/O operations the
   monitor nodes need to perform. And when one server is shared between a
   monitor node and OSD(s), the OSD I/O operations are a limiting factor for
   the monitor node.
  </para>

  <para>
   Another consideration is whether to share disks between an OSD, a monitor
   node, and the operating system on the server. The answer is simple: if
   possible, dedicate a separate disk to OSD, and a separate server to a
   monitor node.
  </para>

  <para>
   Although &ceph; supports directory-based OSDs, an OSD should always have a
   dedicated disk other than the operating system one.
  </para>

  <tip>
   <para>
    If it is <emphasis>really</emphasis> necessary to run OSD and monitor node
    on the same server, run the monitor on a separate disk by mounting the disk
    to the <filename>/var/lib/ceph/mon</filename> directory for slightly better
    performance.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses.bp.minimum_cluster">
  <title>Minimum Cluster Configuration</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Four &osn;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10 Gb Ethernet (two networks bonded to multiple switches)
      </para>
     </listitem>
     <listitem>
      <para>
       32 OSDs per storage cluster
      </para>
     </listitem>
     <listitem>
      <para>
       OSD journal can reside on OSD disk
      </para>
     </listitem>
     <listitem>
      <para>
       Dedicated OS disk for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       1 GB of RAM per TB of raw OSD capacity for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       1.5 GHz per OSD for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;s, gateway and &mds;s can reside on &osn;s
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Three &mon; nodes (requires SSD for dedicated OS drive)
        </para>
       </listitem>
       <listitem>
        <para>
         &mon;s, &rgw;s and &mds;s nodes require redundant deployment
        </para>
       </listitem>
       <listitem>
        <para>
         &igw;s, &rgw;s and &mds;s require incremental 4 GB RAM and four cores
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Separate management node with 4 GB RAM, four cores, 1 TB capacity
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses.bp.production_cluster">
  <title>Recommended Production Cluster Configuration</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Seven &osn;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       No single node exceeds ~15% of total storage
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb Ethernet (four physical networks bonded to multiple switches)
      </para>
     </listitem>
     <listitem>
      <para>
       56+ OSDs per storage cluster
      </para>
     </listitem>
     <listitem>
      <para>
       RAID 1 OS disks for each OSD storage node
      </para>
     </listitem>
     <listitem>
      <para>
       SSDs for Journal with 6:1 ratio SSD journal to OSD
      </para>
     </listitem>
     <listitem>
      <para>
       1.5 GB of RAM per TB of raw OSD capacity for each &osn;
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz per OSD for each &osn;
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Dedicated physical infrastructure nodes
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Three &mon; nodes: 4 GB RAM, 4 core processor, RAID 1 SSDs for disk
      </para>
     </listitem>
     <listitem>
      <para>
       One SES management node: 4 GB RAM, 4 core processor, RAID 1 SSDs for
       disk
      </para>
     </listitem>
     <listitem>
      <para>
       Redundant physical deployment of gateway or &mds; nodes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         &rgw; nodes: 32 GB RAM, 8 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &igw; nodes: 16 GB RAM, 4 core processor, RAID 1 SSDs for disk
        </para>
       </listitem>
       <listitem>
        <para>
         &mds; nodes (one active/one hot standby): 32 GB RAM, 8 core processor,
         RAID 1 SSDs for disk
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req.ses.other">
  <title>&productname; and Other &suse; Products</title>

  <para>
   This section contains important information about integrating &productname;
   with other &suse; products.
  </para>

  <sect2 xml:id="req.ses.suma">
   <title>&susemgr;</title>
   <para>
    &susemgr; and &productname; are not integrated, therefore &susemgr; cannot
    currently manage a &productname; cluster.
   </para>
  </sect2>
 </sect1>
</chapter>
