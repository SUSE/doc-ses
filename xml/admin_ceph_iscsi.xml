<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.iscsi">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>&ceph; iSCSI Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <emphasis>initiators</emphasis>) to send SCSI commands to SCSI storage
  devices (<emphasis>targets</emphasis>) on remote servers. &storage; includes
  a facility that opens &ceph; storage management to heterogeneous clients,
  such as &mswin;* and &vmware;* vSphere, through the iSCSI protocol. Multipath
  iSCSI access enables availability and scalability for these clients, and the
  standardized iSCSI protocol also provides an additional layer of security
  isolation between clients and the &storage; cluster. The configuration
  facility is named <systemitem>lrbd</systemitem>. Using
  <systemitem>lrbd</systemitem>, &ceph; storage administrators can define
  thin-provisioned, replicated, highly-available volumes supporting read-only
  snapshots, read-write clones, and automatic resizing with &ceph; RADOS Block
  Device (RBD). Administrators can then export volumes either via a single
  <systemitem>lrbd</systemitem> gateway host, or via multiple gateway hosts
  supporting multipath failover. Linux, &mswin;, and &vmware; hosts can connect
  to volumes using the iSCSI protocol, which makes them available like any
  other SCSI block device. This means &storage; customers can effectively run a
  complete block-storage infrastructure subsystem on &ceph; that provides all
  features and benefits of a conventional SAN enabling future growth.
 </para>
 <para>
  This chapter introduces detailed information to set up a &ceph; cluster
  infrastructure together with an iSCSI gateway so that the client hosts can
  use remotely stored data as local storage devices using the iSCSI protocol.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>iSCSI Block Storage</title>

  <para>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
   is implemented as a service where a client (the initiator) talks to a server
   (the target) via a session on TCP port 3260. An iSCSI target's IP address
   and port are called an iSCSI portal, where a target can be exposed through
   one or more portals. The combination of a target and one or more portals is
   called the target portal group (TPG).
  </para>

  <para>
   The underlying data link layer protocol for iSCSI is commonly Ethernet. More
   specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet or faster
   networks for optimal throughput. 10 Gigabit Ethernet connectivity between
   the iSCSI gateway and the back-end &ceph; cluster is strongly recommended.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>The Linux Kernel iSCSI Target</title>
   <para>
    The Linux kernel iSCSI target was originally named LIO for linux-iscsi.org,
    the project's original domain and Web site. For some time, no fewer than 4
    competing iSCSI target implementations were available for the Linux
    platform, but LIO ultimately prevailed as the single iSCSI reference
    target. The mainline kernel code for LIO uses the simple, but somewhat
    ambiguous name "target", distinguishing between "target core" and a variety
    of front-end and back-end target modules.
   </para>
   <para>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol is
    supported by &storage;.
   </para>
   <para>
    The most frequently used target back-end module is one that is capable of
    simply re-exporting any available block device on the target host. This
    module is named iblock. However, LIO also has an RBD-specific back-end
    module supporting parallelized multipath I/O access to RBD images.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>iSCSI Initiators</title>
   <para>
    This section introduces a brief information on iSCSI initiators used on
    Linux, &mswin;, and &vmware; platforms.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     The standard initiator for the Linux platform is
     <systemitem>open-iscsi</systemitem>. <systemitem>open-iscsi</systemitem>
     launches a daemon, <systemitem>iscsid</systemitem>, which the user can
     then use to discover iSCSI targets on any given portal, log in to targets,
     and map iSCSI volumes. <systemitem>iscsid</systemitem> communicates with
     the SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <systemitem>open-iscsi</systemitem> initiator can be deploying in
     conjunction with the Device Mapper Multipath
     (<systemitem>dm-multipath</systemitem>) facility to provide a highly
     available iSCSI block device.
    </para>
   </sect3>
   <sect3>
    <title>&mswin; and &hyper;</title>
    <para>
     The default iSCSI initiator for the &mswin; operating system is the
     Microsoft iSCSI initiator. The iSCSI service can be configured via a
     graphical user interface (GUI), and supports multipath I/O for high
     availability.
    </para>
   </sect3>
   <sect3>
    <title>&vmware;</title>
    <para>
     The default iSCSI initiator for &vmware; vSphere and ESX is the &vmware;
     ESX software iSCSI initiator, <systemitem>vmkiscsi</systemitem>. When
     enabled, it can be configured either from the vSphere client, or using the
     <command>vmkiscsi-tool</command> command. You can then format storage
     volumes connected through the vSphere iSCSI storage adapter with VMFS, and
     use them like any other VM storage device. The &vmware; initiator also
     supports multipath I/O for high availability.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrdb">
  <title>General Information about lrdb</title>

  <para>
   <systemitem>lrbd</systemitem> combines the benefits of RADOS Block Devices
   with the ubiquitous versatility of iSCSI. By employing
   <systemitem>lrbd</systemitem> on an iSCSI target host (known as the
   <systemitem>lrbd</systemitem> gateway), any application that needs to make
   use of block storage can benefit from &ceph;, even if it does not speak any
   &ceph; client protocol. Instead, users can use iSCSI or any other target
   front-end protocol to connect to an LIO target, which translates all target
   I/O to RBD storage operations.
  </para>

  <figure>
   <title>&ceph; Cluster with a Single iSCSI Gateway</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   <systemitem>lrbd</systemitem> is inherently highly-available and supports
   multipath operations. Thus, downstream initiator hosts can use multiple
   iSCSI gateways for both high availability and scalability. When
   communicating with an iSCSI configuration with more than one gateway,
   initiators may load-balance iSCSI requests across multiple gateways. In the
   event of a gateway failing, being temporarily unreachable, or being disabled
   for maintenance, I/O will transparently continue via another gateway.
  </para>

  <figure>
   <title>&ceph; Cluster with Multiple iSCSI Gateways</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Deployment Considerations</title>

  <para>
   A minimum configuration of &storage; with <systemitem>lrbd</systemitem>
   consists of the following components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. The &ceph; cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI target server running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI initiator host, running <systemitem>open-iscsi</systemitem>
     (Linux), the Microsoft iSCSI Initiator (&mswin;), or any other compatible
     iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A recommended production configuration of &storage; with
   <systemitem>lrbd</systemitem> consists of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. A production &ceph; cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running 10-12
     object storage daemons (OSDs), with no fewer than three dedicated MON
     hosts.
    </para>
   </listitem>
   <listitem>
    <para>
     Several iSCSI target servers running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>. For iSCSI fail-over and load-balancing,
     these servers must run a kernel supporting the
     <systemitem>target_core_rbd</systemitem> module. Updates packages are
     available from the &sls; maintenance channel.
    </para>
   </listitem>
   <listitem>
    <para>
     Any number of iSCSI initiator hosts, running
     <systemitem>open-iscsi</systemitem> (Linux), the Microsoft iSCSI Initiator
     (&mswin;), or any other compatible iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation and Configuration</title>

  <para>
   This section describes steps to install and configure iSCSI gateway on top
   of &storage;.
  </para>

  <sect2>
   <title>Install &storage; and Deploy a &ceph; Cluster</title>
   <para>
    Before you start installing and configuring an iSCSI gateway, you need to
    install &storage; and deploy a &ceph; cluster as described in
    <xref linkend="cha.ceph.install"/>.
   </para>
  </sect2>

  <sect2>
   <title>Installing the <systemitem>ceph_iscsi</systemitem> Pattern</title>
   <para>
    On your designated iSCSI target server nodes, install the
    <systemitem>ceph_iscsi</systemitem> pattern. Doing so will automatically
    install <systemitem>lrbd</systemitem>, the necessary &ceph; binaries and
    libraries, and the <command>targetcli</command> command line tool:
   </para>
<screen>sudo zypper in -t pattern ceph_iscsi</screen>
   <para>
    Repeat this step on any node that you want to act as a fail-over or
    load-balancing target server node.
   </para>
  </sect2>

  <sect2>
   <title>Create RBD Images</title>
   <para>
    RBD images are created in the &ceph; store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
    You can create a volume from any host that is able to connect to your
    storage cluster using the &ceph; <command>rbd</command> command-line
    utility. This requires the client to have at least a minimal ceph.conf
    configuration file, and appropriate CephX authentication credentials.
   </para>
   <para>
    To create a new volume for subsequent export via iSCSI, use the
    <command>rbd create</command> command, specifying the volume size in
    megabytes. For example, in order to create a 100GB volume named
    <literal>testvol</literal> in the pool named <literal>iscsi</literal>, run:
   </para>
<screen>rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    The above command creates an RBD volume in the default format 2. 
   </para>
   <note>
    <para>
     Since &storage; 3, the default volume format is 2, and format 1 is
     deprecated.  However, you can still create the deprecated format 1 volumes
     with the <option>--image-format 1</option> option.
    </para>
   </note>
   </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.export">
   <title>Export RBD Images via iSCSI</title>
   <para>
    To export RBD images via iSCSI, use the <systemitem>lrbd</systemitem>
    utility. <systemitem>lrbd</systemitem> allows you to create, review, and
    modify the iSCSI target configuration, which uses a JSON format.
   </para>
   <para>
    In order to edit the configuration, use <command>lrbd -e</command> or
    <command>lrbd --edit</command>. This command will invoke the default
    editor, as defined by the <literal>EDITOR</literal> environment variable.
    You may override this behavior by setting the <option>-E</option> option in
    addition to <option>-e</option>.
   </para>
   <para>
    Below is an example configuration for
   </para>
   <itemizedlist>
    <listitem>
     <para>
      two iSCSI gateway hosts named <literal>iscsi1.example.com</literal> and
      <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      with a single iSCSI Logical Unit (LU),
     </para>
    </listitem>
    <listitem>
     <para>
      backed by an RBD image named <literal>testvol</literal> in the RADOS pool
      <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      and exporting the target via two portals named "east" and "west":
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "authentication": "none"
        }
    ], 
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "hosts": [
                {
                    "host": "iscsi1.example.com", 
                    "portal": "east"
                }, 
                {
                    "host": "iscsi2.example.com", 
                    "portal": "west"
                }
            ]
        }
    ], 
    "portals": [
        {
            "name": "east", 
            "addresses": [
                "192.168.124.104"
            ]
        }, 
        {
            "name": "west", 
            "addresses": [
                "192.168.124.105"
            ]
        }
    ], 
    "pools": [
        {
            "pool": "rbd", 
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</screen>
   <para>
    Note that whenever you refer to a host name in the configuration, this host
    name must match the iSCSI gateway's <command>uname -n</command> command
    output.
   </para>
   <para>
    The edited JSON is stored in the extended attributes (xattrs) of a single
    RADOS object per pool. This object is available to the gateway hosts where
    the JSON is edited, and all gateway hosts connected to the same &ceph;
    cluster. No configuration information is stored locally on the
    <systemitem>lrbd</systemitem> gateway.
   </para>
   <para>
    To activate the configuration, store it in the &ceph; cluster, and do one
    of the following things (as &rootuser;):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run the <command>lrbd</command> command (without additional options) from
      the command line,
     </para>
    </listitem>
   </itemizedlist>
   <para>
    or
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Restart the <systemitem>lrbd</systemitem> service with <command>service
      lrbd restart</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <systemitem>lrbd</systemitem> "service" does not operate any background
    daemon. Instead, it simply invokes the <command>lrbd</command> command.
    This type of service is known as a "one-shot" service.
   </para>
   <para>
    You should also enable <systemitem>lrbd</systemitem> to auto-configure on
    system start-up. To do so, run the <command>systemctl enable lrbd</command>
    command.
   </para>
   <para>
    The configuration above reflects a simple, one-gateway setup.
    <systemitem>lrbd</systemitem> configuration can be much more complex and
    powerful. The <systemitem>lrbd</systemitem> RPM package comes with an
    extensive set of configuration examples, which you may refer to by checking
    the contents of the
    <filename>/usr/share/doc/packages/lrbd/samples</filename> directory after
    installation. The samples are also available from
    <link xlink:href="https://github.com/SUSE/lrbd/tree/master/samples"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.optional">
   <title>Optional Settings</title>
   <para>
    The following settings may be useful for some environments. For images,
    there are <option>uuid</option>, <option>lun</option>,
    <option>retries</option>, <option>sleep</option>, and
    <option>retry_errors</option> attributes. The first
    two&mdash;<option>uuid</option> and <option>lun</option>&mdash;allow
    hardcoding of the 'uuid' or 'lun' for a specific image. You can specify
    either of them for an image. The <option>retries</option>,
    <option>sleep</option> and <option>retry_errors</option> affect the
    attempts to map an rbd image.
   </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1", 
        "tpg": [
                    {
                        "image": "archive",
                        "uuid": "12345678-abcd-9012-efab-345678901234",
                        "lun": "2",
                        "retries": "3",
                        "sleep": "4",
                        "retry_errors": [ 95 ],
                        [...]
                    }
                ]
            } 
        ] 
    }
]</screen>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.advanced">
   <title>Advanced Settings</title>
   <para>
    <systemitem>lrdb</systemitem> can be configured with advanced parameters
    which are subsequently passed on to the LIO I/O target. The parameters are
    divided up into iSCSI and backing store components, which can then be
    specified in the "targets" and "tpg" sections, respectively, of the
    <systemitem>lrbd</systemitem> configuration.
   </para>
   <warning>
    <para>
     Changing these parameters from the default setting is not recommended.
    </para>
   </warning>
<screen>"targets": [
				{
								[...]
								"tpg_default_cmdsn_depth": "64",
								"tpg_default_erl": "0",
								"tpg_login_timeout": "10",
								"tpg_netif_timeout": "2",
								"tpg_prod_mode_write_protect": "0",
		  }
]</screen>
   <para>
    Description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>tpg_default_cmdsn_depth</term>
     <listitem>
      <para>
       Default CmdSN (Command Sequence Number) depth. Limits the amount of
       requests that an iSCSI initiator can have outstanding at any moment.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_default_erl</term>
     <listitem>
      <para>
       Default error recovery level.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_login_timeout</term>
     <listitem>
      <para>
       Login timeout value in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_netif_timeout</term>
     <listitem>
      <para>
       NIC failure timeout in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_prod_mode_write_protect</term>
     <listitem>
      <para>
       If set to 1, prevent writes to LUNs.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1", 
        "tpg": [
                    {
                        "image": "archive",
                        "backstore_block_size": "512",
                        "backstore_emulate_3pc": "1",
                        "backstore_emulate_caw": "1",
                        "backstore_emulate_dpo": "0",
                        "backstore_emulate_fua_read": "0",
                        "backstore_emulate_fua_write": "1",
                        "backstore_emulate_model_alias": "0",
                        "backstore_emulate_rest_reord": "0",
                        "backstore_emulate_tas": "1",
                        "backstore_emulate_tpu": "0",
                        "backstore_emulate_tpws": "0",
                        "backstore_emulate_ua_intlck_ctrl": "0",
                        "backstore_emulate_write_cache": "0",
                        "backstore_enforce_pr_isids": "1",
                        "backstore_fabric_max_sectors": "8192",
                        "backstore_hw_block_size": "512",
                        "backstore_hw_max_sectors": "8192",
                        "backstore_hw_pi_prot_type": "0",
                        "backstore_hw_queue_depth": "128",
                        "backstore_is_nonrot": "1",
                        "backstore_max_unmap_block_desc_count": "1",
                        "backstore_max_unmap_lba_count": "8192",
                        "backstore_max_write_same_len": "65535",
                        "backstore_optimal_sectors": "8192",
                        "backstore_pi_prot_format": "0",
                        "backstore_pi_prot_type": "0",
                        "backstore_queue_depth": "128",
                        "backstore_unmap_granularity": "8192",
                        "backstore_unmap_granularity_alignment": "4194304"
                    }
                ]
            } 
        ] 
    }
    ]</screen>
   <para>
    Description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>backstore_block_size</term>
     <listitem>
      <para>
       Block size of the underlying device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_3pc</term>
     <listitem>
      <para>
       If set to 1, enable Third Party Copy.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_caw</term>
     <listitem>
      <para>
       If set to 1, enable Compare and Write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_dpo</term>
     <listitem>
      <para>
       If set to 1, turn on Disable Page Out.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_read</term>
     <listitem>
      <para>
       If set to 1, enable Force Unit Access read.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_write</term>
     <listitem>
      <para>
       If set to 1, enable Force Unit Access write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_model_alias</term>
     <listitem>
      <para>
       If set to 1, use the backend device name for the model alias.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_rest_reord</term>
     <listitem>
      <para>
       If set to 0, the Queue Algorithm Modifier is Restricted Reordering.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tas</term>
     <listitem>
      <para>
       If set to 1, enable Task Aborted Status.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpu</term>
     <listitem>
      <para>
       If set to 1, enable Thin Provisioning Unmap.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpws</term>
     <listitem>
      <para>
       If set to 1, enable Thin Provisioning Write Same.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_ua_intlck_ctrl</term>
     <listitem>
      <para>
       If set to 1, enable Unit Attention Interlock.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_write_cache</term>
     <listitem>
      <para>
       If set to 1, turn on Write Cache Enable.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_enforce_pr_isids</term>
     <listitem>
      <para>
       If set to 1, enforce persistent reservation ISIDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_fabric_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the fabric can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_block_size</term>
     <listitem>
      <para>
       Hardware block size in bytes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the hardware can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_pi_prot_type</term>
     <listitem>
      <para>
       If non-zero, DIF protection is enabled on the underlying hardware.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_queue_depth</term>
     <listitem>
      <para>
       Hardware queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_is_nonrot</term>
     <listitem>
      <para>
       If set to 1, the backstore is a non rotational device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_block_desc_count</term>
     <listitem>
      <para>
       Maximum number of block descriptors for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>"backstore_max_unmap_lba_count":</term>
     <listitem>
      <para>
       Maximum number of LBA for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_write_same_len</term>
     <listitem>
      <para>
       Maximum length for WRITE_SAME.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_optimal_sectors</term>
     <listitem>
      <para>
       Optimal request size in sectors.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_format</term>
     <listitem>
      <para>
       DIF protection format.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_type</term>
     <listitem>
      <para>
       DIF protection type.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_queue_depth</term>
     <listitem>
      <para>
       Queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity</term>
     <listitem>
      <para>
       UNMAP granularity.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity_alignment</term>
     <listitem>
      <para>
       UNMAP granularity alignment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For targets, the <option>tpg</option> attributes allow tuning of kernel
    parameters. Use with caution.
   </para>
<screen>"targets": [
{
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "tpg_login_timeout": "10",
    "tpg_default_cmdsn_depth": "64",
    "tpg_default_erl": "0",
    "tpg_login_timeout": "10",
    "tpg_netif_timeout": "2",
    "tpg_prod_mode_write_protect": "0",
    "tpg_t10_pi": "0"
}</screen>
   <tip>
    <para>
     If a site needs statically assigned LUNs, then assign numbers to each LUN.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.connect">
  <title>Connecting to lrbd-managed Targets</title>

  <para>
   This chapter describes how to connect to lrdb-managed targets from clients
   running Linux, &mswin;, or &vmware;.
  </para>

  <sect2 xml:id="ceph.iscsi.connect.linux">
   <title>Linux (<systemitem>open-iscsi</systemitem>)</title>
   <para>
    Connecting to lrbd-backed iSCSI targets with
    <systemitem>open-iscsi</systemitem> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </para>
   <para>
    Both steps require that the <systemitem>open-iscsi</systemitem> daemon is
    running. The way you start the <systemitem>open-iscsi</systemitem> daemon
    is dependent on your Linux distribution:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      On &sls; (SLES); and &rhel; (RHEL) hosts, run <command>systemctl start
      iscsid</command> (or <command>service iscsid start</command> if
      <command>systemctl</command> is not available).
     </para>
    </listitem>
    <listitem>
     <para>
      On Debian and Ubuntu hosts, run <command>systemctl start
      open-iscsi</command> (or <command>service open-iscsi start</command>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If your initiator host runs &sls;, refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/sec_iscsi_initiator.html"/>
    or
    <link xlink:href="https://www.suse.com/documentation/sles11/stor_admin/data/sec_inst_system_iscsi_initiator.html"/>
    for details on how to connect to an iSCSI target.
   </para>
   <para>
    For any other Linux distribution supporting
    <systemitem>open-iscsi</systemitem>, proceed to discover targets on your
    <systemitem>lrbd</systemitem> gateway (this example uses iscsi1.example.com
    as the portal address; for multipath access repeat these steps with
    iscsi2.example.com):
   </para>
<screen>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</screen>
   <para>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    Repeat this process for other portal IP addresses or hosts.
   </para>
   <para>
    If your system has the <systemitem>lsscsi</systemitem> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </para>
<screen>lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde 
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</screen>
   <para>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <systemitem>multipath</systemitem> utility:
   </para>
<screen>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</screen>
   <para>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </para>
<screen>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca 
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   <para>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </para>
   <para>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </para>
   <sect3 xml:id="ceph.iscsi.connect.linux.multipath">
    <title>Multipath Configuration</title>
    <para>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <systemitem>lrbd</systemitem> configuration. Select
     a strategy prior to using block storage. After editing the
     <filename>/etc/multipath.conf</filename>, restart
     <systemitem>multipathd</systemitem> with
    </para>
<screen>sudo systemctl restart multipathd</screen>
    <para>
     For an active-passive configuration with friendly names, add
    </para>
<screen>defaults {
  user_friendly_names yes
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. After connecting to your
     targets successfully, run
    </para>
<screen>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</screen>
    <para>
     Note the status of each link. For an active-active configuration, add
    </para>
<screen>defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. Restart
     <systemitem>multipathd</systemitem> and run
    </para>
<screen>multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.win">
   <title>&mswin; (Microsoft iSCSI initiator)</title>
   <para>
    To connect to a &storage; iSCSI target from a Windows 2012 server, follow
    these steps:
   </para>
   <procedure>
    <step>
     <para>
      Open Windows Server Manager. From the Dashboard, select
      <menuchoice><guimenu>Tools</guimenu><guimenu>iSCSI
      Initiator</guimenu></menuchoice>. The <guimenu>iSCSI Initiator
      Properties</guimenu> dialog appears. Select the
      <guimenu>Discovery</guimenu> tab:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>Discover Target Portal</guimenu> dialog, enter the
      target's host name or IP address in the <guimenu>Target</guimenu> field
      and click <guimenu>OK</guimenu>:
     </para>
     <figure>
      <title>Discover Target Portal</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <guimenu>Target Portals</guimenu> list:
     </para>
     <figure>
      <title>Target Portals</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Next, switch to the <guimenu>Targets</guimenu> tab and review your
      discovered target(s).
     </para>
     <figure>
      <title>Targets</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Click <guimenu>Connect</guimenu> in the <guimenu>Targets</guimenu> tab.
      The <guimenu>Connect To Target</guimenu> dialog appears. Select the
      <guimenu>Enable Multi-path</guimenu> check box to enable multipath I/O
      (MPIO), then click <guimenu>OK</guimenu>:
     </para>
    </step>
    <step>
     <para>
      When the <guimenu>Connect to Target</guimenu> dialog closes, select
      <guimenu>Properties</guimenu> to review the target's properties:
     </para>
     <figure>
      <title>iSCSI Target Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select <guimenu>Devices</guimenu>, and click <guimenu>MPIO</guimenu> to
      review the multipath I/O configuration:
     </para>
     <figure>
      <title>Device Details</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      The default <guimenu>Load Balance policy</guimenu> is <guimenu>Round
      Robin With Subset</guimenu>. If you prefer a pure fail-over
      configuration, change it to <guimenu>Fail Over Only</guimenu>.
     </para>
    </step>
   </procedure>
   <para>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <guimenu>OK</guimenu> to close the <guimenu>iSCSI
    Initiator Properties</guimenu> dialog, and proceed with the<guimenu> File
    and Storage Services</guimenu> role from the <guimenu>Server
    Manager</guimenu> dashboard.
   </para>
   <para>
    Observe the newly connected volume. It identifies as <emphasis>SUSE RBD
    SCSI Multi-Path Drive</emphasis> on the iSCSI bus, and is initially marked
    with an <emphasis>Offline</emphasis> status and a partition table type of
    <emphasis>Unknown</emphasis>. If the new volume does not appear
    immediately, select <guimenu>Rescan Storage</guimenu> from the
    <guimenu>Tasks</guimenu> drop-down box to rescan the iSCSI bus.
   </para>
   <procedure>
    <step>
     <para>
      Right-click on the iSCSI volume and select <guimenu>New Volume</guimenu>
      from the context menu. The <guimenu>New Volume Wizard</guimenu> appears.
      Click <guimenu>Next</guimenu>, highlight the newly connected iSCSI volume
      and click <guimenu>Next</guimenu> to begin.
     </para>
     <figure>
      <title>New Volume Wizard</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </para>
     <figure>
      <title>Offline Disk Prompt</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or folder name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <guimenu>Create</guimenu> to finish creating the volume:
     </para>
     <figure>
      <title>Confirm Volume Selections</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When the process finishes, review the results, then
      <guimenu>Close</guimenu> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.vmware">
   <title>&vmware;</title>
   <para></para>
   <procedure>
    <step>
     <para>
      To connect to <systemitem>lrbd</systemitem> managed iSCSI volumes you
      need a configured iSCSI software adapter. If no such adapter is available
      in your vSphere configuration, create one by selecting
      <menuchoice><guimenu>Configuration</guimenu><guimenu>Storage
      Adapters</guimenu> <guimenu>Add</guimenu><guimenu>iSCSI Software
      initiator</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <guimenu>Properties</guimenu> from the context
      menu:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>iSCSI Software Initiator</guimenu> dialog, click the
      <guimenu>Configure</guimenu> button. Then go to the <guimenu>Dynamic
      Discovery</guimenu> tab and select <guimenu>Add</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the IP address or host name of your <systemitem>lrbd</systemitem>
      iSCSI gateway. If you run multiple iSCSI gateways in a failover
      configuration, repeat this step for as many gateways as you operate.
     </para>
     <figure>
      <title>Add Target Server</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When you have entered all iSCSI gateways, click <guimenu>OK</guimenu> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </para>
    </step>
    <step>
     <para>
      When the rescan completes, the new iSCSI device appears below the
      <guimenu>Storage Adapters</guimenu> list in the
      <guimenu>Details</guimenu> pane. For multipath devices, you can now
      right-click on the adapter and select <guimenu>Manage Paths</guimenu>
      from the context menu:
     </para>
     <figure>
      <title>Manage Multipath Devices</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      You should now see all paths with a green light under
      <guimenu>Status</guimenu>. One of your paths should be marked
      <guimenu>Active (I/O)</guimenu> and all others simply
      <guimenu>Active</guimenu>:
     </para>
     <figure>
      <title>Paths Listing for Multipath</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      You can now switch from <guimenu>Storage Adapters</guimenu> to the item
      labeled <guimenu>Storage</guimenu>. Select <guimenu>Add
      Storage...</guimenu> in the top-right corner of the pane to bring up the
      <guimenu>Add Storage</guimenu> dialog. Then, select
      <guimenu>Disk/LUN</guimenu> and click <guimenu>Next</guimenu>. The newly
      added iSCSI device appears in the <guimenu>Select Disk/LUN</guimenu>
      list. Select it, then click <guimenu>Next</guimenu> to proceed:
     </para>
     <figure>
      <title>Add Storage Dialog</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Next</guimenu> to accept the default disk layout.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Properties</guimenu> pane, assign a name to the new
      datastore, and click <guimenu>Next</guimenu>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <guimenu>Custom Space Setting</guimenu> for a smaller datastore:
     </para>
     <figure>
      <title>Custom Space Setting</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Finish</guimenu> to complete the datastore creation.
     </para>
     <para>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the
      <systemitem>lrbd</systemitem>-backed iSCSI volume like any other vSphere
      datastore.
     </para>
     <figure>
      <title>iSCSI Datastore Overview</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.conclude">
  <title>Conclusion</title>

  <para>
   <systemitem>lrbd</systemitem> is a key component of &storage; that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using
   <systemitem>lrbd</systemitem> on one or more iSCSI gateway hosts, &ceph; RBD
   images become available as Logical Units (LUs) associated with iSCSI
   targets, which can be accessed in an optionally load-balanced, highly
   available fashion.
  </para>

  <para>
   Since all of <systemitem>lrbd</systemitem>'s configuration is stored in the
   &ceph; RADOS object store, <systemitem>lrbd</systemitem> gateway hosts are
   inherently without persistent state and thus can be replaced, augmented, or
   reduced at will. As a result, &storage; enables SUSE customers to run a
   truly distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open-source
   platform.
  </para>
 </sect1>
</chapter>
