<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.iscsi">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>&ceph; iSCSI Gateway</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  The chapter focuses on administration tasks related to the iSCSI Gateway. For
  a procedure of deployment refer to <xref linkend="cha.ceph.as.iscsi"/>.
  <remark role="fixme">Can YaST be used for iSCSI?</remark>
 </para>
 <sect1 xml:id="ceph.iscsi.connect">
  <title>Connecting to lrbd-managed Targets</title>

  <para>
   This chapter describes how to connect to lrdb-managed targets from clients
   running Linux, &mswin;, or &vmware;.
  </para>

  <sect2 xml:id="ceph.iscsi.connect.linux">
   <title>Linux (<systemitem>open-iscsi</systemitem>)</title>
   <para>
    Connecting to lrbd-backed iSCSI targets with
    <systemitem>open-iscsi</systemitem> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </para>
   <para>
    Both steps require that the <systemitem>open-iscsi</systemitem> daemon is
    running. The way you start the <systemitem>open-iscsi</systemitem> daemon
    is dependent on your Linux distribution:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      On &sls; (SLES); and &rhel; (RHEL) hosts, run <command>systemctl start
      iscsid</command> (or <command>service iscsid start</command> if
      <command>systemctl</command> is not available).
     </para>
    </listitem>
    <listitem>
     <para>
      On Debian and Ubuntu hosts, run <command>systemctl start
      open-iscsi</command> (or <command>service open-iscsi start</command>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If your initiator host runs &sls;, refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/sec_iscsi_initiator.html"/>
    or
    <link xlink:href="https://www.suse.com/documentation/sles11/stor_admin/data/sec_inst_system_iscsi_initiator.html"/>
    for details on how to connect to an iSCSI target.
   </para>
   <para>
    For any other Linux distribution supporting
    <systemitem>open-iscsi</systemitem>, proceed to discover targets on your
    <systemitem>lrbd</systemitem> gateway (this example uses iscsi1.example.com
    as the portal address; for multipath access repeat these steps with
    iscsi2.example.com):
   </para>
<screen>&prompt.root;iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</screen>
   <para>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available on
    the system SCSI bus:
   </para>
<screen>&prompt.root;iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    Repeat this process for other portal IP addresses or hosts.
   </para>
   <para>
    If your system has the <systemitem>lsscsi</systemitem> utility installed,
    you use it to enumerate available SCSI devices on your system:
   </para>
<screen>lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</screen>
   <para>
    In a multipath configuration (where two connected iSCSI devices represent
    one and the same LU), you can also examine the multipath device state with
    the <systemitem>multipath</systemitem> utility:
   </para>
<screen>&prompt.root;multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</screen>
   <para>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it. The
    example below demonstrates how to create an XFS file system on the newly
    connected multipath iSCSI volume:
   </para>
<screen>&prompt.root;mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   <para>
    Note that XFS being a non-clustered file system, you may only ever mount it
    on a single iSCSI initiator node at any given time.
   </para>
   <para>
    If at any time you want to discontinue using the iSCSI LUs associated with
    a particular target, run the following command:
   </para>
<screen>&prompt.root;iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or host names.
   </para>
   <sect3 xml:id="ceph.iscsi.connect.linux.multipath">
    <title>Multipath Configuration</title>
    <para>
     The multipath configuration is maintained on the clients or initiators and
     is independent of any <systemitem>lrbd</systemitem> configuration. Select
     a strategy prior to using block storage. After editing the
     <filename>/etc/multipath.conf</filename>, restart
     <systemitem>multipathd</systemitem> with
    </para>
<screen>&prompt.root;systemctl restart multipathd</screen>
    <para>
     For an active-passive configuration with friendly names, add
    </para>
<screen>defaults {
  user_friendly_names yes
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. After connecting to your
     targets successfully, run
    </para>
<screen>&prompt.root;multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-0 SUSE,RBD
size=2.0G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 2:0:0:3 sdl 8:176 active ready running
|-+- policy='service-time 0' prio=1 status=enabled
| `- 3:0:0:3 sdj 8:144 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
  `- 4:0:0:3 sdk 8:160 active ready running</screen>
    <para>
     Note the status of each link. For an active-active configuration, add
    </para>
<screen>defaults {
  user_friendly_names yes
}

devices {
  device {
    vendor "(LIO-ORG|SUSE)"
    product "RBD"
    path_grouping_policy "multibus"
    path_checker "tur"
    features "0"
    hardware_handler "1 alua"
    prio "alua"
    failback "immediate"
    rr_weight "uniform"
    no_path_retry 12
    rr_min_io 100
  }
}</screen>
    <para>
     to your <filename>/etc/multipath.conf</filename>. Restart
     <systemitem>multipathd</systemitem> and run
    </para>
<screen>&prompt.root;multipath -ll
mpathd (36001405dbb561b2b5e439f0aed2f8e1e) dm-3 SUSE,RBD
size=2.0G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
`-+- policy='service-time 0' prio=50 status=active
  |- 4:0:0:3 sdj 8:144 active ready running
  |- 3:0:0:3 sdk 8:160 active ready running
  `- 2:0:0:3 sdl 8:176 active ready running</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.win">
   <title>&mswin; (Microsoft iSCSI initiator)</title>
   <para>
    To connect to a &storage; iSCSI target from a Windows 2012 server, follow
    these steps:
   </para>
   <procedure>
    <step>
     <para>
      Open Windows Server Manager. From the Dashboard, select
      <menuchoice><guimenu>Tools</guimenu><guimenu>iSCSI
      Initiator</guimenu></menuchoice>. The <guimenu>iSCSI Initiator
      Properties</guimenu> dialog appears. Select the
      <guimenu>Discovery</guimenu> tab:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>Discover Target Portal</guimenu> dialog, enter the
      target's host name or IP address in the <guimenu>Target</guimenu> field
      and click <guimenu>OK</guimenu>:
     </para>
     <figure>
      <title>Discover Target Portal</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Repeat this process for all other gateway host names or IP addresses.
      When completed, review the <guimenu>Target Portals</guimenu> list:
     </para>
     <figure>
      <title>Target Portals</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Next, switch to the <guimenu>Targets</guimenu> tab and review your
      discovered target(s).
     </para>
     <figure>
      <title>Targets</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Click <guimenu>Connect</guimenu> in the <guimenu>Targets</guimenu> tab.
      The <guimenu>Connect To Target</guimenu> dialog appears. Select the
      <guimenu>Enable Multi-path</guimenu> check box to enable multipath I/O
      (MPIO), then click <guimenu>OK</guimenu>:
     </para>
    </step>
    <step>
     <para>
      When the <guimenu>Connect to Target</guimenu> dialog closes, select
      <guimenu>Properties</guimenu> to review the target's properties:
     </para>
     <figure>
      <title>iSCSI Target Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select <guimenu>Devices</guimenu>, and click <guimenu>MPIO</guimenu> to
      review the multipath I/O configuration:
     </para>
     <figure>
      <title>Device Details</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      The default <guimenu>Load Balance policy</guimenu> is <guimenu>Round
      Robin With Subset</guimenu>. If you prefer a pure fail-over
      configuration, change it to <guimenu>Fail Over Only</guimenu>.
     </para>
    </step>
   </procedure>
   <para>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are now
    available like any other SCSI devices, and may be initialized for use as
    volumes and drives. Click <guimenu>OK</guimenu> to close the <guimenu>iSCSI
    Initiator Properties</guimenu> dialog, and proceed with the<guimenu> File
    and Storage Services</guimenu> role from the <guimenu>Server
    Manager</guimenu> dashboard.
   </para>
   <para>
    Observe the newly connected volume. It identifies as <emphasis>SUSE RBD
    SCSI Multi-Path Drive</emphasis> on the iSCSI bus, and is initially marked
    with an <emphasis>Offline</emphasis> status and a partition table type of
    <emphasis>Unknown</emphasis>. If the new volume does not appear
    immediately, select <guimenu>Rescan Storage</guimenu> from the
    <guimenu>Tasks</guimenu> drop-down box to rescan the iSCSI bus.
   </para>
   <procedure>
    <step>
     <para>
      Right-click on the iSCSI volume and select <guimenu>New Volume</guimenu>
      from the context menu. The <guimenu>New Volume Wizard</guimenu> appears.
      Click <guimenu>Next</guimenu>, highlight the newly connected iSCSI volume
      and click <guimenu>Next</guimenu> to begin.
     </para>
     <figure>
      <title>New Volume Wizard</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </para>
     <figure>
      <title>Offline Disk Prompt</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or directory name where the newly
      created volume will become available. Then select a file system to create
      on the new volume, and finally confirm your selections with
      <guimenu>Create</guimenu> to finish creating the volume:
     </para>
     <figure>
      <title>Confirm Volume Selections</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When the process finishes, review the results, then
      <guimenu>Close</guimenu> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system) becomes
      available like a newly initialized local drive.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.vmware">
   <title>&vmware;</title>
   <para/>
   <procedure>
    <step>
     <para>
      To connect to <systemitem>lrbd</systemitem> managed iSCSI volumes you
      need a configured iSCSI software adapter. If no such adapter is available
      in your vSphere configuration, create one by selecting
      <menuchoice><guimenu>Configuration</guimenu><guimenu>Storage
      Adapters</guimenu> <guimenu>Add</guimenu><guimenu>iSCSI Software
      initiator</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      When available, select the adapter's properties by right-clicking the
      adapter and selecting <guimenu>Properties</guimenu> from the context
      menu:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>iSCSI Software Initiator</guimenu> dialog, click the
      <guimenu>Configure</guimenu> button. Then go to the <guimenu>Dynamic
      Discovery</guimenu> tab and select <guimenu>Add</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the IP address or host name of your <systemitem>lrbd</systemitem>
      iSCSI gateway. If you run multiple iSCSI gateways in a failover
      configuration, repeat this step for as many gateways as you operate.
     </para>
     <figure>
      <title>Add Target Server</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When you have entered all iSCSI gateways, click <guimenu>OK</guimenu> in
      the dialog to initiate a rescan of the iSCSI adapter.
     </para>
    </step>
    <step>
     <para>
      When the rescan completes, the new iSCSI device appears below the
      <guimenu>Storage Adapters</guimenu> list in the
      <guimenu>Details</guimenu> pane. For multipath devices, you can now
      right-click on the adapter and select <guimenu>Manage Paths</guimenu>
      from the context menu:
     </para>
     <figure>
      <title>Manage Multipath Devices</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      You should now see all paths with a green light under
      <guimenu>Status</guimenu>. One of your paths should be marked
      <guimenu>Active (I/O)</guimenu> and all others simply
      <guimenu>Active</guimenu>:
     </para>
     <figure>
      <title>Paths Listing for Multipath</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      You can now switch from <guimenu>Storage Adapters</guimenu> to the item
      labeled <guimenu>Storage</guimenu>. Select <guimenu>Add
      Storage...</guimenu> in the top-right corner of the pane to bring up the
      <guimenu>Add Storage</guimenu> dialog. Then, select
      <guimenu>Disk/LUN</guimenu> and click <guimenu>Next</guimenu>. The newly
      added iSCSI device appears in the <guimenu>Select Disk/LUN</guimenu>
      list. Select it, then click <guimenu>Next</guimenu> to proceed:
     </para>
     <figure>
      <title>Add Storage Dialog</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Next</guimenu> to accept the default disk layout.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Properties</guimenu> pane, assign a name to the new
      datastore, and click <guimenu>Next</guimenu>. Accept the default setting
      to use the volume's entire space for the datastore, or select
      <guimenu>Custom Space Setting</guimenu> for a smaller datastore:
     </para>
     <figure>
      <title>Custom Space Setting</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Finish</guimenu> to complete the datastore creation.
     </para>
     <para>
      The new datastore now appears in the datastore list and you can select it
      to retrieve details. You are now able to use the
      <systemitem>lrbd</systemitem>-backed iSCSI volume like any other vSphere
      datastore.
     </para>
     <figure>
      <title>iSCSI Datastore Overview</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.conclude">
  <title>Conclusion</title>

  <para>
   <systemitem>lrbd</systemitem> is a key component of &storage; that enables
   access to distributed, highly available block storage from any server or
   client capable of speaking the iSCSI protocol. By using
   <systemitem>lrbd</systemitem> on one or more iSCSI gateway hosts, &ceph; RBD
   images become available as Logical Units (LUs) associated with iSCSI
   targets, which can be accessed in an optionally load-balanced, highly
   available fashion.
  </para>

  <para>
   Since all of <systemitem>lrbd</systemitem>'s configuration is stored in the
   &ceph; RADOS object store, <systemitem>lrbd</systemitem> gateway hosts are
   inherently without persistent state and thus can be replaced, augmented, or
   reduced at will. As a result, &storage; enables SUSE customers to run a
   truly distributed, highly-available, resilient, and self-healing enterprise
   storage technology on commodity hardware and an entirely open source
   platform.
  </para>
 </sect1>
</chapter>
