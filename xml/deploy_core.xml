<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-core">
 <info>
  <title>Deploying the remaining core services using &cephadm;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  After deploying the basic &ceph; cluster, deploy core services to more
  cluster nodes. To make the cluster data accessible to clients, deploy
  additional services as well.
 </para>
 <para>
  Currently, we support deployment of &ceph; services on the command line by
  using the &ceph; orchestrator (<command>ceph orch</command> subcommands).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>The <command>ceph orch</command> command</title>

  <para>
   The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
   an interface to the &cephadm; module&mdash;will take care of listing cluster
   components and deploying &ceph; services on new cluster nodes.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Displaying the orchestrator status</title>
   <para>
    The following command shows the current mode and status of the &ceph;
    orchestrator.
   </para>
<screen>&prompt.cephuser;ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Listing devices, services, and daemons</title>
   <para>
    To list all disk devices, run the following:
   </para>
<screen>
&prompt.cephuser;ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-main   /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-node1  /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-node1  /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>Services and daemons</title>
    <para>
     <emphasis>Service</emphasis> is a general term for a &ceph; service of a
     specific type, for example &mgr;.
    </para>
    <para>
     <emphasis>Daemon</emphasis> is a specific instance of a service, for
     example, a process <literal>mgr.ses-node1.gdlcik</literal> running on a
     node called <literal>ses-node1</literal>.
    </para>
   </tip>
   <para>
    To list all services known to &cephadm;, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     You can limit the list to services on a particular node with the optional
     <option>-â€“host</option> parameter, and services of a particular type with
     the optional <option>--service-type</option> parameter. Acceptable types
     are <literal>mon</literal>, <literal>osd</literal>,
     <literal>mgr</literal>, <literal>mds</literal>, and
     <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    To list all running daemons deployed by &cephadm;, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME             HOST      STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-node1.gd ses-node1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-node1    ses-node1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     To query the status of a particular daemon, use
     <option>--daemon_type</option> and <option>--daemon_id</option>. For OSDs,
     the ID is the numeric OSD ID. For MDS, the ID is the file system name:
    </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Service and placement specification</title>

  <para>
   The recommended way to specify the deployment of &ceph; services is to
   create a YAML-formatted file with the specification of the services that you
   intend to deploy.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Creating service specifications</title>
   <para>
    You can create a separate specification file for each type of service, for
    example:
   </para>
<screen>
&prompt.smaster;cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Alternatively, you can specify multiple (or all) service types in one
    file&mdash;for example, <filename>cluster.yml</filename>&mdash;that
    describes which nodes will run specific services. Remember to separate
    individual service types with three dashes (<literal>---</literal>):
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
---
[...]
</screen>
   <para>
    The aforementioned properties have the following meaning:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       The type of the service. It can be either a &ceph; service
       (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>,
       <literal>crash</literal>, <literal>osd</literal>, or
       <literal>rbd-mirror</literal>), a gateway (<literal>nfs</literal> or
       <literal>rgw</literal>), or part of the monitoring stack
       (<literal>alertmanager</literal>, <literal>grafana</literal>,
       <literal>node-exporter</literal>, or <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       The name of the service. Specifications of type <literal>mon</literal>,
       <literal>mgr</literal>, <literal>alertmanager</literal>,
       <literal>grafana</literal>, <literal>node-exporter</literal>, and
       <literal>prometheus</literal> do not require the
       <literal>service_id</literal> property.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Specifies which nodes will be running the service. Refer to
       <xref linkend="cephadm-placement-specs"/> for more details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Additional specification relevant for the service type.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Applying specific services</title>
    <para>
     &ceph; cluster services have usually a number of properties specific to
     them. For examples and details of individual services' specification,
     refer to <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Creating placement specification</title>
   <para>
    To deploy &ceph; services, &cephadm; needs to know on which nodes to deploy
    them. Use the <literal>placement</literal> property and list the short host
    names of the nodes that the service applies to:
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
   <sect3 xml:id="cephadm-placement-specs-label">
    <title>Placement by labels</title>
    <para>
     You can limit the placement of &ceph; services to hosts that match a
     specific <emphasis>label</emphasis>. In the following example, we create a
     label <literal>prometheus</literal>, assign it to specific hosts, and
     apply the service &prometheus; to the group of hosts with the
     <literal>prometheus</literal> label.
    </para>
    <procedure>
     <step>
      <para>
       Add the <literal>prometheus</literal> label to <literal>host1</literal>,
       <literal>host2</literal>, and <literal>host3</literal>:
      </para>
<screen>
&prompt.cephuser;ceph orch host label add host1 prometheus
&prompt.cephuser;ceph orch host label add host2 prometheus
&prompt.cephuser;ceph orch host label add host3 prometheus
</screen>
     </step>
     <step>
      <para>
       Verify that the labels were correctly assigned:
      </para>
<screen>
&prompt.cephuser;ceph orch host ls
HOST   ADDR   LABELS      STATUS
host1         prometheus
host2         prometheus
host3         prometheus
[...]
</screen>
     </step>
     <step>
      <para>
       Create a placement specification using the <literal>label</literal>
       property:
      </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
placement:
  label: "prometheus"
[...]
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Applying cluster specification</title>
   <para>
    After you have created a full <filename>cluster.yml</filename> file with
    specifications of all services and their placement, you can apply the
    cluster by running the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yml</screen>
   <para>
    To view the status of the cluster, run the <command>ceph orch
    status</command> command. For more details, see
    <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exporting the specification of a running cluster</title>
   <para>
    Although you deployed services to the &ceph; cluster by using the
    specification files as described in
    <xref
      linkend="cephadm-service-and-placement-specs" />, the
    configuration of the cluster may diverge from the original specification
    during its operation. Also, you may have removed the specification files
    accidentally.
   </para>
   <para>
    To retrieve a complete specification of a running cluster, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ls --export
placement:
  hosts:
  - hostname: ses-node1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     You can append the <option>--format</option> option to change the default
     <literal>yaml</literal> output format. You can select from
     <literal>json</literal>, <literal>json-pretty</literal>, or
     <literal>yaml</literal>. For example:
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Deploy &ceph; services</title>

  <para>
   After the basic cluster is running, you can deploy &ceph; services to
   additional nodes.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Deploying &mon;s and &mgr;s</title>
   <para>
    &ceph; cluster has three or five MONs deployed across different nodes. If
    there are five or more nodes in the cluster, we recommend deploying five
    MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
   </para>
   <important>
    <title>Include Bootstrap MON</title>
    <para>
     When deploying MONs and MGRs, remember to include the first MON that you
     added when configuring the basic cluster in
     <xref linkend="deploy-cephadm-configure-mon"/>.
    </para>
   </important>
   <para>
    To deploy MONs, apply the following specification:
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
</screen>
   <note>
    <para>
     If you need to add another node, append the host name to the same YAML
     list. For example:
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-node1
 - ses-node2
 - ses-node3
 - ses-node4
</screen>
   </note>
   <para>
    Similarly, to deploy MGRs, apply the following specification:
   </para>
   <important>
    <para>
     Ensure your deployment has at least three &mgr;s in each deployment.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
</screen>
   <tip>
    <para>
     If MONs or MGRs are <emphasis>not</emphasis> on the same subnet, you need
     to append the subnet addresses. For example:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-node1:10.1.2.0/24
  - ses-node2:10.1.5.0/24
  - ses-node3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Deploying &osd;s</title>
   <important>
    <title>When Storage Device is Available</title>
    <para>
     A storage device is considered <emphasis>available</emphasis> if all of
     the following conditions are met:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The device has no partitions.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not have any LVM state.
      </para>
     </listitem>
     <listitem>
      <para>
       The device is not be mounted.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not contain a file system.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not contain a &bluestore; OSD.
      </para>
     </listitem>
     <listitem>
      <para>
       The device is larger than 5&nbsp;GB.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If the above conditions are not met, &ceph; refuses to provision such
     OSDs.
    </para>
   </important>
   <para>
    There are two ways you can deploy OSDs:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Tell &ceph; to consume all available and unused storage devices:
     </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Use &drvgrps; (see <xref linkend="drive-groups"/>) to create OSD
      specification describing devices that will be deployed based on their
      properties, such as device type (SSD or HDD), device model names, size,
      or the nodes on which the devices exist. Then apply the specification by
      running the following command:
     </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Deploying &mds;s</title>
   <para>
    &cephfs; requires one or more &mds; (MDS) services. To create a &cephfs;,
    first create MDS servers by applying the following specification:
   </para>
   <note>
    <para>
     Ensure you have at least two pools, one for &cephfs; data and one for
     &cephfs; metadata, created before applying the following specification.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
</screen>
   <para>
    After MDSs are functional, create the &cephfs;:
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Deploying &ogw;s</title>
   <para>
    &cephadm; deploys an &ogw; as a collection of daemons that manage a
    particular <emphasis>realm</emphasis> and <emphasis>zone</emphasis>.
   </para>
   <para>
    You can either relate an &ogw; service to already existing realm and zone,
    (refer to <xref linkend="ceph-rgw-fed"/> for more details), or you can
    specify a non-existing <replaceable>REALM_NAME</replaceable> and
    <replaceable>ZONE_NAME</replaceable> and they will be created automatically
    after you apply the following configuration:
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Using secure SSL access</title>
    <para>
     To use a secure SSL connection to the &ogw;, you need a file containing
     both the valid SSL certificate and the key (see
     <xref linkend="ceph-rgw-https"/> for more details). You need to enable
     SSL, specify a port number for SSL connections, and the SSL certificate
     file.
    </para>
    <para>
     To enable SSL and specify the port number, include the following in your
     specification:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     To specify the SSL certificate and key, you can paste their contents
     directly into the YAML specification file. The pipe sign
     (<literal>|</literal>) at the end of line tells the parser to expect a
     multi-line string as a value. For example:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      Instead of pasting the content of an SSL certificate file, you can omit
      the <literal>rgw_frontend_ssl_certificate:</literal> directive and upload
      the certificate file to the configuration database:
     </para>
<screen>
&prompt.cephuser;ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>Configure the &ogw; to listen on both ports 443 and 80</title>
     <para>
      To configure the &ogw; to listen on both ports 443 (HTTPS) and 80 (HTTP),
      follow these steps:
     </para>
     <note>
      <para>
       The commands in the procedure use realm and zone
       <literal>default</literal>.
      </para>
     </note>
     <procedure>
      <step>
       <para>
        Deploy the &ogw; by supplying a specification file. Refer to
        <xref
         linkend="deploy-cephadm-day2-service-ogw"/> for more
        details on the &ogw; specification. Use the following command:
       </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        If SSL certificates are not supplied in the specification file, add
        them by using the following command:
       </para>
<screen>
&prompt.cephuser;ceph config-key set rgw/cert/default/default.crt -i certificate.pem
&prompt.cephuser;ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        Change the default value of the <option>rgw_frontends</option> option:
       </para>
<screen>
&prompt.cephuser;ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      <step>
       <para>
        Remove the specific configuration created by &cephadm;. Identify for
        which target the <option>rgw_frontends</option> option was configured
        by running the command:
       </para>
<screen>&prompt.cephuser;ceph config dump | grep rgw</screen>
       <para>
        For example, the target is
        <literal>client.rgw.default.default.node4.yiewdu</literal>. Remove the
        current specific <option>rgw_frontends</option> value:
       </para>
<screen>&prompt.cephuser;ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         Instead of removing a value for <option>rgw_frontends</option>, you
         can specify it. For example:
        </para>
<screen>
&prompt.cephuser;ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      <step>
       <para>
        Restart &ogw;s:
       </para>
<screen>&prompt.cephuser;ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Deploying with a subcluster</title>
    <para>
     <emphasis>Subclusters</emphasis> help you organize the nodes in your
     clusters to isolate workloads and make elastic scaling easier. If you are
     deploying with a subcluster, apply the following configuration:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
   <sect3 xml:id="ceph-ogw-ha">
    <title>Deploying &ha; for the &ogw;</title>
    <para>
     You can create high availability (HA) endpoints for the &ogw; service by
     deploying &ingress;. &ingress; includes a combination of &haproxy; and
     &keepalived; to provide load balancing on a floating virtual IP address. A
     virtual IP address is automatically configured only on one of &ingress;
     hosts at a time.
    </para>
    <para>
     Each &keepalived; daemon checks every few seconds whether the &haproxy;
     daemon on the same host is responding. &keepalived; also checks whether
     the master &keepalived; daemon is running without problems. If the master
     &keepalived; or the active &haproxy; daemon is not responding, one of the
     remaining &keepalived; daemons running in backup mode is elected as the
     master daemon, and the virtual IP address will be assigned to that node.
    </para>
    <figure>
     <title>&ha; setup for &ogw;</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="ses-rgw-ha.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="ses-rgw-ha.png" width="100%"/>
      </imageobject>
      <textobject role="description"><phrase>&ha; setup for &ogw;</phrase>
      </textobject>
     </mediaobject>
    </figure>
    <important>
     <para>
      If your &ogw; deployment uses an SSL connection, disable it and configure
      &ingress; to use the SSL certificate instead of the &ogw; node.
     </para>
    </important>
    <procedure>
     <step>
      <para>
       Create specifications for the &ogw; and &ingress; services, for example:
      </para>
<screen>&prompt.user;cat rgw.yaml
service_type: rgw
service_id: myrealm.myzone
placement:
  hosts:
    - ses-node1
    - ses-node2
    - ses-node3
[...]
</screen>
<screen>&prompt.user;cat ingress.yaml
service_type: ingress
service_id: rgw.myrealm.myzone
placement:
  hosts:
    - ingress-host1
    - ingress-host2
    - ingress-host3
spec:
  backend_service: rgw.myrealm.myzone
  virtual_ip: 192.168.20.1/24
  frontend_port: 8080
  monitor_port: 1967     # used by HAproxy for load balancer status
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
    -----BEGIN PRIVATE KEY-----
    ...
    -----END PRIVATE KEY-----
</screen>
     </step>
     <step>
      <para>
       Apply the specifications:
      </para>
<screen>
&prompt.cephuser;ceph orch apply -i rgw.yaml
&prompt.cephuser;ceph orch apply -i ingress.yaml
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Deploying &igw;s</title>
   <para>
    &cephadm; deploys an &igw; which is a storage area network (SAN) protocol
    that allows clients (called initiators) to send SCSI commands to SCSI
    storage devices (targets) on remote servers.
   </para>
   <para>
    Apply the following configuration to deploy. Ensure
    <literal>trusted_ip_list</literal> contains the IP addresses of all &igw;
    and &mgr; nodes (see the example output below).
   </para>
   <note>
    <para>
     Ensure the pool is created before applying the following specification.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
  - ses-node3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Ensure the IPs listed for <literal>trusted_ip_list</literal> do
     <emphasis>not</emphasis> have a space after the comma separation.
    </para>
   </note>
   <sect3>
    <title>Secure SSL configuration</title>
    <para>
     To use a secure SSL connection between the &dashboard; and the &iscsi;
     target API, you need a pair of valid SSL certificate and key files. These
     can be either CA-issued or self-signed (see
     <xref linkend="self-sign-certificates"/>). To enable SSL, include
     the <literal>api_secure: true</literal> setting in your specification
     file:
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     To specify the SSL certificate and key, you can paste the content directly
     into the YAML specification file. The pipe sign (<literal>|</literal>) at
     the end of line tells the parser to expect a multi-line string as a value.
     For example:
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Deploying &ganesha;</title>
    &ganesha_support;
    <para>
    &cephadm; deploys &ganesha; using a standard pool <filename>.nfs</filename>
    instead of a user-predefined pool. To deploy &ganesha;, apply the following
    specification:
   </para>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-node1
  - ses-node2
</screen>
   <para>
    <replaceable>EXAMPLE_NFS</replaceable> should be replaced with an arbitrary
    string that identifies the NFS export.
   </para>
   <para>
    Enable the <literal>nfs</literal> module in the &mgr; daemon. This allows
    defining NFS exports through the NFS section in the &dashboard;:
   </para>
<screen>&prompt.cephuser;ceph mgr module enable nfs</screen>
   <sect3 xml:id="ceph-nfs-ha">
    <title>Deploying &ha; for the &ganesha;</title>
    <para>
     You can create high availability (HA) endpoints for the &ganesha;
     service by deploying &ingress;. &ingress; includes a combination of
     &haproxy; and &keepalived; to provide load balancing on a floating virtual
     IP address. A virtual IP address is automatically configured only on one
     of &ingress; hosts at a time.
    </para>
    <para>
     Each &keepalived; daemon checks every few seconds whether the &haproxy;
     daemon on the same host is responding. &keepalived; also checks whether
     the master &keepalived; daemon is running without problems. If the master
     &keepalived; or the active &haproxy; daemon is not responding, one of the
     remaining &keepalived; daemons running in backup mode is elected as the
     master daemon, and the virtual IP address will be assigned to that node.
    </para>
    <figure>
     <title>&ha; setup for &ganesha;</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="ses-nfs-ha.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="ses-nfs-ha.png" width="100%"/>
      </imageobject>
      <textobject role="description"><phrase>&ha; setup for &ganesha;</phrase>
      </textobject>
     </mediaobject>
    </figure>
    <procedure>
     <step>
      <para>
       Ensure that the following sysctl variables are set on each &ingress;
       host:
      </para>
<screen>
net.ipv4.ip_nonlocal_bind=1
net.ipv4.ip_forward=1
</screen>
      <tip>
       <para>
        You can persistently set the sysctl variables by creating a file
        <filename>/etc/sysctl.d/90-ingress.conf</filename> and including the
        above variables. To apply the changes, run <command>sysctl
        --system</command> or reboot the host.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Create specifications for the &ingress; and &ganesha; services, for
       example:
      </para>
<screen>&prompt.user;cat nfs.yaml
service_type: nfs
service_id: nfs-ha
placement:
  hosts:
  - nfs-node1
  - nfs-node2
  - nfs-node3
spec:
  port: 12049
</screen>
<screen>&prompt.user;cat ingress.yaml
service_type: ingress
service_id: nfs.nfs-ha
placement:
  hosts:
  - ingress-node1
  - ingress-node2
  - ingress-node3
spec:
  backend_service: nfs.nfs-ha
  frontend_port: 2049
  monitor_port: 9049
  virtual_ip: 10.20.27.111
</screen>
      <warning>
       <para>
        We recommend running at least three &ganesha; daemons, otherwise,
        mounted clients may experience excessively long hangs during failovers.
       </para>
      </warning>
     </step>
     <step>
      <para>
       Apply the customized service specifications:
      </para>
<screen>
&prompt.cephuser;ceph orch apply -i nfs.yaml
&prompt.cephuser;ceph orch apply -i ingress.yaml
</screen>
     </step>
     <step>
      <para>
       Create a &ganesha; export. Find more details in
       <xref
       linkend="dash-webui-nfs-create"/>. Remember to select the
       &ha; &ganesha; cluster and an appropriate storage back-end and pseudo
       path.
      </para>
      <para>
       Alternatively, create the export from the command line on the MGR node:
      </para>
<screen>
&prompt.cephuser;ceph nfs export create \
 <replaceable>STORAGE_BACKEND</replaceable> \
 <replaceable>NFS_HA_CLUSTER</replaceable> \
 <replaceable>NFS_PSEUDO_PATH</replaceable> \
 <replaceable>STORAGE_BACKEND_INSTANCE</replaceable> \
 <replaceable>EXPORT_PATH</replaceable>
</screen>
     </step>
     <step>
      <para>
       Mount the &ganesha; share on your clients:
      </para>
<screen>
&prompt.root;mount -t nfs \
<replaceable>VIRTUAL_IP</replaceable>:<replaceable>PORT</replaceable>/<replaceable>NFS_PSEUDO_PATH</replaceable> \
/mnt/nfs
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Deploying &rbdmirror;</title>
   <para>
    The &rbdmirror; service takes care of synchronizing &rbd; images between
    two &ceph; clusters (for more details, see
    <xref
     linkend="ceph-rbd-mirror"/>). To deploy &rbdmirror;, use the
    following specification:
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-node3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Deploying the monitoring stack</title>
   <para>
    The monitoring stack consists of &prometheus;, &prometheus; exporters,
    &prometheus; &alertmanager;, and &grafana;. &dashboard; makes use of these
    components to store and visualize detailed metrics on cluster usage and
    performance.
   </para>
   <tip>
    <para>
     If your deployment requires custom or locally served container images of
     the monitoring stack services, refer to
     <xref
       linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    To deploy the monitoring stack, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Enable the <literal>prometheus</literal> module in the &mgr; daemon. This
      exposes the internal &ceph; metrics so that &prometheus; can read them:
     </para>
<screen>&prompt.cephuser;ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Ensure this command is run before &prometheus; is deployed. If the
       command was not run before the deployment, you must redeploy
       &prometheus; to update &prometheus;' configuration:
      </para>
<screen>&prompt.cephuser;ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Create a specification file (for example
      <filename>monitoring.yaml</filename>) with a content similar to the
      following:
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-node2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-node4
---
service_type: grafana
placement:
  hosts:
  - ses-node3
</screen>
    </step>
    <step>
     <para>
      Apply monitoring services by running:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i monitoring.yaml</screen>
     <para>
      It may take a minute or two for the monitoring services to be deployed.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     &prometheus;, &grafana;, and the &dashboard; are all automatically
     configured to talk to each other, resulting in a fully functional
     &grafana; integration in the &dashboard; when deployed as described above.
    </para>
    <para>
     The only exception to this rule is monitoring with RBD images. See
     <xref linkend="monitoring-rbd-image"/> for more information.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
