<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="deploy-core">
 <info>
  <title>Deploying the remaining core services using &cephadm;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  After deploying the basic &ceph; cluster, deploy core services to more
  cluster nodes. To make the cluster data accessible to clients, deploy
  additional services as well.
 </para>
 <para>
  Currently, we support deployment of &ceph; services on the command line by
  using the &ceph; orchestrator (<command>ceph orch</command> subcommands).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>The <command>ceph orch</command> command</title>

  <para>
   The &ceph; orchestrator command <command>ceph orch</command>&mdash;which is
   an interface to the &cephadm; module&mdash;will take care of listing cluster
   components and deploying &ceph; services on new cluster nodes.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Displaying the orchestrator status</title>
   <para>
    The following command shows the current mode and status of the &ceph;
    orchestrator.
   </para>
<screen>&prompt.cephuser;ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Listing devices, services, and daemons</title>
   <para>
    To list all disk devices, run the following:
   </para>
<screen>
&prompt.cephuser;ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>Services and daemons</title>
    <para>
     <emphasis>Service</emphasis> is a general term for a &ceph; service of a
     specific type, for example &mgr;.
    </para>
    <para>
     <emphasis>Daemon</emphasis> is a specific instance of a service, for
     example a process <literal>mgr.ses-min1.gdlcik</literal> running on a node
     called <literal>ses-min1</literal>.
    </para>
   </tip>
   <para>
    To list all services known to &cephadm;, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec>  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     You can limit the list to services on a particular node with the optional
     <option>-â€“host</option> parameter, and services of a particular type with
     the optional <option>--service-type</option> parameter. Acceptable types
     are <literal>mon</literal>, <literal>osd</literal>,
     <literal>mgr</literal>, <literal>mds</literal>, and
     <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    To list all running daemons deployed by &cephadm;, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     To query the status of a particular daemon, use
     <option>--daemon_type</option> and <option>--daemon_id</option>. For OSDs,
     the ID is the numeric OSD ID. For MDS, the ID is the file system name:
    </para>
<screen>
&prompt.cephuser;ceph orch ps --daemon_type osd --daemon_id 0
&prompt.cephuser;ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Service and placement specification</title>

  <para>
   The recommended way to specify the deployment of &ceph; services is to
   create a YAML-formatted file with the specification of the services that you
   intend to deploy.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Creating service specifications</title>
   <para>
    You can create a separate specification file for each type of service, for
    example:
   </para>
<screen>
&prompt.smaster;cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Alternatively, you can specify multiple (or all) service types in one
    file&mdash;for example, <filename>cluster.yml</filename>&mdash;that
    describes which nodes will run specific services. Remember to separate
    individual service types with three dashes (<literal>---</literal>):
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    The aforementioned properties have the following meaning:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       The type of the service. It can be either a &ceph; service
       (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>,
       <literal>crash</literal>, <literal>osd</literal>, or
       <literal>rbd-mirror</literal>), a gateway (<literal>nfs</literal> or
       <literal>rgw</literal>), or part of the monitoring stack
       (<literal>alertmanager</literal>, <literal>grafana</literal>,
       <literal>node-exporter</literal>, or <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       The name of the service. Specifications of type <literal>mon</literal>,
       <literal>mgr</literal>, <literal>alertmanager</literal>,
       <literal>grafana</literal>, <literal>node-exporter</literal>, and
       <literal>prometheus</literal> do not require the
       <literal>service_id</literal> property.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Specifies which nodes will be running the service. Refer to
       <xref linkend="cephadm-placement-specs"/> for more details.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Additional specification relevant for the service type.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Applying specific services</title>
    <para>
     &ceph; cluster services have usually a number of properties specific to
     them. For examples and details of individual services' specification,
     refer to <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Creating placement specification</title>
   <para>
    To deploy &ceph; services, &cephadm; needs to know on which nodes to deploy
    them. Use the <literal>placement</literal> property and list the short host
    names of the nodes that the service applies to:
   </para>
<screen>
&prompt.cephuser;cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Applying cluster specification</title>
   <para>
    After you have created a full <filename>cluster.yml</filename> file with
    specifications of all services and their placement, you can apply the
    cluster by running the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i cluster.yml</screen>
   <para>
    To view the status of the cluster, run the <command>ceph orch
    status</command> command. For more details, see
    <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exporting the specification of a running cluster</title>
   <para>
    Although you deployed services to the &ceph; cluster by using the
    specification files as described in
    <xref
      linkend="cephadm-service-and-placement-specs" />, the
    configuration of the cluster may diverge from the original specification
    during its operation. Also, you may have removed the specification files
    accidentally.
   </para>
   <para>
    To retrieve a complete specification of a running cluster, run:
   </para>
<screen>
&prompt.cephuser;ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     You can append the <option>--format</option> option to change the default
     <literal>yaml</literal> output format. You can select from
     <literal>json</literal>, <literal>json-pretty</literal>, or
     <literal>yaml</literal>. For example:
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Deploy &ceph; services</title>

  <para>
   After the basic cluster is running, you can deploy &ceph; services to
   additional nodes.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Deploying &mon;s and &mgr;s</title>
   <para>
    &ceph; cluster has three or five MONs deployed across different nodes. If
    there are five or more nodes in the cluster, we recommend deploying five
    MONs. A good practice is to have MGRs deployed on the same nodes as MONs.
   </para>
   <important>
    <title>Include Bootstrap MON</title>
    <para>
     When deploying MONs and MGRs, remember to include the first MON that you
     added when configuring the basic cluster in
     <xref linkend="deploy-cephadm-configure-mon"/>.
    </para>
   </important>
   <para>
    To deploy MONs, apply the following specification:
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     If you need to add another node, append the host name to the same YAML
     list. For example:
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    Similarly, to deploy MGRs, apply the following specification:
   </para>
   <important>
    <para>
     Ensure your deployment has at least three &mgr;s in each deployment.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     If MONs or MGRs are <emphasis>not</emphasis> on the same subnet, you need
     to append the subnet addresses. For example:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Deploying &osd;s</title>
   <important>
    <title>When Storage Device is Available</title>
    <para>
     A storage device is considered <emphasis>available</emphasis> if all of
     the following conditions are met:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The device has no partitions.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not have any LVM state.
      </para>
     </listitem>
     <listitem>
      <para>
       The device is not be mounted.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not contain a file system.
      </para>
     </listitem>
     <listitem>
      <para>
       The device does not contain a &bluestore; OSD.
      </para>
     </listitem>
     <listitem>
      <para>
       The device is larger than 5&nbsp;GB.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     If the above conditions are not met, &ceph; refuses to provision such
     OSDs.
    </para>
   </important>
   <para>
    There are two ways you can deploy OSDs:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Tell &ceph; to consume all available and unused storage devices:
     </para>
<screen>&prompt.cephuser;ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Use &drvgrps; (see <xref linkend="drive-groups"/>) to create OSD
      specification describing devices that will be deployed based on their
      properties, such as device type (SSD or HDD), device model names, size,
      or the nodes on which the devices exist. Then apply the specification by
      running the following command:
     </para>
<screen>&prompt.cephuser;ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Deploying &mds;s</title>
   <para>
    &cephfs; requires one or more &mds; (MDS) services. To create a &cephfs;,
    first create MDS servers by applying the following specification:
   </para>
   <note>
    <para>
     Ensure you have at least two pools, one for &cephfs; data and one for
     &cephfs; metadata, created before applying the following specification.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    After MDSs are functional, create the &cephfs;:
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Deploying &ogw;s</title>
   <para>
    &cephadm; deploys an &ogw; as a collection of daemons that manage a
    particular <emphasis>realm</emphasis> and <emphasis>zone</emphasis>.
   </para>
   <para>
    You can either relate an &ogw; service to already existing realm and zone,
    (refer to <xref linkend="ceph-rgw-fed"/> for more details), or you can
    specify a non-existing <replaceable>REALM_NAME</replaceable> and
    <replaceable>ZONE_NAME</replaceable> and they will be created automatically
    after you apply the following configuration:
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Using secure SSL access</title>
    <para>
     To use a secure SSL connection to the &ogw;, you need a pair of valid SSL
     certificate and key files (see <xref linkend="ceph-rgw-https"/> for more
     details). You need to enable SSL, specify a port number for SSL
     connections, and the SSL certificate and key files.
    </para>
    <para>
     To enable SSL and specify the port number, include the following in your
     specification:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     To specify the SSL certificate and key, you can paste their contents
     directly into the YAML specification file. The pipe sign
     (<literal>|</literal>) at the end of line tells the parser to expect a
     multi-line string as a value. For example:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      Instead of pasting the content of SSL certificate and key files, you can
      omit the <literal>rgw_frontend_ssl_certificate:</literal> and
      <literal>rgw_frontend_ssl_key:</literal> keywords and upload them to the
      configuration database:
     </para>
<screen>
&prompt.cephuser;ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
&prompt.cephuser;ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Deploying with a subcluster</title>
    <para>
     <emphasis>Subclusters</emphasis> help you organize the nodes in your
     clusters to isolate workloads and make elastic scaling easier. If you are
     deploying with a subcluster, apply the following configuration:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Deploying &igw;s</title>
   <para>
    &cephadm; deploys an &igw; which is a storage area network (SAN) protocol
    that allows clients (called initiators) to send SCSI commands to SCSI
    storage devices (targets) on remote servers.
   </para>
   <para>
    Apply the following configuration to deploy. Ensure
    <literal>trusted_ip_list</literal> contains the IP addresses of all &igw;
    and &mgr; nodes (see the example output below).
   </para>
   <note>
    <para>
     Ensure the pool is created before applying the following specification.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Ensure the IPs listed for <literal>trusted_ip_list</literal> do
     <emphasis>not</emphasis> have a space after the comma separation.
    </para>
   </note>
   <sect3>
    <title>Secure SSL configuration</title>
    <para>
     To use a secure SSL connection between the &dashboard; and the &iscsi;
     target API, you need a pair of valid SSL certificate and key files. These
     can be either CA-issued or self-signed (see
     <xref
      linkend="self-sign-certificates"/>). To enable SSL, include
     the <literal>api_secure: true</literal> setting in your specification
     file:
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     To specify the SSL certificate and key, you can paste the content directly
     into the YAML specification file. The pipe sign (<literal>|</literal>) at
     the end of line tells the parser to expect a multi-line string as a value.
     For example:
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Deploying &ganesha;</title>
    &ganesha_support;
    <para>
    &cephadm; deploys &ganesha; using a pre-defined &rados; pool and an
    optional name-space. To deploy &ganesha;, apply the following
    specification:
   </para>
   <note>
    <para>
     You need to have a pre-defined &rados; pool otherwise the <command>ceph
     orch apply</command> operation will fail. For more information on creating
     a pool, see <xref linkend="ceph-pools-operate-add-pool"/>.
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NFS</replaceable> with an arbitrary string that
      identifies the NFS export.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_POOL</replaceable> with the name of the pool where
      the &ganesha; RADOS configuration object will be stored.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NAMESPACE</replaceable> (optional) with the desired
      &ogw; NFS namespace (for example, <literal>ganesha</literal>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Deploying &rbdmirror;</title>
   <para>
    The &rbdmirror; service takes care of synchronizing &rbd; images between
    two &ceph; clusters (for more details, see
    <xref
     linkend="ceph-rbd-mirror"/>). To deploy &rbdmirror;, use the
    following specification:
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Deploying the monitoring stack</title>
   <para>
    The monitoring stack consists of &prometheus;, &prometheus; exporters,
    &prometheus; &alertmanager;, and &grafana;. &dashboard; makes use of these
    components to store and visualize detailed metrics on cluster usage and
    performance.
   </para>
   <tip>
    <para>
     If your deployment requires custom or locally served container images of
     the monitoring stack services, refer to
     <xref
       linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    To deploy the monitoring stack, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Enable the <literal>prometheus</literal> module in the &mgr; daemon. This
      exposes the internal &ceph; metrics so that &prometheus; can read them:
     </para>
<screen>&prompt.cephuser;ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Ensure this command is run before &prometheus; is deployed. If the
       command was not run before the deployment, you must redeploy
       &prometheus; to update &prometheus;' configuration:
      </para>
<screen>&prompt.cephuser;ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Create a specification file (for example
      <filename>monitoring.yaml</filename>) with a content similar to the
      following:
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Apply monitoring services by running:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i monitoring.yaml</screen>
     <para>
      It may take a minute or two for the monitoring services to be deployed.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     &prometheus;, &grafana;, and the &dashboard; are all automatically
     configured to talk to each other, resulting in a fully functional
     &grafana; integration in the &dashboard; when deployed as described above.
    </para>
    <para>
     The only exception to this rule is monitoring with RBD images. See
     <xref linkend="monitoring-rbd-image"/> for more information.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
