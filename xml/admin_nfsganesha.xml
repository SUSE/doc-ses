<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ceph-nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; is an NFS server that runs in a user address space instead of as
  part of the operating system kernel. With &ganesha;, you can plug in your own
  storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client. For installation instructions, see
  <xref linkend ="deploy-cephadm-day2-service-nfs"/>.
 </para>
 <note>
  <title>&ganesha; performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &ceph; via
   an NFS Gateway may significantly reduce application performance when
   compared to native &cephfs;.
  </para>
 </note>
 <para>
  Each &ganesha; service consists of a configuration hierarchy that contains:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    A bootstrap <filename>ganesha.conf</filename>
   </para>
  </listitem>
  <listitem>
   <para>
    A per-service &rados; common configuration object
   </para>
  </listitem>
  <listitem>
   <para>
    A per export &rados; configuration object
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The bootstrap configuration is the minimal configuration to start the
  <systemitem class="daemon">nfs-ganesha</systemitem> daemon within a
  container. Each bootstrap configuration will contain a
  <literal>%url</literal> directive that includes any additional configuration
  from the &rados; common configuration object. The common configuration object
  can include additional <literal>%url</literal> directives for each of the NFS
  exports defined in the export &rados; configuration objects.
 </para>
 <figure>
  <title>&ganesha; structure</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="nfs_ganesha_structure.png" width="75%"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="nfs_ganesha_structure.png" width="75%"/>
   </imageobject>
  </mediaobject>
 </figure>
 <sect1 xml:id="ceph-nfsganesha-nfservice">
  <title>Creating an NFS service</title>

  <para>
   The recommended way to specify the deployment of &ceph; services is to
   create a YAML-formatted file with the specification of the services that you
   intend to deploy. You can create a separate specification file for each type
   of service, or you specify multiple (or all) services types in one file.
  </para>

  <para>
   Depending on what you have chosen to do, you will need to update or create a
   relevant YAML-formatted file to create a &ganesha; service. For more
   information on creating the file, see
   <xref linkend="cephadm-service-and-placement-specs"/>.
  </para>

  <para>
   One you have updated or created the file, execute the following to create a
   <literal>nfs-ganesha</literal> service:
  </para>

<screen>
&prompt.cephuser;ceph orch apply -i <replaceable>FILE_NAME</replaceable>
</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starting or Restarting &ganesha;</title>

  <important>
   <para>
    Starting the &ganesha; service does not automatically export a &cephfs; file system. To
    export a &cephfs; file system, create an export
    configuration file. Refer to <xref linkend="ceph-nfsganesha-create-export"/>
    for more details.
   </para>
  </important>

  <para>
   To start the &ganesha; service, run:
  </para>

<screen>&prompt.cephuser;ceph orch start nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
   To restart the &ganesha; service, run:
  </para>

<screen>&prompt.cephuser;ceph orch restart nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
   If you only want to restart a single &ganesha; daemon, run:
  </para>

<screen>&prompt.cephuser;ceph orch daemon restart nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in the
   grace period.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-list-objects">
  <title>Listing objects in the NFS recovery pool</title>

  <para>
   Execute the following to list the objects in the NFS recovery pool:
  </para>

<screen>&prompt.cephuser;rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> ls</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-create-export">
  <title>Creating an NFS export</title>

  <para>
   You can create an NFS export either in the &dashboard;, or manually on the
   command line. To create the export by using the &dashboard;, refer to
   <xref linkend="dash-webui-nfs"/>, more specifically to
   <xref linkend="dash-webui-nfs-create"/>.
  </para>
  <para>
   To create an NFS export manually, create a configuration
   file for the export. For example, a file <filename>/tmp/export-1</filename>
   with the following content:
  </para>
<screen>
EXPORT {
    export_id = 1;
    path = "/";
    pseudo = "/";
    access_type = "RW";
    squash = "no_root_squash";
    protocols = 3, 4;
    transports = "TCP", "UDP";
    FSAL {
        name = "CEPH";
        user_id = "admin";
        filesystem = "a";
        secret_access_key = "<replaceable>SECRET_ACCESS_KEY</replaceable>";
    }
}
</screen>
   <para>
    After you have created and saved the configuration file for the new export,
    run the following command to create the export:
   </para>
<screen>rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> put <replaceable>EXPORT_NAME</replaceable> <replaceable>EXPORT_CONFIG_FILE</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;rados --pool example_pool --namespace example_namespace put export-1 /tmp/export-1</screen>
   <note>
    <para>
     The FSAL block should be modified to include the desired &cephx; user ID and
     secret access key.
    </para>
   </note>

 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verifying the NFS export</title>

  <para>
   NFS v4 will build a list of exports at the root of a pseudo file system. You
   can verify that the NFS shares are exported by mounting
   <filename>/</filename> of the &ganesha; server node:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable>
&prompt.root;<command>ls</command> <replaceable>/path/to/local/mountpoint</replaceable> cephfs</screen>

  <note>
   <title>&ganesha; is v4 only</title>
   <para>
    By default, &cephadm; will configure an NFS v4 server. NFS v4 does not
    interact with <literal>rpcbind</literal> nor the <literal>mountd</literal>
    daemon. NFS client tools such as <command>showmount</command> will not show
    any configured exports.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Mounting the NFS export</title>

  <para>
   To mount the exported NFS share on a client host, run:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Multiple &ganesha; clusters</title>

  <para>
   Multiple &ganesha; clusters can be defined. This allows for:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; clusters for accessing &cephfs;.
    </para>
   </listitem>
  </itemizedlist>

<!--  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Supported &ogw; Operations</title>
   <para>
    The &ogw; NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      All write operations should be treated as an upload. Many typical I/O
      operations, such as editing files in place, will fail because they
      perform non-sequential stores. There are file utilities that perform
      writes sequentially (for example, some versions of GNU
      <command>tar</command>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <option>-o sync</option> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS &ogw; supports read-write operations only for block sizes smaller than
      4 MB.
     </para>
    </listitem>
   </itemizedlist>
 </sect2> -->
 </sect1>
</chapter>
