<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ceph-nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;: Export &ceph; Data via NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; is an NFS server (refer to
  <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Sharing
  File Systems with NFS</link> ) that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha;, you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client.
 </para>
 <para>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
 </para>
 <para>
  A &cephfs; is exported by default via the path
  <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename>.
 </para>
 <note>
  <title>&ganesha; Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &ceph; via
   an NFS Gateway may significantly reduce application performance when
   compared to native &cephfs; or &ogw; clients.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Installation</title>

  <para>
   For installation instructions, see <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configuration</title>

  <para>
   For a list of all parameters available within the configuration file, see:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> for &cephfs; File System
     Abstraction Layer (FSAL) options.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> for &rgw; FSAL options.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   This section includes information to help you configure the &ganesha; server
   to export the cluster data accessible via &rgw; and &cephfs;.
  </para>

  <para>
   &ganesha; configuration consists of two parts: service configuration and
   exports configuration. The service configuration is controlled by
   <filename>/etc/ganesha/ganesha.conf</filename>. Note that changes to this
   file are overwritten when &deepsea; stage.4 is executed. To persistently
   change the settings, edit the file
   <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> located on
   the &smaster;. The exports configuration is stored in the &ceph; cluster as
   &rados; objects.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Service Configuration</title>
   <para>
    The service configuration is stored in
    <filename>/etc/ganesha/ganesha.conf</filename> and controls all &ganesha;
    daemon settings, including where the exports configuration are stored in
    the &ceph; cluster. Note that changes to this file are overwritten when
    &deepsea; stage.4 is executed. To persistently change the settings, edit
    the file <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>
    located on the &smaster;.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>RADOS_URLS section</title>
    <para>
     The <literal>RADOS_URLS</literal> section configures the &ceph; cluster
     access for reading &ganesha; configuration from &rados; objects.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        &ceph; configuration file path location.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        The cephx user ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        The &rados; object URL to watch for reload notifications.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>RGW Section</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Points to the <filename>ceph.conf</filename> file. When deploying with
        &deepsea;, it is not necessary to change this value.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        The name of the &ceph; client user used by &ganesha;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Name of the &ceph; cluster. &productname; &productnumber; currently
        only supports one cluster name, which is <literal>ceph</literal> by
        default.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>&rados; Object URL</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     &ganesha; supports reading the configuration from a &rados; object. The
     <literal>%url</literal> directive allows to specify a &rados; URL that
     identifies the location of the &rados; object.
    </para>
    <para>
     A &rados; URL can be of two forms:
     <literal>rados://&lt;POOL&gt;/&lt;OBJECT&gt;</literal>, or
     <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</literal>
     where <literal>POOL</literal> is the &rados; pool where the object is
     stored, <literal>NAMESPACE</literal> the pool namespace where the object
     is stored, and <literal>OBJECT</literal> the object name.
    </para>
    <para>
     To support the &dashboard;'s &ganesha; management capabilities, you need
     to follow a convention on the name of the &rados; object for each service
     daemon. The name of the object must be of the form
     <literal>conf-<replaceable>MINION_ID</replaceable></literal> where
     MINION_ID corresponds to the &sminion; ID of the node where this service
     is running.
    </para>
    <para>
     &deepsea; already takes care of correctly generating this URL, and you do
     not need to make any change.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Changing Default &ganesha; Ports</title>
    <para>
     &ganesha; uses the port 2049 for NFS and 875 for the rquota support by
     default. To change the default port numbers, use the
     <option>NFS_Port</option> and <option>RQUOTA_Port</option> options inside
     the <literal>NFS_CORE_PARAM</literal> section, for example:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Exports Configuration</title>
   <para>
    Exports configuration is stored as &rados; objects in the Ceph cluster.
    Each export block is stored in its own &rados; object named
    <literal>export-&lt;id&gt;</literal>, where <literal>&lt;id&gt;</literal>
    must match the <literal>Export_ID</literal> attribute of the export
    configuration. The association between exports and &ganesha; services is
    done through the <literal>conf-MINION_ID</literal> objects. Each service
    object contains a list of RADOS URLs for each export exported by that
    service. An export block looks like the following:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    To create the &rados; object for the above export block we need first to
    store the export block code in a file. Then we can use the &rados; CLI tool
    to store the contents of the previous saved file in a &rados; object.
   </para>
<screen>
&prompt.root;rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    After creating the export object, we can associate the export with a
    service instance by adding the corresponding &rados; URL of the export
    object to the service object. The following sections describe how to
    configure an export block.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Export Main Section</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Export path in the related &cephfs; pool (mandatory). This allows
        subdirectories to be exported from the &cephfs;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </para>
       <para>
        Example: with the value <literal>/cephfs/</literal> and after executing
       </para>
<screen>
&prompt.root;mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        The &cephfs; data is available in the directory
        <filename>/mnt/cephfs/</filename> on the client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </para>
       <tip>
        <title>Limit Access to Clients</title>
        <para>
         If you leave <literal>Access_Type = RW</literal> in the main
         <literal>EXPORT</literal> section and limit access to a specific
         client in the <literal>CLIENT</literal> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <literal>Access_Type =
         None</literal> in the <literal>EXPORT</literal> section and then
         specify less restrictive access mode for one or more clients in the
         <literal>CLIENT</literal> section:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS squash option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exporting 'File System Abstraction Layer'. See
        <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>FSAL Subsection</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Defines which back-end &ganesha; uses. Allowed values are
        <literal>CEPH</literal> for &cephfs; or <literal>RGW</literal> for
        &rgw;. Depending on the choice, a <literal>role-mds</literal> or
        <literal>role-rgw</literal> must be defined in the
        <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Custom &ganesha; Roles</title>

  <para>
   Custom &ganesha; roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <filename>policy.cfg</filename>. The roles
   allow for:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; nodes for accessing &rgw; and &cephfs;.
    </para>
   </listitem>
   <listitem>
    <para>
     Assigning different &rgw; users to &ganesha; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Having different &rgw; users enables &ganesha; nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with &ceph; buckets used in the &crushmap;.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Different &rgw; Users for &ganesha;</title>
   <para>
    The following example procedure for the &smaster; shows how to create two
    &ganesha; roles with different &rgw; users. In this example, the roles
    <literal>gold</literal> and <literal>silver</literal> are used, for which
    &deepsea; already provides example configuration files.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/stack/global.yml</filename> with
      the editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Create a file
      <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> and add
      the following content:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Create a file
      <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> and add
      the following content:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Now, templates for the <filename>ganesha.conf</filename> need to be
      created for each role. The original template of &deepsea; is a good
      start. Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 silver.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 silver.j2
&prompt.root;<command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copy the keyring for the &rgw;:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/rgw/files/
&prompt.root;<command>cp</command> rgw.j2 silver.j2
&prompt.root;<command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      &rgw; also needs the configuration for the different roles:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/configuration/files/
&prompt.root;<command>cp</command> ceph.conf.rgw silver.conf
&prompt.root;<command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and
      <replaceable>NODE2</replaceable> with the names of the nodes to which you
      want to assign the roles.
     </para>
    </step>
    <step>
     <para>
      Execute &deepsea; Stages 0 to 4.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Separating &cephfs; and &rgw; FSAL</title>
   <para>
    The following example procedure for the &smaster; shows how to create 2 new
    different roles that use &cephfs; and &rgw;:
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/rgw.sls</filename> with the
      editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Now, templates for the <filename>ganesha.conf</filename> need to be
      created for each role. The original template of &deepsea; is a good
      start. Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_rgw.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_cfs.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 ganesha_rgw.j2
&prompt.root;<command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      The line <literal>caps mds = "allow *"</literal> can be removed from the
      <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copy the keyring for the &rgw;:
     </para>
<screen>&prompt.root;<command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      &rgw; needs the configuration for the new role:
     </para>
<screen>&prompt.root;<command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and
      <replaceable>NODE2</replaceable> with the names of the nodes to which you
      want to assign the roles.
     </para>
    </step>
    <step>
     <para>
      Execute &deepsea; Stages 0 to 4.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Supported Operations</title>
   <para>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example some versions of GNU
      <command>tar</command>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, application's sequential
      I/O can generally be forced to sequential writes to the NFS server via
      synchronous mounting (the <option>-o sync</option> option). NFS clients
      that cannot mount synchronously (for example Microsoft Windows*) will not
      be able to upload files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW supports read-write operations only for block size smaller than
      4MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starting or Restarting &ganesha;</title>

  <para>
   To enable and start the &ganesha; service, run:
  </para>

<screen>&prompt.root;<command>systemctl</command> enable nfs-ganesha
&prompt.root;<command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Restart &ganesha; with:
  </para>

<screen>&prompt.root;<command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Setting the Log Level</title>

  <para>
   You change the default debug level <literal>NIV_EVENT</literal> by editing
   the file <filename>/etc/sysconfig/nfs-ganesha</filename>. Replace
   <literal>NIV_EVENT</literal> with <literal>NIV_DEBUG</literal> or
   <literal>NIV_FULL_DEBUG</literal>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   A restart of the service is required when changing the log level.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verifying the Exported NFS Share</title>

  <para>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   &ganesha; server node:
  </para>

<screen>&prompt.root;<command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Mounting the Exported NFS Share</title>

  <para>
   To mount the exported NFS share (as configured in
   <xref linkend="ceph-nfsganesha-config"/>) on a client host, run:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
