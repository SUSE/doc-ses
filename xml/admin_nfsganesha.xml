<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ceph-nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;: Export &ceph; Data via NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; is an NFS server (refer to
  <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-nfs">Sharing
  File Systems with NFS</link> ) that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha;, you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client.
 </para>
 <note>
  <title>&ganesha; Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &ceph; via
   an NFS Gateway may significantly reduce application performance when
   compared to native &cephfs; or &ogw; clients.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Installation</title>

  <para>
   For installation instructions, see <xref linkend ="deploy-cephadm-day2-service-nfs"/>.
  </para>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-namespace">
   <title>&ogw; Namespace Conventions</title>
   <para>
     The implementation conforms to Amazon Web Services (AWS) hierarchical
     namespace conventions which map UNIX-style path names onto S3 buckets
     and objects.
   </para>
   <para>
     The top level of the attached namespace consists of S3 buckets,
     represented as NFS directories. Files and directories subordinate to
     buckets are each represented as objects, following S3 prefix and delimiter
     conventions, with <literal>/</literal> being the only supported path delimiter.
   </para>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configuration</title>

  <para>
   For a list of all parameters available within the configuration file, see:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> for &cephfs; File System
     Abstraction Layer (FSAL) options.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> for &ogw; FSAL options.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   This section includes information to help you configure the &ganesha; server
   to export the cluster data accessible via &ogw; and &cephfs;.
  </para>

  <important>
   <title>Restart the &ganesha; Service</title>
   <para>
    When you configure &ganesha; via the &dashboard;, the
    <systemitem class="daemon">nfs-ganesha.service</systemitem> service is
    restarted automatically for the changes to take effect.
   </para>
   <para>
    When you configure &ganesha; manually, you need to restart the
    <systemitem class="daemon">nfs-ganesha.service</systemitem> service on the
    &ganesha; node to re-read the new configuration. See <xref linkend="ceph-nfsganesha-services"/>
    for more information.
   </para>
  </important>

  <para>
   &ganesha; configuration consists of two parts: service configuration and
   exports configuration. The service configuration is controlled by
   <filename>/etc/ganesha/ganesha.conf</filename>. The exports configuration
   is stored in the &ceph; cluster as &rados; objects.
  </para>


<sect2 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>RGW Section</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Points to the <filename>ceph.conf</filename> file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        The name of the &ceph; client user used by &ganesha;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        The name of the &ceph; cluster. &productname; &productnumber; currently
        only supports one cluster name, which is <literal>ceph</literal> by
        default.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>

  </sect2>
   <sect2 xml:id="ceph-nfsganesha-config-service-url">
    <title>&rados; Object URL</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     &ganesha; supports reading the configuration from a &rados; object. The
     <literal>%url</literal> directive allows to specify a &rados; URL that
     identifies the location of the &rados; object.
    </para>
    <para>
     A &rados; URL can be of two forms:
     <literal>rados://&lt;POOL&gt;/&lt;OBJECT&gt;</literal> or
     <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</literal>,
     where <literal>POOL</literal> is the &rados; pool where the object is
     stored, <literal>NAMESPACE</literal> the pool namespace where the object
     is stored, and <literal>OBJECT</literal> the object name.
    </para>
    <para>
     To support the &dashboard;'s &ganesha; management capabilities, you need
     to follow a convention on the name of the &rados; object for each service
     daemon. The name of the object must be of the form
     <literal>conf-<replaceable>MINION_ID</replaceable></literal> where
     MINION_ID corresponds to the &sminion; ID of the node where this service
     is running.
    </para>
  </sect2>
   <sect2 xml:id="ganesha-nfsport">
    <title>Changing Default &ganesha; Ports</title>
    <para>
     &ganesha; uses the port 2049 for NFS and 875 for the rquota support by
     default. To change the default port numbers, use the
     <option>NFS_Port</option> and <option>RQUOTA_Port</option> options inside
     the <literal>NFS_CORE_PARAM</literal> section, for example:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
</sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Exports Configuration</title>
   <para>
    Exports configuration is stored as &rados; objects in the &ceph; cluster.
    Each export block is stored in its own &rados; object named
    <literal>export-<replaceable>ID</replaceable></literal>, where
    <replaceable>ID</replaceable> must match the <literal>Export_ID</literal>
    attribute of the export configuration. The association between exports and
    &ganesha; services is done through the
    <literal>conf-<replaceable>MINION_ID</replaceable></literal> objects. Each
    service object contains a list of RADOS URLs for each export exported by
    that service. An export block looks like the following:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    To create the &rados; object for the above export block, we first need to
    store the export block code in a file. Then we can use the &rados; CLI tool
    to store the contents of the previously saved file in a &rados; object.
   </para>
<screen>
&prompt.cephuser;rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    After creating the export object, we can associate the export with a
    service instance by adding the corresponding &rados; URL of the export
    object to the service object. The following sections describe how to
    configure an export block.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Export Main Section</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Export path in the related &cephfs; pool (mandatory). This allows
        subdirectories to be exported from the &cephfs;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </para>
       <para>
        Example: with the value <literal>/cephfs/</literal> and after executing
       </para>
<screen>
&prompt.root;mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        The &cephfs; data is available in the directory
        <filename>/mnt/cephfs/</filename> on the client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </para>
       <tip>
        <title>Limit Access to Clients</title>
        <para>
         If you leave <literal>Access_Type = RW</literal> in the main
         <literal>EXPORT</literal> section and limit access to a specific
         client in the <literal>CLIENT</literal> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <literal>Access_Type =
         None</literal> in the <literal>EXPORT</literal> section and then
         specify less restrictive access mode for one or more clients in the
         <literal>CLIENT</literal> section:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS squash option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exporting 'File System Abstraction Layer'. See
        <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>FSAL Subsection</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Defines which back-end &ganesha; uses. Allowed values are
        <literal>CEPH</literal> for &cephfs; or <literal>RGW</literal> for
        &ogw;.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>

  <sect2 xml:id="ganesha-getting-exports">
   <title>Obtaining Exports Configuration</title>
   <para>
    To obtain existing exports configuration, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Find the &rados; pool name and namespace for &ganesha; exports. The
      following command outputs a string of the
      <replaceable>POOL_NAME</replaceable>/<replaceable>NAMESPACE</replaceable>
      form.
     </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
cephfs_data/ganesha</screen>
    </step>
    <step>
     <para>
      By using the obtained pool name and namespace, list the &rados; objects
      available on that pool:
     </para>
<screen>&prompt.cephuser;rados -p cephfs_data -N ganesha ls
conf-osd-node1
export-1
conf-osd-node2</screen>
     <tip>
      <para>
       To see how each node is configured, view its content using the following
       command:
      </para>
<screen>&prompt.cephuser;cat conf-osd-node1
%url "rados://cephfs_data/ganesha/export-1"
cat conf-osd-node2
%url "rados://cephfs_data/ganesha/export-1"</screen>
      <para>
       In this case, both nodes will use
       <literal>rados://cephfs_data/ganesha/export-1</literal>. If there are
       multiple configurations, each node can use a different configuration.
      </para>
     </tip>
    </step>
    <step>
     <para>
      Each export configuration is stored in a single object with the name
      <literal>export-<replaceable>ID</replaceable></literal>. Use the
      following command to obtain the contents of the object and save it to
      <filename>/tmp/export-1</filename>:
     </para>
<screen>&prompt.cephuser;rados -p cephfs_data -N ganesha get export-1 /tmp/export-1
&prompt.cephuser;cat /tmp/export-1
EXPORT {
    export_id = 1;
    path = "/";
    pseudo = "/cephfs";
    access_type = "RW";
    squash = "no_root_squash";
    protocols = 3, 4;
    transports = "UDP", "TCP";
    FSAL {
        name = "CEPH";
        user_id = "admin";
        filesystem = "cephfs";
        secret_access_key = "<replaceable>SECRET_KEY</replaceable>";
    }

    CLIENT {
        clients = &ftpip;;
        access_type = "RW";
        squash = "no_root_squash";
    }
}</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Multiple &ganesha; Clusters</title>

  <para>
   Multiple &ganesha; clusters can be defined. This allows for:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; clusters for accessing &ogw; and &cephfs;.
    </para>
   </listitem>
   <listitem>
    <para>
     Assigning different &ogw; users to &ganesha; clusters.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Having different &ogw; users enables &ganesha; clusters to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with &ceph; buckets used in the &crushmap;.
  </para>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Supported Operations</title>
   <para>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example, some versions of GNU
      <command>tar</command>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <option>-o sync</option> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW supports read-write operations only for block sizes smaller than
      4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starting or Restarting &ganesha;</title>

  <para>
   To start the &ganesha; service, run:
  </para>

<screen>&prompt.cephuser;ceph orch start nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
   To restart the entire &ganesha; service, run:
  </para>

<screen>&prompt.cephuser;ceph orch restart nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
    If you only want to restart a single &ganesha; daemon, run:
  </para>

<screen>&prompt.cephuser;ceph orch daemon restart nfs.<replaceable>SERVICE_ID</replaceable></screen>

  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Setting the Log Level</title>

  <para>
   You change the default debug level <literal>NIV_EVENT</literal> by editing
   the file <filename>/etc/sysconfig/ganesha</filename>. Replace
   <literal>NIV_EVENT</literal> with <literal>NIV_DEBUG</literal> or
   <literal>NIV_FULL_DEBUG</literal>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   A restart of the service is required when changing the log level.
  </para>

  <note>
   <para>
    &ganesha; uses &ceph; client libraries to connect to the &ceph; cluster. By
    default, client libraries do not log errors or any other output. To see
    more details about &ganesha; interacting with the &ceph; cluster (for
    example, connection issues details) logging needs to be explicitly defined
    in the <filename>ceph.conf</filename> configuration file under the
    <literal>[client]</literal> section. For example:
   </para>
<screen>[client]
	log_file = "/var/log/ceph/ceph-client.log"</screen>
  </note>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verifying the Exported NFS Share</title>

  <para>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   &ganesha; server node:
  </para>

<screen>&prompt.sminion;<command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Mounting the Exported NFS Share</title>

  <para>
   To mount the exported NFS share (as configured in
   <xref linkend="ceph-nfsganesha-config"/>) on a client host, run:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
