<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha.ceph.nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;: Export &ceph; Data via NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <warning os="ses4">
  <title>Technology Preview</title>
  <para>
   As of &storage; 4, &ganesha; is considered a technology preview and is not
   supported.
  </para>
 </warning>
 <para>
  &ganesha; is an NFS server (refer to
  <link
   xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_nfs.html">Sharing
  File Systems with NFS</link> ) that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha; you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client.
 </para>
 <sect1 xml:id="ceph.nfsganesha.install">
  <title>Installation</title>
  <para>
   For installation instructions, see <xref linkend="cha.as.ganesha" />.
  </para>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.config">
  <title>Configuration</title>
  <para>
   For a list of all parameters available within the configuration file,
   see <command>man ganesha-config</command>.
  </para>
  <para>
   This section includes information to help you configure the &ganesha; server
   to export the cluster data accessible via &rgw; and &cephfs;.
  </para>
  <para>
   &ganesha; configuration is controlled by
   <filename>/etc/ganesha/ganesha.conf</filename>. Note that changes to
   this file are overwritten when &deepsea;
   stage 4 is executed. To persistently change the settings, edit the file
   <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> located
   on the &smaster;.
  </para>
  
  <sect2 xml:id="ceph.nfsganesha.config.general">
   <title>Export Section</title>
   <para>
    This section describes how to configure the <literal>EXPORT</literal>
    sections in the <filename>ganesha.conf</filename>.
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <sect3 xml:id="ceph.nfsganesha.config.general.export">
    <title>Export Main Section</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Each export needs to have a unique 'Export_Id' (mandatory)
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Export path in the related &cephfs; pool (mandatory). This allows
        exporting subfolders from the &cephfs;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </para>
       <para>
        Example: with the value <literal>/cephfs/</literal> and after executing
       </para>
<screen>
&prompt.root;mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        the &cephfs; data is available in the folder <filename>/mnt/cephfs/</filename>
        on the client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        'RO' for read-only access, default is 'None'
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS squash option
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exporting 'File System Abstraction Layer'. See
        <xref linkend="ceph.nfsganesha.config.general.fsal" />.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.nfsganesha.config.general.fsal">
    <title>FSAL Subsection</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Defines which backend &ganesha; uses. Allowed values are
        <literal>CEPH</literal> for &cephfs; or <literal>RGW</literal> for
        &rgw;. Depending on the choice, a <literal>role-mds</literal> or
        <literal>role-rgw</literal> must be defined in the
        <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
  <sect2 xml:id="ceph.nfsganesha.config.rgw">
   <title>RGW Section</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Points to the <filename>ceph.conf</filename> file. When deploying
        with &deepsea;, it is not necessary to change this value.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        The name of the &ceph; client user used by &ganesha;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Name of the ceph cluster. &productname; 5 currently only supports
        one cluster name, which is by default <literal>ceph</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
  </sect2>

 </sect1>
 <sect1 xml:id="ceph.nfsganesha.customrole">
  <title>Custom &ganesha; Roles</title>
  <para>
   Custom &ganesha; roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <filename>policy.cfg</filename>. The roles
   allow for:
  </para>
  <itemizedlist>
   <listitem>
    <para>Separated &ganesha; nodes for accessing &rgw; and &cephfs;.</para>
   </listitem>
   <listitem>
    <para>Assigning different &rgw; users to &ganesha; nodes.</para>
   </listitem>
  </itemizedlist>
  <para>
   Having different &rgw; users enables &ganesha; nodes to access different
   S3 buckets. S3 buckets can be used for access control. Note: S3 buckets
   are not to be confused with &ceph; buckets used in the &crushmap;.
  </para>
  <sect2 xml:id="ceph.nfsganesha.customrole.rgw_multiusers">
   <title>Different &rgw; Users for &ganesha;</title>
   <para>
    The following example procedure for the &smaster; shows how to create
    2 &ganesha; roles with different &rgw; users. In this
    examples the roles <literal>gold</literal> and <literal>silver</literal>
    are used, for which &deepsea; already provides example configuration
    files.
   </para>
   <procedure  xml:id="proc.ceph.nfsganesha.rgw_multiusers">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/rgw.sls</filename> with the
      editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  silver:
    users:
      - { uid: "user1", name: "Demo1", email: "user1@demo.nil" }

  gold:
    users:
      - { uid: "user2", name: "Demo2", email: "user2@demo.nil" }

ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Now templates for the <filename>ganesha.conf</filename> have to be
      created for each role. The original template of &deepsea; is a good start.
      Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 silver.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 silver.j2
&prompt.root;<command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>Copy the keyring for the &rgw;:</para>
     <screen>&prompt.root;<command>cd</command> /srv/salt/ceph/rgw/files/
&prompt.root;<command>cp</command> rgw.j2 silver.j2
&prompt.root;<command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      &rgw; also needs the configuration for the different roles:
     </para>
     <screen>&prompt.root;<command>cd</command> /srv/salt/ceph/configuration/files/
&prompt.root;<command>cp</command> ceph.conf.rgw ceph.conf.silver
&prompt.root;<command>cp</command> ceph.conf.rgw ceph.conf.gold</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
 <screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and <replaceable>NODE2</replaceable>
      with the names of the nodes you want to assign the roles.
     </para>
    </step>
    <step>
     <para>Execute &deepsea; stages 0 to 4.</para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="ceph.nfsganesha.customrole.rgw_cephfs">
   <title>Separating &cephfs; and &rgw; FSAL</title>
   <para>
    The following example procedure for the &smaster; shows how to create
    2 new different roles that use &cephfs; and &rgw;:
   </para>
   <procedure  xml:id="proc.ceph.nfsganesha.customrole">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/rgw.sls</filename> with the
      editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Now templates for the <filename>ganesha.conf</filename> have to be
      created for each role. The original template of &deepsea; is a good start.
      Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_rgw.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_cfs.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 ganesha_rgw.j2
&prompt.root;<command>cp</command> ganesha.j2 ganesha_rgw.j2</screen>
     <para>
      The line <literal>caps mds = "allow *"</literal> can be removed
      from the <filename>ganesha_rgw.j2</filename>
     </para>
    </step>
    <step>
     <para>Copy the keyring for the &rgw;:</para>
     <screen>&prompt.root;<command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      &rgw; needs the configuration for the new role:
     </para>
     <screen>&prompt.root;<command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
 <screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and <replaceable>NODE2</replaceable>
      with the names of the nodes you want to assign the roles.
     </para>
    </step>
    <step>
     <para>Execute &deepsea; stages 0 to 4.</para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.services">
  <title>Starting &ganesha; Related Services</title>

  <para>
   Enable and start the RPC service required by &ganesha;:
  </para>

<screen>sudo systemctl enable rpcbind rpc-statd
sudo systemctl start rpcbind rpc-statd</screen>

  <para>
   Enable and start the &ganesha; service:
  </para>

<screen>sudo systemctl enable nfs-ganesha
sudo systemctl start nfs-ganesha</screen>

  <tip>
   <title>Starting the &ganesha; Service Manually</title>
   <para>
    Instead of using &systemd; targets, you can start &ganesha; manually. The
    following command starts &ganesha; with the debugging information enabled:
   </para>
<screen>sudo /usr/bin/ganesha.nfsd -L /var/log/ganesha.log \
 -f /etc/ganesha/ganesha.conf -N NIV_DEBUG</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.verify">
  <title>Verifying the Exported NFS Share</title>

  <para>
   After you configure and start &ganesha;, you can verify whether the NFS
   shares are exported on the &ganesha; server node:
  </para>

<screen>sudo showmount -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.mount">
  <title>Mounting the Exported NFS Share</title>

  <para>
   To mount the exported NFS share (as configured in
   <xref
    linkend="ceph.nfsganesha.config"/>) on a client host, run:
  </para>

<screen>sudo mount -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.more">
  <title>Additional Resources</title>
  <para>
   The original &ganesha; documentation can be found on 
   <link xlink:href="https://github.com/nfs-ganesha/nfs-ganesha/wiki/Docs"/>.
  </para>
 </sect1>
</chapter>
