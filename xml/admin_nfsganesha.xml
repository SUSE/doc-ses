<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-ceph-nfsganesha">
<!-- ============================================================== -->
<title>&ganesha;</title>
<info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; is an NFS server that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha;, you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client. For installation instructions, see
  <xref linkend ="deploy-cephadm-day2-service-nfs"/>.
 </para>
 <note>
  <title>&ganesha; Performance</title>
  <para>
   Because of increased protocol overhead and additional latency caused by
   extra network hops between the client and the storage, accessing &ceph; via
   an NFS Gateway may significantly reduce application performance when
   compared to native &cephfs; or &ogw; clients.
  </para>
 </note>
 <para>
   Each &ganesha; service consists of a configuration hierarchy that contains:
 </para>
 <itemizedlist>
   <listitem>
     <para>
       A bootstrap <filename>ganesha.conf</filename>
     </para>
   </listitem>
   <listitem>
     <para>
       A per-service &rados; common configuration object
     </para>
   </listitem>
   <listitem>
     <para>
       A per export &rados; configuration object
     </para>
   </listitem>
 </itemizedlist>
 <para>
   The bootstrap configuration is the minimal configuration to start the
   <systemitem class="daemon">nfs-ganesha</systemitem> daemon within a container. Each bootstrap
   configuration will contain a <literal>%url</literal> directive that includes
   any additional configuration from the &rados; common configuration object.
   The common configuration object can include additional <literal>%url</literal>
   directives for each of the NFS exports defined in the export &rados;
   configuration objects.
 </para>
 <figure>
  <title>&ganesha; Structure</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="nfs_ganesha_structure.png" width="75%"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="nfs_ganesha_structure.png" width="75%"/>
   </imageobject>
  </mediaobject>
 </figure>

 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Starting or Restarting &ganesha;</title>
  <para>
   To start the &ganesha; service, run:
  </para>
 <screen>&prompt.cephuser;ceph orch start nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
   To restart the entire &ganesha; service, run:
  </para>
 <screen>&prompt.cephuser;ceph orch restart nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
    If you only want to restart a single &ganesha; daemon, run:
  </para>
 <screen>&prompt.cephuser;ceph orch daemon restart nfs.<replaceable>SERVICE_ID</replaceable></screen>
  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </para>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-nfservice">
   <title>Create an NFS Service</title>
   <para>
     The recommended way to specify the deployment of &ceph; services is to create a
     YAML-formatted file with the specification of the services that you intend
     to deploy. You can create a separate specification file for each type of
     service, or you specify multiple (or all) services types in one file.
   </para>
   <para>
     Depending on what you have chosen to do, you will need to update or create
     a relevant YAML-formatted file to create a &ganesha; service. For more
     information on creating the file, see <xref linkend="cephadm-service-and-placement-specs"/>.
   </para>
   <para>
     One you have updated or created the file, execute the following to create
     a <literal>nfs-ganesha</literal> service:
   </para>
<screen>
&prompt.cephuser;ceph orch apply -i <replaceable>FILE_NAME</replaceable>
</screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-list-objects">
   <title>List Objects in the NFS Recovery Pool</title>
   <para>
     Execute the following to list the objects in the NFS recovery pool:
   </para>
<screen>&prompt.cephuser;rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> ls</screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-create-export">
   <title>Create an NFS Export</title>
   <para>
     Execute the following to create an NFS export.
   </para>
   <note>
     <para>
       The FSAL block should be modified to include the desired cephx
       user ID and secret access key.
     </para>
   </note>
<screen>&prompt.cephuser;rados --pool <replaceable>POOL_NAME</replaceable> --namespace <replaceable>NAMESPACE_NAME</replaceable> put <replaceable>export-1</replaceable> <replaceable>export-1</replaceable></screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verify the NFS Export</title>
  <para>
   NFS v4 will build a list of exports at the root of a pseudo file system.
   You can verify that the NFS shares are exported by mounting
   <filename>/</filename> of the &ganesha; server node:
  </para>
<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable>
&prompt.root;<command>ls</command> <replaceable>/path/to/local/mountpoint</replaceable> cephfs rgw</screen>
  <note>
   <title>&ganesha; is v4 Only</title>
   <para>
    By default, &cephadm; will configure an NFS v4 server.
    NFS v4 does not interact with <literal>rpcbind</literal> nor the
    <literal>mountd</literal> daemon. NFS client tools such as
    <command>showmount</command> will not show any configured exports.
   </para>
 </note>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Mount the NFS Export</title>
  <para>
   To mount the exported NFS share on a client host, run:
  </para>
<screen>&prompt.root;<command>mount</command> -t nfs <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>

 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Multiple &ganesha; Clusters</title>
  <para>
   Multiple &ganesha; clusters can be defined. This allows for:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; clusters for accessing &ogw; and &cephfs;.
    </para>
   </listitem>
   <listitem>
    <para>
     Assigning different &ogw; users to &ganesha; clusters.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Having different &ogw; users enables &ganesha; clusters to access different S3
   buckets. S3 buckets can be used for access control.
  </para>
  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Supported &ogw; Operations</title>
   <para>
    The &ogw; NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      All write operations should be treated as an upload. Many typical I/O
      operations, such as editing files in place, will fail because they
      perform non-sequential stores. There are file utilities that perform
      writes sequentially (for example, some versions of GNU
      <command>tar</command>), but may fail because of infrequent
      non-sequential stores. When mounting via NFS, an application's sequential
      I/O can generally be forced to perform sequential writes to the NFS
      server via synchronous mounting (the <option>-o sync</option> option).
      NFS clients that cannot mount synchronously (for example, Microsoft
      Windows*) will not be able to upload files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS &ogw; supports read-write operations only for block sizes smaller than
      4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
