<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha.ceph.nfsganesha">
<!-- ============================================================== -->
 <title>&ganesha;: Export &ceph; Data via NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ganesha; is an NFS server (refer to
  <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_nfs.html">Sharing
  File Systems with NFS</link> ) that runs in a user address space instead of
  as part of the operating system kernel. With &ganesha;, you can plug in your
  own storage mechanism&mdash;such as &ceph;&mdash;and access it from any NFS
  client.
 </para>
 <para>
  S3 buckets are exported to NFS on a per-user basis, for example via the path
  <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
 </para>
 <para>
  A &cephfs; is exported by default via the path
  <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename>.
 </para>
 <sect1 xml:id="ceph.nfsganesha.install">
  <title>Installation</title>

  <para>
   For installation instructions, see <xref linkend="cha.as.ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.config">
  <title>Configuration</title>

  <para>
   For a list of all parameters available within the configuration file, see:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> for &cephfs; File System
     Abstraction Layer (FSAL) options.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> for &rgw; FSAL options.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   This section includes information to help you configure the &ganesha; server
   to export the cluster data accessible via &rgw; and &cephfs;.
  </para>

  <para>
   &ganesha; configuration is controlled by
   <filename>/etc/ganesha/ganesha.conf</filename>. Note that changes to this
   file are overwritten when &deepsea; Stage 4 is executed. To persistently
   change the settings, edit the file
   <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> located on
   the &smaster;.
  </para>

  <sect2 xml:id="ceph.nfsganesha.config.general">
   <title>Export Section</title>
   <para>
    This section describes how to configure the <literal>EXPORT</literal>
    sections in the <filename>ganesha.conf</filename>.
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <sect3 xml:id="ceph.nfsganesha.config.general.export">
    <title>Export Main Section</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Each export needs to have a unique 'Export_Id' (mandatory).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Export path in the related &cephfs; pool (mandatory). This allows
        subdirectories to be exported from the &cephfs;.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Target NFS export path (mandatory for NFSv4). It defines under which
        NFS export path the exported data is available.
       </para>
       <para>
        Example: with the value <literal>/cephfs/</literal> and after executing
       </para>
<screen>
&prompt.root;mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        The &cephfs; data is available in the directory
        <filename>/mnt/cephfs/</filename> on the client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        'RO' for read-only access, 'RW' for read-write access, and 'None' for
        no access.
       </para>
       <tip>
        <title>Limit Access to Clients</title>
        <para>
         If you leave <literal>Access_Type = RW</literal> in the main
         <literal>EXPORT</literal> section and limit access to a specific
         client in the <literal>CLIENT</literal> section, other clients will be
         able to connect anyway. To disable access to all clients and enable
         access for specific clients only, set <literal>Access_Type =
         None</literal> in the <literal>EXPORT</literal> section and then
         specify less restrictive access mode for one or more clients in the
         <literal>CLIENT</literal> section:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS squash option.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exporting 'File System Abstraction Layer'. See
        <xref linkend="ceph.nfsganesha.config.general.fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.nfsganesha.config.general.fsal">
    <title>FSAL Subsection</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Defines which back-end &ganesha; uses. Allowed values are
        <literal>CEPH</literal> for &cephfs; or <literal>RGW</literal> for
        &rgw;. Depending on the choice, a <literal>role-mds</literal> or
        <literal>role-rgw</literal> must be defined in the
        <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.nfsganesha.config.rgw">
   <title>RGW Section</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
   <variablelist>
    <varlistentry>
     <term>ceph_conf</term>
     <listitem>
      <para>
       Points to the <filename>ceph.conf</filename> file. When deploying with
       &deepsea;, it is not necessary to change this value.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       The name of the &ceph; client user used by &ganesha;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cluster</term>
     <listitem>
      <para>
       Name of the &ceph; cluster. &productname; 5 currently only supports one
       cluster name, which is <literal>ceph</literal> by default.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ganesha.nfsport">
   <title>Changing Default &ganesha; Ports</title>
   <para>
    &ganesha; uses the port 2049 for NFS and 875 for the rquota support by
    default. To change the default port numbers, use the
    <option>NFS_Port</option> and <option>RQUOTA_Port</option> options inside
    the <literal>NFS_CORE_PARAM</literal> section, for example:
   </para>
<screen>
NFS_CORE_PARAM
{
 NFS_Port = 2060;
 RQUOTA_Port = 876;
}
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.customrole">
  <title>Custom &ganesha; Roles</title>

  <para>
   Custom &ganesha; roles for cluster nodes can be defined. These roles are
   then assigned to nodes in the <filename>policy.cfg</filename>. The roles
   allow for:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Separated &ganesha; nodes for accessing &rgw; and &cephfs;.
    </para>
   </listitem>
   <listitem>
    <para>
     Assigning different &rgw; users to &ganesha; nodes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Having different &rgw; users enables &ganesha; nodes to access different S3
   buckets. S3 buckets can be used for access control. Note: S3 buckets are not
   to be confused with &ceph; buckets used in the &crushmap;.
  </para>

  <sect2 xml:id="ceph.nfsganesha.customrole.rgw_multiusers">
   <title>Different &rgw; Users for &ganesha;</title>
   <para>
    The following example procedure for the &smaster; shows how to create two
    &ganesha; roles with different &rgw; users. In this example, the roles
    <literal>gold</literal> and <literal>silver</literal> are used, for which
    &deepsea; already provides example configuration files.
   </para>
   <procedure xml:id="proc.ceph.nfsganesha.rgw_multiusers">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/stack/global.yml</filename> with
      the editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Create a file
      <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> and add
      the following content:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Create a file
      <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> and add
      the following content:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Now, templates for the <filename>ganesha.conf</filename> need to be
      created for each role. The original template of &deepsea; is a good
      start. Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 silver.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 silver.j2
&prompt.root;<command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copy the keyring for the &rgw;:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/rgw/files/
&prompt.root;<command>cp</command> rgw.j2 silver.j2
&prompt.root;<command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      &rgw; also needs the configuration for the different roles:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/configuration/files/
&prompt.root;<command>cp</command> ceph.conf.rgw silver.conf
&prompt.root;<command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and
      <replaceable>NODE2</replaceable> with the names of the nodes to which you
      want to assign the roles.
     </para>
    </step>
    <step>
     <para>
      Execute &deepsea; Stages 0 to 4.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.nfsganesha.customrole.rgw_cephfs">
   <title>Separating &cephfs; and &rgw; FSAL</title>
   <para>
    The following example procedure for the &smaster; shows how to create 2 new
    different roles that use &cephfs; and &rgw;:
   </para>
   <procedure xml:id="proc.ceph.nfsganesha.customrole">
    <step>
     <para>
      Open the file <filename>/srv/pillar/ceph/rgw.sls</filename> with the
      editor of your choice. Create the file if it does not exist.
     </para>
    </step>
    <step>
     <para>
      The file needs to contain the following lines:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      These roles can later be assigned in the <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Now, templates for the <filename>ganesha.conf</filename> need to be
      created for each role. The original template of &deepsea; is a good
      start. Create two copies:
     </para>
<screen>&prompt.root;<command>cd</command> /srv/salt/ceph/ganesha/files/
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
&prompt.root;<command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_rgw.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Edit the <filename>ganesha_cfs.conf.j2</filename> and remove the section:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      The new roles require keyrings to access the cluster. To provide access,
      copy the <filename>ganesha.j2</filename>:
     </para>
<screen>&prompt.root;<command>cp</command> ganesha.j2 ganesha_rgw.j2
&prompt.root;<command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      The line <literal>caps mds = "allow *"</literal> can be removed from the
      <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copy the keyring for the &rgw;:
     </para>
<screen>&prompt.root;<command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      &rgw; needs the configuration for the new role:
     </para>
<screen>&prompt.root;<command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Assign the newly created roles to cluster nodes in the
      <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Replace <replaceable>NODE1</replaceable> and
      <replaceable>NODE2</replaceable> with the names of the nodes to which you
      want to assign the roles.
     </para>
    </step>
    <step>
     <para>
      Execute &deepsea; Stages 0 to 4.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha.rgw.supported_operations">
   <title>Supported Operations</title>
   <para>
    The RGW NFS interface supports most operations on files and directories,
    with the following restrictions:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Links including symbolic links are not supported.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>NFS access control lists (ACLs) are not supported.</emphasis>
      Unix user and group ownership and permissions <emphasis>are</emphasis>
      supported.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Directories may not be moved or renamed.</emphasis> You
      <emphasis>may</emphasis> move files between directories.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Only full, sequential write I/O is supported.</emphasis>
      Therefore, write operations are forced to be uploads. Many typical I/O
      operations, such as editing files in place, will necessarily fail as they
      perform non-sequential stores. There are file utilities that apparently
      write sequentially (for example some versions of GNU
      <command>tar</command>), but may fail due to infrequent non-sequential
      stores. When mounting via NFS, application's sequential I/O can generally
      be forced to sequential writes to the NFS server via synchronous mounting
      (the <option>-o sync</option> option). NFS clients that cannot mount
      synchronously (for example Microsoft Windows*) will not be able to upload
      files.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW supports read-write operations only for block size smaller than
      4MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.services">
  <title>Starting or Restarting &ganesha;</title>

  <para>
   To enable and start the &ganesha; service, run:
  </para>

<screen>&prompt.root;<command>systemctl</command> enable nfs-ganesha
&prompt.root;<command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Restart &ganesha; with:
  </para>

<screen>&prompt.root;<command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   When &ganesha; is started or restarted, it has a grace timeout of 90 seconds
   for NFS v4. During the grace period, new requests from clients are actively
   rejected. Hence, clients may face a slowdown of requests when NFS is in
   grace state.
  </para>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.loglevel">
  <title>Setting the Log Level</title>

  <para>
   You change the default debug level <literal>NIV_EVENT</literal> by editing
   the file <filename>/etc/sysconfig/nfs-ganesha</filename>. Replace
   <literal>NIV_EVENT</literal> with <literal>NIV_DEBUG</literal> or
   <literal>NIV_FULL_DEBUG</literal>. Increasing the log verbosity can produce
   large amounts of data in the log files.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   A restart of the service is required when changing the log level.
  </para>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.verify">
  <title>Verifying the Exported NFS Share</title>

  <para>
   When using NFS v3, you can verify whether the NFS shares are exported on the
   &ganesha; server node:
  </para>

<screen>&prompt.root;<command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph.nfsganesha.mount">
  <title>Mounting the Exported NFS Share</title>

  <para>
   To mount the exported NFS share (as configured in
   <xref linkend="ceph.nfsganesha.config"/>) on a client host, run:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
