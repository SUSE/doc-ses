<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.ceph.cluster">
	<title>&ceph; Cluster Administration</title>
<para>
The chapter gives details about basic administration tasks that you may need to perform on your cluster deployed by using <command>ceph-deploy</command>. The <command>ceph-deploy</command> is a command line utility that provides you with a wide range of subcommands to administrate the &ceph; cluster. Most of the tasks can be performed without the need to stop your cluster. 
</para>
<para>
The general <command>ceph-deply</command> syntax is the following:
</para>
<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>

<para>
   A list of selected <command>ceph-deploy</command> subcommands follows.
  </para>
<itemizedlist>
	<listitem>		
		<para>
		<command>gatherkeys</command> - the subcommand is used when you are adding monitors,OSDs or MDS to the cluster. it gathers authentication keys for provisioning new nodes. It takes host
      names as arguments. It checks for and fetches <literal>client.admin
      keyring</literal>, monitor keyring and
      <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor host. The command syntax is the following:  
		</para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
     <para>
      <replaceable>hostname</replaceable> is the host name of the monitor from
      where keys are to be pulled.
     </para>
	</listitem>
	<listitem>		
		<para>
		<command>mon add</command> - performs all steps required to add a monitor to your cluster, for more details reffer to <xref linkend="Adding.ceph.deploy.monitors"/>. 
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>osd prepare</command> - prepares a disk to be used as OSD, For details refer to <xref linkend=""/>.
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>osd activate</command> - activates a disk to be used as OSD.
		</para>
	</listitem>
	<listitem>		
		<para>
		<command>rgw prepare/activate/create</command>
		</para>
	</listitem>
	<listitem>
	<para>
		<command>purge, purgedata, forgetkeys</command>
	</para>
	</listitem>	
</itemizedlist>


<tip>
   <para>
    Administer &ceph; nodes with <command>ceph-deploy</command> from the admin
    node. Before administering them, always create a new temporary directory
    and <command>cd</command> into it. Then choose one monitor node and gather
    the authentication keys with the <command>gatherkeys</command> subcommand
    from it, and copy the <filename>/etc/ceph/ceph.conf</filename> file from
    the monitor node into the current local directory.
   </para>
<screen>&prompt.cephuser; mkdir ceph_tmp
&prompt.cephuser; cd ceph_tmp
&prompt.cephuser; ceph-deploy gatherkeys ceph_mon_host
&prompt.cephuser; scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
  </tip>

<sect1 xml:id="ceph.deploy.monitors.managment">
	<title>Monitors Managment by Using <command>ceph-deploy</command></title>
	<para>
Adding monitoring nodes to the cluster or removing nodes from the cluster can be performed by using <command>ceph-deploy</command> in several steps described in following sections. But you need to take the following into account:
</para>
<important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <command>ceph-deploy</command> restricts you to only install one monitor
      per host.
     </para>
    </listitem>
    <listitem>
     <para>
      We do not recommend mixing monitors and OSDs on the same host.
     </para>
    </listitem>
    <listitem>
     <para>
      For high availability, you should run a production &ceph; cluster with
      <emphasis>at least</emphasis> three monitors.
     </para>
    </listitem>
   </itemizedlist>
  </important>
	<sect2 xml:id="Adding.ceph.deploy.monitors">
		<title>Adding Monitors</title>
<para>
    After you create a cluster and install &ceph; packages to the monitor
    host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
    information), you may deploy the monitors to the monitor hosts. You may
    specify more monitor host names in the same command.
   </para>
   <note>
    <para>
     When adding a monitor on a host that was not in hosts initially defined
     with the <command>ceph-deploy new</command> command, a <option>public
     network</option> statement needs to be added to the
     <filename>ceph.conf</filename> file. The monitor must be added to the <literal>mon initial members</literal> and <literal>monmap</literal> options or the <option>public_addr</option> nor <option>public_network</option> keys must defined for monitors, otherwise errors arise during the monitor startup.
    </para>
   </note>
<screen>ceph-deploy mon add <replaceable>host name</replaceable></screen>
<para>
When you run the command, the following tasks are performed:
</para>
<orderedlist>
	<listitem>
		<para>
The platform and the distribution of the target host is detected.
</para>
</listitem>
<listitem>
	<para>
 The monitor is configured and added to the cluster. If there is a record for the particular monitor in the &ceph; configuration, the <literal>mon addr</literal> of the monitor will be used, otherwise <command>ceph-deploy</command> falls back and resolve the IP address.
</para>
</listitem>
<listitem>
	<para>
	</para>
</listitem>
<listitem>
	<para>
	<command>ceph-deploy</command> tries to start the monitor and checks the monitor status. 
	</para>
</listitem>
</orderedlist>
   
	</sect2>
	<sect2 xml:id="Removing.ceph.deploy.monitors">
		<title>Removing Monitors</title>
		<para>
    If you have a monitor in your cluster that you want to remove, you may use
    the destroy option. You may specify more monitor host names in the same
    command.
   </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     Ensure that if you remove a monitor, the remaining monitors will be able
     to establish a consensus. If that is not possible, consider adding a
     monitor before removing the monitor you want to take offline.
    </para>
   </note>
	</sect2>

</sect1>
<sect1 xml:id="ceph.deploy.OSD.managment">
	<title>OSDs Managment by Using <command>ceph-deploy</command></title>
	<note>
   <title>Limitations</title>
   <para>
    The procedures described further can be performed only with the
    default CRUSH map created by <command>ceph-deploy</command>.
   </para>
  </note>
<para>
This section gives you information how to add deploy OSD Daemons on particular nodes and also how to add a disk to an existing OSD node. As you may also want to remove a disk or the OSD daemon or the whole OSD node, this section provides you with procedures how to handle these tasks.
</para>

<sect2 xml:id="Adding.OSD.Nodes">
   <title>Adding &ceph; OSD Nodes</title>
   <tip>
   <para>
    When adding an OSD to an existing cluster, be aware that the cluster will
    be rebalancing for some time afterward. To minimize the rebalancing
    periods, it is best to add all the OSDs you intend to add at the same time.
   </para>
  </tip>
   <para>
    The procedure below describes adding of a &ceph; OSD node to your cluster.
   </para>
   <procedure xml:id="proc.Adding.Ceph.Node">
    <title>Adding a &ceph; OSD Node</title>
    <step>
     <para>
      List all &ceph; OSD nodes and then choose a proper name for the new
      node/s
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      Inspect your CRUSH map to find out the bucket type, for a procedure refer
      to <xref linkend="op.crush"/>. Typically the bucket type is
      <emphasis>host</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Create a record for the new node in your CRUSH map.
     </para>
<screen>ceph osd crush add-bucket <replaceable>{bucket name} {bucket type}</replaceable></screen>
     <para>
      for example:
     </para>
<screen>ceph osd crush add-bucket ses4-4 host</screen>
    </step>
    <step>
     <para>
      Add all OSDs that the new node should use. For a procedure refer to
      <xref linkend="storage.bp.inst.add_osd_cephdeploy"/>.
     </para>
    </step>    
   </procedure>
   <sect3 xml:id="storage.bp.inst.add_osd_cephdeploy">
   	<title>Adding OSD Disks</title>
   	<para>
   	In case you need to add a new OSD to a OSD node (called <emphasis>node2</emphasis> in our case), follows steps described further. Before you start, see the following points:
   	</para>
    <itemizedlist>
     <listitem>
     <para>
      This can be done on a live cluster without downtime.
     </para>
    </listitem>
    <listitem>
     <para>
      This will cause increased replication traffic between servers.
     </para>
    </listitem>
    <listitem>
     <para>
      Doing this operation repeatedly before the last operation has completed
      replication can save the cluster overall rebuild time.
     </para>
    </listitem>
    </itemizedlist>
    <para>
    	The disk must be prepared and mounted as described in the following procedure. Let's use a device <literal>/dev/sdd</literal> as an example.
    </para>
    <procedure>
     <step>
    <para>
     Create a partition <literal>sdd1</literal> on the disk:
    </para>
<screen>sudo parted /dev/sdd1 mkpart primary 0.0 -1s</screen>
   </step>
   <step>
    <para>
     Format the partition with XFS file system:
    </para>
<screen>sudo mkfs.xfs -f /dev/sdd1</screen>
   </step>
   <step>
    <para>
     Find out the UUID (Universally Unique Identifier) of the disk:
    </para>
<screen>ls -l /dev/disk/by-uuid | grep sdd1
 [...] 04bb24f1-d631-47ff-a2ee-22d94ad4f80c -&gt; ../../sdd1</screen>
   </step>
   <step>
    <para>
     Add the corresponding line to <filename>/etc/fstab</filename> for the
     example disk <literal>osd.12</literal>:
    </para>
<screen>[...]
 UUID=04bb24f1-d631-47ff-a2ee-22d94ad4f80c /mnt/osd.12 xfs \
 defaults,errors=remount-ro 0 1
 [...]</screen>
   </step>
   <step>
    <para>
     Mount the disk:
    </para>
<screen>sudo mount /mnt/osd.12</screen>
   </step>
  </procedure>
  
  <para>
    Now you can turn the mounted device to an OSD:
  </para>
   
  <procedure>
  <step>
    <para>
     List available disks on a node:
    </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/sdb other, unknown</screen>
    
    <para>
     <filename>/dev/sdb</filename> seems to be unused, so let us focus on
     adding it as an OSD.
    </para>
   </step>
   <step>
   <warning>
     <para>
      This step&mdash;zapping of the disk&mdash;deletes all data from the disk
     </para>
    </warning>
    <para>
     Zap the disk. Zapping deletes the disk's partition table.
    </para>
<screen>ceph-deploy disk zap node2:vdc</screen>    
   </step>
   <step>
   	<para>
   	Prepare the OSD. The <command>prepare</command> command expects you to
     specify the disk for data, and optionally the disk for its journal. We
     recommend storing the journal on a separate drive to maximize throughput.
   	</para>
   	<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
   	  </step>
   	  <step>
   	  	<para>
   	  	Activate the OSD: 
   	  	</para>
   	  	<screen>ceph-deploy osd activate node2:vdc:/dev/ssd</screen>
   	  	<para>
   	  	The command checks the status of the prepared OSD and checks the OSD tree. Once the disk is activated, the command waits for the OSD to be started and then checks for possible errors.
   	  	</para>
   	  </step>
  </procedure>
   </sect3>
  </sect2>
  
  <sect2 xml:id="Removing.OSD.Nodes">
  	<title>Removing &ceph; OSD Nodes</title>
  	<note>
  	 <title>Ensure Sufficient OSD Space</title>
  	 <para>
   When removing an OSD from an existing cluster, make sure there are enough
   OSDs left in the cluster so that the replication rules can be followed. Also
   be aware that the cluster will be rebalancing for some time after removing
   the OSD.
    </para>
  	</note>
  	<para>
  	To remove a &ceph; OSD node follow the procedure:
  	</para>
  	<procedure xml:id="proc.Removing.Ceph.Node">
    <title>Removing a &ceph; OSD Node</title>
    <step>
     <para>
      Remove all OSD on the node you want to delete as described in
      <xref linkend="storage.bp.disk.del"/>.
     </para>
    </step>
    <step>
     <para>
      Verify that all OSDs have been removed:
     </para>
<screen>ceph osd tree</screen>
     <para>
      The OSD to be removed must not have any OSD.
     </para>
    </step>
    <step>
     <para>
      Remove the node from the cluster:
     </para>
<screen>ceph osd crush remove <replaceable>{bucket name}</replaceable></screen>
    </step>
   </procedure>   
  </sect2>

</sect1>

<sect1 xml:id="storage.bp.inst.rgw">
  <title>Usage of <command>ceph-deploy rgw</command></title>

  <para>
   The <command>ceph-deploy</command> script includes the
   <command>rgw</command> component that helps you manage &rgw; instances. Its
   general form follows this pattern:
  </para>

<screen>ceph-deploy rgw <replaceable>subcommand</replaceable> <replaceable>rgw-host</replaceable>:<replaceable>rgw-instance</replaceable>:<replaceable>fqdn</replaceable>:<replaceable>port</replaceable>:<replaceable>redirect</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term>subcommand</term>
    <listitem>
     <para>
      One of <command>list</command>, <command>prepare</command>,
      <command>activate</command>, <command>create</command> (=
      <command>prepare</command> + <command>activate</command>), or
      <command>delete</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-host</term>
    <listitem>
     <para>
      Host name where you want to operate the &rgw;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-instance</term>
    <listitem>
     <para>
      &ceph; instance name. Default is 'rgw-host'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>fqdn</term>
    <listitem>
     <para>
      Virtual host to listen to. Default is 'None'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>port</term>
    <listitem>
     <para>
      Port to listen to. Default is 80.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>redirect</term>
    <listitem>
     <para>
      The URL redirect. Default is '^/(.*)'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For example:
  </para>

<screen>ceph-deploy rgw prepare example_host2:gateway1</screen>

  <para>
   or
  </para>

<screen>ceph-deploy activate example_host1:gateway1:virtual_srv2:81</screen>

  <tip>
   <title>Specifying Multiple &rgw; Instances</title>
   <para>
    You can specify more <option>rgw_hostname:rgw_instance</option> pairs on
    the same command line if you separate them with a comma:
   </para>
<screen>ceph-deploy rgw create hostname1:rgw,hostname2:rgw,hostname3:rgw</screen>
  </tip>

  <para>
   For a practical example of setting &rgw; with
   <command>ceph-deploy</command>, see <xref linkend="ses.rgw.config"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw_client">
  <title>&rgw; Client Usage</title>

  <para>
   To use &rgw; REST interfaces, you need to create a user for the S3
   interface, then a subuser for the Swift interface. Find more information on
   creating &rgw; users in <xref linkend="adding.s3.swift.users"/>.
  </para>

  <sect2>
   <title>S3 Interface Access</title>
   <para>
    To access the S3 interface, you need to write a Python script. The script
    will connect to &rgw;, create a new bucket, and list all buckets. The
    values for <option>aws_access_key_id</option> and
    <option>aws_secret_access_key</option> are taken from the values of
    <option>access_key</option> and <option>secret_key</option> returned by the
    <command>radosgw_admin</command> command from
    <xref linkend="adding.s3.swift.users"/>.
   </para>
   <procedure>
    <step>
     <para>
      Install the <systemitem>python-boto</systemitem> package:
     </para>
<screen>sudo zypper in python-boto</screen>
    </step>
    <step>
     <para>
      Create a new Python script called <filename>s3test.py</filename> with the
      following content:
     </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
     <para>
      Replace <literal>{hostname}</literal> with the host name of the host
      where you configured &rgw; service, for example
      <literal>gateway_host</literal>.
     </para>
    </step>
    <step>
     <para>
      Run the script:
     </para>
<screen>python s3test.py</screen>
     <para>
      The script outputs something like the following:
     </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Swift Interface Access</title>
   <para>
    To access &rgw; via Swift interface, you need the <command>swift</command>
    command line client. Its manual page <command>man 1 swift</command> tells
    you more about its command line options.
   </para>
   <para>
    To install <command>swift</command>, run the following:
   </para>
<screen>sudo zypper in python-swiftclient</screen>
   <para>
    The swift access uses the following syntax:
   </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
   <para>
    Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
    gateway server, and <replaceable>swift_secret_key</replaceable> with its
    value from the output of the <command>radosgw-admin key create</command>
    command executed for the <systemitem>swift</systemitem> user in
    <xref linkend="adding.s3.swift.users"/>.
   </para>
   <para>
    For example:
   </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
   <para>
    The output is:
   </para>
<screen>my-new-bucket</screen>
  </sect2>
 </sect1>

</chapter>
