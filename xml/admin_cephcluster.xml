<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.ceph.cluster">
 <title>&ceph; Cluster Administration</title>
 <para>
  The chapter gives details about basic administration tasks that you may need
  to perform on your cluster deployed by using <command>ceph-deploy</command>.
  The <command>ceph-deploy</command> is a command line utility that provides
  you with a wide range of subcommands to administrate the &ceph; cluster. Most
  of the tasks can be performed without the need to stop your cluster.
 </para>
 <para>
  The general <command>ceph-deply</command> syntax is the following:
 </para>
<screen>ceph-deploy <replaceable>subcommands</replaceable> <replaceable>options</replaceable></screen>
 <para>
  A list of selected <command>ceph-deploy</command> subcommands follows.
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <command>gatherkeys</command> - the subcommand is used when you are adding
    monitors,OSDs or MDS to the cluster. it gathers authentication keys for
    provisioning new nodes. It takes host names as arguments. It checks for and
    fetches <literal>client.admin keyring</literal>, monitor keyring and
    <literal>bootstrap-mds/bootstrap-osd</literal> keyring from monitor host.
    The command syntax is the following:
   </para>
<screen>ceph-deploy gatherkeys <replaceable>hostname</replaceable></screen>
   <para>
    <replaceable>hostname</replaceable> is the host name of the monitor from
    where keys are to be pulled.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>mon add</command> - performs all steps required to add a monitor
    to your cluster, for more details reffer to
    <xref linkend="Adding.ceph.deploy.monitors"/>.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>osd prepare</command> - prepares a disk to be used as OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>osd activate</command> - activates a disk to be used as OSD.
   </para>
  </listitem>
  <listitem>
   <para>
    <command>rgw prepare/activate/create</command>
   </para>
  </listitem>
  <listitem>
   <para>
    <command>purge, purgedata, forgetkeys</command>
   </para>
  </listitem>
 </itemizedlist>
 <tip>
  <para>
   Administer &ceph; nodes with <command>ceph-deploy</command> from the admin
   node. Before administering them, always create a new temporary directory and
   <command>cd</command> into it. Then choose one monitor node and gather the
   authentication keys with the <command>gatherkeys</command> subcommand from
   it, and copy the <filename>/etc/ceph/ceph.conf</filename> file from the
   monitor node into the current local directory.
  </para>
<screen>&prompt.cephuser; mkdir ceph_tmp
&prompt.cephuser; cd ceph_tmp
&prompt.cephuser; ceph-deploy gatherkeys ceph_mon_host
&prompt.cephuser; scp ceph_mon_host:/etc/ceph/ceph.conf .</screen>
 </tip>
 <sect1 xml:id="ceph.deploy.monitors.managment">
  <title>Monitors Managment by Using <command>ceph-deploy</command></title>

  <para>
   Adding monitoring nodes to the cluster or removing nodes from the cluster
   can be performed by using <command>ceph-deploy</command> in several steps
   described in following sections. But you need to take the following into
   account:
  </para>

  <important>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <command>ceph-deploy</command> restricts you to only install one monitor
      per host.
     </para>
    </listitem>
    <listitem>
     <para>
      We do not recommend mixing monitors and OSDs on the same host.
     </para>
    </listitem>
    <listitem>
     <para>
      For high availability, you should run a production &ceph; cluster with
      <emphasis>at least</emphasis> three monitors.
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <sect2 xml:id="Adding.ceph.deploy.monitors">
   <title>Adding Monitors</title>
   <para>
    After you create a cluster and install &ceph; packages to the monitor
    host(s) (see <xref linkend="ceph.install.ceph-deploy"/> for more
    information), you may deploy the monitors to the monitor hosts. You may
    specify more monitor host names in the same command.
   </para>
   <note>
    <para>
     When adding a monitor on a host that was not in hosts initially defined
     with the <command>ceph-deploy new</command> command, a <option>public
     network</option> statement needs to be added to the
     <filename>ceph.conf</filename> file. The monitor must be added to the
     <literal>mon initial members</literal> and <literal>monmap</literal>
     options or the <option>public_addr</option> nor
     <option>public_network</option> keys must defined for monitors, otherwise
     errors arise during the monitor startup.
    </para>
   </note>
<screen>ceph-deploy mon add <replaceable>host name</replaceable></screen>
   <para>
    When you run the command, the following tasks are performed:
   </para>
   <orderedlist>
    <listitem>
     <para>
      The platform and the distribution of the target host is detected.
     </para>
    </listitem>
    <listitem>
     <para>
      The monitor is configured and added to the cluster. If there is a record
      for the particular monitor in the &ceph; configuration, the <literal>mon
      addr</literal> of the monitor will be used, otherwise
      <command>ceph-deploy</command> falls back and resolve the IP address.
     </para>
    </listitem>
    <listitem>
     <para></para>
    </listitem>
    <listitem>
     <para>
      <command>ceph-deploy</command> tries to start the monitor and checks the
      monitor status.
     </para>
    </listitem>
   </orderedlist>
  </sect2>

  <sect2 xml:id="Removing.ceph.deploy.monitors">
   <title>Removing Monitors</title>
   <para>
    If you have a monitor in your cluster that you want to remove, you may use
    the destroy option. You may specify more monitor host names in the same
    command.
   </para>
<screen>ceph-deploy mon destroy <replaceable>host-name</replaceable></screen>
   <note>
    <para>
     Ensure that if you remove a monitor, the remaining monitors will be able
     to establish a consensus. If that is not possible, consider adding a
     monitor before removing the monitor you want to take offline.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.deploy.OSD.managment">
  <title>OSDs Managment by Using <command>ceph-deploy</command></title>

  <note>
   <title>Limitations</title>
   <para>
    The procedures described further can be performed only with the default
    CRUSH map created by <command>ceph-deploy</command>.
   </para>
  </note>

  <para>
   This section gives you information how to add deploy OSD Daemons on
   particular nodes and also how to add a disk to an existing OSD node. As you
   may also want to remove a disk or the OSD daemon or the whole OSD node, this
   section provides you with procedures how to handle these tasks.
  </para>

  <sect2 xml:id="Adding.OSD.Nodes">
   <title>Adding &ceph; OSD Nodes</title>
   <tip>
    <para>
     When adding an OSD to an existing cluster, be aware that the cluster will
     be rebalancing for some time afterward. To minimize the rebalancing
     periods, it is best to add all the OSDs you intend to add at the same
     time.
    </para>
   </tip>
   <para>
    The procedure below describes adding of a &ceph; OSD node to your cluster.
   </para>
   <procedure xml:id="proc.Adding.Ceph.Node">
    <title>Adding a &ceph; OSD Node</title>
    <step>
     <para>
      List all &ceph; OSD nodes and then choose a proper name for the new
      node/s
     </para>
<screen>ceph osd tree</screen>
    </step>
    <step>
     <para>
      Inspect your CRUSH map to find out the bucket type, for a procedure refer
      to <xref linkend="op.crush"/>. Typically the bucket type is
      <emphasis>host</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Create a record for the new node in your CRUSH map.
     </para>
<screen>ceph osd crush add-bucket <replaceable>{bucket name} {bucket type}</replaceable></screen>
     <para>
      for example:
     </para>
<screen>ceph osd crush add-bucket ses4-4 host</screen>
    </step>
    <step>
     <para>
      Add all OSDs that the new node should use. For a procedure refer to
      <xref linkend="storage.bp.inst.add_osd_cephdeploy"/>.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="storage.bp.inst.add_osd_cephdeploy">
    <title>Adding OSD Disks</title>
    <para>
     In case you need to add a new OSD to a OSD node (called
     <emphasis>node2</emphasis> in our case), follow steps described further.
     Before you start, see the following points:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       This can be done on a live cluster without downtime.
      </para>
     </listitem>
     <listitem>
      <para>
       This will cause increased replication traffic between servers.
      </para>
     </listitem>
     <listitem>
      <para>
       Doing this operation repeatedly before the last operation has completed
       replication can save the cluster overall rebuild time.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The disk must be prepared and mounted as described in the following
     procedure. Let's use a device <literal>/dev/sdd</literal> as an example.
    </para>
    <procedure>
     <step>
      <para>
       Create a partition <literal>sdd1</literal> on the disk:
      </para>
<screen>sudo parted /dev/sdd1 mkpart primary 0.0 -1s</screen>
     </step>
     <step>
      <para>
       Format the partition with XFS file system:
      </para>
<screen>sudo mkfs.xfs -f /dev/sdd1</screen>
     </step>
     <step>
      <para>
       Find out the UUID (Universally Unique Identifier) of the disk:
      </para>
<screen>ls -l /dev/disk/by-uuid | grep sdd1
 [...] 04bb24f1-d631-47ff-a2ee-22d94ad4f80c -&gt; ../../sdd1</screen>
     </step>
     <step>
      <para>
       Add the corresponding line to <filename>/etc/fstab</filename> for the
       example disk <literal>osd.12</literal>:
      </para>
<screen>[...]
 UUID=04bb24f1-d631-47ff-a2ee-22d94ad4f80c /mnt/osd.12 xfs \
 defaults,errors=remount-ro 0 1
 [...]</screen>
     </step>
     <step>
      <para>
       Mount the disk:
      </para>
<screen>sudo mount /mnt/osd.12</screen>
     </step>
    </procedure>
    <para>
     Now you can turn the mounted device to an OSD, for details refer to
     <xref linkend="adding.osd.ceph.cluster"/>.
    </para>
   </sect3>
   <sect3 xml:id="adding.osd.ceph.cluster">
    <title>Adding OSDs</title>
    <para>
     To add an OSD you can use two tools&mdash;<command>ceph-deploy</command>
     or <command>ceph-disk</command>. For using <command>ceph-deploy</command>
     refer to <xref linkend="ceph_deploy"/>. For using
     <command>ceph-disk</command>, refer to <xref linkend="ceph_disk"/>.
    </para>
    <procedure xml:id="ceph_deploy">
     <title>Adding OSD Using <command>ceph-deploy</command></title>
     <step>
      <para>
       List available disks on a node:
      </para>
<screen>ceph-deploy disk list node2
[...]
[node2][DEBUG ] /dev/sr0 other, unknown
[node2][DEBUG ] /dev/vda :
[node2][DEBUG ]  /dev/vda1 swap, swap
[node2][DEBUG ]  /dev/vda2 other, btrfs, mounted on /
[node2][DEBUG ] /dev/vdb :
[node2][DEBUG ]  /dev/vdb1 ceph data, active, cluster ceph, osd.1, journal /dev/vdb2
[node2][DEBUG ]  /dev/vdb2 ceph journal, for /dev/vdb1
[node2][DEBUG ] /dev/sdb other, unknown</screen>
      <para>
       <filename>/dev/sdb</filename> seems to be unused, so let us focus on
       adding it as an OSD.
      </para>
     </step>
     <step>
      <warning>
       <para>
        This step&mdash;zapping of the disk&mdash;deletes all data from the
        disk
       </para>
      </warning>
      <para>
       Zap the disk. Zapping deletes the disk's partition table.
      </para>
<screen>ceph-deploy disk zap node2:vdc</screen>
     </step>
     <step>
      <para>
       Prepare the OSD. The <command>prepare</command> command expects you to
       specify the disk for data, and optionally the disk for its journal. We
       recommend storing the journal on a separate drive to maximize
       throughput.
      </para>
<screen>ceph-deploy osd prepare node2:vdc:/dev/ssd</screen>
     </step>
     <step>
      <para>
       Activate the OSD:
      </para>
<screen>ceph-deploy osd activate node2:vdc:/dev/ssd</screen>
      <para>
       The command checks the status of the prepared OSD and checks the OSD
       tree. Once the disk is activated, the command waits for the OSD to be
       started and then checks for possible errors.
      </para>
     </step>
    </procedure>
    <para>
     <command>ceph-disk</command> is a utility that can prepare and activate a
     disk, partition or directory as a &ceph; OSD. It automates the multiple
     steps involved in manual creation and start of an OSD into two steps of
     preparing and activating the OSD by using the subcommands
     <command>prepare</command> and <command>activate</command>.
    </para>
    <variablelist>
     <varlistentry>
      <term><command>prepare</command>
      </term>
      <listitem>
       <para>
        Prepares a directory, disk or drive for a &ceph; OSD. It creates a GPT
        partition, marks the partition with &ceph; type uuid, creates a file
        system, marks the file system as ready for &ceph; consumption, uses
        entire partition and adds a new partition to the journal disk.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>activate</command>
      </term>
      <listitem>
       <para>
        Activates the &ceph; OSD. It mounts the volume in a temporary location,
        allocates an OSD ID (if needed), remounts in the correct location
        <filename>/var/lib/ceph/osd/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></filename>
        and starts <command>ceph-osd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <procedure xml:id="ceph_disk">
     <title>Adding OSD Using <command>ceph-disk</command></title>
     <step>
      <para>
       Make sure a new disk is physically present on the node where you want to
       add the OSD. In our example, it is <emphasis>node1</emphasis> belonging
       to cluster <emphasis>ceph</emphasis>.
      </para>
     </step>
     <step>
      <para>
       <command>ssh</command> to node1.
      </para>
     </step>
     <step>
      <para>
       Generate a unique identification for the new OSD:
      </para>
<screen>uuidgen
c70c032a-6e88-4962-8376-4aa119cb52ee</screen>
     </step>
     <step>
      <para>
       Prepare the disk:
      </para>
<screen>sudo ceph-disk prepare --cluster ceph \
--cluster-uuid c70c032a-6e88-4962-8376-4aa119cb52ee --fs-type xfs /dev/hdd1</screen>
     </step>
     <step>
      <para>
       Activate the OSD:
      </para>
<screen>sudo ceph-disk activate /dev/hdd1</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="Removing.OSD.Nodes">
   <title>Removing &ceph; OSD Nodes</title>
   <note>
    <title>Ensure Sufficient OSD Space</title>
    <para>
     When removing an OSD from an existing cluster, make sure there are enough
     OSDs left in the cluster so that the replication rules can be followed.
     Also be aware that the cluster will be rebalancing for some time after
     removing the OSD.
    </para>
   </note>
   <para>
    To remove a &ceph; OSD node follow the procedure:
   </para>
   <procedure xml:id="proc.Removing.Ceph.Node">
    <title>Removing a &ceph; OSD Node</title>
    <step>
     <para>
      Remove all OSD on the node you want to delete as described in
      <xref linkend="storage.bp.disk.del"/>.
     </para>
    </step>
    <step>
     <para>
      Verify that all OSDs have been removed:
     </para>
<screen>ceph osd tree</screen>
     <para>
      The OSD to be removed must not have any OSD.
     </para>
    </step>
    <step>
     <para>
      Remove the node from the cluster:
     </para>
<screen>ceph osd crush remove <replaceable>{bucket name}</replaceable></screen>
    </step>
   </procedure>
   <sect3 xml:id="storage.bp.disk.del">
    <title>Removing OSDs</title>
    <important>
     <itemizedlist mark="bullet" spacing="normal">
      <listitem>
       <para>
        This can be done on a live cluster without downtime.
       </para>
      </listitem>
      <listitem>
       <para>
        This will cause increased replication traffic between servers.
       </para>
      </listitem>
      <listitem>
       <para>
        Be sure not to remove too many disks from your cluster to be able to
        keep the replication rules. See <xref linkend="datamgm.rules"/> for
        more information.
       </para>
      </listitem>
     </itemizedlist>
    </important>
    <para>
     To delete a disk (for example <literal>osd.12</literal>) from a &ceph;
     cluster, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Make sure you have the right disk:
      </para>
<screen>ceph osd tree</screen>
     </step>
     <step>
      <para>
       If the disk is a member of a pool and/or active:
      </para>
      <substeps performance="required">
       <step>
        <para>
         <emphasis>Drain</emphasis> the OSD by setting its weight to zero:
        </para>
<screen>ceph osd crush reweight osd.12 0</screen>
        <para>
         Then wait for all the placement groups to be moved away to other OSDs
         with <command>ceph -w</command>. Optionally, you can check if the OSD
         is emptying with <command>df -h</command>.
        </para>
       </step>
       <step>
        <para>
         Mark the disk out:
        </para>
<screen>ceph osd out 12</screen>
       </step>
       <step>
        <para>
         Stop the related OSD service:
        </para>
<screen>sudo systemctl stop ceph-osd@12.service</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Remove the disk from &crushmap;:
      </para>
<screen>ceph osd crush remove osd.12</screen>
     </step>
     <step>
      <para>
       Remove authentication information for the disk:
      </para>
<screen>ceph auth del osd.12</screen>
     </step>
     <step>
      <para>
       Remove the disk from the cluster:
      </para>
<screen>ceph osd rm 12</screen>
     </step>
     <step>
      <para>
       Wipe the disk to remove all the data:
      </para>
<screen>sudo sgdisk --zap-all -- <replaceable>disk_device_name</replaceable>
sudo sgdisk --clear --mbrtogpt -- <replaceable>disk_device_name</replaceable></screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw">
  <title>Usage of <command>ceph-deploy rgw</command></title>

  <para>
   The <command>ceph-deploy</command> script includes the
   <command>rgw</command> component that helps you manage &rgw; instances. Its
   general form follows this pattern:
  </para>

<screen>ceph-deploy rgw <replaceable>subcommand</replaceable> <replaceable>rgw-host</replaceable>:<replaceable>rgw-instance</replaceable>:<replaceable>fqdn</replaceable>:<replaceable>port</replaceable>:<replaceable>redirect</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term>subcommand</term>
    <listitem>
     <para>
      One of <command>list</command>, <command>prepare</command>,
      <command>activate</command>, <command>create</command> (=
      <command>prepare</command> + <command>activate</command>), or
      <command>delete</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-host</term>
    <listitem>
     <para>
      Host name where you want to operate the &rgw;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>rgw-instance</term>
    <listitem>
     <para>
      &ceph; instance name. Default is 'rgw-host'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>fqdn</term>
    <listitem>
     <para>
      Virtual host to listen to. Default is 'None'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>port</term>
    <listitem>
     <para>
      Port to listen to. Default is 80.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>redirect</term>
    <listitem>
     <para>
      The URL redirect. Default is '^/(.*)'.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   For example:
  </para>

<screen>ceph-deploy rgw prepare example_host2:gateway1</screen>

  <para>
   or
  </para>

<screen>ceph-deploy activate example_host1:gateway1:virtual_srv2:81</screen>

  <tip>
   <title>Specifying Multiple &rgw; Instances</title>
   <para>
    You can specify more <option>rgw_hostname:rgw_instance</option> pairs on
    the same command line if you separate them with a comma:
   </para>
<screen>ceph-deploy rgw create hostname1:rgw,hostname2:rgw,hostname3:rgw</screen>
  </tip>

  <para>
   For a practical example of setting &rgw; with
   <command>ceph-deploy</command>, see <xref linkend="ses.rgw.config"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.inst.rgw_client">
  <title>&rgw; Client Usage</title>

  <para>
   To use &rgw; REST interfaces, you need to create a user for the S3
   interface, then a subuser for the Swift interface. Find more information on
   creating &rgw; users in <xref linkend="adding.s3.swift.users"/>.
  </para>

  <sect2>
   <title>S3 Interface Access</title>
   <para>
    To access the S3 interface, you need to write a Python script. The script
    will connect to &rgw;, create a new bucket, and list all buckets. The
    values for <option>aws_access_key_id</option> and
    <option>aws_secret_access_key</option> are taken from the values of
    <option>access_key</option> and <option>secret_key</option> returned by the
    <command>radosgw_admin</command> command from
    <xref linkend="adding.s3.swift.users"/>.
   </para>
   <procedure>
    <step>
     <para>
      Install the <systemitem>python-boto</systemitem> package:
     </para>
<screen>sudo zypper in python-boto</screen>
    </step>
    <step>
     <para>
      Create a new Python script called <filename>s3test.py</filename> with the
      following content:
     </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
     <para>
      Replace <literal>{hostname}</literal> with the host name of the host
      where you configured &rgw; service, for example
      <literal>gateway_host</literal>.
     </para>
    </step>
    <step>
     <para>
      Run the script:
     </para>
<screen>python s3test.py</screen>
     <para>
      The script outputs something like the following:
     </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
    </step>
   </procedure>
  </sect2>

  <sect2>
   <title>Swift Interface Access</title>
   <para>
    To access &rgw; via Swift interface, you need the <command>swift</command>
    command line client. Its manual page <command>man 1 swift</command> tells
    you more about its command line options.
   </para>
   <para>
    To install <command>swift</command>, run the following:
   </para>
<screen>sudo zypper in python-swiftclient</screen>
   <para>
    The swift access uses the following syntax:
   </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
   <para>
    Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
    gateway server, and <replaceable>swift_secret_key</replaceable> with its
    value from the output of the <command>radosgw-admin key create</command>
    command executed for the <systemitem>swift</systemitem> user in
    <xref linkend="adding.s3.swift.users"/>.
   </para>
   <para>
    For example:
   </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
   <para>
    The output is:
   </para>
<screen>my-new-bucket</screen>
  </sect2>
 </sect1>
</chapter>
