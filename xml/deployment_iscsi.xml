<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.as.iscsi">
<!-- ============================================================== -->
 <title>Installation of iSCSI Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <emphasis>initiators</emphasis>) to send SCSI commands to SCSI storage
  devices (<emphasis>targets</emphasis>) on remote servers. &storage; includes
  a facility that opens &ceph; storage management to heterogeneous clients,
  such as &mswin;* and &vmware;* vSphere, through the iSCSI protocol. Multipath
  iSCSI access enables availability and scalability for these clients, and the
  standardized iSCSI protocol also provides an additional layer of security
  isolation between clients and the &storage; cluster. The configuration
  facility is named <systemitem>lrbd</systemitem>. Using
  <systemitem>lrbd</systemitem>, &ceph; storage administrators can define
  thin-provisioned, replicated, highly-available volumes supporting read-only
  snapshots, read-write clones, and automatic resizing with &ceph; RADOS Block
  Device (RBD). Administrators can then export volumes either via a single
  <systemitem>lrbd</systemitem> gateway host, or via multiple gateway hosts
  supporting multipath failover. Linux, &mswin;, and &vmware; hosts can connect
  to volumes using the iSCSI protocol, which makes them available like any
  other SCSI block device. This means &storage; customers can effectively run a
  complete block-storage infrastructure subsystem on &ceph; that provides all
  the features and benefits of a conventional SAN, enabling future growth.
 </para>
 <para>
  This chapter introduces detailed information to set up a &ceph; cluster
  infrastructure together with an iSCSI gateway so that the client hosts can
  use remotely stored data as local storage devices using the iSCSI protocol.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>iSCSI Block Storage</title>

  <para>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720. iSCSI
   is implemented as a service where a client (the initiator) talks to a server
   (the target) via a session on TCP port 3260. An iSCSI target's IP address
   and port are called an iSCSI portal, where a target can be exposed through
   one or more portals. The combination of a target and one or more portals is
   called the target portal group (TPG).
  </para>

  <para>
   The underlying data link layer protocol for iSCSI is commonly Ethernet. More
   specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet or faster
   networks for optimal throughput. 10 Gigabit Ethernet connectivity between
   the iSCSI gateway and the back-end &ceph; cluster is strongly recommended.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>The Linux Kernel iSCSI Target</title>
   <para>
    The Linux kernel iSCSI target was originally named LIO for linux-iscsi.org,
    the project's original domain and Web site. For some time, no fewer than
    four competing iSCSI target implementations were available for the Linux
    platform, but LIO ultimately prevailed as the single iSCSI reference
    target. The mainline kernel code for LIO uses the simple, but somewhat
    ambiguous name "target", distinguishing between "target core" and a variety
    of front-end and back-end target modules.
   </para>
   <para>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol is
    supported by &storage;.
   </para>
   <para>
    The most frequently used target back-end module is one that is capable of
    simply re-exporting any available block device on the target host. This
    module is named iblock. However, LIO also has an RBD-specific back-end
    module supporting parallelized multipath I/O access to RBD images.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>iSCSI Initiators</title>
   <para>
    This section introduces brief information on iSCSI initiators used on
    Linux, &mswin;, and &vmware; platforms.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     The standard initiator for the Linux platform is
     <systemitem>open-iscsi</systemitem>. <systemitem>open-iscsi</systemitem>
     launches a daemon, <systemitem>iscsid</systemitem>, which the user can
     then use to discover iSCSI targets on any given portal, log in to targets,
     and map iSCSI volumes. <systemitem>iscsid</systemitem> communicates with
     the SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <systemitem>open-iscsi</systemitem> initiator can be deployed in
     conjunction with the Device Mapper Multipath
     (<systemitem>dm-multipath</systemitem>) facility to provide a highly
     available iSCSI block device.
    </para>
   </sect3>
   <sect3>
    <title>&mswin; and &hyper;</title>
    <para>
     The default iSCSI initiator for the &mswin; operating system is the
     Microsoft iSCSI initiator. The iSCSI service can be configured via a
     graphical user interface (GUI), and supports multipath I/O for high
     availability.
    </para>
   </sect3>
   <sect3>
    <title>&vmware;</title>
    <para>
     The default iSCSI initiator for &vmware; vSphere and ESX is the &vmware;
     ESX software iSCSI initiator, <systemitem>vmkiscsi</systemitem>. When
     enabled, it can be configured either from the vSphere client, or using the
     <command>vmkiscsi-tool</command> command. You can then format storage
     volumes connected through the vSphere iSCSI storage adapter with VMFS, and
     use them like any other VM storage device. The &vmware; initiator also
     supports multipath I/O for high availability.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrbd">
  <title>General Information about lrbd</title>

  <para>
   <systemitem>lrbd</systemitem> combines the benefits of RADOS Block Devices
   with the ubiquitous versatility of iSCSI. By employing
   <systemitem>lrbd</systemitem> on an iSCSI target host (known as the
   <systemitem>lrbd</systemitem> gateway), any application that needs to make
   use of block storage can benefit from &ceph;, even if it does not speak any
   &ceph; client protocol. Instead, users can use iSCSI or any other target
   front-end protocol to connect to an LIO target, which translates all target
   I/O to RBD storage operations.
  </para>

  <figure>
   <title>&ceph; Cluster with a Single iSCSI Gateway</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   <systemitem>lrbd</systemitem> is inherently highly-available and supports
   multipath operations. Thus, downstream initiator hosts can use multiple
   iSCSI gateways for both high availability and scalability. When
   communicating with an iSCSI configuration with more than one gateway,
   initiators may load-balance iSCSI requests across multiple gateways. In the
   event of a gateway failing, being temporarily unreachable, or being disabled
   for maintenance, I/O will transparently continue via another gateway.
  </para>

  <figure>
   <title>&ceph; Cluster with Multiple iSCSI Gateways</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Deployment Considerations</title>

  <para>
   A minimum configuration of &storage; with <systemitem>lrbd</systemitem>
   consists of the following components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. The &ceph; cluster consists of a minimum of four
     physical servers hosting at least eight object storage daemons (OSDs)
     each. In such a configuration, three OSD nodes also double as a monitor
     (MON) host.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI target server running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI initiator host, running <systemitem>open-iscsi</systemitem>
     (Linux), the Microsoft iSCSI Initiator (&mswin;), or any other compatible
     iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A recommended production configuration of &storage; with
   <systemitem>lrbd</systemitem> consists of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. A production &ceph; cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running 10-12
     object storage daemons (OSDs), with no fewer than three dedicated MON
     hosts.
    </para>
   </listitem>
   <listitem>
    <para>
     Several iSCSI target servers running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>. For iSCSI fail-over and load-balancing,
     these servers must run a kernel supporting the
     <systemitem>target_core_rbd</systemitem> module. Update packages are
     available from the &sls; maintenance channel.
    </para>
   </listitem>
   <listitem>
    <para>
     Any number of iSCSI initiator hosts, running
     <systemitem>open-iscsi</systemitem> (Linux), the Microsoft iSCSI Initiator
     (&mswin;), or any other compatible iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation and Configuration</title>

  <para>
   This section describes steps to install and configure an &igw; on top of
   &storage;.
  </para>

  <sect2>
   <title>Deploy the &igw; to a &ceph; Cluster</title>
   <para>
    You can deploy the &igw; either during &ceph; cluster deployment process,
    or add it to an existing cluster using &deepsea;.
   </para>
   <para>
    To include the &igw; during the cluster deployment process, refer to
    <xref linkend="policy.role.assignment"/>.
   </para>
   <para>
    To add the &igw; to an existing cluster, refer to
    <xref linkend="salt.adding.services"/>.
   </para>
  </sect2>

  <sect2>
   <title>Create RBD Images</title>
   <para>
    RBD images are created in the &ceph; store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this purpose.
    You can create a volume from any host that is able to connect to your
    storage cluster using the &ceph; <command>rbd</command> command line
    utility. This requires the client to have at least a minimal ceph.conf
    configuration file, and appropriate CephX authentication credentials.
   </para>
   <para>
    To create a new volume for subsequent export via iSCSI, use the
    <command>rbd create</command> command, specifying the volume size in
    megabytes. For example, in order to create a 100 GB volume named
    <literal>testvol</literal> in the pool named <literal>iscsi</literal>, run:
   </para>
<screen>&prompt.root;rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    The above command creates an RBD volume in the default format 2.
   </para>
   <note>
    <para>
     Since &storage; 3, the default volume format is 2, and format 1 is
     deprecated. However, you can still create the deprecated format 1 volumes
     with the <option>--image-format 1</option> option.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.export">
   <title>Export RBD Images via iSCSI</title>
   <para>
    To export RBD images via iSCSI, use the <systemitem>lrbd</systemitem>
    utility. <systemitem>lrbd</systemitem> allows you to create, review, and
    modify the iSCSI target configuration, which uses a JSON format.
   </para>
   <tip>
    <title>Import Changes into &oa;</title>
    <para>
     Any changes made to the &igw; configuration using the
     <command>lrbd</command> command are not visible to &deepsea; and &oa;. To
     import your manual changes, you need to export the &igw; configuration to
     a file:
    </para>
<screen>
&prompt.sminion;lrbd -o /tmp/lrbd.conf
</screen>
    <para>
     Then copy it to the &smaster; so that &deepsea; and &oa; can see it:
    </para>
<screen>
&prompt.sminion;scp /tmp/lrbd.conf ses5master:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
    <para>
     Finally, edit <filename>/srv/pillar/ceph/stack/global.yml</filename> and
     set:
    </para>
<screen>
igw_config: default-ui
</screen>
   </tip>
   <para>
    In order to edit the configuration, use <command>lrbd -e</command> or
    <command>lrbd --edit</command>. This command will invoke the default
    editor, as defined by the <literal>EDITOR</literal> environment variable.
    You may override this behavior by setting the <option>-E</option> option in
    addition to <option>-e</option>.
   </para>
   <para>
    Below is an example configuration for
   </para>
   <itemizedlist>
    <listitem>
     <para>
      two iSCSI gateway hosts named <literal>iscsi1.example.com</literal> and
      <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      with a single iSCSI Logical Unit (LU),
     </para>
    </listitem>
    <listitem>
     <para>
      backed by an RBD image named <literal>testvol</literal> in the RADOS pool
      <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      and exporting the target via two portals named "east" and "west":
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "authentication": "none"
        }
    ],
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "hosts": [
                {
                    "host": "iscsi1.example.com",
                    "portal": "east"
                },
                {
                    "host": "iscsi2.example.com",
                    "portal": "west"
                }
            ]
        }
    ],
    "portals": [
        {
            "name": "east",
            "addresses": [
                "192.168.124.104"
            ]
        },
        {
            "name": "west",
            "addresses": [
                "192.168.124.105"
            ]
        }
    ],
    "pools": [
        {
            "pool": "rbd",
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</screen>
   <para>
    Note that whenever you refer to a host name in the configuration, this host
    name must match the iSCSI gateway's <command>uname -n</command> command
    output.
   </para>
   <para>
    The edited JSON is stored in the extended attributes (xattrs) of a single
    RADOS object per pool. This object is available to the gateway hosts where
    the JSON is edited, as well as to all gateway hosts connected to the same
    &ceph; cluster. No configuration information is stored locally on the
    <systemitem>lrbd</systemitem> gateway.
   </para>
   <para>
    To activate the configuration, store it in the &ceph; cluster, and do one
    of the following things (as &rootuser;):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run the <command>lrbd</command> command (without additional options) from
      the command line,
     </para>
    </listitem>
   </itemizedlist>
   <para>
    or
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Restart the <systemitem>lrbd</systemitem> service with <command>service
      lrbd restart</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <systemitem>lrbd</systemitem> "service" does not operate any background
    daemon. Instead, it simply invokes the <command>lrbd</command> command.
    This type of service is known as a "one-shot" service.
   </para>
   <para>
    You should also enable <systemitem>lrbd</systemitem> to auto-configure on
    system start-up. To do so, run the <command>systemctl enable lrbd</command>
    command.
   </para>
   <para>
    The configuration above reflects a simple, one-gateway setup.
    <systemitem>lrbd</systemitem> configuration can be much more complex and
    powerful. The <systemitem>lrbd</systemitem> RPM package comes with an
    extensive set of configuration examples, which you may refer to by checking
    the content of the
    <filename>/usr/share/doc/packages/lrbd/samples</filename> directory after
    installation. The samples are also available from
    <link xlink:href="https://github.com/SUSE/lrbd/tree/master/samples"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.optional">
   <title>Optional Settings</title>
   <para>
    The following settings may be useful for some environments. For images,
    there are <option>uuid</option>, <option>lun</option>,
    <option>retries</option>, <option>sleep</option>, and
    <option>retry_errors</option> attributes. The first
    two&mdash;<option>uuid</option> and <option>lun</option>&mdash;allow
    hardcoding of the 'uuid' or 'lun' for a specific image. You can specify
    either of them for an image. The <option>retries</option>,
    <option>sleep</option> and <option>retry_errors</option> affect attempts to
    map an rbd image.
   </para>
   <tip>
    <para>
     If a site needs statically assigned LUNs, then assign numbers to each LUN.
    </para>
   </tip>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "uuid": "12345678-abcd-9012-efab-345678901234",
                        "lun": "2",
                        "retries": "3",
                        "sleep": "4",
                        "retry_errors": [ 95 ],
                        [...]
                    }
                ]
            }
        ]
    }
]</screen>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.advanced">
   <title>Advanced Settings</title>
   <para>
    <systemitem>lrbd</systemitem> can be configured with advanced parameters
    which are subsequently passed on to the LIO I/O target. The parameters are
    divided up into iSCSI and backing store components, which can then be
    specified in the "targets" and "tpg" sections, respectively, of the
    <systemitem>lrbd</systemitem> configuration.
   </para>
   <warning>
    <para>
     Changing these parameters from the default setting is not recommended.
    </para>
   </warning>
<screen>"targets": [
    {
        [...]
        "tpg_default_cmdsn_depth": "64",
        "tpg_default_erl": "0",
        "tpg_login_timeout": "10",
        "tpg_netif_timeout": "2",
        "tpg_prod_mode_write_protect": "0",
    }
]</screen>
   <para>
    A description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>tpg_default_cmdsn_depth</term>
     <listitem>
      <para>
       Default CmdSN (Command Sequence Number) depth. Limits the amount of
       requests that an iSCSI initiator can have outstanding at any moment.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_default_erl</term>
     <listitem>
      <para>
       Default error recovery level.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_login_timeout</term>
     <listitem>
      <para>
       Login timeout value in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_netif_timeout</term>
     <listitem>
      <para>
       NIC failure timeout in seconds.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_prod_mode_write_protect</term>
     <listitem>
      <para>
       If set to 1, prevents writes to LUNs.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "backstore_block_size": "512",
                        "backstore_emulate_3pc": "1",
                        "backstore_emulate_caw": "1",
                        "backstore_emulate_dpo": "0",
                        "backstore_emulate_fua_read": "0",
                        "backstore_emulate_fua_write": "1",
                        "backstore_emulate_model_alias": "0",
                        "backstore_emulate_rest_reord": "0",
                        "backstore_emulate_tas": "1",
                        "backstore_emulate_tpu": "0",
                        "backstore_emulate_tpws": "0",
                        "backstore_emulate_ua_intlck_ctrl": "0",
                        "backstore_emulate_write_cache": "0",
                        "backstore_enforce_pr_isids": "1",
                        "backstore_fabric_max_sectors": "8192",
                        "backstore_hw_block_size": "512",
                        "backstore_hw_max_sectors": "8192",
                        "backstore_hw_pi_prot_type": "0",
                        "backstore_hw_queue_depth": "128",
                        "backstore_is_nonrot": "1",
                        "backstore_max_unmap_block_desc_count": "1",
                        "backstore_max_unmap_lba_count": "8192",
                        "backstore_max_write_same_len": "65535",
                        "backstore_optimal_sectors": "8192",
                        "backstore_pi_prot_format": "0",
                        "backstore_pi_prot_type": "0",
                        "backstore_queue_depth": "128",
                        "backstore_unmap_granularity": "8192",
                        "backstore_unmap_granularity_alignment": "4194304"
                    }
                ]
            }
        ]
    }
]</screen>
   <para>
    A description of the options follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>backstore_block_size</term>
     <listitem>
      <para>
       Block size of the underlying device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_3pc</term>
     <listitem>
      <para>
       If set to 1, enables Third Party Copy.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_caw</term>
     <listitem>
      <para>
       If set to 1, enables Compare and Write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_dpo</term>
     <listitem>
      <para>
       If set to 1, turns on Disable Page Out.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_read</term>
     <listitem>
      <para>
       If set to 1, enables Force Unit Access read.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_write</term>
     <listitem>
      <para>
       If set to 1, enables Force Unit Access write.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_model_alias</term>
     <listitem>
      <para>
       If set to 1, uses the back-end device name for the model alias.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_rest_reord</term>
     <listitem>
      <para>
       If set to 0, the Queue Algorithm Modifier has Restricted Reordering.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tas</term>
     <listitem>
      <para>
       If set to 1, enables Task Aborted Status.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpu</term>
     <listitem>
      <para>
       If set to 1, enables Thin Provisioning Unmap.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpws</term>
     <listitem>
      <para>
       If set to 1, enables Thin Provisioning Write Same.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_ua_intlck_ctrl</term>
     <listitem>
      <para>
       If set to 1, enables Unit Attention Interlock.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_write_cache</term>
     <listitem>
      <para>
       If set to 1, turns on Write Cache Enable.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_enforce_pr_isids</term>
     <listitem>
      <para>
       If set to 1, enforces persistent reservation ISIDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_fabric_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the fabric can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_block_size</term>
     <listitem>
      <para>
       Hardware block size in bytes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_max_sectors</term>
     <listitem>
      <para>
       Maximum number of sectors the hardware can transfer at once.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_pi_prot_type</term>
     <listitem>
      <para>
       If non-zero, DIF protection is enabled on the underlying hardware.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_queue_depth</term>
     <listitem>
      <para>
       Hardware queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_is_nonrot</term>
     <listitem>
      <para>
       If set to 1, the backstore is a non-rotational device.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_block_desc_count</term>
     <listitem>
      <para>
       Maximum number of block descriptors for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_lba_count:</term>
     <listitem>
      <para>
       Maximum number of LBAs for UNMAP.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_write_same_len</term>
     <listitem>
      <para>
       Maximum length for WRITE_SAME.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_optimal_sectors</term>
     <listitem>
      <para>
       Optimal request size in sectors.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_format</term>
     <listitem>
      <para>
       DIF protection format.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_type</term>
     <listitem>
      <para>
       DIF protection type.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_queue_depth</term>
     <listitem>
      <para>
       Queue depth.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity</term>
     <listitem>
      <para>
       UNMAP granularity.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity_alignment</term>
     <listitem>
      <para>
       UNMAP granularity alignment.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For targets, the <option>tpg</option> attributes allow tuning of kernel
    parameters. Use with caution.
   </para>
<screen>"targets": [
{
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "tpg_default_cmdsn_depth": "64",
    "tpg_default_erl": "0",
    "tpg_login_timeout": "10",
    "tpg_netif_timeout": "2",
    "tpg_prod_mode_write_protect": "0",
    "tpg_t10_pi": "0"
}</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="iscsi.tcmu">
  <title>Exporting &rbd; Images using <systemitem>tcmu-runner</systemitem></title>

  <para>
   Since version 5, &storage; ships a user space RBD back-end for
   <systemitem>tcmu-runner</systemitem> (see <command>man 8
   tcmu-runner</command> for details).
  </para>

  <warning>
   <title>Technology Preview</title>
   <para>
    <systemitem>tcmu-runner</systemitem> based &igw; deployments are currently
    a technology preview. See <xref linkend="cha.ceph.as.iscsi"/> for
    instructions on kernel-based &igw; deployment with
    <systemitem>lrbd</systemitem>.
   </para>
  </warning>

  <para>
   Unlike kernel-based <systemitem>lrbd</systemitem> &igw; deployments,
   <systemitem>tcmu-runner</systemitem> based &igw;s do not offer support for
   multipath I/O or SCSI Persistent Reservations.
  </para>

  <para>
   As &deepsea; and &oa; do not currently support
   <systemitem>tcmu-runner</systemitem> deployments, you need to manage the
   installation, deployment, and monitoring manually.
  </para>

  <sect2 xml:id="iscsi.tcmu.install">
   <title>Installation</title>
   <para>
    On your &igw; node, install the
    <systemitem>tcmu-runner-handler-rbd</systemitem> package from the &storage;
    5 media, together with the <systemitem>libtcmu1</systemitem> and
    <systemitem>tcmu-runner</systemitem> package dependencies. Install the
    <systemitem>targetcli-fb</systemitem> package for configuration purposes.
    Note that the <systemitem>targetcli-fb</systemitem> package is incompatible
    with the 'non-fb' version of the <systemitem>targetcli</systemitem>
    package.
   </para>
   <para>
    Confirm that the <systemitem>tcmu-runner</systemitem> &systemd; service is
    running:
   </para>
<screen>&prompt.root;systemctl enable tcmu-runner
tcmu-gw:~ # systemctl status tcmu-runner
‚óè tcmu-runner.service - LIO Userspace-passthrough daemon
  Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; static; vendor
  preset: disabled)
    Active: active (running) since ...</screen>
  </sect2>

  <sect2 xml:id="iscsi.tcmu.depl">
   <title>Configuration and Deployment</title>
   <para>
    Create a &rbd; image on your existing &ceph; cluster. In the following
    example, we will use a 10G image called 'tcmu-lu' located in the 'rbd'
    pool.
   </para>
   <para>
    Following &rbd; image creation, run <command>targetcli</command>, and
    ensure that the tcmu-runner RBD handler (plug-in) is available:
   </para>
<screen>&prompt.root;targetcli
targetcli shell version 2.1.fb46
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/&gt; ls
o- / ................................... [...]
  o- backstores ........................ [...]
...
  | o- user:rbd ......... [Storage Objects: 0]</screen>
   <para>
    Create a backstore configuration entry for the RBD image:
   </para>
<screen>/&gt; cd backstores/user:rbd
/backstores/user:rbd&gt; create tcmu-lu 10G /rbd/tcmu-lu
Created user-backed storage object tcmu-lu size 10737418240.</screen>
   <para>
    Create an &iscsi; transport configuration entry. In the following example,
    the target IQN "iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a" is
    automatically generated by <command>targetcli</command> for use as a unique
    &iscsi; target identifier:
   </para>
<screen>/backstores/user:rbd&gt; cd /iscsi
/iscsi&gt; create
Created target iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.</screen>
   <para>
    Create an ACL entry for the &iscsi; initiator(s) that you want to connect
    to the target. In the following example, an initiator IQN of
    "iqn.1998-01.com.vmware:esxi-872c4888" is used:
   </para>
<screen>/iscsi&gt; cd
iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a/tpg1/acls/
/iscsi/iqn.20...a3a/tpg1/acls&gt; create iqn.1998-01.com.vmware:esxi-872c4888</screen>
   <para>
    Finally, link the previously created RBD backstore configuration to the
    &iscsi; target:
   </para>
<screen>/iscsi/iqn.20...a3a/tpg1/acls&gt; cd ../luns
/iscsi/iqn.20...a3a/tpg1/luns&gt; create /backstores/user:rbd/tcmu-lu
Created LUN 0.
Created LUN 0-&gt;0 mapping in node ACL iqn.1998-01.com.vmware:esxi-872c4888</screen>
   <para>
    Exit the shell to save the existing configuration:
   </para>
<screen>/iscsi/iqn.20...a3a/tpg1/luns&gt; exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup.
Configuration saved to /etc/target/saveconfig.json</screen>
  </sect2>

  <sect2 xml:id="iscsi.tcmu.use">
   <title>Usage</title>
   <para>
    From your &iscsi; initiator (client) node, connect to your newly
    provisioned &iscsi; target using the IQN and host name configured above.
   </para>
  </sect2>
 </sect1>
</chapter>
