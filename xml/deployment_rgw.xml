<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.additional.software.installation">
<!-- ============================================================== -->
 <title>&ceph; &rgw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph RADOS Gateway is an object storage interface built on top of
  <literal>librgw</literal> to provide applications with a RESTful gateway to
  Ceph Storage Clusters. Ceph Object Storage supports two interfaces:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Ceph Object Storage uses the Ceph RADOS Gateway daemon
  (<command>radosgw</command>), which uses an embedded HTTP server (CivetWeb)
  for interacting with a Ceph Storage Cluster. Since it provides interfaces
  compatible with OpenStack Swift and Amazon S3, the Ceph RADOS Gateway has its
  own user management. Ceph RADOS Gateway can store data in the same Ceph
  Storage Cluster used to store data from Ceph File System clients or Ceph
  Block Device clients. The S3 and Swift APIs share a common name space, so you
  may write data with one API and retrieve it with the other.
 </para>
 <para>
  This section helps you install and manage the Ceph RADOS Gateway (RADOS
  Gateway). You can either choose to use the
  <emphasis role="bold"><command>ceph-deploy</command></emphasis> tool, or do
  the installation and management manually.
 </para>
 <important>
  <para>
   Before installing RADOS Gateway, you need to have the Ceph cluster installed
   first (see <xref linkend="ses.deployment"/> for more information).
  </para>
 </important>
 <sect1 xml:id="rgw.installation">
  <title>&rgw; Installation</title>

  <para>
   There are several possible ways how you can install &rgw; &mdash;by using
   <command>ceph-deploy</command>, by using &salt; or manually by using
   <command>zypper</command>.
  </para>

  <sect2 xml:id="rgw.instal.ceph-deploy">
   <title>&rgw; Installation via <command>ceph-deploy</command></title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <command>rgw</command> component that helps you manage the &rgw; creation
    and operation.
   </para>
   <important>
    <title>Install &ceph;</title>
    <para>
     Before running <command>ceph-deploy rgw</command> as suggested in the
     following step, make sure that &ceph; together with the object gateway
     package are correctly installed on the node where you want to setup &rgw;:
    </para>
<screen>ceph-deploy install --rgw <replaceable>short_rgw_hostname</replaceable>
</screen>
   </important>
   <para>
    Prepare and activate the nodes in one step. You can specify several pairs
    of
    <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
    to install &rgw; on a required number of nodes.
   </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw create ceph-node1:rgw.gateway1</screen>
   <para>
    You now have a working &rgw; on the specified nodes, and you need to give
    access to a client. For more information, see
    <xref linkend="ceph.rgw.access"/>.
   </para>
  </sect2>

  <sect2 xml:id="rgw.install.manually">
   <title>&rgw; Manual Installation</title>
   <procedure>
    <step>
     <para>
      Install &rgw;. The following command installs all required components:
     </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
    </step>
    <step>
     <para>
      If the Apache server from the previous &rgw; instance is running, stop it
      and disable the relevant service:
     </para>
<screen>sudo systemctl stop disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.rgw.gateway]
 rgw frontends = "civetweb port=80"</screen>
     <tip>
      <para>
       If you want to configure &rgw;/CivetWeb for use with SSL encryption,
       modify the line accordingly:
      </para>
<screen>rgw frontends = civetweb port=7480s ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Restart the &rgw; service.
     </para>
<screen>systemctl restart ceph-radosgw@rgw.gateway_host</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ses.rgw.config">
   <title>&rgw; Configuration</title>
   <para>
    Several steps are required to configure a &rgw;.
   </para>
   <note>
    <title>Configuration after <command>ceph-deploy</command> Installation</title>
    <para>
     If you installed &rgw; by using <command>ceph-deploy</command>, you don't
     have to configure &rgw; as it is done automatically.
    </para>
   </note>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a &ceph; &rgw; requires a running &ceph; Storage Cluster. The
     &ceph; &rgw; is a client of the &ceph; Storage Cluster. As a &ceph;
     Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions and a keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the &ceph; Configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a &ceph;
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the &rgw; instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the &ceph; Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a &ceph; &rgw; user name and key for each instance. As an
       example, we will use the name <systemitem>gateway</systemitem> after
       <systemitem>client.radosgw</systemitem>:
      </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>sudo ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the &ceph; Object
       Gateway with access to the &ceph; Storage Cluster, add the key to your
       &ceph; Storage Cluster. For example:
      </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>sudo scp /etc/ceph/ceph.client.rgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
    </procedure>
    <tip>
     <title>Use Bootstrap Keyring</title>
     <para>
      An alternative way is to create the &rgw; bootstrap keyring, and then
      create the &rgw; keyring from it:
     </para>
     <procedure>
      <step>
       <para>
        Create a &rgw; bootstrap keyring on one of the monitor nodes:
       </para>
<screen>sudo ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<replaceable>node_host</replaceable>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</screen>
      </step>
      <step>
       <para>
        Create the
        <filename>/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></filename>
        directory for storing the bootstrap keyring:
       </para>
<screen>sudo mkdir \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></screen>
      </step>
      <step>
       <para>
        Create a &rgw; keyring from the newly created bootstrap keyring:
       </para>
<screen>sudo ceph \
 auth get-or-create client.rgw.<replaceable>rgw_name</replaceable> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
      <step>
       <para>
        Copy the &rgw; keyring to the &rgw; host:
       </para>
<screen>sudo scp \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring \
<replaceable>rgw_host</replaceable>:/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
     </procedure>
    </tip>
   </sect3>
   <sect3>
    <title>Create Pools (Optional)</title>
    <para>
     &ceph; &rgw;s require &ceph; Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the &ceph;
     configuration file.
    </para>
    <para>
     When configuring a gateway with the default region and zone, the naming
     convention for pools typically uses 'default' for region and zone naming,
     but you can use any naming convention you prefer:
    </para>
<screen>.rgw.root
default.rgw.control
default.rgw.data.root
default.rgw.gc
default.rgw.log
default.rgw.users.uid
default.rgw.users.email
default.rgw.users.keys
default.rgw.meta
default.rgw.users.swift</screen>
    <para>
     To create the pools manually, see
     <xref linkend="ceph.pools.operate.add_pool"/>.
    </para>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to &ceph;</title>
    <para>
     Add the &ceph; &rgw; configuration to the &ceph; Configuration file. The
     &ceph; &rgw; configuration requires you to identify the &ceph; &rgw;
     instance. Then, specify the host name where you installed the &ceph; &rgw;
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </para>
<screen>[client.rgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <tip>
     <title>&rgw; Log File</title>
     <para>
      To override the default &rgw; log file, include the follwing:
     </para>
<screen>log file = /var/log/radosgw/client.rgw.<replaceable>instance-name</replaceable>.log</screen>
    </tip>
    <para>
     The <literal>[client.rgw.*]</literal> portion of the gateway instance
     identifies this portion of the &ceph; configuration file as configuring a
     &ceph; Storage Cluster client where the client type is a &ceph; &rgw;
     (radosgw). The instance name follows. For example:
    </para>
<screen>[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
    <para>
     To use a &ceph; &rgw; with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the &ceph;
     &rgw; DNS name under the <literal>[client.rgw.gateway]</literal> section
     of the &ceph; configuration file:
    </para>
<screen>[client.rgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address as
     the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Redeploy &ceph; Configuration</title>
    <para>
     Use <command>ceph-deploy</command> to push a new copy of the configuration
     to the hosts in your cluster:
    </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default &ceph; &rgw; data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <literal>host</literal> variables in the &ceph;
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name
     and the daemon ID.
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the exemplary ceph.conf settings above, you would execute the
     following:
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your &ceph; Storage Cluster service. Then, start up
     the <systemitem>radosgw</systemitem> service. For more information, see
     <xref linkend="cha.ceph.operating"/> and
     <xref linkend="ceph.rgw.operating"/>.
    </para>
    <para>
     After the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
