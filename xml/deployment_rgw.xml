<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.additional.software.installation">
<!-- ============================================================== -->
 <title>&ceph; &rgw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; &rgw; is an object storage interface built on top of
  <literal>librgw</literal> to provide applications with a RESTful gateway to
  &ceph; clusters. It supports two interfaces:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the &ostack; Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The &rgw; daemon uses an embedded HTTP server (CivetWeb) for interacting with
  the &ceph; cluster. Since it provides interfaces compatible with &ostack;
  Swift and Amazon S3, the &rgw; has its own user management. &rgw; can store
  data in the same cluster that is used to store data from &cephfs; clients or
  &rbd; clients. The S3 and Swift APIs share a common name space, so you may
  write data with one API and retrieve it with the other.
 </para>
 <important>
  <title>&rgw; Deployed by &deepsea;</title>
  <para>
   Since &storage; 5, the &rgw; is installed as a &deepsea; role, therefore you
   do not need to install it manually.
  </para>
  <para>
   To install the &rgw; during the cluster deployment, see
   <xref linkend="ceph.install.stack"/>.
  </para>
  <para>
   To add a new node with &rgw; to the cluster, see
   <xref linkend="salt.adding.services"/>.
  </para>
 </important>
 <sect1 xml:id="rgw.installation">
  <title>&rgw; Manual Installation</title>

  <procedure>
   <step>
    <para>
     Install &rgw; on a node that is not using port 80. For example a node
     already running &oa; is already using port 80. The following command
     installs all required components:
    </para>
<screen>&prompt.cephuser;sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
   </step>
   <step>
    <para>
     If the Apache server from the previous &rgw; instance is running, stop it
     and disable the relevant service:
    </para>
<screen>&prompt.cephuser;sudo systemctl stop disable apache2.service</screen>
   </step>
   <step>
    <para>
     Edit <filename>/etc/ceph/ceph.conf</filename> and add the following lines:
    </para>
<screen>[client.rgw.gateway_host]
 rgw frontends = "civetweb port=80"</screen>
    <tip>
     <para>
      If you want to configure &rgw;/CivetWeb for use with SSL encryption,
      modify the line accordingly:
     </para>
<screen>rgw frontends = civetweb port=7480s ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
    </tip>
   </step>
   <step>
    <para>
     Restart the &rgw; service.
    </para>
<screen>&prompt.cephuser;sudo systemctl restart ceph-radosgw@rgw.gateway_host</screen>
   </step>
  </procedure>

  <sect2 xml:id="ses.rgw.config">
   <title>&rgw; Configuration</title>
   <para>
    Several steps are required to configure a &rgw;.
   </para>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a &ceph; &rgw; requires a running &ceph; Storage Cluster. The
     &ceph; &rgw; is a client of the &ceph; Storage Cluster. As a &ceph;
     Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions and a keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the &ceph; configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a &ceph;
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the &rgw; instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the &ceph; Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>&prompt.cephuser;sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
&prompt.cephuser;sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a &ceph; &rgw; user name and key for each instance. As an
       example, we will use the name <systemitem>gateway</systemitem> after
       <systemitem>client.radosgw</systemitem>:
      </para>
<screen>&prompt.cephuser;sudo ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>&prompt.cephuser;sudo ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the &ceph; Object
       Gateway with access to the &ceph; Storage Cluster, add the key to your
       &ceph; Storage Cluster. For example:
      </para>
<screen>&prompt.cephuser;sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>&prompt.cephuser;sudo scp /etc/ceph/ceph.client.rgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
&prompt.cephuser;ssh <replaceable>hostname</replaceable>
&prompt.cephuser;sudo mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
    </procedure>
    <tip>
     <title>Use Bootstrap Keyring</title>
     <para>
      An alternative way is to create the &rgw; bootstrap keyring, and then
      create the &rgw; keyring from it:
     </para>
     <procedure>
      <step>
       <para>
        Create a &rgw; bootstrap keyring on one of the monitor nodes:
       </para>
<screen>&prompt.cephuser;sudo ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<replaceable>node_host</replaceable>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</screen>
      </step>
      <step>
       <para>
        Create the
        <filename>/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></filename>
        directory for storing the bootstrap keyring:
       </para>
<screen>&prompt.cephuser;sudo mkdir \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></screen>
      </step>
      <step>
       <para>
        Create a &rgw; keyring from the newly created bootstrap keyring:
       </para>
<screen>&prompt.cephuser;sudo ceph \
 auth get-or-create client.rgw.<replaceable>rgw_name</replaceable> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
      <step>
       <para>
        Copy the &rgw; keyring to the &rgw; host:
       </para>
<screen>&prompt.cephuser;sudo scp \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring \
<replaceable>rgw_host</replaceable>:/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
       <remark role="fixme">Does the scp command fail because it is not executed as remote root but local root?</remark>
      </step>
     </procedure>
    </tip>
   </sect3>
   <sect3 xml:id="ogw.pool.create">
    <title>Create Pools (Optional)</title>
    <para>
     &ceph; &rgw;s require &ceph; Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the &ceph;
     configuration file.
    </para>
    <para>
     The pool names follow the
     <literal><replaceable>ZONE_NAME</replaceable>.<replaceable>POOL_NAME</replaceable></literal>
     syntax. When configuring a gateway with the default region and zone, the
     default zone name is 'default' as in our example:
    </para>
<screen>.rgw.root
default.rgw.control
default.rgw.meta
default.rgw.log
default.rgw.buckets.index
default.rgw.buckets.data</screen>
    <para>
     To create the pools manually, see
     <xref linkend="ceph.pools.operate.add_pool"/>.
    </para>
    <important>
     <title>&ogw; and Erasure-coded Pools</title>
     <para>
      Only the <literal>default.rgw.buckets.data</literal> pool can be
      erasure-coded. All other pools need to be replicated, otherwise the
      gateway is not accessible.
     </para>
    </important>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to &ceph;</title>
    <para>
     Add the &ceph; &rgw; configuration to the &ceph; Configuration file. The
     &ceph; &rgw; configuration requires you to identify the &ceph; &rgw;
     instance. Then, specify the host name where you installed the &ceph; &rgw;
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </para>
<screen>[client.rgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <tip>
     <title>&rgw; Log File</title>
     <para>
      To override the default &rgw; log file, include the following:
     </para>
<screen>log file = /var/log/radosgw/client.rgw.<replaceable>instance-name</replaceable>.log</screen>
    </tip>
    <para>
     The <literal>[client.rgw.*]</literal> portion of the gateway instance
     identifies this portion of the &ceph; configuration file as configuring a
     &ceph; Storage Cluster client where the client type is a &ceph; &rgw;
     (radosgw). The instance name follows. For example:
    </para>
<screen>[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
    <para>
     To use a &ceph; &rgw; with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the &ceph;
     &rgw; DNS name under the <literal>[client.rgw.gateway]</literal> section
     of the &ceph; configuration file:
    </para>
<screen>[client.rgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address as
     the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default &ceph; &rgw; data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <literal>host</literal> variables in the &ceph;
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name,
     and the daemon ID.
    </para>
<screen>&prompt.cephuser;sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the example <filename>ceph.conf</filename> settings above, you would
     execute the following:
    </para>
<screen>&prompt.cephuser;sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your &ceph; Storage Cluster service. Then, start up
     the <systemitem>radosgw</systemitem> service. For more information, see
     <xref linkend="cha.ceph.operating"/> and
     <xref linkend="ceph.rgw.operating"/>.
    </para>
    <para>
     When the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
