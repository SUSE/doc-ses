<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="monitoring-alerting">
 <title>Monitoring and alerting</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In &productname; &productnumber;, &cephadm; deploys a monitoring and alerting
  stack. Users have to either define the services (such as &prometheus;,
  &alertmanager;, and &grafana;) that they want to deploy with &cephadm; in a
  YAML configuration file, or they can use the CLI to deploy them. When
  multiple services of the same type are deployed, a highly-available setup is
  deployed. The node exporter is an exception to this rule.
 </para>
 <para>
  The following monitoring services can be deployed with &cephadm;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">&prometheus;</emphasis> is the monitoring and
    alerting toolkit. It collects the data provided by Prometheus exporters and
    fires pre-configured alerts if predefined thresholds have been reached.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">&alertmanager;</emphasis> handles alerts sent by the
    &prometheus; server. It deduplicates, groups, and routes the alerts to the
    correct receiver. By default, the &dashboard; will automatically be
    configured as the receiver.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">&grafana;</emphasis> is the visualization and
    alerting software. The alerting functionality of &grafana; is not utilized by
    this monitoring stack. For alerting, the Alertmanager is used.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Node exporter</emphasis> is a exporter for Prometheus
    which provides data about the node it is installed on. It is recommended to
    install the node exporter on all nodes.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The &prometheusmgr; provides a &prometheus; exporter to pass on &ceph;
  performance counters from the collection point in
  <literal>ceph-mgr</literal>.
 </para>
 <para>
  The &prometheus; configuration, including <emphasis>scrape</emphasis> targets
  (metrics providing daemons), is set up automatically by &cephadm;. &cephadm;
  also deploys a list of default alerts, for example <literal>health
  error</literal>, <literal>10% OSDs down</literal>, or <literal>pgs
  inactive</literal>.
 </para>
 <para>
  By default, traffic to &grafana; is encrypted with TLS. You can either supply
  your own TLS certificate or use a self-signed one. If no custom certificate
  has been configured before &grafana; has been deployed, then a self-signed
  certificate is automatically created and configured for &grafana;.
 </para>
 <para>
  Custom certificates for &grafana; can be configured using the following
  commands:
 </para>
<screen>
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
  </screen>
 <para>
  The &alertmanager; handles alerts sent by the &prometheus; server. It takes
  care of deduplicating, grouping, and routing them to the correct receiver.
  Alerts can be silenced using the &alertmanager;, but silences can also be
  managed using the &dashboard;.
 </para>
 <para>
  We recommend that the <systemitem class="daemon">Node exporter</systemitem>
  is deployed on all nodes. This can be done using the
  <filename>monitoring.yaml</filename> file with the
  <literal>node-exporter</literal> service type. See
  <xref linkend="deploy-cephadm-day2-service-monitoring" /> for more
  information on deploying services.
 </para>
 <sect1 xml:id="monitoring-custom-images">
  <title>Deploying using custom or local images</title>

  <tip>
   <para>
    The recommended method to deploy the monitoring stack is by applying its
    specification as described in
    <xref linkend="deploy-cephadm-day2-service-monitoring"/>.
   </para>
  </tip>

  <para>
   To deploy custom or local container images, the images need to be set in
   &cephadm;. To do so, you will need to run the following command:
  </para>

<screen>&prompt.cephuser;ceph config set mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable> <replaceable>VALUE</replaceable></screen>

  <para>
   Where <replaceable>OPTION_NAME</replaceable> is any of the following names:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     container_image_prometheus
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_node_exporter
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_alertmanager
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_grafana
    </para>
   </listitem>
  </itemizedlist>

  <para>
   By default, the following images are used as
   <replaceable>VALUE</replaceable>:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-server:2.18.0
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-node-exporter:0.18.1
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-alertmanager:0.16.2
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/ses/7/ceph/grafana:7.0.3
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For example:
  </para>

<screen>&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1</screen>

  <note>
   <para>
    By setting a custom image, the default value will be overridden (but not
    overwritten). The default value changes when updates become available. By
    setting a custom image, you will not be able to update the component you
    have set the custom image for automatically. You will need to manually
    update the configuration (image name and tag) to be able to install
    updates.
   </para>
   <para>
    If you choose to go with the recommendations instead, you can reset the
    custom image you have set before. After that, the default value will be
    used again. Use <command>ceph config rm</command> to reset the
    configuration option:
   </para>
<screen>&prompt.cephuser;ceph config rm mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph config rm mgr mgr/cephadm/container_image_prometheus</screen>
  </note>
 </sect1>
 <sect1 xml:id="monitoring-stack-disable">
  <title>Disabling monitoring</title>

  <para>
   To disable the monitoring stack, run the following commands:
  </para>

<screen>
&prompt.cephuser;ceph orch rm grafana
&prompt.cephuser;ceph orch rm prometheus --force   # this will delete metrics data collected so far
&prompt.cephuser;ceph orch rm node-exporter
&prompt.cephuser;ceph orch rm alertmanager
&prompt.cephuser;ceph mgr module disable prometheus
      </screen>
 </sect1>
 <sect1 xml:id="monitoring-grafana-config">
  <title>Configuring &grafana;</title>

  <para>
    The &dashboard; backend requires the &grafana; URL to be able to verify the
    existence of &grafana; Dashboards before the frontend even loads them. Due to
    the nature of how &grafana; is implemented in &dashboard;, this means that
    two working connections are required in order to be able to see &grafana;
    graphs in &dashboard;:
  </para>

  <itemizedlist>
    <listitem>
      <para>
        The backend (&ceph; MGR module) needs to verify the existence of the
        requested graph. If this request succeeds, it lets the frontend know
        that it can safely access &grafana;.
      </para>
    </listitem>
    <listitem>
      <para>
        The frontend then requests the &grafana; graphs directly from the user's
        browser using an <literal>iframe</literal>. The &grafana; instance is
        accessed directly without any detour through &dashboard;.
      </para>
    </listitem>
  </itemizedlist>

  <para>
    Now, it might be the case that your environment makes it difficult for the
    user's browser to directly access the URL configured in &dashboard;. To
    solve this issue, a separate URL can be configured which will solely be used
    to tell the frontend (the user's browser) which URL it should use to access
    &grafana;.
  </para>

  <para>
    To change the URL that is returned to the frontend issue the following command:
  </para>

<screen>&prompt.cephuser;ceph dashboard set-grafana-frontend-api-url <replaceable>GRAFANA-SERVER-URL</replaceable></screen>

  <para>
    If no value is set for that option, it will simply fall back to the value of
    the <replaceable>GRAFANA_API_URL</replaceable> option, which is set
    automatically and periodically updated by &cephadm;. If set, it will instruct the
    browser to use this URL to access &grafana;.
  </para>
 </sect1>
 <sect1 xml:id="monitoring-cephadm-config">
  <title>Configuring the &prometheusmgr;</title>

  <para>
   The &prometheusmgr; is a module inside &ceph; that extends &ceph;'s
   functionality. The module reads (meta-)data from &ceph; about its state and
   health, providing the (scraped) data in a consumable format to &prometheus;.
  </para>

  <note>
   <para>
    The &prometheusmgr; needs to be restarted for the configuration changes to
    be applied.
   </para>
  </note>

  <sect2 xml:id="monitoring-http-requests">
   <title>Configuring the network interface</title>
   <para>
    By default, the &prometheusmgr; accepts HTTP requests on port 9283 on all
    IPv4 and IPv6 addresses on the host. The port and listen address are both
    configurable with <option>ceph config-key set</option> , with keys
    <option>mgr/prometheus/server_addr</option> and
    <option>mgr/prometheus/server_port</option> . This port is registered
    with&prometheus;'s registry.
   </para>
   <para>
    To update the <literal>server_addr</literal> execute the following command:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_addr <replaceable>0.0.0.0</replaceable>
      </screen>
   <para>
    To update the <literal>server_port</literal> execute the following command:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_port <replaceable>9283</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-scrape-intervals">
   <title>Configuring <literal>scrape_interval</literal></title>
   <para>
    By default, the &prometheusmgr; is configured with a scrape interval of 15
    seconds. We do not recommend using a scrape interval below 10 seconds. To
    set a different scrape interval in the &prometheus; module, set
    <literal>scrape_interval</literal> to the desired value:
   </para>
   <important>
    <para>
     To work properly and not cause any issues, the
     <literal>scrape_interval</literal> of this module should always be set to
     match the &prometheus; scrape interval .
    </para>
   </important>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/scrape_interval <replaceable>15</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-stale-cache">
   <title>Configuring the cache</title>
   <para>
    On large clusters (more than 1000 OSDs), the time to fetch the metrics may
    become significant. Without the cache, the &prometheusmgr; can overload the
    manager and lead to unresponsive or crashing &ceph; &mgr; instances. As a
    result, the cache is enabled by default and cannot be disabled, but this
    does mean that the cache can become stale. The cache is considered stale
    when the time to fetch the metrics from &ceph; exceeds the configured
    <literal>scrape_interval</literal>.
   </para>
   <para>
    If this is the case, a warning will be logged and the module will either:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Respond with a 503 HTTP status code (service unavailable).
     </para>
    </listitem>
    <listitem>
     <para>
      Return the content of the cache, even though it might be stale.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This behavior can be configured using the <command>ceph config
    set</command> commands.
   </para>
   <para>
    To tell the module to respond with possibly-stale data, set it to
    <literal>return</literal>:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy return</screen>
   <para>
    To tell the module to respond with <literal>service unavailable</literal>,
    set it to <literal>fail</literal>:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy fail</screen>
  </sect2>

  <sect2 xml:id="monitoring-rbd-image">
   <title>Enabling RBD-image monitoring</title>
   <para>
    The &prometheusmgr; can optionally collect RBD per-image IO statistics by
    enabling dynamic OSD performance counters. The statistics are gathered for
    all images in the pools that are specified in the
    <literal>mgr/prometheus/rbd_stats_pools</literal> configuration parameter.
   </para>
   <para>
    The parameter is a comma- or space-separated list of
    <literal>pool[/namespace]</literal> entries. If the namespace is not
    specified, the statistics are collected for all namespaces in the pool.
   </para>
   <para>
    For example:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools "<replaceable>pool1,pool2,poolN</replaceable>"
      </screen>
   <para>
    The module scans the specified pools and namespaces and makes a list of all
    available images, and refreshes it periodically. configurable via the
    <literal>mgr/prometheus/rbd_stats_pools_refresh_interval</literal>
    parameter (in seconds), and is 300 seconds (five minutes) by default.
   </para>
   <para>
    For example, if you changed the sync interval to 10 minutes:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <replaceable>600</replaceable>
      </screen>
  </sect2>
 </sect1>
 <sect1 xml:id="prometheus-security-model">
  <title>&prometheus; security model</title>

  <para>
   &prometheus;' security model presumes that untrusted users have access to
   the &prometheus; HTTP endpoint and logs. Untrusted users have access to all
   the (meta-)data &prometheus; collects that is contained in the database,
   plus a variety of operational and debugging information.
  </para>

  <para>
   However, &prometheus;' HTTP API is limited to read-only operations.
   Configurations can not be changed using the API, and secrets are not
   exposed. Moreover, &prometheus; has some built-in measures to mitigate the
   impact of denial-of-service attacks.
  </para>
 </sect1>
 <sect1 xml:id="prometheus-webhook-snmp">
  <title>&prometheus; &alertmanager; SNMP webhook</title>

  <para>
   If you want to get notified about &prometheus; alerts via SNMP traps, then
   you can install the &prometheus; &alertmanager; SNMP webhook via &cephadm;.
   To do so, you need to create a service and placement specification file with
   the following content:
  </para>

  <note>
   <para>
    For more information on service and placement files, see
    <xref linkend="cephadm-service-and-placement-specs"/>.
   </para>
  </note>

<screen>
service_type: container
service_id: prometheus-webhook-snmp
placement:
    <replaceable>ADD_PLACEMENT_HERE</replaceable>
image: registry.suse.com/ses/7/prometheus-webhook-snmp:latest
args:
    - "--publish 9099:9099"
envs:
    - ARGS="--debug --snmp-host=<replaceable>ADD_HOST_GATEWAY_HERE</replaceable>"
    - RUN_ARGS="--metrics"
EOF
</screen>

  <para>
   Use this service specification to get the service running using its default
   settings.
  </para>

  <para>
   You need to publish the port the &prometheus; receiver is listening on by
   using the command line argument <literal>--publish
   <replaceable>HOST_PORT</replaceable>:<replaceable>CONTAINER_PORT</replaceable></literal>
   when running the service, because the port is not exposed automatically by
   the container. This can be done by adding the following lines to the
   specification:
  </para>

<screen>
args:
    - "--publish 9099:9099"
</screen>

  <para>
   Alternatively, connect the container to the host network by using the
   command line argument <literal>--network=host</literal>.
  </para>

<screen>
args:
    - "--network=host"
</screen>

  <para>
   If the SNMP trap receiver is not installed on the same host as the
   container, then you must also specify the FQDN of the SNMP host. Use the
   container's network gateway to be able to receive SNMP traps outside the
   container/host:
  </para>

<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
</screen>

  <sect2 xml:id="configure-prometheus-webhook-snmp">
   <title>Configuring the <literal>prometheus-webhook-snmp</literal> service</title>
   <para>
    The container can be configured by environment variables or by using a
    configuration file.
   </para>
   <para>
    For the environment variables, use <literal>ARGS</literal> to set global
    options and <literal>RUN_ARGS</literal> for the <command>run</command>
    command options. You need to adapt the service specification the following
    way:
   </para>
<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
    - RUN_ARGS="--metrics --port=9101"
</screen>
   <para>
    To use a configuration file, the service specification must be adapted the
    following way:
   </para>
<screen>
files:
    etc/prometheus-webhook-snmp.conf:
        - "debug: True"
        - "snmp_host: <replaceable>ADD_HOST_GATEWAY_HERE</replaceable>"
        - "metrics: True"
volume_mounts:
    etc/prometheus-webhook-snmp.conf: /etc/prometheus-webhook-snmp.conf
</screen>
   <para>
    To deploy, run the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SERVICE_SPEC_FILE</replaceable></screen>
   <para>
    See <xref linkend="deploy-cephadm-day2-services"/> for more information.
   </para>
  </sect2>

  <sect2 xml:id="configure-prometheus-alertmanager-for-snmp">
   <title>Configuring the &prometheus; &alertmanager; for SNMP</title>
   <para>
    Finally, the &prometheus; &alertmanager; needs to be configured
    specifically for SNMP traps. If this service has not been deployed already,
    create a service specification file. You need to replace
    <literal>IP_OR_FQDN</literal> with the IP address or FQDN of the host where
    the &prometheus; &alertmanager; SNMP webhook has been installed. For
    example:
   </para>
   <note>
    <para>
     If you have already deployed this service, then to ensure the
     &alertmanager; is set up correctly for SNMP, re-deploy with the following
     settings.
    </para>
   </note>
<screen>
  service_type: alertmanager
  placement:
    hosts:
    - <replaceable>HOSTNAME</replaceable>
  webhook_configs:
    - 'http://<replaceable>IP_OR_FQDN</replaceable>:9099/'
</screen>
   <para>
    Apply the service specification with the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SERVICE_SPEC_FILE</replaceable></screen>
  </sect2>
 </sect1>
</chapter>
