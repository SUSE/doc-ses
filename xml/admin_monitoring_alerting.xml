<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="monitoring-alerting">
  <title>Monitoring and Alerting</title>
  <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
  <para>
  In &productname; &productnumber;, &cephadm; deploys a monitoring and alerting
  stack. Users have to either define the services (such as &prometheus;,
  &alertmanager;, and &grafana;) that they want to deploy with &cephadm; in a
  YAML configuration file, or they can use the CLI to deploy them. When multiple
  services of the same type are deployed, a highly-available setup is deployed.
  The node exporter is an exception to this rule.
  </para>
  <para>
  The following monitoring services can be deployed with &cephadm;:
  </para>
  <itemizedlist>
    <listitem>
      <para>
        <emphasis role="bold">&prometheus;</emphasis> is the monitoring and
        alerting toolkit. It collects the data provided by Prometheus exporters
        and fires pre-configured alerts if predefined thresholds have been
        reached.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">&alertmanager;</emphasis> handles alerts sent by
        the &prometheus; server. It deduplicates, groups, and routes the alerts to the
        correct receiver. By default, the &dashboard; will automatically be
        configured as the receiver.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">&grafana;</emphasis> is the visualization and
        alerting software. The alerting functionality of Grafana is not utilized
        by this monitoring stack. For alerting, the Alertmanager is used.
      </para>
    </listitem>
    <listitem>
      <para>
        <emphasis role="bold">Node exporter</emphasis> is a exporter for
        Prometheus which provides data about the node it is installed on. It is
        recommended to install the node exporter on all nodes.
      </para>
    </listitem>
  </itemizedlist>
  <para>
    The &prometheusmgr; provides a &prometheus; exporter to pass on &ceph;
    performance counters from the collection point in
    <literal>ceph-mgr</literal>.
  </para>
  <para>
    The &prometheus; configuration, including <emphasis>scrape</emphasis>
    targets (metrics providing daemons), is set up automatically by &cephadm;.
    &cephadm; also deploys a list of default alerts, for example <literal>health
    error</literal>, <literal>10% OSDs down</literal>, or <literal>pgs
    inactive</literal>.
  </para>
  <para>
    By default, traffic to &grafana; is encrypted with TLS. You can either
    supply your own TLS certificate or use a self-signed one. A self-signed
    certificate is automatically created and configured for Grafana if no custom
    certificate has been configured before Grafana has been deployed.
  </para>

  <!-- TODO find a nicer place for this? -->
  <para>Custom certificates for &grafana; can be configured using the following commands:</para>
  <screen>
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
  </screen>

  <para>
    The &alertmanager; handles alerts sent by the &prometheus; server. It takes
    care of deduplicating, grouping, and routing them to the correct receiver.
    Alerts can be silenced using the &alertmanager;, but silences can also be
    managed using the Ceph Dashboard.
  </para>
  <para>
    We recommend that the <systemitem class="daemon">Node exporter</systemitem>
    is deployed on all nodes. This can be done using the
    <filename>monitoring.yaml</filename> file with the
    <literal>node-exporter</literal> service type. See <xref
    linkend="monitoring-stack-deploy" /> for more information on deploying
    services.
  </para>

  <sect1 xml:id="monitoring-stack-deploy">
    <title>Deploying Monitoring With &cephadm;</title>
    <para>
      To deploy the monitoring stack you need to either use a YAML configuration
      file or the CLI. Whichever method you will use, you need to ensure the
      &prometheusmgr; is enabled.
    </para>
    <para>
      The &prometheusmgr; needs to be enabled using the following command:
    </para>
    <screen>&prompt.cephuser;ceph mgr module enable prometheus</screen>
    <note>
      <para>
        Ensure this command is run before &prometheus; is deployed. If the
        command was not run before the
        deployment, you must redeploy &prometheus; to update Prometheus'
        configuration:
      </para>
      <screen>&prompt.cephuser;ceph orch redeploy prometheus</screen>
    </note>
    <important>
      <para>
        &cephadm; handles the configuration of &prometheus;, &grafana;, and the
        &alertmanager; automatically.
      </para>
      <para>
        The &prometheusmgr; can be configured, though. See <xref
        linkend="monitoring-cephadm-config" /> for more options.
      </para>
    </important>
    <sect2 xml:id="monitoring-service-spec">
      <title>Deploying Using a Service and Placement Specification</title>
      <para>
        &cephadm; also uses a <filename>.yaml</filename> configuration file to
        deploy the relevant services, see <xref
        linkend="cephadm-service-and-placement-specs" /> for more information on
        creating service and placement specifications.
      </para>
      <para>
        To apply a configuration file, and thereby deploy the configured
        services, create a file called <filename>monitoring.yaml</filename> with
        the following content:
      </para>
      <screen>
service_type: prometheus
placement:
  hosts:
    - host1
---
service_type: grafana
placement:
  hosts:
    - host2
---
service_type: alertmanager
placement:
  hosts:
    - host1
---
service_type: node-exporter
placement:
  host_pattern: "*"
      </screen>
      <para>
        To apply the configuration file and deploy the services, run the
        following command:
      </para>
      <screen>&prompt.cephuser;ceph orch apply -i monitoring.yaml</screen>
    </sect2>
    <sect2 xml:id="monitoring-cli-spec">
      <title>Deploying Using the Command Line Interface</title>
      <para>Services of a single type can also be deployed using the Command Line Interface:</para>
      <procedure>
        <step>
          <para>
            Deploy a <emphasis role="bold">Node exporter</emphasis> service on
            every node of the cluster:
          </para>
          <screen>&prompt.cephuser;ceph orch apply node-exporter '*'</screen>
        </step>
        <step>
          <para>
            Deploy a single &alertmanager; instance on any host:
          </para>
          <screen>&prompt.cephuser;ceph orch apply alertmanager</screen>
        </step>
        <step>
          <para>
            Deploy a single &prometheus; instance on any host:
          </para>
          <screen>&prompt.cephuser;ceph orch apply prometheus 1    # or 2 for redundancy</screen>
        </step>
        <step>
          <para>
            Deploy &grafana;:
          </para>
          <screen>&prompt.cephuser;ceph orch apply grafana</screen>
        </step>
      </procedure>
    </sect2>
    <sect2 xml:id="monitoring-stack-disable">
      <title>Disabling Monitoring</title>
      <para>
        To disable the monitoring stack, run the following commands:
      </para>
      <screen>
&prompt.cephuser;ceph orch rm grafana
&prompt.cephuser;ceph orch rm prometheus --force   # this will delete metrics data collected so far
&prompt.cephuser;ceph orch rm node-exporter
&prompt.cephuser;ceph orch rm alertmanager
&prompt.cephuser;ceph mgr module disable prometheus
      </screen>
    </sect2>
  </sect1>

  <sect1 xml:id="monitoring-cephadm-config">
    <title>Configuring the &prometheusmgr;</title>
    <para>
      The &prometheusmgr; is a module inside &ceph; that extends &ceph;'s
      functionality. The module reads (meta-)data from &ceph; about its state
      and health, providing the (scraped) data in a consumable format to
      &prometheus;.
    </para>
    <note>
      <para>
        The &prometheusmgr; needs to be restarted for the
        configuration changes to be applied.
      </para>
    </note>
    <sect2 xml:id="monitoring-http-requests">
      <title>Configuring the Network Interface</title>
      <para>
        By default, the &prometheusmgr; accepts HTTP requests on port 9283 on
        all IPv4 and IPv6 addresses on the host. The port and listen address are
        both configurable with <option>ceph config-key set</option> , with keys
        <option>mgr/prometheus/server_addr</option> and
        <option>mgr/prometheus/server_port</option> . This port is registered
        with&prometheus;'s registry.
      </para>
      <para>
        To update the <literal>server_addr</literal> execute the following
        command:
      </para>
      <screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_addr <replaceable>0.0.0.0</replaceable>
      </screen>
      <para>
        To update the <literal>server_port</literal> execute the following
        command:
      </para>
      <screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_port <replaceable>9283</replaceable>
      </screen>
    </sect2>
    <sect2 xml:id="monitoring-scrape-intervals">
      <title>
        Configuring <literal>scrape_interval</literal>
      </title>
      <para>
        By default, the &prometheusmgr; is configured with a scrape interval of
        15 seconds. We do not recommend using a scrape interval below 10
        seconds. To set a different scrape interval in the &prometheus; module,
        set <literal>scrape_interval</literal> to the desired value:
      </para>
      <important>
        <para>
          To work properly and not cause any issues, the <literal>scrape_interval</literal>
          of this module should always be set to match the &prometheus; scrape interval .
        </para>
      </important>
      <screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/scrape_interval <replaceable>15</replaceable>
      </screen>
    </sect2>
    <sect2 xml:id="monitoring-stale-cache">
      <title>Configuring The Cache</title>
      <para>
        On large clusters (more than 1000 OSDs), the time to fetch the metrics may
        become significant. Without the cache, the &prometheusmgr; can overload
        the manager and lead to unresponsive or crashing &ceph; &mgr;
        instances. As a result, the cache is enabled by default and cannot be
        disabled, but this does mean that the cache can become stale.  The cache
        is considered stale when the time to fetch the metrics from &ceph;
        exceeds the configured <literal>scrape_interval</literal>.
      </para>
      <para>
        If this is the case, a warning will be logged and the module will
        either:
      </para>
      <itemizedlist>
        <listitem>
          <para>
             Respond with a 503 HTTP status code (service unavailable).
          </para>
        </listitem>
        <listitem>
          <para>
             Return the content of the cache, even though it might be stale.
          </para>
        </listitem>
      </itemizedlist>
      <para>
        This behavior can be configured using the <command>ceph config
        set</command> commands.
      </para>
      <para>
        To tell the module to respond with possibly-stale data, set it to
        <literal>return</literal>:
      </para>
      <screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy return</screen>
      <para>
        To tell the module to respond with <literal>service unavailable</literal>, set it to
        <literal>fail</literal>:
      </para>
      <screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy fail</screen>
    </sect2>
    <sect2 xml:id="monitoring-rbd-image">
      <title>Enabling RBD-image Monitoring</title>
      <para>
        The &prometheusmgr; can optionally collect RBD per-image IO statistics
        by enabling dynamic OSD performance counters. The statistics are
        gathered for all images in the pools that are specified in the
        <literal>mgr/prometheus/rbd_stats_pools</literal> configuration
        parameter.
      </para>
      <para>
        The parameter is a comma- or space-separated list of
        <literal>pool[/namespace]</literal> entries. If the namespace is not
        specified, the statistics are collected for all namespaces in the pool.
      </para>
      <para>
        For example:
      </para>
      <screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools "<replaceable>pool1,pool2,poolN</replaceable>"
      </screen>
      <para>
        The module scans the specified pools and namespaces and makes a
        list of all available images, and refreshes it periodically.
        configurable via the
        <literal>mgr/prometheus/rbd_stats_pools_refresh_interval</literal>
        parameter (in seconds), and is 300 seconds (five minutes) by default.
      </para>
      <para>
        For example, if you changed the sync interval to 10 minutes:
      </para>
      <screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <replaceable>600</replaceable>
      </screen>
    </sect2>
  </sect1>

  <sect1 xml:id="prometheus-security-model">
    <title>&prometheus; Security Model</title>
    <para>
      &prometheus;' security model presumes that untrusted users have access to
      the &prometheus; HTTP endpoint and logs. Untrusted users have access to
      all the (meta-)data &prometheus; collects that is contained in the
      database, plus a variety of operational and debugging information.
    </para>
    <para>
      However, &prometheus;' HTTP API is limited to read-only operations.
      Configurations can not be changed using the API, and secrets are not
      exposed. Moreover, &prometheus; has some built-in measures to mitigate the
      impact of denial-of-service attacks.
    </para>
  </sect1>

  <sect1>
   <title>SNMP Trap Receiver</title>
   <para>
    If you want to get notified about &prometheus; alerts via SNMP traps, then
    you can install the &prometheus; &alertmanager; SNMP trap receiver via
    &cephadm;. To do so, you need to create a service and placement
    specification file with the following content:
   </para>
   <note>
     <para>
       For more information on service and placement files, see
       <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
   </note>
<screen>
service_type: container
service_id: prometheus-webhook-snmp
placement:
    <replaceable>ADD_PLACEMENT_HERE</replaceable>
image: registry.opensuse.org/opensuse/prometheus-webhook-snmp:latest
args:
    - "--publish 9099:9099"
envs:
    - RUN_ARGS="--metrics"
EOF
</screen>
     <para>
      This service specification is used to get the service running using
      its default settings.
     </para>
     <para>
      You need to publish the port the &prometheus; receiver is listening on by
      using the command line argument <literal>--publish <replaceable>HOST_PORT</replaceable>:<replaceable>CONTAINER_PORT</replaceable></literal>
      when running the service because the port is not exposed automatically by the
      container. This can be done by adding the following lines specification:
     </para>
<screen>
args:
    - "--publish 9099:9099"
</screen>
     <para>
       Alternatively, connect the container to the host network by using
       the command line argument <literal>--network=host</literal>.
     </para>
<screen>
args:
    - "--network=host"
</screen>
     <para>
      Additionally, the SNMP host needs to be configured when the
      container is connection to the host network via <literal>--network=host</literal>.
      Use the container's network gateway to be able to receive SNMP traps
      outside the container:
     </para>
<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
</screen>
     <sect2 xml:id="configure-prometheus-">
       <title>Configure the prometheus-webhook-snmp service</title>
       <para>
         The container can be configured by environment variables or by
         using a configuration file.
       </para>
       <para>
         Use <literal>ARGS</literal> to set global options and <literal>RUN_ARGS</literal>
         for the run command options. You need to adapt the service specification
         the following way:
       </para>
<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
    - RUN_ARGS="--metrics --port=9101"
</screen>
       <para>
         The service specification must be adapted the following way:
       </para>
<screen>
files:
    etc/prometheus-webhook-snmp.conf:
        - debug: True
        - snmp_retries: 1
        - snmp_community: private
        - metrics: True
        - port: 9101
volume_mounts:
    etc/prometheus-webhook-snmp.conf: /etc/prometheus-webhook-snmp.conf
</screen>
       <para>
         To deploy, run the following command:
       </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SERVICE_SPEC_FILE</replaceable></screen>
       <para>
         See <xref linkend="deploy-cephadm-day2-services"/> for more
         information.
       </para>
     </sect2>
  </sect1>
</chapter>
