<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter [
  <!ENTITY % entities SYSTEM "generic-entities.ent">
  %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="monitoring-alerting">
 <title>Monitoring and alerting</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In &productname; &productnumber;, &cephadm; deploys a monitoring and alerting
  stack. Users need to either define the services (such as &prometheus;,
  &alertmanager;, and &grafana;) that they want to deploy with &cephadm; in a
  YAML configuration file, or they can use the CLI to deploy them. When
  multiple services of the same type are deployed, a highly-available setup is
  deployed. The node exporter is an exception to this rule.
 </para>
 <para>
  The following monitoring services can be deployed with &cephadm;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">&prometheus;</emphasis> is the monitoring and
    alerting toolkit. It collects the data provided by Prometheus exporters and
    fires preconfigured alerts if predefined thresholds have been reached.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">&alertmanager;</emphasis> handles alerts sent by the
    &prometheus; server. It deduplicates, groups, and routes the alerts to the
    correct receiver. By default, the &dashboard; will automatically be
    configured as the receiver.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">&grafana;</emphasis> is the visualization and
    alerting software. The alerting functionality of &grafana; is not used by
    this monitoring stack. For alerting, the Alertmanager is used.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Node exporter</emphasis> is an exporter for
    Prometheus which provides data about the node it is installed on. It is
    recommended to install the node exporter on all nodes.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  The &prometheusmgr; provides a &prometheus; exporter to pass on &ceph;
  performance counters from the collection point in
  <literal>ceph-mgr</literal>.
 </para>
 <para>
  The &prometheus; configuration, including <emphasis>scrape</emphasis> targets
  (metrics providing daemons), is set up automatically by &cephadm;. &cephadm;
  also deploys a list of default alerts, for example <literal>health
  error</literal>, <literal>10% OSDs down</literal>, or <literal>pgs
  inactive</literal>.
 </para>
 <para>
  By default, traffic to &grafana; is encrypted with TLS. You can either supply
  your own TLS certificate or use a self-signed one. If no custom certificate
  has been configured before &grafana; has been deployed, then a self-signed
  certificate is automatically created and configured for &grafana;.
 </para>
 <para>
  Custom certificates for &grafana; can be configured using the following
  commands:
 </para>
<screen>
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
&prompt.cephuser; ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
</screen>
 <para>
  If you already deployed &grafana;, you need to re-configure the service to
  reflect the new certificate paths and set the right URL for the &dashboard;:
 </para>
<screen>&prompt.cephuser;ceph orch reconfig grafana</screen>
 <para>
  The &alertmanager; handles alerts sent by the &prometheus; server. It takes
  care of deduplicating, grouping, and routing them to the correct receiver.
  Alerts can be silenced using the &alertmanager;, but silences can also be
  managed using the &dashboard;.
 </para>
 <para>
  We recommend that the <systemitem class="daemon">Node exporter</systemitem>
  is deployed on all nodes. This can be done using the
  <filename>monitoring.yaml</filename> file with the
  <literal>node-exporter</literal> service type. See
  <xref linkend="deploy-cephadm-day2-service-monitoring" /> for more
  information on deploying services.
 </para>
 <sect1 xml:id="monitoring-custom-images">
  <title>Configuring custom or local images</title>

  <tip>
   <para>
    This section describes how to change the configuration of container images
    which are used when services are deployed or updated. It does not include
    the commands necessary to deploy or re-deploy services.
   </para>
   <para>
    The recommended method to deploy the monitoring stack is by applying its
    specification as described in
    <xref linkend="deploy-cephadm-day2-service-monitoring"/>.
   </para>
  </tip>

  <para>
   To deploy custom or local container images, the images need to be set in
   &cephadm;. To do so, you will need to run the following command:
  </para>

<screen>&prompt.cephuser;ceph config set mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable> <replaceable>VALUE</replaceable></screen>

  <para>
   Where <replaceable>OPTION_NAME</replaceable> is any of the following names:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     container_image_prometheus
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_node_exporter
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_alertmanager
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_grafana
    </para>
   </listitem>
  </itemizedlist>

  <para>
   If no option is set or if the setting has been removed, the following images
   are used as <replaceable>VALUE</replaceable>:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-server:2.18.0
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-node-exporter:1.0.1
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/caasp/v4.5/prometheus-alertmanager:0.16.2
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/ses/7/ceph/grafana:7.3.1
    </para>
   </listitem>
  </itemizedlist>

  <para>
   For example:
  </para>

<screen>&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1</screen>

  <note>
   <para>
    By setting a custom image, the default value will be overridden (but not
    overwritten). The default value changes when updates become available. By
    setting a custom image, you will not be able to update the component you
    have set the custom image for automatically. You will need to manually
    update the configuration (image name and tag) to be able to install
    updates.
   </para>
   <para>
    If you choose to go with the recommendations instead, you can reset the
    custom image you have set before. After that, the default value will be
    used again. Use <command>ceph config rm</command> to reset the
    configuration option:
   </para>
<screen>&prompt.cephuser;ceph config rm mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph config rm mgr mgr/cephadm/container_image_prometheus</screen>
  </note>
 </sect1>
 <sect1 xml:id="monitoring-applying-updates">
  <title>Updating monitoring services</title>

  <para>
   As mentioned in <xref linkend="monitoring-custom-images" />, &cephadm; is
   shipped with the URLs of the recommended and tested container images, and
   they are used by default.
  </para>

  <para>
   By updating the &ceph; packages, new versions of these URLs may be shipped.
   This just updates where the container images are pulled from but does not
   update any services.
  </para>

  <para>
   After the URLs to the new container images have been updated, either
   manually as described in <xref linkend="monitoring-custom-images" />, or
   automatically through an update of the &ceph; package, the monitoring
   services can be updated.
  </para>

  <para>
   To do so, use <command>ceph orch reconfig</command> like so:
  </para>

<screen>
&prompt.cephuser;ceph orch reconfig node-exporter
&prompt.cephuser;ceph orch reconfig prometheus
&prompt.cephuser;ceph orch reconfig alertmanager
&prompt.cephuser;ceph orch reconfig grafana
</screen>

  <para>
   Currently no single command to update all monitoring services exists. The
   order in which these services are updated is not important.
  </para>

  <note>
   <para>
    If you use custom container images, the URLs specified for the monitoring
    services will not change automatically if the &ceph; packages are updated.
    If you have specified custom container images, you will need to specify the
    URLs of the new container images manually. This may be the case if you use
    a local container registry.
   </para>
   <para>
    You can find the URLs of the recommended container images to be used in the
    <xref linkend="monitoring-custom-images" /> section.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitoring-stack-disable">
  <title>Disabling monitoring</title>

  <para>
   To disable the monitoring stack, run the following commands:
  </para>

<screen>
&prompt.cephuser;ceph orch rm grafana
&prompt.cephuser;ceph orch rm prometheus --force   # this will delete metrics data collected so far
&prompt.cephuser;ceph orch rm node-exporter
&prompt.cephuser;ceph orch rm alertmanager
&prompt.cephuser;ceph mgr module disable prometheus
      </screen>
 </sect1>
 <sect1 xml:id="monitoring-grafana-config">
  <title>Configuring &grafana;</title>

  <para>
   The &dashboard; back-end requires the &grafana; URL to be able to verify the
   existence of &grafana; Dashboards before the front-end even loads them.
   Because of the nature of how &grafana; is implemented in &dashboard;, this
   means that two working connections are required in order to be able to see
   &grafana; graphs in &dashboard;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The back-end (&ceph; MGR module) needs to verify the existence of the
     requested graph. If this request succeeds, it lets the front-end know that
     it can safely access &grafana;.
    </para>
   </listitem>
   <listitem>
    <para>
     The front-end then requests the &grafana; graphs directly from the user's
     browser using an <literal>iframe</literal>. The &grafana; instance is
     accessed directly without any detour through &dashboard;.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Now, it might be the case that your environment makes it difficult for the
   user's browser to directly access the URL configured in &dashboard;. To
   solve this issue, a separate URL can be configured which will solely be used
   to tell the front-end (the user's browser) which URL it should use to access
   &grafana;.
  </para>

  <para>
   To change the URL that is returned to the front-end issue the following
   command:
  </para>

<screen>&prompt.cephuser;ceph dashboard set-grafana-frontend-api-url <replaceable>GRAFANA-SERVER-URL</replaceable></screen>

  <para>
   If no value is set for that option, it will simply fall back to the value of
   the <replaceable>GRAFANA_API_URL</replaceable> option, which is set
   automatically and periodically updated by &cephadm;. If set, it will
   instruct the browser to use this URL to access &grafana;.
  </para>
 </sect1>
 <sect1 xml:id="monitoring-cephadm-config">
  <title>Configuring the &prometheusmgr;</title>

  <para>
   The &prometheusmgr; is a module inside &ceph; that extends &ceph;'s
   functionality. The module reads (meta-)data from &ceph; about its state and
   health, providing the (scraped) data in a consumable format to &prometheus;.
  </para>

  <note>
   <para>
    The &prometheusmgr; needs to be restarted for the configuration changes to
    be applied.
   </para>
  </note>

  <sect2 xml:id="monitoring-http-requests">
   <title>Configuring the network interface</title>
   <para>
    By default, the &prometheusmgr; accepts HTTP requests on port 9283 on all
    IPv4 and IPv6 addresses on the host. The port and listen address are both
    configurable with <option>ceph config-key set</option> , with keys
    <option>mgr/prometheus/server_addr</option> and
    <option>mgr/prometheus/server_port</option> . This port is registered
    with&prometheus;'s registry.
   </para>
   <para>
    To update the <literal>server_addr</literal> execute the following command:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_addr <replaceable>0.0.0.0</replaceable>
      </screen>
   <para>
    To update the <literal>server_port</literal> execute the following command:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/server_port <replaceable>9283</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-scrape-intervals">
   <title>Configuring <literal>scrape_interval</literal></title>
   <para>
    By default, the &prometheusmgr; is configured with a scrape interval of 15
    seconds. We do not recommend using a scrape interval below 10 seconds. To
    set a different scrape interval in the &prometheus; module, set
    <literal>scrape_interval</literal> to the desired value:
   </para>
   <important>
    <para>
     To work properly and not cause any issues, the
     <literal>scrape_interval</literal> of this module should always be set to
     match the &prometheus; scrape interval .
    </para>
   </important>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/scrape_interval <replaceable>15</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-stale-cache">
   <title>Configuring the cache</title>
   <para>
    On large clusters (more than 1000 OSDs), the time to fetch the metrics may
    become significant. Without the cache, the &prometheusmgr; can overload the
    manager and lead to unresponsive or crashing &mgr; instances. As a result,
    the cache is enabled by default and cannot be disabled, but this does mean
    that the cache can become stale. The cache is considered stale when the
    time to fetch the metrics from &ceph; exceeds the configured
    <literal>scrape_interval</literal>.
   </para>
   <para>
    If this is the case, a warning will be logged and the module will either:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Respond with a 503 HTTP status code (service unavailable).
     </para>
    </listitem>
    <listitem>
     <para>
      Return the content of the cache, even though it might be stale.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This behavior can be configured using the <command>ceph config
    set</command> commands.
   </para>
   <para>
    To tell the module to respond with possibly-stale data, set it to
    <literal>return</literal>:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy return</screen>
   <para>
    To tell the module to respond with <literal>service unavailable</literal>,
    set it to <literal>fail</literal>:
   </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy fail</screen>
  </sect2>

  <sect2 xml:id="monitoring-rbd-image">
   <title>Enabling RBD-image monitoring</title>
   <para>
    The &prometheusmgr; can optionally collect RBD per-image IO statistics by
    enabling dynamic OSD performance counters. The statistics are gathered for
    all images in the pools that are specified in the
    <literal>mgr/prometheus/rbd_stats_pools</literal> configuration parameter.
   </para>
   <para>
    The parameter is a comma- or space-separated list of
    <literal>pool[/namespace]</literal> entries. If the namespace is not
    specified, the statistics are collected for all namespaces in the pool.
   </para>
   <para>
    For example:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools "<replaceable>pool1,pool2,poolN</replaceable>"
      </screen>
   <para>
    The module scans the specified pools and namespaces and makes a list of all
    available images, and refreshes it periodically. The interval is
    configurable via the
    <literal>mgr/prometheus/rbd_stats_pools_refresh_interval</literal>
    parameter (in seconds), and is 300 seconds (five minutes) by default.
   </para>
   <para>
    For example, if you changed the synchronization interval to 10 minutes:
   </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <replaceable>600</replaceable>
      </screen>
  </sect2>
 </sect1>
 <sect1 xml:id="prometheus-security-model">
  <title>&prometheus; security model</title>

  <para>
   &prometheus;' security model presumes that untrusted users have access to
   the &prometheus; HTTP endpoint and logs. Untrusted users have access to all
   the (meta-)data &prometheus; collects that is contained in the database,
   plus a variety of operational and debugging information.
  </para>

  <para>
   However, &prometheus;' HTTP API is limited to read-only operations.
   Configurations cannot be changed using the API, and secrets are not exposed.
   Moreover, &prometheus; has some built-in measures to mitigate the impact of
   denial-of-service attacks.
  </para>
 </sect1>
 <sect1 xml:id="prometheus-webhook-snmp">
  <title>&prometheus; &alertmanager; SNMP webhook</title>

  <para>
   If you want to get notified about &prometheus; alerts via SNMP traps, then
   you can install the &prometheus; &alertmanager; SNMP webhook via &cephadm;.
   To do so, you need to create a service and placement specification file with
   the following content:
  </para>

  <note>
   <para>
    For more information on service and placement files, see
    <xref linkend="cephadm-service-and-placement-specs"/>.
   </para>
  </note>

<screen>
service_type: container
service_id: prometheus-webhook-snmp
placement:
    <replaceable>ADD_PLACEMENT_HERE</replaceable>
image: registry.suse.com/ses/7/prometheus-webhook-snmp:latest
args:
    - "--publish 9099:9099"
envs:
    - ARGS="--debug --snmp-host=<replaceable>ADD_HOST_GATEWAY_HERE</replaceable>"
    - RUN_ARGS="--metrics"
EOF
</screen>

  <para>
   Use this service specification to get the service running using its default
   settings.
  </para>

  <para>
   You need to publish the port the &prometheus; receiver is listening on by
   using the command line argument <literal>--publish
   <replaceable>HOST_PORT</replaceable>:<replaceable>CONTAINER_PORT</replaceable></literal>
   when running the service, because the port is not exposed automatically by
   the container. This can be done by adding the following lines to the
   specification:
  </para>

<screen>
args:
    - "--publish 9099:9099"
</screen>

  <para>
   Alternatively, connect the container to the host network by using the
   command line argument <literal>--network=host</literal>.
  </para>

<screen>
args:
    - "--network=host"
</screen>

  <para>
   If the SNMP trap receiver is not installed on the same host as the
   container, then you must also specify the FQDN of the SNMP host. Use the
   container's network gateway to be able to receive SNMP traps outside the
   container/host:
  </para>

<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
</screen>

  <sect2 xml:id="configure-prometheus-webhook-snmp">
   <title>Configuring the <literal>prometheus-webhook-snmp</literal> service</title>
   <para>
    The container can be configured by environment variables or by using a
    configuration file.
   </para>
   <para>
    For the environment variables, use <literal>ARGS</literal> to set global
    options and <literal>RUN_ARGS</literal> for the <command>run</command>
    command options. You need to adapt the service specification the following
    way:
   </para>
<screen>
envs:
    - ARGS="--debug --snmp-host=<replaceable>CONTAINER_GATEWAY</replaceable>"
    - RUN_ARGS="--metrics --port=9101"
</screen>
   <para>
    To use a configuration file, the service specification must be adapted the
    following way:
   </para>
<screen>
files:
    etc/prometheus-webhook-snmp.conf:
        - "debug: True"
        - "snmp_host: <replaceable>ADD_HOST_GATEWAY_HERE</replaceable>"
        - "metrics: True"
volume_mounts:
    etc/prometheus-webhook-snmp.conf: /etc/prometheus-webhook-snmp.conf
</screen>
   <para>
    To deploy, run the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SERVICE_SPEC_FILE</replaceable></screen>
   <para>
    See <xref linkend="deploy-cephadm-day2-services"/> for more information.
   </para>
  </sect2>

  <sect2 xml:id="configure-prometheus-alertmanager-for-snmp">
   <title>Configuring the &prometheus; &alertmanager; for SNMP</title>
   <para>
    Finally, the &prometheus; &alertmanager; needs to be configured
    specifically for SNMP traps. If this service has not been deployed already,
    create a service specification file. You need to replace
    <literal>IP_OR_FQDN</literal> with the IP address or FQDN of the host where
    the &prometheus; &alertmanager; SNMP webhook has been installed. For
    example:
   </para>
   <note>
    <para>
     If you have already deployed this service, then to ensure the
     &alertmanager; is set up correctly for SNMP, re-deploy with the following
     settings.
    </para>
   </note>
<screen>
  service_type: alertmanager
  placement:
    hosts:
    - <replaceable>HOSTNAME</replaceable>
  webhook_configs:
    - 'http://<replaceable>IP_OR_FQDN</replaceable>:9099/'
</screen>
   <para>
    Apply the service specification with the following command:
   </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>SERVICE_SPEC_FILE</replaceable></screen>
  </sect2>
 </sect1>
</chapter>
