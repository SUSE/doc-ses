<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"  xml:id="monitoring-alerting">
 <title>Monitoring and Alerting</title>
 <para>
  In &productname; &productnumber;, &cephadm; deploys a monitoring and
  alerting stack. Users have to define the services that they want to deploy
  with &cephadm;. When multiple services of the same type are deployed, a
  highly-available setup is deployed.
 </para>
 <para>
   The following services are automatically deployed with &cephadm;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">&prometheus;</emphasis> is the monitoring and alerting
    toolkit.
   </para>
 </listitem>
 <listitem>
  <para>
   <emphasis role="bold">&alertmanager;</emphasis> handles alerts sent by
   the &prometheus; server.
  </para>
 </listitem>
  <listitem>
   <para>
    <emphasis role="bold">&grafana;</emphasis> is the visualization and alerting
    software.
   </para>
  </listitem>
  <listitem>
   <para>
    The <systemitem class="daemon">prometheus-node_exporter</systemitem>
    is the service running on all nodes.
   </para>
  </listitem>
 </itemizedlist>
 <para>
   The &prometheusmgr; provides a &prometheus; exporter to pass
   on &ceph; performance counters from the collection point in
   <literal>ceph-mgr</literal>.
 </para>
 <para>
  The &prometheus; configuration and <emphasis>scrape</emphasis> targets
  (exporting daemons) are setup automatically by &cephadm;. &cephadm; also
  deploys a list of default alerts, for example <literal>health
  error</literal>, <literal>10% OSDs down</literal>, or <literal>pgs
  inactive</literal>.
 </para>
 <para>&prometheus; uses the following variables:</para>
   <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold"><literal>scrape_interval</literal></emphasis>:
      change the scrape interval, how often an exporter is to be scraped.
    </para>
  </listitem>
  </itemizedlist>
 <para>
   All traffic is encrypted through &grafana;. You can either supply your
   own SSL certs or create self-signed one.</para>
 <para>&grafana; uses the following variables:</para>
 <itemizedlist>
   <listitem>
     <para>
       <emphasis role="bold"><literal>ssl_cert</literal></emphasis>
     </para>
   </listitem>
   <listitem>
     <para>
       <emphasis role="bold"><literal>ssl_key</literal></emphasis>
     </para>
   </listitem>
 </itemizedlist>
 <para>For more information on supplying your own SSL certificates, see <xref linkend="cert-sign-CA"/>
   or for creating your own, see <xref linkend="self-sign-certificates"/>.
 </para>
 <para>
  The &alertmanager; handles alerts sent by the &prometheus; server. It takes
  care of deduplicating, grouping, and routing them to the correct receiver.
  Alerts can also be silenced using the &alertmanager;.
 </para>
  <para>
    We recommend that the <systemitem class="daemon">prometheus-node_exporter</systemitem>
    is present on all nodes. This can be done using the <filename>monitoring.yaml</filename>
    file with the <literal>node-exporter</literal> service type. See
    <xref linkend="monitoring-stack-deploy"/> for more information on deploying services.
  </para>

 <sect1 xml:id="monitoring-stack-deploy">
   <title>Deploying Monitoring With &cephadm;</title>
   <para>
     By default, bootstrap will deploy a basic monitoring stack. If you did
     not do this (by passing -<option>-skip-monitoring-stack</option>, or
     if you converted an existing cluster to cephadm management, you can set
     up monitoring by following the steps below.
   </para>
   <procedure>
     <step>
       <para>
         The &prometheusmgr; needs to be enabled using the following
         command:
       </para>
 <screen>
 &prompt.cephuser;ceph mgr module enable prometheus
 </screen>
       <note>
         <para>
           Ensure this command is run before &prometheus; is deployed. If the
           <command>ceph mgr module enable prometheus</command> was not run
           before deployment, you must redeploy &prometheus;:
         </para>
 <screen>
 &prompt.cephuser;ceph orch redeploy prometheus
 </screen>
       </note>
     </step>
     <step>
       <para>
         Deploy a <systemitem class="daemon">prometheus-node_exporter</systemitem>
         service on every node of the cluster:
       </para>
<screen>
 &prompt.cephuser;ceph orch apply node-exporter '*'
</screen>
     </step>
     <step>
       <para>
         Deploy &alertmanager;:
       </para>
<screen>
&prompt.cephuser;ceph orch apply alertmanager 1
</screen>
     </step>
     <step>
       <para>
         Deploy &prometheus;:
       </para>
<screen>
&prompt.cephuser;ceph orch apply prometheus 1    # or 2
</screen>
     </step>
     <step>
       <para>
         Deploy &grafana;:
       </para>
<screen>
&prompt.cephuser;ceph orch apply grafana 1
</screen>
     </step>
   </procedure>
   <important>
     <para>
       &cephadm; handles the configuration of &prometheus;, &grafana;, and the
       &alertmanager; automatically. See <xref linkend="monitoring-cephadm-config"/>
       for more configuration options.
     </para>
   </important>
   <sect2 xml:id="monitoring-service-spec">
     <title>Deploying Using a Service and Placement Specification</title>
     <para>
       &cephadm; also uses a <filename>.yaml</filename> configuration file to deploy
       the relevant services, see <xref linkend="cephadm-service-and-placement-specs"/>
       for more information on creating service and placement specifications.
     </para>
     <para>
      To apply a configuration file and deploy the configured service, create a
      file called <filename>monitoring.yaml</filename> with the following content:
     </para>
  <screen>
    ---
    service_type: prometheus
    placement:
      hosts:
        - host1
    ---
    service_type: grafana
    placement:
      hosts:
        - host2
    ---
    service_type: alertmanager
    placement:
      hosts:
        - host1
    ---
    service_type: node-exporter
    placement:
      host_pattern: "*"
    ---
  </screen>
       <para>
         To apply the configuration file and deploy the services, run the
         following command:
       </para>
  <screen>
  &prompt.cephuser;ceph orch apply -i monitoring.yaml
  </screen>
</sect2>
   <sect2 xml:id="monitoring-stack-disable">
     <title>Disabling Monitoring</title>
     <para>
       To disable the monitoring stack, run the following commands:
     </para>
<screen>
&prompt.cephuser;ceph orch rm grafana
&prompt.cephuser;ceph orch rm prometheus --force   # this will delete metrics data collected so far
&prompt.cephuser;ceph orch rm node-exporter
&prompt.cephuser;ceph orch rm alertmanager
&prompt.cephuser;ceph mgr module disable prometheus
</screen>
   </sect2>
 </sect1>

 <sect1 xml:id="monitoring-cephadm-config">
   <title>Configuring the &prometheusmgr;</title>
     <para>
       The &prometheusmgr; is a module inside &ceph; that extends &ceph;'s
       functionality. The module reads (meta-)data from &ceph; about its state
       and health, providing the (scraped) data in a consumable format to &prometheus;.
     </para>
     <note>
       <para>
          The &prometheusmgr; needs to be restarted for any of the
          following configuration changes to be applied.
       </para>
     </note>
     <sect2 xml:id="monitoring-http-requests">
       <title>Configuring HTTP Requests</title>
       <para>
         By default, the &prometheusmgr; accepts HTTP requests on port
         9283 on all IPv4 and IPv6 addresses on the host. The port and listen
         address are both configurable with <option>ceph config-key set</option>,
         with keys <option>mgr/prometheus/server_addr</option> and
         <option>mgr/prometheus/server_port</option>. This port is registered with
         &prometheus;'s registry.</para>
       <para>
         To update the <literal>server_addr</literal> execute the following command:
       </para>
  <screen>
  &prompt.cephuser;ceph config set mgr mgr/prometheus/server_addr <replaceable>0.0.0.0</replaceable>
  </screen>
       <para>
         To update the <literal>server_port</literal> execute the following command:
       </para>
  <screen>
  &prompt.cephuser;ceph config set mgr mgr/prometheus/server_port <replaceable>9283</replaceable>
  </screen>
     </sect2>
     <sect2 xml:id="monitoring-scrape-intervals">
       <title>Configuring <literal>scrape_interval</literal></title>
       <para>
         By default, the &prometheusmgr; is configured with a scrape
         interval of 15 seconds. We do not recommend to use a scrape interval
         below 10 seconds. To set a different scrape interval in the &prometheus;
         module, set <literal>scrape_interval</literal> to the desired value:
       </para>
       <important>
         <para>
            The <literal>scrape_interval</literal> of this module should always
            be set to match the &prometheus; scrape interval to work properly and
            not cause any issues.
         </para>
       </important>
  <screen>
  &prompt.cephuser;ceph config set mgr mgr/prometheus/scrape_interval <replaceable>20</replaceable>
  </screen>
     </sect2>
     <sect2 xml:id="monitoring-stale-cache">
       <title>Configuring The Cache</title>
       <para>
         On large clusters (&lt;1000 OSDs), the time to fetch the metrics may
         become significant. Without the cache, the &prometheusmgr;
         can overload the manager and lead to unresponsive or crashing of the
         &ceph; &mgr; instances. As a result, the cache is enabled by default and
         cannot be disabled, but this does mean that the cache can become stale.
         The cache is considered stale when the time to fetch the metrics from
         &ceph; exceeds the configured <literal>scrape_interval</literal>.
       </para>
       <para>
          If this is the case, a warning will be logged and the module will
          either:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Respond with a 503 HTTP status code (service unavailable).
           </para>
         </listitem>
         <listitem>
           <para>
             Return the content of the cache, even though it might be stale.
           </para>
         </listitem>
       </itemizedlist>
       <para>
         This behaviour can be configured using the <command>ceph config set</command>
         commands.
       </para>
       <para>
         To tell the module to respond with possibly-stale data, set
         it to <literal>return</literal>:
       </para>
  <screen>
  &prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy return
  </screen>
       <para>
         To tell the module to respond with “service unavailable”, set
         it to <literal>fail</literal>:
       </para>
  <screen>
  &prompt.cephuser;ceph config set mgr mgr/prometheus/stale_cache_strategy fail
  </screen>
   </sect2>
   <sect2 xml:id="monitoring-rbd-image">
     <title>Enabling RBD-image Monitoring</title>
     <para>
       The &prometheusmgr; can optionally collect RBD per-image IO
       statistics by enabling dynamic OSD performance counters. The statistics
       are gathered for all images in the pools that are specified in the
       <literal>mgr/prometheus/rbd_stats_pools</literal> configuration parameter.
     </para>
     <para>
       The parameter is a comma or space separated list of <literal>pool[/namespace]</literal>
       entries. If the namespace is not specified the statistics are collected
       for all namespaces in the pool.
     </para>
     <para>
       For example:
     </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools "<replaceable>pool1,pool2,poolN</replaceable>"
</screen>
     <para>
       The module makes the list of all available images scanning the specified
       pools and namespaces and refreshes it periodically. The period is
       configurable via the <literal>mgr/prometheus/rbd_stats_pools_refresh_interval</literal>
       parameter (in sec) and is 300 sec (5 minutes) by default.
     </para>
     <para>
       For example, if you changed the sync interval to 10 minutes:
     </para>
<screen>
&prompt.cephuser;ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <replaceable>600</replaceable>
</screen>
   </sect2>
 </sect1>

 <sect1 xml:id="prometheus-security-model">
   <title>&prometheus; Security Model</title>
   <para>
     &prometheus;' security model presumes that untrusted users have access to the
     &prometheus; HTTP endpoint and logs. Untrusted users have access to all the
     (meta-)data &prometheus; collects that is contained in the database, plus a
     variety of operational and debugging information.
   </para>
   <para>
     However, &prometheus;' HTTP API is limited to read-only operations.
     Configurations can not be changed using the API, and secrets are not
     exposed. Moreover, &prometheus; has some built-in measures to mitigate the
     impact of denial-of-service attacks.
   </para>
 </sect1>

<!--  <sect1>
   <title>SNMP Trap Receiver</title>
   <para>
    If you want to get notified about &prometheus; alerts via SNMP traps, then
    you can install the &prometheus; &alertmanager; SNMP trap receiver via
    &cephadm;. To do so, you need to enable it in the Pillar under the
    <option>monitoring:alertmanager_receiver_snmp:enabled</option> key. The
    configuration of the receiver must be set under the
    <option>monitoring:alertmanager_receiver_snmp:config</option> key.
   </para>
   <example>
    <title>SNMP Trap Configuration</title>
<screen>
monitoring:
 alertmanager:
   receiver:
      snmp:
        enabled: True
        config:
          host: localhost
          port: 9099
          snmp_host: snmp.foo-bar.com
          snmp_community: private
          metrics: True
</screen>
     <para>
      Refer to the receiver manual at
      <link xlink:href="https://github.com/SUSE/prometheus-webhook-snmp#global-configuration-file" />.
      for more details about the configuration options.
     </para>
    </example>
  </sect1>-->
</chapter>
