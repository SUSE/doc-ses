<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage.bp.cluster_mntc">
 <title>Cluster Maintenance and Troubleshooting</title>
 <para/>
 
 <sect1 xml:id="storage.bp.cluster_mntc.calamari_addpool">
  <title>Creating and Deleting Pools from Calamari</title>

  <para>
   Apart from using the command line to create or delete pools (see
   <xref linkend="storage.bp.cluster_mntc.add_pool"/> and
   <xref linkend="storage.bp.cluster_mntc.del_pool"/>), you can do the same
   from within Calamari in a more comfortable user interface.
  </para>

  <para>
   To create a new pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png" width="25px"/>
     </imageobject>
     </inlinemediaobject> in the right top.
    </para>
   </step>
   <step>
    <para>
     Enter a name for the new pool, and either change the number of replicas,
     number of placement groups, and the CRUSH ruleset, or leave them at
     default values.
    </para>
   </step>
   <step>
    <para>
     Click <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_adpool_plus.png" width="25px"/>
     </imageobject>
     </inlinemediaobject> to confirm, then <guimenu>Cancel</guimenu> the
     warning dialog.
    </para>
   </step>
   <step>
    <para>
     Now you can see the new pool in the list of all existing pools. You can
     verify the existence of the new pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>

  <para>
   To delete an existing pool using Calamari, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Log in to a running instance of Calamari.
    </para>
   </step>
   <step>
    <para>
     Go to
     <menuchoice><guimenu>Manage</guimenu><guimenu>Pools</guimenu></menuchoice>.
     You can see a list of the cluster's existing pools.
    </para>
   </step>
   <step>
    <para>
     From the list of pools, choose the one to delete and click the related
     <inlinemediaobject>
     <imageobject>
      <imagedata fileref="calamari_rmpool_thrash.png" width="25px"/>
     </imageobject>
     </inlinemediaobject>
    </para>
   </step>
   <step>
    <para>
     Confirm the deletion and <guimenu>Cancel</guimenu> the warning dialog.
    </para>
   </step>
   <step>
    <para>
     You can verify the deletion of the pool on the command line with
    </para>
<screen>ceph osd lspools</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.mng_keyrings">
  <title>Managing Keyring Files</title>

  <para>
   When &ceph; runs with authentication and authorization enabled (enabled by
   default), you must specify a user name and a keyring containing the secret
   key of the specified user. If you do not specify a user name, &ceph; will use
   <literal>client.admin</literal> as the default user name. If you do not
   specify a keyring, &ceph; will look for a keyring via the
   <option>keyring</option> setting in the &ceph; configuration. For example,
   if you execute the <command>ceph health</command> command without specifying
   a user or keyring:
  </para>

<screen>ceph health</screen>

  <para>
   &ceph; interprets the command like this:
  </para>

<screen>ceph -n client.admin --keyring=/etc/ceph/ceph.client.admin.keyring health</screen>

  <para>
   <command>ceph-authtool</command> is a utility to create, view, and modify a
   &ceph; keyring file. A keyring file stores one or more &ceph; authentication
   keys and possibly an associated capability specification. Each key is
   associated with an entity name, of the form {client,mon,mds,osd}.name.
  </para>

  <para>
   To create a new <filename>keyring</filename> file in the current directory
   containing a key for <literal>client.example1</literal>:
  </para>

<screen>ceph-authtool -C -n client.example1 --gen-key keyring</screen>

  <para>
   To add a new key for <literal>client.example2</literal>, omit the
   <option>-C</option> option:
  </para>

<screen>ceph-authtool -n client.example2 --gen-key keyring</screen>

  <para>
   The <filename>keyring</filename> now has two entries:
  </para>

<screen>ceph-authtool -l keyring
 [client.example1]
     key = AQCQ04NV8NE3JBAAHurrwc2BTVkMGybL1DYtng==
 [client.example2]
     key = AQBv2INVWMqFIBAAf/4/H3zxzAsPBTH4jsN80w==</screen>

  <para>
   For more information on <filename>ceph-authtool</filename>, see its manual
   page <filename>man 8 ceph-authtool</filename>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.create_client_keys">
  <title>Creating Client Keys</title>

  <para>
   User management functionality provides &ceph; cluster administrators with
   the ability to create, update and delete users directly in the cluster
   environment.
  </para>

  <tip>
   <para>
    When you create or delete users in the &ceph; cluster, you may need to
    distribute keys to clients so that they can be added to keyrings.
   </para>
  </tip>

  <para>
   Adding a user creates a user name (TYPE.ID), a secret key and possibly
   capabilities included in the command you use to create the user. A user’s
   key enables the user to authenticate with the cluster. The user’s
   capabilities authorize the user to read, write, or execute on monitors,
   OSDs, or metadata servers.
  </para>

  <para>
   Authentication key creation usually follows cluster user creation. There are
   several ways to add a user. The most convenient seems to be using
  </para>

<screen>ceph auth get-or-create</screen>

  <para>
   It returns a keyfile format with the user name [in brackets] and the key. If
   the user already exists, this command simply returns the user name and key
   in the keyfile format. You may use the<option>-o
   <replaceable>filename</replaceable></option> option to save the output to a
   file.
  </para>

<screen>ceph auth get-or-create client.example1
 [client.example1]
    key = AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   You can verify that the client key was added to the cluster keyring:
  </para>

<screen>ceph auth list
    [...]
 client.example1
    key: AQDs+odVODCGGxAAvmSnsNx3XYHJ7Ri6sZFfhw==</screen>

  <para>
   When creating client users, you may create a user with no capabilities. A
   user with no capabilities is useless beyond mere authentication, because the
   client cannot retrieve the cluster map from the monitor. However, you can
   create a user with no capabilities if you want to defer adding capabilities
   later using the <command>ceph auth caps</command> command.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>

  <para>
   Find more details in the related upstream documentation, see
   <link xlink:href="http://ceph.com/docs/master/rados/operations/user-management/">User
   Management</link>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.revoke_client_keys">
  <title>Revoking Client Keys</title>

  <para>
   If you need to remove an already generated client key from the keyring file,
   use the <command>ceph auth del</command> command. To remove the key for user
   <literal>client.example1</literal> that we added in
   <xref linkend="storage.bp.cluster_mntc.create_client_keys"/>:
  </para>

<screen>ceph auth del client.example1</screen>

  <para>
   and check the deletion with <command>ceph auth list</command>.
  </para>

  <tip>
   <para>
    After you add a key to the cluster keyring, go to the relevant client(s)
    and copy the keyring from the cluster host to the client(s).
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells &ceph; how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given rule set, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written&mdash;you can increase their weight to
   have &ceph; write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="Cluster_Time_Setting">
  	<title>Time Synchronization of Nodes</title>
  	<para>
  	&ceph; requires precise time synchronization between particular nodes. You should set up a node with your own NTP server. Even though you can point all ntpd instances to a remote public time server, we do not recommend it with &ceph;.  With such a configuration, each node in the cluster has its own NTP daemon that communicate continually over the Internet with a set of three or four time servers, all of which are quite some hops away. This solution introduces a large degree of latency variability that makes it difficult or impossible to keep the clock drift under 0.05 seconds (which is what the &ceph; monitors require).
  	</para>
  	<para>
  	Thus use a single machine as the NTP server for the whole cluster. Your NTP server ntpd instance may then point to the remote (public) NTP server or it can have its own time source. The ntpd instances on all nodes are then pointed to this local server. Such a solution has several advantages like&mdash;eliminating unnecessary network traffic and clock skews, decreasing load on the public NTP servers. For details how to set up the NTP server refer to <link xlink:href="https://www.suse.com/documentation/sled11/book_sle_admin/data/cha_netz_xntp.html">&sls; Administration Guide</link>.
  	</para>
  	<para>
  	Then to change the time on your cluster, do the following:
  	</para>
  	<important>
  		<title>Setting Time</title>
  		<para>
  		You may face a situation when you need to set the time back, e.g. if the time changes from the summer to the standard time. We do not recommend to move the time backward for a longer period than the cluster is down. Moving the time forward does not cause any trouble.
  		</para>
  	</important>
  	<procedure>
  		<title>Time Synchronization on the Cluster</title>
  		<step>
  			<para>
  			Stop all clients accessing the &ceph; cluster, especially those using iSCSI.
  			</para>
  		</step>
  		<step>
  			<para>
  			Shut down your &ceph; cluster. On each node run:
  			</para>
  			<screen>rcceph stop</screen>
  			<note>
  				<para>
  				If you use &ceph; and &ocloud;, stop also the &ocloud;. 
  				</para>
  			</note>
  		</step>
  		<step>
  			<para>
  			Verify that your NTP server is set up correctly&mdash;all ntpd daemons get their time from a source or sources in the local network.
  			</para>
  		</step>
  		<step>
  			<para>
  			Set the correct time on your NTP server.
  			</para>
  		</step>
  		<step>
  			<para>
  			Verify that NTP is running and working properly, on all nodes run:
  			</para>
  			<screen>status ntpd.service</screen>
  			<para>or</para>
  			<screen>ntpq -p</screen>
  		</step>
  		<step>
  			<para>
  			Start all monitoring nodes and verify that there is no clock skew:
  			</para>
  			<screen>systemctl start <replaceable>target</replaceable></screen>
  		</step>
  		<step>
  			<para>
  			Start all OSD nodes.
  			</para>
  		</step>
  		<step>
  			<para>
  			Start other &ceph; services.
  			</para>
  		</step>
  		<step>
  			<para>
  			Start the &ocloud; if you have it.
  			</para>
  		</step>
  	</procedure>
  	
  </sect1>

 
 <sect1 xml:id="storage.bp.cluster_mntc.sw_upg">
  <title>Upgrading Software</title>

  <para>
   Both &sls; and &storage; products are provided with regular package updates.
   To apply new updates to the whole cluster, you need to run
  </para>

<screen>sudo zypper dup</screen>

  <para>
   on all cluster nodes. Remember to upgrade all the monitor nodes first, and
   then all the OSD nodes one by one.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pgnum">
  <title>Increasing the Number of Placement Groups</title>

  <para>
   When creating a new pool, you specify the number of placement groups for the
   pool (see <xref linkend="ceph.pools.operate.add_pool"/>). After adding more
   OSDs to the cluster, you usually need to increase the number of placement
   groups as well for performance and data durability reasons. For each
   placement group, OSD and monitor nodes need memory, network and CPU at all
   times and even more during recovery. From which follows that minimizing the
   number of placement groups saves significant amounts of resources.
  </para>

  <warning>
   <title>Too High Value of <option>pg_num</option></title>
   <para>
    When changing the <option>pg_num</option> value for a pool, it may happen
    that the new number of placement groups exceeds the allowed limit. For
    example
   </para>
<screen>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
   <para>
    The limit prevents extreme placement group splitting, and is derived from
    the <option>mon_osd_max_split_count</option> value.
   </para>
  </warning>

  <para>
   To determine the right new number of placement groups for a resized cluster
   is a complex task. One approach is to continuously grow the number of
   placement groups up to the state when the cluster performance is optimal. To
   determine the new incremented number of placement groups, you need to get
   the value of the <option>mon_osd_max_split_count</option> parameter, and add
   it to the current number of placement groups. To give you a basic idea, take
   a look at the following script:
  </para>

<screen>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
 pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
 echo "current pg_num value: $pg_num, max increment: $max_inc"
 next_pg_num="$(($pg_num+$max_inc))"
 echo "allowed increment of pg_num: $next_pg_num"</screen>

  <para>
   After finding out the next number of placement groups, increase it with
  </para>

<screen>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.add_pool">
  <title>Adding a Pool</title>

  <para>
   After you first deploy a cluster, &ceph; uses the default pools to store
   data. You can later create a new pool with
  </para>

<screen>ceph osd pool create</screen>

  <para>
   For more information on cluster pool creation, see
   <xref linkend="ceph.pools.operate.add_pool"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.del_pool">
  <title>Deleting a Pool</title>

  <para>
   By deleting a pool, you permanently destroy all data stored in that pool.
   You can delete a previously created pool with
  </para>

<screen>ceph osd pool delete</screen>

  <para>
   For more information on cluster pool deletion, see
   <xref linkend="ceph.pools.operate.del_pool"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.troubleshooting">
 	<title>Troubleshooting</title>
 	<para>
 	This section describes several issues that you may face when you operate a &ceph; cluster.
 	</para>
 	
 	<sect2 xml:id="storage.bp.cluster_mntc.rados_striping">
  <title>Sending Large Objects with <command>rados</command> Fails with Full OSD</title>

  <para>
   <command>rados</command> is a command line utility to manage RADOS object
   storage. For more information, see <command>man 8 rados</command>.
  </para>

  <para>
   If you send a large object to a &ceph; cluster with the
   <command>rados</command> utility, such as
  </para>

<screen>rados -p mypool put myobject /file/to/send</screen>

  <para>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance. RADOS has a 'striper' API that enables applications to
   stripe large objects over multiple OSDs. If you turn the striping feature on
   with the <option>--striper</option> option, you can prevent the OSD from
   filling up.
  </para>

<screen>rados --striper -p mypool put myobject /file/to/send</screen>
 </sect2>
 
 <sect2 xml:id="ceph.xfs.corruption">
 	<title>Corrupted XFS File System</title>
 	<para>
 	In rare circumstances like kernel bug or broken/misconfigured hardware, the underlying file system (XFS) in which an OSD stores its data might be damaged and unmountable.
 	</para>
 	<para>
 	If you are sure there is no problem with your hardware and the system is configured properly, raise a bug against the XFS subsystem of the &sls; kernel and mark the particular OSD as down:
 	</para>
 	<screen>ceph osd down <replaceable>OSD identification</replaceable></screen>
 	<warning>
 	<title>Do Not Format or Otherwise Modify the Damaged Device</title>
 	<para>
 	Even though using <command>xfs_repair</command> to fix the problem in the file system may seem reasonable, do not use it as the command modifies the file system. The OSD may start but its functioning may be influenced.
 	</para>
 	</warning>
 	<para>
 	Now zap the underlying disk and recreate the OSD by running:
 	</para>
 	<screen>ceph-disk prepare --zap $OSD_DISK_DEVICE $OSD_JOURNAL_DEVICE"</screen>
 	<para>for example:</para>
 	<screen>ceph-disk prepare --zap /dev/sdb /dev/sdd2</screen>
 </sect2>
 </sect1>
</chapter>
