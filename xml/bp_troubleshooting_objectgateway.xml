<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-objectgateway">
 <title>Troubleshooting &ogw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="troubleshooting-gateway-healthcheck">
   <title>Basic healthcheck</title>
   <para>
     The most basic health check to test a running &ogw; process is to simply
     point your browser or client at the &ogw; endpoint. This should return
     an empty bucket results for the anonymous user:
   </para>
<screen>
&prompt.smaster;curl -I
</screen>
   <para>
     If the result returns a 405 status, this indicates MethodNotAllowed.
     The output will result in a NoSuchBucket XML file. If the result returns
     a 404 status, this means the &ogw; DNS name is misconfigured or the
     requests were not made at an endpoint resolving to &ogw; DNS name.
   </para>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-notrunning">
   <title>The Gateway is Not Running</title>
   <para>
     Usually a restarted gateway (usually automatic under systemd) should
     restore the service. This can be achieved via:
   </para>
<screen>
&prompt.cephuser;ceph orch daemon restart <replaceable>rgw-daemon-name</replaceable>
</screen>
   <para>
     For bumping up the log level, under the <literal>client.<replaceable>rgw-name</replaceable></literal>
     section, the &ogw; configurable can be increased from 1 (default)
     up to 20 (very verbose). For the messages sent to the cluster itself the
     messenger debug levels can be raised (off by default). This is controlled
     via <literal>debug ms</literal>. Usually a level of 1 is sufficient to
     gather enough information. For injecting arguments in a running process:
   </para>
<screen>
&prompt.cephuser;ceph-daemon <replaceable>client.rgw</replaceable> config set debug_rgw 20
</screen>
   <para>
     Or, alternatively, using the &ceph; CLI:
   </para>
<screen>
&prompt.cephuser;ceph config set <replaceable>client.rgw-name</replaceable> debug_rgw 20
</screen>
   <para>
     For persistent changes or a non running process it is best to set these
     lines in the <filename>ceph.conf</filename> file under the <literal>client.<replaceable>rgw-name</replaceable></literal>
     section. In this case, the debug levels are extremely verbose, so it is
     best to do this only to capture error logs for a short time.
   </para>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-crashed-rrgw">
   <title>Crashed &ogw; Process</title>
   <para>
     If the &ogw; process dies, you will normally see a connection refused
     at the client. In that situation, restarting &ogw; will restore the service.
   </para>
   <para>
     To diagnose the cause of the crash, check the log in <filename>/var/log/ceph</filename>
     or the core file (if one was generated).
   </para>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-blocked">
   <title>Blocked &ogw; Requests</title>
   <para>
     If some (or all) &ogw; requests appear to be blocked, you can get some
     insight into the internal state of the &ogw; daemon via its admin socket.
     By default, there will be a socket configured to reside in
     <filename>/var/run/ceph</filename>, and the daemon can be queried with:
   </para>
<screen>
&prompt.cephuser;ceph daemon /var/run/ceph/client.<replaceable>rgw-name</replaceable> help
  help                list available commands
  objecter_requests   show in-progress osd requests
  perfcounters_dump   dump perfcounters value
  perfcounters_schema dump perfcounters schema
  version             get protocol version
</screen>
   <para>
     Of particular interest:
   </para>
<screen>
&prompt.cephuser;ceph daemon /var/run/ceph/client.rgw objecter_requests
...
</screen>
  <para>
    This will dump information about current in-progress requests with the
    &rados; cluster. This allows one to identify if any requests are blocked
    by a non-responsive OSD. For example, one might see:
  </para>
<screen>
{ "ops": [
      { "tid": 1858,
        "pg": "2.d2041a48",
        "osd": 1,
        "last_sent": "2012-03-08 14:56:37.949872",
        "attempts": 1,
        "object_id": "fatty_25647_object1857",
        "object_locator": "@2",
        "snapid": "head",
        "snap_context": "0=[]",
        "mtime": "2012-03-08 14:56:37.949813",
        "osd_ops": [
              "write 0~4096"]},
      { "tid": 1873,
        "pg": "2.695e9f8e",
        "osd": 1,
        "last_sent": "2012-03-08 14:56:37.970615",
        "attempts": 1,
        "object_id": "fatty_25647_object1872",
        "object_locator": "@2",
        "snapid": "head",
        "snap_context": "0=[]",
        "mtime": "2012-03-08 14:56:37.970555",
        "osd_ops": [
              "write 0~4096"]}],
"linger_ops": [],
"pool_ops": [],
"pool_stat_ops": [],
"statfs_ops": []}
</screen>
  <para>
    In this dump, two requests are in progress. The <literal>last_sent</literal>
    field is the time the &rados; request was sent. If this is a while ago,
    it suggests that the OSD is not responding. For example, for request
    1858, you could check the OSD status with:
  </para>
<screen>
&prompt.cephuser;ceph pg map 2.d2041a48
osdmap e9 pg 2.d2041a48 (2.0) -> up [1,0] acting [1,0]
</screen>
  <para>
    This tells us to look at <literal>osd.1</literal>, the primary copy for this PG:
  </para>
<screen>
  ceph daemon osd.1 ops
  { "num_ops": 651,
   "ops": [
         { "description": "osd_op(client.4124.0:1858 fatty_25647_object1857 [write 0~4096] 2.d2041a48)",
           "received_at": "1331247573.344650",
           "age": "25.606449",
           "flag_point": "waiting for sub ops",
           "client_info": { "client": "client.4124",
               "tid": 1858}},
  ...
</screen>
  <para>
    The <literal>flag_point</literal> field indicates that the OSD is
    currently waiting for replicas to respond, in this case <literal>osd.0</literal>.
  </para>
 </sect1>
 <sect1 xml:id="large-omap-issues">
  <title>Large OMAP Issues</title>
  <para>
    If you receive a cluster warning for a large OMAP issue, it means that
    some object has exceeded one of the osd deep scrub large OMAP object *
    threshold, usually set to 200K keys and 1G of storage. The relevant pool or PG
    involved can be found by grepping for large OMAP object as mentioned in
    the warning message. Depending on the pool and the object key involved
    it might indicate one of the following:
  </para>
  <sect2 xml:id="resharding-issues">
    <title>Resharding Issues</title>
    <para>
      If the large omap object name happens to be in the
      <literal><replaceable>zone</replaceable>.rgw.buckets.index</literal> pool
      this means that a bucket has more than 200K keys, dynamic resharding is
      set on by default in single site clusters which would automatically have
      resharded the bucket, for multi site clusters however this is not supported
      and would have to be manually done <xref linkend="ogw-bucket-reshard"/>. Looking at the
      bucket stats would reveal the total object count and the number of shards.
      This is helpful to understand if the bucket has already been resharded,
      in which case the warning is just because the deep scrub happened to run
      before the reshard process and would be cleared in the next deep scrub.
    </para>
  </sect2>
  <sect2 xml:id="usage-statistics">
    <title>Usage Statistics</title>
    <para>
      Usage stats are also stored as omap keys in the <literal><replaceable>zone</replaceable>.rgw.log</literal>
      pool, however these are not automatically trimmed, so a manual trimming
      of usage stats would have to be done. For example:
    </para>
<screen>
&prompt.smaster;radosgw-admin usage trim [--uid=<replaceable>user-id</replaceable>] --start-date=2019-01-01 --end-date=2019-03-03
</screen>
  </sect2>
 </sect1>
</chapter>
