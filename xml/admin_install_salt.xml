<?xm version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &deepsea;/&salt;</title>
 <note>
  <title><command>ceph-deploy</command> Removed in &storage; 5</title>
  <para>
   The <command>ceph-deploy</command> cluster deployment tool was deprecated in
   &storage; 4 and is completely removed in favor of &deepsea; as of &storage;
   5.
  </para>
 </note>
 <para>
  &salt; along with &deepsea; is a <emphasis>stack</emphasis> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with &salt;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
    the &ceph; environment, &sminion; is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    &smaster; runs its own &sminion;. It is required for running privileged
    tasks&mdash;for example creating, authorizing, and copying keys to
    minions&mdash;so that remote minions never need to run privileged tasks.
    The master minion is not supposed to run &ceph;-related services such as
    OSD, MON, or &rgw;.
   </para>
  </listitem>
  <listitem>
   <para>
    &sminion;s need to correctly resolve the &smaster;'s host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name, but you can specify any other network-reachable host name in the
    <filename>/etc/salt/minion</filename> file, see
    <xref linkend="ceph.install.stack"/>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha.ceph.install.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package>,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link
     xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="deepsea.description">
  <title>Introduction to &deepsea;</title>

  <para>
   The goal of &deepsea; is to save the administrator time and confidently
   perform complex operations on a &ceph; cluster.
  </para>

  <para>
   &ceph; is a very configurable software solution. It increases both freedom
   and responsibility of system administrators.
  </para>

  <para>
   The minimal &ceph; setup is good for demonstration purposes, but does not
   show interesting features of &ceph; that you can see with a big number of
   nodes.
  </para>

  <para>
   &deepsea; collects and stores data about individual servers, such as
   addresses and device names. For a distributed storage system such as &ceph;,
   there can be hundreds of such items to collect and store. Collecting the
   information and entering the data manually into a configuration management
   tool is exhausting and error prone.
  </para>

  <para>
   The steps necessary to prepare the servers, collect the configuration, and
   configure and deploy &ceph; are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </para>

  <para>
   &deepsea; addresses these observations with the following strategy:
   &deepsea; consolidates the administrators decisions in a single file. The
   decisions include cluster assignment, role assignment and profile
   assignment. And &deepsea; collects each set of tasks into a simple goal.
   Each goal is a <emphasis>stage</emphasis>:
  </para>

  <itemizedlist xml:id="deepsea.stage.description">
   <title>&deepsea; Stages Description</title>
   <listitem>
    <para>
     <emphasis role="bold">Stage 0</emphasis>&mdash;the
     <emphasis role="bold">preparation</emphasis>&mdash; during this stage all
     required updates are applied and your system may be rebooted. Some parts
     of this stage are optional&mdash;for example you can distribute updates to
     your nodes your own way&mdash;while some are required. To run only the
     required steps of Stage 0, you have to invoke them manually.
    </para>
    <important>
     <title>Re-run Stage 0 after &smaster; Reboot</title>
     <para>
      If during Stage 0 the &smaster; reboots to load new kernel version, you
      need to run Stage 0 again, otherwise minions will not be targeted.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 1</emphasis>&mdash;the
     <emphasis role="bold">discovery</emphasis>&mdash; here you detect all
     hardware in your cluster and collect necessary information for the &ceph;
     configuration. For details about configuration refer to
     <xref linkend="deepsea.pillar.salt.configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 2</emphasis>&mdash;the
     <emphasis role="bold">configuration</emphasis>&mdash; you need to prepare
     configuration data in a particular format.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 3</emphasis>&mdash;the
     <emphasis role="bold">deployment</emphasis>&mdash; creates a basic &ceph;
     cluster with OSD and monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 4</emphasis>&mdash;the
     <emphasis role="bold">services</emphasis>&mdash; additional features of
     &ceph; like iSCSI, &rgw; and &cephfs; can be installed in this stage. Each
     is optional.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 5</emphasis>&mdash;the removal stage. This
     stage is not mandatory and during the initial setup it is usually not
     needed. In this stage the roles of minions and also the cluster
     configuration are removed. You need to run this stage when you need to
     remove a storage node from your cluster. For details refer to
     <xref linkend="salt.node.removing"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="deepsea.organisation.locations">
   <title>Organization and Important Locations</title>
   <para>
    &salt; has several standard locations and several naming conventions used
    on your master node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       The directory stores configuration data for your cluster minions.
       <emphasis>Pillar</emphasis> is an interface for providing global
       configuration values to all your cluster minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       The directory stores &salt; state files (also called
       <emphasis>sls</emphasis> files). State files are formatted description
       of states in which the cluster should be. For more information, refer to
       the
       <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">&salt;
       documentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       The directory stores Python scripts known as runners. Runners are
       executed on the master node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       The directory stores Python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       The directory is used by &deepsea;. Collected configuration data are
       stored here.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       A directory used by &deepsea;. It stores sls files that can be in
       different format, but each subdirectory contains sls files. Each
       subdirectory contains only one type of sls file. For example,
       <filename>/srv/salt/ceph/stage</filename> contains orchestration files
       that are executed by the <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.stack">
  <title>Cluster Deployment</title>

  <para>
   The cluster deployment process has several phases. First, you need to
   prepare all nodes of the cluster by configuring &salt; and then deploy and
   configure &ceph;.
  </para>

  <para>
   The following procedure describes the cluster preparation in detail.
  </para>

  <procedure>
   <step>
    <para>
     Install and register &cephos; together with &storage; extension on each
     node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Enable and start the NTP time synchronization server:
    </para>
<screen>&prompt.smaster;systemctl enable ntpd.service
&prompt.smaster;systemctl start ntpd.service</screen>
   </step>
   <step>
    <para>
     Check whether the &aa; service is running and disable it on each cluster
     node. Start &yast; &aa; module, and select <guimenu>Settings</guimenu> and
     then deactivate the <guimenu>Enable Apparmor</guimenu> checkbox. Confirm
     with <guimenu>Done</guimenu>.
    </para>
    <para>
     Note that &storage; will <emphasis>not</emphasis> work with &aa; enabled.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the node which will be the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
    <para>
     The command installs the <literal>salt-master</literal> and
     <literal>salt-minion</literal> packages as a dependence.
    </para>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all minion nodes.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.sminion;systemctl enable salt-minion.service
&prompt.sminion;systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the &smaster; node has ports 4505 and 4506 open to all
     &sminion;s nodes on the firewall. If not, you can open them using the
     <command>yast2 firewall</command> command and allowing the
     <guimenu>SaltStack</guimenu> service.
    </para>
   </step>
   <step>
    <para>
     Configure all minions (including the master minion) to connect to the
     master. If your &smaster; is not reachable by the DNS name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master:<replaceable> DNS_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the file
     <filename>/srv/pillar/ceph/master_minion.sls</filename> on the &smaster;
     points to your &smaster;. If you used the default host name for your
     &smaster; - <emphasis>salt</emphasis> in the <emphasis>ses</emphasis>
     domain, then the file looks as follows:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Prior to deploying &storage; make sure that all disks that were used as
     OSD by previous clusters are empty without partitions. To ensure this, you
     have to manually zap all the disks. Remember to replace 'X' with the
     correct disk letter:
    </para>
    <substeps>
     <step>
      <para>
       Wipe the beginning of each partition:
      </para>
<screen>for partition in /dev/sdX?*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       Wipe the partition table:
      </para>
<screen>sgdisk -Z --clear -g /dev/sdX</screen>
     </step>
     <step>
      <para>
       Wipe the backup partition tables:
      </para>
<screen>size=`blockdev --getsz /dev/sdX`
position=$((size/4096 - 33))
dd if=/dev/zero of=/dev/sdX bs=4096 count=33 seek=$position oflag=direct</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <para>
   Now you deploy and configure &ceph;. Unless specified otherwise, all steps
   are mandatory.
  </para>

  <note>
   <title>&salt; Command Conventions</title>
   <para>
    There are two possibilities how to run <command>salt-run
    state.orch</command> - one is with <literal>stage.&lt;stage
    number&gt;</literal>, the other is with a name of the stage. Both notations
    have the same impact and it is fully up to your preferences which command
    will you use.
   </para>
  </note>

  <important>
   <title>Targeting</title>
   <para>
    The semantic
   </para>
<screen>salt '<replaceable>target-identifier</replaceable>' example.module</screen>
   <para>
    is used to target a set of minions by their name. In the following example,
    we assume that you only control minions that relate to your &storage;
    deployment. Hence we use &salt;'s syntax for matching all available
    minions:
   </para>
<screen>&prompt.smaster;salt '*' example.module</screen>
   <para>
    If this does not correspond to your deployment, substitute the '*' target
    identifier to match your desired patterns. For example:
   </para>
<screen>&prompt.smaster;salt 'datacenter1.storage.example.com.*' example.module</screen>
  </important>

  <procedure>
   <step>
    <para>
     Prepare your cluster. You can omit specific parts of this step, see
     <xref
      linkend="deepsea.stage.description"/> for more details.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.prep</screen>
   </step>
   <step>
    <para>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <filename>/srv/pillar/ceph/proposals</filename>. The data are stored in
     the YAML format in *.sls or *.yml files.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     After the previous command finishes successfully, create a
     <filename>policy.cfg</filename> file in
     <filename>/srv/pillar/ceph/proposals</filename>. For details refer to
     <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     The configuration stage parses the <filename>policy.cfg</filename> file
     and merges the included files into their final form. Cluster and role
     related contents are placed in
     <filename>/srv/pillar/ceph/cluster</filename>, while &ceph; specific
     content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Run the following command to trigger the configuration stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.configure</screen>
    <para>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for the specified minions (for
     example named <literal>ceph_minion1</literal>,
     <literal>ceph_minion2</literal> ...) by running:
    </para>
<screen>&prompt.smaster;salt 'ceph_minion*' pillar.items</screen>
    <note>
     <title>Overwriting Defaults</title>
     <para>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <xref linkend="custom.configuration"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Now you run the deployment stage. In this stage, the pillar is validated
     and monitors and ODS daemons are started on the storage nodes. Run the
     following to start the stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.deploy
    </screen>
    <para>
     The command may take several minutes. If it fails, you have to fix the
     issue and run the previous stages again. After the command succeeds, run
     the following to check the status:
    </para>
<screen>&prompt.smaster;ceph -s</screen>
   </step>
   <step>
    <para>
     The last step of the &ceph; cluster deployment is the
     <emphasis>services</emphasis> stage. Here you instantiate any of the
     currently supported services: &igw;, &cephfs;, &rgw;, &oa;, and &ganesha;.
     In this stage, the necessary pools, authorizing keyrings and starting
     services are created. To start the stage, run the following:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.services</screen>
    <para>
     Depending on the setup, the command may run several minutes. By default,
     &oa; will be installed on the master node. If you need to install &oa; on
     a different node, refer to
     <xref linkend="deepsea.pillar.salt.configuration"/>. If you do not need to
     install &oa; at all (it is not a mandatory part of &storage;), refer to
     <xref linkend="ceph.oa"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea.pillar.salt.configuration">
  <title>Configuration and Customization</title>

  <sect2 xml:id="policy.configuration">
   <title>The <filename>policy.cfg</filename> File</title>
   <para>
    The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    configuration file is used to determine roles of individual cluster nodes.
    For example which node acts as an OSD or which as a monitor node. Edit
    <filename>policy.cfg</filename> in order to reflect your desired cluster
    setup. The order of the sections is arbitrary, but the content of included
    lines overwrites matching keys from the contents of previous lines.
   </para>
   <tip>
    <title>Examples of <filename>policy.cfg</filename></title>
    <para>
     You can find several examples of complete policy files in the
     <filename>/usr/share/doc/packages/deepsea/examples/</filename> directory.
    </para>
   </tip>
   <sect3 xml:id="policy.cluster.assignment">
    <title>Cluster Assignment</title>
    <para>
     In the <emphasis role="bold">cluster</emphasis> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <emphasis role="bold">ceph</emphasis> follows.
    </para>
    <para>
     To include <emphasis role="bold">all</emphasis> minions, add the following
     lines:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     To <emphasis role="bold">whitelist</emphasis> a particular minion:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     or a group of minions&mdash;you can shell glob matching:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s
     to <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy.role.assignment">
    <title>Role Assignment</title>
    <para>
     In this section you need to assign roles to your cluster nodes. The
     assignment follows this pattern:
    </para>
<screen>role-<replaceable>role name</replaceable>/<replaceable>path</replaceable>/<replaceable>included files</replaceable></screen>
    <para>
     Where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>role name</replaceable> is any of the following: 'master',
       'admin', 'mon', 'mds', 'igw', 'rgw', or 'ganesha'. For a detailed
       description, see below.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>path</replaceable> is a relative path to sls or yml files.
       Usually in case of sls files it is <filename>cluster</filename>, while
       yml files are located at
       <filename>stack/default/ceph/minions</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>included files</replaceable> are the &salt; state files or
       YAML configuration files. Shell globing can be used for more specific
       matching.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     An example for each role follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - the node has admin keyrings to all &ceph;
       clusters. Currently, only a single &ceph; cluster is supported. The
       <emphasis>master</emphasis> role is mandatory, always add a similar line
       to the following:
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - the minion will have an admin keyring. You
       define the role as follows:
      </para>
<screen>role-master/custer/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - the minion will provide the monitoring
       service to the &ceph; cluster. This role requires addresses of the
       assigned minions, thus you need to include the files from the
       <filename>stack</filename> directory in addition to the sls files:
      </para>
<screen>role-mon/stack/default/ceph/minions/mon*.yml
role-mon/cluster/mon*.sls</screen>
      <para>
       The example assigns the monitoring role to a group of minions.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - the minion will provide the metadata service
       to support &cephfs;.
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - the minion will act as an &igw;. This role
       requires addresses of the assigned minions, thus you need to also
       include the files from the <filename>stack</filename> directory:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - the minion will act as a &rgw;:
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>ganesha</emphasis> - the minion will act as a &ganesha;
       server. The 'ganesha' role requires either a 'rgw' or 'mds' role in
       cluster, otherwise the validation will fail in Stage 3.
      </para>
      <para>
       To successfully install &ganesha;, additional configuration is required.
       If you want to use &ganesha;, read <xref linkend="cha.as.ganesha" />
       before executing stages 2 and 4. However, it is possible to install
       &ganesha; later.
      </para>
      <para>
       In some cases it can be useful to define custom roles for &ganesha;
       nodes. For details, see <xref linkend="ceph.nfsganesha.customrole" />.
      </para>
      <para>
       To successfully install &ganesha;, additional configuration is
       required. If you want to use &ganesha;, read
       <xref linkend="cha.as.ganesha" />
       before executing stages 2 and 4. However, it is possible to install
       &ganesha; later.
      </para>
      <para>
       In some cases it can be useful to define custom roles for &ganesha;
       nodes. For details, see <xref linkend="ceph.nfsganesha.customrole" />.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Multiple Roles of Cluster Nodes</title>
     <para>
      You can assign several roles to a single node. For instance, you can
      assign to two monitor nodes also the mds role:
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy.common.configuration">
    <title>Common Configuration</title>
    <para>
     The common configuration section includes configuration files generated
     during the <emphasis>discovery (stage 1)</emphasis>. These configuration
     files store parameters like <literal>fsid</literal> or
     <literal>public_network</literal>. To include the required &ceph; common
     configuration, add the following lines:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="policy.profile.assignment">
    <title>Profile Assignment</title>
    <para>
     In &ceph;, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. Therefore,
     stage 1 will generate multiple profiles when possible for the same storage
     node. The administrator adds the cluster and stack related lines similar
     to the mon and igw roles.
    </para>
    <para>
     The profile names begin with <emphasis>profile</emphasis> and end with a
     single digit. The format is the following:
    </para>
<screen>profile-<replaceable>&lt;label&gt;</replaceable>-<replaceable>&lt;single digit&gt;</replaceable><replaceable>&lt;path to sls or yml files&gt;</replaceable></screen>
    <para>
     where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>&lt;label&gt;</replaceable> is dynamically generated based
       on quantity, model and size of the media, e.g. 2Disk2GB.
      </para>
      <para>
       To view the value, you can run:
      </para>
<screen>&prompt.smaster;salt '*' cephdisks.list</screen>
      <para>
       If the command outputs some results, inspect files in
       <filename>/srv/pillar/ceph/proposals/</filename>. The filenames contains
       the <replaceable>&lt;label&gt;</replaceable> values.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;single digit&gt;</replaceable> - defines the type of
       profile and reflects the count of medias attached to the minion. When
       <literal>1</literal> is specified, the media is treated like an
       individual OSD. When you specify <literal>2</literal> the node is with
       solid state media drive (SSD or NVMe) and the solid state media is
       considered as separate journals. Depending on the number of models and
       ratio of drives, additional profiles may be created by incrementing the
       digit.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;path to sls or yml files&gt;</replaceable> - replace it
       with a proper path to cluster sls files or to stack yml configuration
       files.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Now check the content of yml files in the
     <filename>stack/default/ceph/minions</filename> for the specific
     configuration. Then configure the profiles according to the following
     examples:
    </para>
    <para>
     A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
    </para>
<screen>profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml</screen>
    <para>
     A minion with two disks <emphasis>2Intel745GB</emphasis> and
     <emphasis>6INTEL372GB</emphasis>.
    </para>
<screen>profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml</screen>
    <para>
     You can add as many lines as needed to define each a profile for each
     storage node:
    </para>
<screen>profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml</screen>
   </sect3>
   <sect3 xml:id="deepsea.policy.filtering">
    <title>Item Filtering</title>
    <para>
     Sometimes it is not practical to include all files from a given directory
     with *.sls globbing. The <filename>policy.cfg</filename> file parser
     understands the following filters:
    </para>
    <warning>
     <title>Advanced Techniques</title>
     <para>
      This section describes filtering techniques for advanced users. When not
      used correctly, filtering can cause problems for example in case your
      node numbering changes.
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        Use the slice filter to only items <emphasis>start</emphasis> through
        <emphasis>end-1</emphasis>. Note that items in the given directory are
        sorted alphanumerically. The following line includes third to fifth
        file from the <filename>role-mon/cluster/</filename> subdirectory:
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        Use the regular expression filter to include only items matching the
        given expressions. For example:
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea.example.policy_cfg">
    <title>Example <filename>policy.cfg</filename> File</title>
    <para>
     Following is an example of a basic <filename>policy.cfg</filename> file:
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co.policy.1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co.policy.2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co.policy.3"/>

# MON
role-mon/stack/default/ceph/minions/ses-example-[123].yml <co
 xml:id="co.policy.4"/>
role-mon/cluster/ses-example-[123].sls <co xml:id="co.policy.5"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co.policy.6"/>

# IGW
role-igw/stack/default/ceph/minions/ses-example-4.yml <co xml:id="co.policy.7"/>
role-igw/cluster/ses-example-4.sls <co xml:id="co.policy.10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co.policy.11"/>

# COMMON
config/stack/default/global.yml <co xml:id="co.policy.8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co.policy.13"/>

## Profiles
profile-*-1/cluster/*.sls <co xml:id="co.policy.9"/>
profile-*-1/stack/default/ceph/minions/*.yml <co xml:id="co.policy.12"/></screen>
    <calloutlist>
     <callout arearefs="co.policy.1">
      <para>
       Indicates that all minions are included in the &ceph; cluster. If you
       have minions you do not want to include in the &ceph; cluster, use:
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       The first line marks all minions unassigned. The second line overrides
       minions matching 'ses-example-*.sls', and assigns them to the &ceph;
       cluster.
      </para>
     </callout>
     <callout arearefs="co.policy.4">
      <para>
       Ensures that &deepsea; knows the IP addresses of the monitor nodes.
      </para>
     </callout>
     <callout arearefs="co.policy.2">
      <para>
       The minion called 'examplesesadmin' has the 'master' role. This by the
       way means it will get admin keys to the cluster.
      </para>
     </callout>
     <callout arearefs="co.policy.3">
      <para>
       All minions matching 'sesclient*' will get admin keys as well.
      </para>
     </callout>
     <callout arearefs="co.policy.5">
      <para>
       All minions matching 'ses-example-[123]' (presumably three minions:
       ses-example-1, ses-example-2, and ses-example-3) will be set up as MON
       nodes.
      </para>
     </callout>
     <callout arearefs="co.policy.6">
      <para>
       Minion 'ses-example-4' will have the MDS role.
      </para>
     </callout>
     <callout arearefs="co.policy.10">
      <para>
       Minion 'ses-example-4' will have the IGW role.
      </para>
     </callout>
     <callout arearefs="co.policy.11">
      <para>
       Minion 'ses-example-4' will have the RGW role.
      </para>
     </callout>
     <callout arearefs="co.policy.7">
      <para>
       Ensures that &deepsea; knows the IP address of the IGW node.
      </para>
     </callout>
     <callout arearefs="co.policy.8">
      <para>
       Mean that we accept the default values for common configuration
       parameters such as <option>fsid</option> and
       <option>public_network</option>.
      </para>
     </callout>
     <callout arearefs="co.policy.13">
      <para>
       Mean that we accept the default values for common configuration
       parameters such as <option>fsid</option> and
       <option>public_network</option>.
      </para>
     </callout>
     <callout arearefs="co.policy.9">
      <para>
       We are telling &deepsea; to use the '*-1' hardware profile for each
       minion. Choosing the '*-1' hardware profile means that we want all
       additional disks (other than the root disk) act as OSDs.
      </para>
     </callout>
     <callout arearefs="co.policy.12">
      <para>
       We are telling &deepsea; to use the '*-1' hardware profile for each
       minion. Choosing the '*-1' hardware profile means that we want all
       additional disks (other than the root disk) act as OSDs.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="custom.configuration">
   <title>Customizing the Default Configuration</title>
   <para>
    You can change the default cluster configuration generated in the stage 2
    (refer to <xref linkend="deepsea.stage.description"/>). For example you
    need to change network settings, or software that is by default installed
    on your &smaster;. You can perform the former by modifying the pillar
    updated after the stage 2, while the latter is usually done by creating a
    custom <literal>sls</literal> file and adding it to the pillar. Details are
    described in following sections.
   </para>
   <sect3 xml:id="using.customized.files">
    <title>Using Customized Configuration Files</title>
    <para>
     This section lists several tasks that require adding/changing your own
     <literal>sls</literal> files. Such a procedure is typically used when you
     need to change the default deployment process.
    </para>
    <example>
     <title>Installing &oa; on a &sminion; Node</title>
     <para>
      &oa; is installed on the &smaster; by default. The following procedure
      shows how to install &oa; on a &sminion; node:
     </para>
     <procedure>
      <step>
       <para>
        Navigate to the &oa; directory:
       </para>
<screen>&prompt.smaster;cd /srv/salt/ceph/stage/openattic</screen>
      </step>
      <step>
       <para>
        Create your <filename>custom.sls</filename> by using
        <filename>default.sls</filename> as a template:
       </para>
<screen>&prompt.smaster;cp default.sls custom.sls</screen>
      </step>
      <step>
       <para>
        Edit the <filename>custom.sls</filename> file. Change the value of
       </para>
<screen>I@roles:master</screen>
       <para>
        to the desired &sminion;, <literal>oa.minion</literal> in our case:
       </para>
<screen>I@roles:oa.minion</screen>
       <note>
        <para>
         Bear in mind that the &sminion; target must match exactly the
         &sminion; name. If you use values like <emphasis>oa*</emphasis>, make
         sure to remove the line <literal>tgt_type: compound</literal> as
         &salt; defaults to globbing.
        </para>
       </note>
      </step>
      <step>
       <para>
        Edit the file
        <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> by adding
        the following line:
       </para>
<screen>stage_openattic: custom</screen>
      </step>
      <step>
       <para>
        Now run the stage 2 again, and then run stages 3 and 4.
       </para>
      </step>
     </procedure>
     <tip>
      <title>Enable &oa; Service</title>
      <para>
       After you finish a custom installation of &oa;, check that &oa; related
       services are enabled:
      </para>
<screen>&prompt.root;systemctl is-enabled openattic-systemd.service</screen>
      <para>
       If they are disabled, enable them with:
      </para>
<screen>&prompt.root;systemctl enable openattic-systemd.service</screen>
     </tip>
    </example>
    <sect4>
     <title>Disabling a Deployment Step</title>
     <para>
      If you address a specific task outside of the &deepsea; deployment
      process and therefore need to skip it, create a 'no-operation' file
      following this example:
     </para>
     <example>
      <title>Disabling Time Synchronization</title>
      <procedure>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/time/disabled.sls</filename> with the
         following content and save it:
        </para>
<screen>disable time setting:
 test.nop</screen>
       </step>
       <step>
        <para>
         Edit <filename>/srv/pillar/ceph/stack/global.yml</filename>, add the
         following line, and save it:
        </para>
<screen>time_init: disabled</screen>
       </step>
       <step>
        <para>
         Verify by refreshing the pillar and running the step:
        </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh
&prompt.smaster;salt 'admin.ceph' state.apply ceph.time
admin.ceph:
  Name: disable time setting - Function: test.nop - Result: Clean

Summary for admin.ceph
------------
Succeeded: 1
Failed:    0
------------
Total states run:     1</screen>
        <note>
         <title>Unique ID</title>
         <para>
          The task ID 'disable time setting' may be any message unique within
          an <literal>sls</literal> file. Prevent ID collisions by specifying
          unique descriptions.
         </para>
        </note>
       </step>
      </procedure>
     </example>
    </sect4>
    <sect4 xml:id="deepsea.replacing_step">
     <title>Replacing a Deployment Step</title>
     <para>
      If you need to replace the default behavior of a specific step with a
      custom one, create a custom <literal>sls</literal> file with a
      replacement content.
     </para>
     <example>
      <title>Replacing the <emphasis>demo</emphasis> rbd Image Two Custom rbd Images</title>
      <para>
       By default <filename>/srv/salt/ceph/pool/default.sls</filename> creates
       an rbd image called 'demo'. In our example, we do not want this image to
       be created, but we need two images: 'archive1' and 'archive2'.
      </para>
      <procedure>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/pool/custom.sls</filename> with the
         following content and save it:
        </para>
<screen>wait:
  module.run:
    - name: wait.out
    - kwargs:
        'status': "HEALTH_ERR"<co xml:id="co.deepsea.replace.wait"/>
    - fire_event: True

archive1:
  cmd.run:
    - name: "rbd -p rbd create archive1 --size=1024"<co xml:id="co.deepsea.replace.rbd"/>
    - unless: "rbd -p rbd ls | grep -q archive1$"
    - fire_event: True

archive2:
  cmd.run:
    - name: "rbd -p rbd create archive2 --size=768"
    - unless: "rbd -p rbd ls | grep -q archive2$"
    - fire_event: True</screen>
        <calloutlist>
         <callout arearefs="co.deepsea.replace.wait">
          <para>
           The <emphasis role="bold">wait</emphasis> module will pause until
           the &ceph; cluster does not have a status of
           <literal>HEALTH_ERR</literal>. In fresh installations, a &ceph;
           cluster may have this status until a sufficient number of OSDs
           become available and creation of pools has completed.
          </para>
         </callout>
         <callout arearefs="co.deepsea.replace.rbd">
          <para>
           The <command>rbd</command> command is not idempotent. If the same
           creation command is rerun after the image exists, the salt state
           will fail. The <emphasis role="bold">unless</emphasis> statement
           prevents this.
          </para>
         </callout>
        </calloutlist>
       </step>
       <step>
        <para>
         To call the newly created custom file instead of the default, you need
         to edit <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>,
         add the following line, and save it:
        </para>
<screen>pool_init: custom</screen>
       </step>
       <step>
        <para>
         Verify by refreshing the pillar and running the step:
        </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh
&prompt.smaster;salt 'admin.ceph' state.apply ceph.pool</screen>
       </step>
      </procedure>
      <note>
       <title>Authorization</title>
       <para>
        The creation of pools or images requires sufficient authorization. The
        <literal>admin.ceph</literal> minion has an admin keyring.
       </para>
      </note>
      <tip>
       <title>Alternative Way</title>
       <para>
        Another option is to change the variable in
        <filename>/srv/pillar/ceph/stack/ceph/roles/master.yml</filename>
        instead. Using this file will reduce the clutter of pillar data for
        other minions.
       </para>
      </tip>
     </example>
    </sect4>
    <sect4>
     <title>Modifying a Deployment Step</title>
     <para>
      Sometimes you may need a specific step to do some additional tasks. We do
      not recommend to modify the related state file as it may complicate a
      future upgrade. Instead, create a separate file to carry out the
      additional tasks identical to what was described in
      <xref
       linkend="deepsea.replacing_step"/>.
     </para>
     <example>
      <title>Creating Additional Two rbd Images</title>
      <para>
       Name the new <literal>sls</literal> file descriptive. For example, if
       you need to create two rbd images in addition to the demo image, name
       the file <filename>archive.sls</filename>.
      </para>
      <procedure>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/pool/custom.sls</filename> with the
         following content and save it:
        </para>
<screen>include:
 - .archive
 - .default</screen>
        <tip>
         <title>Include Precedence</title>
         <para>
          In this example, &salt; will create the <emphasis>archive</emphasis>
          images and then create the <emphasis>demo</emphasis> image. The order
          does not matter in this example. To change the order, reverse the
          lines after the <literal>include:</literal> directive.
         </para>
         <para>
          You can add the include line directly to
          <filename>archive.sls</filename> and all the images will get created
          as well. However, regardless of where the include line is placed,
          &salt; processes the steps in the included file first. Although this
          behavior can be overridden with <emphasis>requires</emphasis> and
          <emphasis>order</emphasis> statements, a separate file that includes
          the others guarantees the order and reduces the chances of confusion.
         </para>
        </tip>
       </step>
       <step>
        <para>
         Edit <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>, add
         the following line, and save it:
        </para>
<screen>pool_init: custom</screen>
       </step>
       <step>
        <para>
         Verify by refreshing the pillar and running the step:
        </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh
&prompt.smaster;salt 'admin.ceph' state.apply ceph.pool</screen>
       </step>
      </procedure>
     </example>
    </sect4>
    <sect4>
     <title>Modifying a Deployment Stage</title>
     <para>
      If you need to add a completely separate deployment step, create three
      new files&mdash;an <literal>sls</literal> file that performs the command,
      an orchestration file, and a custom file which aligns the new step with
      the original deployment steps.
     </para>
     <para>
      For example, you need to to run <command>logrotate</command> on all
      minions as part of the preparation stage:
     </para>
     <example>
      <title>Running <command>logrotate</command> on all &sminion;s</title>
      <para>
       First create an <literal>sls</literal> file and include the
       <command>logrotate</command> command.
      </para>
      <procedure>
       <step>
        <para>
         Create a directory such as
         <filename>/srv/salt/ceph/logrotate</filename>.
        </para>
       </step>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/logrotate/init.sls</filename> with the
         following content and save it:
        </para>
<screen>rotate logs:
  cmd.run:
    - name: "/usr/sbin/logrotate /etc/logrotate.conf"</screen>
       </step>
       <step>
        <para>
         Verify that the command works on a minion:
        </para>
<screen>&prompt.smaster;salt 'admin.ceph' state.apply ceph.logrotate</screen>
       </step>
      </procedure>
      <para>
       Because the orchestration file needs to run before all other preparation
       steps, add it to the <emphasis>Prep</emphasis> stage 0:
      </para>
      <procedure>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/stage/prep/logrotate.sls</filename>
         with the following content and save it:
        </para>
<screen>logrotate:
  salt.state:
    - tgt: '*'
    - sls: ceph.logrotate</screen>
       </step>
       <step>
        <para>
         Verify that the orchestration file works:
        </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.prep.logrotate</screen>
       </step>
      </procedure>
      <para>
       The last file is the custom one which includes the additional step with
       the original steps:
      </para>
      <procedure>
       <step>
        <para>
         Create <filename>/srv/salt/ceph/stage/prep/custom.sls</filename> with
         the following content and save it:
        </para>
<screen>include:
  - .logrotate
  - .master
  - .minion</screen>
       </step>
       <step>
        <para>
         Override the default behavior. Edit
         <filename>/srv/pillar/ceph/stack/global.yml</filename>, add the
         following line, and save the file:
        </para>
<screen>stage_prep: custom</screen>
       </step>
       <step>
        <para>
         Verify that Stage 0 works:
        </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
       </step>
      </procedure>
      <note>
       <title>Why <filename>global.yml</filename>?</title>
       <para>
        The <filename>global.yml</filename> file is chosen over the
        <filename>cluster.yml</filename> because during the
        <emphasis>prep</emphasis> stage, no minion belongs to the &ceph;
        cluster and has no access to any settings in
        <filename>cluster.yml</filename>.
       </para>
      </note>
     </example>
    </sect4>
   </sect3>
   <sect3 xml:id="discovered.configuration.modification">
    <title>Modifying Discovered Configuration</title>
    <para>
     After you completed stage 2, you may want to change the discovered
     configuration. To view the current settings, run:
    </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    <para>
     The output of default configuration for a single minion is usually similar
     to the following:
    </para>
<screen>----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</screen>
    <para>
     The above mentioned settings are distributed into several configuration
     files. The directory structure with these files is defined in the
     <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The
     following files usually describe your cluster:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/global.yml</filename> - the file
       affects all minions in the &salt; cluster.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename>
       - the file affects all minions in the &ceph; cluster called
       <literal>ceph</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename>
       - affects all minions that are assigned the specific role in the
       <literal>ceph</literal> cluster.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion
       ID</replaceable>/yml</filename> - affects the individual minion.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Overwriting Directories with Default Values</title>
     <para>
      There is a parallel directory tree that stores default configuration
      setup in <filename>/srv/pillar/ceph/stack/default</filename>. Do not
      change values here, as they are overwritten.
     </para>
    </note>
    <para>
     The typical procedure of changing the collected configuration is the
     following:
    </para>
    <procedure>
     <step>
      <para>
       Find the location of the configuration item you need to change. For
       example, if you need to change cluster related thing like cluster
       network, edit the file
       <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
      </para>
     </step>
     <step>
      <para>
       Save the file.
      </para>
     </step>
     <step>
      <para>
       Verify the changes by running:
      </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
      <para>
       and then
      </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
</chapter>
