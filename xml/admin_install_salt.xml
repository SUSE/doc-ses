<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &deepsea; and &salt;</title>
 <para>
  &salt; along with &deepsea; is a <emphasis>stack</emphasis> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with &salt;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
    deployment. Dedicate all the host resources to the &smaster; services. Do
    not install &ceph; on the host where you want to run &smaster;.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
    the &ceph; environment, &sminion; is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    &sminion;s need to correctly resolve the &smaster;'s host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name. Therefore, we recommend to set the &smaster;'s host name to
    <systemitem>salt</systemitem>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="deepsea.description">
  <title>Introduction to &deepsea;</title>

  <para>
   The goal of &deepsea; is to save the administrator time and confidently
   perform complex operations on a Ceph cluster. This idea has driven a few
   choices. Before presenting those choices, some observations are necessary.
  </para>

  <para>
   All software has configuration. Sometimes the default is sufficient. This is
   not the case with &ceph;. &ceph; is flexible almost to a fault. Reducing
   this complexity would force administrators into preconceived configurations.
   Several of the existing &ceph; solutions for an installation create a
   demonstration cluster of three nodes. However, the most interesting features
   of &ceph; require more.
  </para>

  <para>
   One aspect of configuration management tools is accessing the data such as
   addresses and device names of the individual servers. For a distributed
   storage system such as &ceph;, that aggregate can run into the hundreds.
   Collecting the information and entering the data manually into a
   configuration management tool is prohibitive and error prone.
  </para>

  <para>
   The steps necessary to provision the servers, collect the configuration,
   configure and deploy &ceph; are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </para>

    <para>
     With these observations in mind, &deepsea; addresses them with the following
   strategy. &deepsea; Consolidates the administrators decisions in a single location. The
     decisions revolve around cluster assignment, role assignment and profile
     assignment. And &deepsea; collects each set of tasks into a simple goal. Each goal is a Stage:
    </para>
    
     <itemizedlist>
      <listitem>
       <para>
        <emphasis role="bold">Stage 0</emphasis>&mdash;the
        <emphasis role="bold">provisioning</emphasis>&mdash; this stage is
        optional as many sites provides their own provisioning of servers. If
        you do not have your provisioning tool, you should run this stage.
        During this stage all required updates are applied and your system may
        be rebooted.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 1</emphasis>&mdash;the
        <emphasis role="bold">discovery</emphasis>&mdash; here you detect all
        hardware in your cluster and collect necessary information for the
        &ceph; configuration. For details about configuration refer to
        <xref linkend="deepsea.pillar.salt.configuration"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 2</emphasis>&mdash;the
        <emphasis role="bold">configuration</emphasis>&mdash; you need to
        prepare configuration data in a particular format.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 3</emphasis>&mdash;the
        <emphasis role="bold">deployment</emphasis>&mdash; creates a basic
        &ceph; cluster with OSD and monitors.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 4</emphasis>&mdash;the
        <emphasis role="bold">services</emphasis>&mdash; additional features of
        &ceph; like iSCSI, RadosGW and CephFS can be installed in this stage.
        Each is optional.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 5</emphasis>&mdash;the removal stage. This
        stage is not mandatory and during the initial setup it is usually not
        needed. In this stage the roles of minions and also the cluster
        configuration are removed. This stage is usually need when you need to
        remove a storage node from your cluster, for details refer to
        <xref linkend="salt.node.removing"/>.
       </para>
      </listitem>
     </itemizedlist>
  
   

  <sect2 xml:id="deepsea.organisation.locations">
   <title>Organization and Important Locations</title>
   <para>
    &salt; has several standard locations and several naming conventions used
    on your master node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       The directory stores configuration data for your cluster minions.
       <emphasis>Pillar</emphasis> is an interface for providing global
       configuration values to all your cluster minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       The directory stores &salt; state files (also called
       <emphasis>sls</emphasis> files). State files are formatted description
       of states in which the cluster should be. For more refer to the
       <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">&salt;
       documentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts known as runners. Runners are
       executed on the master node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       The directory is used by &deepsea;. Collected configuration data are
       stored there.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       Is a directory used by &deepsea;. The directory stores aggregated
       configuration data that are ready to use by various salt commands. The
       directory stores sls files that can be in different format, but each
       subdirectory contains sls files in the same format. For example,
       <filename>/srv/salt/ceph/stage</filename> contains orchestration files
       that are executed by the <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.stack">
  <title>Deploying with &deepsea; and &salt;</title>

  <para>
   The cluster deployment process by using &salt; has several phases. First,
   you need to prepare all nodes of the cluster and then you deploy and
   configure &ceph;.
  </para>

  <para>
   The following procedure describes the cluster preparation in detail.
  </para>

  <procedure>
   <step>
    <para>
     Install and register &cephos; together with &storage; 4 extension on each
     node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the node which you will used as the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
    <para>
     The command installs also the <literal>salt-master</literal> package.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all cluster nodes
     including the &smaster; node.
    </para>
<screen>&prompt.root;zypper in salt-minion</screen>
   </step>
   <step>
   <para>
   Configure all minions  to connect to the master. If your &salt; master is not reachable by the DNS name <literal>salt</literal>, edit the file <filename>/etc/salt/minion</filename> or create a new file <filename>/etc/salt/minion.d/master.conf</filename> with the following:
   </para>
   <screen>master:<replaceable>&lt;DNS name of you &salt; master&gt;</replaceable></screen>
   </step>
   <step>
   <para>
   If you performed changes to files mentioned in the previous step, restart the &salt; service on all minions:
   </para>
   <screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the &salt; State file:
     <filename>/srv/pillar/ceph/master_minion.sls</filename> points to your
     &smaster;. If you used the default host name for your &smaster; -
     <emphasis>salt</emphasis> in the <emphasis>ses</emphasis> domain, then the
     file looks as follows:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
   <step>
   <para>
   Restart the &salt; service on the master node:
   </para>
   <screen>&prompt.smaster;systemctl restart salt-master.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   
   <step>
    <para>
     Ensure that you have access to &ceph; Jewel repositories.
    </para>
   </step>
  </procedure>

  <para>
   Now you deploy and configure &ceph;
  </para>

  <note>
   <title>&salt; Command Conventions</title>
   <para>
    There are two possibilities how to run <command>salt-run
    state.orch</command> - one is with <literal>stage.&lt;stage
    number&gt;</literal>, the other is with a name of the stage. Both notations
    have the same impact and it is fully up to your preferences which command
    will you use.
   </para>
  </note>

  <para>
   Unless specified otherwise, all steps are mandatory.
  </para>

  <procedure>
   <step>
    <para>
     Now optionally provision your cluster. You cam omit this step if you have
     your own provisioning server.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.prep</screen>
   </step>
   <step>
    <para>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <filename>/srv/pillar/ceph/proposals</filename>. The data are stored in
     the YAML format in sls or yml files.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     After the previous command finishes successfully, create a
     <filename>policy.cfg</filename> file in
     <filename>/srv/pillar/ceph/proposals</filename>. For details refer to
     <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     The configuration stage parses the <filename>policy.cfg</filename> file
     and merges the included files into their final form. Cluster and role
     related contents are placed in
     <filename>/srv/pillar/ceph/cluster</filename>, while &ceph; specific
     content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Run the following command <emphasis role="bold">twice</emphasis> to
     trigger the configuration stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.configure</screen>
    <para>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for all minions by running:
    </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    <note>
     <title>Overwriting Defaults</title>
     <para>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <xref linkend="custom.configuration"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Now you run the deploy stage. In this stage the pillar is validated and
     monitors and ODS daemons are created on the storage nodes. Run the
     following to start the stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.deploy
  		</screen>
    <para>
     The command may take several minutes. By default, &oa; will be installed on the master node. If you need to install &oa; on a different node, refer to <xref linkend="deepsea.pillar.salt.configuration"/>. If you don't want to install &oa; at all as it is not a mandatory part of &storage;, refer to <xref linkend="ceph.oa"/>.
     </para> 
     <para>If it fails, you have to fix the
     issue (rerunning of previous stages may be required). After the command
     succeeds, run the following to check the status:
    </para>
<screen>ceph -s</screen>
   </step>
   <step>
    <para>
     The last step of the &ceph; cluster deployment is the
     <emphasis>services</emphasis> stage. Here you instantiate any of the
     currently supported services: iSCSI, &cephfs;, &rgw; and &oa;. In this
     stage necessary pool, authorizing keyrings and starting services are
     created. To start the stage, run the following:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.services</screen>
    <para>
     Depending on the setup, the command may run several minutes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea.pillar.salt.configuration">
  <title>Configuration and Customization</title>

  <sect2 xml:id="policy.configuration">
   <title>The <filename>policy.cfg</filename> File</title>
   <para>
    The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    configuration file is used to determine functions of individual cluster
    nodes (which node acts as OSD, which is a monitoring node, etc.). The file
    then includes configuration for individual nodes.
   </para>
   <para>
    Currently the only way how to configure policy is by manually editing the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration
    file. The file is divided into four sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="policy.cluster.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.role.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.common.configuration" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.profile.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The order of the sections is arbitrary, but the content of included lines
    overwrites matching keys from the contents of previous lines.
   </para>
   <sect3 xml:id="policy.cluster.assignment">
    <title>Cluster Assignment</title>
    <para>
     In the <emphasis role="bold">cluster</emphasis> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <emphasis role="bold">ceph</emphasis> follows.
    </para>
    <para>
     To include <emphasis role="bold">all</emphasis> minions, add the following
     lines:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     To <emphasis role="bold">whilelist</emphasis> a particular minion:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls
  			</screen>
    <para>
     or a group of minions&mdash;you can shell glob matching:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s
     to <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy.role.assignment">
    <title>Role Assignment</title>
    <para>
     In the <emphasis role="bold"></emphasis> section you need to assign roles
     to your cluster nodes. The general pattern is the following:
     </para>
     <screen>role-<replaceable>&lt;role name&gt;</replaceable>/<replaceable>&lt;path&gt;</replaceable>/<replaceable>&lt;included files&gt;</replaceable></screen>
     <para>
      Where the items have the following meaning and values:
      </para>
      <itemizedlist>
      <listitem>
      	<para>
      	<replaceable>&lt;role name&gt;</replaceable>
     is any of the following: <emphasis>master, admin, mon, mds, igw</emphasis>
     or <emphasis>rgw</emphasis>. Detailed description see below.
      	</para>
      </listitem>
      <listitem>
      	<para>
      	<replaceable>&lt;path&gt;</replaceable> is a relative path to sls or yml files. Usually in case of sls files it is <filename>cluster</filename>, while yml files are located at <filename>stack/default/ceph/minions</filename>.
      	</para>
      </listitem>
      <listitem>
      	<para>
      	<replaceable>&lt;included files&gt;</replaceable> are the &salt; state files or YAML configuration files. Shell globing can be used for more specific matching.
      	</para>
      	
      </listitem>
      </itemizedlist>
      
    <para>
    An example for each role follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - the node has admin keyrings to all &ceph;
       clusters. Currently, only a single &ceph; cluster is supported. The <emphasis>master</emphasis> role is mandatory, always add a similar line like the following: 
      </para>
      <screen>role-master/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - the minion will have an admin keyring. You define the role as follows:
      </para>
      <screen>role-admin/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - the minion will provide the monitoring
       service to the &ceph; cluster. This role requires addresses of the
       assigned minions, thus you need to include the files from the
       <filename>stack</filename> directory on top of the sls files:
      </para>
<screen>role-mon/stack/default/ceph/minions/mon*.yml
role-mon/cluster/*.sls</screen>
<para>
The example assigns the monitoring role to a group of minions.
</para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - the minion will provide the metadata service to
       support &cephfs;.
      </para>
      <screen>role-mds/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - the minion will act as a iSCSI gateway. This
       role requires addresses of the assigned minions, thus you need to
       also include the files from the <filename>stack</filename> directory:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - the minion will act as a &rgw;:
      </para>
      <screen>role-rgw/cluster/*.sls</screen>
     </listitem>
    </itemizedlist>
    
    <note>
     <title>Multiple Roles of Cluster Nodes</title>
     <para>
      You can assign several roles to a single node. For instance, you can
      assign to two monitor nodes also the mds role:
     </para>
<screen>role-mds/cluster/mon[12]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy.common.configuration">
    <title>Common Configuration</title>
    <para>
     The common configuration section includes configuration files generated
     during the <emphasis>discovery (stage 1)</emphasis>. These configuration
     files store parameters like <literal>fsid</literal> or
     <literal>public_network</literal>. To include the required &ceph; common
     configuration, add the following lines:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>

   </sect3>
   <sect3 xml:id="policy.profile.assignment">
    <title>Profile Assignment</title>
    <para>
     In &ceph;, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. Therefore,
     stage 1 will generate multiple profiles when possible for the same storage
     node. The administrator adds the cluster and stack related lines similar
     to the mon and igw roles.
    </para>
    <para>
     The profile names begin with <emphasis>profile</emphasis> and end with a
     single digit. The format is the following:
    </para>
<screen>profile-<replaceable>&lt;label&gt;</replaceable>-<replaceable>&lt;single digit&gt;</replaceable><replaceable>&lt;path to sls or yml files&gt;</replaceable></screen>
    <para>
     where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>&lt;label&gt;</replaceable> is dynamically generated based on quantity,
       model and size of the media, e.g. 2Disk2GB.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;single digit&gt;</replaceable> - defines the type of profile and
       reflects the count of medias attached to the minion. When
       <literal>1</literal> is specified, the media is treated like an
       individual OSD. When you specify <literal>2</literal> the node is with
       solid state media drive (SSD or NVMe) and the solid state media is
       considered as separate journals. Depending on the number of models and
       ratio of drives, additional profiles may be created by incrementing the
       digit.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;path to sls or yml files&gt;</replaceable> - replace it with a proper
       path to cluster sls files or to stack yml configuration files.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Now check the content of yml files in the
     <filename>stack/default/ceph/minions</filename> for the specific
     configuration. Then configure the profiles according to the following
     examples:
    </para>
    <para>
     A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
    </para>
<screen>profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml</screen>
    <para>
     A minion with two disks <emphasis>2Intel745GB</emphasis> and
     <emphasis>6INTEL372GB</emphasis>.
    </para>
<screen>profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml</screen>
    <para>
     You can add as many lines as needed to define each a profile for each
     storage node:
    </para>
<screen>profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="custom.configuration">
   <title>Customizing the Default Configuration</title>
   <para>
    You can change the default configuration generated in the stage 2, for example you might need to change network settings or software that is by default installed on your &smaster;. The first mentioned task is performed by modifying the pillar updated after the stage 2, the later is usually done by creating a custom <literal>sls</literal> file and adding it to the pillar. Details are described in following sections.</para>
    
     <sect3 xml:id="using.customized.files">
      <title>Using Customized Configuration Files</title>
      <para>
      This section describes steps required to apply your own <literal>sls</literal> files. Such a procedure is typically used when you need to change the default deployment process, e.g. install additional software on a different node. Let's use installation of &oa; on a &sminion; node as an example, which is by default installed on the &smaster;:
      </para>
      <procedure>
       <title>Installing &oa; on a &sminion;</title>
       <step>
        <para>
        Navigate to the &oa; directory:
        </para>
        <screen>cd /srv/salt/ceph/stage/openattic</screen>
       </step>
       <step>
        <para>
         Create your <filename>custom.sls</filename>, e.g. by using the <filename>default.sls</filename> as a template:
        </para>
        <screen>cp default.sls custom.sls</screen>
       </step>
       <step>
        <para>
        Edit the <filename>custom.sls</filename> file. Change the value of 
        </para>
        <screen>I@roles:master</screen>
        <para>
        to the desired &sminion;, <literal>oa.minion</literal> in our case:
        </para>
        <screen>I@roles:oa.minion</screen>
        <note>
         <para>
          Bear in mind that the &sminion; target must match exactly the &sminion; name. If you use values like <emphasis>oa*</emphasis>, make sure to remove the line <literal>tgt_type: compound</literal> as &salt; defaults to globbing.
         </para>
        </note>
       </step>
       <step>
        <para>
        Edit the file <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> by adding the following line:
        </para>
        <screen>stage_openattic: custom</screen>
       </step>
       <step>
        <para>
        Now run the stage 2 again and then run stages 3 and 4.
        </para>
       </step>
      </procedure>
     </sect3>
     
        <sect3 xml:id="discovered.configuration.modification">
         <title>Modyfying Discovered Configuration</title>    
     <para>
     After you completed stage 2, you may want to change the discovered configuration. To view the current settings, run:
   </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
   <para>
    The output of default configuration for a single minion is usually similar
    to the following:
   </para>
<screen>----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</screen>
   <para>
    The above mentioned settings are distributed into several configuration
    files. The directory structure with these files is defined in the
    <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The
    following files usually describe your cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/global.yml</filename> - the file affects
      all minions in the &salt; cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename>
      - the file affects all minions in the &ceph; cluster called
      <literal>ceph</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename>
      - affects all minions that are assigned the specific role in the
      <literal>ceph</literal> cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion
      ID</replaceable>/yml</filename> - affects the individual minion.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Overwriting Directories with Default Values</title>
    <para>
     There is a parallel directory tree that stores default configuration setup
     in <filename>/srv/pillar/ceph/stack/default</filename>. Do not change
     values here, as they are overwritten.
    </para>
   </note>
   <para>
    The typical procedure of changing the collected configuration is the
    following:
   </para>
   <procedure>
    <step>
     <para>
      Find the location of the configuration item you need to change. For
      example, if you need to change cluster related thing like cluster
      network, edit the file
      <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
     </para>
    </step>
    <step>
     <para>
      Save the file.
     </para>
    </step>
    <step>
     <para>
      Verify the changes by running:
     </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
     <para>
      and then
     </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    </step>
   </procedure>
   </sect3>
  </sect2>
 </sect1>
</chapter>
