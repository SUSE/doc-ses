<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &deepsea; and &salt;</title>
 <para>
  &salt; along with &deepsea; is a <emphasis>stack</emphasis> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with &salt;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
    deployment. Dedicate all the host resources to the &smaster; services. Do
    not install &ceph; on the host where you want to run &smaster;.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
    the &ceph; environment, &sminion; is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    &sminion;s need to correctly resolve the &smaster;'s host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name. Therefore, we recommend to set the &smaster;'s host name to
    <systemitem>salt</systemitem>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="deepsea.description">
  <title>Introduction to &deepsea;</title>

  <para>
   The goal of &deepsea; is to save the administrator time and confidently
   perform complex operations on a Ceph cluster. This idea has driven a few
   choices. Before presenting those choices, some observations are necessary.
  </para>

  <para>
   All software has configuration. Sometimes the default is sufficient. This is
   not the case with &ceph;. &ceph; is flexible almost to a fault. Reducing
   this complexity would force administrators into preconceived configurations.
   Several of the existing &ceph; solutions for an installation create a
   demonstration cluster of three nodes. However, the most interesting features
   of &ceph; require more.
  </para>

  <para>
   One aspect of configuration management tools is accessing the data such as
   addresses and device names of the individual servers. For a distributed
   storage system such as &ceph;, that aggregate can run into the hundreds.
   Collecting the information and entering the data manually into a
   configuration management tool is prohibitive and error prone.
  </para>

  <para>
   The steps necessary to provision the servers, collect the configuration,
   configure and deploy &ceph; are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </para>

  <para>
   With these observations in mind, &deepsea; addresses them with the following
   strategy. &deepsea; consolidates the administrators decisions in a single
   location. The decisions revolve around cluster assignment, role assignment
   and profile assignment. And &deepsea; collects each set of tasks into a
   simple goal. Each goal is a Stage:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">Stage 0</emphasis>&mdash;the
     <emphasis role="bold">provisioning</emphasis>&mdash; this stage is
     optional as many sites provides their own provisioning of servers. If you
     do not have your provisioning tool, you should run this stage. During this
     stage all required updates are applied and your system may be rebooted.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 1</emphasis>&mdash;the
     <emphasis role="bold">discovery</emphasis>&mdash; here you detect all
     hardware in your cluster and collect necessary information for the &ceph;
     configuration. For details about configuration refer to
     <xref linkend="deepsea.pillar.salt.configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 2</emphasis>&mdash;the
     <emphasis role="bold">configuration</emphasis>&mdash; you need to prepare
     configuration data in a particular format.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 3</emphasis>&mdash;the
     <emphasis role="bold">deployment</emphasis>&mdash; creates a basic &ceph;
     cluster with OSD and monitors.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 4</emphasis>&mdash;the
     <emphasis role="bold">services</emphasis>&mdash; additional features of
     &ceph; like iSCSI, RadosGW and CephFS can be installed in this stage. Each
     is optional.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Stage 5</emphasis>&mdash;the removal stage. This
     stage is not mandatory and during the initial setup it is usually not
     needed. In this stage the roles of minions and also the cluster
     configuration are removed. This stage is usually need when you need to
     remove a storage node from your cluster, for details refer to
     <xref linkend="salt.node.removing"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="deepsea.organisation.locations">
   <title>Organization and Important Locations</title>
   <para>
    &salt; has several standard locations and several naming conventions used
    on your master node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       The directory stores configuration data for your cluster minions.
       <emphasis>Pillar</emphasis> is an interface for providing global
       configuration values to all your cluster minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       The directory stores &salt; state files (also called
       <emphasis>sls</emphasis> files). State files are formatted description
       of states in which the cluster should be. For more refer to the
       <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">&salt;
       documentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts known as runners. Runners are
       executed on the master node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       The directory is used by &deepsea;. Collected configuration data are
       stored there.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       Is a directory used by &deepsea;. The directory stores sls files that
       can be in different format, but each subdirectory contains sls files.
       Each subdirectory contains only one type of sls file. For example,
       <filename>/srv/salt/ceph/stage</filename> contains orchestration files
       that are executed by the <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.stack">
  <title>Deploying with &deepsea; and &salt;</title>

  <para>
   The cluster deployment process by using &salt; has several phases. First,
   you need to prepare all nodes of the cluster by configuring &salt; and then
   you deploy and configure &ceph;.
  </para>

  <para>
   The following procedure describes the cluster preparation in detail.
  </para>

  <procedure>
   <step>
    <para>
     Install and register &cephos; together with &storage; 4 extension on each
     node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the node which will be the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
    <para>
     The command installs the <literal>salt-master</literal> package as a
     dependence.
    </para>
    <para>
     Check that the <systemitem>salt-master</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.smaster;systemctl enable salt-master.service
&prompt.smaster;systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all cluster nodes
     including the &smaster; node.
    </para>
<screen>&prompt.sminion;zypper in salt-minion</screen>
    <para>
     Check that the <systemitem>salt-minion</systemitem> service is enabled and
     started, and enable and start it if needed:
    </para>
<screen>&prompt.sminion;systemctl enable salt-minion.service
&prompt.sminion;systemctl start salt-minion.service</screen>
   </step>
   <step>

   <para>
   Configure all minions to connect to the master. If your &smaster; is not
   reachable by the DNS name <literal>salt</literal>, edit the file
   <filename>/etc/salt/minion</filename> or create a new file
   <filename>/etc/salt/minion.d/master.conf</filename> with the following
   content:
   </para>
   <screen>master:<replaceable>DNS_name_of_your_salt_master</replaceable></screen>
   <para>
    If you performed any changes to the configuration files mentioned above, restart the &salt; service on all &sminion;s:
   </para>
   <screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Check that the &salt; State file:
     <filename>/srv/pillar/ceph/master_minion.sls</filename> points to your
     &smaster;. If you used the default host name for your &smaster; -
     <emphasis>salt</emphasis> in the <emphasis>ses</emphasis> domain, then the
     file looks as follows:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
   <step>
    <para>
     Restart the &salt; service on the master node:
    </para>
<screen>&prompt.smaster;systemctl restart salt-master.service</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Ensure that you have access to &ceph; Jewel repositories.
    </para>
   </step>
   <step>
    <para>
     Prior to deploying &storage; make sure that all disks that were used as
     OSD by previous clusters are empty without partitions. To ensure this, you
     have to manually zap all the disks:
    </para>
<screen>&prompt.cephuser;sudo ceph-disk zap <replaceable>device_name</replaceable></screen>
   </step>
  </procedure>

  <para>
   Now you deploy and configure &ceph;
  </para>

  <note>
   <title>&salt; Command Conventions</title>
   <para>
    There are two possibilities how to run <command>salt-run
    state.orch</command> - one is with <literal>stage.&lt;stage
    number&gt;</literal>, the other is with a name of the stage. Both notations
    have the same impact and it is fully up to your preferences which command
    will you use.
   </para>
  </note>

  <para>
   Unless specified otherwise, all steps are mandatory.
  </para>

  <procedure>
   <step>
    <para>
     Now optionally prepare your cluster. You cam omit this step if you have
     your own provisioning server.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.prep</screen>
   </step>
   <step>
    <para>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <filename>/srv/pillar/ceph/proposals</filename>. The data are stored in
     the YAML format in sls or yml files.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     After the previous command finishes successfully, create a
     <filename>policy.cfg</filename> file in
     <filename>/srv/pillar/ceph/proposals</filename>. For details refer to
     <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     The configuration stage parses the <filename>policy.cfg</filename> file
     and merges the included files into their final form. Cluster and role
     related contents are placed in
     <filename>/srv/pillar/ceph/cluster</filename>, while &ceph; specific
     content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Run the following command to trigger the configuration stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.configure</screen>
    <para>
     The configuration step may take several seconds. After the command
     finishes, you can view the pillar data for all minions by running:
    </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    <note>
     <title>Overwriting Defaults</title>
     <para>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <xref linkend="custom.configuration"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Now you run the deploy stage. In this stage, the pillar is validated and
     monitors and ODS daemons are started on the storage nodes. Run the
     following to start the stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.deploy
    </screen>
    <para>
     The command may take several minutes.
     If it fails, you have to fix the
     issue. Then run the previous stages again. After the command
     succeeds, run the following to check the status:
     <remark role="fixme">master or minion?</remark>
    </para>
<screen>&prompt.smaster;ceph -s</screen>
   </step>
   <step>
    <para>
     The last step of the &ceph; cluster deployment is the
     <emphasis>services</emphasis> stage. Here you instantiate any of the
     currently supported services: iSCSI, &cephfs;, &rgw; and &oa;. In this
     stage necessary pool, authorizing keyrings and starting services are
     created. To start the stage, run the following:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.services</screen>
    <para>
     Depending on the setup, the command may run several minutes. By default,
     &oa; will be installed on the master node. If you need to install &oa; on
     a different node, refer to
     <xref linkend="deepsea.pillar.salt.configuration"/>. If you don't want to
     install &oa; at all as it is not a mandatory part of &storage;, refer to
     <xref linkend="ceph.oa"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea.pillar.salt.configuration">
  <title>Configuration and Customization</title>

  <sect2 xml:id="policy.configuration">
   <title>The <filename>policy.cfg</filename> File</title>
   <para>
    The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    configuration file is used to determine functions of individual cluster
    nodes (which node acts as OSD, which is a monitoring node, etc.). The file
    then includes configuration for individual nodes.
   </para>
   <para>
    Currently the only way how to configure policy is by manually editing the
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration
    file. The file is divided into four sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="policy.cluster.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.role.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.common.configuration" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.profile.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The order of the sections is arbitrary, but the content of included lines
    overwrites matching keys from the contents of previous lines.
   </para>
   <sect3 xml:id="policy.cluster.assignment">
    <title>Cluster Assignment</title>
    <para>
     In the <emphasis role="bold">cluster</emphasis> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <emphasis role="bold">ceph</emphasis> follows.
    </para>
    <para>
     To include <emphasis role="bold">all</emphasis> minions, add the following
     lines:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     To <emphasis role="bold">whitelist</emphasis> a particular minion:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     or a group of minions&mdash;you can shell glob matching:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s
     to <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy.role.assignment">
    <title>Role Assignment</title>
    <para>
     In the <emphasis role="bold"></emphasis> section you need to assign roles
     to your cluster nodes. The general pattern is the following:
    </para>
<screen>role-<replaceable>&lt;role name&gt;</replaceable>/<replaceable>&lt;path&gt;</replaceable>/<replaceable>&lt;included files&gt;</replaceable></screen>
    <para>
     Where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>&lt;role name&gt;</replaceable> is any of the following:
       <emphasis>master, admin, mon, mds, igw</emphasis> or
       <emphasis>rgw</emphasis>. Detailed description see below.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;path&gt;</replaceable> is a relative path to sls or yml
       files. Usually in case of sls files it is <filename>cluster</filename>,
       while yml files are located at
       <filename>stack/default/ceph/minions</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;included files&gt;</replaceable> are the &salt; state
       files or YAML configuration files. Shell globing can be used for more
       specific matching.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     An example for each role follows:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - the node has admin keyrings to all &ceph;
       clusters. Currently, only a single &ceph; cluster is supported. The
       <emphasis>master</emphasis> role is mandatory, always add a similar line
       like the following:
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - the minion will have an admin keyring. You
       define the role as follows:
      </para>
<screen>role-master/custer/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - the minion will provide the monitoring
       service to the &ceph; cluster. This role requires addresses of the
       assigned minions, thus you need to include the files from the
       <filename>stack</filename> directory in addition to the sls files:
      </para>
<screen>role-mon/stack/default/ceph/minions/mon*.yml
role-mon/cluster/mon*.sls</screen>
      <para>
       The example assigns the monitoring role to a group of minions.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - the minion will provide the metadata service
       to support &cephfs;.
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - the minion will act as a iSCSI gateway. This
       role requires addresses of the assigned minions, thus you need to also
       include the files from the <filename>stack</filename> directory:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml
role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - the minion will act as a &rgw;:
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
    </itemizedlist>
    <note>
     <title>Multiple Roles of Cluster Nodes</title>
     <para>
      You can assign several roles to a single node. For instance, you can
      assign to two monitor nodes also the mds role:
     </para>
<screen>role-mds/cluster/mon[12]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy.common.configuration">
    <title>Common Configuration</title>
    <para>
     The common configuration section includes configuration files generated
     during the <emphasis>discovery (stage 1)</emphasis>. These configuration
     files store parameters like <literal>fsid</literal> or
     <literal>public_network</literal>. To include the required &ceph; common
     configuration, add the following lines:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="policy.profile.assignment">
    <title>Profile Assignment</title>
    <para>
     In &ceph;, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. Therefore,
     stage 1 will generate multiple profiles when possible for the same storage
     node. The administrator adds the cluster and stack related lines similar
     to the mon and igw roles.
    </para>
    <para>
     The profile names begin with <emphasis>profile</emphasis> and end with a
     single digit. The format is the following:
    </para>
<screen>profile-<replaceable>&lt;label&gt;</replaceable>-<replaceable>&lt;single digit&gt;</replaceable><replaceable>&lt;path to sls or yml files&gt;</replaceable></screen>
    <para>
     where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>&lt;label&gt;</replaceable> is dynamically generated based
       on quantity, model and size of the media, e.g. 2Disk2GB.
      </para>
      <para>
      To view the value, you can run:
      </para>
      <screen>&prompt.smaster;salt '*' cephdisks.list</screen>
      <para>
      If the command outputs some results, inspect files in <filename>/srv/pillar/ceph/proposals/</filename>. The filenames contains the  <replaceable>&lt;label&gt;</replaceable> values.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;single digit&gt;</replaceable> - defines the type of
       profile and reflects the count of medias attached to the minion. When
       <literal>1</literal> is specified, the media is treated like an
       individual OSD. When you specify <literal>2</literal> the node is with
       solid state media drive (SSD or NVMe) and the solid state media is
       considered as separate journals. Depending on the number of models and
       ratio of drives, additional profiles may be created by incrementing the
       digit.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>&lt;path to sls or yml files&gt;</replaceable> - replace it
       with a proper path to cluster sls files or to stack yml configuration
       files.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Now check the content of yml files in the
     <filename>stack/default/ceph/minions</filename> for the specific
     configuration. Then configure the profiles according to the following
     examples:
    </para>
    <para>
     A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
    </para>
<screen>profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml</screen>
    <para>
     A minion with two disks <emphasis>2Intel745GB</emphasis> and
     <emphasis>6INTEL372GB</emphasis>.
    </para>
<screen>profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml</screen>
    <para>
     You can add as many lines as needed to define each a profile for each
     storage node:
    </para>
<screen>profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="custom.configuration">
   <title>Customizing the Default Configuration</title>
   <para>
    You can change the default configuration generated in the stage 2, for
    example you might need to change network settings or software that is by
    default installed on your &smaster;. The first mentioned task is performed
    by modifying the pillar updated after the stage 2, the latter is usually
    done by creating a custom <literal>sls</literal> file and adding it to the
    pillar. Details are described in following sections.
   </para>

     <sect3 xml:id="using.customized.files">
      <title>Using Customized Configuration Files</title>
      <para>
      This section describes steps required to apply your own <literal>sls</literal> files. Such a procedure is typically used when you need to change the default deployment process, e.g. install additional software on a different node. Let's use installation of &oa; on a &sminion; node as an example, which is by default installed on the &smaster;:
      </para>
      <procedure>
       <title>Installing &oa; on a &sminion;</title>
       <step>
        <para>
        Navigate to the &oa; directory:
        </para>
        <screen>&prompt.smaster;cd /srv/salt/ceph/stage/openattic</screen>
       </step>
       <step>
        <para>
         Create your <filename>custom.sls</filename>, e.g. by using the <filename>default.sls</filename> as a template:
        </para>
        <screen>&prompt.smaster;cp default.sls custom.sls</screen>
       </step>
       <step>
        <para>
        Edit the <filename>custom.sls</filename> file. Change the value of
        </para>
        <screen>I@roles:master</screen>
        <para>
        to the desired &sminion;, <literal>oa.minion</literal> in our case:
        </para>
        <screen>I@roles:oa.minion</screen>
        <note>
         <para>
          Bear in mind that the &sminion; target must match exactly the &sminion; name. If you use values like <emphasis>oa*</emphasis>, make sure to remove the line <literal>tgt_type: compound</literal> as &salt; defaults to globbing.
         </para>
        </note>
       </step>
       <step>
        <para>
        Edit the file <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> by adding the following line:
        </para>
        <screen>stage_openattic: custom</screen>
       </step>
       <step>
        <para>
        Now run the stage 2 again and then run stages 3 and 4.
        </para>
       </step>
      </procedure>
      <tip>
       <title>Enable Custom &oa; Service</title>
       <para>
        After you finish a custom installation of &oa;, check that
        &oa; related services are enabled:
       </para>
<screen>&prompt.root;systemctl is-enabled openattic-rpcd.service openattic-systemd.service</screen>
       <para>
        If they are disabled, enable them with:
       </para>
<screen>&prompt.root;systemctl enable openattic-rpcd.service openattic-systemd.service</screen>
      </tip>
     </sect3>

        <sect3 xml:id="discovered.configuration.modification">
         <title>Modifying Discovered Configuration</title>
     <para>
     After you completed stage 2, you may want to change the discovered configuration. To view the current settings, run:
     </para>

<screen>&prompt.smaster;salt '*' pillar.items</screen>
    <para>
     The output of default configuration for a single minion is usually similar
     to the following:
    </para>
<screen>----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</screen>
    <para>
     The above mentioned settings are distributed into several configuration
     files. The directory structure with these files is defined in the
     <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The
     following files usually describe your cluster:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/global.yml</filename> - the file
       affects all minions in the &salt; cluster.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename>
       - the file affects all minions in the &ceph; cluster called
       <literal>ceph</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename>
       - affects all minions that are assigned the specific role in the
       <literal>ceph</literal> cluster.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion
       ID</replaceable>/yml</filename> - affects the individual minion.
      </para>
     </listitem>
    </itemizedlist>
    <note>
     <title>Overwriting Directories with Default Values</title>
     <para>
      There is a parallel directory tree that stores default configuration
      setup in <filename>/srv/pillar/ceph/stack/default</filename>. Do not
      change values here, as they are overwritten.
     </para>
    </note>
    <para>
     The typical procedure of changing the collected configuration is the
     following:
    </para>
    <procedure>
     <step>
      <para>
       Find the location of the configuration item you need to change. For
       example, if you need to change cluster related thing like cluster
       network, edit the file
       <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
      </para>
     </step>
     <step>
      <para>
       Save the file.
      </para>
     </step>
     <step>
      <para>
       Verify the changes by running:
      </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
      <para>
       and then
      </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
</chapter>
