<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &deepsea; and &salt;</title>
  	<para>
  	&salt; along with &deepsea; is a <emphasis>stack</emphasis> of components that help you deploy
   and manage server infrastructure. It is very scalable, fast, and relatively
   easy to get running. Read the following considerations before you start
   deploying the cluster with &salt;:
  	</para>
  	
  <itemizedlist>
   <listitem>
    <para>
     <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
     deployment. Dedicate all the host resources to the &smaster; services. Do
     not install &ceph; on the host where you want to run &smaster;.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
     the &ceph; environment, &sminion; is typically an OSD or monitor.
    </para>
   </listitem>
   <listitem>
    <para>
     &sminion;s need to correctly resolve the &smaster;'s host name over the
     network. By default, they look for the <systemitem>salt</systemitem> host
     name. Therefore, we recommend to set the &smaster;'s host name to
     <systemitem>salt</systemitem>.
    </para>
   </listitem>   
  </itemizedlist>
  <sect1 xml:id="deepsea.description">
  	<title>Introduction do &deepsea;</title>
  	<para>
  	The goal of &deepsea; is to save the administrator time and confidently perform complex operations on a Ceph cluster. This idea has driven a few choices. Before presenting those choices, some observations are necessary.
  	</para>
  	<para>All software has configuraton. Sometimes the default is sufficient. This is not the case with &ceph;. &ceph; is flexible almost to a fault. Reducing this complexity would force administrators into preconceived configurations. Several of the existing &ceph; solutions for an installation create a demonstration cluster of three nodes. However, the most interesting features of &ceph; require more.   	
  	</para>
  	<para>
  	One aspect of configuration management tools is accessing the data such as addresses and device names of the individual servers. For a distributed storage system such as &ceph;, that aggregate can run into the hundreds. Collecting the information and entering the data manually into a configuration management tool is prohibitive and error prone.
  	</para>
  	<para>
  	The steps necessary to provision the servers, collect the configuration, configure and deploy &ceph; are mostly the same. However, this does not address managing the separate functions. For day to day operations, the ability to trivially add hardware to a given function and remove it gracefully is a requirement.
  	</para>
  	<para>
  	With these observations in mind, &deepsea; addresses them with the following strategy:
  	</para>
  	<itemizedlist>
  		<listitem>
  		<para>
  		Collect each set of tasks into a simple goal. Each goal is a Stage:
  		<itemizedlist>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 0</emphasis>&mdash;the <emphasis role="bold">provisioning</emphasis>&mdash; this stage is optional as many sites provides their own provisioning of servers. If you do not have your provisioning tool, you should run this stage. During this stage all required updates are applied and your system may be rebooted.
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 1</emphasis>&mdash;the <emphasis role="bold">discovery</emphasis>&mdash; here you detect alle hardware in your cluster and collect necessary information for the &ceph; configuration. For details about configuration refer to <xref linkend="deepsea.pillar.salt.configuration"/>.
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 2</emphasis>&mdash;the <emphasis role="bold">configuration</emphasis>&mdash; you need to prepare configuration data in a particular format.  
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 3</emphasis>&mdash;the <emphasis role="bold">deployment</emphasis>&mdash; creates a basic &ceph; cluster with OSD and monitors.
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 4</emphasis>&mdash;the <emphasis role="bold">services</emphasis>&mdash; additional features of &ceph; like iSCSI, RadosGW and CephFS can be installed in this stage. Each is optional.  
  		</para>
  	</listitem>
  	<listitem>
  		<para>
  		<emphasis role="bold">Stage 5</emphasis>&mdash;the removal stage. This stage is not mandatory and during the initial setup it is usually not needed. In this stage the roles of minions and also the cluster configuration are removed. This stage is usually need when you need to remove a storage node from your cluster, for details refer to <xref linkend="salt.node.removing"/>.
  		</para>
  	</listitem>
  </itemizedlist> 
  		  		</para>
  		</listitem>
  		<listitem> 
  			<para>
  			Consolidate the administrators decisions in a single location. The decisions revolve around cluster assignment, role assignment and profile assignment.
  			</para>		
  		</listitem>
  	</itemizedlist>
  	<sect2 xml:id="deepsea.organisation.locations">
  		<title>Organisation and Important Locations</title>
  		<para>
  		&salt; has several standard locations and several naming conventions used on your master node: 
  		</para>
  		<variablelist>
  			<varlistentry>
  			<term>
  			<filename>/srv/pillar</filename>
  			</term>
  			<listitem>
  				<para>
  				The directory stores configuration data for your cluster minions. <emphasis>Pillar</emphasis> is an interface for providing global configuration values to all your cluster minions.
  				</para>
  			</listitem>
  			</varlistentry>
  			<varlistentry>
  			<term>
  				<filename>/srv/salt/</filename>
  			</term>
  			<listitem>
  				<para>
  				The directory stores &salt; state files (also called <emphasis>sls</emphasis> files). State files are formatted description of states in which the cluster should be. For more refer to the <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">&salt; documentation</link>.
  				</para>
  			</listitem>
  			</varlistentry>
  			<varlistentry>
  				<term>
  					<filename>/srv/module/runners</filename>
  				</term>
  				<listitem>
  					<para>
  					The directory stores python scripts known as runners. Runners are executed on the master node.
  					</para>
  				</listitem>
  			</varlistentry>
  			<varlistentry>
  				<term>
  				<filename>/srv/salt/_modules</filename>
  				</term>
  				<listitem>
  					<para>
  					The directory stores python scripts that are called modules. The modules are applied to all minions in your cluster. 
  					</para>
  				</listitem>
  			</varlistentry>
  			<varlistentry>
  				<term>
  				<filename>/srv/pillar/ceph</filename> 
  				</term>
  				<listitem>
  					<para>
  					The directory is used by &deepsea;. Collected configuration data are stored there.
  					</para>
  				</listitem>
  			</varlistentry>
			<varlistentry>
  				<term>
  				<filename>/srv/salt/ceph</filename>
  				</term>
  				<listitem>
  					<para>
  					Is a directory used by &deepsea;. The directory stores aggregated configuration data that are ready to use by various salt commands. The directory stores sls files that can be in different format, but each subdirectory contains sls files in the same format. For example, <filename>/srv/salt/ceph/stage</filename> contains orchestration files that are executed by the <command>salt-run state.orchestrate</command>.
  					</para>
  				</listitem>
  			</varlistentry>
  		</variablelist>
  	</sect2>
  </sect1> 
  <sect1 xml:id="ceph.install.stack">
  <title>Deploying with &deepsea; and &salt;</title>
  <para>
  The cluster deployment process by using &salt; has several phases. First, you need to prepare all nodes of the cluster and then you deploy and configure &ceph;.
  </para>
  <para>
  The following procedure describes the cluster preparation in detail.
  </para>
       
  <procedure>
  	<step>
  		<para>
     Install and register &cephos; together with &storage; 4 extension on each
     node of the cluster.
    		</para>
  	</step>
  	<step>
  		<para>
  		Install &deepsea; on the node which you will used as the &smaster;:
  		</para>
  		<screen>
  		&prompt.smaster;zypper in deepsea
  		</screen>
  		<para>
  		The command installs also the <literal>salt-master</literal> package.
  		</para>
  	</step>
  	<step>
  		<para>
  		Install the package <literal>salt-minion</literal> on all cluster nodes including the &smaster; node.
  		</para>
  		<screen>
  		&prompt.root;zypper in salt-minion
  		</screen>
  	</step>
  	<step>
  		<para>
  		Accept all salt keys on the &smaster;:
  		</para>
  		<screen>
  		&prompt.smaster;salt-key --accept-all
  		</screen>
  	</step>
  	<step>
  		<para>
  		Verify that the keys have been accepted:
  		</para>
  		<screen>
  		&prompt.smaster;salt-key --list-all
  		</screen>
  	</step>
  	<step>
  		<para>
  		Check that the &salt; State file: <filename>/srv/pillar/ceph/master_minion.sls</filename> points to your &smaster;. If you used the default hostname for your &smaster; - <emphasis>salt</emphasis> in the <emphasis>ses</emphasis> domain, then the file looks as follows
  		</para>
  		<screen>
  		master_minion: salt.ses
  		</screen>
  	</step>
  	<step>
  		<para>
  		Ensure that you have access to &ceph; Jewel repositories.
  		</para>
  	</step>
  </procedure>
  <para>
  Now you deploy and configure &ceph;
  </para>
  <note>
  	<title>&salt; Command Conventions</title>
  	<para>
  	There are two possibilities how to run  <command>&salt;-run state.orch</command> - one is with <literal>stage.&lt;stage number&gt;</literal>, the other is with a name of the stage. Both notations have the same impact and it is fully up to your preferences which command will you use.
  	</para>
  </note>
  <para>
  Unless specified otherwise, all steps are mandatory.
  </para>
  <procedure>
  	<step>
  		<para>
  		Now optionally provision your cluster. You cam omit this step if you have your own provisiong server.
  		</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.0
  		</screen>
  		<para>or</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.prep
  		</screen>
  	</step>
  	<step>
  		<para>
  		The discovery stage collects data from all minions and creates configuration fragments that are stored in the directory 
  		<filename>/srv/pillar/ceph/proposals</filename>. The data are stored in the YAML format in sls or yml files.
  		</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.1
  		</screen>
  		<para>or</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.discovery
  		</screen>
  	</step>
  	<step>
  		<para>
  		After the previous command finishes successfully, create a <filename>policy.cfg</filename> file in <filename>/srv/pillar/ceph/proposals</filename>. For details refer to <xref linkend="policy.configuration"/>.
  		</para>
  	</step>
  	<step>
  		<para>
  		The configuration stage parses the <filename>policy.cfg</filename> file and merges the included files into their final form. Cluster and role related contents are placed in <filename>/srv/pillar/ceph/cluster</filename>, while &ceph; specific content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
  		</para>
  		<para>
  		Run the following command <emphasis role="bold">twice</emphasis> to trigger the configuration stage:
  		</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.2
  		</screen>
  		<para>or</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.configure
  		</screen>
  		<para>
  		The configuration step may take several seconds. After the command finishes you can view the pillar data for all minions by running:
  		</para>
  		<screen>
  		&prompt.smaster;salt '*' pillar.items
  		</screen> 
  		<note>
  			<title>Overwriting Defaults</title>
  			<para>
  			As soon as the command finishes, you can view the default configuration and change it to suit your needs. For details refer to <xref linkend="custom.configuration"/>.
  			</para>
  		</note> 		
  	</step>
  	<step>
  		<para>
  		Now you run the deploy stage. In this stage the pillar is validated and monitors and ODS daemons are created on the storage nodes. Run the following to start the stage:
  		</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.3
  		</screen>
  		<para>or</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.deploy
  		</screen>
  		
  		<para>
  		The command may take several minutes. If it fails, you have to fix the issue (rerunning of previous stages may be required). After the command succeeds, run the following to check the status:
  		</para>
  		<screen>
  		ceph -s
  		</screen>
  		
  	</step>
  	<step>
  		<para>
  		The last step of the &ceph; cluster deployment is the <emphasis>services</emphasis> stage. Here you instantiate any of the currently supported services: iSCSI, &cephfs;, &rgw; and &oa;. In this stage necessary pool, authorizing keyrings and starting services are created. To start the stage, run the following:
  		</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.4
  		</screen>
  		<para>or</para>
  		<screen>
  		&prompt.smaster;salt-run state.orch ceph.stage.services
  		</screen>
  		<para>
  		Depending on the setup, the command may run several minutes.
  		</para>
  		
  	</step>
  
  </procedure>
  </sect1>
  <sect1 xml:id="deepsea.pillar.salt.configuration">
  	<title>Configuration</title>
  	<sect2 xml:id="policy.configuration">
  		<title>The <filename>policy.cfg</filename> File</title>
  		<para>
  		The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration file is used to determine functions of individual cluster nodes (which node acts as OSD, which is a monitoring node, etc.). The file then includes configuration for individual nodes. 
  		</para>
  		<para>
  		Currently the only way how to configure policy is by manually editing the you can configure policies only by manually editing <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration file. The file is divided into four sections:
  		</para>
  		<itemizedlist>
  			<listitem>
  				<para>
  				<xref linkend="policy.cluster.assignment" xrefstyle="select: title"/>.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<xref linkend="policy.role.assignment" xrefstyle="select: title"/>.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<xref linkend="policy.common.configuration" xrefstyle="select: title"/>.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<xref linkend="policy.profile.assignment" xrefstyle="select: title"/>.
  				</para>
  			</listitem>
  		</itemizedlist>
  		<para>
  		The order of the sections is arbitrary, but the content of included lines overwrites matching keys from the contents of previous lines.
  		</para>
  		<sect3 xml:id="policy.cluster.assignment">
  			<title>Cluster Assignment</title>
  			<para>
  			In the <emphasis role="bold">cluster</emphasis> section you select minions for your cluster. You can select all minions, or you can blacklist or whitelist minions. Examples for a cluster called <emphasis role="bold">ceph</emphasis> follows.
  			</para>
  			<para>
  			To include <emphasis role="bold">all</emphasis> minions, add the following lines:
  			</para>
  			<screen>
  			cluster-ceph/cluster/*.sls
  			</screen>
  			<para>
  			To <emphasis role="bold">whilelist</emphasis> a particular minion:
  			</para>
  			<screen>
  			cluster-ceph/cluster/abc.domain.sls
  			</screen>
  			<para>
  			or a group of minions&mdash;you can shell glob matching:
  			</para>
  			<screen>
  			cluster-ceph/cluster/mon*.sls
  			</screen>
  			<para>
  			To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s to <literal>unassigned</literal>:
  			</para>
  			<screen>
  			cluster-unassigned/cluster/client*.sls
  			</screen>
  		</sect3>
  		<sect3 xml:id="policy.role.assignment">
  			<title>Role Assignment</title>
  			<para>
  			In the <emphasis role="bold"></emphasis> section you need to assign roles to your cluster nodes. The general pattern is the following: <emphasis>role-&lt;role name&gt;</emphasis>, where the &lt;role name&gt; is any of the following: 
  			<emphasis>master, admin, mon, mds, igw</emphasis> or <emphasis>rgw</emphasis>. 
  			</para>
  			<itemizedlist>
  				<listitem>
  					<para>
  					<emphasis>master</emphasis> - the node has admin keyrings to all &ceph; clusters. Currently, only a single &ceph; cluster is supported.
  					</para>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>admin</emphasis> - the minion will have an admin keyring.
  					</para>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>mon</emphasis> - the minion will provide the monitoring service to the &ceph; cluster. This role requires addresses of the assigned minions, thus you need to include the files from the <filename>stack</filename> directory:
  					</para>
  					<screen>
  					role-mon/stack/default/ceph/minions/mon*.yml  					
  					</screen>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>mds</emphasis> - the minion will provide the metada service to support &cephfs;.
  					</para>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>igw</emphasis> - the minion will act as a  iSCSI gateway. This role requires addresses of the assigned minions, thus you need to include the files from the <filename>stack</filename> directory:
  					</para>
  					<screen>
  					role-igw/stack/default/ceph/minions/xyz.domain.yml
  					</screen>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>rgw</emphasis> - the minion will act as a &rgw;.
  					</para>
  				</listitem>
  			</itemizedlist>
  			<para>
  			For example, to assign monitoring roles to a group of minions, add the following lines:
  			</para>
  			<screen>
  			role-mon/cluster/mon*.sls
  			</screen>
  			<para>
  			Shell globing can be used for more specific matching.
  			</para>
  			<note>
  				<title>Multiple Roles of Cluster Nodes</title>
  				<para>
  				You can assign several roles to a single node. For instance, you can assign to two monitor nodes also the mds role:
  				</para>
  				<screen>
  				role-mds/cluster/mon[12]*.sls
  				</screen>
  			</note>  			
  			
  		</sect3>
  		<sect3 xml:id="policy.common.configuration">
  			<title>Common Configuration</title>
  			<para>
  			The common configuration section includes configuration files generated during the <emphasis>discovery (stage 1)</emphasis>. These configuration files store parameters like <literal>fsid</literal> or <literal>public_network</literal>. To include the required &ceph; common configuration, add the following lines:
  			</para>
  			<screen>
  			config/stack/default/global.yml
config/stack/default/ceph/cluster.yml  			
  			</screen>
  		</sect3>
  		<sect3 xml:id="policy.profile.assignment">
  			<title>Profile Assignment</title>
  			<para>In &ceph;, a single storage role would be insufficient to describe the many disk configurations available with the same hardware. Therefore, stage 1 will generate multiple profiles when possible for the same storage node. The administrator adds the cluster and stack related lines similar to the mon and igw roles.  			
  			</para>
  			<para>
  			The profile names begin with <emphasis>profile</emphasis> and end with a single digit. The format is the following:
  			</para>
  			<screen>
  			profile-<replaceable>label</replaceable>-<replaceable>single digit</replaceable><replaceable>path to sls or yml files</replaceable>
  			</screen>
  			<para>
  			where the items have the following meaning and values:
  			</para>
  			<itemizedlist>
  				<listitem>
  					<para>
  					<emphasis>label</emphasis> is dynamically generated based on quantity, model and size of the media, e.g. 2Disk2GB.
  					</para>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>single digit</emphasis> - defines the type of profile and reflects the count of medias attached to the minion. When <literal>1</literal> is specified, the media is treated like an individual OSD. When you specify <literal>2</literal> the node is with solid state media drive (SSD or NVMe) and the solid state media is considered as separate journals. Depending on the number of models and ratio of drives, additional profiles may be created by incrementing the digit. 
  					</para>
  				</listitem>
  				<listitem>
  					<para>
  					<emphasis>path to sls or yml files</emphasis> - replace is with a proper path to cluster sls files or to stack yml configuration files.
  					</para>
  				</listitem>
  			</itemizedlist>
  			<para>
  			Now check the content of yml files in the <filename>stack/default/ceph/minions</filename> for the specific configuration. Then configure the profiles according to the following examples:
  			</para>
  			<para>
  			A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
  			</para>
  			<screen>
  			
  			profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml
  			</screen>
  			<para>
  			A minion with two disks <emphasis>2Intel745GB</emphasis> and <emphasis>6INTEL372GB</emphasis>.
  			</para>
  			<screen>
  			profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml
  			</screen>
  			<para>
  			You can add as many lines as needed to define each a profile for each storage node:
  			</para>
  			<screen>
  			profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml
  			</screen>
  		</sect3>
  	</sect2> 
  	<sect2 xml:id="custom.configuration">
  		<title>Customizing the Default Configuration</title>
  		<para>
  		You can change the default configuration generated in the stage 2. The pillar is updated after the stage 2, thus you can view the current settings by running:
  		</para>
  		<screen>
  		&prompt.smaster;salt '*' pillar.items
  		</screen>
  		<para>
  		The output of default configuration for a single minion is usually similar to the following:
  		</para>
  		<screen>
  		----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp

  		</screen>
  		<para>
  		The above mentioned settings are distributed into several configuration files. The directory structure with these files is defined in the <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The following files usually describe your cluster:
  		</para>
  		<itemizedlist>
  			<listitem>
  				<para>
  				<filename>/srv/pillar/ceph/stack/global.yml</filename> - the file affects all minions in the &salt; cluster.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename> - the file affects all minions in the &ceph; cluster called <literal>ceph</literal>.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename> - affects all minions that are assigned the specific role in the <literal>ceph</literal> cluster.
  				</para>
  			</listitem>
  			<listitem>
  				<para>
  				<filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion ID</replaceable>/yml</filename> - affects the individual minion.
  				</para>
  			</listitem>
  		</itemizedlist>
  		<note>
  		<title>Overwriting Directories with Default Values</title>
  		<para>
  		There is a parallel directory tree that stores default configuration setup in <filename>/srv/pillar/ceph/stack/default</filename>. Do not change values here, as they are overwritten.
  		</para>
  		</note>
  		<para>
  		The typical procedure of changing the collected configuration is the following:
  		</para>
  		<procedure>
  			<step>
  				<para>
  				Find the location of the configuration item you need to change. For example, if you need to change cluster related thing like cluster network, edit the file <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
  				</para>
  			</step>
  			<step>
  				<para>
  				Save the file.
  				</para>
  			</step>
  			<step>
  				<para>
  				Verify the changes by running:
  				</para>
  				<screen>
  				&prompt.smaster;salt '*' saltutil.pillar_refresh
  				</screen>
  				<para>and then</para>
  				<screen>
  				&prompt.smaster;salt '*' pillar.items
  				</screen>
  			</step>
  		</procedure>
  		
  	</sect2> 	
  </sect1>
  
</chapter>
