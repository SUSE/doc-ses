<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.install.saltstack">
 <title>Deploying with &deepsea; and &salt;</title>
 <para>
  &salt; along with &deepsea; is a <emphasis>stack</emphasis> of components
  that help you deploy and manage server infrastructure. It is very scalable,
  fast, and relatively easy to get running. Read the following considerations
  before you start deploying the cluster with &salt;:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>&smaster;</emphasis> is the host that controls the whole cluster
    deployment. Dedicate all the host resources to the &smaster; services. Do
    not install &ceph; on the host where you want to run &smaster;.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>&sminion;s</emphasis> are the nodes controlled by &smaster;. In
    the &ceph; environment, &sminion; is typically an OSD or monitor.
   </para>
  </listitem>
  <listitem>
   <para>
    &sminion;s need to correctly resolve the &smaster;'s host name over the
    network. By default, they look for the <systemitem>salt</systemitem> host
    name. Therefore, we recommend to set the &smaster;'s host name to
    <systemitem>salt</systemitem>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="deepsea.description">
  <title>Introduction to &deepsea;</title>

  <para>
   The goal of &deepsea; is to save the administrator time and confidently
   perform complex operations on a Ceph cluster. This idea has driven a few
   choices. Before presenting those choices, some observations are necessary.
  </para>

  <para>
   All software has configuration. Sometimes the default is sufficient. This is
   not the case with &ceph;. &ceph; is flexible almost to a fault. Reducing
   this complexity would force administrators into preconceived configurations.
   Several of the existing &ceph; solutions for an installation create a
   demonstration cluster of three nodes. However, the most interesting features
   of &ceph; require more.
  </para>

  <para>
   One aspect of configuration management tools is accessing the data such as
   addresses and device names of the individual servers. For a distributed
   storage system such as &ceph;, that aggregate can run into the hundreds.
   Collecting the information and entering the data manually into a
   configuration management tool is prohibitive and error prone.
  </para>

  <para>
   The steps necessary to provision the servers, collect the configuration,
   configure and deploy &ceph; are mostly the same. However, this does not
   address managing the separate functions. For day to day operations, the
   ability to trivially add hardware to a given function and remove it
   gracefully is a requirement.
  </para>

    <para>
     With these observations in mind, &deepsea; addresses them with the following
   strategy. &deepsea; Consolidates the administrators decisions in a single location. The
     decisions revolve around cluster assignment, role assignment and profile
     assignment. And &deepsea; collects each set of tasks into a simple goal. Each goal is a Stage:
    </para>

     <itemizedlist>
      <listitem>
       <para>
        <emphasis role="bold">Stage 0</emphasis>&mdash;the
        <emphasis role="bold">provisioning</emphasis>&mdash; this stage is
        optional as many sites provides their own provisioning of servers. If
        you do not have your provisioning tool, you should run this stage.
        During this stage all required updates are applied and your system may
        be rebooted.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 1</emphasis>&mdash;the
        <emphasis role="bold">discovery</emphasis>&mdash; here you detect all
        hardware in your cluster and collect necessary information for the
        &ceph; configuration. For details about configuration refer to
        <xref linkend="deepsea.pillar.salt.configuration"/>.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 2</emphasis>&mdash;the
        <emphasis role="bold">configuration</emphasis>&mdash; you need to
        prepare configuration data in a particular format.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 3</emphasis>&mdash;the
        <emphasis role="bold">deployment</emphasis>&mdash; creates a basic
        &ceph; cluster with OSD and monitors.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 4</emphasis>&mdash;the
        <emphasis role="bold">services</emphasis>&mdash; additional features of
        &ceph; like iSCSI, RadosGW and CephFS can be installed in this stage.
        Each is optional.
       </para>
      </listitem>
      <listitem>
       <para>
        <emphasis role="bold">Stage 5</emphasis>&mdash;the removal stage. This
        stage is optional and during the initial setup it is usually not
        needed. In this stage the roles of minions and also the cluster
        configuration are removed. This stage can be useful when removing
        a storage node from your cluster, for details refer to
        <xref linkend="salt.node.removing"/>.
       </para>
      </listitem>
     </itemizedlist>



  <sect2 xml:id="deepsea.organisation.locations">
   <title>Organization and Important Locations</title>
   <para>
    &salt; has several standard locations and several naming conventions used
    on your master node:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename>
     </term>
     <listitem>
      <para>
       The directory stores configuration data for your cluster minions.
       <emphasis>Pillar</emphasis> is an interface for providing global
       configuration values to all your cluster minions.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename>
     </term>
     <listitem>
      <para>
       The directory stores &salt; state files (also called
       <emphasis>sls</emphasis> files). State files are formatted description
       of states in which the cluster should be. For more refer to the
       <link xlink:href="https://docs.saltstack.com/en/latest/topics/tutorials/starting_states.html">&salt;
       documentation</link>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts known as runners. Runners are
       executed on the master node.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename>
     </term>
     <listitem>
      <para>
       The directory stores python scripts that are called modules. The modules
       are applied to all minions in your cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename>
     </term>
     <listitem>
      <para>
       The directory is used by &deepsea;. Collected configuration data are
       stored there.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename>
     </term>
     <listitem>
      <para>
       Is a directory used by &deepsea;. The directory stores aggregated
       configuration data that are ready to use by various salt commands. The
       directory stores sls files that can be in different format, but each
       subdirectory contains sls files in the same format. For example,
       <filename>/srv/salt/ceph/stage</filename> contains orchestration files
       that are executed by the <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.install.stack">
  <title>Deploying with &deepsea; and &salt;</title>

  <para>
   The cluster deployment process by using &salt; has several phases. First,
   you need to prepare all nodes of the cluster and then you deploy and
   configure &ceph;.
  </para>

  <para>
   The following procedure describes the cluster preparation in detail.
  </para>

  <procedure>
   <step>
    <para>
     Install and register &cephos; together with &storage; 4 extension on each
     node of the cluster.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the node which you will used as the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
    <para>
     The command installs also the <literal>salt-master</literal> package.
    </para>
   </step>
   <step>
    <para>
     Install the package <literal>salt-minion</literal> on all cluster nodes
     including the &smaster; node.
    </para>
<screen>&prompt.root;zypper in salt-minion</screen>
   </step>
   <step>
    <para>
     Accept all salt keys on the &smaster;:
    </para>
<screen>&prompt.smaster;salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verify that the keys have been accepted:
    </para>
<screen>&prompt.smaster;salt-key --list-all</screen>
   </step>
   <step>
    <para>
     Check that the &salt; State file:
     <filename>/srv/pillar/ceph/master_minion.sls</filename> points to your
     &smaster;. If you used the default host name for your &smaster; -
     <emphasis>salt</emphasis> in the <emphasis>ses</emphasis> domain, then the
     file looks as follows
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
   <step>
    <para>
     Ensure that you have access to &ceph; Jewel repositories.
    </para>
   </step>
  </procedure>

  <para>
   Now you deploy and configure &ceph;
  </para>

  <note>
   <title>&salt; Command Conventions</title>
   <para>
    There are two possibilities how to run <command>&salt;-run
    state.orch</command> - one is with <literal>stage.&lt;stage
    number&gt;</literal>, the other is with a name of the stage. Both notations
    have the same impact and it is fully up to your preferences which command
    will you use.
   </para>
  </note>

  <para>
   Unless specified otherwise, all steps are mandatory.
  </para>

  <procedure>
   <step>
    <para>
     Now optionally provision your cluster. You cam omit this step if you have
     your own provisioning server.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.prep</screen>
   </step>
   <step>
    <para>
     The discovery stage collects data from all minions and creates
     configuration fragments that are stored in the directory
     <filename>/srv/pillar/ceph/proposals</filename>. The data are stored in
     the YAML format in sls or yml files.
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     After the previous command finishes successfully, create a
     <filename>policy.cfg</filename> file in
     <filename>/srv/pillar/ceph/proposals</filename>. For details refer to
     <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     The configuration stage parses the <filename>policy.cfg</filename> file
     and merges the included files into their final form. Cluster and role
     related contents are placed in
     <filename>/srv/pillar/ceph/cluster</filename>, while &ceph; specific
     content is placed in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Run the following command <emphasis role="bold">twice</emphasis> to
     trigger the configuration stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.configure</screen>
    <para>
     The configuration step may take several seconds. After the command
     finishes you can view the pillar data for all minions by running:
    </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    <note>
     <title>Overwriting Defaults</title>
     <para>
      As soon as the command finishes, you can view the default configuration
      and change it to suit your needs. For details refer to
      <xref linkend="custom.configuration"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Now you run the deploy stage. In this stage the pillar is validated and
     monitors and ODS daemons are created on the storage nodes. Run the
     following to start the stage:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.deploy
</screen>
    <para>
     The command may take several minutes. If it fails, you have to fix the
     issue (rerunning of previous stages may be required). After the command
     succeeds, run the following to check the status:
    </para>
<screen>ceph -s</screen>
   </step>
   <step>
    <para>
     The last step of the &ceph; cluster deployment is the
     <emphasis>services</emphasis> stage. Here you instantiate any of the
     currently supported services: iSCSI, &cephfs;, &rgw; and &oa;. In this
     stage necessary pool, authorizing keyrings and starting services are
     created. To start the stage, run the following:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <para>
     or
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.services</screen>
    <para>
     Depending on the setup, the command may run several minutes.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea.pillar.salt.configuration">
  <title>Configuration</title>

  <sect2 xml:id="policy.configuration">
   <title>The <filename>policy.cfg</filename> File</title>
   <para>
    The <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    configuration file is used to determine functions of individual cluster
    nodes (which node acts as OSD, which is a monitoring node, etc.). The file
    then includes configuration for individual nodes.
   </para>
   <para>
    Currently the only way how to configure policy is by manually editing the
    you can configure policies only by manually editing
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> configuration
    file. The file is divided into four sections:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <xref linkend="policy.cluster.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.role.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.common.configuration" xrefstyle="select: title"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="policy.profile.assignment" xrefstyle="select: title"/>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The order of the sections is arbitrary, but the content of included lines
    overwrites matching keys from the contents of previous lines.
   </para>
   <sect3 xml:id="policy.cluster.assignment">
    <title>Cluster Assignment</title>
    <para>
     In the <emphasis role="bold">cluster</emphasis> section you select minions
     for your cluster. You can select all minions, or you can blacklist or
     whitelist minions. Examples for a cluster called
     <emphasis role="bold">ceph</emphasis> follows.
    </para>
    <para>
     To include <emphasis role="bold">all</emphasis> minions, add the following
     lines:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     To <emphasis role="bold">whilelist</emphasis> a particular minion:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls
  			</screen>
    <para>
     or a group of minions&mdash;you can shell glob matching:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     To <emphasis role="bold">blacklist</emphasis> a minion/s, set the minion/s
     to <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy.role.assignment">
    <title>Role Assignment</title>
    <para>
     In the <emphasis role="bold"></emphasis> section you need to assign roles
     to your cluster nodes. The general pattern is the following:
     <emphasis>role-&lt;role name&gt;</emphasis>, where the &lt;role name&gt;
     is any of the following: <emphasis>master, admin, mon, mds, igw</emphasis>
     or <emphasis>rgw</emphasis>.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - the node has admin keyrings to all &ceph;
       clusters. Currently, only a single &ceph; cluster is supported.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - the minion will have an admin keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - the minion will provide the monitoring
       service to the &ceph; cluster. This role requires addresses of the
       assigned minions, thus you need to include the files from the
       <filename>stack</filename> directory:
      </para>
<screen>role-mon/stack/default/ceph/minions/mon*.yml</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - the minion will provide the metadata service to
       support &cephfs;.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - the minion will act as a iSCSI gateway. This
       role requires addresses of the assigned minions, thus you need to
       include the files from the <filename>stack</filename> directory:
      </para>
<screen>role-igw/stack/default/ceph/minions/xyz.domain.yml</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - the minion will act as a &rgw;.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     For example, to assign monitoring roles to a group of minions, add the
     following lines:
    </para>
<screen>role-mon/cluster/mon*.sls</screen>
    <para>
     Shell globing can be used for more specific matching.
    </para>
    <note>
     <title>Multiple Roles of Cluster Nodes</title>
     <para>
      You can assign several roles to a single node. For instance, you can
      assign to two monitor nodes also the mds role:
     </para>
<screen>role-mds/cluster/mon[12]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy.common.configuration">
    <title>Common Configuration</title>
    <para>
     The common configuration section includes configuration files generated
     during the <emphasis>discovery (stage 1)</emphasis>. These configuration
     files store parameters like <literal>fsid</literal> or
     <literal>public_network</literal>. To include the required &ceph; common
     configuration, add the following lines:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>

   </sect3>
   <sect3 xml:id="policy.profile.assignment">
    <title>Profile Assignment</title>
    <para>
     In &ceph;, a single storage role would be insufficient to describe the
     many disk configurations available with the same hardware. Therefore,
     stage 1 will generate multiple profiles when possible for the same storage
     node. The administrator adds the cluster and stack related lines similar
     to the mon and igw roles.
    </para>
    <para>
     The profile names begin with <emphasis>profile</emphasis> and end with a
     single digit. The format is the following:
    </para>
<screen>profile-<replaceable>label</replaceable>-<replaceable>single digit</replaceable><replaceable>path to sls or yml files</replaceable></screen>
    <para>
     where the items have the following meaning and values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>label</emphasis> is dynamically generated based on quantity,
       model and size of the media, e.g. 2Disk2GB.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>single digit</emphasis> - defines the type of profile and
       reflects the count of medias attached to the minion. When
       <literal>1</literal> is specified, the media is treated like an
       individual OSD. When you specify <literal>2</literal> the node is with
       solid state media drive (SSD or NVMe) and the solid state media is
       considered as separate journals. Depending on the number of models and
       ratio of drives, additional profiles may be created by incrementing the
       digit.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>path to sls or yml files</emphasis> - replace is with a proper
       path to cluster sls files or to stack yml configuration files.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Now check the content of yml files in the
     <filename>stack/default/ceph/minions</filename> for the specific
     configuration. Then configure the profiles according to the following
     examples:
    </para>
    <para>
     A minion with a single disk called <emphasis>3HP5588GB</emphasis>:
    </para>
<screen>profile-3HP5588-1/cluster/*.sls
profile-3HP5588-1/stack/default/ceph/minions/*.yml</screen>
    <para>
     A minion with two disks <emphasis>2Intel745GB</emphasis> and
     <emphasis>6INTEL372GB</emphasis>.
    </para>
<screen>profile-2Intel745GB-6INTEL372GB-2/cluster/*.sls
profile-2Intel745GB-6INTEL372GB-2/stack/default/ceph/minions/*.yml</screen>
    <para>
     You can add as many lines as needed to define each a profile for each
     storage node:
    </para>
<screen>profile-24HP5588-1/cluster/cold*.sls
profile-24HP5588-1/stack/default/ceph/minions/cold*.yml
profile-18HP5588-6INTEL372GB-2/cluster/data*.sls
profile-18HP5588-6INTEL372GB-2/stack/default/ceph/minions/data*.yml</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="custom.configuration">
   <title>Customizing the Default Configuration</title>
   <para>
    You can change the default configuration generated in the stage 2. The
    pillar is updated after the stage 2, thus you can view the current settings
    by running:
   </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
   <para>
    The output of default configuration for a single minion is usually similar
    to the following:
   </para>
<screen>----------
    available_roles:
        - admin
        - mon
        - storage
        - mds
        - igw
        - rgw
        - client-cephfs
        - client-radosgw
        - client-iscsi
        - mds-nfs
        - rgw-nfs
        - master
    cluster:
        ceph
    cluster_network:
        172.16.22.0/24
    fsid:
        e08ec63c-8268-3f04-bcdb-614921e94342
    master_minion:
        admin.ceph
    mon_host:
        - 172.16.21.13
        - 172.16.21.11
        - 172.16.21.12
    mon_initial_members:
        - mon3
        - mon1
        - mon2
    public_address:
        172.16.21.11
    public_network:
        172.16.21.0/24
    roles:
        - admin
        - mon
        - mds
    time_server:
        admin.ceph
    time_service:
        ntp</screen>
   <para>
    The above mentioned settings are distributed into several configuration
    files. The directory structure with these files is defined in the
    <filename>/srv/pillar/ceph/stack/stack.cfg</filename> directory. The
    following files usually describe your cluster:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/global.yml</filename> - the file affects
      all minions in the &salt; cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/cluster.yml</filename>
      - the file affects all minions in the &ceph; cluster called
      <literal>ceph</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>/roles/<replaceable>role</replaceable>.yml</filename>
      - affects all minions that are assigned the specific role in the
      <literal>ceph</literal> cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <filename>/srv/pillar/ceph/stack/<replaceable>ceph</replaceable>minions/<replaceable>minion
      ID</replaceable>/yml</filename> - affects the individual minion.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <title>Overwriting Directories with Default Values</title>
    <para>
     There is a parallel directory tree that stores default configuration setup
     in <filename>/srv/pillar/ceph/stack/default</filename>. Do not change
     values here, as they are overwritten.
    </para>
   </note>
   <para>
    The typical procedure of changing the collected configuration is the
    following:
   </para>
   <procedure>
    <step>
     <para>
      Find the location of the configuration item you need to change. For
      example, if you need to change cluster related thing like cluster
      network, edit the file
      <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>.
     </para>
    </step>
    <step>
     <para>
      Save the file.
     </para>
    </step>
    <step>
     <para>
      Verify the changes by running:
     </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
     <para>
      and then
     </para>
<screen>&prompt.smaster;salt '*' pillar.items</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
