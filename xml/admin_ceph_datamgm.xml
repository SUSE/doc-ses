<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.storage.datamgm">
 <title>Stored Data Management</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The CRUSH algorithm determines how to store and retrieve data by computing
  data storage locations. CRUSH empowers &ceph; clients to communicate with
  OSDs directly rather than through a centralized server or broker. With an
  algorithmically determined method of storing and retrieving data, &ceph;
  avoids a single point of failure, a performance bottleneck, and a physical
  limit to its scalability.
 </para>
 <para>
  CRUSH requires a map of your cluster, and uses the &crushmap; to
  pseudo-randomly store and retrieve data in OSDs with a uniform distribution
  of data across the cluster.
 </para>
 <para>
  CRUSH maps contain a list of OSDs, a list of 'buckets' for aggregating the
  devices into physical locations, and a list of rules that tell CRUSH how it
  should replicate data in a &ceph; cluster’s pools. By reflecting the
  underlying physical organization of the installation, CRUSH can model—and
  thereby address—potential sources of correlated device failures. Typical
  sources include physical proximity, a shared power source, and a shared
  network. By encoding this information into the cluster map, CRUSH placement
  policies can separate object replicas across different failure domains while
  still maintaining the desired distribution. For example, to address the
  possibility of concurrent failures, it may be desirable to ensure that data
  replicas are on devices using different shelves, racks, power supplies,
  controllers, and/or physical locations.
 </para>
 <para>
  After you deploy a &ceph; cluster, a default &crushmap; is generated. It is
  fine for your &ceph; sandbox environment. However, when you deploy a
  large-scale data cluster, you should give significant consideration to
  developing a custom &crushmap;, because it will help you manage your &ceph;
  cluster, improve performance and ensure data safety.
 </para>
 <para>
  For example, if an OSD goes down, a &crushmap; can help you locate the
  physical data center, room, row and rack of the host with the failed OSD in
  the event you need to use on-site support or replace hardware.
 </para>
 <para>
  Similarly, CRUSH may help you identify faults more quickly. For example, if
  all OSDs in a particular rack go down simultaneously, the fault may lie with
  a network switch or power to the rack or the network switch rather than the
  OSDs themselves.
 </para>
 <para>
  A custom &crushmap; can also help you identify the physical locations where
  &ceph; stores redundant copies of data when the placement group(s) associated
  with a failed host are in a degraded state.
 </para>
 <para>
  There are three main sections to a &crushmap;.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm.devices" xrefstyle="select: title"/> consist of any
    object storage device corresponding to a <systemitem>ceph-osd</systemitem>
    daemon.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.buckets" xrefstyle="select: title"/> consist of a
    hierarchical aggregation of storage locations (for example rows, racks,
    hosts, etc.) and their assigned weights.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm.rules" xrefstyle="select: title"/> consist of the
    manner of selecting buckets.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm.devices">
  <title>Devices</title>

  <para>
   To map placement groups to OSDs, a &crushmap; requires a list of OSD devices
   (the name of the OSD daemon). The list of devices appears first in the
   &crushmap;.
  </para>

<screen>#devices
device <replaceable>NUM</replaceable> osd.<replaceable>OSD_NAME</replaceable> class <replaceable>CLASS_NAME</replaceable></screen>

  <para>
   For example:
  </para>

<screen>
#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3class ssd
</screen>

  <para>
   As a general rule, an OSD daemon maps to a single disk.
  </para>
 </sect1>
 <sect1 xml:id="datamgm.buckets">
  <title>Buckets</title>

  <para>
   CRUSH maps contain a list of OSDs, which can be organized into 'buckets' for
   aggregating the devices into physical locations.
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        osd
       </para>
      </entry>
      <entry>
       <para>
        An OSD daemon (osd.1, osd.2, etc.).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        host
       </para>
      </entry>
      <entry>
       <para>
        A host name containing one or more OSDs.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        chassis
       </para>
      </entry>
      <entry>
       <para>
        Chassis of which the rack is composed.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        rack
       </para>
      </entry>
      <entry>
       <para>
        A computer rack. The default is <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        row
       </para>
      </entry>
      <entry>
       <para>
        A row in a series of racks.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        pdu
       </para>
      </entry>
      <entry>
       <para>
        Power distribution unit.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        pod
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        room
       </para>
      </entry>
      <entry>
       <para>
        A room containing racks and rows of hosts.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        datacenter
       </para>
      </entry>
      <entry>
       <para>
        A physical data center containing rooms.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        region
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        root
       </para>
      </entry>
      <entry>
       <para/>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    You can modify the existing types and create your own bucket types.
   </para>
  </tip>

  <para>
   &ceph;’s deployment tools generate a &crushmap; that contains a bucket for
   each host, and a root named 'default', which is useful for the default
   <literal>rbd</literal> pool. The remaining bucket types provide a means for
   storing information about the physical location of nodes/buckets, which
   makes cluster administration much easier when OSDs, hosts, or network
   hardware malfunction and the administrator needs access to physical
   hardware.
  </para>

  <para>
   A bucket has a type, a unique name (string), a unique ID expressed as a
   negative integer, a weight relative to the total capacity/capability of its
   item(s), the bucket algorithm ( <literal>straw2</literal> by default), and
   the hash (<literal>0</literal> by default, reflecting CRUSH Hash
   <literal>rjenkins1</literal>). A bucket may have one or more items. The
   items may consist of other buckets or OSDs. Items may have a weight that
   reflects the relative weight of the item.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   The following example illustrates how you can use buckets to aggregate a
   pool and physical locations like a data center, a room, a rack and a row.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm.rules">
  <title>Rule Sets</title>

  <para>
   CRUSH maps support the notion of 'CRUSH rules', which are the rules that
   determine data placement for a pool. For large clusters, you will likely
   create many pools where each pool may have its own CRUSH ruleset and rules.
   The default &crushmap; has a rule for the default root. If you want more
   roots and more rules, you need to create them later or they will be created
   automatically when new pools are created.
  </para>

  <note>
   <para>
    In most cases, you will not need to modify the default rules. When you
    create a new pool, its default ruleset is 0.
   </para>
  </note>

  <para>
   A rule takes the following form:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      An integer. Classifies a rule as belonging to a set of rules. Activated
      by setting the ruleset in a pool. This option is required. Default is
      <literal>0</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      A string. Describes a rule for either for 'replicated' or 'erasure' coded
      pool. This option is required. Default is <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      An integer. If a pool group makes fewer replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <literal>2</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      An integer. If a pool group makes more replicas than this number, CRUSH
      will NOT select this rule. This option is required. Default is
      <literal>10</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable></term>
    <listitem>
     <para>
      Takes a bucket specified by a name, and begins iterating down the tree.
      This option is required. For an explanation about iterating through the
      tree, see <xref linkend="datamgm.rules.step.iterate"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable></term>
    <listitem>
     <para>
      <replaceable>target</replaceable> can either be <literal>choose</literal>
      or <literal>chooseleaf</literal>. When set to <literal>choose</literal>,
      a number of buckets is selected. <literal>chooseleaf</literal> directly
      selects the OSDs (leaf nodes) from the sub-tree of each bucket in the set
      of buckets.
     </para>
     <para>
      <replaceable>mode</replaceable> can either be <literal>firstn</literal>
      or <literal>indep</literal>. See
      <xref linkend="datamgm.rules.step.mode"/>.
     </para>
     <para>
      Selects the number of buckets of the given type. Where N is the number of
      options available, if <replaceable>num</replaceable> &gt; 0 &amp;&amp;
      &lt; N, choose that many buckets; if <replaceable>num</replaceable> &lt;
      0, it means N - <replaceable>num</replaceable>; and, if
      <replaceable>num</replaceable> == 0, choose N buckets (all available).
      Follows <literal>step take</literal> or <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Outputs the current value and empties the stack. Typically used at the
      end of a rule, but may also be used to form different trees in the same
      rule. Follows <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="datamgm.rules.step.iterate">
   <title>Iterating Through the Node Tree</title>
   <para>
    The structure defined with the buckets can be viewed as a node tree.
    Buckets are nodes and OSDs are leafs in this tree.
   </para>
   <para>
    Rules in the &crushmap; define how OSDs are selected from this tree. A rule
    starts with a node and then iterates down the tree to return a set of OSDs.
    It is not possible to define which branch needs to be selected. Instead the
    CRUSH algorithm assures that the set of OSDs fulfills the replication
    requirements and evenly distributes the data.
   </para>
   <para>
    With <literal>step take</literal> <replaceable>bucket</replaceable> the
    iteration through the node tree begins at the given bucket (not bucket
    type). If OSDs from all branches in the tree are to be returned, the bucket
    must be the root bucket. Otherwise the following steps are only iterating
    through a sub-tree.
   </para>
   <para>
    After <literal>step take</literal> one or more <literal>step
    choose</literal> entries follow in the rule definition. Each <literal>step
    choose</literal> chooses a defined number of nodes (or branches) from the
    previously selected upper node.
   </para>
   <para>
    In the end the selected OSDs are returned with <literal>step
    emit</literal>.
   </para>
   <para>
    <literal>step chooseleaf</literal> is a convenience function that directly
    selects OSDs from branches of the given bucket.
   </para>
   <para>
    <xref linkend="datamgm.rules.step.iterate.figure"/> provides an example of
    how <literal>step</literal> is used to iterate through a tree. The orange
    arrows and numbers correspond to <literal>example1a</literal> and
    <literal>example1b</literal>, while blue corresponds to
    <literal>example2</literal> in the following rule definitions.
   </para>
   <figure xml:id="datamgm.rules.step.iterate.figure">
    <title>Example Tree</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm.rules.step.mode">
   <title>firstn and indep</title>
   <para>
    A CRUSH rule defines replacements for failed nodes or OSDs (see
    <xref linkend="datamgm.rules"/>). The keyword <literal>step</literal>
    requires either <literal>firstn</literal> or <literal>indep</literal> as
    parameter. Figure <xref linkend="datamgm.rules.step.mode.indep.figure"/>
    provides an example.
   </para>
   <para>
    <literal>firstn</literal> adds replacement nodes to the end of the list of
    active nodes. In case of a failed node, the following healthy nodes are
    shifted to the left to fill the gap of the failed node. This is the default
    and desired method for <emphasis>replicated pools</emphasis>, because a
    secondary node already has all data and therefore can take over the duties
    of the primary node immediately.
   </para>
   <para>
    <literal>indep</literal> selects fixed replacement nodes for each active
    node. The replacement of a failed node does not change the order of the
    remaining nodes. This is desired for <emphasis>erasure coded
    pools</emphasis>. In erasure coded pools the data stored on a node depends
    on its position in the node selection. When the order of nodes changes, all
    data on affected nodes needs to be relocated.
   </para>
   <figure xml:id="datamgm.rules.step.mode.indep.figure">
    <title>Node Replacement Methods</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op.crush">
  <title>&crushmap; Manipulation</title>

  <para>
   This section introduces ways to basic &crushmap; manipulation, such as
   editing a &crushmap;, changing &crushmap; parameters, and
   adding/moving/removing an OSD.
  </para>

  <sect2>
   <title>Editing a &crushmap;</title>
   <para>
    To edit an existing CRUSH map, do the following:
   </para>
   <procedure>
    <step>
     <para>
      Get a &crushmap;. To get the &crushmap; for your cluster, execute the
      following:
     </para>
<screen>&prompt.root;ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will output (<option>-o</option>) a compiled &crushmap; to the
      file name you specified. Since the &crushmap; is in a compiled form, you
      must decompile it first before you can edit it.
     </para>
    </step>
    <step>
     <para>
      Decompile a &crushmap;. To decompile a &crushmap;, execute the following:
     </para>
<screen>&prompt.cephuser;crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will decompile (<option>-d</option>) the compiled &crushmap;and
      output (<option>-o</option>) it to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      Edit at least one of Devices, Buckets and Rules parameters.
     </para>
    </step>
    <step>
     <para>
      Compile a &crushmap;. To compile a &crushmap;, execute the following:
     </para>
<screen>&prompt.cephuser;crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      &ceph; will store a compiled &crushmap;to the file name you specified.
     </para>
    </step>
    <step>
     <para>
      Set a &crushmap;. To set the &crushmap; for your cluster, execute the
      following:
     </para>
<screen>&prompt.root;ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      &ceph; will input the compiled &crushmap; of the file name you specified
      as the &crushmap; for the cluster.
     </para>
    </step>
   </procedure>
   <tip>
    <title>Use Versioning System</title>
    <para>
     Use a versioning system&mdash;such as git or svn&mdash;for the exported
     and modified &crushmap; files. It makes a possible rollback simple.
    </para>
   </tip>
   <tip>
    <title>Test the New &crushmap;</title>
    <para>
     Test the new adjusted &crushmap; using the <command>crushtool
     --test</command> command, and compare to the state before applying the new
     &crushmap;. You may find the following command switches useful:
     <option>--show-statistics</option>, <option>--show-mappings</option>,
     <option>--show-bad-mappings</option>, <option>--show-utilization</option>,
     <option>--show-utilization-all</option>,
     <option>--show-choose-tries</option>
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op.crush.addosd">
   <title>Add/Move an OSD</title>
   <para>
    To add or move an OSD in the &crushmap; of a running cluster, execute the
    following:
   </para>
<screen>&prompt.root;ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       An integer. The numeric ID of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>name</term>
     <listitem>
      <para>
       A string. The full name of the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       A double. The CRUSH weight for the OSD. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>root</term>
     <listitem>
      <para>
       A key/value pair. By default, the CRUSH hierarchy contains the pool
       default as its root. This option is required.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Key/value pairs. You may specify the OSD’s location in the CRUSH
       hierarchy.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following example adds <literal>osd.0</literal> to the hierarchy, or
    moves the OSD from a previous location.
   </para>
<screen>&prompt.root;ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op.crush.osdweight">
   <title>Difference between <command>ceph osd reweight</command> and <command>ceph osd crush reweight</command></title>
   <para>
    There are two similar commands that change the 'weight' of an &osd;.
    Context of their usage is different and may cause confusion.
   </para>
   <sect3>
    <title><command>ceph osd reweight</command></title>
    <para>
     Usage:
    </para>
<screen>&prompt.root;ceph osd reweight <replaceable>OSD_NAME</replaceable> <replaceable>NEW_WEIGHT</replaceable></screen>
    <para>
     <command>ceph osd reweight</command> sets an override weight on the &osd;.
     This value is in the range 0 to 1, and forces CRUSH to re-place of the
     data that would otherwise live on this drive. It does
     <emphasis role="bold">not</emphasis> change the weights assigned to the
     buckets above the OSD, and is a corrective measure in case the normal
     CRUSH distribution is not working out quite right. For example, if one of
     your OSDs is at 90% and the others are at 40%, you could reduce this
     weight to try and compensate for it.
    </para>
    <note>
     <title>OSD Weight is Temporary</title>
     <para>
      Note that <command>ceph osd reweight</command> is not a persistent
      setting. When an OSD gets marked out, its weight will be set to 0 and
      when it gets marked in again, the weight will be changed to 1.
     </para>
    </note>
   </sect3>
   <sect3>
    <title><command>ceph osd crush reweight</command></title>
    <para>
     Usage:
    </para>
<screen>&prompt.root;ceph osd crush reweight <replaceable>OSD_NAME</replaceable> <replaceable>NEW_WEIGHT</replaceable></screen>
    <para>
     <command>ceph osd crush reweight</command> sets the
     <emphasis role="bold">CRUSH</emphasis> weight of the OSD. This weight is
     an arbitrary value&mdash;generally the size of the disk in TB&mdash;and
     controls how much data the system tries to allocate to the OSD.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="op.crush.osdremove">
   <title>Remove an OSD</title>
   <para>
    To remove an OSD from the &crushmap; of a running cluster, execute the
    following:
   </para>
<screen>&prompt.root;ceph osd crush remove <replaceable>OSD_NAME</replaceable></screen>
  </sect2>

  <sect2 xml:id="op.crush.addbaucket">
   <title>Add a Bucket</title>
   <para>
    To add a bucket in the &crushmap; of a running cluster, execute the
    <command>ceph osd crush add-bucket</command> command:
   </para>
<screen>&prompt.root;ceph osd crush add-bucket <replaceable>BUCKET_NAME</replaceable> <replaceable>BUCKET_TYPE</replaceable></screen>
  </sect2>

  <sect2 xml:id="op.crush.movebucket">
   <title>Move a Bucket</title>
   <para>
    To move a bucket to a different location or position in the &crushmap;
    hierarchy, execute the following:
   </para>
<screen>&prompt.root;ceph osd crush move <replaceable>BUCKET_NAME</replaceable> <replaceable>BUCKET_TYPE</replaceable>=<replaceable>BUCKET_NAME</replaceable> [...]</screen>
   <para>
    For example:
   </para>
<screen>
&prompt.root;ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
</screen>
  </sect2>

  <sect2 xml:id="op.crush.rmbucket">
   <title>Remove a Bucket</title>
   <para>
    To remove a bucket from the &crushmap; hierarchy, execute the following:
   </para>
<screen>
&prompt.root;ceph osd crush remove <replaceable>BUCKET_NAME</replaceable>
</screen>
   <note>
    <title>Empty Bucket Only</title>
    <para>
     A bucket must be empty before removing it from the CRUSH hierarchy.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing">
  <title>Scrubbing</title>

  <para>
   In addition to making multiple copies of objects, &ceph; insures data
   integrity by <emphasis>scrubbing</emphasis> placement groups (find more
   information about placement groups in
   <xref linkend="storage.intro.structure.pg"/>). &ceph; scrubbing is analogous
   to running <command>fsck</command> on the object storage layer. For each
   placement group, &ceph; generates a catalog of all objects and compares each
   primary object and its replicas to ensure that no objects are missing or
   mismatched. Daily light scrubbing checks the object size and attributes,
   while weekly deep scrubbing reads the data and uses checksums to ensure data
   integrity.
  </para>

  <para>
   Scrubbing is important for maintaining data integrity, but it can reduce
   performance. You can adjust the following settings to increase or decrease
   scrubbing operations:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option></term>
    <listitem>
     <para>
      The maximum number of simultaneous scrub operations for a &osd;. Default
      is 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>, <option>osd scrub end hour</option></term>
    <listitem>
     <para>
      The hours of day (0 to 24) that define a time window when the scrubbing
      can happen. By default begins at 0 and ends at 24.
     </para>
     <important>
      <para>
       If the placement group’s scrub interval exceeds the <option>osd scrub
       max interval</option> setting, the scrub will happen no matter what time
       window you define for scrubbing.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option></term>
    <listitem>
     <para>
      Allows scrubs during recovery. Setting this to 'false' will disable
      scheduling new scrubs while there is an active recovery. Already running
      scrubs will continue. This option is useful for reducing load on busy
      clusters. Default is 'true'.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option></term>
    <listitem>
     <para>
      The maximum time in seconds before a scrub thread times out. Default is
      60.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option></term>
    <listitem>
     <para>
      The maximum time in seconds before a scrub finalize thread time out.
      Default is 60*10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option></term>
    <listitem>
     <para>
      The normalized maximum load. &ceph; will not scrub when the system load
      (as defined by the ratio of <literal>getloadavg()</literal> / number of
      <literal>online cpus</literal>) is higher than this number. Default is
      0.5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option></term>
    <listitem>
     <para>
      The minimal interval in seconds for scrubbing &osd; when the &ceph;
      cluster load is low. Default is 60*60*24 (once a day).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option></term>
    <listitem>
     <para>
      The maximum interval in seconds for scrubbing &osd; irrespective of
      cluster load. 7*60*60*24 (once a week).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option></term>
    <listitem>
     <para>
      The minimal number of object store chunks to scrub during single
      operation. &ceph; blocks writes to a single chunk during scrub. Default
      is 5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option></term>
    <listitem>
     <para>
      The maximum number of object store chunks to scrub during single
      operation. Default is 25.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option></term>
    <listitem>
     <para>
      Time to sleep before scrubbing next group of chunks. Increasing this
      value slows down the whole scrub operation while client operations are
      less impacted. Default is 0.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option></term>
    <listitem>
     <para>
      The interval for 'deep' scrubbing (fully reading all data). The
      <option>osd scrub load threshold</option> option does not affect this
      setting. Default is 60*60*24*7 (once a week).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option></term>
    <listitem>
     <para>
      Add a random delay to the <option>osd scrub min interval</option> value
      when scheduling the next scrub job for a placement group. The delay is a
      random value smaller than the result of <option>osd scrub min
      interval</option> * <option>osd scrub interval randomized ratio</option>.
      Therefore, the default setting practically randomly spreads the scrubs
      out in the allowed time window of [1, 1.5] * <option>osd scrub min
      interval</option>. Default is 0.5
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option></term>
    <listitem>
     <para>
      Read size when doing a deep scrub. Default is 524288 (512 kB).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="op.mixed_ssd_hdd">
  <title>Mixed Storage Devices on the Same Node</title>

  <para>
   It can be desirable to configure a &ceph; cluster such that each node has a
   mix of SSDs and HDDs, with one storage pool on the fast SSDs and one storage
   pool on the slower HDDs. To do this, the &crushmap; needs to be edited.
  </para>

  <para>
   The default &crushmap; will have a simple hierarchy, where the default root
   contains hosts, and the hosts contain OSDs, for example:
  </para>

<screen>&prompt.root;ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000 </screen>

  <para>
   This provides no distinction between disk types. In order to split the OSDs
   into SSDs and HDDs, we need to create a second hierarchy in the &crushmap;:
  </para>

<screen>&prompt.root;ceph osd crush add-bucket ssd root</screen>

  <para>
   Having created the new root for SSDs, we need to add hosts to it. This means
   creating new host entries. But because the same host name cannot appear more
   than once in a &crushmap;, this uses fake host names. These fake host names
   do not need to be resolvable by DNS. CRUSH does not care what the host names
   are, they only need to create the right hierarchies. The one thing that
   <emphasis>does</emphasis> need to be changed in order to support fake host
   names is that you must set
  </para>

<screen>osd crush update on start = false</screen>

  <remark>mweiss: This is not required in case the "root default " is </remark>

  <para>
   in the
   <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
   file, and then run &deepsea; Stage 3 to distribute the change (refer to
   <xref
    linkend="ds.custom.cephconf"/> for more information):
  </para>

<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>

  <para>
   Otherwise the OSDs you move will be reset later to their original location
   in the default root, and the cluster will not behave as expected.
  </para>

  <para>
   Once that setting is changed, add the new fake hosts to the SSD root:
  </para>

<screen>&prompt.root;ceph osd crush add-bucket node1-ssd host
&prompt.root;ceph osd crush move node1-ssd root=ssd
&prompt.root;ceph osd crush add-bucket node2-ssd host
&prompt.root;ceph osd crush move node2-ssd root=ssd
&prompt.root;ceph osd crush add-bucket node3-ssd host
&prompt.root;ceph osd crush move node3-ssd root=ssd</screen>

  <para>
   Finally, for each SSD OSD, move the OSD to the SSD root. In this example, we
   assume that osd.0, osd.1 and osd.2 are physically hosted on SSDs:
  </para>

<screen>&prompt.root;ceph osd crush add osd.0 1 root=ssd
&prompt.root;ceph osd crush set osd.0 1 root=ssd host=node1-ssd
&prompt.root;ceph osd crush add osd.1 1 root=ssd
&prompt.root;ceph osd crush set osd.1 1 root=ssd host=node2-ssd
&prompt.root;ceph osd crush add osd.2 1 root=ssd
&prompt.root;ceph osd crush set osd.2 1 root=ssd host=node3-ssd</screen>

  <para>
   The CRUSH hierarchy should now look like this:
  </para>

<screen>&prompt.root;ceph osd tree
ID WEIGHT  TYPE NAME                   UP/DOWN REWEIGHT PRIMARY-AFFINITY
-5 3.00000 root ssd
-6 1.00000     host node1-ssd
 0 1.00000         osd.0                    up  1.00000          1.00000
-7 1.00000     host node2-ssd
 1 1.00000         osd.1                    up  1.00000          1.00000
-8 1.00000     host node3-ssd
 2 1.00000         osd.2                    up  1.00000          1.00000
-1 0.11096 root default
-2 0.03699     host node1
 3 0.01849         osd.3                    up  1.00000          1.00000
 6 0.01849         osd.6                    up  1.00000          1.00000
-3 0.03699     host node2
 4 0.01849         osd.4                    up  1.00000          1.00000
 7 0.01849         osd.7                    up  1.00000          1.00000
-4 0.03699     host node3
 5 0.01849         osd.5                    up  1.00000          1.00000
 8 0.01849         osd.8                    up  1.00000          1.00000</screen>

  <para>
   Now, create a CRUSH rule that targets the SSD root:
  </para>

<screen>&prompt.root;ceph osd crush rule create-simple ssd_replicated_ruleset ssd host</screen>

  <para>
   The original default <option>replicated_ruleset</option> (with ID 0) will
   target the HDDs. The new <option>ssd_replicated_ruleset</option> (with ID 1)
   will target the SSDs.
  </para>

  <para>
   Any existing pools will still be using the HDDs, because they are in the
   default hierarchy in the &crushmap;. A new pool can be created to use SSDs
   only:
  </para>

<screen>&prompt.root;ceph osd pool create ssd-pool 64 64
&prompt.root;ceph osd pool set ssd-pool crush_rule ssd_replicated_ruleset</screen>

  <remark>mweiss: It is possible to specify the correct rule already during initial pool creation (better than adjusting it later)</remark>

  <remark>Also application enable is required since 5</remark>
 </sect1>
</chapter>
