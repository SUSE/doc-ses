<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-faqs">
 <title>Frequently Asked Questions</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para/>
 <sect1 xml:id="storage-bp-tuneups-pg-num">
  <title>How Does the Number of Placement Groups Affect the Cluster Performance?</title>

  <para>
   When your cluster is becoming 70% to 80% full, it is time to add more OSDs
   to it. When you increase the number of OSDs, you may consider increasing the
   number of placement groups as well.
  </para>

  <warning>
   <para>
    Changing the number of PGs causes a lot of data transfer within the
    cluster.
   </para>
  </warning>

  <para>
   To calculate the optimal value for your newly-resized cluster is a complex
   task.
  </para>

  <para>
   A high number of PGs creates small chunks of data. This speeds up recovery
   after an OSD failure, but puts a lot of load on the monitor nodes as they
   are responsible for calculating the data location.
  </para>

  <para>
   On the other hand, a low number of PGs takes more time and data transfer to
   recover from an OSD failure, but does not impose that much load on monitor
   nodes as they need to calculate locations for less (but larger) data chunks.
  </para>

  <para>
   Find more information on the optimal number of PGs for your cluster in
   <xref linkend="op-pgs-pg-num"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-tuneups-mix-ssd">
  <title>Can I Use SSDs and Hard Disks on the Same Cluster?</title>

  <para>
   Solid-state drives (SSD) are generally faster than hard disks. If you mix
   the two types of disks for the same write operation, the data writing to the
   SSD disk will be slowed down by the hard disk performance. Thus, you should
   <emphasis>never mix SSDs and hard disks</emphasis> for data writing
   following <emphasis>the same rule</emphasis> (see
   <xref linkend="datamgm-rules"/> for more information on rules for storing
   data).
  </para>

  <para>
   There are generally 2 cases where using SSD and hard disk on the same
   cluster makes sense:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     Use each disk type for writing data following different rules. Then you
     need to have a separate rule for the SSD disk, and another rule for the
     hard disk.
    </para>
   </listitem>
   <listitem>
    <para>
     Use each disk type for a specific purpose. For example the SSD disk for
     journal, and the hard disk for storing data.
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="storage-bp-tuneups-ssd-tradeoffs">
  <title>What are the Trade-offs of Using a Journal on SSD?</title>

  <para>
   Using SSDs for OSD journal(s) is better for performance as the journal is
   usually the bottleneck of hard disk-only OSDs. SSDs are often used to share
   journals of several OSDs.
  </para>

  <para>
   Following is a list of potential disadvantages of using SSDs for OSD
   journal:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     SSD disks are more expensive than hard disks. But as one OSD journal
     requires up to 6GB of disk space only, the price may not be so crucial.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD disk consumes storage slots which can be otherwise used by a large
     hard disk to extend the cluster capacity.
    </para>
   </listitem>
   <listitem>
    <para>
     SSD disks have reduced write cycles compared to hard disks, but modern
     technologies are beginning to eliminate the problem.
    </para>
   </listitem>
   <listitem>
    <para>
     If you share more journals on the same SSD disk, you risk losing all the
     related OSDs after the SSD disk fails. This will require a lot of data to
     be moved to rebalance the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Hotplugging disks becomes more complex as the data mapping is not 1:1 the
     failed OSD and the journal disk.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-diskfails">
  <title>What Happens When a Disk Fails?</title>

  <para>
   When a disk with a stored cluster data has a hardware problem and fails to
   operate, here is what happens:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     The related OSD crashed and is automatically removed from the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The failed disk's data is replicated to another OSD in the cluster from
     other copies of the same data stored in other OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     Then you should remove the disk from the cluster &crushmap;, and
     physically from the host hardware.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-journalfails">
  <title>What Happens When a Journal Disk Fails?</title>

  <para>
   &ceph; can be configured to store journals or write ahead logs on devices
   separate from the OSDs. When a disk dedicated to a journal fails, the
   related OSD(s) fail as well (see
   <xref linkend="storage-bp-monitoring-diskfails"/>).
  </para>

  <warning>
   <title>Hosting Multiple Journals on One Disk</title>
   <para>
    For performance boost, you can use a fast disk (such as SSD) to store
    journal partitions for several OSDs. We do not recommend to host journals
    for more than 4 OSDs on one disk, because in case of the journals' disk
    failure, you risk losing stored data for all the related OSDs' disks.
   </para>
  </warning>
 </sect1>
</chapter>
