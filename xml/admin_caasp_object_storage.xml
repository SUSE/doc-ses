<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
  xml:id="admin-caasp-object-storage">
<!-- ============================================================== -->
 <title>&objstore;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="rook-object-storage">
  <title>&objstore;</title>

  <para>
   &objstore; exposes an S3 API to the storage cluster for applications to
   <literal>put</literal> and <literal>get</literal> data.
  </para>

  <sect2 xml:id="configure-an-object-store">
   <title>Configuring the &objstore;</title>
   <para>
    Rook has the ability to either deploy an &objstore; in &kube; or to connect
    to an external &ogw; service. Most commonly, the &objstore; will be
    configured locally by &rook;.
   </para>
   <sect3 xml:id="create-a-local-object-store">
    <title>Creating a local &objstore;</title>
    <para>
     The below sample will create a <literal>CephObjectStore</literal> that
     starts the &ogw; service in the cluster with an S3 API.
    </para>
    <note>
     <para>
      This sample requires at least three &bluestore; OSDs, with each OSD
      located on a different node.
     </para>
    </note>
    <para>
     The OSDs must be located on different nodes, because the
     <literal>failureDomain</literal> is set to <literal>host</literal> and the
     <literal>erasureCoded</literal> chunk settings require at least three
     different OSDs (two <literal>dataChunks</literal> + one
     <literal>codingChunks</literal>).
    </para>
<screen>
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: my-store
    namespace: rook-ceph
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPool:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
    preservePoolsOnDelete: true
    gateway:
      type: s3
      sslCertificateRef:
      port: 80
      securePort:
      instances: 1
    healthCheck:
      bucket:
        disabled: false
        interval: 60s
</screen>
    <para>
     After the <literal>CephObjectStore</literal> is created, the Rook operator
     will then create all the pools and other resources necessary to start the
     service. This may take a minute to complete.
    </para>
    <para>
     Create the object store:
    </para>
<screen>&prompt.kubeuser;kubectl create -f object.yaml</screen>
    <para>
     To confirm the object store is configured, wait for the rgw pod to start:
    </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get pod -l app=rook-ceph-rgw</screen>
   </sect3>
   <sect3 xml:id="connect-external-object-store">
    <title>Connecting to an external &objstore;</title>
    <para>
     &rook; can connect to existing &ogw; gateways to work in conjunction with
     the external mode of the CephCluster CRD. If you have an external
     CephCluster CR, you can instruct &rook; to consume external gateways with
     the following:
    </para>
<screen>
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: external-store
    namespace: rook-ceph
  spec:
    gateway:
      port: 8080
      externalRgwEndpoints:
        - ip: 192.168.39.182
    healthCheck:
      bucket:
        enabled: true
        interval: 60s
</screen>
    <para>
     You can use the existing <filename>object-external.yaml</filename> file.
     When ready the <literal>ceph-object-controller</literal> will output a
     message in the Operator log similar to this one:
    </para>
<screen>ceph-object-controller: ceph object store gateway service running at 10.100.28.138:8080</screen>
    <para>
     You can now get and access the store via:
    </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get svc -l app=rook-ceph-rgw
NAME                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
rook-ceph-rgw-my-store   ClusterIP   10.100.28.138     none        8080/TCP   6h59m
</screen>
    <para>
     Any pod from your cluster can now access this endpoint:
    </para>
<screen>&prompt.kubeuser;curl 10.100.28.138:8080</screen>
    <para>
     It is also possible to use the internally registered DNS name:
    </para>
<screen>&prompt.kubeuser;curl rook-ceph-rgw-my-store.rook-ceph:8080</screen>
    <para>
     The DNS name is created with the following schema:
     <literal>rook-ceph-rgw-$STORE_NAME.$NAMESPACE</literal>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="create-a-bucket">
   <title>Creating a bucket</title>
   <para>
    Now that the object store is configured, next we need to create a bucket
    where a client can read and write objects. A bucket can be created by
    defining a storage class, similar to the pattern used by block and file
    storage. First, define the storage class that will allow object clients to
    create a bucket. The storage class defines the object storage system, the
    bucket retention policy, and other properties required by the
    administrator. Save the following as
    <literal>storageclass-bucket-delete.yaml</literal> (the example is named as
    such due to the <literal>Delete</literal> reclaim policy).
   </para>
<screen>
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
     name: rook-ceph-bucket
  provisioner: rook-ceph.ceph.rook.io/bucket
  reclaimPolicy: Delete
  parameters:
    objectStoreName: my-store
    objectStoreNamespace: rook-ceph
    region: us-east-1
</screen>
<screen>&prompt.kubeuser;kubectl create -f storageclass-bucket-delete.yaml</screen>
   <para>
    Based on this storage class, an object client can now request a bucket by
    creating an Object Bucket Claim (OBC). When the OBC is created, the
    &rookceph; bucket provisioner will create a new bucket. Notice that the OBC
    references the storage class that was created above. Save the following as
    <literal>object-bucket-claim-delete.yaml</literal> (the example is named as
    such due to the <literal>Delete</literal> reclaim policy):
   </para>
<screen>
  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    name: ceph-bucket
  spec:
    generateBucketName: ceph-bkt
    storageClassName: rook-ceph-bucket
</screen>
<screen>&prompt.kubeuser;kubectl create -f object-bucket-claim-delete.yaml</screen>
   <para>
    Now that the claim is created, the operator will create the bucket as well
    as generate other artifacts to enable access to the bucket. A secret and
    ConfigMap are created with the same name as the OBC and in the same
    namespace. The secret contains credentials used by the application pod to
    access the bucket. The ConfigMap contains bucket endpoint information and
    is also consumed by the pod.
   </para>
   <sect3 xml:id="client-connections">
    <title>Client connections</title>
    <para>
     The following commands extract key pieces of information from the secret
     and configmap:
    </para>
<screen>
#config-map, secret, OBC will part of default if no specific name space mentioned
export AWS_HOST=$(kubectl -n default get cm ceph-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}')
export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | base64 --decode)
export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | base64 --decode)
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="consume-the-object-storage">
   <title>Consuming the &objstore;</title>
   <para>
    Now that you have the &objstore; configured and a bucket created, you can
    consume the object storage from an S3 client.
   </para>
   <para>
    This section will guide you through testing the connection to the
    <literal>CephObjectStore</literal> and uploading and downloading from it.
    Run the following commands after you have connected to the Rook toolbox.
   </para>
   <sect3 xml:id="connection-environment-variables">
    <title>Setting environment variables</title>
    <para>
     To simplify the S3 client commands, you will want to set the four
     environment variables for use by your client (for example, inside the
     toolbox). See above for retrieving the variables for a bucket created by
     an <literal>ObjectBucketClaim</literal>.
    </para>
<screen>
export AWS_HOST=<replaceable>HOST</replaceable>
export AWS_ENDPOINT=<replaceable>ENDPOINT</replaceable>
export AWS_ACCESS_KEY_ID=<replaceable>ACCESS_KEY</replaceable>
export AWS_SECRET_ACCESS_KEY=<replaceable>SECRET_KEY</replaceable>
</screen>
    <itemizedlist>
     <listitem>
      <para>
       <literal>Host</literal>: The DNS host name where the &ogw; service is
       found in the cluster. Assuming you are using the default
       <literal>rook-ceph</literal> cluster, it will be
       <literal>rook-ceph-rgw-my-store.rook-ceph</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>Endpoint</literal>: The endpoint where the &ogw; service is
       listening. Run the following command and then combine the clusterIP and
       the port.
      </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get svc rook-ceph-rgw-my-store</screen>
     </listitem>
     <listitem>
      <para>
       <literal>Access key</literal>: The user’s
       <literal>access_key</literal> as printed above
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>Secret key</literal>: The user’s
       <literal>secret_key</literal> as printed above
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The variables for the user generated in this example might be:
    </para>
<screen>
export AWS_HOST=rook-ceph-rgw-my-store.rook-ceph
export AWS_ENDPOINT=10.104.35.31:80
export AWS_ACCESS_KEY_ID=XEZDB3UJ6X7HVBE7X7MA
export AWS_SECRET_ACCESS_KEY=7yGIZON7EhFORz0I40BFniML36D2rl8CQQ5kXU6l
</screen>
    <para>
     The access key and secret key can be retrieved as described in the section
     above on <xref linkend="client-connections"/> or below in the section
     <xref linkend="create-a-user"/> if you are not creating the buckets with
     an <literal>ObjectBucketClaim</literal>.
    </para>
   </sect3>
   <sect3 xml:id="install-s3cmd">
    <title>Installing the <package>s3cmd</package> package</title>
    <para>
     To test the <literal>CephObjectStore</literal> we will install the
     <literal>s3cmd</literal> tool into the toolbox pod.
    </para>
<screen>zypper --assumeyes install s3cmd</screen>
   </sect3>
   <sect3 xml:id="put-or-get-an-object">
    <title>PUT or GET an object</title>
    <para>
     Upload a file to the newly created bucket:
    </para>
<screen>echo &quot;Hello Rook&quot; &gt; /tmp/rookObj
s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://rookbucket
</screen>
    <para>
     Download and verify the file from the bucket:
    </para>
<screen>s3cmd get s3://rookbucket/rookObj /tmp/rookObj-download --no-ssl --host=${AWS_HOST} --host-bucket=
cat /tmp/rookObj-download
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="access-external-cluster">
   <title>Setting up external access to the cluster</title>
   <para>
    &rook; sets up the object storage so pods will have access internal to the
    cluster. If your applications are running outside the cluster, you will
    need to setup an external service through a <literal>NodePort</literal>.
   </para>
   <para>
    First, note the service that exposes RGW internal to the cluster. We will
    leave this service intact and create a new service for external access.
   </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get service rook-ceph-rgw-my-store
NAME                     CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE
rook-ceph-rgw-my-store   10.3.0.177   none        80/TCP      2m
</screen>
   <para>
    Save the external service as <filename>rgw-external.yaml</filename>:
   </para>
<screen>
  apiVersion: v1
  kind: Service
  metadata:
    name: rook-ceph-rgw-my-store-external
    namespace: rook-ceph
    labels:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: my-store
  spec:
    ports:
    - name: rgw
      port: 80
      protocol: TCP
      targetPort: 80
    selector:
      app: rook-ceph-rgw
      rook_cluster: rook-ceph
      rook_object_store: my-store
    sessionAffinity: None
    type: NodePort
</screen>
   <para>
    Now create the external service:
   </para>
<screen>&prompt.kubeuser;kubectl create -f rgw-external.yaml</screen>
   <para>
    See both &ogw; services running and notice what port the external service
    is running on:
   </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph get service rook-ceph-rgw-my-store rook-ceph-rgw-my-store-external
NAME                              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
rook-ceph-rgw-my-store            ClusterIP   10.104.82.228    none          80/TCP         4m
rook-ceph-rgw-my-store-external   NodePort    10.111.113.237   none          80:31536/TCP   39s
</screen>
   <para>
    Internally the &ogw; service is running on port 80. The external port in
    this case is 31536.
   </para>
  </sect2>

  <sect2 xml:id="create-a-user">
   <title>Creating a user</title>
   <para>
    If you need to create an independent set of user credentials to access the
    S3 endpoint, create a <literal>CephObjectStoreUser</literal>. The user will
    be used to connect to the &ogw; service in the cluster using the S3 API.
    The user will be independent of any object bucket claims that you might
    have created in the earlier instructions in this document.
   </para>
<screen>
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStoreUser
  metadata:
    name: my-user
    namespace: rook-ceph
  spec:
    store: my-store
    displayName: "my display name"
</screen>
   <para>
    When the <literal>CephObjectStoreUser</literal> is created, the &rook;
    operator will then create the RGW user on the specified
    <literal>CephObjectStore</literal> and store the Access Key and Secret Key
    in a kubernetes secret in the same namespace as the
    <literal>CephObjectStoreUser</literal>.
   </para>
   <para>
    Create the object store user:
   </para>
<screen>&prompt.kubeuser;kubectl create -f object-user.yaml</screen>
   <para>
    To confirm the object store user is configured, describe the secret:
   </para>
<screen>&prompt.kubeuser;kubectl -n rook-ceph describe secret rook-ceph-object-user-my-store-my-user
  Name:		rook-ceph-object-user-my-store-my-user
  Namespace:	rook-ceph
  Labels:			app=rook-ceph-rgw
  			      rook_cluster=rook-ceph
  			      rook_object_store=my-store
  Annotations:	none

  Type:	kubernetes.io/rook

  Data
  ====
  AccessKey:	20 bytes
  SecretKey:	40 bytes
</screen>
   <para>
    The <literal>AccessKey</literal> and <literal>SecretKey</literal> data
    fields can be mounted in a pod as an environment variable.
   </para>
   <para>
    To directly retrieve the secrets:
   </para>
<screen>
&prompt.kubeuser;kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml \
 | grep AccessKey | awk '{print $2}' | base64 --decode
&prompt.kubeuser;kubectl -n rook-ceph get secret rook-ceph-object-user-my-store-my-user -o yaml \
 | grep SecretKey | awk '{print $2}' | base64 --decode
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-object-store-crd">
  <title>&ceph; &objstore; CRD</title>

  <para>
   &rook; allows creation and customization of object stores through the custom
   resource definitions (CRDs). The following settings are available for &ceph;
   &objstore;.
  </para>

  <sect2 xml:id="rook-objectstore-crd-sample">
   <title>Sample</title>
   <sect3 xml:id="rook-objectstore-crd-erasure-coded">
    <title>Erasure coded</title>
    <para>
     Erasure-coded pools require the OSDs to use <literal>bluestore</literal>
     for the configured <literal>storeType</literal>. Additionally, erasure-
     coded pools can only be used with <literal>dataPools</literal>. The
     <literal>metadataPool</literal> must use a replicated pool.
    </para>
    <note>
     <para>
      This sample requires at least three &bluestore; OSDs, with each OSD
      located on a different node.
     </para>
    </note>
    <para>
     The OSDs must be located on different nodes, because the
     <literal>failureDomain</literal> is set to <literal>host</literal> and the
     <literal>erasureCoded</literal> chunk settings require at least three
     different OSDs (two <literal>dataChunks</literal> + one
     <literal>codingChunks</literal>).
    </para>
<screen>
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStore
  metadata:
    name: my-store
    namespace: rook-ceph
  spec:
    metadataPool:
      failureDomain: host
      replicated:
        size: 3
    dataPool:
      failureDomain: host
      erasureCoded:
        dataChunks: 2
        codingChunks: 1
    preservePoolsOnDelete: true
    gateway:
      type: s3
      sslCertificateRef:
      port: 80
      securePort:
      instances: 1
      # A key/value list of annotations
      annotations:
      #  key: value
      placement:
      #  nodeAffinity:
      #    requiredDuringSchedulingIgnoredDuringExecution:
      #      nodeSelectorTerms:
      #      - matchExpressions:
      #        - key: role
      #          operator: In
      #          values:
      #          - rgw-node
      #  tolerations:
      #  - key: rgw-node
      #    operator: Exists
      #  podAffinity:
      #  podAntiAffinity:
      #  topologySpreadConstraints:
      resources:
      #  limits:
      #    cpu: "500m"
      #    memory: "1024Mi"
      #  requests:
      #    cpu: "500m"
      #    memory: "1024Mi"
    #zone:
      #name: zone-a
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="object-store-settings">
   <title>Object store settings</title>
   <sect3 xml:id="objectstore-settings-metadata">
    <title>Metadata</title>
    <itemizedlist>
     <listitem>
      <para>
       <literal>name</literal>: The name of the object store to create, which
       will be reflected in the pool and other resource names.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>namespace</literal>: The namespace of the Rook cluster where
       the object store is created.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="pools">
    <title>Pools</title>
    <para>
     The pools allow all of the settings defined in the Pool CRD spec. In the
     example above, there must be at least three hosts (size 3) and at least
     three devices (two data + one coding chunks) in the cluster.
    </para>
    <para>
     When the <literal>zone</literal> section is set, pools with the object
     store's name will not be created, since the object-store will the using the
     pools created by the ceph-object-zone.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>metadataPool</literal>: The settings used to create all of the
       object store metadata pools. Must use replication.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>dataPool</literal>: The settings to create the object store
       data pool. Can use replication or erasure coding.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>preservePoolsOnDelete</literal>: If it is set to
       <quote>true</quote>, the pools used to support the object store will
       remain when the object store will be deleted. This is a security measure
       to avoid accidental loss of data. It is set to <quote>false</quote> by
       default. If it is not specified, this is also deemed as
       <quote>false</quote>.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="gateway-settings">
   <title>Creating gateway settings</title>
   <para>
    The gateway settings correspond to the &ogw; daemon settings.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>type</literal>: <literal>S3</literal> is supported
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>sslCertificateRef</literal>: If the certificate is not
      specified, SSL will not be configured. If specified, this is the name of
      the &kube; secret that contains the SSL certificate to be used for secure
      connections to the object store. &rook; will look in the secret provided
      at the <literal>cert</literal> key name. The value of the
      <literal>cert</literal> key must be in the format expected by the &ogw;
      service: <quote>The server key, server certificate, and any other CA or
      intermediate certificates be supplied in one file. Each of these items
      must be in pem form.</quote>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>port</literal>: The port on which the Object service will be
      reachable. If host networking is enabled, the &ogw; daemons will also
      listen on that port. If running on SDN, the &ogw; daemon listening port
      will be 8080 internally.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>securePort</literal>: The secure port on which &ogw; pods will
      be listening. An SSL certificate must be specified.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>instances</literal>: The number of pods that will be started to
      load-balance this object store.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>externalRgwEndpoints</literal>: A list of IP addresses to
      connect to external existing &ogw;s (works with external mode). This
      setting will be ignored if the <literal>CephCluster</literal> does not
      have <literal>external</literal> spec enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>annotations</literal>: Key-value pair list of annotations to
      add.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>labels</literal>: Key-value pair list of labels to add.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>placement</literal>: The &kube; placement settings to determine
      where the &ogw; pods should be started in the cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>resources</literal>: Set resource requests/limits for the
      Gateway Pod(s).
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>priorityClassName</literal>: Set priority class name for the
      Gateway Pod(s).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Example of external &ogw; endpoints to connect to:
   </para>
<screen>
  gateway:
    port: 80
    externalRgwEndpoints:
      - ip: 192.168.39.182
</screen>
   <para>
    This will create a service with the endpoint
    <literal>192.168.39.182</literal> on port <literal>80</literal>, pointing
    to the Ceph object external gateway. All the other settings from the
    gateway section will be ignored, except for <literal>securePort</literal>.
   </para>
  </sect2>

  <sect2 xml:id="zone-settings">
   <title>Zone settings</title>
   <para>
    The <link xlink:href="ceph-object-multisite.md">zone</link> settings allow
    the object store to join custom created
    <link xlink:href="ceph-object-multisite-crd.md">ceph-object-zone</link>.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>name</literal>: the name of the
      <literal>ceph-object-zone</literal> the object store will be in.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="runtime-settings">
   <title>Runtime settings</title>
   <sect3 xml:id="mime-types">
    <title>MIME types</title>
    <para>
     Rook provides a default <filename>mime.types</filename> file for each
     &ceph; &objstore;. This file is stored in a Kubernetes ConfigMap with the
     name <literal>rook-ceph-rgw-&lt;STORE-NAME&gt;-mime-types</literal>. For
     most users, the default file should suffice, however, the option is
     available to users to edit the <filename>mime.types</filename> file in the
     ConfigMap as they desire. Users may have their own special file types, and
     particularly security conscious users may wish to pare down the file to
     reduce the possibility of a file type execution attack.
    </para>
    <para>
     &rook; will not overwrite an existing <filename>mime.types</filename>
     ConfigMap so that user modifications will not be destroyed. If the object
     store is destroyed and recreated, the ConfigMap will also be destroyed and
     created anew.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="health-settings">
   <title>Health settings</title>
   <para>
    &rookceph; will be default monitor the state of the object store endpoints.
    The following CRD settings are available:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>healthCheck</literal>: main object store health monitoring
      section
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For example:
   </para>
<screen>
  healthCheck:
    bucket:
      disabled: false
      interval: 60s
</screen>
   <para>
    The endpoint health check procedure is the following:
   </para>
   <orderedlist>
    <listitem>
     <para>
      Create an S3 user.
     </para>
    </listitem>
    <listitem>
     <para>
      Create a bucket with that user.
     </para>
    </listitem>
    <listitem>
     <para>
      PUT the file in the object store.
     </para>
    </listitem>
    <listitem>
     <para>
      GET the file from the object store.
     </para>
    </listitem>
    <listitem>
     <para>
      Verify object consistency.
     </para>
    </listitem>
    <listitem>
     <para>
      Update CR health status check.
     </para>
    </listitem>
   </orderedlist>
   <para>
    &rookceph; always keeps the bucket and the user for the health check; it
    just does a PUT and GET of an S3 object, since creating a bucket is an
    expensive operation.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-object-bucket-claim">
  <title>&ceph; object bucket claim</title>

  <para>
   Rook supports the creation of new buckets and access to existing buckets via
   two custom resources:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     An <literal>Object Bucket Claim (OBC)</literal> is custom resource which
     requests a bucket (new or existing) and is described by a Custom Resource
     Definition (CRD) shown below.
    </para>
   </listitem>
   <listitem>
    <para>
     An <literal>Object Bucket (OB)</literal> is a custom resource
     automatically generated when a bucket is provisioned. It is a global
     resource, typically not visible to non-admin users, and contains
     information specific to the bucket. It is described by an OB CRD, also
     shown below.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   An OBC references a storage class which is created by an administrator. The
   storage class defines whether the bucket requested is a new bucket or an
   existing bucket. It also defines the bucket retention policy. Users request
   a new or existing bucket by creating an OBC which is shown below. The ceph
   provisioner detects the OBC and creates a new bucket or grants access to an
   existing bucket, depending the the storage class referenced in the OBC. It
   also generates a Secret which provides credentials to access the bucket, and
   a ConfigMap which contains the bucket’s endpoint. Application pods consume
   the information in the Secret and ConfigMap to access the bucket. Please
   note that to make provisioner watch the cluster namespace only you need to
   set <literal>ROOK_OBC_WATCH_OPERATOR_NAMESPACE</literal> to
   <literal>true</literal> in the operator manifest, otherwise it watches all
   namespaces.
  </para>

  <sect2 xml:id="rook-objectstore-bucket-sample">
   <title>Sample</title>
   <sect3 xml:id="obc-custom-resource">
    <title>OBC custom resource</title>
<screen>
  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    name: ceph-bucket [1]
    namespace: rook-ceph [2]
  spec:
    bucketName: [3]
    generateBucketName: photo-booth [4]
    storageClassName: rook-ceph-bucket [4]
    additionalConfig: [5]
      maxObjects: "1000"
      maxSize: "2G"
</screen>
    <orderedlist numeration="arabic">
     <listitem>
      <para>
       <literal>name</literal> of the <literal>ObjectBucketClaim</literal>.
       This name becomes the name of the Secret and ConfigMap.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>namespace</literal>(optional) of the
       <literal>ObjectBucketClaim</literal>, which is also the namespace of the
       ConfigMap and Secret.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>bucketName</literal> name of the <literal>bucket</literal>.
       <emphasis role="strong">Not</emphasis> recommended for new buckets, since
       names must be unique within an entire object store.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>generateBucketName</literal> value becomes the prefix for a
       randomly-generated name; if supplied, then <literal>bucketName</literal>
       must be empty. If both <literal>bucketName</literal> and
       <literal>generateBucketName</literal> are supplied, then
       <literal>BucketName</literal> has precedence and
       <literal>GenerateBucketName</literal> is ignored. If both
       <literal>bucketName</literal> and <literal>generateBucketName</literal>
       are blank or omitted, then the storage class is expected to contain the
       name of an <emphasis>existing</emphasis> bucket. It is an error if all
       three bucket-related names are blank or omitted.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>storageClassName</literal> which defines the StorageClass which
       contains the names of the bucket provisioner, the object-store, and
       specifies the bucket-retention policy.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>additionalConfig</literal> is an optional list of key-value
       pairs used to define attributes specific to the bucket being provisioned
       by this OBC. This information is typically tuned to a particular bucket
       provisioner, and may limit application portability. Options supported:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>maxObjects</literal>: The maximum number of objects in the
         bucket
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>maxSize</literal>: The maximum size of the bucket, please note
         minimum recommended value is 4K.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="obc-custom-resource-after-bucket-provisioning">
    <title>OBC custom resource after bucket provisioning</title>
<screen>
  apiVersion: objectbucket.io/v1alpha1
  kind: ObjectBucketClaim
  metadata:
    creationTimestamp: "2019-10-18T09:54:01Z"
    generation: 2
    name: ceph-bucket
    namespace: default [1]
    resourceVersion: "559491"
  spec:
    ObjectBucketName: obc-default-ceph-bucket [2]
    additionalConfig: null
    bucketName: photo-booth-c1178d61-1517-431f-8408-ec4c9fa50bee [3]
    cannedBucketAcl: ""
    ssl: false
    storageClassName: rook-ceph-bucket [4]
    versioned: false
  status:
    Phase: bound [5]
</screen>
    <orderedlist>
     <listitem>
      <para>
       <literal>namespace</literal> where OBC got created.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>ObjectBucketName</literal> generated OB name created using name
       space and OBC name.
      </para>
     </listitem>
     <listitem>
      <para>
       The generated (in this case), unique <literal>bucket name</literal> for
       the new bucket.
      </para>
     </listitem>
     <listitem>
      <para>
       Name of the storage class from OBC got created.
      </para>
     </listitem>
     <listitem>
      <para>
       Phases of bucket creation:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <emphasis>Pending</emphasis>: the operator is processing the request.
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Bound</emphasis>: the operator finished processing the
         request and linked the OBC and OB
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Released</emphasis>: the OB has been deleted, leaving the
         OBC unclaimed but unavailable.
        </para>
       </listitem>
       <listitem>
        <para>
         <emphasis>Failed</emphasis>: not currently set.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="app-pod">
    <title>App pod</title>
<screen>
  apiVersion: v1
  kind: Pod
  metadata:
    name: app-pod
    namespace: dev-user
  spec:
    containers:
    - name: mycontainer
      image: redis
      envFrom: [1]
      - configMapRef:
          name: ceph-bucket [2]
      - secretRef:
          name: ceph-bucket [3]
</screen>
    <orderedlist numeration="arabic">
     <listitem>
      <para>
       Use <literal>env:</literal> if mapping of the defined key names to the
       environment-variable names used by the app is needed.
      </para>
     </listitem>
     <listitem>
      <para>
       Makes available to the pod as environment variables:
       <literal>BUCKET_HOST</literal>, <literal>BUCKET_PORT</literal>,
       <literal>BUCKET_NAME</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       makes available to the pod as environment variables:
       <literal>AWS_ACCESS_KEY_ID</literal>,
       <literal>AWS_SECRET_ACCESS_KEY</literal>
      </para>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="storageclass">
    <title><literal>StorageClass</literal></title>
<screen>
  apiVersion: storage.k8s.io/v1
  kind: StorageClass
  metadata:
    name: rook-ceph-bucket
    labels:
      aws-s3/object [1]
  provisioner: rook-ceph.ceph.rook.io/bucket [2]
  parameters: [3]
    objectStoreName: my-store
    objectStoreNamespace: rook-ceph
    region: us-west-1
    bucketName: ceph-bucket [4]
  reclaimPolicy: Delete [5]
</screen>
    <orderedlist numeration="arabic">
     <listitem>
      <para>
       <literal>label</literal>(optional) here associates this
       <literal>StorageClass</literal> to a specific provisioner.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>provisioner</literal> responsible for handling
       <literal>OBCs</literal> referencing this
       <literal>StorageClass</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="strong">all</emphasis> <literal>parameter</literal>
       required.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>bucketName</literal> is required for access to existing buckets
       but is omitted when provisioning new buckets. Unlike greenfield
       provisioning, the brownfield bucket name appears in the
       <literal>StorageClass</literal>, not the <literal>OBC</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       rook-ceph provisioner decides how to treat the
       <literal>reclaimPolicy</literal> when an <literal>OBC</literal> is
       deleted for the bucket.
      </para>
     </listitem>
    </orderedlist>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>Delete</emphasis> = physically delete the bucket.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Retain</emphasis> = do not physically delete the bucket.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-object-store-user-crd">
  <title>&ceph; &objstore; user custom resource definitions (CRD)</title>

  <para>
   &rook; allows creation and customization of object store users through the
   custom resource definitions (CRDs). The following settings are available for
   &ceph; object store users.
  </para>

  <sect2 xml:id="rook-objectstore-user-crd-sample">
   <title>Sample</title>
<screen>
  apiVersion: ceph.rook.io/v1
  kind: CephObjectStoreUser
  metadata:
    name: my-user
    namespace: rook-ceph
  spec:
    store: my-store
    displayName: my-display-name
</screen>
  </sect2>

  <sect2 xml:id="object-store-user-settings">
   <title>&objstore; user settings</title>
   <sect3 xml:id="objectstore-user-settings-metadata">
    <title>Metadata</title>
    <itemizedlist>
     <listitem>
      <para>
       <literal>name</literal>: The name of the object store user to create,
       which will be reflected in the secret and other resource names.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>namespace</literal>: The namespace of the Rook cluster where
       the object store user is created.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="spec">
    <title>Specification</title>
    <itemizedlist>
     <listitem>
      <para>
       <literal>store</literal>: The object store in which the user will be
       created. This matches the name of the &objstore; CRD.
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>displayName</literal>: The display name which will be passed to
       the <literal>radosgw-admin user create</literal> command.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>
 </sect1>
</chapter>
