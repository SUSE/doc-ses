<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-mgr-modules">
 <title>&mgr; Modules</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The architecture of the &mgr; (refer to
  <xref linkend="storage-intro-core-nodes"/> for a brief introduction) allows
  extending its functionality via <emphasis>modules</emphasis>, such as
  'dashboard' (see <xref linkend="ceph-dashboard"/>), 'prometheus' (see
  <xref linkend="monitoring-alerting"/>, or 'balancer'.
 </para>
 <para>
  To list all available modules, run:
 </para>
<screen>&prompt.cephuser;ceph mgr module ls
{
        "enabled_modules": [
                "restful",
                "status"
        ],
        "disabled_modules": [
                "dashboard"
        ]
}</screen>
 <para>
  To enable or disable a specific module, run:
 </para>
<screen>&prompt.cephuser;ceph mgr module enable <replaceable>MODULE-NAME</replaceable></screen>
 <para>
  For example:
 </para>
<screen>&prompt.cephuser;ceph mgr module disable dashboard</screen>
 <para>
  To list service that the enabled modules provide, run:
 </para>
<screen>&prompt.cephuser;ceph mgr services
{
        "dashboard": "http://myserver.com:7789/",
        "restful": "https://myserver.com:8789/"
}</screen>
 <sect1 xml:id="mgr-modules-balancer">
  <title>Balancer</title>

  <para>
   The balancer module optimizes the placement group (PG) distribution across
   OSDs for a more balanced deployment. Although the module is activated by
   default, it is inactive. It supports the following two modes: 'crush-compat'
   and 'upmap'.
  </para>

  <tip>
   <title>Current Balancer Configuration</title>
   <para>
    To view the current balancer configuration, run:
   </para>
<screen>&prompt.cephuser;ceph config-key dump</screen>
  </tip>

  <important>
   <title>Supported Mode</title>
   <para>
    We currently only support the 'crush-compat' mode because the 'upmap' mode
    requires an OSD feature that prevents any pre-luminous OSDs from connecting
    to the cluster.
   </para>
  </important>

  <sect2 xml:id="mgr-balancer-crush-compat">
   <title>The 'crush-compat' Mode</title>
   <para>
    In 'crush-compat' mode, the balancer adjusts the OSDs reweight-sets to
    achieve improved distribution of the data. It moves PGs between OSDs,
    temporarily causing a HEALTH_WARN cluster state resulting from misplaced
    PGs.
   </para>
   <tip>
    <title>Mode Activation</title>
    <para>
     Although 'crush-compat' is the default mode, we recommend activating it
     explicitly:
    </para>
<screen>&prompt.cephuser;ceph balancer mode crush-compat</screen>
   </tip>
  </sect2>

  <sect2 xml:id="mgr-balancer-planning-executing">
   <title>Planning and Executing of Data Balancing</title>
   <para>
    Using the balancer module, you can create a plan for data balancing. You
    can then execute the plan manually, or let the balancer balance PGs
    continuously.
   </para>
   <para>
    The decision whether to run the balancer in manual or automatic mode
    depends on several factors, such as the current data imbalance, cluster
    size, PG count, or I/O activity. We recommend creating an initial plan and
    executing it at a time of low I/O load in the cluster. The reason is that
    the initial imbalance will probably be considerable and it is a good
    practice to keep the impact on clients low. After an initial manual run,
    consider activating the automatic mode and monitor the rebalance traffic
    under normal I/O load. The improvements in PG distribution need to be
    weighed against the rebalance traffic caused by the balancer.
   </para>
   <tip>
    <title>Movable Fraction of Placement Groups (PGs)</title>
    <para>
     During the process of balancing, the balancer module throttles PG
     movements so that only a configurable fraction of PGs is moved. The
     default is 5% and you can adjust the fraction to for example 9% by running
     the following command:
    </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/balancer/max_misplaced .09</screen>
   </tip>
   <para>
    To create and execute a balancing plan, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Check the current cluster score:
     </para>
<screen>&prompt.cephuser;ceph balancer eval</screen>
    </step>
    <step>
     <para>
      Have the balancer create a plan called for example 'great_plan':
     </para>
<screen>&prompt.cephuser;ceph balancer optimize great_plan</screen>
    </step>
    <step>
     <para>
      See what changes the 'great_plan' will entail:
     </para>
<screen>&prompt.cephuser;ceph balancer show great_plan</screen>
    </step>
    <step>
     <para>
      Check the potential cluster score if you decide to apply the
      'great_plan':
     </para>
<screen>&prompt.cephuser;ceph balancer eval great_plan</screen>
    </step>
    <step>
     <para>
      Execute the 'great_plan' for one time only:
     </para>
<screen>&prompt.cephuser;ceph balancer execute great_plan</screen>
    </step>
    <step>
     <para>
      Observe the cluster balancing with the <command>ceph -s</command>
      command. If you are satisfied with the result, activate the automatic
      balancing:
     </para>
<screen>&prompt.cephuser;ceph balancer on</screen>
     <para>
      If you later decide to deactivate the automatic balancing, run:
     </para>
<screen>&prompt.cephuser;ceph balancer off</screen>
    </step>
   </procedure>
   <tip>
    <title>Automatic Balancing without Initial Plan</title>
    <para>
     You can activate the automatic balancing without executing an initial
     plan. In such case, expect a potentially long running rebalancing of
     placement groups.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="mgr-modules-telemetry">
  <title>Telemetry Module</title>
  <para>
    The telemetry plugin sends anonymous data about the cluster, in which it
    is running, back to the Ceph project.</para>
  <para>
    This (opt-in) component contains counters and statistics on how the cluster has been
    deployed, the version of &ceph;, the distribition of the hosts and other
    parameters which help the project to gain a better understanding of the
    way &ceph; is used. It does not contain any sensitive data like pool names,
    object names, object contents or hostnames.</para>
  <para>The purpose of the telemetry module is to provide an automated feedback
    loop for the developers to help quantify adoption rates, tracking, or
    pointing out things that need to be better explained or validated during
    configuration to prevent undesirable outcomes.</para>
  <note>
    <para>
      The Telemetry Module requires the &mgr; nodes to have the ability to push
      data over HTTPS to the upstream servers. Ensure your corporate firewalls
      permit this action.
    </para>
  </note>
  <!-- <para>If you would like to read more, or see some of the early findings
    of using this tool, we recommend reading <link xlink:href="https://ceph.io/community/the-first-telemetry-results-are-in/"/>.
  </para> -->
  <procedure>
    <step>
      <para>
        To enable the telemetry module:
      </para>
      <screen>&prompt.cephuser;ceph mgr module enable telemetry</screen>
      <note>
        <para>
          This command only enables you to view your data locally. This
          command does not share your data with the &ceph; community.
        </para>
      </note>
    </step>
    <step>
      <para>To allow the telemetry module to start sharing data:</para>
      <screen>&prompt.cephuser;ceph telemetry on</screen>
    </step>
    <step>
      <para>To disable telemetry data sharing:</para>
      <screen>&prompt.cephuser;ceph telemetry off</screen>
    </step>
    <step>
      <para>To generate a JSON report that can be printed:</para>
      <screen>&prompt.cephuser;ceph telemetry show</screen>
    </step>
    <step>
      <para>To add a contact and description to the report:</para>
      <screen>&prompt.cephuser;ceph config set mgr mgr/telemetry/contact ‘John Doe <literal>john.doe@example.com</literal>’
        ceph config set mgr mgr/telemetry/description ‘My first Ceph cluster’</screen>
    </step>
    <step>
      <para>The module compiles and sends a new report every 24 hours by default.
      To adjust this interval:</para>
      <screen>&prompt.cephuser;ceph config set mgr mgr/telemetry/interval HOURS</screen>
    </step>
  </procedure>
 <!-- <para>For more information on the telemetry modules, see
    <link xlink:href="https://docs.ceph.com/docs/master/mgr/telemetry/"/>.</para> -->
 </sect1>
</chapter>
