<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-mgr-modules">
 <title>&mgr; Modules</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The architecture of the &mgr; (refer to
  <xref linkend="storage-intro-core-nodes"/> for a brief introduction) allows
  extending its functionality via <emphasis>modules</emphasis>, such as
  'dashboard' (see <xref linkend="ceph-dashboard"/>), 'prometheus' (see
  <xref linkend="monitoring-alerting"/>), or 'balancer'.
 </para>
 <para>
  To list all available modules, run:
 </para>
<screen>&prompt.cephuser;ceph mgr module ls
{
        "enabled_modules": [
                "restful",
                "status"
        ],
        "disabled_modules": [
                "dashboard"
        ]
}</screen>
 <para>
  To enable or disable a specific module, run:
 </para>
<screen>&prompt.cephuser;ceph mgr module enable <replaceable>MODULE-NAME</replaceable></screen>
 <para>
  For example:
 </para>
<screen>&prompt.cephuser;ceph mgr module disable dashboard</screen>
 <para>
  To list the services that the enabled modules provide, run:
 </para>
<screen>&prompt.cephuser;ceph mgr services
{
        "dashboard": "http://myserver.com:7789/",
        "restful": "https://myserver.com:8789/"
}</screen>
 <sect1 xml:id="mgr-modules-balancer">
  <title>Balancer</title>

  <para>
   The balancer module optimizes the placement group (PG) distribution across
   OSDs for a more balanced deployment. Although the module is activated by
   default, it is inactive. It supports the following two modes: 'crush-compat'
   and 'upmap'.
  </para>

  <tip>
   <title>Current Balancer Configuration</title>
   <para>
    To view the current balancer configuration, run:
   </para>
<screen>&prompt.cephuser;ceph config-key dump</screen>
  </tip>

  <important>
   <title>Supported Mode</title>
   <para>
    We currently only support the 'crush-compat' mode because the 'upmap' mode
    requires an OSD feature that prevents any pre-luminous OSDs from connecting
    to the cluster.
   </para>
  </important>

  <sect2 xml:id="mgr-balancer-crush-compat">
   <title>The 'crush-compat' Mode</title>
   <para>
    In 'crush-compat' mode, the balancer adjusts the OSDs' reweight-sets to
    achieve improved distribution of the data. It moves PGs between OSDs,
    temporarily causing a HEALTH_WARN cluster state resulting from misplaced
    PGs.
   </para>
   <tip>
    <title>Mode Activation</title>
    <para>
     Although 'crush-compat' is the default mode, we recommend activating it
     explicitly:
    </para>
<screen>&prompt.cephuser;ceph balancer mode crush-compat</screen>
   </tip>
  </sect2>

  <sect2 xml:id="mgr-balancer-planning-executing">
   <title>Planning and Executing of Data Balancing</title>
   <para>
    Using the balancer module, you can create a plan for data balancing. You
    can then execute the plan manually, or let the balancer balance PGs
    continuously.
   </para>
   <para>
    The decision whether to run the balancer in manual or automatic mode
    depends on several factors, such as the current data imbalance, cluster
    size, PG count, or I/O activity. We recommend creating an initial plan and
    executing it at a time of low I/O load in the cluster. The reason for this
    is that the initial imbalance will probably be considerable and it is a
    good practice to keep the impact on clients low. After an initial manual
    run, consider activating the automatic mode and monitor the rebalance
    traffic under normal I/O load. The improvements in PG distribution need to
    be weighed against the rebalance traffic caused by the balancer.
   </para>
   <tip>
    <title>Movable Fraction of Placement Groups (PGs)</title>
    <para>
     During the process of balancing, the balancer module throttles PG
     movements so that only a configurable fraction of PGs is moved. The
     default is 5% and you can adjust the fraction, to 9% for example, by
     running the following command:
    </para>
<screen>&prompt.cephuser;ceph config set mgr target_max_misplaced_ratio .09</screen>
   </tip>
   <para>
    To create and execute a balancing plan, follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Check the current cluster score:
     </para>
<screen>&prompt.cephuser;ceph balancer eval</screen>
    </step>
    <step>
     <para>
      Create a plan. For example, 'great_plan':
     </para>
<screen>&prompt.cephuser;ceph balancer optimize great_plan</screen>
    </step>
    <step>
     <para>
      See what changes the 'great_plan' will entail:
     </para>
<screen>&prompt.cephuser;ceph balancer show great_plan</screen>
    </step>
    <step>
     <para>
      Check the potential cluster score if you decide to apply the
      'great_plan':
     </para>
<screen>&prompt.cephuser;ceph balancer eval great_plan</screen>
    </step>
    <step>
     <para>
      Execute the 'great_plan' for one time only:
     </para>
<screen>&prompt.cephuser;ceph balancer execute great_plan</screen>
    </step>
    <step>
     <para>
      Observe the cluster balancing with the <command>ceph -s</command>
      command. If you are satisfied with the result, activate automatic
      balancing:
     </para>
<screen>&prompt.cephuser;ceph balancer on</screen>
     <para>
      If you later decide to deactivate automatic balancing, run:
     </para>
<screen>&prompt.cephuser;ceph balancer off</screen>
    </step>
   </procedure>
   <tip>
    <title>Automatic Balancing without Initial Plan</title>
    <para>
     You can activate automatic balancing without executing an initial plan. In
     such case, expect a potentially long running rebalancing of placement
     groups.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="mgr-modules-telemetry">
  <title>Telemetry Module</title>

  <para>
   The telemetry plugin sends the &ceph; project anonymous data about the
   cluster in which the plugin is running.
  </para>

  <para>
   This (opt-in) component contains counters and statistics on how the cluster
   has been deployed, the version of &ceph;, the distribution of the hosts and
   other parameters which help the project to gain a better understanding of
   the way &ceph; is used. It does not contain any sensitive data like pool
   names, object names, object contents, or host names.
  </para>

  <para>
   The purpose of the telemetry module is to provide an automated feedback loop
   for the developers to help quantify adoption rates, tracking, or point out
   things that need to be better explained or validated during configuration to
   prevent undesirable outcomes.
  </para>

  <note>
   <para>
    The telemetry module requires the &mgr; nodes to have the ability to push
    data over HTTPS to the upstream servers. Ensure your corporate firewalls
    permit this action.
   </para>
  </note>

<!-- <para>If you would like to read more, or see some of the early findings
    of using this tool, we recommend reading <link xlink:href="https://ceph.io/community/the-first-telemetry-results-are-in/"/>.
  </para> -->

  <procedure>
   <step>
    <para>
     To enable the telemetry module:
    </para>
<screen>&prompt.cephuser;ceph mgr module enable telemetry</screen>
    <note>
     <para>
      This command only enables you to view your data locally. This command
      does not share your data with the &ceph; community.
     </para>
    </note>
   </step>
   <step>
    <para>
     To allow the telemetry module to start sharing data:
    </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
   </step>
   <step>
    <para>
     To disable telemetry data sharing:
    </para>
<screen>&prompt.cephuser;ceph telemetry off</screen>
   </step>
   <step>
    <para>
     To generate a JSON report that can be printed:
    </para>
<screen>&prompt.cephuser;ceph telemetry show</screen>
   </step>
   <step>
    <para>
     To add a contact and description to the report:
    </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/telemetry/contact ‘John Doe <literal>john.doe@example.com</literal>’
&prompt.cephuser;ceph config set mgr mgr/telemetry/description ‘My first Ceph cluster’</screen>
   </step>
   <step>
    <para>
     The module compiles and sends a new report every 24 hours by default. To
     adjust this interval:
    </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/telemetry/interval <replaceable>HOURS</replaceable></screen>
   </step>
  </procedure>

<!-- <para>For more information on the telemetry modules, see
    <link xlink:href="https://docs.ceph.com/docs/master/mgr/telemetry/"/>.</para> -->
 </sect1>
</chapter>
