<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="bp-troubleshooting-monitors">
 <title>Troubleshooting Monitors</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="mons-initial-troubleshooting">
   <title>Initial Troubleshooting</title>
   <qandaset defaultlabel='qanda'>
     <qandaentry>
       <question>
         <para>
           Are the monitors running?
         </para>
       </question>
       <answer>
         <para>
           Ensure the monitors are running, this is an important to check if
           you have performed an upgrade and not manually restarted the monitors.
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           Are you able to connect to the monitor’s servers?
         </para>
       </question>
       <answer>
         <para>
           Occasionally, you can be running iptable rules that block access
           to monitor servers or monitor ports. This can often be the case from monitor
           stress-testing that were forgotten. We recommend to try ssh’ing into
           the server and, if that succeeds, try connecting to the monitor’s
           port using your tool of choice (telnet, nc).
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           Does <command>ceph -s</command> run and obtain a reply from the cluster?
         </para>
       </question>
       <answer>
         <para>
           If the answer is yes, then your cluster is up and running. The
           monitors will only answer to a status request if there is a formed quorum.
           If <command>ceph -s</command> is blocked, without obtaining a reply
           from the cluster or showing a lot of fault messages, then it is
           possible that your monitors are either down completely or just a
           portion is up – a portion that is not enough to form a quorum
           (keep in mind that a quorum if formed by a majority of monitors).
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           What if <command>ceph -s</command> does not finish?
         </para>
       </question>
       <answer>
         <para>
            Contact each monitor individually for the status, regardless of a
            quorum being formed. This can be achieved using
            <command>ceph tell mon.ID mon_status</command> with the ID being
            the monitor’s identifier. Perform this for each monitor in the cluster.
            The section <xref linkend="mons-understanding-mons-status"/> explains
            how to interpret the output of this command.
         </para>
       </answer>
     </qandaentry>
   </qandaset>
 </sect1>
 <sect1 xml:id="mons-admin-socket">
   <title>Using the Monitor's Admin Socket</title>
   <para>
     The admin socket allows you to interact with a given daemon directly using
     a Unix socket file. This file can be found in your monitor’s run directory.
     By default, the admin socket will be kept in <filename>/var/run/ceph/ceph-mon.ID.asok</filename>
     but this can vary if you defined it otherwise. If you are unable to find it
     there, check your <filename>ceph.conf</filename> for an alternative path or run:
   </para>
<screen>ceph-conf --name mon.ID --show-config-value admin_socket</screen>
   <para>
     Keep in mind that the admin socket is only available while the monitor is
     running. When the monitor is properly shutdown, the admin socket is
     removed. If however the monitor is not running and the admin socket
     still persists, it is likely that the monitor was improperly shutdown.
     Regardless, if the monitor is not running, you will not be able to use
     the admin socket, with ceph likely returning <literal>Error 111: Connection Refused</literal>.
     To accessing the admin socket run <command>ceph tell</command> on the
     daemon you are interested in. For example:
   </para>
<screen>ceph tell mon.<replaceable>ID</replaceable> mon_status</screen>
   <para>
     This passes the command help to the running MON daemon <replaceable>ID</replaceable>
     via the admin socket, which is a file ending in <filename>.asok</filename>
     somewhere under <filename>/var/run/ceph</filename>. When you know the
     full path to the file, you can run the following:
   </para>
<screen>ceph --admin-daemon <replaceable>PATH_TO_FILE</replaceable> <replaceable>COMMAND</replaceable></screen>
   <para>
     Using help as the command to the <command>ceph tool</command> shows the
     supported commands available through the admin socket. Take a look
     at <option>config get</option>, <option>config show</option>, <option>mon stat</option>
     and <option>quorum_status</option>, as those can be enlightening when
     troubleshooting a monitor.
   </para>
 </sect1>
 <sect1 xml:id="mons-understanding-mons-status">
   <title>Understanding <literal>mons_status</literal></title>
   <para>
     <literal>mon_status</literal> can be obtained via the admin socket.
     This command outputs a multitude of information about the monitor
     including the same output you would get with <option>quorum_status</option>.
     For example, the following example output of <command>ceph tell mon.c mon_status</command>:
   </para>
<screen>
  { "name": "c",
  "rank": 2,
  "state": "peon",
  "election_epoch": 38,
  "quorum": [
        1,
        2],
  "outside_quorum": [],
  "extra_probe_peers": [],
  "sync_provider": [],
  "monmap": { "epoch": 3,
      "fsid": "5c4e9d53-e2e1-478a-8061-f543f8be4cf8",
      "modified": "2013-10-30 04:12:01.945629",
      "created": "2013-10-29 14:14:41.914786",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6795\/0"}]}}
</screen>
   <para>
     This example shows that there are three montiors in the monmap
     (<literal>a</literal>, <literal>b</literal> and <literal>c</literal>), the
     quorum is formed by only two monitors, and <literal>c</literal> is in
     the quorum as a <literal>peon</literal>. This means that monitor <literal>a</literal>
     is out of quorum. This is because there are two monitors in this set:
     1 and 2. These are not monitor names. These are monitor ranks, as
     established in the current monmap. It shows that the missing monitor is the one
     with a rank of 0, and according to the monmap that would be <literal>mon.a</literal>.
   </para>
   <para>
     Ranks (re)calculated whenever you add or remove monitors and follow this
     rule: the greater the <literal>IP:PORT</literal> combination, the lower the
     rank is. In this case, considering that <literal>127.0.0.1:6789</literal> is lower than all
     the remaining <literal>IP:PORT</literal> combinations, <literal>mon.a</literal> has rank 0.
   </para>
 </sect1>
 <sect1 xml:id="mons-common-issues">
   <title>Most Common Monitor Issues</title>
   <sect2 xml:id="monitor-down">
     <title>Have Quorum But At Least One Monitor Is Down</title>
     <para>
       When this happens, depending on the version of Ceph you are running,
       you should be seeing something similar to:
     </para>
<screen>$ ceph health detail
[snip]
mon.a (rank 0) addr 127.0.0.1:6789/0 is down (out of quorum)</screen>
     <para>
       To troubleshoot, make sure that <literal>mon.a</literal> is running. After
       that, make sure you are able to connect to <literal>mon.a</literal>'s
       server from the other monitors' servers. Check the ports as well. Check
       iptables on all your monitor nodes and make sure you are not
       dropping or rejecting connections.
       If this initial troubleshooting does not solve your problems, check
       the problematic monitor’s <literal>mon_status</literal> via the admin socket.
       Considering the monitor is out of the quorum, its state should be one
       of <literal>probing</literal>, <literal>electing</literal> or <literal>synchronizing</literal>.
       If it happens to be either <literal>leader</literal>
       or <literal>peon</literal>, then the monitor believes to be in quorum, while the
       remaining cluster is sure it is not; or maybe it got into the quorum
       while we were troubleshooting the monitor, check <command>ceph -s</command>
       again just to make sure. Continue if the monitor is not yet in quorum.
     </para>
  <qandaset>
     <qandaentry>
       <question>
         <para>
           What if the state is <literal>probing</literal>?
         </para>
       </question>
       <answer>
         <para>
           This means the monitor is still looking for the other monitors.
           Every time you start a monitor, the monitor stays in this state
           for some time while trying to find the rest of the monitors specified
           in the <literal>monmap</literal>. The time a monitor spends in this state can vary.
           For instance, when on a single-monitor cluster, the monitor passes
           through the probing state almost instantaneously, since there are no
           other monitors around. On a multi-monitor cluster, the monitors
           will stay in this state until they find enough monitors to form a
           quorum – this means that if you have 2 out of 3 monitors down, the
           one remaining monitor stays in this state indefinitely until
           one of the other monitors is brought up manually.
         </para>
         <para>
           If there is a quorum, the monitor should be able to find the
           remaining monitors as long as they can be reached. If your
           monitor is stuck probing and you have gone through with all the
           communication troubleshooting, then there is a chance that the
           monitor is trying to reach the other monitors on a wrong address.
           <literal>mon_status</literal> outputs the <literal>monmap</literal> known to the monitor
           and checks if the other monitor’s locations match reality. If they do not, jump to
           <xref linkend="broken-mon-map"/>. If they do, then it may be
           related to severe clock skews amongst the monitor nodes and you should
           refer to <xref linkend="clock-skews"/>.
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           What if the state is <literal>electing</literal>?
         </para>
       </question>
       <answer>
         <para>
           This means the monitor is in the middle of an election. These should
           be fast to complete, but at times the monitors can get stuck electing.
           This is usually a sign of a clock skew among the monitor nodes.
           See <xref linkend="clock-skews"/> for more information. This is not a
           state that is likely to persist and aside from old bugs there is not
           an obvious reason besides clock skews on why this would happen.
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           What if the state is <literal>synchronizing</literal>?
         </para>
       </question>
       <answer>
         <para>
           This means the monitor is synchronizing with the rest of the cluster
           in order to join the quorum.
           However, if you notice that the monitor jumps from synchronizing to
           electing and then back to synchronizing, then it can mean that
           the cluster state is advancing (i.e., generating new maps) way too
           fast for the synchronization process to keep up.
         </para>
       </answer>
     </qandaentry>
     <qandaentry>
       <question>
         <para>
           What if the state is <literal>leader</literal> or <literal>peon</literal>?
         </para>
       </question>
       <answer>
         <para>
           This should not happen. If this does happen, it is likely related
           to clock skews, see <xref linkend="clock-skews"/> for more information.
           If you see no issue with the clock skews, prepare your logs
           and reach out to your support representative.
         </para>
       </answer>
     </qandaentry>
   </qandaset>
   </sect2>
   <sect2 xml:id="broken-mon-map">
     <title>Recovering a Monitor's Broken <literal>monmap</literal></title>
     <para>
       A typical <literal>monmap</literal> looks like the following depending
       on the number of monitors:
     </para>
<screen>
epoch 3
fsid 5c4e9d53-e2e1-478a-8061-f543f8be4cf8
last_changed 2013-10-30 04:12:01.945629
created 2013-10-29 14:14:41.914786
0: 127.0.0.1:6789/0 mon.a
1: 127.0.0.1:6790/0 mon.b
2: 127.0.0.1:6795/0 mon.c
</screen>
     <para>
       If your <literal>monmap</literal> does not look like this, you have two
       possible solutions:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Scrap the monitor and create a new one
         </para>
         <important>
           <para>
             You should only take this route if you are certain of zero data loss for
             that monitor.
           </para>
         </important>
       </listitem>
       <listitem>
         <para>
           Inject a <literal>monmap</literal> into the monitor
         </para>
       </listitem>
     </itemizedlist>
     <para>
       Injecting a <literal>monmap</literal> into the monitor is the safest
       path. Take the <literal>monmap</literal> from the remaining monitors and inject it into
       the monitor with the corrupted or lost monmap.
     </para>
     <procedure>
       <step>
         <para>
           Is there a formed quorum? If so, grab the monmap from the quorum:
         </para>
<screen>$ ceph mon getmap -o /tmp/monmap</screen>
       </step>
       <step>
         <para>
           No quorum? Grab the monmap directly from another monitor (this
           assumes the monitor you are grabbing the monmap from has ID
           ID-FOO and has been stopped):
         </para>
<screen>$ ceph-mon -i ID-FOO --extract-monmap /tmp/monmap</screen>
       </step>
       <step>
         <para>
           Stop the monitor you are going to inject the monmap into.
         </para>
       </step>
       <step>
         <para>
           Inject the monmap:
         </para>
<screen>$ ceph-mon -i ID --inject-monmap /tmp/monmap</screen>
       </step>
       <step>
         <para>
           Start the monitor.
         </para>
       </step>
     </procedure>
     <note>
       <para>
         Keep in mind that the ability to inject monmaps is a powerful feature
         that can cause havoc with your monitors. If misused it will
         overwrite the latest existing monmap kept by the monitor.
       </para>
     </note>
   </sect2>
   <sect2 xml:id="clock-skews">
     <title>Clock Skews</title>
     <para>
       Monitors can be severely affected by significant clock skews across
       the monitor nodes. This usually translates into weird behavior with
       no obvious cause. To avoid such issues, run a clock
       synchronization tool on your monitor nodes.
     </para>
     <para>
       By defult, the maximum tolerated clock skew allows clocks to drift
       up to 0.05 seconds. This value is configurable via the
       <option>mon-clock-drift-allowed</option> option, however we do not
       recommend doing this. The clock skew mechanism is in place because
       clock skewed monitor may not properly behave.  Changing this value
       without testing it first may cause unforeseen effects on the
       stability of the monitors and overall cluster healthiness,
       although there is no risk of data loss.
     </para>
     <para>
       The monitors will warn you if there is a clock skew by sending a
       <literal>HEALTH_WARN</literal> alert. Run the <command>ceph health detail</command>
       command to determine what monitor is flagging a clock skew. For example:
     </para>
<screen>mon.c addr 10.10.0.1:6789/0 clock skew 0.08235s > max 0.05s (latency 0.0045s)</screen>
     <para>
       If you have a clock skew, synchronize your clocks. Running an NTP client
       may help. If you are already using one and you hit this sort of issue,
       check if you are using an NTP server remote to your network and consider
       hosting your own NTP server on your network. This last option tends
       to reduce the amount of issues with monitor clock skews.
     </para>
   </sect2>
   <sect2 xml:id="client-connect-mount">
     <title>Client Cannot Connect or Mount</title>
     <para>
       Check your iptables. Some OS install utilities add a <option>REJECT</option> rule to
       iptables. The rule rejects all clients trying to connect to the host
       except for SSH. If your monitor host’s IP tables have a <option>REJECT</option>
       rule in place, clients connecting from a separate node will fail to mount
       with a timeout error. You need to address iptables rules that reject
       clients trying to connect to Ceph daemons. For example, address rules
       that look like similar to this:
     </para>
<screen>REJECT all -- anywhere anywhere reject-with icmp-host-prohibited</screen>
     <para>
       You may also need to add rules to iptables on your Ceph hosts to ensure
       that clients can access the ports associated with your Ceph monitors
       (i.e., port 6789 by default) and Ceph OSDs (i.e., 6800 through 7300 by default).
       For example:
     </para>
<screen>iptables -A INPUT -m multiport -p tcp -s {ip-address}/{netmask} --dports 6789,6800:7300 -j ACCEPT</screen>
   </sect2>
 </sect1>
 <sect1 xml:id="mons-store-failures">
   <title>Monitor Store Failures</title>
   <sect2 xml:id="store-corruption">
     <title>Symptoms of Store Corruption</title>
     <para>
       Ceph monitor stores the cluster map in a key/value store such
       as LevelDB. If a monitor fails due to the key/value store
       corruption, following error messages might be found in the monitor log:
     </para>
<screen>Corruption: error in middle of record</screen>
     <para>
       Or:
     </para>
<screen>Corruption: 1 missing files; e.g.: /var/lib/ceph/mon/mon.foo/store.db/1234567.ldb</screen>
   </sect2>
   <sect2 xml:id="healthy-monitor-recovery">
     <title>Recovery Using Healthy Monitors</title>
     <para>
       If there are any survivors, replace the corrupted one with a new one.
       After booting up, the new joiner will sync up with a healthy peer,
       and once it is fully synchronized, it will be able to serve the clients.
     </para>
   </sect2>
   <sect2 xml:id="recovery-using-osds">
     <title>Recovery Using OSDs</title>
     <para>
       But what if all monitors fail at the same time? Since users are
       encouraged to deploy at least three (and preferably five) monitors in a
       Ceph cluster, the chance of simultaneous failure is rare. But
       unplanned power-downs in a data center with improperly configured
       disk/fs settings could fail the underlying file system, and hence
       kill all the monitors. In this case, we can recover the monitor
       store with the information stored in OSDs.
     </para>
<screen>
  ms=/root/mon-store
  mkdir $ms

  # collect the cluster map from stopped OSDs
  for host in $hosts; do
    rsync -avz $ms/. user@$host:$ms.remote
    rm -rf $ms
    ssh user@$host EOF
      for osd in /var/lib/ceph/osd/ceph-*; do
        ceph-objectstore-tool --data-path \$osd --no-mon-config --op update-mon-db --mon-store-path $ms.remote
      done
  EOF
    rsync -avz user@$host:$ms.remote/. $ms
  done

  # rebuild the monitor store from the collected map, if the cluster does not
  # use cephx authentication, we can skip the following steps to update the
  # keyring with the caps, and there is no need to pass the "--keyring" option.
  # i.e. just use "ceph-monstore-tool $ms rebuild" instead
  ceph-authtool /path/to/admin.keyring -n mon. \
    --cap mon 'allow *'
  ceph-authtool /path/to/admin.keyring -n client.admin \
    --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *'
  # add one or more ceph-mgr's key to the keyring. in this case, an encoded key
  # for mgr.x is added, you can find the encoded key in
  # /etc/ceph/${cluster}.${mgr_name}.keyring on the machine where ceph-mgr is
  # deployed
  ceph-authtool /path/to/admin.keyring --add-key 'AQDN8kBe9PLWARAAZwxXMr+n85SBYbSlLcZnMA==' -n mgr.x \
    --cap mon 'allow profile mgr' --cap osd 'allow *' --cap mds 'allow *'
  # if your monitors' ids are not single characters like 'a', 'b', 'c', please
  # specify them in the command line by passing them as arguments of the "--mon-ids"
  # option. if you are not sure, please check your ceph.conf to see if there is any
  # sections named like '[mon.foo]'. don't pass the "--mon-ids" option, if you are
  # using DNS SRV for looking up monitors.
  ceph-monstore-tool $ms rebuild -- --keyring /path/to/admin.keyring --mon-ids alpha beta gamma

  # make a backup of the corrupted store.db just in case!  repeat for
  # all monitors.
  mv /var/lib/ceph/mon/mon.foo/store.db /var/lib/ceph/mon/mon.foo/store.db.corrupted

  # move rebuild store.db into place.  repeat for all monitors.
  mv $ms/store.db /var/lib/ceph/mon/mon.foo/store.db
  chown -R ceph:ceph /var/lib/ceph/mon/mon.foo/store.db
</screen>
     <procedure>
       <step>
         <para>
           Collect the map from all OSD hosts.
         </para>
       </step>
       <step>
         <para>
           Rebuild the store.
         </para>
       </step>
       <step>
         <para>
           Fill the entities in the keyring file with appropriate caps.
         </para>
       </step>
       <step>
         <para>
           Replace the corrupted store on <literal>mon.foo</literal> with the
           recovered copy.
         </para>
       </step>
     </procedure>
     <sect3>
       <title>Known Limitations</title>
       <para>
         The following information is not recoverable using the steps above:
       </para>
       <itemizedlist>
         <listitem>
           <para>
             Some added keyrings: all the OSD keyrings added using the <command>ceph auth
             add</command> command are recovered from the OSD’s copy. The
             <literal>client.admin</literal> keyring is imported using <literal>ceph-monstore-tool</literal>.
             The MDS keyrings and other keyrings are missing in the
             recovered monitor store. You may need to re-add them manually.
           </para>
         </listitem>
         <listitem>
           <para>
             Creating pools: If any RADOS pools were in the process of being
             creating, that state is lost. The recovery tool assumes that all
             pools have been created. If there are PGs that are stuck in the
             <literal>unknown</literal> state after the recovery for a partially created pool, you
             can force creation of the empty PG with the <command>ceph osd force-create-pg</command>
             command. This will create an empty PG, so only do this
             if you know the pool is empty.
           </para>
         </listitem>
         <listitem>
           <para>
             MDS Maps: the MDS maps are lost.
           </para>
         </listitem>
       </itemizedlist>
     </sect3>
   </sect2>
 </sect1>
 <sect1 xml:id ="next-steps">
   <title>Next Steps</title>
   <sect2 xml:id="preparing-logs-mons">
     <title>Preparing Your Logs</title>
     <para>
       Monitor logs are, by default, kept in <filename>/var/log/ceph/ceph-mon.FOO.log*</filename>.
       However, your logs may not have the necessary information. If you do not
       find your monitor logs at their default location, you can check where
       they are by running:
     </para>
<screen>ceph-conf --name mon.FOO --show-config-value log_file</screen>
     <para>
       The amount of information in the logs are subject to the debug levels
       being enforced by your configuration files. If you have not enforced
       a specific debug level, then Ceph is using the default levels and your
       logs may not contain important information to track down you issue.
       A first step in getting relevant information into your logs will be
       to raise debug levels. Similarly to what happens on other
       components, different parts of the monitor will output their debug
       information on different subsystems.
       You will have to raise the debug levels of those subsystems more
       closely related to your issue. For most situations, setting the
       following options on your monitors will be enough to pinpoint a
       potential source of the issue:
     </para>
<screen>
debug mon = 10
debug ms = 1
</screen>
   </sect2>
   <sect2 xml:id="restart-adjust-debug">
     <title>Do I Need to Restart a Monitor to Adjust Debug Levels?</title>
     <para>
       No. You may do it in one of two ways:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           If you have a quorum, either inject the debug option into the
           monitor you want to debug:
         </para>
<screen>ceph tell mon.FOO config set debug_mon 10/10</screen>
         <para>
           Or into all monitors at once:
         </para>
<screen>ceph tell mon.* config set debug_mon 10/10</screen>
       </listitem>
       <listitem>
         <para>
           If you have no quorum, use the monitor’s admin socket and
           directly adjust the configuration options:
         </para>
<screen>ceph daemon mon.FOO config set debug_mon 10/10</screen>
       </listitem>
     </itemizedlist>
       <para>
         Going back to default values is as easy as rerunning the above
         commands using the debug level 1/10 instead. You can check your
         current values using the admin socket and the following commands:
       </para>
<screen>ceph daemon mon.FOO config show</screen>
       <para>
         Or:
       </para>
<screen>ceph daemon mon.FOO config get 'OPTION_NAME'</screen>
   </sect2>
 </sect1>
</chapter>
