<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-operating">
 <title>Operating &ceph; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  You can operate all &ceph; services and targets using the standard
  <command>systemctl</command> command. For more detailed information about
  &systemd;, refer to
  <link xlink:href="https://documentation.suse.com/sles/15-SP2/single-html/SLES-admin/#cha-systemd"/>
 </para>
 <sect1 xml:id="ceph-operating-services-finding-names">
  <title>Identifying &ceph; Services and Targets</title>

  <para>
   Before operating &ceph; services and targets, you need to identify the file
   names of their unit files. File names of the services have the following
   pattern:
  </para>

<screen>ceph-<replaceable>FSID</replaceable>@<replaceable>SERVICE_TYPE</replaceable>.<replaceable>ID</replaceable>.service</screen>

  <para>
   For example:
  </para>

<screen>ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@mon.doc-ses-min1.service</screen>

<screen>ceph-b4b30c6e-9681-11ea-ac39-525400d7702d@rgw.myrealm.myzone.doc-ses-min1.kwwazo.service</screen>

  <variablelist>
   <varlistentry>
    <term>FSID</term>
    <listitem>
     <para>
      Unique ID of the &ceph; cluster. You can find it in
      <filename>/etc/ceph/ceph.conf</filename>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SERVICE_TYPE</term>
    <listitem>
     <para>
      Type of the service, for example <literal>osd</literal>,
      <literal>mon</literal>, or <literal>rgw</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>ID</term>
    <listitem>
     <para>
      Identification string of the service. For OSDs, it is the ID number of
      the service. For other services, it can be either a host name of the
      node, or additional strings relevant for the service type.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   The following commands will help you list the unit files specific to the
   node where you run them.
  </para>

  <para>
   To list all (even inactive) &ceph; services and targets on a node, run:
  </para>

<screen>
&prompt.root;systemctl list-units --all --type=service,target ceph*
</screen>

  <para>
   To list only the inactive services, run:
  </para>

<screen>
&prompt.root;systemctl list-units --all --state=inactive --type=service ceph*
</screen>

  <tip>
   <title>Listing Services and Targets across Multiple Nodes</title>
   <para>
    You can also use <command>salt</command> to query services across multiple
    nodes:
   </para>
<screen>
&prompt.smaster;salt <replaceable>TARGET</replaceable> cmd.shell \
 "systemctl list-units --all --type=service ceph* | sed -e '/^$/,$ d'"
</screen>
<!--
   <para>
    Query storage nodes only:
   </para>
<screen>
&prompt.smaster;salt -I 'roles:storage' cmd.shell \
 'systemctl list-units \-\-all \-\-type=service ceph*'
</screen>
-->
  </tip>
 </sect1>
 <sect1 xml:id="operate-services-systemd">
  <title>Operating &ceph; Cluster Related Services Using &systemd;</title>

  <para>
   Use the <command>systemctl</command> command to operate all &ceph; related
   services. The operation takes place on the node you are currently logged in
   to. You need to have &rootuser; privileges to be able to operate on &ceph;
   services.
  </para>

  <sect2 xml:id="ceph-operating-services-targets">
   <title>Starting, Stopping, and Restarting Services Using Targets</title>
   <para>
    To simplify starting, stopping, and restarting all the services of a
    particular type (for example all &ceph; services, or all MONs, or all OSDs)
    on a node, &ceph; provides the following &systemd; unit files:
   </para>
<screen>&prompt.cephuser;ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</screen>
   <para>
    To start/stop/restart all &ceph; services on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph.target
&prompt.root;systemctl stop ceph.target
&prompt.root;systemctl restart ceph.target
</screen>
   <para>
    To start/stop/restart all OSDs on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd.target
&prompt.root;systemctl stop ceph-osd.target
&prompt.root;systemctl restart ceph-osd.target
</screen>
   <para>
    Commands for the other targets are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph-operating-services-individual">
   <title>Starting, Stopping, and Restarting Individual Services</title>
   <para>
    You can operate individual services using the following parameterized
    &systemd; unit files:
   </para>
<screen>
ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-mgr@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service
</screen>
   <para>
    To use these commands, you first need to identify the name of the service
    you want to operate. See
    <xref linkend="ceph-operating-services-finding-names"/> to learn more about
    services identification.
   </para>
   <para>
    To start, stop or restart the <literal>osd.1</literal> service, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd@1.service
&prompt.root;systemctl stop ceph-osd@1.service
&prompt.root;systemctl restart ceph-osd@1.service
</screen>
   <para>
    Commands for the other service types are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph-operating-services-status">
   <title>Service Status</title>
   <para>
    You can query &systemd; for the status of services. For example:
   </para>
<screen>&prompt.root;systemctl status ceph-osd@1.service
&prompt.root;systemctl status ceph-mon@<replaceable>HOSTNAME</replaceable>.service</screen>
   <para>
    Replace <replaceable>HOSTNAME</replaceable> with the host name the daemon
    is running on.
   </para>
   <para>
    If you do not know the exact name/number of the service, see
    <xref linkend="ceph-operating-services-finding-names"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="Deepsea-restart">
  <title>Restarting &ceph; Services Using &deepsea;</title>

  <para>
   After applying updates to the cluster nodes, the affected &ceph; related
   services need to be restarted. Normally, restarts are performed
   automatically by &deepsea;. This section describes how to restart the
   services manually.
  </para>

  <tip>
   <title>Watching the Restart</title>
   <para>
    The process of restarting the cluster may take some time. You can watch the
    events by using the &salt; event bus by running:
   </para>
<screen>&prompt.smaster;salt-run state.event pretty=True</screen>
   <para>
    Another command to monitor active jobs is
   </para>
<screen>
&prompt.smaster;salt-run jobs.active
</screen>
  </tip>

  <sect2 xml:id="deepsea-restart-all">
   <title>Restarting All Services</title>
   <warning>
    <title>Interruption of Services</title>
    <para>
     If &ceph; related services&mdash;specifically &iscsi; or
     &ganesha;&mdash;are configured as single points of access with no &ha;
     setup, restarting them will result in their temporary outage as viewed
     from the client side.
    </para>
   </warning>
   <tip>
    <title>Samba Not Managed by &deepsea;</title>
    <para>
     Because &deepsea; and the &dashboard; do not currently support &samba;
     deployments, you need to manage &samba; related services manually. For
     more details, see <xref linkend="cha-ses-cifs"/>.
    </para>
   </tip>
   <para>
    To restart <emphasis>all</emphasis> services on the cluster, run the
    following command:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
   <itemizedlist>
    <listitem>
     <para>
      For &deepsea; prior to version 0.8.4, the &mds;, &igw;, &ogw;, and
      &ganesha; services restart in parallel.
     </para>
    </listitem>
    <listitem>
     <para>
      For &deepsea; 0.8.4 and newer, all roles you have configured restart in
      the following order: &mon;, &mgr;, &osd;, &mds;, &rgw;, &igw;, &ganesha;.
      To keep the downtime low and to find potential issues as early as
      possible, nodes are restarted sequentially. For example, only one
      monitoring node is restarted at a time.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The command waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </para>
  </sect2>

  <sect2 xml:id="deepsea-restart-specific">
   <title>Restarting Specific Services</title>
   <para>
    To restart a specific service on the cluster, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.<replaceable>service_name</replaceable></screen>
   <para>
    For example, to restart all &rgw;s, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
   <para>
    You can use the following targets:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mon</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mgr</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.osd</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mds</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.igw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.ganesha</screen>
   <para>
    The restart orchestration checks if the installated binary is newer than
    the current one, or if configuration changes exist for this daemon and only
    restarts in those cases. If you run the above command and nothing happens,
    this is due to these conditions. See
    <xref linkend="ceph-operating-services-individual"/> for more information.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cluster-shutdown">
  <title>Shutdown and Restart of the Whole &ceph; Cluster</title>

  <para>
   Shutting down and restarting the cluster may be necessary in the case of a
   planned power outage. To stop all &ceph; related services and restart
   without issue, follow the steps below.
  </para>

  <procedure>
   <title>Shutting Down the Whole &ceph; Cluster</title>
   <step>
    <para>
     Shut down or disconnect any clients accessing the cluster.
    </para>
   </step>
   <step>
    <para>
     To prevent CRUSH from automatically rebalancing the cluster, set the
     cluster to <literal>noout</literal>:
    </para>
<screen>&prompt.smaster;ceph osd set noout</screen>
   </step>
   <step>
    <para>
     Disable safety measures and run the <command>ceph.shutdown</command>
     runner:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.shutdown
</screen>
   </step>
   <step>
    <para>
     Power off all cluster nodes:
    </para>
<screen>&prompt.smaster;salt -C 'G@deepsea:*' cmd.run "shutdown -h"</screen>
   </step>
  </procedure>

  <procedure>
   <title>Starting the Whole &ceph; Cluster</title>
   <step>
    <para>
     Power on the &adm;.
    </para>
   </step>
   <step>
    <para>
     Power on the &mon; nodes.
    </para>
   </step>
   <step>
    <para>
     Power on the &osd; nodes.
    </para>
   </step>
   <step>
    <para>
     Unset the previously set <literal>noout</literal> flag:
    </para>
<screen>&prompt.smaster;ceph osd unset noout</screen>
   </step>
   <step>
    <para>
     Power on all configured gateways.
    </para>
   </step>
   <step>
    <para>
     Power on or connect cluster clients.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
