<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph.operating.services">
 <title>Operating &ceph; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  You can operate &ceph; services either using &systemd;, or using &deepsea;.
 </para>
 <sect1 xml:id="operate.services.systemd">
  <title>Operating &ceph; Cluster Related Services using &systemd;</title>

  <para>
   Use the <command>systemctl</command> command to operate all &ceph; related
   services. The operation takes place on the node you are currently logged in
   to. You need to have &rootuser; privileges to be able to operate on &ceph;
   services.
  </para>

  <sect2 xml:id="ceph.operating.services.targets">
   <title>Starting, Stopping, and Restarting Services using Targets</title>
   <para>
    To simplify starting, stopping, and restarting all the services of a
    particular type (for example all &ceph; services, or all MONs, or all OSDs)
    on a node, &ceph; provides the following &systemd; unit files:
   </para>
<screen>&prompt.cephuser;ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</screen>
   <para>
    To start/stop/restart all &ceph; services on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph.target
&prompt.root;systemctl stop ceph.target
&prompt.root;systemctl restart ceph.target
</screen>
   <para>
    To start/stop/restart all OSDs on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd.target
&prompt.root;systemctl stop ceph-osd.target
&prompt.root;systemctl restart ceph-osd.target
</screen>
   <para>
    Commands for the other targets are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph.operating.services.individual">
   <title>Starting, Stopping, and Restarting Individual Services</title>
   <para>
    You can operate individual services using the following parameterized
    &systemd; unit files:
   </para>
<screen>
ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-mgr@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service
</screen>
   <para>
    To use these commands, you first need to identify the name of the service
    you want to operate. See
    <xref linkend="ceph.operating.services.finding_names"/> to learn more about
    services identification.
   </para>
   <para>
    To start/stop/restart the <literal>osd.1</literal> service, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd@1.service
&prompt.root;systemctl stop ceph-osd@1.service
&prompt.root;systemctl restart ceph-osd@1.service
</screen>
   <para>
    Commands for the other service types are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph.operating.services.finding_names">
   <title>Identifying Individual Services</title>
   <para>
    You can find out the names/numbers of a particular type of service in
    several ways. The following commands provide results for
    <literal>ceph*</literal> services. You can run them on any node of the
    &ceph; cluster.
   </para>
   <para>
    To list all (even inactive) services of type <literal>ceph*</literal>, run:
   </para>
<screen>
&prompt.root;systemctl list-units --all --type=service ceph*
</screen>
   <para>
    To list only the inactive services, run:
   </para>
<screen>
&prompt.root;systemctl list-units --all --state=inactive --type=service ceph*
</screen>
   <para>
    You can also use <command>salt</command> to query services across multiple
    nodes:
   </para>
<screen>
&prompt.smaster;salt <replaceable>TARGET</replaceable> cmd.shell \
 "systemctl list-units --all --type=service ceph* | sed -e '/^$/,$ d'"
</screen>
   <para>
    Query storage nodes only:
   </para>
<screen>
&prompt.smaster;salt -I 'roles:storage' cmd.shell \
 'systemctl list-units --all --type=service ceph*'
</screen>
  </sect2>

  <sect2 xml:id="ceph.operating.services.status">
   <title>Service Status</title>
   <para>
    You can query &systemd; for the status of services. For example:
   </para>
<screen>&prompt.root;systemctl status ceph-osd@1.service
&prompt.root;systemctl status ceph-mon@<replaceable>HOSTNAME</replaceable>.service</screen>
   <para>
    Replace <replaceable>HOSTNAME</replaceable> with the host name the daemon
    is running on.
   </para>
   <para>
    If you do not know the exact name/number of the service, see
    <xref linkend="ceph.operating.services.finding_names"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="Deepsea.restart">
  <title>Restarting &ceph; Services using &deepsea;</title>

  <para>
   After applying updates to the cluster nodes, the affected &ceph; related
   services need to be restarted. Normally, restarts are performed
   automatically by &deepsea;. This section describes how to restart the
   services manually.
  </para>

  <tip>
   <title>Watching the Restart</title>
   <para>
    The process of restarting the cluster may take some time. You can watch the
    events by using the &salt; event bus by running:
   </para>
<screen>&prompt.smaster;salt-run state.event pretty=True</screen>
   <para>
    Another command to monitor active jobs is
   </para>
<screen>
&prompt.smaster;salt-run jobs.active
</screen>
  </tip>

  <sect2 xml:id="deepsea.restart.all">
   <title>Restarting All Services</title>
   <warning>
    <title>Interruption of Services</title>
    <para>
     If &ceph; related services&mdash;specifically &iscsi; or
     &ganesha;&mdash;are configured as single points of access with no &ha;
     setup, restarting then will result in their temporary outage as viewed
     from the client side.
    </para>
   </warning>
   <tip>
    <title>Samba not Managed by &deepsea;</title>
    <para>
     Because &deepsea; and the &dashboard; do not currently support &samba;
     deployments, you need to manage &samba; related services manually. For
     more details, see <xref linkend="cha.ses.cifs"/>.
    </para>
   </tip>
   <para>
    To restart <emphasis>all</emphasis> services on the cluster, run the
    following command:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
   <para>
    All roles you have configured restart in the following order: &mon;, &mgr;,
    &osd;, &mds;, &rgw;, &igw;, &ganesha;. To keep the downtime low and to find
    potential issues as early as possible, nodes are restarted sequentially.
    For example, only one monitoring node is restarted at a time.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      For &deepsea; prior to version 0.8.4, the &mds;, &igw;, &ogw;, and
      &ganesha; services restart in parallel.
     </para>
    </listitem>
    <listitem>
     <para>
      For &deepsea; 0.8.4 and newer, all roles you have configured restart in
      the following order: &mon;, &mgr;, &osd;, &mds;, &rgw;, &igw;, &ganesha;.
      To keep the downtime low and to find potential issues as early as
      possible, nodes are restarted sequentially. For example, only one
      monitoring node is restarted at a time.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The command waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </para>
  </sect2>

  <sect2 xml:id="deepsea.restart.specific">
   <title>Restarting Specific Services</title>
   <para>
    To restart a specific service on the cluster, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.<replaceable>service_name</replaceable></screen>
   <para>
    For example, to restart all &rgw;s, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
   <para>
    You can use the following targets:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mon</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mgr</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.osd</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mds</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.igw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.ganesha</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cluster.shutdown">
  <title>Graceful Shutdown of the Whole &ceph; Cluster</title>

  <para>
   There are occasions when you need to stop all &ceph; related services in the
   cluster in the recommended order, and then be able to simply start them
   again. For example in case of a planned power outage.
  </para>

  <para>
   To shut down the whole &ceph; cluster, disable safety measures and run the
   <command>ceph.shutdown</command> runner:
  </para>

<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.shutdown
</screen>

  <para>
   To start the whole &ceph; cluster, run the <command>ceph.startup</command>
   runner:
  </para>

<screen>
&prompt.smaster;salt-run state.orch ceph.startup
</screen>
 </sect1>
</chapter>
