<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="ceph-operating-services">
 <title>Operating &ceph; Services</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  You can operate &ceph; services either using &systemd; or using &deepsea;.
 </para>
 <sect1 xml:id="operate-services-systemd">
  <title>Operating &ceph; Cluster Related Services Using &systemd;</title>

  <para>
   Use the <command>systemctl</command> command to operate all &ceph; related
   services. The operation takes place on the node you are currently logged in
   to. You need to have &rootuser; privileges to be able to operate on &ceph;
   services.
  </para>

  <sect2 xml:id="ceph-operating-services-targets">
   <title>Starting, Stopping, and Restarting Services Using Targets</title>
   <para>
    To simplify starting, stopping, and restarting all the services of a
    particular type (for example all &ceph; services, or all MONs, or all OSDs)
    on a node, &ceph; provides the following &systemd; unit files:
   </para>
<screen>&prompt.cephuser;ls /usr/lib/systemd/system/ceph*.target
ceph.target
ceph-osd.target
ceph-mon.target
ceph-mgr.target
ceph-mds.target
ceph-radosgw.target
ceph-rbd-mirror.target</screen>
   <para>
    To start/stop/restart all &ceph; services on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph.target
&prompt.root;systemctl stop ceph.target
&prompt.root;systemctl restart ceph.target
</screen>
   <para>
    To start/stop/restart all OSDs on the node, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd.target
&prompt.root;systemctl stop ceph-osd.target
&prompt.root;systemctl restart ceph-osd.target
</screen>
   <para>
    Commands for the other targets are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph-operating-services-individual">
   <title>Starting, Stopping, and Restarting Individual Services</title>
   <para>
    You can operate individual services using the following parameterized
    &systemd; unit files:
   </para>
<screen>
ceph-osd@.service
ceph-mon@.service
ceph-mds@.service
ceph-mgr@.service
ceph-radosgw@.service
ceph-rbd-mirror@.service
</screen>
   <para>
    To use these commands, you first need to identify the name of the service
    you want to operate. See
    <xref linkend="ceph-operating-services-finding-names"/> to learn more about
    services identification.
   </para>
   <para>
    To start/stop/restart the <literal>osd.1</literal> service, run:
   </para>
<screen>
&prompt.root;systemctl start ceph-osd@1.service
&prompt.root;systemctl stop ceph-osd@1.service
&prompt.root;systemctl restart ceph-osd@1.service
</screen>
   <para>
    Commands for the other service types are analogous.
   </para>
  </sect2>

  <sect2 xml:id="ceph-operating-services-finding-names">
   <title>Identifying Individual Services</title>
   <para>
    You can find out the names/numbers of a particular type of service in
    several ways. The following commands provide results for
    <literal>ceph*</literal> services. You can run them on any node of the
    &ceph; cluster.
   </para>
   <para>
    To list all (even inactive) services of type <literal>ceph*</literal>, run:
   </para>
<screen>
&prompt.root;systemctl list-units --all --type=service ceph*
</screen>
   <para>
    To list only the inactive services, run:
   </para>
<screen>
&prompt.root;systemctl list-units --all --state=inactive --type=service ceph*
</screen>
   <para>
    You can also use <command>salt</command> to query services across multiple
    nodes:
   </para>
<screen>
&prompt.smaster;salt <replaceable>TARGET</replaceable> cmd.shell \
 "systemctl list-units --all --type=service ceph* | sed -e '/^$/,$ d'"
</screen>
   <para>
    Query storage nodes only:
   </para>
<screen>
&prompt.smaster;salt -I 'roles:storage' cmd.shell \
 'systemctl list-units --all --type=service ceph*'
</screen>
  </sect2>

  <sect2 xml:id="ceph-operating-services-status">
   <title>Service Status</title>
   <para>
    You can query &systemd; for the status of services. For example:
   </para>
<screen>&prompt.root;systemctl status ceph-osd@1.service
&prompt.root;systemctl status ceph-mon@<replaceable>HOSTNAME</replaceable>.service</screen>
   <para>
    Replace <replaceable>HOSTNAME</replaceable> with the host name the daemon
    is running on.
   </para>
   <para>
    If you do not know the exact name/number of the service, see
    <xref linkend="ceph-operating-services-finding-names"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="Deepsea-restart">
  <title>Restarting &ceph; Services Using &deepsea;</title>

  <para>
   After applying updates to the cluster nodes, the affected &ceph; related
   services need to be restarted. Normally, restarts are performed
   automatically by &deepsea;. This section describes how to restart the
   services manually.
  </para>

  <tip>
   <title>Watching the Restart</title>
   <para>
    The process of restarting the cluster may take some time. You can watch the
    events by using the &salt; event bus by running:
   </para>
<screen>&prompt.smaster;salt-run state.event pretty=True</screen>
   <para>
    Another command to monitor active jobs is
   </para>
<screen>
&prompt.smaster;salt-run jobs.active
</screen>
  </tip>

  <sect2 xml:id="deepsea-restart-all">
   <title>Restarting All Services</title>
   <warning>
    <title>Interruption of Services</title>
    <para>
     If &ceph; related services&mdash;specifically &iscsi; or
     &ganesha;&mdash;are configured as single points of access with no &ha;
     setup, restarting them will result in their temporary outage as viewed
     from the client side.
    </para>
   </warning>
   <tip>
    <title>Samba Not Managed by &deepsea;</title>
    <para>
     Because &deepsea; and the &dashboard; do not currently support &samba;
     deployments, you need to manage &samba; related services manually. For
     more details, see <xref linkend="cha-ses-cifs"/>.
    </para>
   </tip>
   <para>
    To restart <emphasis>all</emphasis> services on the cluster, run the
    following command:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart</screen>
   <itemizedlist>
    <listitem>
     <para>
      For &deepsea; prior to version 0.8.4, the &mds;, &igw;, &ogw;, and
      &ganesha; services restart in parallel.
     </para>
    </listitem>
    <listitem>
     <para>
      For &deepsea; 0.8.4 and newer, all roles you have configured restart in
      the following order: &mon;, &mgr;, &osd;, &mds;, &rgw;, &igw;, &ganesha;.
      To keep the downtime low and to find potential issues as early as
      possible, nodes are restarted sequentially. For example, only one
      monitoring node is restarted at a time.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The command waits for the cluster to recover if the cluster is in a
    degraded, unhealthy state.
   </para>
  </sect2>

  <sect2 xml:id="deepsea-restart-specific">
   <title>Restarting Specific Services</title>
   <para>
    To restart a specific service on the cluster, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.<replaceable>service_name</replaceable></screen>
   <para>
    For example, to restart all &rgw;s, run:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
   <para>
    You can use the following targets:
   </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mon</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mgr</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.osd</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.mds</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.igw</screen>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.ganesha</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cluster-shutdown">
  <title>Shutdown and Restart of the Whole &ceph; Cluster</title>

  <para>
   Shutting down and restarting the cluster may be necessary such as in the
   case of a planned power outage.  To stop all &ceph; related services and
   restart without issue, follow the steps below.
  </para>

  <procedure>
   <title>Shutting Down the Whole &ceph; Cluster</title>
   <step>
    <para>
     Shut down or disconnect any clients accessing the cluster.
    </para>
   </step>
   <step>
    <para>
     To prevent CRUSH from automatically rebalancing the cluster, set the
     cluster to <literal>noout</literal>:
    </para>
<screen>&prompt.smaster;ceph osd set noout</screen>
   </step>
   <step>
    <para>
     Disable safety measures and run the <command>ceph.shutdown</command> runner:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
&prompt.smaster;salt-run state.orch ceph.shutdown
</screen>
   </step>
   <step>
    <para>
     Power off all cluster nodes:
    </para>
<screen>&prompt.smaster;salt -C 'G@deepsea:*' cmd.run "shutdown -h"</screen>
   </step>
  </procedure>

  <procedure>
   <title>Starting the Whole &ceph; Cluster</title>
   <step>
    <para>
     Power on the &adm;.
    </para>
   </step>
   <step>
    <para>
     Power on the &mon; nodes.
    </para>
   </step>
   <step>
    <para>
     Power on the &osd; nodes.
    </para>
   </step>
   <step>
    <para>
     Unset the previously set <literal>noout</literal> flag:
    </para>
<screen>&prompt.smaster;ceph osd unset noout</screen>
   </step>
   <step>
    <para>
     Power on all configured gateways.
    </para>
   </step>
   <step>
    <para>
     Power on or connect cluster clients.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
