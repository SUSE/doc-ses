<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="lvmcache">
<!-- ============================================================== -->
 <title>Improving Performance with &lvmcache;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <warning>
  <title>Technology Preview</title>
  <para>
   &lvmcache; is currently a technology preview.
  </para>
 </warning>
 <para>
  &lvmcache; is a caching mechanism used to improve the performance of a
  logical volume (LV). Typically, a smaller and faster device is used to
  improve I/O performance of a larger and slower LV. Refer to its manual page
  (<command>man 7 lvmcache</command>) to find more details about &lvmcache;.
 </para>
 <para>
  In &productname;, &lvmcache; can improve the performance of OSDs. Support for
  &lvmcache; is provided via a <command>ceph-volume</command> plugin. You can
  find detailed information about its usage by running <command>ceph-volume
  lvmcache</command>.
 </para>
 <sect1 xml:id="lvmcache-prerequisites">
  <title>Prerequisites</title>

  <para>
   To use &lvmcache; features to improve the performance of a &ceph; cluster,
   you need to have:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A running &ceph; cluster in a stable state ('HEALTH_OK').
    </para>
   </listitem>
   <listitem>
    <para>
     OSDs deployed with &bluestore; and LVM. This is the default if the OSDs
     were deployed using &productname; 5 or later.
    </para>
   </listitem>
   <listitem>
    <para>
     Empty disks or partitions that will be used for caching.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="lvmcache-planning">
  <title>Points to Consider</title>

  <para>
   Consider the following points before configuring your OSDs to use
   &lvmcache;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis role="bold">Verify that &lvmcache; is suitable for your use
     case.</emphasis> If you have only a few fast drives available that are
     <emphasis role="bold">not</emphasis> used for OSDs, the general
     recommendation is to use them as WAL/DB devices for the OSDs. In such
     case, WAL and DB operations (small and rare operations) are applied on the
     fast drive while data operations are applied on the slower OSD drive.
    </para>
    <tip>
     <para>
      If latency is more important for your deployment than IOPS or throughput,
      you can use the fast drives as &lvmcache; rather than WAL/DB partitions.
     </para>
    </tip>
   </listitem>
   <listitem>
    <para>
     If you plan to use a fast drive as an &lvmcache; for multiple OSDs, be
     aware that <emphasis role="bold">all OSD operations (including
     replication) will go through the caching device</emphasis>. All reads will
     be queried from the caching device, and are only served from the slow
     device in case of a cache miss. Writes are always applied to the caching
     device first, and are flushed to the slow device at a later time
     ('writeback' is the default caching mode).
    </para>
    <para>
     When deciding whether to utilize an &lvmcache;, verify whether the fast
     drive can serve as a front for multiple OSDs while still providing an
     acceptable number of IOPS. You can test it by measuring the maximum amount
     of IOPS that the fast device can serve, and then dividing the result by
     the number of OSDs behind the fast device. If the result is lower or close
     to the maximum amount of IOPS that the OSD can provide without the cache,
     &lvmcache; is probably not suited for this setup.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">The interaction of the &lvmcache; device with OSDs
     is important.</emphasis> Writes are periodically flushed from the caching
     device to the slow device. If the incoming traffic is sustained and
     significant, the caching device will struggle to keep up with incoming
     requests as well as the flushing process, resulting in performance drop.
     Unless the fast device can provide much more IOPS with better latency than
     the slow device, do not use &lvmcache; with a sustained high volume
     workload. Traffic in a burst pattern is more suited for &lvmcache; as it
     gives the cache time to to flush its dirty data without interfering with
     client traffic. For a sustained low traffic workload, it is difficult to
     guess in advance whether using &lvmcache; will improve performance. The
     best test is to benchmark and compare the &lvmcache; setup against the
     WAL/DB setup. Moreover, as small writes are heavy on the WAL partition, it
     is suggested to use the fast device for the DB and/or WAL instead of an
     &lvmcache;.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are not certain whether to use &lvmcache;, use the fast device as a
     WAL and/or DB device.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="lvmcache-preparation">
  <title>Preparation</title>

  <para>
   You need to split the fast device into multiple partitions. Each OSD needs
   two partitions for its cache: one for the cache data, and one for the cache
   metadata. The minimal size of either partition is 2GB. You can use a single
   fast device to cache multiple OSDs. It simply needs to be partitioned
   accordingly.
  </para>
 </sect1>
 <sect1 xml:id="lvmcache-configuring">
  <title>Configuring &lvmcache;</title>

  <para>
   You can find detailed information about adding, removing, and configuring
   &lvmcache; by running the <command>ceph-volume lvmcache</command> command.
  </para>

  <sect2 xml:id="lvmcache-adding">
   <title>Adding &lvmcache;</title>
   <para>
    To add &lvmcache; to an existing OSD, use the following command:
   </para>
<screen>
&prompt.sminion;ceph-volume lvmcache add
 --cachemetadata <replaceable>METADATA-PARTITION</replaceable>
 --cachedata <replaceable>DATA-PARTITION</replaceable>
 --osd-id <replaceable>OSD-ID</replaceable>
</screen>
   <para>
    The optional <option>--data</option>, <option>--db</option> or
    <option>--wal</option> specifies which partition to cache. Default is
    <option>--data</option>.
   </para>
   <tip>
    <title>Specify Logical Volume (LV)</title>
    <para>
     Alternatively, you can use the <option>--origin</option> instead of
     <option>--osd-id</option> option to specify which LV to cache:
    </para>
<screen>
[...]
--origin <replaceable>VOLUME-GROUP</replaceable>/<replaceable>LOGICAL-VOLUME</replaceable>
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="lvmcache-removing">
   <title>Removing &lvmcache;</title>
   <para>
    To remove existing &lvmcache; from an OSD, use the following command:
   </para>
<screen>
&prompt.sminion;ceph-volume lvmcache rm --osd-id <replaceable>OSD-ID</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="lvmcache-mode">
   <title>Setting &lvmcache; Mode</title>
   <para>
    To specify caching mode, use the following command:
   </para>
<screen>
&prompt.sminion;ceph-volume lvmcache mode --set <replaceable>CACHING-MODE</replaceable> --osd-id <replaceable>OSD-ID</replaceable>
</screen>
   <para>
    <replaceable>CACHING-MODE</replaceable> is either 'writeback' (default) or
    'writethrough'
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="lvmcache-failures">
  <title>Handling Failures</title>

  <para>
   If the caching device fails, all OSDs behind the caching device need to be
   removed from the cluster (see <xref linkend="salt-removing-osd"/>), purged
   and redeployed. If the OSD drive fails, the OSD's LV as well as its cache's
   LV will be active but not functioning. Use <command>pvremove
   <replaceable>PARTITION</replaceable></command> to purge the partitions
   (physical volumes) used for the OSD's cache data and metadata partitions.
   You can use <command>pvs</command> to list all physical volumes.
  </para>
 </sect1>
 <sect1 xml:id="lvmcache-faq">
  <title>Frequently Asked Questions</title>

  <qandaset>
   <qandaentry>
    <question>
     <para>
      What happens if an OSD is removed?
     </para>
    </question>
    <answer>
     <para>
      When removing the OSD's LV using <command>lvremove</command>, the cache
      LVs will be removed as well. However, you will still need to call
      <command>pvremove</command> on the partitions to make sure all labels
      have been wiped out.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      What happens if the OSD is zapped using <command>ceph-volume
      zap</command>?
     </para>
    </question>
    <answer>
     <para>
      The same answer applies as to the question <emphasis role="bold">What
      happens if an OSD is removed?</emphasis>.
     </para>
    </answer>
   </qandaentry>
   <qandaentry>
    <question>
     <para>
      What happens if the origin drive fails?
     </para>
    </question>
    <answer>
     <para>
      The cache LVs still exist, and <command>cache info</command> still shows
      them as being available. You will not be able to uncache because LVM will
      fail to flush the cache as the origin LV's device is gone. The situation
      now is that the origin LV exists, but its backing device does not. You
      can fix it by using the <command>pvs</command> command and locating the
      devices that are associated with the origin LV. You can then remove them
      using
     </para>
<screen>
&prompt.sminion;pvremove /dev/<replaceable>DEVICE</replaceable> or <replaceable>PARTITION</replaceable>
</screen>
     <para>
      You can do the same for the cache partitions. This procedure will make
      the origin LV as well as the cache LVs disappear. You can also use
     </para>
<screen>
&prompt.sminion;dd if=/dev/zero of=/dev/<replaceable>DEVICE</replaceable> or <replaceable>PARTITION</replaceable>
</screen>
     <para>
      to wipe them out before using <command>pvremove</command>.
     </para>
    </answer>
   </qandaentry>
  </qandaset>
 </sect1>
</chapter>
