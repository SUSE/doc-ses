<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package> ,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Consider the following items before starting the upgrade procedure:
  </para>

  <variablelist>
   <varlistentry>
    <term>Upgrade Order</term>
    <listitem>
     <para>
      Before upgrading the &ceph; cluster, you need to have both the underlying
      &sls; and &storage; correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example &ceph;
      &rgw;s depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
      upgrading in this order:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        &mon;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mgr;s
       </para>
      </listitem>
      <listitem>
       <para>
        &osd;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mds;s
       </para>
      </listitem>
      <listitem>
       <para>
        &rgw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &igw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha;
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Delete Unnecessary Operating System Snapshots</term>
    <listitem>
     <para>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Check Cluster Health</term>
    <listitem>
     <para>
      We recommend to check the cluster health before starting the upgrade
      procedure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Upgrade One by One</term>
    <listitem>
     <para>
      We recommend upgrading all the daemons of a specific type&mdash;for
      example all monitor daemons or all OSD daemons&mdash;one by one to ensure
      that they are all on the same release. We also recommend that you upgrade
      all the daemons in your cluster before you try to exercise new
      functionality in a release.
     </para>
     <para>
      After all the daemons of a specific type are upgraded, check their
      status.
     </para>
     <para>
      Ensure each monitor has rejoined the quorum after all monitors are
      upgraded:
     </para>
<screen>&prompt.root;ceph mon stat</screen>
     <para>
      Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
      upgraded:
     </para>
<screen>&prompt.root;ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Set <option>require-osd-release luminous</option> Flag
    </term>
    <listitem>
     <para>
      When the last OSD is upgraded to &storage; 5, the monitor nodes will
      detect that all OSDs are running the 'luminous' version of &ceph; and
      they may complain that the <option>require-osd-release luminous</option>
      osdmap flag is not set. In that case, you need to set this flag manually
      to acknowledge that&mdash;now that the cluster has been upgraded to
      'luminous'&mdash;it cannot be downgraded back to &ceph; 'jewel'. Set the
      flag by running the following command:
     </para>
<screen>&prompt.sminion;sudo ceph osd require-osd-release luminous</screen>
     <para>
      After the command completes, the warning disappears.
     </para>
     <para>
      On fresh installs of &storage; 5, this flag is set automatically when the
      &ceph; monitors create the initial osdmap, so no end user action is
      needed.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds.migrate.osd.encrypted">
  <title>Encrypting OSDs during Upgrade</title>

  <para>
   Since &productname; 5, OSDs are by default deployed using &bluestore;
   instead of &filestore;. Although &bluestore; supports encryption, &osd;s are
   deployed unencrypted by default. The following procedure describes steps to
   encrypt OSDs during the upgrade process. Let us assume that both data and
   WAL/DB disks to be used for OSD deployment are clean with no partitions. If
   the disk were previously used, wipe them following the procedure described
   in <xref linkend="deploy.wiping.disk"/>.
  </para>

  <important>
   <title>One OSD at a Time</title>
   <para>
    You need to deploy encrypted OSDs one by one, not simultaneously. The
    reason is that OSD's data is drained, and the cluster goes through several
    iterations of rebalancing.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Choose the <option>bluestore block db size</option> and <option>bluestore
     block wal size</option> values for your deployment and add them to the
     <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
     file on the &smaster;. The values need to be specified in bytes.
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     For more information on customizing the <filename>ceph.conf</filename>
     file, refer to <xref linkend="ds.custom.cephconf"/>.
    </para>
   </step>
   <step>
    <para>
     Run &deepsea; Stage 3 to distribute the changes:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Verify that the <filename>ceph.conf</filename> file is updated on the
     relevant OSD nodes:
    </para>
<screen>
&prompt.sminion;cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     Edit the *.yml files in the
     <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename>
     directory that are relevant to the OSDs you are encrypting. Double check
     their path with the one defined in the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> file to ensure
     that you modify the correct *.yml files.
    </para>
    <important>
     <title>Long Disk Identifiers</title>
     <para>
      When identifying OSD disks in the
      <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>
      files, use long disk identifiers.
     </para>
    </important>
    <para>
     An example of an OSD configuration follows. Note that because we need
     encryption, the <option>db_size</option> and <option>wal_size</option>
     options are removed:
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     Deploy the new &blockstore; OSDs with encryption by running &deepsea;
     Stages 2 and 3:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    <para>
     You can watch the progress with <command>ceph -s</command> or
     <command>ceph osd tree</command>. It is critical that you let the cluster
     rebalance before repeating the process on the next OSD node.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>Upgrade from &storage; 4 (&deepsea; Deployment) to 5</title>

  <important xml:id="u4to5.softreq">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In addition, before starting the upgrade, you need to upgrade the &smaster;
    node to &sls; 12 SP3 and &storage; 5 by running <command>zypper
    migration</command> (or your preferred way of upgrading).
   </para>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Check whether the &aa; service is running and disable it on each cluster
      node. Start the &yast; &aa; module, select <guimenu>Settings</guimenu>,
      and then deactivate the <guimenu>Enable Apparmor</guimenu> check box.
      Confirm with <guimenu>Done</guimenu>.
     </para>
     <para>
      Note that &storage; will <emphasis>not</emphasis> work with &aa; enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order, based on their assigned role as recommended by &ceph; upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent the prescribed order from being
      violated if a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply, for example, new kernel versions. To
      reduce waiting I/O operations, we recommend declining incoming requests
      for the duration of the upgrade process.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Since &ceph; Luminous, the <option>osd crush location</option> config
      option is no longer supported. Please update your &deepsea; configuration
      files to use <command>crush location</command> before upgrading.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &storage; 4 cluster to version 5, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.root;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.root;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Using <command>rpm -q deepsea</command>, verify that the version of the
     &deepsea; package on the &smaster; node starts with at least
     <literal>0.7</literal>. For example:
    </para>
<screen>&prompt.root;rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     If the &deepsea; package version number starts with 0.6, double check
     whether you successfully migrated the &smaster; node to &sls; 12 SP3 and
     &storage; 5 (refer to <xref linkend="u4to5.softreq"/> at the beginning of
     this section). This is a prerequisite that must be completed before
     starting the upgrade procedure.
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken. Continue with
       <xref linkend="step.updatepillar"/>.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <command>zypper</command> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
      <para>
       Then change your Pillar data in order to use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        The <literal>zypper-dup</literal> strategy requires you to manually add
        the latest software repositories, while the default
        <literal>zypper-migration</literal> relies on the repositories provided
        by SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     Update your Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     See <xref linkend="ds.minion.targeting"/> for details about &sminion;s
     targeting.
    </para>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Upgrade &sminion;s:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to
     <xref linkend="ds.minion.targeting"/> of <xref linkend="ds.depl.stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Check that &aa; is disabled and stopped on all nodes after the upgrade:
    </para>
<screen>&prompt.root;systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     After the upgrade, the &mgr;s are not installed yet. To reach a healthy
     cluster state, do the following:
    </para>
    <substeps>
     <step>
      <para>
       Run Stage 0 to enable the &salt; REST API:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       Run Stage 1 to create the <filename>role-mgr/</filename> subdirectory:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edit <guimenu>policy.cfg</guimenu> as described in
       <xref linkend="policy.configuration"/> and add a &mgr; role to the nodes
       where &mon;s are deployed. Also, add the &oa; role to one of the cluster
       nodes. Refer to <xref linkend="ceph.oa"/> for more details.
      </para>
     </step>
     <step>
      <para>
       Run Stage 2 to update the Pillar:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       &deepsea; uses a different approach to generate the
       <filename>ceph.conf</filename> configuration file now, refer to
       <xref
        linkend="ds.custom.cephconf"/> for more details.
      </para>
     </step>
     <step>
      <para>
       Run Stage 3 to deploy &mgr;s:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       Run Stage 4 to configure &oa; properly:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>&ceph; Key Caps Mismatch</title>
     <para>
      If <literal>ceph.stage.3</literal> fails with "Error EINVAL: entity
      client.bootstrap-osd exists but caps do not match", it means the key
      capabilities (caps) for the existing cluster's
      <literal>client.bootstrap.osd</literal> key do not match the caps that
      &deepsea; is trying to set. Above the error message, in red text, you can
      see a dump of the <command>ceph auth</command> command that failed. Look
      at this command to check the key ID and file being used. In the case of
      <literal>client.bootstrap-osd</literal>, the command will be
     </para>
<screen>&prompt.root;ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      To fix mismatched key caps, check the content of the keyring file
      &deepsea; is trying to deploy, for example:
     </para>
<screen>&prompt.cephuser;cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Compare this with the output of <command>ceph auth get
      client.bootstrap-osd</command>:
     </para>
<screen>&prompt.root;ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Note how the latter key is missing <literal>caps mgr = "allow
      r"</literal>. To fix this, run:
     </para>
<screen>&prompt.root;ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      Running <literal>ceph.stage.3</literal> should now succeed.
     </para>
     <para>
      The same issue can occur with &mds; and &ogw; keyrings when running
      <literal>ceph.stage.4</literal>. The same procedure as above applies:
      check the command that failed, the keyring file being deployed, and the
      caps of the existing key. Then run <command>ceph auth caps</command> to
      update the existing key caps to match what is being deployed by
      &deepsea;.
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrading to &storage; 5, FileStore OSDs need approximately five
    minutes longer to start as the OSD will do a one-off conversion of its
    on-disk files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade&mdash;you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, and &sls;, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>OSD Migration to &bluestore;</title>
   <para>
    OSD &bluestore; is a new back end for the OSD daemons. It is the default
    option since &storage; 5. Compared to FileStore, which stores objects as
    files in an XFS file system, &bluestore; can deliver increased performance
    because it stores objects directly on the underlying block device.
    &bluestore; also enables other features, such as built-in compression and
    EC overwrites, that are unavailable with FileStore.
   </para>
   <para>
    Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a &bluestore; OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on faster/different
    media.
   </para>
   <para>
    In SES5, both FileStore and &bluestore; are supported and it is possible
    for FileStore and &bluestore; OSDs to co-exist in a single cluster. During
    the SUSE Enterprise Storage upgrade procedure, FileStore OSDs are not
    automatically converted to &bluestore;. Be aware that the
    &bluestore;-specific features will not be available on OSDs that have not
    been migrated to &bluestore;.
   </para>
   <para>
    Before converting to &bluestore;, the OSDs need to be running &storage; 5.
    The conversion is a slow process as all data gets re-written twice. Though
    the migration process can take a long time to complete, there is no cluster
    outage and all clients can continue accessing the cluster during this
    period. However, do expect lower performance for the duration of the
    migration. This is caused by rebalancing and backfilling of cluster data.
   </para>
   <para>
    Use the following procedure to migrate FileStore OSDs to &bluestore;:
   </para>
   <tip>
    <title>Turn Off Safety Measures</title>
    <para>
     &salt; commands needed for running the migration are blocked by safety
     measures. In order to turn these precautions off, run the following
     command:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      Migrate hardware profiles:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.policy</screen>
     <para>
      This runner migrates any hardware profiles currently in use by the
      <filename>policy.cfg</filename> file. It processes
      <filename>policy.cfg</filename>, finds any hardware profile using the
      original data structure, and converts it to the new data structure. The
      result is a new hardware profile named
      'migrated-<replaceable>original_name</replaceable>'.
      <filename>policy.cfg</filename> is updated as well.
     </para>
     <para>
      If the original configuration had separate journals, the &bluestore;
      configuration will use the same device for the 'wal' and 'db' for that
      OSD.
     </para>
    </step>
    <step>
     <para>
      &deepsea; migrates OSDs by setting their weight to 0 which 'vacuums' the
      data until the OSD is empty. You can either migrate OSDs one by one, or
      all OSDs at once. In either case, when the OSD is empty, the
      orchestration removes it and then re-creates it with the new
      configuration.
     </para>
     <tip>
      <title>Recommended Method</title>
      <para>
       Use <command>ceph.migrate.nodes</command> if you have a large number of
       physical storage nodes or almost no data. If one node represents less
       than 10% of your capacity, then the
       <command>ceph.migrate.nodes</command> may be marginally faster moving
       all the data from those OSDs in parallel.
      </para>
      <para>
       If you are not sure about which method to use, or the site has few
       storage nodes (for example each node has more than 10% of the cluster
       data), then select <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        To migrate OSDs one at a time, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        To migrate all OSDs on each node in parallel, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       As the orchestration gives no feedback about the migration progress, use
      </para>
<screen>&prompt.root;ceph osd tree</screen>
      <para>
       to see which OSDs have a weight of zero periodically.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    After the migration to &bluestore;, the object count will remain the same
    and disk usage will be nearly the same.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>Upgrade from &storage; 4 (<command>ceph-deploy</command> Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Choose the &smaster; for your cluster. If your cluster has Calamari
    deployed, then the Calamari node already <emphasis>is</emphasis> the
    &smaster;. Alternatively, the admin node from which you ran the
    <command>ceph-deploy</command> command will become the &smaster;.
   </para>
   <para>
    Before starting the procedure below, you need to upgrade the &smaster; node
    to &sls; 12 SP3 and &storage; 5 by running <command>zypper
    migration</command> (or your preferred way of upgrading).
   </para>
  </important>

  <para>
   To upgrade the &storage; 4 cluster which was deployed with
   <command>ceph-deploy</command> to version 5, follow these steps:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>Steps to Apply to All Cluster Nodes (including the Calamari Node)</title>
   <step>
    <para>
     Install the <systemitem>salt</systemitem> package from SLE-12-SP2/SES4:
    </para>
<screen>&prompt.root;zypper install salt</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>salt-minion</systemitem> package from
     SLE-12-SP2/SES4, then enable and start the related service:
    </para>
<screen>&prompt.root;zypper install salt-minion
&prompt.root;systemctl enable salt-minion
&prompt.root;systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Ensure that the host name 'salt' resolves to the IP address of the
     &smaster; node. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      The existing &sminion;s have the <option>master:</option> option already
      set in <filename>/etc/salt/minion.d/calamari.conf</filename>. The
      configuration file name does not matter, the
      <filename>/etc/salt/minion.d/</filename> directory is important.
     </para>
    </tip>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <command>zypper</command> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Steps to Apply to the &smaster; Node</title>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.smaster;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.smaster;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Upgrade the &smaster; node to &sls; 12 SP3 and &storage; 5. For
     SCC-registered systems, use <command>zypper migration</command>. If you
     provide the required software repositories manually, use <command>zypper
     dup</command>. After the upgrade, ensure that only repositories for &sls;
     12 SP3 and &storage; 5 are active (and refreshed) on the &smaster; node
     before proceeding.
    </para>
   </step>
   <step>
    <para>
     If not already present, install the <systemitem>salt-master</systemitem>
     package, then enable and start the related service:
    </para>
<screen>&prompt.smaster;zypper install salt-master
&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verify the presence of all &sminion;s by listing their keys:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Add all &sminion;s keys to &smaster; including the minion master:
    </para>
<screen>&prompt.smaster;salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Ensure that all &sminion;s' keys were accepted:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Make sure that the software on your &smaster; node is up to date:
    </para>
<screen>&prompt.smaster;zypper migration</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>deepsea</systemitem> package:
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to
     <xref linkend="ds.minion.targeting"/> of <xref linkend="ds.depl.stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Import the existing <command>ceph-deploy</command> installed cluster:
    </para>
<screen>&prompt.smaster;salt-run populate.engulf_existing_cluster</screen>
    <para>
     The command will do the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribute all the required &salt; and &deepsea; modules to all the
       &sminion;s.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspect the running &ceph; cluster and populate
       <filename>/srv/pillar/ceph/proposals</filename> with a layout of the
       cluster.
      </para>
      <para>
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> will be
       created with roles matching all detected running &ceph; services. View
       this file to verify that each of your existing MON, OSD, RGW and MDS
       nodes have the appropriate roles. OSD nodes will be imported into the
       <filename>profile-import/</filename> subdirectory, so you can examine
       the files in
       <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename>
       and
       <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename>
       to confirm that the OSDs were correctly picked up.
      </para>
      <note>
       <para>
        The generated <filename>policy.cfg</filename> will only apply roles for
        detected &ceph; services 'role-mon', 'role-mgr', 'role-mds',
        'role-rgw', 'role-admin', and 'role-master' for the &smaster; node. Any
        other desired roles will need to be added to the file manually (see
        <xref linkend="policy.role.assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The existing cluster's <filename>ceph.conf</filename> will be saved to
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
       will include the cluster's fsid, cluster and public networks, and also
       specifies the <option>configuration_init: default-import</option>
       option, which makes &deepsea; use the
       <filename>ceph.conf.import</filename> configuration file mentioned
       previously, rather than using &deepsea;'s default
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
       template.
      </para>
      <note>
       <title>Custom <filename>ceph.conf</filename></title>
       <para>
        If you need to integrate the <filename>ceph.conf</filename> file with
        custom changes, wait until the engulf/upgrade process successfully
        finishes. Then edit the
        <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
        file and comment the following line:
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        Save the file and follow the information in
        <xref
        linkend="ds.custom.cephconf"/>.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The cluster's various keyrings will be saved to the following
       directories:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       Verify that the keyring files exist, and that there is
       <emphasis>no</emphasis> keyring file in the following directory (the
       &mgr; did not exist before &productname; 5):
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &oa; configuration. You need to manually
     edit the <filename>policy.cfg</filename> file and add a
     <literal>role-openattic</literal> line. Refer to
     <xref
      linkend="policy.configuration"/> for more details.
    </para>
   </step>
<!-- 2018-03-21,tbazant: remove this step after
   https://github.com/SUSE/DeepSea/issues/845 is resolved -->
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &igw;s configurations. If your cluster
     includes &igw;s, import their configurations manually:
    </para>
    <substeps>
     <step>
      <para>
       On one of &igw; nodes, export the current <filename>lrbd.conf</filename>
       and copy it to the &smaster; node:
      </para>
<screen>
&prompt.sminion;lrbd -o >/tmp/lrbd.conf
&prompt.sminion;scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       On the &smaster; node, add the default &igw; configuration to the
       &deepsea; setup:
      </para>
<screen>
&prompt.smaster;mkdir -p /srv/pillar/ceph/stack/ceph/
&prompt.smaster;echo 'igw_config: default-ui' >> /srv/pillar/ceph/stack/ceph/cluster.yml
&prompt.smaster;chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Add the &igw; roles to <filename>policy.cfg</filename> and save the
       file:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Run Stage 1 to create all possible roles:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Generate required subdirectories under
     <filename>/srv/pillar/ceph/stack</filename>:
    </para>
<screen>&prompt.smaster;salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     Verify that there is a working &deepsea;-managed cluster with correctly
     assigned roles:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     Compare the output with the actual layout of the cluster.
    </para>
   </step>
   <step>
    <para>
     Calamari leaves a scheduled &salt; job running to check the cluster
     status. Remove the job:
    </para>
<screen>
&prompt.sminion;salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
   </step>
   <step>
    <para>
     From this point on, follow the procedure described in
     <xref linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5crowbar">
  <title>Upgrade from &storage; 4 (&crow; Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade &storage; 4 deployed using &crow; to version 5, follow these
   steps:
  </para>

  <procedure>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), stop and disable all
     &crow;-related services :
    </para>
<screen>
&prompt.sminion;sudo systemctl stop chef-client
&prompt.sminion;sudo systemctl disable chef-client
&prompt.sminion;sudo systemctl disable crowbar_join
&prompt.sminion;sudo systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), verify that the
     software repositories point to &storage; 5 and &sls; 12 SP3 products. If
     repositories pointing to older product versions are still present, disable
     them.
    </para>
   </step>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), verify that the
     <package>salt-minion</package> is installed. If not, install it:
    </para>
<screen>&prompt.sminion;sudo zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     For the &ceph; nodes that did not have the <package>salt-minion</package>
     package installed, create the file
     <filename>/etc/salt/minion.d/master.conf</filename> with the
     <option>master</option> option pointing to the full Calamari node
     hostname:
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      The existing &sminion;s have the <option>master:</option> option already
      set in <filename>/etc/salt/minion.d/calamari.conf</filename>. The
      configuration file name does not matter, the
      <filename>/etc/salt/minion.d/</filename> directory is important.
     </para>
    </tip>
    <para>
     Enable and start the <systemitem class="daemon">salt-minion</systemitem>
     service:
    </para>
<screen>
&prompt.sminion;sudo systemctl enable salt-minion
&prompt.sminion;sudo systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     On the Calamari node, accept any remaining salt minion keys:
    </para>
<screen>
&prompt.smaster;salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

&prompt.smaster;salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     If &ceph; was deployed on the public network and no VLAN interface is
     present, add a VLAN interface on &crow;'s public network to the Calamari
     node.
    </para>
   </step>
   <step>
    <para>
     Upgrade the Calamari node to &sls; 12 SP3 and &storage; 5, either by using
     <command>zypper migration</command> or your favorite method. From here
     onwards, the Calamari node becomes the <emphasis>&smaster;</emphasis>.
     After the upgrade, reboot the &smaster;.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Specify the <option>deepsea_minions</option> option to include the correct
     group of &sminion;s into deployment stages. Refer to
     <xref
      linkend="ds.minion.targeting.dsminions"/> for more details.
    </para>
   </step>
   <step>
    <para>
     &deepsea; expects all &ceph; nodes to have an identical
     <filename>/etc/ceph/ceph.conf</filename>. &crow; deploys a slightly
     different <filename>ceph.conf</filename> to each node, so you need to
     consolidate them:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Remove the <option>osd crush location hook</option> option, it was
       included by Calamari.
      </para>
     </listitem>
     <listitem>
      <para>
       Remove the <option>public addr</option> option from the
       <literal>[mon]</literal> section.
      </para>
     </listitem>
     <listitem>
      <para>
       Remove the port numbers from the <option>mon host</option> option.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If you were running the &ogw;, &crow; deployed a separate
     <filename>/etc/ceph/ceph.conf.radosgw</filename> file to keep the keystone
     secrets separated from the regular <filename>ceph.conf</filename> file.
     &crow; also added a custom
     <filename>/etc/systemd/system/ceph-radosgw@.service</filename> file.
     Because &deepsea; does not support it, you need to remove it:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Append all <literal>[client.rgw....]</literal> sections from the
       <filename>ceph.conf.radosgw</filename> file to
       <filename>/etc/ceph/ceph.conf</filename> on all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       On the &ogw; node, run the following:
      </para>
<screen>&prompt.sminion;rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Double check that <command>ceph status</command> works when run from the
     &smaster;:
    </para>
<screen>&prompt.smaster;ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     Import the existing cluster:
    </para>
<screen>
&prompt.smaster;salt-run populate.engulf_existing_cluster
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run push.proposal
</screen>
   </step>
<!-- 2018-03-21, remove this step after
   https://github.com/SUSE/DeepSea/issues/845 is resolved -->
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &igw;s configurations. If your cluster
     includes &igw;s, import their configurations manually:
    </para>
    <substeps>
     <step>
      <para>
       On one of &igw; nodes, export the current <filename>lrbd.conf</filename>
       and copy it to the &smaster; node:
      </para>
<screen>
&prompt.sminion;lrbd -o > /tmp/lrbd.conf
&prompt.sminion;scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       On the &smaster; node, add the default &igw; configuration to the
       &deepsea; setup:
      </para>
<screen>
&prompt.smaster;mkdir -p /srv/pillar/ceph/stack/ceph/
&prompt.smaster;echo 'igw_config: default-ui' >> /srv/pillar/ceph/stack/ceph/cluster.yml
&prompt.smaster;chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Add the &igw; roles to <filename>policy.cfg</filename> and save the
       file:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <command>zypper</command> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
      <para>
       Then change your Pillar data in order to use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        The <literal>zypper-dup</literal> strategy requires you to manually add
        the latest software repositories, while the default
        <literal>zypper-migration</literal> relies on the repositories provided
        by SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Fix host grains to make &deepsea; use short host names on the public
     network for the &ceph; daemon instance IDs. For each node, you need to run
     <command>grains.set</command> with the new (short) host name. Before
     running <command>grains.set</command>, verify the current monitor
     instances by running <command>ceph status</command>. A before and after
     example follows:
    </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
&prompt.smaster;salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
&prompt.smaster;salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
&prompt.smaster;salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
&prompt.smaster;salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     Run the upgrade:
    </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates
&prompt.smaster;salt <replaceable>target</replaceable> test.version
&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     Every node will reboot. The cluster will come back up complaining that
     there is no active &mgr; instance. This is normal. Calamari should not be
     installed/running anymore at this point.
    </para>
   </step>
   <step>
    <para>
     Run all the required deployment stages to get the cluster to a healthy
     state:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     To deploy &oa; (see <xref linkend="ceph.oa"/>), add an appropriate
     <literal>role-openattic</literal> (see
     <xref
      linkend="policy.role.assignment"/>) line to
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>, then run:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     During the upgrade, you may receive "Error EINVAL: entity [...] exists but
     caps do not match" errors. To fix them, refer to
     <xref
      linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
   <step>
    <para>
     Do the remaining cleanup:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &crow; creates entries in <filename>/etc/fstab</filename> for each OSD.
       They are not necessary, so delete them.
      </para>
     </listitem>
     <listitem>
      <para>
       Calamari leaves a scheduled &salt; job running to check the cluster
       status. Remove the job:
      </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       There are still some unnecessary packages installed, mostly rubygems,
       and chef related. Their removal is not required but you may want to
       delete them by running <command>zypper rm
       <replaceable>pkg_name</replaceable></command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>Upgrade from &storage; 3 to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 3 cluster to version 5, follow the steps described
   in <xref linkend="upgrade4to5cephdeploy.all"/> and then
   <xref linkend="upgrade4to5cephdeploy.admin"/>.
  </para>
 </sect1>
</chapter>
