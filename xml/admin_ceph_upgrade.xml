<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Note that version &prevproductnumber; is basically 5
  with all latest patches applied.
 </para>
 <note>
  <title>Upgrade from Older Releases not Supported</title>
  <para>
   The upgrade from &productname; version older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow steps in this chapter.
  </para>
 </note>
 <sect1 xml:id="upgrade-consider-points">
  <title>Points to Consider before the Upgrade</title>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Read the release notes</emphasis> - there you can find
     additional information on changes since the previous release of
     &productname;. Check the release notes to see whether:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       your hardware needs special considerations.
      </para>
     </listitem>
     <listitem>
      <para>
       any used software packages have changed significantly.
      </para>
     </listitem>
     <listitem>
      <para>
       special precautions are necessary for your installation.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The release notes also provide information that could not make it into the
     manual on time. They also contain notes about known issues.
    </para>
    <para>
     After having installed the package <package>release-notes-ses</package>,
     find the release notes locally in the directory
     <filename>/usr/share/doc/release-notes</filename> or online at
     <link xlink:href="https://www.suse.com/releasenotes/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     In case you previously upgraded from version 4, verify that the upgrade to
     version 5 was completed successfully:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Check for the existence of the file
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
      <para>
       It is created by the engulf process during the upgrade from SES 4 to 5.
       Also, the <option>configuration_init: default-import</option> option is
       set in the file
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <para>
       If <option>configuration_init</option> is still set to
       <option>default-import</option>, the cluster is using
       <filename>ceph.conf.import</filename> as its configuration file and not
       the &deepsea;'s default <filename>ceph.conf</filename> which is compiled
       from files in
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       Therefore you need to inspect <filename>ceph.conf.import</filename> for
       any custom configuration, and possibly move the configuration to one of
       the files in
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       Then remove the <option>configuration_init: default-import</option> line
       from
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <warning>
       <title>Default &deepsea; Configuration</title>
       <para>
        If you <emphasis role="bold">do not</emphasis> merge the configuration
        from <filename>ceph.conf.import</filename> and remove the
        <option>configuration_init: default-import</option> option, any default
        configuration settings we ship as part of &deepsea; (stored in
        <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>)
        will not be applied to the cluster.
       </para>
      </warning>
     </listitem>
     <listitem>
      <para>
       Check if the cluster uses the new bucket type 'straw2':
      </para>
<screen>
&prompt.cephuser;ceph osd crush dump | grep straw
</screen>
     </listitem>
     <listitem>
      <para>
       Check that &ceph; 'jewel' profile is used:
      </para>
<screen>
&prompt.cephuser;ceph osd crush dump | grep profile
</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     In case old RBD kernel clients (older than &prevcephos;) are being used,
     refer to <xref linkend="rbd-old-clients-map" />. We recommend upgrading
     old RBD kernel clients if possible.
    </para>
   </listitem>
   <listitem>
    <para>
     If &oa; is located on the &adm;, it will be unavailable after you upgrade
     the node. The new &dashboard; will not be available until you deploy it by
     using &deepsea;.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster upgrade may take a long time&mdash;approximately the time it
     takes to upgrade one machine multiplied by the number of cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     A single node cannot be upgraded while running the previous &sls; release,
     but needs to be rebooted into the new version's installer. Therefore the
     services that the node provides will be unavailable for some time. The
     core cluster services will still be available&mdash;for example if one MON
     is down during upgrade, there are still at least two active MONs.
     Unfortunately, single instance services, such as a single &igw;, will be
     unavailable.
    </para>
   </listitem>
   <listitem>
    <para>
     Certain types of daemons depend upon others. For example &ceph; &rgw;s
     depend upon &ceph; MON and OSD daemons. We recommend upgrading in this
     order:
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       &adm;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;s/&mgr;s
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;s
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ogw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &igw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ganesha;
      </para>
     </listitem>
     <listitem>
      <para>
       &sgw;s
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     If you used &aa; in either 'complain' or 'enforce' mode, you need to set a
     &spillar; variable before upgrading. Because &cephos; ships with &aa; by
     default, &aa; management was integrated into &deepsea; stage.0. The
     default behavior in &productname; &productnumber; is to remove &aa; and
     related profiles. If you want to retain the behavior configured in
     &productname; &prevproductnumber;, verify that one of the following lines
     is present in the <filename>/srv/pillar/ceph/stack/global.yml</filename>
     file before starting the upgrade:
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     or
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     Since &productname; &productnumber;, MDS names starting with a digit are
     no longer allowed and MDS daemons will refuse to start. You can check
     whether your daemons have such names either by running the <command>ceph
     fs status</command> command, or by restarting an MDS and checking its logs
     for the following message:
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     If you see the above message, the MDS names will need to be migrated
     before attempting to upgrade to &productname; &productnumber;. &deepsea;
     provides an orchestration to automate such migration. MDS names starting
     with a digit will be prepended with 'mds.':
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <tip>
     <title>Custom Configuration Bound to MDS Names</title>
     <para>
      If you have configuration settings that are bound to MDS names and your
      MDS daemons have names starting with a digit, verify that your
      configuration settings apply to the new names as well (with the 'mds.'
      prefix). Consider the following example section in the
      <filename>/etc/ceph/ceph.conf</filename> file:
     </para>
<screen>
[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
 mds cache memory limit = 1073741824
 mds standby for name = 456-another-mds
</screen>
     <para>
      The <command>ceph.mds.migrate-numerical-names</command> orchestrator will
      change the MDS daemon name '123-my-mds' to 'mds.123-my-mds'. You need to
      adjust the configuration to reflect the new name:
     </para>
<screen>
[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds
</screen>
    </tip>
    <para>
     This will add MDS daemons with the new names before removing the old MDS
     daemons. The number of MDS daemons will double for a short time. Clients
     will be able to access &cephfs; with a short pause to failover. Therefore
     plan the migration for times when you expect little or no &cephfs; load.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-backup">
  <title>Backup Cluster Data</title>

  <para>
   Although creating backups of cluster's configuration and data is not
   mandatory, we strongly recommend backing up important configuration files
   and cluster data. Refer to <xref linkend="cha-deployment-backup"/> for more
   details.
  </para>
 </sect1>
 <sect1 xml:id="upgrade-ntp">
  <title>Migrate from <systemitem class="daemon">ntpd</systemitem> to <systemitem class="daemon">chronyd</systemitem></title>

  <para>
   &cephos; no longer uses <systemitem class="daemon">ntpd</systemitem> to
   synchronize the local host time. Instead,
   <systemitem class="daemon">chronyd</systemitem> is used. You need to migrate
   the time synchronization daemon on each cluster node. You can migrate to
   <systemitem>chronyd</systemitem> either
   <emphasis role="bold">before</emphasis> the cluster, or upgrade the cluster
   and migrate to <systemitem class="daemon">chronyd</systemitem> afterwards.
  </para>

  <procedure>
   <title>Migrate to <systemitem class="daemon">chronyd</systemitem> <emphasis>before</emphasis> the Cluster Upgrade</title>
   <step>
    <para>
     Install the <package>chrony</package> package:
    </para>
<screen>&prompt.sminion;zypper install chrony</screen>
   </step>
   <step>
    <para>
     Edit the <systemitem class="daemon">chronyd</systemitem> configuration
     file <filename>/etc/chrony.conf</filename> and add NTP sources from the
     current <systemitem class="daemon">ntpd</systemitem> configuration in
     <filename>/etc/ntp.conf</filename>.
    </para>
   </step>
   <step>
    <para>
     Disable and stop the <systemitem class="daemon">ntpd</systemitem> service:
    </para>
<screen>&prompt.smaster;systemctl disable ntpd.service &amp;&amp;systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     Start and enable the <systemitem class="daemon">chronyd</systemitem>
     service:
    </para>
<screen>&prompt.smaster;systemctl start chronyd.service &amp;&amp;systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     Verify the status of chrony:
    </para>
<screen>&prompt.smaster;chronyc tracking</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migrate to <systemitem class="daemon">chronyd</systemitem> <emphasis>after</emphasis> the Cluster Upgrade</title>
   <step>
    <para>
     During cluster upgrade, add the following software repositories:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Upgrade the cluster to version &productnumber;.
    </para>
   </step>
   <step>
    <para>
     Edit the <systemitem class="daemon">chronyd</systemitem> configuration
     file <filename>/etc/chrony.conf</filename> and add NTP sources from the
     current <systemitem class="daemon">ntpd</systemitem> configuration in
     <filename>/etc/ntp.conf</filename>.
    </para>
   </step>
   <step>
    <para>
     Disable and stop the <systemitem class="daemon">ntpd</systemitem> service:
    </para>
<screen>&prompt.smaster;systemctl disable ntpd.service &amp;&amp;systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     Start and enable the <systemitem class="daemon">chronyd</systemitem>
     service:
    </para>
<screen>&prompt.smaster;systemctl start chronyd.service &amp;&amp;systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     Migrate from <systemitem class="daemon">ntpd</systemitem> to
     <systemitem class="daemon">chronyd</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Verify the status of chrony:
    </para>
<screen>&prompt.smaster;chronyc tracking</screen>
   </step>
   <step>
    <para>
     Remove the legacy software repositories that you added to keep
     <systemitem class="daemon">ntpd</systemitem> in the system during the
     upgrade process.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-prepare">
  <title>Patch Cluster Prior to Upgrade</title>

  <para>
   Apply the latest patches to all cluster nodes prior to upgrade.
  </para>

  <sect2 xml:id="upgrade-prepare-repos">
   <title>Required Software Repositories</title>
   <para>
    Check that required repositories are configured on each cluster's node. To
    list all available repositories, run
   </para>
<screen>
&prompt.sminion;zypper lr
</screen>
   <para>
    &productname; &prevproductnumber; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    NFS/SMB Gateway on SLE-HA on &prevcephos; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-staging">
   <title>Repository Staging Systems</title>
   <para>
    If you are using one of repository staging systems&mdash;SMT, RMT;, or
    &susemgr;&mdash;create a new frozen patch level for the current and the new
    &productname; version.
   </para>
   <para>
    Find more information in:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>,
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-manager-3/index.html"/>,
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-patch">
   <title>Patch the Whole Cluster to the Latest Patches</title>
   <procedure>
    <step>
     <para>
      Apply latest patches of &productname; &prevproductnumber; and
      &prevcephos; to each &ceph; cluster node. Verify that correct software
      repositories are connected to each cluster node (see
      <xref linkend="upgrade-prepare-repos" />) and run &deepsea; stage.0:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      After stage.0 completes, verify that each cluster node's status includes
      'HEALTH_OK'. If not, resolve the problem before possible reboots in next
      steps.
     </para>
    </step>
    <step>
     <para>
      Run <command>zypper ps</command> to check for processes that may run with
      outdated libraries or binaries, and reboot if there are some.
     </para>
    </step>
    <step>
     <para>
      Verify that the running kernel is the latest available and reboot if not.
      Check outputs of the following commands:
     </para>
<screen>
&prompt.cephuser;uname -a
&prompt.cephuser;rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      Verify that the <package>ceph</package> package is version 12.2.12 or
      newer. Verify that the <package>deepsea</package> package is version
      0.8.9 or newer.
     </para>
    </step>
    <step>
     <para>
      If you previously used any of the <option>bluestore_cache</option>
      settings, they are not effective any more since <package>ceph</package>
      version 12.2.10. The new setting
      <option>bluestore_cache_autotune</option> that is set to 'true' by
      default disables manual cache sizing. To turn on the old behavior, you
      need to set <option>bluestore_cache_autotune=false</option>. Refer to
      <xref linkend="config-auto-cache-sizing" /> for details.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-verify-current">
  <title>Verify the Current Environment</title>

  <itemizedlist>
   <listitem>
    <para>
     If the system has obvious problems, fix them before starting the upgrade.
     Upgrade never fixes existing system problems.
    </para>
   </listitem>
   <listitem>
    <para>
     Check cluster performance. You can use commands such as <command>rados
     bench</command>, <command>ceph tell osd.* bench</command>, or
     <command>iperf3</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify access to gateways (such as &igw;, or &ogw;) and &rbd;.
    </para>
   </listitem>
   <listitem>
    <para>
     Document specific parts of the system setup, such as network setup,
     partitioning, or installation details.
    </para>
   </listitem>
   <listitem>
    <para>
     Use <command>supportconfig</command> to collect important system
     information and save it outside cluster nodes. Find more information in
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_admsupport_supportconfig.html"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure there is enough free disk space on each cluster node. Check free
     disk space with <command>df -h</command>. When needed, free disk space by
     removing unneeded files/directories or removing obsolete OS snapshots. If
     there is not enough free disk space, do not continue with the upgrade
     unless you free enough disk space.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-verify-state">
  <title>Check the Cluster's State</title>

  <itemizedlist>
   <listitem>
    <para>
     Check the <command>cluster health</command> command before starting the
     upgrade procedure. Do not start the upgrade unless each cluster node
     reports 'HEALTH_OK'.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that all services are running:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &smaster; and &smaster; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &mon; and &mgr; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &mds; server daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &osd; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &rgw; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &igw; daemons.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   The following commands provide details cluster state and specific
   configuration:
  </para>

  <variablelist>
   <varlistentry>
    <term><command>ceph -s</command></term>
    <listitem>
     <para>
      Prints a brief summary of &ceph; cluster health, running services, data
      usage, and IO statistics. Verify that it reports 'HEALTH_OK' before
      starting the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph health detail</command></term>
    <listitem>
     <para>
      Prints details if &ceph; cluster health is not OK.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph versions</command></term>
    <listitem>
     <para>
      Prints versions of running &ceph; daemons.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph df</command></term>
    <listitem>
     <para>
      Prints total and free disk space on the cluster. Do not start the upgrade
      if the cluster's free disk space is less than 25% of the total disk
      space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>salt '*' cephprocesses.check results=true</command></term>
    <listitem>
     <para>
      Prints running &ceph; processes and their PID's sorted by &sminion;s.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph osd dump | grep ^flags</command></term>
    <listitem>
     <para>
      Verify that 'recovery_deletes' and 'purged_snapdirs' flags are present.
      If not, you can force a scrub on all placement groups by running the
      following command. Be aware that this forced scrub may possibly have a
      negative impact on your &ceph; clients’ performance.
     </para>
<screen>
&prompt.cephuser;ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>Offline Upgrade of CTDB Clusters</title>

  <para>
   CTDB provides a clustered database used by &sgw;s. The CTDB protocol is very
   simple and does not support clusters of nodes communicating with different
   protocol versions. Therefore CTDB nodes need to be taken offline prior to
   performing an upgrade.
  </para>
 </sect1>
 <sect1 xml:id="upgrade-one-node">
  <title>Per Node Upgrade&mdash;Basic Procedure</title>

  <para>
   To ensure the core cluster services are available during the upgrade, you
   need to upgrade the cluster nodes sequentially one by one. There are two
   ways you can perform the upgrade of a node: either using <emphasis>the
   installer DVD</emphasis>, or using the <emphasis>distribution migration
   system</emphasis>.
  </para>

  <tip>
   <title>Orphaned Packages</title>
   <para>
    After a node is upgraded, a number of packages will be in an 'orphaned'
    state without a parent repository. This happens because python3 related
    packages do not obsolete python2 packages.
   </para>
   <para>
    Find more information about listing orphaned packages in
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_zypper.html#sec_zypper_softup_orphaned"/>.
   </para>
  </tip>

  <sect2 xml:id="upgrade-one-node-manual">
   <title>Manual Node Upgrade using the Installer DVD</title>
   <procedure>
    <step>
     <para>
      Reboot the node from the &cephos; installer DVD/image.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>Upgrade</guimenu> from the boot menu.
     </para>
    </step>
    <step>
     <para>
      On the <guimenu>Select the Migration Target</guimenu> screen, verify that
      '&cephos;' is selected and activate the <guimenu>Manually Adjust the
      Repositories for Migration</guimenu> check box.
     </para>
     <figure>
      <title>Select the Migration Target</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the following modules to install:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SUSE Enterprise Storage 6 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Basesystem Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Desktop Applications Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Legacy Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Server Applications Module 15 SP1 x86_64
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      On the <guimenu>Previously Used Repositories</guimenu> screen, verify
      that the correct repositories are selected. If the system is not
      registered with SCC/SMT, you need to add the repositories manually.
     </para>
     <para>
      &productname; &productnumber; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If you intend to migrate <systemitem>ntpd</systemitem> to
      <systemitem class="daemon">chronyd</systemitem> after SES migration
      (refer to <xref linkend="upgrade-ntp"/>), include the following
      repositories:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      NFS/SMB Gateway on SLE-HA on &cephos; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Review the <guimenu>Installation Settings</guimenu> and start the
      installation procedure by clicking <guimenu>Update</guimenu>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-one-node-auto">
   <title>Node Upgrade using the &suse; Distribution Migration System</title>
   <para>
    The <emphasis>Distribution Migration System</emphasis> (DMS) provides an
    upgrade path for an installed &sle; system from one major version to
    another. The following procedure utilizes DMS to upgrade &productname;
    &prevproductnumber; to version &productnumber; including the underlying
    &prevcephos; to &cephos; migration.
   </para>
   <para>
    Refer to
    <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>
    to find both general and detailed information about DMS.
   </para>
   <procedure>
    <step>
     <para>
      Install the migration RPM packages. They adjust the &grub; boot loader to
      automatically trigger the upgrade on next reboot. Install the
      <package>SLES15-SES-Migration</package> and
      <package>suse-migration-sle15-activation</package> packages:
     </para>
<screen>&sminion;zypper install SLES15-SES-Migration suse-migration-sle15-activation</screen>
    </step>
    <step>
     <para>
      If the node being upgraded is <emphasis role="bold">not</emphasis>
      registered with a repository staging system such as SCC, SMT, RMT, or
      &susemgr;, perform the following changes:
     </para>
     <substeps>
      <step>
       <para>
        Create the <filename>/etc/sle-migration-service.yml</filename> with the
        following content:
       </para>
<screen>use_zypper_migration: false</screen>
      </step>
      <step>
       <para>
        Disable or remove the SLE 12 SP3 and SES 5 repos, and add the SLE 15
        SP1 and SES6 repos. Find the list of related repositories in
        <xref linkend="upgrade-prepare-repos"/>.
       </para>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Reboot to start the upgrade. The node will reboot automatically after the
      upgrade.
     </para>
     <tip>
      <title>Upgrade Failure</title>
      <para>
       If the upgrade fails, inspect
       <filename>/var/log/distro_migration.log</filename>. Fix the
       problem, re-install the migration RPM packages, and reboot the node.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-adm">
  <title>Upgrade the &adm;</title>

  <itemizedlist>
   <listitem>
    <para>
     The following commands will still work, although &sminion;s are running
     old version of &ceph; and &salt;: <command>salt '*' test.ping</command>
     and <command>ceph status</command>
    </para>
   </listitem>
   <listitem>
    <para>
     After the upgrade of the &adm;, &oa; will no longer be installed.
    </para>
   </listitem>
   <listitem>
    <para>
     If the &adm; hosted SMT, complete its migration to RMT (refer to
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_migrate.html"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-mons">
  <title>Upgrade &mon;/&mgr; Nodes</title>

  <itemizedlist>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">does not use</emphasis> MDS roles,
     upgrade MON/MGR nodes one by one.
    </para>
   </listitem>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">uses</emphasis> MDS roles, and
     MON/MGR and MDS roles are co-located, you need to shrink the MDS cluster
     and then upgrade the co-located nodes. Refer to
     <xref linkend="upgrade-mds"/> for more details.
    </para>
   </listitem>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">uses</emphasis> MDS roles and they
     run on <emphasis role="bold">dedicated</emphasis> servers, upgrade all
     MON/MGR nodes one by one, then shrink the MDS cluster and upgrade it.
     Refer to <xref linkend="upgrade-mds"/> for more details.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>&mon; Upgrade</title>
   <para>
    Due to a limitation in the &mon; design, once two MONs have been upgraded
    to &productname; &productnumber; and have formed a quorum, the third MON
    (while still on &productname; &prevproductnumber;) will not rejoin the MON
    cluster if it restarted for any reason, including a node reboot. Therefore,
    when two MONs have been upgraded it is best to upgrade the rest as soon as
    possible.
   </para>
  </note>

  <para>
   <emphasis role="bold">Use the procedure described in
   <xref linkend="upgrade-one-node"/>.</emphasis>
  </para>
 </sect1>
 <sect1 xml:id="upgrade-mds">
  <title>Upgrade &mds;s</title>

  <para>
   You need to shrink the &mds; (MDS) cluster. Because of incompatible features
   between the &productname; &prevproductnumber; and &productnumber; versions,
   the older MDS daemons will shutdown as soon as they see a single SES
   &productnumber; level MDS join the cluster. Therefore it is necessary to
   shrink the MDS cluster to a single active MDS (and no standby's) for the
   duration of the MDS node upgrades. As soon as the second node is upgraded,
   you can extend the MDS cluster again.
  </para>

  <tip>
   <para>
    On a heavily loaded MDS cluster, you may need to to reduce the load (for
    example by stopping clients) so that a single active MDS is able to handle
    the workload.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Note the current value of the <option>max_mds</option> option:
    </para>
<screen>
&prompt.cephuser;ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     Shrink the MDS cluster if you have more then 1 active MDS daemons, i.e.
     <option>max_mds</option> is &gt; 1. To shrink the MDS cluster, run
    </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     where <replaceable>FS_NAME</replaceable> is the name of your &cephfs;
     instance ('cephfs' by default).
    </para>
   </step>
   <step>
    <para>
     Find the node hosting one of standby MDS daemons. Consult the output of
     the <command>ceph fs status</command> command and start the upgrade of the
     MDS cluster on this node.
    </para>
<screen>
&prompt.cephuser;ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     In this example, you need to start the upgrade procedure either on node
     'mon3-6' or 'mon2-6'.
    </para>
   </step>
   <step>
    <para>
     Upgrade the node with standby MDS daemon. After the upgraded MDS node
     starts, the outdated MDS daemons will shutdown automatically. At this
     point, clients may experience a short downtime of the &cephfs; service.
    </para>
    <para>
     <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </step>
   <step>
    <para>
     Upgrade the remaining MDS nodes.
    </para>
   </step>
   <step>
    <para>
     Reset <option>max_mds</option> to the desired configuration:
    </para>
<screen>
&prompt.smaster;ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-main-osd">
  <title>Upgrade &osd;s</title>

  <para>
   For each storage node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Identify which OSD daemons are running on a particular node:
    </para>
<screen>
&prompt.cephuser;ceph osd tree
</screen>
   </step>
   <step>
    <para>
     Set the 'noout' flag for each OSD daemon on the node that is being
     upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</screen>
    <para>
     Verify with:
    </para>
<screen>&prompt.cephuser;ceph health detail | grep noout</screen>
    <para>
     or
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</screen>
   </step>
   <step>
    <para>
     Create <filename>/etc/ceph/osd/*.json</filename> files for all existing
     OSDs by running the following command on the node that is going be
     upgraded:
    </para>
<screen>
&prompt.cephuser;ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     Upgrade the OSD node. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </step>
   <step>
    <para>
     Activate all OSDs found in the system:
    </para>
<screen>
&prompt.cephuser;ceph-volume simple activate --all
</screen>
    <tip>
     <title>Activating Data Partitions Individually</title>
     <para>
      If you want to activate data partitions individually, you need to find
      the correct <command>ceph-volume</command> command for each partition to
      activate it. Replace <replaceable>X1</replaceable> with the partition's
      correct letter/number:
     </para>
<screen>
 &prompt.cephuser;ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      For example:
     </para>
<screen>
&prompt.cephuser;ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      The last line of the output contains the command to activate the
      partition:
     </para>
<screen>
&prompt.cephuser;ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--> All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--> Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     Verify that the OSD node will start properly after the reboot.
    </para>
   </step>
   <step>
    <para>
     Address the 'Legacy BlueStore stats reporting detected on XX OSD(s)'
     message:
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
    <para>
     The warning is normal when upgrading &ceph; to 14.2.2. You can disable it
     by setting:
    </para>
<screen>bluestore_warn_on_legacy_statfs = false</screen>
    <para>
     The proper fix is to run the following command on all OSDs while they are
     stopped:
    </para>
<screen>&prompt.cephuser;ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</screen>
    <para>
     Following is a helper script that runs the <command>ceph-bluestore-tool
     repair</command> for all OSDs on the <replaceable>NODE_NAME</replaceable>
     node:
    </para>
<screen>OSDNODE=<replaceable>OSD_NODE_NAME</replaceable>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE cmd.run 'systemctl stop ceph-osd@$OSD';\
 salt $OSDNODE cmd.run 'ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD';\
 salt $OSDNODE cmd.run 'systemctl start ceph-osd@$OSD';\
 done</screen>
   </step>
   <step>
    <para>
     Unset the 'noout' flag for each OSD daemon on the node that is upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</screen>
    <para>
     Verify with:
    </para>
<screen>&prompt.cephuser;ceph health detail | grep noout</screen>
    <para>
     Note:
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
   </step>
   <step>
    <para>
     Verify the cluster status. It will be similar to the following output:
    </para>
<screen>
&prompt.cephuser;ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
    <tip>
     <title>Check for the Version of Cluster Components/Nodes</title>
     <para>
      When you need to find out the versions of individual cluster components
      and nodes&mdash;for example to find out if all your nodes are actually on
      the same patch level after the upgrade&mdash;you can run
     </para>
<screen>&prompt.smaster;salt-run status.report</screen>
     <para>
      The command goes through the connected &sminion;s and scans for the
      version numbers of &ceph;, &salt;, and &sls;, and gives you a report
      displaying the version that the majority of nodes have and showing nodes
      whose version is different from the majority.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Verify that all OSD nodes were rebooted and that OSDs started
     automatically after the reboot.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-appnodes-order">
  <title>Upgrade Application Nodes</title>

  <para>
   Upgrade application nodes in the following order:
  </para>

  <orderedlist>
   <listitem>
    <para>
     &ogw;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the &ogw;s are fronted by a load balancer, then a rolling upgrade of
       the &ogw;s should be possible without an outage.
      </para>
     </listitem>
     <listitem>
      <para>
       Validate that the &ogw; daemons are running after each upgrade, and test
       with S3/&swift; client.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Use the procedure described in
       <xref linkend="upgrade-one-node"/>.</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &igw;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If &iscsi; initiators are configured with multipath, then a rolling
       upgrade of the &igw;s should be possible without an outage.
      </para>
     </listitem>
     <listitem>
      <para>
       Validate that the <systemitem class="daemon">lrbd</systemitem> daemon is
       running after each upgrade, and test with initiator.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Use the procedure described in
       <xref linkend="upgrade-one-node"/>.</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &ganesha;. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     &sgw;s. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="upgrade-main-policy">
  <title>Update <filename>policy.cfg</filename> and Deploy &dashboard; using &deepsea;</title>

  <para>
   On the &adm;, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and apply the
   following changes:
  </para>

  <important>
   <title>No New Services</title>
   <para>
    During cluster upgrade, do not add new services to the
    <filename>policy.cfg</filename> file. Change the cluster architecture only
    after the upgrade is completed.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Remove <literal>role-openattic</literal>.
    </para>
   </step>
   <step>
    <para>
     Add <literal>role-prometheus</literal> and <literal>role-grafana</literal>
     to the node that had &prometheus; and &grafana; installed, usually the
     &adm;.
    </para>
   </step>
   <step>
    <para>
     Role <literal>profile-<replaceable>PROFILE_NAME</replaceable></literal> is
     now ignored. Add new corresponding role, <literal>role-storage</literal>
     line. For example, for existing
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     add
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     Synchronize all &salt; modules:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     Update the &spillar; by running &deepsea; stage.1 and stage.2:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Cleanup &oa;:
    </para>
<screen>&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic</screen>
   </step>
   <step>
    <para>
     Unset the 'restart_igw' grain to prevent stage.0 from restarting &igw;
     which is not installed yet:
    </para>
<screen>&smaster;salt '*' grains.delkey restart_igw</screen>
   </step>
   <step>
    <para>
     Finally, run through &deepsea; stages 0-4:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <tip>
     <title>'subvolume missing' Errors during stage.3</title>
     <para>
      &deepsea; stage.3 may fail with an error similar to the following:
     </para>
<screen>subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</screen>
     <para>
      In this case, you need to edit
      <filename role="bold">/srv/pillar/ceph/stack/global.yml</filename> and
      add the following line:
     </para>
<screen>subvolume_init: disabled</screen>
     <para>
      Then refresh the &spillar; and re-run &deepsea; stage.3:
     </para>
<screen>&prompt.smaster;salt '*' saltutil.refresh_pillar
 &prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     <para>
      After &deepsea; successfully finished stage.3, the &dashboard; will be
      running. Refer to <xref linkend="ceph-dashboard"/> for a detailed
      overview of &dashboard; features.
     </para>
     <para>
      To list node running dashboard run:
     </para>
<screen>&prompt.cephuser;ceph mgr services | grep dashboard</screen>
     <para>
      To list admin credentials run:
     </para>
<screen>&prompt.smaster;salt-call grains.get dashboard_creds</screen>
    </tip>
   </step>
   <step>
    <para>
     Sequentially restart the &ogw; services to use 'beast' Web server instead
     of the outdated 'civetweb':
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw.force</screen>
   </step>
   <step>
    <para>
     Before you continue, we strongly recommend enabling the &ceph; telemetry
     module. For more information, see <xref linkend="mgr-modules-telemetry"/>
     for information and instructions.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-drive-groups">
  <title>Migration from Profile-based Deployments to &drvgrps;</title>

  <para>
   In &productname; &prevproductnumber;, &deepsea; offered so called 'profiles'
   to describe the layout of your OSDs. Starting with &productname;
   &productnumber;, we moved to a different approach called
   <emphasis>&drvgrps;</emphasis> (find more details in
   <xref linkend="ds-drive-groups" />).
  </para>

  <note>
   <para>
    Migrating to the new approach is not immediately mandatory. Destructive
    operations, such as <command>salt-run osd.remove</command>,
    <command>salt-run osd.replace</command>, or <command>salt-run
    osd.purge</command> are still available. However, adding new OSDs will
    require your action.
   </para>
  </note>

  <para>
   Because of the different approach of these implementations, we do not offer
   an automated migration path. However, we offer a variety of
   tools&mdash;&salt; runners&mdash;to make the migration as simple as
   possible.
  </para>

  <sect2>
   <title>Analyze the Current Layout</title>
   <para>
    To view information about the currently deployed OSDs, use the following
    command:
   </para>
<screen>
&prompt.smaster;salt-run disks.discover
</screen>
   <para>
    Alternatively, you can inspect the content of files in the
    <filename>/srv/pillar/ceph/proposals/profile-*/</filename> directories. The
    have similar structure to the following:
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>Create &drvgrps; Matching the Current Layout</title>
   <para>
    Refer to <xref linkend="ds-drive-groups-specs" /> for more details on
    &drvgrps; specification.
   </para>
   <para>
    The difference between a fresh deployment and upgrade scenario is that the
    drives to be migrated are already 'used'. Because
   </para>
<screen>
&prompt.smaster;salt-run disks.list
</screen>
   <para>
    looks for unused disks only, use
   </para>
<screen>
&prompt.smaster;salt-run disks.list include_unavailable=True
</screen>
   <para>
    Adjust &drvgrps; until you match your current setup. For a more visual
    representation of what will be happening, use the following command. Note
    that it has no output if there are no free disks:
   </para>
<screen>
&prompt.smaster;salt-run disks.report bypass_pillar=True
</screen>
   <para>
    If you verified that your &drvgrps; are properly configured and want to
    apply the new approach, remove files form the
    <filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename>
    directory, remove corresponding
    <literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal>
    lines from the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    file, and run &deepsea; stage.2 to refresh the &spillar;.
   </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
   <para>
    Verify the result by running the following commands:
   </para>
<screen>
&prompt.smaster;salt target_node pillar.get ceph:storage
&prompt.smaster;salt-run disks.report
</screen>
   <warning>
    <title>Incorrect &drvgrps; Configuration</title>
    <para>
     If your &drvgrps; are not properly configured and there are spare disks in
     your setup, they will be deployed in the way you specified them. We
     recommend running:
    </para>
<screen>
&prompt.smaster;salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2 xml:id="upgrade-osd-deployment">
   <title>OSD Deployment</title>
   <para>
    For simple cases such as standalone OSDs, the migration will happen
    over-time. Whenever you remove or replace an OSD from the cluster, it will
    be replaced by a new, LVM based OSD.
   </para>
   <tip>
    <title>Migrate to LVM Format</title>
    <para>
     Whenever a single 'legacy' OSD needs to be replaced on a node, all OSDs
     that share devices with it need to be migrated to the LVM-based format.
    </para>
    <para>
     For completeness, consider migrating OSDs on the whole node.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>More Complex Setups</title>
   <para>
    If you have a more sophisticated setup than just sandalone OSDs, for
    example dedicated WAL/DBs or encrypted OSDs, the migration can only happen
    when all OSDs assigned to that WAL/DB device are removed. This is caused by
    the <command>ceph-volume</command> that creates Logical Volumes on disks
    before the deployment. This prevents the user from mixing partition based
    deployments with LV based deployments. In such cases it is best to manually
    remove all OSDs that are assigned to a WAL/DB device and re-deploy them
    using the &drvgrps; approach.
   </para>
  </sect2>
 </sect1>
</chapter>
