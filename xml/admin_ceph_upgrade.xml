<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Upgrading from &prevproductnumber; to
  &productnumber; means the following:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrading from Ceph Nautilus to Octopus.
   </para>
  </listitem>
  <listitem>
   <para>
    Switching from installing and running &ceph; via RPM packages to running in
    containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Complete removal of &deepsea; and replacing with &cephsalt; and &cephadm;.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   The upgrade information in this chapter <emphasis>only</emphasis> applies to
   upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
   instructions if you want to deploy &productname; on the &casp; platform.
  </para>
 </warning>
 <para>
  Upgrading from &productname; versions older than &prevproductnumber; is not
  supported. You first need to upgrade to the latest version of &productname;
  &prevproductnumber; and then follow the steps in this chapter.
 </para>
 <sect1 xml:id="before-upgrade">
  <title>Before Upgrading</title>

  <para>
   Before upgrading, ensure you read through the following sections to ensure
   you understand all tasks that need to be executed.
  </para>

  <sect2 xml:id="upgrade-consider-points">
   <title>Points to Consider</title>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Read the release notes</emphasis> - there you can find
      additional information on changes since the previous release of
      &productname;. Check the release notes to see whether:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Your hardware needs special considerations.
       </para>
      </listitem>
      <listitem>
       <para>
        Any used software packages have changed significantly.
       </para>
      </listitem>
      <listitem>
       <para>
        Special precautions are necessary for your installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </para>
     <para>
      After having installed the package <package>release-notes-ses</package>,
      find the release notes locally in the directory
      <filename>/usr/share/doc/release-notes</filename> or online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a long time&mdash;approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      You have to upgrade the &smaster; first, and replace &deepsea; with
      &cephsalt; and &cephadm; after. You will <emphasis>not</emphasis> be able
      to start using the &cephadm; orchestrator module until at least all
      &mgr;'s are upgraded.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade from using Nautilus RPMs to Octopus Containers needs to
      happen all in one step. This means upgrade an entire node at a time, not
      one daemon at a time.
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD migration from FileStore to Bluestore <emphasis>needs</emphasis>
      to happen before the upgrade as it is unsupported in &productname;
      &productnumber;. See <xref linkend="filestore2bluestore"/> for more
      information.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are running an older cluster that still uses
      <literal>ceph-disk</literal> OSDs, you <emphasis>need</emphasis> to
      switch to <literal>ceph-volume</literal> before the upgrade.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Backing Up Cluster Configuration and Data</title>
   <para>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to &productname; &productnumber;. For instruction on
    how to back up all your data, see
    <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verify Steps From Previous Upgrade</title>
   <para>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </para>
   <para>
    Check for the existence of the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
    file.
   </para>
   <para>
    This file is created by the engulf process during the upgrade from
    &productname; 5 to 6. The <option>configuration_init:
    default-import</option> option is set in
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    If <option>configuration_init</option> is still set to
    <option>default-import</option>, the cluster is using
    <filename>ceph.conf.import</filename> as its configuration file and not
    &deepsea;'s default <filename>ceph.conf</filename> which is compiled from
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Therefore, you need to inspect <filename>ceph.conf.import</filename> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Then remove the <option>configuration_init: default-import</option> line
    from
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
<!-- Notes: This section should also contain information on:
1. installing latest SES 6 updates (before check upgrade steps)
2. Checking the cluster state (before check upgrade steps)
3. Ensuring you have enough disk space -->
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrading &smaster;</title>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrading the &mon; and &mgr; Nodes</title>
 </sect1>
 <sect1 xml:id="upgrade-osd-nodes">
  <title>Upgrading the OSD Nodes</title>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Upgrading the Gateways</title>

  <sect2 xml:id="upgrade-mds">
   <title>Upgrading the &mds;</title>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrading the &igw;</title>
  </sect2>

  <sect2 xml:id="upgrade-ogw">
   <title>Upgrading the &ogw;</title>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrading &ganesha;</title>
   <para>
    The following demonstrates how to migrate an existing &ganesha; service
    running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
   </para>
   <warning>
    <para>
     The following documentation requires you to have already successfully
     upgraded the core &ceph; services.
    </para>
   </warning>
   <para>
    &ganesha; stores additional per-daemon config and export configs in a
    &rados; pool. The configured &rados; pool can be found on the
    <literal>watch_url</literal> line of the <literal>RADOS_URLS</literal>
    block in the <filename>ganesha.conf</filename> file.
   </para>
   <para>
    Before attempting any migration, we strongly recommend to make a copy of
    the export and daemon config objects located in the RADOS pool To locate
    the configured RADOS pool, run the following command:
   </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    To list the contents of the RADOS pool:
   </para>
<screen>&prompt.cephuser;rados --pool cephfs_data --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    To copy the RADOS objects:
   </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    On a per node basis, an existing &ganesha; service needs to be stopped and
    then replaced with a container managed by &cephadm;.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &ganesha; service:
     </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      After the existing &ganesha; service has been stopped, a new one can be
      deployed in a container using &cephadm;. To do so, you need to create a
      service specification that contains a <literal>service_id</literal> that
      will be used to identify this new NFS cluster, the hostname of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
     </para>
     <para>
      Deploy a new &ganesha; container by creating a new placement
      specification. For information on creating a placement specification see
      <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Apply the placement specification:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirm the &ganesha; daemon is running on the host:
     </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
   </procedure>
   <para>
    The existing exports can be migrated in two different ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Manually re-created or re-assigned using the &dashboard;.
     </para>
<!-- depends on https://github.com/ceph/ceph/pull/36948 -->
    </listitem>
    <listitem>
     <para>
      Manually copy the contents of each per-daemon &rados; object into the
      newly created &ganesha; common configuration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manually Copying Exports to &ganesha; Common Configuration File</title>
    <step>
     <para>
      Determine the list of per-daemon RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Make a copy of the per-daemon &rados; objects:
     </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.foo
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sort and merge into a single list of exports:
     </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.foo
&prompt.cephuser;cat conf-nfs.foo
%url "rados://cephfs_data/ganesha/export-1"
%url "rados://cephfs_data/ganesha/export-2"
%url "rados://cephfs_data/ganesha/export-3"
%url "rados://cephfs_data/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Write the new &ganesha; common configuration file:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.foo conf-nfs.foo</screen>
    </step>
    <step>
     <para>
      Notify the &ganesha; daemon:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.foo conf-nfs.foo</screen>
     <note>
      <para>
       This action will cause the daemon to reload the configuration.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    After the service has been successfully migrated, the Nautilus based
    &ganesha; service can be removed.
   </para>
   <procedure>
    <step>
     <para>
      Remove &ganesha;:
     </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remove the legacy cluster settings from the &dashboard;:
     </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard set-ganesha-clusters-rados-pool-namespace cluster2:pool2/ns2
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Post-upgrade Clean-up</title>

<screen>
  # ceph versions
  # ceph osd require-osd-release octopus
  # ceph mgr module enable pg_autoscaler
  # ceph mgr module enable telemetry
  # ceph telemetry on
  # ceph osd set require-min-compat-client luminous
  # ceph balancer mode upmap
  # ceph balancer on
  [...]
</screen>
 </sect1>
 <sect1 xml:id="filestore2bluestore">
  <title>OSD Migration to &bluestore;</title>

  <para>
   OSD &bluestore; is the back-end for the OSD daemons. It is the default
   option since &productname; 5 and required for &productname; &productnumber;.
   Compared to &filestore;, which stores objects as files in an XFS file
   system, &bluestore; can deliver increased performance because it stores
   objects directly on the underlying block device. &bluestore; also enables
   other features, such as built-in compression and EC overwrites, that are
   unavailable with &filestore;.
  </para>

  <para>
   You must upgrade to &bluestore; before starting your upgrade to
   &productname; &productnumber;. For instruction on how to upgrade from
   &filestore; to &bluestore;, see
   <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
  </para>
 </sect1>
</chapter>
