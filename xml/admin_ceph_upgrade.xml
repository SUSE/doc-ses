<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package> ,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster, you need to have both the underlying
   &sls; and &storage; correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &mon;s
    </para>
   </listitem>
   <listitem>
    <para>
     &mgr;s
    </para>
   </listitem>
   <listitem>
    <para>
     &osd;s
    </para>
   </listitem>
   <listitem>
    <para>
     &mds;s
    </para>
   </listitem>
   <listitem>
    <para>
     &rgw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &igw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &ganesha;
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>&prompt.root;ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>&prompt.root;ceph osd stat</screen>
  </tip>

  <tip xml:id="ceph.upgrade.luminous_flag">
   <title>Set require-osd-release luminous Flag</title>
   <para>
    When the last OSD is upgraded to &storage; 5, the monitor nodes will detect
    that all OSDs are running the 'luminous' version of &ceph; and they may
    complain that the <option>require-osd-release luminous</option> osdmap flag
    is not set. In that case, you need to set this flag manually to acknowledge
    that&mdash;now that the cluster has been upgraded to 'luminous'&mdash;it
    cannot be downgraded back to &ceph; 'jewel'. Set the flag by running the
    following command:
   </para>
<screen>&prompt.sminion;sudo ceph osd require-osd-release luminous</screen>
   <para>
    After the command completes, the warning disappears.
   </para>
   <para>
    On fresh installs of &storage; 5, this flag is set automatically when the
    &ceph; monitors create the initial osdmap, so no end user action is needed.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>Upgrade from &storage; 4 (&deepsea; Deployment) to 5</title>

  <important xml:id="u4to5.softreq">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In addition, before starting the upgrade, you need to upgrade the &smaster;
    node to &sls; 12 SP3 and &storage; 5 by running <command>zypper
    migration</command> (or your preferred way of upgrading).
   </para>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Check whether the &aa; service is running and disable it on each cluster
      node. Start the &yast; &aa; module, select <guimenu>Settings</guimenu>,
      and then deactivate the <guimenu>Enable Apparmor</guimenu> check box.
      Confirm with <guimenu>Done</guimenu>.
     </para>
     <para>
      Note that &storage; will <emphasis>not</emphasis> work with &aa; enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order, based on their assigned role as recommended by &ceph; upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent the prescribed order from being
      violated if a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply, for example, new kernel versions. To
      reduce waiting I/O operations, we recommend declining incoming requests
      for the duration of the upgrade process.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Since &ceph; Luminous, the <option>osd crush location</option> config
      option is no longer supported. Please update your &deepsea; configuration
      files to use <command>crush location</command> before upgrading.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &storage; 4 cluster to version 5, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.root;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.root;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Using <command>rpm -q deepsea</command>, verify that the version of the
     &deepsea; package on the &smaster; node starts with at least
     <literal>0.7</literal>. For example:
    </para>
<screen>&prompt.root;rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     If the &deepsea; package version number starts with 0.6, double check
     whether you successfully migrated the &smaster; node to &sls; 12 SP3 and
     &storage; 5 (refer to <xref linkend="u4to5.softreq"/> at the beginning of
     this section). This is a prerequisite that must be completed before
     starting the upgrade procedure.
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken. Continue with
       <xref linkend="step.updatepillar"/>.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, change your Pillar data in order to
       use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-dup</screen>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     Update your Pillar:
    </para>
    <screen>&prompt.smaster;salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     See <xref linkend="ds.minion.targeting"/> for details about &sminion;s
     targeting.
    </para>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Upgrade &sminion;s:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to <xref linkend="ds.depl.s1"/> of
     <xref linkend="ds.depl.stages"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Check that &aa; is disabled and stopped on all nodes after the upgrade:
    </para>
<screen>&prompt.root;systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     After the upgrade, the &mgr;s are not installed yet. To reach a healthy
     cluster state, do the following:
    </para>
    <substeps>
     <step>
      <para>
       Run Stage 0 to enable the &salt; REST API:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       Run Stage 1 to create the <filename>role-mgr/</filename> subdirectory:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edit <guimenu>policy.cfg</guimenu> as described in
       <xref linkend="policy.configuration"/> and add a &mgr; role to the nodes
       where &mon;s are deployed. Also add the &oa; role to your admin node.
      </para>
     </step>
     <step>
      <para>
       Run Stage 2 to update the Pillar:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       &deepsea; uses a different approach to generate the
       <filename>ceph.conf</filename> configuration file now, refer to
       <xref
        linkend="ds.custom.cephconf"/> for more details.
      </para>
     </step>
     <step>
      <para>
       Run Stage 3 to deploy &mgr;s:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       Run Stage 4 to configure &oa; properly:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>&ceph; Key Caps Mismatch</title>
     <para>
      If <literal>ceph.stage.3</literal> fails with "Error EINVAL: entity
      client.bootstrap-osd exists but caps do not match", it means the key
      capabilities (caps) for the existing cluster's
      <literal>client.bootstrap.osd</literal> key do not match the caps that
      &deepsea; is trying to set. Above the error message, in red text, you can
      see a dump of the <command>ceph auth</command> command that failed. Look
      at this command to check the key ID and file being used. In the case of
      <literal>client.bootstrap-osd</literal>, the command will be
     </para>
<screen>&prompt.root;ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      To fix mismatched key caps, check the content of the keyring file
      &deepsea; is trying to deploy, for example:
     </para>
<screen>&prompt.cephuser;cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Compare this with the output of <command>ceph auth get
      client.bootstrap-osd</command>:
     </para>
<screen>&prompt.root;ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Note how the latter key is missing <literal>caps mgr = "allow
      r"</literal>. To fix this, run:
     </para>
<screen>&prompt.root;ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      Running <literal>ceph.stage.3</literal> should now succeed.
     </para>
     <para>
      The same issue can occur with &mds; and &ogw; keyrings when running
      <literal>ceph.stage.4</literal>. The same procedure as above applies:
      check the command that failed, the keyring file being deployed, and the
      caps of the existing key. Then run <command>ceph auth caps</command> to
      update the existing key caps to match what is being deployed by
      &deepsea;.
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrading to &storage; 5, FileStore OSDs need approximately five
    minutes longer to start as the OSD will do a one-off conversion of its
    on-disk files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade&mdash;you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, and &sls;, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>OSD Migration to &bluestore;</title>
   <para>
    OSD &bluestore; is a new back end for the OSD daemons. It is the default
    option since &storage; 5. Compared to FileStore, which stores objects as
    files in an XFS filesystem, &bluestore; can deliver increased performance
    because it stores objects directly on the underlying block device.
    &bluestore; also enables other features, such as built-in compression and
    EC overwrites, that are unavailable with FileStore.
   </para>
   <para>
    Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a &bluestore; OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on faster/different
    media.
   </para>
   <para>
    In SES5, both FileStore and &bluestore; are supported and it is possible
    for FileStore and &bluestore; OSDs to co-exist in a single cluster. During
    the SUSE Enterprise Storage upgrade procedure, FileStore OSDs are not
    automatically converted to &bluestore;. Be aware that the
    &bluestore;-specific features will not be available on OSDs that have not
    been migrated to &bluestore;.
   </para>
   <para>
    Before converting to &bluestore;, the OSDs need to be running &storage; 5.
    The conversion is a slow process as all data gets re-written twice. Though
    the migration process can take a long time to complete, there is no cluster
    outage and all clients can continue accessing the cluster during this
    period. However, do expect lower performance for the duration of the
    migration. This is caused by rebalancing and backfilling of cluster data.
   </para>
   <para>
    Use the following procedure to migrate FileStore OSDs to &bluestore;:
   </para>
   <procedure>
    <step>
     <para>
      Migrate hardware profiles:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.policy</screen>
     <para>
      This runner migrates any hardware profiles currently in use by the
      <filename>policy.cfg</filename> file. It processes
      <filename>policy.cfg</filename>, finds any hardware profile using the
      original data structure, and converts it to the new data structure. The
      result is a new hardware profile named
      'migrated-<replaceable>original_name</replaceable>'.
      <filename>policy.cfg</filename> is updated as well.
     </para>
     <para>
      If the original configuration had separate journals, the &bluestore;
      configuration will use the same device for the 'wal' and 'db' for that
      OSD.
     </para>
    </step>
    <step>
     <para>
      &deepsea; migrates OSDs by setting their weight to 0 which 'vacuums' the
      data until the OSD is empty. You can either migrate OSDs one by one, or
      all OSDs at once. In either case, when the OSD is empty, the
      orchestration removes it and then re-creates it with the new
      configuration.
     </para>
     <tip>
      <title>Recommended Method</title>
      <para>
       Use <command>ceph.migrate.nodes</command> if you have a large number of
       physical storage nodes or almost no data. If one node represents less
       than 10% of your capacity, then the
       <command>ceph.migrate.nodes</command> may be marginally faster moving
       all the data from those OSDs in parallel.
      </para>
      <para>
       If you are not sure about which method to use, or the site has few
       storage nodes (for example each node has more than 10% of the cluster
       data), then select <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        To migrate OSDs one at a time, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        To migrate all OSDs on each node in parallel, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       As the orchestration gives no feedback about the migration progress, use
      </para>
<screen>&prompt.root;ceph osd tree</screen>
      <para>
       to see which OSDs have a weight of zero periodically.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    After the migration to &bluestore;, the object count will remain the same
    and disk usage will be nearly the same.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>Upgrade from &storage; 4 (<command>ceph-deploy</command> Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In addition, before starting the procedure below, you need to upgrade the
    admin node to &sls; 12 SP3 and &storage; 5 by running <command>zypper
    migration</command> (or your preferred way of upgrading).
   </para>
  </important>

  <para>
   To upgrade the &storage; 4 cluster which was deployed with
   <command>ceph-deploy</command> to version 5, follow these steps:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>Steps to Apply to All Cluster Nodes</title>
   <step>
    <para>
     Install the <systemitem>salt</systemitem> package from SLE-12-SP2/SES4:
    </para>
<screen>&prompt.root;zypper install salt</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>salt-minion</systemitem> package from
     SLE-12-SP2/SES4, then enable and start the related service:
    </para>
<screen>&prompt.root;zypper install salt-minion
&prompt.root;systemctl enable salt-minion
&prompt.root;systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Ensure that the host name 'salt' resolves to the IP address of the
     &smaster;/admin node. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     If you use SCC-registered systems, take no further action. Otherwise
     prepare software repositories. Disable all repositories except for &sls;
     12 SP3 and &storage; 5 repositories. If these repositories are missing,
     add them.
    </para>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Steps to Apply to the Admin Node/&smaster;</title>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&smaster;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&smaster;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Upgrade the admin node to &sls; 12 SP3 and &storage; 5. If you use
     SCC-registered systems, we recommend <command>zypper migration</command>.
     After the upgrade, ensure that only repositories for &sls; 12 SP3 and
     &storage; 5 are active (and refreshed) on the admin node before proceeding.
    </para>
   </step>
   <step>
    <para>
     Install the <systemitem>salt-master</systemitem> package, then enable and
     start the related service (this makes the admin node also the master node):
    </para>
<screen>&prompt.smaster;zypper install salt-master
&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verify the presence of all &sminion;s by listing their keys:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Add all &sminion;s keys to &smaster; including the minion master:
    </para>
<screen>&prompt.smaster;salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Ensure that all &sminion;s' keys were accepted:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Make sure that the software on your admin node is up to date:
    </para>
<screen>&prompt.smaster;zypper migration</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>deepsea</systemitem> package:
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to <xref linkend="ds.depl.s1"/> of
     <xref linkend="ds.depl.stages"/> for more details.
    </para>
   </step>
   <step>
    <para>
     Engulf the existing <command>ceph-deploy</command> installed cluster:
    </para>
<screen>&prompt.smaster;salt-run populate.engulf_existing_cluster</screen>
    <para>
     The command will do the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribute all the required &salt; and &deepsea; modules to all the
       &sminion;s.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspect the running &ceph; cluster and populate
       <filename>/srv/pillar/ceph/proposals</filename> with a layout of the
       cluster.
      </para>
      <para>
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> will be
       created with roles matching all detected running &ceph; services. View
       this file to verify that each of your existing MON, OSD, RGW and MDS
       nodes have the appropriate roles. OSD nodes will be imported into the
       <filename>profile-import/</filename> subdirectory, so you can examine
       the files in
       <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename>
       and
       <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename>
       to confirm that the OSDs were correctly picked up.
      </para>
      <note>
       <para>
        The generated <filename>policy.cfg</filename> will only apply roles for
        detected &ceph; services 'role-mon', 'role-mgr', 'role-mds',
        'role-rgw', 'role-admin', and 'role-master' for the &smaster; node. Any
        other desired roles will need to be added to the file manually (see
        <xref linkend="policy.role.assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The existing cluster's <filename>ceph.conf</filename> will be saved to
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
       will include the cluster's fsid, cluster and public networks, and also
       specifies the <option>configuration_init: default-import</option>
       option, which makes &deepsea; use the
       <filename>ceph.conf.import</filename> configuration file mentioned
       previously, rather than using &deepsea;'s default
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
       template.
      </para>
     </listitem>
     <listitem>
      <para>
       The cluster's various keyrings will be saved to the following
       directories:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mgr/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Run Stage 1 to create all possible roles:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Generate required subdirectories under
     <filename>/srv/pillar/ceph/stack</filename>:
    </para>
<screen>&prompt.smaster;salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     Verify that there is a working &deepsea;-managed cluster with correctly
     assigned roles:
    </para>
    <substeps>
     <step>
      <para>
       If &deepsea; is the only &salt; consumer in your environment, run:
      </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get roles</screen>
     </step>
     <step>
      <para>
       If you target the minions using &deepsea; grain, run:
      </para>
<screen>&prompt.smaster;salt -G 'deepsea:*' pillar.get roles</screen>
     </step>
    </substeps>
    <para>
     Compare the output with the actual layout of the cluster.
    </para>
   </step>
   <step>
    <para>
     From this point on, follow the procedure described in
     <xref linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>Upgrade from &storage; 3 to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 3 cluster to version 5, follow the steps described
   in <xref linkend="upgrade4to5cephdeploy.all"/> and then
   <xref linkend="upgrade4to5cephdeploy.admin"/>.
  </para>
 </sect1>
</chapter>
