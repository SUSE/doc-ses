<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Note that version &prevproductnumber; is basically 5
  with all latest patches applied.
 </para>
 <note>
  <title>Upgrade from Older Releases not Supported</title>
  <para>
   The upgrade from &productname; version older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow steps in this chapter.
  </para>
 </note>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package>,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="upgrade.consider_points">
  <title>Points to Consider before the Upgrade</title>

  <itemizedlist>
   <listitem>
    <para>
     A single node cannot be upgraded while running the previous &sls; release,
     but needs to be rebooted into the new version's installer. Therefore the
     services that the node provides will be unavailable for some time. The
     core cluster services will still be available&mdash;for example if one MON
     is down during upgrade, there are still at least two active MONs.
     Unfortunately, single instance services, such as a single &igw;, will be
     unavailable.
    </para>
    <important>
     <title>Offline Upgrade of CTDB Clusters</title>
     <para>
      CTDB provides a clustered database used by &sgw;s. The CTDB protocol is
      very simple and does not support clusters of nodes communicating with
      different protocol versions. Therefore CTDB nodes need to be taken
      offline prior to performing an upgrade.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     Certain types of daemons depend upon others. For example &ceph; &rgw;s
     depend upon &ceph; MON and OSD daemons. We recommend upgrading in this
     order:
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       &adm;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;s
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;s
      </para>
     </listitem>
     <listitem>
      <para>
       &mgr;s
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ogw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &igw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ganesha;
      </para>
     </listitem>
     <listitem>
      <para>
       &sgw;s
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     If possible, upgrade all the daemons of a specific type&mdash;for example
     all monitor daemons or all OSD daemons&mdash;in a row to ensure that they
     are all on the same release version. Also, upgrade all daemons in your
     cluster before trying to exercise new functionality in a release.
    </para>
    <para>
     After all daemons of a specific type are upgraded, check their status.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Ensure that each monitor has rejoined the quorum after all monitors are
       upgraded.
      </para>
     </listitem>
     <listitem>
      <para>
       Ensure each &osd; daemon has rejoined the cluster after all OSDs are
       upgraded.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     If you used &aa; in either 'complain' or 'enforce' mode, you need to set a
     &spillar; variable before upgrading. Because &cephos; ships with &aa; by
     default, &aa; management was integrated into &deepsea; stage.0. The
     default behavior in &productname; &productnumber; is to remove &aa; and
     related profiles. If you want to retain the behavior configured in
     &productname; &prevproductnumber;, verify that one of the following lines
     is present in the <filename>/srv/pillar/ceph/stack/global.yml</filename>
     file before starting the upgrade:
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     or
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     Since &productname; &productnumber;, MDS names starting with a digit are
     no longer allowed and MDS daemons will refuse to start. You can check
     whether your daemons have such names either by running the <command>ceph
     fs status</command> command, or by restarting a MDS and checking its logs
     for the following message:
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     If you see the above message, the MDS names will need to be migrated
     before attempting to upgrade to &productname; &productnumber;. &deepsea;
     provides an orchestration to automate such migration. MDS names starting
     with a digit will be prepended with 'mds.'.
    </para>
    <tip>
     <title>Custom MDS Names</title>
     <para>
      If you have MDS configuration settings that specify an MDS name, verify
      that the configuration will also reflect the new MDS names.
      (mds.<replaceable>OLD_DAEMON_NAME</replaceable>).
     </para>
    </tip>
    <para>
     After you adjust the configuration or if you do not customize MDS names,
     run the migration:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <para>
     This will add MDS daemons with the new names before removing the old MDS
     daemons. The number of MDS daemons will double for a short time. Clients
     will be able to access &cephfs; with a short pause to failover. Therefore
     plan the migration for times when you expect little or no &cephfs; load.
    </para>
   </listitem>
   <listitem>
    <para>
     In case old RBD kernel clients (older than &prevcephos;) are being used,
     refer to <xref linkend="rbd.old_clients_map" />.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure there is enough free disk space on each cluster node. If there is
     not enough free disk space, do not continue with the upgrade unless you
     free enough disk space.
    </para>
   </listitem>
   <listitem>
    <para>
     Check <command>cluster health</command> before starting the upgrade
     procedure and do not start the upgrade unless each cluster node reports
     'HEALTH_OK'.
    </para>
   </listitem>
   <listitem>
    <para>
     If &oa; is located on the &adm;, it will be unavailable after you upgrade
     the node. The new &dashboard; will not be available until you deploy it by
     using &deepsea;.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster upgrade may take a long time&mdash;approximately the time it
     takes to upgrade one machine multiplied by the number of cluster nodes.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade.main">
  <title>Roadmap of Upgrade Steps</title>

  <para>
   This section provides detailed steps that you need to follow to successfully
   upgrade the whole &ceph; cluster. By 'upgrading' a node, we mean following
   the steps described in <xref linkend="upgrade.one_node" />.
  </para>

  <procedure>
   <step>
    <para>
     Back up important files and data. Refer to
     <xref linkend="upgrade.backup" />.
    </para>
   </step>
   <step>
    <para>
     Verify the current cluster environment. Refer to
     <xref linkend="upgrade.verify_current" />.
    </para>
   </step>
   <step>
    <para>
     Prepare the current environment for upgrade. Refer to
     <xref linkend="upgrade.prepare" />.
    </para>
   </step>
   <step>
    <para>
     Upgrade the &adm;.
    </para>
    <itemizedlist>
     <listitem>
      <para>
       The following commands will still work, although &sminion;s are running
       old version of &ceph; and &salt;:
      </para>
<screen>
&prompt.cephuser;salt '*' test.ping
&prompt.cephuser;ceph status
</screen>
     </listitem>
     <listitem>
      <para>
       After the upgrade the &adm;, &oa; will no longer be installed.
      </para>
     </listitem>
     <listitem>
      <para>
       If the &adm; hosted SMT, complete its migration to RMT (refer to
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_migrate.html"/>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If your cluster <emphasis role="bold">does not use</emphasis> MDS roles,
       upgrade MON nodes one by one.
      </para>
     </step>
     <step>
      <para>
       If your cluster <emphasis role="bold">uses</emphasis> MDS roles, and MON
       and MDS roles are co-located, you need to shrink the MDS cluster and
       then upgrade the co-located nodes. Refer to
       <xref linkend="upgrade.mds" /> for more details.
      </para>
     </step>
     <step>
      <para>
       If your cluster <emphasis role="bold">uses</emphasis> MDS roles and they
       run on <emphasis role="bold">dedicated</emphasis> servers, upgrade all
       MON nodes one by one, then shrink the MDS cluster and upgrade it. Refer
       to <xref linkend="upgrade.mds" /> for more details.
      </para>
     </step>
    </substeps>
    <note>
     <title>Regarding &mon; Upgrade</title>
     <para>
      Due to a limitation in the &mon; design, once two MONs have been upgraded
      to &productname; &productnumber; and have formed a quorum, the third MON
      (while still on &productname; &prevproductnumber;) will not rejoin the
      MON cluster if it restarted for any reason, including a node reboot.
      Therefore, when two MONs have been upgraded it is best to upgrade the
      rest as soon as possible.
     </para>
    </note>
   </step>
   <step>
    <para>
     Upgrade &osd; nodes. Refer to <xref linkend="upgrade.main.osd" /> for more
     details. After all OSD nodes are upgraded, run:
    </para>
<screen>
&prompt.cephuser;ceph osd require-osd-release nautilus
</screen>
   </step>
   <step>
    <para>
     Upgrade the remaining cluster nodes in the recommended order as listed in
     <xref linkend="upgrade.consider_points" />.
    </para>
   </step>
   <step>
    <para>
     Verify that all OSD nodes were rebooted and that OSDs started
     automatically after the reboot.
    </para>
   </step>
   <step>
    <para>
     Synchronize all &salt; modules:
    </para>
<screen>
 &prompt.smaster;salt <replaceable>TARGET</replaceable> saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Update the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
     file. Refer to <xref linkend="upgrade.main.policy" /> for more details.
    </para>
   </step>
   <step>
    <para>
     Synchronize &deepsea; modules:
    </para>
<screen>
&prompt.smaster;salt '*' saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Update the &spillar; by running &deepsea; stage.1 and stage.2:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
   </step>
   <step>
    <para>
     Cleanup &oa;:
    </para>
<screen>
&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic
</screen>
   </step>
   <step>
    <para>
     Unset the 'restart_igw' grain to prevent stage.0 from restarting &iscsi;
     which is not installed yet:
    </para>
<screen>
&prompt.smaster;salt '*' grains.delkey restart_igw
</screen>
   </step>
   <step>
    <para>
     Finally, run through &deepsea; stages 0-4:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
</screen>
    <tip>
     <title>'subvolume missing' Errors during stage.3</title>
     <para>
      &deepsea; stage.3 may fail with an error similar to the following:
     </para>
<screen>
subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']
</screen>
     <para>
      In this case, you need to edit
      <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
      following line:
     </para>
<screen>
subvolume_init: disabled
</screen>
     <para>
      Then refresh the &spillar;:
     </para>
<screen>
&prompt.smaster;salt '*' saltutil.refresh_pillar
</screen>
     <para>
      and re-run &deepsea; stage.3:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    </tip>
    <para>
     After &deepsea; successfully finished Stage 3, the &dashboard; will be
     running. Refer to <xref linkend="ceph.dashboard" /> for a detailed
     overview of &dashboard; features.
    </para>
   </step>
   <step>
    <para>
     Sequentially restart &rgw; services to use 'beast' Web server instead of
     the outdated 'civetweb':
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.restart.rgw.force
</screen>
   </step>
   <step>
    <para>
     Migrate OSD profile-based layout to &drvgrps;. Refer to
     <xref linkend="upgrade.drive_groups" /> for more details.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade.backup">
  <title>Backup Cluster Configuration and Data</title>

  <para>
   Although creating backups of cluster's configuration and data is not
   mandatory, we strongly recommend backing up important configuration files
   and cluster data.
  </para>

  <para>
   For more details on which files need to be backed up, refer to
   <xref linkend="cha.deployment_backup" />.
  </para>
 </sect1>
 <sect1 xml:id="upgrade.verify_current">
  <title>Verify the Current Environment</title>

  <para>
   This section helps you verify that your current &ceph; cluster environment
   is healthy and can be safely upgraded.
  </para>

  <sect2 xml:id="upgrade.verify.each_node">
   <title>Examine each Cluster Node</title>
   <itemizedlist>
    <listitem>
     <para>
      Verify that all cluster servers have latest &prevcephos; and
      &productname; &prevproductnumber; patches applied.
     </para>
    </listitem>
    <listitem>
     <para>
      If the system has obvious problems, fix them before starting the upgrade.
      Upgrade never fixes existing system problems.
     </para>
    </listitem>
    <listitem>
     <para>
      Document specific parts of the system setup, such as network setup,
      partitioning, or installation details.
     </para>
    </listitem>
    <listitem>
     <para>
      Use <command>supportconfig</command> to collect important system
      information and save it outside cluster nodes. Find more information in
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_admsupport_supportconfig.html"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Check free disk space with <command>df -h</command>. When needed, free
      disk space by removing unneeded files/directories or removing obsolete OS
      snapshots.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.verify.state">
   <title>Check the Cluster's State</title>
   <para>
    The following commands provide details cluster state and specific
    configuration:
   </para>
   <variablelist>
    <varlistentry>
     <term><command>ceph -s</command></term>
     <listitem>
      <para>
       Prints a brief summary of &ceph; cluster health, running services, data
       usage, and IO statistics. Verify that it reports 'HEALTH_OK' before
       starting the upgrade.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>ceph health detail</command></term>
     <listitem>
      <para>
       Prints details if &ceph; cluster health is not OK.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>ceph versions</command></term>
     <listitem>
      <para>
       Prints versions of running &ceph; daemons.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>ceph df</command></term>
     <listitem>
      <para>
       Prints total and free disk space on the cluster. Do not start the
       upgrade if the cluster's free disk space is less than 25% of the total
       disk space.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>salt <replaceable>TARGET</replaceable> cephprocesses.check results=true</command></term>
     <listitem>
      <para>
       Prints running &ceph; processes and their PID's sorted by &sminion;s.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><command>ceph osd dump | grep ^flags</command></term>
     <listitem>
      <para>
       Verify that 'recovery_deletes' and 'purged_snapdirs' flags are present.
       If not, you can force a scrub on all placement groups by running the
       following command. Be aware that this forced scrub may possibly have a
       negative impact on your &ceph; clients’ performance.
      </para>
<screen>
&prompt.cephuser;ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Apart from checking cluster's state, focus on the following point:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Verify access to gateways (such as &iscsi;, or &ogw;) and &rbd;.
     </para>
    </listitem>
    <listitem>
     <para>
      In case you previously upgraded from version 4, verify that the upgrade
      to version 5 was completed successfully:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Check for the existence of the
        <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
        file. It is a backup of SES 4's <filename>ceph.conf</filename> file.
       </para>
      </listitem>
      <listitem>
       <para>
        Check if the cluster uses the new bucket type 'straw2':
       </para>
<screen>
&prompt.cephuser;ceph osd crush dump | grep straw
</screen>
      </listitem>
      <listitem>
       <para>
        Check that &ceph; 'jewel' profile is used:
       </para>
<screen>
&prompt.cephuser;ceph osd crush dump | grep profile
</screen>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Inspect <filename>/etc/ceph/ceph.conf</filename> and check whether there
      are warnings disabled.
     </para>
    </listitem>
    <listitem>
     <para>
      Check cluster performance. You can use commands such as <command>rados
      bench</command>, <command>ceph tell osd.* bench</command>, or
      <command>iperf3</command>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade.prepare">
  <title>Prepare the Upgrade</title>

  <para>
   Prepare the cluster setup and environment for the upgrade.
  </para>

  <sect2 xml:id="upgrade.prepare.repos">
   <title>Required Software Repositories</title>
   <para>
    Check that required repositories are configured on each cluster's node. To
    list all available repositories, run
   </para>
<screen>
&prompt.sminion;zypper lr
</screen>
   <para>
    &productname; &prevproductnumber; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    NFS/SMB Gateway on SLE-HA on &prevcephos; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.prepare.staging">
   <title>Repository Staging Systems</title>
   <para>
    If you are using one of repository staging systems&mdash;SMT, RMT;, or
    &susemgr;&mdash;create a new frozen patch level for the current and the new
    &productname; version.
   </para>
   <para>
    Find more information in:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>,
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-manager-3/index.html"/>,
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.prepare.patch">
   <title>Patch the Whole Cluster to the Latest Patches</title>
   <procedure>
    <step>
     <para>
      Apply latest patches of &productname; &prevproductnumber; and
      &prevcephos; to each &ceph; cluster node. Verify that correct software
      repositories are connected to each cluster node (see
      <xref linkend="upgrade.prepare.repos" />) and run &deepsea; stage.0:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      After stage.0 completes, verify that each cluster node's status includes
      'HEALTH_OK'. If not, resolve the problem before possible reboots in next
      steps.
     </para>
    </step>
    <step>
     <para>
      Run <command>zypper ps</command> to check for processes that may run with
      outdated libraries or binaries, and reboot if there are some.
     </para>
    </step>
    <step>
     <para>
      Verify that the running kernel is the latest available and reboot if not.
      Check outputs of the following commands:
     </para>
<screen>
&prompt.cephuser;uname -a
&prompt.cephuser;rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      Verify that the <package>ceph</package> package is version 12.2.12 or
      newer. Verify that the <package>deepsea</package> package is version
      0.8.9 or newer.
     </para>
    </step>
    <step>
     <para>
      If you previously used any of the <option>bluestore_cache</option>
      settings, they are not effective any more since <package>ceph</package>
      version 12.2.10. The new setting
      <option>bluestore_cache_autotune</option> that is set to 'true' by
      default disables manual cache sizing. To turn on the old behavior, you
      need to set <option>bluestore_cache_autotune=false</option>. Refer to
      <xref linkend="config.auto_cache_sizing" /> for details.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade.one_node">
  <title>Per Node Upgrade&mdash;Basic Procedure</title>

  <para>
   To ensure the core cluster services are available during the upgrade, you
   need to upgrade the cluster nodes sequentially one by one.
<!-- There are two ways you can perform the upgrade of a node: either using the installer DVD, or using the upgrade RPM package. -->
  </para>

  <tip>
   <title>Orphaned Packages</title>
   <para>
    After a node is upgraded, a number of packages will be in an 'orphaned'
    state without a parent repository. This happens because python3 related
    packages do not obsolete python2 packages.
   </para>
   <para>
    Find more information about listing orphaned packages in
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_zypper.html#sec_zypper_softup_orphaned"/>.
   </para>
  </tip>

  <sect2 xml:id="upgrade.one_node.manual">
   <title>Manual Node Upgrade using the Installer DVD</title>
   <procedure>
    <step>
     <para>
      Reboot the node from the &cephos; installer DVD/image.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>Upgrade</guimenu> from the boot menu.
     </para>
    </step>
    <step>
     <para>
      On the <guimenu>Previously Used Repositories</guimenu> screen, verify
      that the correct repositories are selected. If the system is not
      registered with SCC/SMT, you need to add the repositories manually.
     </para>
     <para>
      &productname; &productnumber; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      NFS/SMB Gateway on SLE-HA on &cephos; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Review the <guimenu>Installation Settings</guimenu> and start the
      installation procedure by clicking <guimenu>Update</guimenu>.
     </para>
    </step>
   </procedure>
  </sect2>

<!--
   <sect2 xml:id="upgrade.one_node.auto">
    <title>Automated Node Upgrade using the <emphasis>SLE15 Migration</emphasis> Package</title>
    <procedure>
     <step>
      <para>
       Disable or remove all existing software repositories:
      </para>
<screen>
&prompt.root;zypper mr -d -a
</screen>
     </step>
     <step>
      <para>
       Manually add the following software repositories:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         SLE-Product-SLES/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Basesystem/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Server-Applications/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         Storage/6
        </para>
       </listitem>
      </itemizedlist>
      <para>
       You can add them by finding out the right repository URL and running:
      </para>
<screen>
zypper ar -f <replaceable>REPO_URL</replaceable>
</screen>
      <para>
       for each required repository. Refresh them with <command>zypper
       ref</command>.
      </para>
     </step>
     <step>
      <para>
       Install the migration RPM packages. They adjust the &grub; boot loader
       to automatically trigger the upgrade on next reboot. Download the
       <package>SLES15-Migration</package> package from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/images/x86_64/"/>
       and the <package>suse-migration-activation</package> from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/SLE_12_SP3/noarch/"/>.
      </para>
     </step>
     <step>
      <para>
       Reboot. The migration system will start automatically and upgrade the
       packages by using the <command>zypper dup</command> command. The node
       will reboot automatically after the upgrade.
      </para>
      <tip>
       <title>Upgrade Failure</title>
       <para>
        If the upgrade fails, inspect
        <filename>/var/log/distro_migration.log</filename>. Then fix the
        problem, re-install the migration RPM packages, and reboot the node.
       </para>
      </tip>
     </step>
    </procedure>
   </sect2>
   -->
 </sect1>
 <sect1 xml:id="upgrade.mds">
  <title>Upgrade &mds;s</title>

  <para>
   You need to shrink the &mds; (MDS) cluster. Because of incompatible features
   between the &productname; &prevproductnumber; and &productnumber; versions,
   the older MDS daemons will shutdown as soon as they see a single SES
   &productnumber; level MDS join the cluster. Therefor it is necessary to
   shrink the MDS cluster to a single active MDS (and no standby's) for the
   duration of the MDS node upgrades. As soon as the second node is upgraded,
   you can extend the MDS cluster again.
  </para>

  <tip>
   <para>
    On a heavily loaded MDS cluster, you may need to to reduce the load (for
    example by stopping clients) so that a single active MDS is able to handle
    the workload.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Note the current value of the <option>max_mds</option> option:
    </para>
<screen>
&prompt.cephuser;ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     Shrink the MDS cluster if you have more then 1 active MDS daemons, i.e.
     <option>max_mds</option> is &gt; 1. To shrink the MDS cluster, run
    </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     where <replaceable>FS_NAME</replaceable> is the name of your &cephfs;
     instance ('cephfs' by default).
    </para>
   </step>
   <step>
    <para>
     Find the node hosting one of standby MDS daemons. Consult the output of
     the <command>ceph fs status</command> command and start the upgrade of the
     MDS cluster on this node.
    </para>
<screen>
&prompt.cephuser;ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     In this example, you need to start the upgrade procedure either on node
     'mon3-6' or 'mon2-6'.
    </para>
   </step>
   <step>
    <para>
     Upgrade the node with standby MDS daemon. After the upgraded MDS node
     starts, the outdated MDS daemons will shutdown automatically. At this
     point, clients may experience a short downtime of the &cephfs; service.
    </para>
   </step>
   <step>
    <para>
     Upgrade the remaining MDS nodes.
    </para>
   </step>
   <step>
    <para>
     Reset <option>max_mds</option> to the desired configuration:
    </para>
<screen>
&prompt.smaster;ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade.main.osd">
  <title>Upgrade &osd;s</title>

  <para>
   For each storage node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Identify which OSD daemons are running on a particular node:
    </para>
<screen>
&prompt.cephuser;ceph osd tree
</screen>
   </step>
   <step>
    <para>
     Set the 'noout' flag for each OSD daemon on the node that is being
     upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
   </step>
   <step>
    <para>
     Create <filename>/etc/ceph/osd/*.json</filename> files for all existing
     OSDs by running the following command on the node that is going be
     upgraded:
    </para>
<screen>
&prompt.cephuser;ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     Upgrade the OSD node.
    </para>
   </step>
   <step>
    <para>
     Activate all OSDs found in the system:
    </para>
<screen>
&prompt.cephuser;ceph-volume simple activate --all
</screen>
    <tip>
     <title>Activating Data Partitions Individually</title>
     <para>
      If you want to activate data partitions individually, you need to find
      the correct <command>ceph-volume</command> command for each partition to
      activate it. Replace <replaceable>X1</replaceable> with the partition's
      correct letter/number:
     </para>
<screen>
 &prompt.cephuser;ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      For example:
     </para>
<screen>
&prompt.cephuser;ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      The last line of the output contains the command to activate the
      partition:
     </para>
<screen>
&prompt.cephuser;ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--> All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--> Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     Verify that the OSD node will start properly after the reboot:
    </para>
    <substeps>
     <step>
      <para>
       If the upgraded OSD node <emphasis role="bold">is</emphasis> co-located
       with MDS, reboot after upgrading all MDS nodes.
      </para>
     </step>
     <step>
      <para>
       If the upgraded OSD node is <emphasis role="bold">not</emphasis>
       co-located with MDS, reboot now.
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Unset the 'noout' flag for each OSD daemon on the node that is upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
   </step>
  </procedure>

  <para>
   Verify the cluster status. It will be similar to the following output:
  </para>

<screen>
&prompt.cephuser;ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade&mdash;you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, and &sls;, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="upgrade.main.policy">
  <title>Update <filename>policy.cfg</filename> and Deploy &dashboard; using &deepsea;</title>

  <para>
   On the &adm;, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and apply the
   following changes:
  </para>

  <important>
   <title>No New Services</title>
   <para>
    During cluster upgrade, do not add new services to the
    <filename>policy.cfg</filename> file. Change the cluster architecture only
    after the upgrade is completed.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Remove <literal>role-openattic</literal>.
    </para>
   </step>
   <step>
    <para>
     Look up line(s) with
     <literal>profile-<replaceable>PROFILE_NAME</replaceable></literal> and add
     corresponding <literal>role-storage</literal> line(s). For example, for
     existing
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     add
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     Add <literal>role-prometheus</literal> and <literal>role-grafana</literal>
     to the node that had &prometheus; and &grafana; installed, usually the
     &adm;.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade.drive_groups">
  <title>Migration from Profile-based Deployments to &drvgrps;</title>

  <para>
   In &productname; &prevproductnumber;, &deepsea; offered so called 'profiles'
   to describe the layout of your OSDs. Starting with &productname;
   &productnumber;, we moved to a different approach called
   <emphasis>&drvgrps;</emphasis> (find more details in
   <xref linkend="ds.drive_groups" />).
  </para>

  <note>
   <para>
    Migrating to the new approach is not immediately mandatory. Destructive
    operations, such as <command>salt-run osd.remove</command>,
    <command>salt-run osd.replace</command>, or <command>salt-run
    osd.purge</command> are still available. However, adding new OSDs will
    require your action.
   </para>
  </note>

  <para>
   Because of the different approach of these implementations, we do not offer
   an automated migration path. However, we offer a variety of
   tools&mdash;&salt; runners&mdash;to make the migration as simple as
   possible.
  </para>

  <sect2>
   <title>Analyze the Current Layout</title>
   <para>
    To view information about the currently deployed OSDs, use the following
    command:
   </para>
<screen>
&prompt.smaster;salt-run disks.discover
</screen>
   <para>
    Alternatively, you can inspect the content of files in the
    <filename>/srv/pillar/ceph/proposals/profile-*/</filename> directories. The
    have similar structure to the following:
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>Create &drvgrps; Matching the Current Layout</title>
   <para>
    Refer to <xref linkend="ds.drive_groups.specs" /> for more details on
    &drvgrps; specification.
   </para>
   <para>
    The difference between a fresh deployment and upgrade scenario is that the
    drives to be migrated are already 'used'. Because
   </para>
<screen>
&prompt.smaster;salt-run disks.list
</screen>
   <para>
    looks for unused disks only, use
   </para>
<screen>
&prompt.smaster;salt-run disks.list include_unavailable=True
</screen>
   <para>
    Adjust &drvgrps; until you match your current setup. For a more visual
    representation of what will be happening, use the following command. Note
    that it has no output if there are no free disks:
   </para>
<screen>
&prompt.smaster;salt-run disks.report bypass_pillar=True
</screen>
   <para>
    If you verified that your &drvgrps; are properly configured and want to
    apply the new approach, remove files form the
    <filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename>
    directory, remove corresponding
    <literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal>
    lines from the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    file, and run &deepsea; stage.2 to refresh the &spillar;.
   </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
   <para>
    Verify the result by running the following commands:
   </para>
<screen>
&prompt.smaster;salt target_node pillar.get ceph:storage
&prompt.smaster;salt-run disks.report
</screen>
   <warning>
    <title>Incorrect &drvgrps; Configuration</title>
    <para>
     If your &drvgrps; are not properly configured and there are spare disks in
     your setup, they will be deployed in the way you specified them. We
     recommend running:
    </para>
<screen>
&prompt.smaster;salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2>
   <title>OSD Deployment</title>
   <para>
    For simple cases such as standalone OSDs, the migration will happen
    over-time. Whenever you remove or replace an OSD from the cluster, it will
    be replaced by a new, LVM based OSD.
   </para>
  </sect2>

  <sect2>
   <title>More Complex Setups</title>
   <para>
    If you have a more sophisticated setup than just sandalone OSDs, for
    example dedicated WAL/DBs or encrypted OSDs, the migration can only happen
    when all OSDs assigned to that WAL/DB device are removed. This is caused by
    the <command>ceph-volume</command> that creates Logical Volumes on disks
    before the deployment. This prevents the user from mixing partition based
    deployments with LV based deployments. In such cases it is best to manually
    remove all OSDs that are assigned to a WAL/DB device and re-deploy them
    using the &drvgrps; approach.
   </para>
  </sect2>
 </sect1>
</chapter>
