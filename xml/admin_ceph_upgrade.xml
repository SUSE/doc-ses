<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; from the previous
  release(s) to version &productnumber;.
 </para>
 <sect1 xml:id="ceph-upgrade-relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package> ,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-upgrade-general">
  <title>General Upgrade Procedure</title>

  <para>
   Consider the following items before starting the upgrade procedure:
  </para>

  <variablelist>
   <varlistentry xml:id="upgrade-order">
    <term>Upgrade Order</term>
    <listitem>
     <para>
      Before upgrading the &ceph; cluster, you need to have both the underlying
      &sls; and &productname; correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example &ceph;
      &rgw;s depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
      upgrading in this order:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        &mon;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mgr;s
       </para>
      </listitem>
      <listitem>
       <para>
        &osd;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mds;s
       </para>
      </listitem>
      <listitem>
       <para>
        &rgw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &igw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha;
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Delete Unnecessary Operating System Snapshots</term>
    <listitem>
     <para>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Check Cluster Health</term>
    <listitem>
     <para>
      We recommend to check the cluster health before starting the upgrade
      procedure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Upgrade One by One</term>
    <listitem>
     <para>
      We recommend upgrading all the daemons of a specific type&mdash;for
      example all monitor daemons or all OSD daemons&mdash;one by one to ensure
      that they are all on the same release. We also recommend that you upgrade
      all the daemons in your cluster before you try to exercise new
      functionality in a release.
     </para>
     <para>
      After all the daemons of a specific type are upgraded, check their
      status.
     </para>
     <para>
      Ensure each monitor has rejoined the quorum after all monitors are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph mon stat</screen>
     <para>
      Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Set <option>require-osd-release luminous</option> Flag</term>
    <listitem>
     <para>
      When the last OSD is upgraded to &productname; &productnumber;, the
      monitor nodes will detect that all OSDs are running the 'luminous'
      version of &ceph; and they may complain that the
      <option>require-osd-release luminous</option> osdmap flag is not set. In
      that case, you need to set this flag manually to acknowledge
      that&mdash;now that the cluster has been upgraded to 'luminous'&mdash;it
      cannot be downgraded back to &ceph; 'jewel'. Set the flag by running the
      following command:
     </para>
<screen>&prompt.cephuser;ceph osd require-osd-release luminous</screen>
     <para>
      After the command completes, the warning disappears.
     </para>
     <para>
      On fresh installs of &productname; &productnumber;, this flag is set
      automatically when the &ceph; monitors create the initial osdmap, so no
      end user action is needed.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds-migrate-osd-encrypted">
  <title>Encrypting OSDs during Upgrade</title>

  <para>
   Since &productname; &productnumber;, OSDs are by default deployed using
   &bluestore; instead of &filestore;. Although &bluestore; supports
   encryption, &osd;s are deployed unencrypted by default. The following
   procedure describes steps to encrypt OSDs during the upgrade process. Let us
   assume that both data and WAL/DB disks to be used for OSD deployment are
   clean with no partitions. If the disk were previously used, wipe them
   following the procedure described in <xref linkend="deploy-wiping-disk"/>.
  </para>

  <important>
   <title>One OSD at a Time</title>
   <para>
    You need to deploy encrypted OSDs one by one, not simultaneously. The
    reason is that OSD's data is drained, and the cluster goes through several
    iterations of rebalancing.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Determine the <option>bluestore block db size</option> and
     <option>bluestore block wal size</option> values for your deployment and
     add them to the
     <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>
     file on the &smaster;. The values need to be specified in bytes.
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     For more information on customizing the <filename>ceph.conf</filename>
     file, refer to <xref linkend="ds-custom-cephconf"/>.
    </para>
   </step>
   <step>
    <para>
     Run &deepsea; Stage 3 to distribute the changes:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Verify that the <filename>ceph.conf</filename> file is updated on the
     relevant OSD nodes:
    </para>
<screen>
&prompt.sminion;cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     Edit the *.yml files in the
     <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename>
     directory that are relevant to the OSDs you are encrypting. Double check
     their path with the one defined in the
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> file to ensure
     that you modify the correct *.yml files.
    </para>
    <important>
     <title>Long Disk Identifiers</title>
     <para>
      When identifying OSD disks in the
      <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>
      files, use long disk identifiers.
     </para>
    </important>
    <para>
     An example of an OSD configuration follows. Note that because we need
     encryption, the <option>db_size</option> and <option>wal_size</option>
     options are removed:
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     Deploy the new &blockstore; OSDs with encryption by running &deepsea;
     Stages 2 and 3:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
    <para>
     You can watch the progress with <command>ceph -s</command> or
     <command>ceph osd tree</command>. It is critical that you let the cluster
     rebalance before repeating the process on the next OSD node.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5">
  <title>Upgrade from &productname; 4 (&deepsea; Deployment) to 5</title>

  <important xml:id="u4to5-softreq">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order, based on their assigned role as recommended by &ceph; upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent the prescribed order from being
      violated if a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply, for example, new kernel versions. To
      reduce waiting I/O operations, we recommend declining incoming requests
      for the duration of the upgrade process.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      Since &ceph; Luminous, the <option>osd crush location</option>
      configuration option is no longer supported. Update your &deepsea;
      configuration files to use <command>crush location</command> before
      upgrading.
     </para>
    </listitem>
    <listitem>
     <para>
      There are two ways to obtain &sls; and &productname; &productnumber;
      update repositories:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        If your cluster nodes are registered with SUSEConnect and use SCC/SMT,
        you will use the <command>zypper migration</command> method and the
        update repositories will be assigned automatically.
       </para>
      </listitem>
      <listitem>
       <para>
        If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
        Media-ISO or other package source, you will use the <command>zypper
        dup</command> method. In this case, you need to add the following
        repositories to all cluster nodes manually: SLE12-SP3 Base, SLE12-SP3
        Update, SES5 Base, and SES5 Update. You can do so using the
        <command>zypper</command> command. First remove all existing software
        repositories, then add the required new ones, and finally refresh the
        repositories sources:
       </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://<replaceable>REPO_SERVER</replaceable>/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &productname; 4 cluster to version 5, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the &smaster; node to &sls; 12 SP3 and &productname;
     &productnumber;. Depending on your upgrade method, use either
     <command>zypper migration</command> or <command>zypper dup</command>.
    </para>
    <para>
     Using <command>rpm -q deepsea</command>, verify that the version of the
     &deepsea; package on the &smaster; node starts with at least
     <literal>0.7</literal>. For example:
    </para>
<screen>&prompt.root;rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     If the &deepsea; package version number starts with 0.6, double check
     whether you successfully migrated the &smaster; node to &sls; 12 SP3 and
     &productname; &productnumber;.
    </para>
   </step>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.cephuser;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     If your cluster nodes are <emphasis role="bold">not</emphasis> registered
     with SUSEConnect and do not use SCC/SMT, you will use the <command>zypper
     dup</command> method. Change your Pillar data in order to use the
     different strategy. Edit
    </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
    <para>
     and add the following line:
    </para>
<screen>upgrade_init: zypper-dup</screen>
   </step>
   <step xml:id="step-updatepillar">
    <para>
     Update your Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     See <xref linkend="ds-minion-targeting"/> for details about &sminion;s
     targeting.
    </para>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Upgrade &sminion;s:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to
     <xref linkend="ds-minion-targeting"/> of <xref linkend="ds-depl-stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <para>
     Refer to <xref linkend="ceph-maintenance-upgrade-details" /> for more
     information.
    </para>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
   <step>
    <para>
     After the upgrade, the &mgr;s are not installed yet. To reach a healthy
     cluster state, do the following:
    </para>
    <substeps>
     <step>
      <para>
       Run Stage 0 to enable the &salt; REST API:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       Run Stage 1 to create the <filename>role-mgr/</filename> subdirectory:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edit <guimenu>policy.cfg</guimenu> as described in
       <xref linkend="policy-configuration"/> and add a &mgr; role to the nodes
       where &mon;s are deployed, or uncomment the 'role-mgr' lines if you
       followed the steps of <xref linkend="ceph-upgrade-4to5cephdeloy"/> until
       here. Also, add the &oa; role to one of the cluster nodes. Refer to
       <xref linkend="ceph-oa"/> for more details.
      </para>
     </step>
     <step>
      <para>
       Run Stage 2 to update the Pillar:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       &deepsea; uses a different approach to generate the
       <filename>ceph.conf</filename> configuration file now, refer to
       <xref linkend="ds-custom-cephconf"/> for more details.
      </para>
     </step>
     <step>
      <para>
       Set any of the three &aa; states to all &deepsea; minions. For example
       to disable them, run
      </para>
<screen>
 &prompt.smaster;salt '<replaceable>TARGET</replaceable>' state.apply ceph.apparmor.default-disable
</screen>
      <para>
       For more information, refer to <xref linkend="admin-apparmor" />.
      </para>
     </step>
     <step>
      <para>
       Run Stage 3 to deploy &mgr;s:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       Run Stage 4 to configure &oa; properly:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>&ceph; Key Caps Mismatch</title>
     <para>
      If <literal>ceph.stage.3</literal> fails with "Error EINVAL: entity
      client.bootstrap-osd exists but caps do not match", it means the key
      capabilities (caps) for the existing cluster's
      <literal>client.bootstrap.osd</literal> key do not match the caps that
      &deepsea; is trying to set. Above the error message, in red text, you can
      see a dump of the <command>ceph auth</command> command that failed. Look
      at this command to check the key ID and file being used. In the case of
      <literal>client.bootstrap-osd</literal>, the command will be
     </para>
<screen>&prompt.cephuser;ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      To fix mismatched key caps, check the content of the keyring file
      &deepsea; is trying to deploy, for example:
     </para>
<screen>&prompt.cephuser;cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Compare this with the output of <command>ceph auth get
      client.bootstrap-osd</command>:
     </para>
<screen>&prompt.cephuser;ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Note how the latter key is missing <literal>caps mgr = "allow
      r"</literal>. To fix this, run:
     </para>
<screen>&prompt.cephuser;ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      Running <literal>ceph.stage.3</literal> should now succeed.
     </para>
     <para>
      The same issue can occur with other daemon and gateway keyrings when
      running <command>ceph.stage.3</command> and
      <command>ceph.stage.4</command>. The same procedure as above applies:
      check the command that failed, the keyring file being deployed, and the
      capabilities of the existing key. Then run <command>ceph auth
      caps</command> to update the existing key capabilities to match to what
      is being deployed by &deepsea;. The keyring files that &deepsea; tries to
      deploy are typically placed under the
      <filename>/srv/salt/ceph/<replaceable>DAEMON_OR_GATEWAY_NAME</replaceable>/cache</filename>
      directory.
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrading to &productname; &productnumber;, FileStore OSDs need
    approximately five minutes longer to start as the OSD will do a one-off
    conversion of its on-disk files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade&mdash;you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, and &sls;, and gives you a report displaying the
    version that the majority of nodes have and showing nodes whose version is
    different from the majority.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>OSD Migration to &bluestore;</title>
   <para>
    OSD &bluestore; is a new back end for the OSD daemons. It is the default
    option since &productname; &productnumber;. Compared to FileStore, which
    stores objects as files in an XFS file system, &bluestore; can deliver
    increased performance because it stores objects directly on the underlying
    block device. &bluestore; also enables other features, such as built-in
    compression and EC overwrites, that are unavailable with FileStore.
   </para>
   <para>
    Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
    and a 'db' (RocksDB database) device. The RocksDB database holds the
    metadata for a &bluestore; OSD. These two devices will reside on the same
    device as an OSD by default, but either can be placed on faster/different
    media.
   </para>
   <para>
    In SES5, both FileStore and &bluestore; are supported and it is possible
    for FileStore and &bluestore; OSDs to co-exist in a single cluster. During
    the SUSE Enterprise Storage upgrade procedure, FileStore OSDs are not
    automatically converted to &bluestore;. Be aware that the
    &bluestore;-specific features will not be available on OSDs that have not
    been migrated to &bluestore;.
   </para>
   <para>
    Before converting to &bluestore;, the OSDs need to be running &productname;
    &productnumber;. The conversion is a slow process as all data gets
    re-written twice. Though the migration process can take a long time to
    complete, there is no cluster outage and all clients can continue accessing
    the cluster during this period. However, do expect lower performance for
    the duration of the migration. This is caused by rebalancing and
    backfilling of cluster data.
   </para>
   <para>
    Use the following procedure to migrate FileStore OSDs to &bluestore;:
   </para>
   <tip>
    <title>Turn Off Safety Measures</title>
    <para>
     &salt; commands needed for running the migration are blocked by safety
     measures. In order to turn these precautions off, run the following
     command:
    </para>
<screen>
&prompt.smaster;salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      Migrate hardware profiles:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.policy</screen>
     <para>
      This runner migrates any hardware profiles currently in use by the
      <filename>policy.cfg</filename> file. It processes
      <filename>policy.cfg</filename>, finds any hardware profile using the
      original data structure, and converts it to the new data structure. The
      result is a new hardware profile named
      'migrated-<replaceable>original_name</replaceable>'.
      <filename>policy.cfg</filename> is updated as well.
     </para>
     <para>
      If the original configuration had separate journals, the &bluestore;
      configuration will use the same device for the 'wal' and 'db' for that
      OSD.
     </para>
    </step>
    <step>
     <para>
      &deepsea; migrates OSDs by setting their weight to 0 which 'vacuums' the
      data until the OSD is empty. You can either migrate OSDs one by one, or
      all OSDs at once. In either case, when the OSD is empty, the
      orchestration removes it and then re-creates it with the new
      configuration.
     </para>
     <tip>
      <title>Recommended Method</title>
      <para>
       Use <command>ceph.migrate.nodes</command> if you have a large number of
       physical storage nodes or almost no data. If one node represents less
       than 10% of your capacity, then the
       <command>ceph.migrate.nodes</command> may be marginally faster moving
       all the data from those OSDs in parallel.
      </para>
      <para>
       If you are not sure about which method to use, or the site has few
       storage nodes (for example each node has more than 10% of the cluster
       data), then select <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        To migrate OSDs one at a time, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        To migrate all OSDs on each node in parallel, run:
       </para>
<screen>&prompt.smaster;salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       As the orchestration gives no feedback about the migration progress, use
      </para>
<screen>&prompt.cephuser;ceph osd tree</screen>
      <para>
       to see which OSDs have a weight of zero periodically.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    After the migration to &bluestore;, the object count will remain the same
    and disk usage will be nearly the same.
   </para>
  </sect2>

  <sect2 xml:id="ceph-maintenance-upgrade-details">
   <title>Details on the <command>salt <replaceable>target</replaceable> ceph.maintenance.upgrade</command> Command</title>
   <para>
    During an upgrade via <command>salt
    <replaceable>target</replaceable>ceph.maintenance.upgrade</command>,
    &deepsea; applies all available updates/patches on all servers in the
    cluster in parallel without rebooting them. After these updates/patches are
    applied, the actual upgrade begins:
   </para>
   <procedure>
    <step>
     <para>
      The admin node is upgraded to &cephos;. This also upgrades the
      <package>salt-master</package> and <package>deepsea</package> packages.
     </para>
    </step>
    <step>
     <para>
      All &sminion;s are upgraded to a version that corresponds to the
      &smaster;.
     </para>
    </step>
    <step>
     <para>
      The migration is performed sequentially on all cluster nodes in the
      recommended order (the &mon;s first, see
      <xref linkend="upgrade-order" />) using the preferred method. As a
      consequence, the <package>ceph</package> package is upgraded.
     </para>
    </step>
    <step>
     <para>
      After updating all &mon;s, their services are restarted but the nodes are
      <emphasis role="bold">not rebooted</emphasis>. This way we ensure that
      all running &mon;s have identical version.
     </para>
     <important>
      <title>Do Not Reboot Monitor Nodes</title>
      <para>
       If the cluster monitor nodes host OSDs, <emphasis role="bold">do not
       reboot</emphasis> the nodes during this stage because the shared OSDs
       will not join the cluster after the reboot.
      </para>
     </important>
    </step>
    <step>
     <para>
      All the remaining cluster nodes are updated and rebooted in the
      recommended order.
     </para>
    </step>
    <step>
     <para>
      After all nodes are on the same patch-level, the following command is
      run:
     </para>
<screen><command>ceph require osd release <replaceable>RELEASE</replaceable></command></screen>
    </step>
   </procedure>
   <para>
    In case this process is interrupted by an accident or intentionally by the
    administrator, <emphasis role="bold">never reboot</emphasis> the nodes
    manually because after rebooting the first OSD node and OSD daemon, it will
    not be able to join the cluster anymore.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5cephdeloy">
  <title>Upgrade from &productname; 4 (<command>ceph-deploy</command> Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Choose the &smaster; for your cluster. If your cluster has Calamari
    deployed, then the Calamari node already <emphasis>is</emphasis> the
    &smaster;. Alternatively, the admin node from which you ran the
    <command>ceph-deploy</command> command will become the &smaster;.
   </para>
   <para>
    Before starting the procedure below, you need to upgrade the &smaster; node
    to &sls; 12 SP3 and &productname; &productnumber; by running
    <command>zypper migration</command> (or your preferred way of upgrading).
   </para>
  </important>

  <para>
   To upgrade the &productname; 4 cluster which was deployed with
   <command>ceph-deploy</command> to version 5, follow these steps:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy-all">
   <title>Steps to Apply to All Cluster Nodes (including the Calamari Node)</title>
   <step>
    <para>
     Install the <systemitem>salt</systemitem> package from SLE-12-SP2/SES4:
    </para>
<screen>&prompt.root;zypper install salt</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>salt-minion</systemitem> package from
     SLE-12-SP2/SES4, then enable and start the related service:
    </para>
<screen>&prompt.root;zypper install salt-minion
&prompt.root;systemctl enable salt-minion
&prompt.root;systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Ensure that the host name 'salt' resolves to the IP address of the
     &smaster; node. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      The existing &sminion;s have the <option>master:</option> option already
      set in <filename>/etc/salt/minion.d/calamari.conf</filename>. The
      configuration file name does not matter, the
      <filename>/etc/salt/minion.d/</filename> directory is important.
     </para>
    </tip>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <command>zypper</command> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy-admin">
   <title>Steps to Apply to the &smaster; Node</title>
   <step>
    <para>
     Set the new internal object sort order, run:
    </para>
<screen>&prompt.cephuser;ceph osd set sortbitwise</screen>
    <tip>
     <para>
      To verify that the command was successful, we recommend running
     </para>
<screen>&prompt.cephuser;;ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Upgrade the &smaster; node to &cephos; and &productname; &productnumber;.
     For SCC-registered systems, use <command>zypper migration</command>. If
     you provide the required software repositories manually, use
     <command>zypper dup</command>. After the upgrade, ensure that only
     repositories for &sls; 12 SP3 and &productname; &productnumber; are active
     (and refreshed) on the &smaster; node before proceeding.
    </para>
   </step>
   <step>
    <para>
     If not already present, install the <systemitem>salt-master</systemitem>
     package, then enable and start the related service:
    </para>
<screen>&prompt.smaster;zypper install salt-master
&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verify the presence of all &sminion;s by listing their keys:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Add all &sminion;s keys to &smaster; including the minion master:
    </para>
<screen>&prompt.smaster;salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Ensure that all &sminion;s' keys were accepted:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Make sure that the software on your &smaster; node is up to date:
    </para>
<screen>&prompt.smaster;zypper migration</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>deepsea</systemitem> package:
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Include the cluster's &sminion;s. Refer to
     <xref linkend="ds-minion-targeting"/> of <xref linkend="ds-depl-stages"/>
     for more details.
    </para>
   </step>
   <step>
    <para>
     Import the existing <command>ceph-deploy</command> installed cluster:
    </para>
<screen>&prompt.smaster;salt-run populate.engulf_existing_cluster</screen>
    <para>
     The command will do the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribute all the required &salt; and &deepsea; modules to all the
       &sminion;s.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspect the running &ceph; cluster and populate
       <filename>/srv/pillar/ceph/proposals</filename> with a layout of the
       cluster.
      </para>
      <para>
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> will be
       created with roles matching all detected running &ceph; services. If no
       <systemitem class="daemon">ceph-mgr</systemitem> daemons are detected a
       'role-mgr' is added for every node with 'role-mon'. View this file to
       verify that each of your existing MON, OSD, RGW and MDS nodes have the
       appropriate roles. OSD nodes will be imported into the
       <filename>profile-import/</filename> subdirectory, so you can examine
       the files in
       <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename>
       and
       <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename>
       to confirm that the OSDs were correctly picked up.
      </para>
      <note>
       <para>
        The generated <filename>policy.cfg</filename> will only apply roles for
        detected &ceph; services 'role-mon', 'role-mds', 'role-rgw',
        'role-admin', and 'role-master' for the &smaster; node. Any other
        desired roles will need to be added to the file manually (see
        <xref linkend="policy-role-assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The existing cluster's <filename>ceph.conf</filename> will be saved to
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
       will include the cluster's fsid, cluster and public networks, and also
       specifies the <option>configuration_init: default-import</option>
       option, which makes &deepsea; use the
       <filename>ceph.conf.import</filename> configuration file mentioned
       previously, rather than using &deepsea;'s default
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
       template.
      </para>
      <note>
       <title>Custom Settings in <filename>ceph.conf</filename></title>
       <para>
        If you need to integrate the <filename>ceph.conf</filename> file with
        custom changes, wait until the engulf/upgrade process successfully
        finishes. Then edit the
        <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
        file and comment the following line:
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        Save the file and follow the information in
        <xref linkend="ds-custom-cephconf"/>.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The cluster's various keyrings will be saved to the following
       directories:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       Verify that the keyring files exist, and that there is
       <emphasis>no</emphasis> keyring file in the following directory (the
       &mgr; did not exist before &productname; &productnumber;):
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If the <command>salt-run populate.engulf_existing_cluster</command>
     command cannot detect <systemitem class="daemon">ceph-mgr</systemitem>
     daemons, the <filename>policy.cfg</filename> file will contain a 'mgr'
     role for each node that has the 'role-mon' assigned. This will deploy
     <systemitem class="daemon">ceph-mgr</systemitem> daemons together with the
     monitor daemons in a later step. Since there are no
     <systemitem class="daemon">ceph-mgr</systemitem> daemons running at this
     time, please edit
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and comment out
     all lines starting with 'role-mgr' by prepending a '#' character.
    </para>
   </step>
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &oa; configuration. You need to manually
     edit the <filename>policy.cfg</filename> file and add a
     <literal>role-openattic</literal> line. Refer to
     <xref linkend="policy-configuration"/> for more details.
    </para>
   </step>
<!-- 2018-03-21,tbazant: remove this step after
   https://github.com/SUSE/DeepSea/issues/845 is resolved -->
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &igw;s configurations. If your cluster
     includes &igw;s, import their configurations manually:
    </para>
    <substeps>
     <step>
      <para>
       On one of &igw; nodes, export the current <filename>lrbd.conf</filename>
       and copy it to the &smaster; node:
      </para>
<screen>
&prompt.sminion;lrbd -o &gt;/tmp/lrbd.conf
&prompt.sminion;scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       On the &smaster; node, add the default &igw; configuration to the
       &deepsea; setup:
      </para>
<screen>
&prompt.smaster;mkdir -p /srv/pillar/ceph/stack/ceph/
&prompt.smaster;echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
&prompt.smaster;chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Add the &igw; roles to <filename>policy.cfg</filename> and save the
       file:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Run Stages 0 and 1 to update packages and create all possible roles:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
</screen>
   </step>
   <step>
    <para>
     Generate required subdirectories under
     <filename>/srv/pillar/ceph/stack</filename>:
    </para>
<screen>&prompt.smaster;salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     Verify that there is a working &deepsea;-managed cluster with correctly
     assigned roles:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     Compare the output with the actual layout of the cluster.
    </para>
   </step>
   <step>
    <para>
     Calamari leaves a scheduled &salt; job running to check the cluster
     status. Remove the job:
    </para>
<screen>&prompt.smaster;salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat</screen>
   </step>
   <step>
    <para>
     From this point on, follow the procedure described in
     <xref linkend="ceph-upgrade-4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-4to5crowbar">
  <title>Upgrade from &productname; 4 (&crow; Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade &productname; 4 deployed using &crow; to version 5, follow these
   steps:
  </para>

  <procedure>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), stop and disable all
     &crow;-related services :
    </para>
<screen>
&prompt.sminion;systemctl stop chef-client
&prompt.sminion;systemctl disable chef-client
&prompt.sminion;systemctl disable crowbar_join
&prompt.sminion;systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), verify that the
     software repositories point to &productname; &productnumber; and &sls; 12
     SP3 products. If repositories pointing to older product versions are still
     present, disable them.
    </para>
   </step>
   <step>
    <para>
     For each &ceph; node (including the Calamari node), verify that the
     <package>salt-minion</package> is installed. If not, install it:
    </para>
<screen>&prompt.sminion;zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     For the &ceph; nodes that did not have the <package>salt-minion</package>
     package installed, create the file
     <filename>/etc/salt/minion.d/master.conf</filename> with the
     <option>master</option> option pointing to the full Calamari node host
     name:
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      The existing &sminion;s have the <option>master:</option> option already
      set in <filename>/etc/salt/minion.d/calamari.conf</filename>. The
      configuration file name does not matter, the
      <filename>/etc/salt/minion.d/</filename> directory is important.
     </para>
    </tip>
    <para>
     Enable and start the <systemitem class="daemon">salt-minion</systemitem>
     service:
    </para>
<screen>
&prompt.sminion;systemctl enable salt-minion
&prompt.sminion;systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     On the Calamari node, accept any remaining salt minion keys:
    </para>
<screen>
&prompt.smaster;salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

&prompt.smaster;salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     If &ceph; was deployed on the public network and no VLAN interface is
     present, add a VLAN interface on &crow;'s public network to the Calamari
     node.
    </para>
   </step>
   <step>
    <para>
     Upgrade the Calamari node to &sls; 12 SP3 and &productname;
     &productnumber;, either by using <command>zypper migration</command> or
     your favorite method. From here onward, the Calamari node becomes the
     <emphasis>&smaster;</emphasis>. After the upgrade, reboot the &smaster;.
    </para>
   </step>
   <step>
    <para>
     Install &deepsea; on the &smaster;:
    </para>
<screen>&prompt.smaster;zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Specify the <option>deepsea_minions</option> option to include the correct
     group of &sminion;s into deployment stages. Refer to
     <xref linkend="ds-minion-targeting-dsminions"/> for more details.
    </para>
   </step>
   <step>
    <para>
     &deepsea; expects all &ceph; nodes to have an identical
     <filename>/etc/ceph/ceph.conf</filename>. &crow; deploys a slightly
     different <filename>ceph.conf</filename> to each node, so you need to
     consolidate them:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Remove the <option>osd crush location hook</option> option, it was
       included by Calamari.
      </para>
     </listitem>
     <listitem>
      <para>
       Remove the <option>public addr</option> option from the
       <literal>[mon]</literal> section.
      </para>
     </listitem>
     <listitem>
      <para>
       Remove the port numbers from the <option>mon host</option> option.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     If you were running the &ogw;, &crow; deployed a separate
     <filename>/etc/ceph/ceph.conf.radosgw</filename> file to keep the keystone
     secrets separated from the regular <filename>ceph.conf</filename> file.
     &crow; also added a custom
     <filename>/etc/systemd/system/ceph-radosgw@.service</filename> file.
     Because &deepsea; does not support it, you need to remove it:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Append all <literal>[client.rgw....]</literal> sections from the
       <filename>ceph.conf.radosgw</filename> file to
       <filename>/etc/ceph/ceph.conf</filename> on all nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       On the &ogw; node, run the following:
      </para>
<screen>&prompt.sminion;rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Double check that <command>ceph status</command> works when run from the
     &smaster;:
    </para>
<screen>&prompt.smaster;ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     Import the existing cluster:
    </para>
<screen>
&prompt.smaster;salt-run populate.engulf_existing_cluster
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run push.proposal
</screen>
   </step>
<!-- 2018-03-21, remove this step after
   https://github.com/SUSE/DeepSea/issues/845 is resolved -->
   <step>
    <para>
     The <command>salt-run populate.engulf_existing_cluster</command> command
     does not handle importing the &igw;s configurations. If your cluster
     includes &igw;s, import their configurations manually:
    </para>
    <substeps>
     <step>
      <para>
       On one of &igw; nodes, export the current <filename>lrbd.conf</filename>
       and copy it to the &smaster; node:
      </para>
<screen>
&prompt.sminion;lrbd -o &gt; /tmp/lrbd.conf
&prompt.sminion;scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       On the &smaster; node, add the default &igw; configuration to the
       &deepsea; setup:
      </para>
<screen>
&prompt.smaster;mkdir -p /srv/pillar/ceph/stack/ceph/
&prompt.smaster;echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
&prompt.smaster;chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Add the &igw; roles to <filename>policy.cfg</filename> and save the
       file:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       If you registered your systems with SUSEConnect and use SCC/SMT, no
       further actions need to be taken.
      </para>
     </step>
     <step>
      <para>
       If you are <emphasis role="bold">not</emphasis> using SCC/SMT but a
       Media-ISO or other package source, add the following repositories
       manually: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base, and SES5 Update.
       You can do so using the <command>zypper</command> command. First remove
       all existing software repositories, then add the required new ones, and
       finally refresh the repositories sources:
      </para>
<screen>
&prompt.root;zypper sd {0..99}
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
&prompt.root;zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
&prompt.root;zypper ref
</screen>
      <para>
       Then change your Pillar data in order to use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        The <literal>zypper-dup</literal> strategy requires you to manually add
        the latest software repositories, while the default
        <literal>zypper-migration</literal> relies on the repositories provided
        by SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Fix host grains to make &deepsea; use short host names on the public
     network for the &ceph; daemon instance IDs. For each node, you need to run
     <command>grains.set</command> with the new (short) host name. Before
     running <command>grains.set</command>, verify the current monitor
     instances by running <command>ceph status</command>. A before and after
     example follows:
    </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
&prompt.smaster;salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
&prompt.smaster;salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
&prompt.smaster;salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
&prompt.smaster;salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     Run the upgrade:
    </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> state.apply ceph.updates
&prompt.smaster;salt <replaceable>target</replaceable> test.version
&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     Every node will reboot. The cluster will come back up complaining that
     there is no active &mgr; instance. This is normal. Calamari should not be
     installed/running anymore at this point.
    </para>
   </step>
   <step>
    <para>
     Run all the required deployment stages to get the cluster to a healthy
     state:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     To deploy &oa; (see <xref linkend="ceph-oa"/>), add an appropriate
     <literal>role-openattic</literal> (see
     <xref linkend="policy-role-assignment"/>) line to
     <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>, then run:
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     During the upgrade, you may receive "Error EINVAL: entity [...] exists but
     caps do not match" errors. To fix them, refer to
     <xref linkend="ceph-upgrade-4to5"/>.
    </para>
   </step>
   <step>
    <para>
     Do the remaining cleanup:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &crow; creates entries in <filename>/etc/fstab</filename> for each OSD.
       They are not necessary, so delete them.
      </para>
     </listitem>
     <listitem>
      <para>
       Calamari leaves a scheduled &salt; job running to check the cluster
       status. Remove the job:
      </para>
<screen>
&prompt.smaster;salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       There are still some unnecessary packages installed, mostly ruby gems,
       and chef related. Their removal is not required but you may want to
       delete them by running <command>zypper rm
       <replaceable>pkg_name</replaceable></command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-upgrade-3to5">
  <title>Upgrade from &productname; 3 to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &productname; 3 cluster to version 5, follow the steps
   described in <xref linkend="upgrade4to5cephdeploy-all"/> and then
   <xref linkend="upgrade4to5cephdeploy-admin"/>.
  </para>
 </sect1>
</chapter>
