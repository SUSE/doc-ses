<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade from &productname; &prevprevproductnumber; to &productnumber;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname;
  &prevprevproductnumber; to version &productnumber;.
 </para>
 <para>
  The upgrade includes the following tasks:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrading from &ceph; &prevprevcephname; to &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    Switching from installing and running &ceph; via RPM packages to running in
    containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Complete removal of &deepsea; and replacing with &cephsalt; and &cephadm;.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   The upgrade information in this chapter <emphasis>only</emphasis> applies to
   upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
   instructions if you want to deploy &productname; on &caasp;.
  </para>
 </warning>
 <important>
  <para>
   Upgrading from &productname; versions older than &prevprevproductnumber; is
   not supported. First, you must upgrade to the latest version of
   &productname; &prevprevproductnumber;, and then follow the steps in this
   chapter.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Before upgrading</title>

  <para>
   The following tasks <emphasis>must</emphasis> be completed before you start
   the upgrade. This can be done at any time during the &productname;
   &prevprevproductnumber; life time.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The OSD migration from &filestore; to &bluestore;
     <emphasis>must</emphasis> happen before the upgrade as &filestore;
     unsupported in &productname; &productnumber;. Find more details about
     &bluestore; and how to migrate from &filestore; at
     <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are running an older cluster that still uses
     <literal>ceph-disk</literal> OSDs, you <emphasis>need</emphasis> to switch
     to <literal>ceph-volume</literal> before the upgrade. Find more details in
     <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Points to consider</title>
   <para>
    Before upgrading, ensure you read through the following sections to ensure
    you understand all tasks that need to be executed.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Read the release notes</emphasis>. In them, you can find
      additional information on changes since the previous release of
      &productname;. Check the release notes to see whether:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Your hardware needs special considerations.
       </para>
      </listitem>
      <listitem>
       <para>
        Any used software packages have changed significantly.
       </para>
      </listitem>
      <listitem>
       <para>
        Special precautions are necessary for your installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </para>
     <para>
      You can find SES &productnumber; release notes online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Additionally, after having installed the package
      <package>release-notes-ses</package> from the SES &productnumber;
      repository, find the release notes locally in the directory
      <filename>/usr/share/doc/release-notes</filename> or online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Read <xref linkend="ses-deployment"/> to familiarise yourself with
      &cephsalt; and the &ceph; orchestrator, and in particular the information
      on service specifications.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a long time&mdash;approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      You need to upgrade the &smaster; first, then replace &deepsea; with
      &cephsalt; and &cephadm;. You will <emphasis>not</emphasis> be able to
      start using the &cephadm; orchestrator module until at least all &mgr;
      nodes are upgraded.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade from using &prevprevcephname; RPMs to &cephname; containers
      needs to happen in a single step. This means upgrading an entire node at
      a time, not one daemon at a time.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade of core services (MON, MGR, OSD) happens in an orderly
      fashion. Each service is available during the upgrade. The gateway
      services (&mds;, &ogw;, &ganesha;, &igw;) need to be redeployed after the
      core services are upgraded. There is a certain amount of downtime for
      each of the following services:
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         &mds;s and &ogw;s are down from the time the nodes are upgraded from
         &prevprevcephos; to &cephos; until the services are redeployed at the
         end of the upgrade procedure. This is particularly important to bear
         in mind if these services are colocated with MONs, MGRs or OSDs
         as they may be down for the duration of the cluster upgrade. If
         this is going to be a problem, consider deploying
         these services separately on additional nodes before upgrading, so
         that they are down for the shortest possible time. This is the
         duration of the upgrade of the gateway nodes, not the duration of the
         upgrade of the entire cluster.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        &ganesha; and &igw;s are down only while nodes are rebooting during
        upgrade from &prevprevcephos; to &cephos;, and again briefly when each
        service is redeployed on the containerized mode.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Backing Up cluster configuration and data</title>
   <para>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to &productname; &productnumber;. For instructions on
    how to back up all your data, see
    <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verifying steps from the previous upgrade</title>
   <para>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </para>
   <para>
    Check for the existence of the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
    file.
   </para>
   <para>
    This file is created by the engulf process during the upgrade from
    &productname; 5 to 6. The <option>configuration_init:
    default-import</option> option is set in
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    If <option>configuration_init</option> is still set to
    <option>default-import</option>, the cluster is using
    <filename>ceph.conf.import</filename> as its configuration file and not
    &deepsea;'s default <filename>ceph.conf</filename>, which is compiled from
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Therefore, you need to inspect <filename>ceph.conf.import</filename> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Then remove the <option>configuration_init: default-import</option> line
    from
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Updating cluster nodes and verifying cluster health</title>
   <para>
    Verify that all latest updates of &prevprevcephos; and &productname;
    &prevprevproductnumber; are applied to all cluster nodes:
   </para>
<screen>&prompt.root;zypper refresh &amp;&amp; zypper patch</screen>
   <tip>
    <para>
     Refer to
     <link
     xlink:href="https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates"/>
     for detailed information about updating the cluster nodes.
    </para>
   </tip>
   <para>
    After updates are applied, restart the &smaster;, synchronize new &salt;
    modules, and check the cluster health:
   </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt '*' saltutil.sync_all
&prompt.cephuser;ceph -s
</screen>
   <sect3 xml:id="upgrade-disable-insecure">
    <title>Disable insecure clients</title>
    <para>
     Since &prevprevcephname; v14.2.20, a new health warning was introduced
     that informs you that insecure clients are allowed to join the cluster.
     This warning is <emphasis>on</emphasis> by default. The &dashboard; will
     show the cluster in the <literal>HEALTH_WARN</literal> status. The command line
     verifies the cluster status as follows:
    </para>
<screen>
 &prompt.cephuser;ceph status
 cluster:
   id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
   health: HEALTH_WARN
   mons are allowing insecure global_id reclaim
 [...]
 </screen>
    <para>
     This warning means that the &mon;s are still allowing old, unpatched
     clients to connect to the cluster. This ensures existing clients can still
     connect while the cluster is being upgraded, but warns you that there is a
     problem that needs to be addressed. When the cluster and all clients are
     upgraded to the latest version of &ceph;, disallow unpatched clients by
     running the following command:
    </para>
<screen>&prompt.cephuser;ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verifying access to software repositories and container images</title>
   <para>
    Verify that each cluster node has access to the &cephos; and &productname;
    &productnumber; software repositories, as well as the registry of container
    images.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Software repositories</title>
    <para>
     If all nodes are registered with SCC, you will be able to use the
     <command>zypper migration</command> command to upgrade. Refer to
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/>
     for more details.
    </para>
    <para>
     If nodes are <emphasis role="bold">not</emphasis> registered with SCC,
     disable all existing software repositories and add both the
     <literal>Pool</literal> and <literal>Updates</literal> repositories for
     each of the following extensions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7.1
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Container images</title>
    <para>
     All cluster nodes need access to the container image registry. In most
     cases, you will use the public SUSE registry at
     <literal>registry.suse.com</literal>. You need the following images:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/prometheus/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/prometheus/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/prometheus/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Alternatively&mdash;for example, for air-gapped
     deployments&mdash;configure a local registry and verify that you have the
     correct set of container images available. Refer to
     <xref
     linkend="deploy-cephadm-configure-registry"/> for more details
     about configuring a local container image registry.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrading the &smaster;</title>

  <para>
   The following procedure describes the process of upgrading the &smaster;:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For cluster whose all nodes are registered with SCC, run <command>zypper
       migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       For cluster whose nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Disable the &deepsea; stages to avoid accidental use. Add the following
     content to <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the locally
     configured registry, edit
     <filename>/srv/pillar/ceph/stack/global.yml</filename> to inform &deepsea;
     which &ceph; container image and registry to use. For example, to use
     <literal>192.168.121.1:5000/my/ceph/image</literal> add the following
     lines:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     If you need to specify authentication information for the registry, add
     the <literal>ses7_container_registry_auth:</literal> block, for example:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
ses7_container_registry_auth:
  registry: 192.168.121.1:5000
  username: <replaceable>USER_NAME</replaceable>
  password: <replaceable>PASSWORD</replaceable>
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Assimilate existing configuration:
    </para>
<screen>&prompt.cephuser;ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verify the upgrade status. Your output may differ depending on your
     cluster configuration:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrading the MON, MGR, and OSD nodes</title>

  <para>
   Upgrade the &mon;, &mgr;, and OSD nodes one at a time. For each service,
   follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     If the node you are upgrading is an OSD node, avoid having the OSD marked
     <literal>out</literal> during the upgrade by running the following
     command:
    </para>
<screen>&prompt.cephuser;ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Replace <replaceable>SHORT_NODE_NAME</replaceable> with the short name of
     the node as it appears in the output of the <command>ceph osd
     tree</command> command. In the following input, the short host names are
     <literal>ses-min1</literal> and <literal>ses-min2</literal>
    </para>
<screen>
&prompt.smaster;ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the cluster's nodes are all registered with SCC, run <command>zypper
       migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       If the cluster's nodes have software repositories assigned manually, run
       <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     After the node is rebooted, containerize all existing MON, MGR, and OSD
     daemons on that node by running the following command on the &smaster;:
    </para>
<screen>&prompt.smaster;salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Replace <replaceable>MINION_ID</replaceable> with the ID of the minion
     that you are upgrading. You can get the list of minion IDs by running the
     <command>salt-key -L</command> command on the &smaster;.
    </para>
    <tip>
     <para>
      To see the status and progress of the <emphasis>adoption</emphasis>,
      check the &dashboard; or run one of the following commands on the
      &smaster;:
     </para>
<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     After the adoption has successfully finished, unset the
     <literal>noout</literal> flag if the node you are upgrading is an OSD
     node:
    </para>
<screen>&prompt.cephuser;ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Upgrading gateway nodes</title>

  <para>
   Upgrade your separate gateway nodes (&sgw;, &mds;, &ogw;, &ganesha;, or
   &igw;) next. Upgrade the underlying OS to &cephos; for each node:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     If the cluster's nodes are all registered with &scc;, run the
     <command>zypper migration</command> command.
    </para>
   </listitem>
   <listitem>
    <para>
     If the cluster's nodes have software repositories assigned manually, run
     the <command>zypper dup</command> followed by the
     <command>reboot</command> commands.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   This step also applies for any nodes that are part of the cluster, but do
   not yet have any roles assigned (if in doubt, check the list of hosts on the
   &smaster; provided by the <command>salt-key -L</command> command and compare
   it to the output of the <command>salt-run upgrade.status</command> command).
  </para>

  <para>
   Once the OS is upgraded on all nodes in the cluster, the next step is to
   install the <package>ceph-salt</package> package and apply the cluster
   configuration. The actual gateway services are redeployed in a containerized
   mode at the end of the upgrade procedure.
  </para>

  <note>
   <para>
    &mds; and &ogw; services are unavailable from the time of upgrading to
    &cephos; until they are redeployed at the end of the upgrade procedure.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Installing &cephsalt; and applying the cluster configuration</title>

  <para>
   Before you start the procedure of installing &cephsalt; and applying the
   cluster configuration, check the cluster and upgrade status by running the
   following commands:
  </para>

<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Remove the &deepsea;-created <literal>rbd_exporter</literal> and
     <literal>rgw_exporter</literal> cron jobs. On the &smaster; as the
     &rootuser; run the <command>crontab -e</command> command to edit the
     crontab. Delete the following items if present:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh > \
 /var/lib/prometheus/node-exporter/rbd.prom 2> /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py > \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2> /dev/null
</screen>
   </step>
   <step>
    <para>
     Export cluster configuration from &deepsea;, by running the following
     commands:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.ceph_salt_config > ceph-salt-config.json
&prompt.smaster;salt-run upgrade.generate_service_specs > specs.yaml
</screen>
   </step>
   <step>
    <para>
     Uninstall &deepsea; and install &cephsalt; on the &smaster;:
    </para>
<screen>
&prompt.smaster;zypper remove 'deepsea*'
&prompt.smaster;zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Restart the &smaster; and synchronize &salt; modules:
    </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Import &deepsea;'s cluster configuration into &cephsalt;:
    </para>
<screen>&prompt.smaster;ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Generate SSH keys for cluster node communication:
    </para>
<screen>&prompt.smaster;ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verify that the cluster configuration was imported from &deepsea; and
      specify potentially missed options:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
     <para>
      For a complete description of cluster configuration, refer to
      <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Apply the configuration and enable &cephadm;:
    </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   </step>
   <step>
    <para>
     If you need to supply local container registry URL and access credentials,
     follow the steps described in
     <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the
     locally-configured registry, inform &ceph; which container image to use by
     running
    </para>
<screen>&prompt.smaster;ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.smaster;ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Stop and disable the &productname; 6
     <systemitem
     class="daemon">ceph-crash</systemitem> daemons. New
     containerized forms of these daemons are started later automatically.
    </para>
<screen>
&prompt.smaster;salt '*' service.stop ceph-crash
&prompt.smaster;salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Upgrading and adopting the monitoring stack</title>

  <para>
   This following procedure adopts all components of the monitoring stack (see
   <xref linkend="monitoring-alerting"/> for more details).
  </para>

  <procedure>
   <step>
    <para>
     Pause the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch pause</screen>
   </step>
   <step>
    <para>
     On whichever node is running &prometheus;, &grafana; and &alertmanager;
     (the &smaster; by default), run the following commands:
    </para>
<screen>
&prompt.cephuser;cephadm adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      If you are <emphasis role="bold">not</emphasis> running the default
      container image registry <literal>registry.suse.com</literal>, you need
      to specify the image to use on each command, for example:
     </para>
<screen>
&prompt.cephuser;cephadm --image 192.168.121.1:5000/ses/7.1/prometheus/prometheus-server:2.27.1 \
  adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/ses/7.1/prometheus/prometheus-alertmanager:0.21.0 \
  adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/ses/7.1/ceph/grafana:7.3.1 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      The container images required and their respective versions are listed in
      <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Remove Node-Exporter from <emphasis role="bold">all</emphasis> nodes. The
     Node-Exporter does not need to be migrated and will be re-installed as a
     container when the <filename>specs.yaml</filename> file is applied.
    </para>
<screen>&prompt.sudo;zypper rm golang-github-prometheus-node_exporter</screen>
    <para>
     Alternatively, you can remove Node-Exporter from all nodes simultaneously
     using &salt; on the admin node:
    </para>
<screen>&prompt.smaster;salt '*' pkg.remove golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Apply the service specifications that you previously exported from
     &deepsea;:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i specs.yaml</screen>
    <tip>
     <para>
      If you are <emphasis role="bold">not</emphasis> running the default
      container image registry <literal>registry.suse.com</literal>, but a
      local container registry, configure &cephadm; to use the container image
      from the local registry for the deployment of Node-Exporter before
      deploying the Node-Exporter. Otherwise you can safely skip this step and
      ignore the following warning.
     </para>
<screen>&prompt.cephuser;ceph config set mgr mgr/cephadm/container_image_node_exporter <replaceable>QUALIFIED_IMAGE_PATH</replaceable></screen>
     <para>
      Make sure that all container images for monitoring services point to the
      local registry, not only the one for Node-Exporter. This step requires
      you to do so for the Node-Exporter only, but it is advised than you set
      all the monitoring container images in &cephadm; to point to the local
      registry at this point.
     </para>
     <para>
      If you do not do so, new deployments of monitoring services as well as
      re-deployments will use the default &cephadm; configuration and you may
      end up being unable to deploy services (in the case of air-gapped
      deployments), or with services deployed with mixed versions.
     </para>
     <para>
      How &cephadm; needs to be configured to use container images from the
      local registry is described in
      <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Resume the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Gateway service redeployment</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Upgrading the &ogw;</title>
   <para>
    In &productname; &productnumber;, the &ogw;s are always configured with a
    realm, which allows for multi-site (see <xref linkend="ceph-rgw-fed"/> for
    more details) in the future. If you used a single-site &ogw; configuration
    in &productname; &prevprevproductnumber;, follow these steps to add a
    realm. If you do not plan to actually use the multi-site functionality, it
    is fine to use <literal>default</literal> for the realm, &zgroup; and zone
    names.
   </para>
   <procedure>
    <step>
     <para>
      Create a new realm:
     </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Optionally, rename the default zone and &zgroup;.
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
&prompt.cephuser;radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure the master &zgroup;:
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure the master zone. For this, you will need the ACCESS_KEY and
      SECRET_KEY of an &ogw; user with the <option>system</option> flag
      enabled. This is usually the <literal>admin</literal> user. To get the
      ACCESS_KEY and SECRET_KEY, run <command>radosgw-admin user info --uid
      admin --rgw-zone=<replaceable>ZONE_NAME</replaceable></command>.
     </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Commit the updated configuration:
     </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    To have the &ogw; service containerized, create its specification file as
    described in <xref linkend="deploy-cephadm-day2-service-ogw"/>, and apply
    it.
   </para>
<screen>
&prompt.cephuser;ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrading &ganesha;</title>
   &ganesha_support;
   <para>
    The following demonstrates how to migrate an existing &ganesha; service
    running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
   </para>
   <warning>
    <para>
     The following documentation requires you to have already successfully
     upgraded the core &ceph; services.
    </para>
   </warning>
   <para>
    &ganesha; stores additional per-daemon configuration and exports
    configuration in a &rados; pool. The configured &rados; pool can be found
    on the <literal>watch_url</literal> line of the
    <literal>RADOS_URLS</literal> block in the
    <filename>ganesha.conf</filename> file. By default, this pool will be named
    <literal>ganesha_config</literal>
   </para>
   <para>
    Before attempting any migration, we strongly recommend making a copy of the
    export and daemon configuration objects located in the RADOS pool. To
    locate the configured RADOS pool, run the following command:
   </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    To list the contents of the RADOS pool:
   </para>
<screen>&prompt.cephuser;rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    To copy the RADOS objects:
   </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool ganesha_config --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    On a per-node basis, any existing &ganesha; service needs to be stopped and
    then replaced with a container managed by &cephadm;.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &ganesha; service:
     </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      After the existing &ganesha; service has been stopped, a new one can be
      deployed in a container using &cephadm;. To do so, you need to create a
      service specification that contains a <literal>service_id</literal> that
      will be used to identify this new NFS cluster, the host name of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
      For example:
     </para>
<screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      For more information on creating a placement specification, see
      <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Apply the placement specification:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirm the &ganesha; daemon is running on the host:
     </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7.1/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Repeat these steps for each &ganesha; node. You do not need to create a
      separate service specification for each node. It is sufficient to add
      each node's host name to the existing NFS service specification and
      re-apply it.
     </para>
    </step>
   </procedure>
   <para>
    The existing exports can be migrated in two different ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Manually re-created or re-assigned using the &dashboard;.
     </para>
    </listitem>
    <listitem>
     <para>
      Manually copy the contents of each per-daemon &rados; object into the
      newly created &ganesha; common configuration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manually copying exports to &ganesha; common configuration file</title>
    <step>
     <para>
      Determine the list of per-daemon RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool ganesha_config --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Make a copy of the per-daemon &rados; objects:
     </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sort and merge into a single list of exports:
     </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.<replaceable>SERVICE_ID</replaceable>
&prompt.cephuser;cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Write the new &ganesha; common configuration file:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Notify the &ganesha; daemon:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       This action will cause the daemon to reload the configuration.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    After the service has been successfully migrated, the Nautilus-based
    &ganesha; service can be removed.
   </para>
   <procedure>
    <step>
     <para>
      Remove &ganesha;:
     </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remove the legacy cluster settings from the &dashboard;:
     </para>
<screen>&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Upgrading the &mds;</title>
   <para>
    Unlike MONs, MGRs and OSDs, &mds; cannot be adopted in-place. Instead, you
    need to redeploy them in containers using the &ceph; orchestrator.
   </para>
   <procedure>
    <step>
     <para>
      Run the <command>ceph fs ls</command> command to obtain the name of your
      file system, for example:
     </para>
<screen>
&prompt.cephuser;ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Create a new service specification file <filename>mds.yml</filename> as
      described in <xref linkend="deploy-cephadm-day2-service-mds"/> by using
      the file system name as the <option>service_id</option> and specifying
      the hosts that will run the MDS daemons. For example:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Run the <command>ceph orch apply -i mds.yml</command> command to apply
      the service specification and start the MDS daemons.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrading the &igw;</title>
   <para>
    To upgrade the &igw;, you need to redeploy it in containers using the
    &ceph; orchestrator. If you have multiple &igw;s, you need to redeploy them
    one-by-one to reduce the service downtime.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &iscsi; daemons on each &igw; node:
     </para>
<screen>
&prompt.sudo;systemctl stop rbd-target-gw
&prompt.sudo;systemctl disable rbd-target-gw
&prompt.sudo;systemctl stop rbd-target-api
&prompt.sudo;systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Create a service specification for the &igw; as described in
      <xref
      linkend="deploy-cephadm-day2-service-igw"/>. For this, you
      need the <option>pool</option>, <option>trusted_ip_list</option>, and
      <option>api_*</option> settings from the existing
      <filename>/etc/ceph/iscsi-gateway.cfg</filename> file. If you have SSL
      support enabled (<literal>api_secure = true</literal>), you also need the
      SSL certificate (<filename>/etc/ceph/iscsi-gateway.crt</filename>) and
      key (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      For example, if <filename>/etc/ceph/iscsi-gateway.cfg</filename> contains
      the following:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Then you need to create the following service specification file
      <filename>iscsi.yml</filename>:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       The <option>pool</option>, <option>trusted_ip_list</option>,
       <option>api_port</option>, <option>api_user</option>,
       <option>api_password</option>, <option>api_secure</option> settings are
       identical to the ones from the
       <filename>/etc/ceph/iscsi-gateway.cfg</filename> file. The
       <option>ssl_cert</option> and <option>ssl_key</option> values can be
       copied in from the existing SSL certificate and key files. Verify that
       they are indented correctly and the <emphasis>pipe</emphasis> character
       <literal>|</literal> appears at the end of the
       <literal>ssl_cert:</literal> and <literal>ssl_key:</literal> lines (see
       the content of the <filename>iscsi.yml</filename> file above).
      </para>
     </note>
    </step>
    <step>
     <para>
      Run the <command>ceph orch apply -i iscsi.yml</command> command to apply
      the service specification and start the &igw; daemons.
     </para>
    </step>
    <step>
     <para>
      Remove the old <package>ceph-iscsi</package> package from each of the
      existing iSCSI gateway nodes:
     </para>
<screen>&prompt.cephuser;zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Post-upgrade Clean-up</title>

  <para>
   After the upgrade, perform the following clean-up steps:
  </para>

  <procedure>
   <step>
    <para>
     Verify that the cluster was successfully upgraded by checking the current
     &ceph; version:
    </para>
<screen>&prompt.cephuser;ceph versions</screen>
   </step>
   <step>
    <para>
     Make sure that no old OSDs will join the cluster:
    </para>
<screen>&prompt.cephuser;ceph osd require-osd-release octopus</screen>
   </step>
   <step>
    <para>
     Enable the autoscaler module:
    </para>
<screen>&prompt.cephuser;ceph mgr module enable pg_autoscaler</screen>
    <important>
     <para>
      Pools in &productname; 6 had the <option>pg_autoscale_mode</option> set
      to <option>warn</option> by default. This resulted in a warning message
      in case of suboptimal number of PGs, but autoscaling did not actually
      happen. The default in &productname; &productnumber; is that the
      <option>pg_autoscale_mode</option> option is set to <option>on</option>
      for new pools, and PGs will actually autoscale. The upgrade process does
      not automatically change the <option>pg_autoscale_mode</option> of
      existing pools. If you want to change it to <option>on</option> to get
      the full benefit of the autoscaler, see the instructions in
      <xref
      linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Find more details in <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Prevent pre-Luminous clients:
    </para>
<screen>&prompt.cephuser;ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Enable the balancer module:
    </para>
<screen>
&prompt.cephuser;ceph balancer mode upmap
&prompt.cephuser;ceph balancer on
</screen>
    <para>
     Find more details in <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Optionally, enable the telemetry module:
    </para>
<screen>
&prompt.cephuser;ceph mgr module enable telemetry
&prompt.cephuser;ceph telemetry on
 </screen>
    <para>
     Find more details in <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
