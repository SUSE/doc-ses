<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster, you need to have both the underlying
   &sls; and &storage; correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     Admin node (if you deployed the cluster using the admin node)
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; monitors
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; &rgw;s
    </para>
   </listitem>
   <listitem>
    <para>
     iSCSI gateways (refer to
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_iscsi_up"/>
     for more details)
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>&prompt.root;ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>&prompt.root;ceph osd stat</screen>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.to4">
  <title>Upgrade from &storage; 2.1 to 4</title>

  <tip>
   <para>
    &storage; 2.1 to 4 upgrade procedure includes all the steps valid for
    upgrading
    <link
    xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html"
    >
    &storage; 2.1 to 3</link>. Therefore the &storage; 2.1 to 4 upgrade
    procedure will often reference them.
   </para>
  </tip>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 2.1
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 2.1 cluster to version 4, follow these steps on
   each cluster node:
  </para>

  <procedure>
   <step>
    <warning>
     <title>Do Not Run <command>zypper dup</command> or Reboot the Node</title>
     <para>
      After you prepare for the upgrade to &sls; 12 SP2 as suggested later in
      this step, do <emphasis>not</emphasis> run <command>zypper dup</command>
      or reboot the node as its &ceph; related services may not start
      correctly.
     </para>
    </warning>
    <para>
     Upgrade the current &sls; to version 12 SP2. Refer to
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_update_sle.html"/>
     for more information on supported upgrade methods.
    </para>
   </step>
   <step>
    <para>
     List all the active services with <command>zypper ls</command>.
    </para>
<screen>&prompt.cephuser;sudo zypper ls
#| Alias                                      | Name | Enabled | Refresh | Type
-+--------------------------------------------+------+---------+---------+------
1| SUSE_Enterprise_Storage_3_x86_64           | ...  | Yes     | Yes     | ris
2| SUSE_Linux_Enterprise_Server_12_SP2_x86_64 | ...  | Yes     | Yes     | ris
[...]</screen>
    <para>
     Verify that services related to &sls; 12 SP2 are present and enabled.
    </para>
   </step>
   <step>
    <para>
     Remove the current &storage; service. You can do it as follows:
    </para>
<screen>&prompt.cephuser;sudo zypper rs <replaceable>ID</replaceable></screen>
   </step>
   <step>
    <para>
     Activate &storage; 4 service. You can use <command>yast2 add-on</command>.
    </para>
   </step>
   <step>
    <para>
     Refresh new software repositories:
    </para>
<screen>&prompt.cephuser;sudo zypper ref</screen>
   </step>
   <step>
    <para>
     Install the upgrade helper package:
    </para>
<screen>&prompt.cephuser;sudo zypper in ses-upgrade-helper</screen>
   </step>
   <step>
    <para>
     Run the upgrade script:
    </para>
<screen>&prompt.cephuser;sudo upgrade-ses.sh</screen>
    <para>
     The script does the distribution upgrade of the node. After reboot, the
     node comes up with &sls; 12 SP2 and &storage; 4 running.
    </para>
   </step>
   <step>
    <para>
     Check
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_varlibceph">
     the ownership of <filename>/var/lib/ceph</filename></link>.
    </para>
   </step>
   <step>
    <para>
     Check
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_rgw">
     the &rgw; instance name</link>.
    </para>
   </step>
   <step>
    <para>
     Check
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_rgwlog">
     the &rgw; log file</link>.
    </para>
   </step>
   <step>
    <para>
     Check
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_tunables">
     CRUSH Tunables</link>.
    </para>
   </step>
   <step>
    <para>
     Check
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_guid">
     OSD partition GUID codes</link>.
    </para>
   </step>
   <step>
    <para>
     Upgrade
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_admin_node">
     the admin node</link>.
    </para>
   </step>
   <step>
    <para>
     Upgrade
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2_1to3_calamari">
     the Calamari server</link>.
    </para>
   </step>
   <step>
    <para>
     Set
     <link
      xlink:href="https://www.suse.com/documentation/ses-3/book_storage_admin/data/ceph_upgrade_2_1to3.html#ceph_upgrade_2to3_jewel">
     the <option>require_jewel_osds osdmap</option> flag</link>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to4">
  <title>Upgrade from &storage; 3 to 4</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   We recommended upgrading &sls; and &storage; in one step using the
   <command>zypper migration</command> command. Find more detailed information
   in the
   <link
    xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/cha_update_spmigration.html">Service
   Pack Migration</link> section, more specifically in the
   <link
    xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_update_migr_zypper_onlinemigr.html">Migrating
   with Zypper</link> subsection and the <command>zypper migration</command>
   manual page (<command>man zypper-migration</command> or <command>zypper help
   migration</command>).
  </para>

  <para>
   <command>zypper migration</command> upgrades both the underlying &sls; and
   the &storage; product. After it successfully finishes, reboot the node
   manually as soon as possible. After reboot, the node comes up with &sls; 12
   SP2 and &storage; 4 running.
  </para>
 </sect1>
</chapter>
