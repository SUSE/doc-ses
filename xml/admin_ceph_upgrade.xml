<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package
   <package>release-notes-ses</package>
   , find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link
     xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster, you need to have both the underlying
   &sls; and &storage; correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &mon;s
    </para>
   </listitem>
   <listitem>
    <para>
     &mgr;s
    </para>
   </listitem>
   <listitem>
    <para>
     &osd;s
    </para>
   </listitem>
   <listitem>
    <para>
     &mds;s
    </para>
   </listitem>
   <listitem>
    <para>
     &rgw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &igw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &ganesha;
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>&prompt.root;ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>&prompt.root;ceph osd stat</screen>
  </tip>

  <tip xml:id="ceph.upgrade.luminous_flag">
   <title>Set require-osd-release luminous Flag</title>
   <para>
    When the last OSD is upgraded to &storage; 5, the monitor
    nodes will detect that all OSDs are running the 'luminous' version of
    &ceph; and they may complain that the
    <option>require-osd-release luminous</option> osdmap flag is not set. In that
    case, you need to set this flag manually to acknowledge that&mdash;now that
    the cluster has been upgraded to 'luminous'&mdash;it cannot be downgraded
    back to &ceph; 'jewel'. Set the flag by running the following command:
   </para>
<screen>&prompt.sminion;sudo ceph osd require-osd-release luminous</screen>
   <para>
    After the command completes, the warning disappears.
   </para>
   <para>
    On fresh installs of &storage; 5, this flag is set automatically when the
    &ceph; monitors create the initial osdmap, so no end-user action is needed.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>Upgrade from &storage; 4 (&deepsea; Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    In addition, before starting the upgrade, you need to upgrade the &smaster; node
    to &sls; 12 SP3 and &storage; 5 by running <command>zypper migration</command>
    (or your preferred way of upgrading).
   </para>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Check whether the &aa; service is running and disable it on each cluster
      node. Start &yast; &aa; module, and select <guimenu>Settings</guimenu>
      and then deactivate the <guimenu>Enable Apparmor</guimenu> check box.
      Confirm with <guimenu>Done</guimenu>.
     </para>
     <para>
      Note that &storage; will <emphasis>not</emphasis> work with &aa; enabled.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order, based on their assigned role as recommended by &ceph; upstream:
      MONs, MGRs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent from violating the prescribed order if
      a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply for example new kernel versions. To reduce
      waiting I/O operations, we recommend declining incoming requests for the
      duration of the upgrade process.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &storage; 4 cluster to version 5, follow these steps:
  </para>

  <procedure>
   <step>
    <substeps>
     <step>
      <para>
       If you are using repositories provided by SCC/SMT, make sure to get the
       latest channels via <command>SUSEConnect</command> on all your
       &sminion;s and &smaster;. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-migration</screen>
     </step>
     <step>
      <para>
       If you are not using SCC/SMT but a Media-ISO or other package source,
       change your Pillar data in order to use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: default</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To update your Pillar, please re-run Stage 2:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt '*' pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt '*' test.version</screen>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
   <step>
    <para>
     After the upgrade, the &mgr;s are not installed yet. To reach a healthy
     cluster state, do the following:
    </para>
    <substeps>
     <step>
      <para>
       Run Stage 1 to create the <filename>role-mgr/</filename> subdirectory:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edit <guimenu>policy.cfg</guimenu> as described in
       <xref
        linkend="policy.configuration"/> and add a &mgr; roles to
       the nodes where &mon;s are deployed.
      </para>
     </step>
     <step>
      <para>
       Run Stage 2 to update the Pillar:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       Run Stage 3 to deploy &mgr;s:
      </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrade to &storage; 5, FileStore OSDs need approximately 5 minutes
    longer to start as the OSD will do a one-off conversion of its on-disk
    files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade, you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, &sls;, and gives you a report displaying the
    version that the majority of nodes have and shows nodes whose version is
    different from the majority.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>Upgrade from &storage; 4 (<command>ceph-deploy</command> Deployment) to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 4 cluster which was deployed with
   <command>ceph-deploy</command> to version 5, follow these steps:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>Steps to Apply to All Cluster Nodes</title>
   <step>
    <para>
     Prepare software repositories. Disable all repositories except for &sls;
     12 SP3 and &storage; 5 repositories. If these repositories are missing,
     add them.
    </para>
   </step>
   <step>
    <para>
     Install the <systemitem>salt</systemitem> package:
    </para>
<screen>&prompt.root;zypper install salt</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>salt-minion</systemitem> package, then enable and
     start the related service:
    </para>
<screen>&prompt.root;zypper install salt-minion
&prompt.root;systemctl enable salt-minion
&prompt.root;systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Ensure that the host name 'salt' resolves to the IP address of the
     &smaster;/admin node. If your &smaster; is not reachable by the host name
     <literal>salt</literal>, edit the file
     <filename>/etc/salt/minion</filename> or create a new file
     <filename>/etc/salt/minion.d/master.conf</filename> with the following
     content:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     If you performed any changes to the configuration files mentioned above,
     restart the &salt; service on all &sminion;s:
    </para>
<screen>&prompt.sminion;systemctl restart salt-minion.service</screen>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Steps to Apply to the Admin Node/&smaster;</title>
   <step>
    <para>
     Install the <systemitem>salt-master</systemitem> package, then enable and
     start the related service:
    </para>
<screen>&prompt.smaster;zypper install salt-master
&prompt.smaster;systemctl enable salt-master
&prompt.smaster;systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verify the presence of all &sminion;s by listing their keys:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Add all &sminion;s keys to &smaster; including the minion master:
    </para>
<screen>&prompt.smaster;salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Ensure that all &sminion;s' keys were accepted:
    </para>
<screen>&prompt.smaster;salt-key -L</screen>
   </step>
   <step>
    <para>
     Install the <systemitem>deepsea</systemitem> package:
    </para>
<screen>&prompt.smaster;zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Engulf Existing <command>ceph-deploy</command> Installed Cluster:
    </para>
<screen>&prompt.smaster;salt-run populate.engulf_existing_cluster</screen>
    <para>
     The command will do the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribute all the required &salt; and &deepsea; modules to all the
       &sminion;s.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspect the running &ceph; cluster and populate
       <filename>/srv/pillar/ceph/proposals</filename> with a layout of the
       cluster.
      </para>
      <para>
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> will be
       created with roles matching all detected running &ceph; services. View
       this file to verify that each of your existing MON, OSD, RGW and MDS
       nodes have the appropriate roles. OSD nodes will be imported into the
       <filename>profile-import/</filename> subdirectory, so you can examine the
       files in
       <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename>
       and
       <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename>
       to confirm that the OSDs were correctly picked up.
      </para>
      <note>
       <para>
        The generated <filename>policy.cfg</filename> will only apply roles for
        detected &ceph; services 'role-mon', 'role-mgr', 'role-mds', 'role-rgw',
        'role-admin', and 'role-master' for the &smaster; node.  Any other
        desired roles will need to be added to the file manually (see <xref
         linkend="policy.role.assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       The existing cluster's <filename>ceph.conf</filename> will be saved to
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>
       will include the cluster's fsid, cluster and public networks, and also
       specifies the <option>configuration_init: default-import</option> option,
       which makes &deepsea; use the <filename>ceph.conf.import</filename>
       config file mentioned previously, rather than using &deepsea;'s default
       <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>
       template.
      </para>
     </listitem>
     <listitem>
      <para>
       The cluster's various keyrings will be saved to the following
       directories:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mgr/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     From this point on, follow the procedure described in
     <xref
      linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>Upgrade from &storage; 3 to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   To upgrade the &storage; 3 cluster to version 5, follow the steps described
   in <xref linkend="upgrade4to5cephdeploy.all"/> and then
   <xref
    linkend="upgrade4to5cephdeploy.admin"/>.
  </para>
 </sect1>
</chapter>
