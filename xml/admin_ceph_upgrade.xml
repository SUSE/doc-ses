<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &storage; from the previous
  release(s) to the current one.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package>,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link
     xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Before upgrading the &ceph; cluster, you need to have both the underlying
   &sls; and &storage; correctly registered against SCC or SMT. You can upgrade
   daemons in your cluster while the cluster is online and in service. Certain
   types of daemons depend upon others. For example &ceph; &rgw;s depend upon
   &ceph; monitors and &ceph; OSD daemons. We recommend upgrading in this
   order:
  </para>

  <orderedlist spacing="normal">
   <listitem>
    <para>
     &ceph; monitors
    </para>
   </listitem>
   <listitem>
    <para>
     &ceph; OSD daemons
    </para>
   </listitem>
   <listitem>
    <para>
     MDS
    </para>
   </listitem>
   <listitem>
    <para>
     &rgw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &igw;s
    </para>
   </listitem>
   <listitem>
    <para>
     &ganesha;
    </para>
   </listitem>
  </orderedlist>

  <tip>
   <para>
    We recommend upgrading all the daemons of a specific type&mdash;for example
    all monitor daemons or all OSD daemons&mdash;one by one to ensure that they
    are all on the same release. We also recommend that you upgrade all the
    daemons in your cluster before you try to exercise new functionality in a
    release.
   </para>
   <para>
    After all the daemons of a specific type are upgraded, check their status.
   </para>
   <para>
    Ensure each monitor has rejoined the quorum after all monitors are
    upgraded:
   </para>
<screen>&prompt.root;ceph mon stat</screen>
   <para>
    Ensure each &ceph; OSD daemon has rejoined the cluster after all OSDs are
    upgraded:
   </para>
<screen>&prompt.root;ceph osd stat</screen>
  </tip>
  <sect2 xml:id="ceph.upgrade.luminous_flag">
   <title>Set require_luminous_osds osdmap Flag</title>
   <para>
    When the last OSD is upgraded from &storage;; 3 or 4 to 5, the monitor nodes
    will detect that all OSDs are running the 'luminous' version of &ceph;  and
    they may complain that the <option>require_luminous_osds</option> osdmap
    flag is not set. In that case, you need to set this flag manually to
    acknowledge that&mdash;now that the cluster has been upgraded to
    'luminous'&mdash;it cannot be downgraded back to &ceph; 'jewel'. Set the
    flag by running the following command:
   </para>
<screen>&prompt.sminion;sudo osd set require_luminous_osds</screen>
   <para>
    After the command completes, the warning disappears.
   </para>
   <para>
    On fresh installs of &storage; 5, this flag is set automatically when the
    &ceph; monitors create the initial osdmap, so no end-user action is needed.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.to4">
  <title>Upgrade from &storage; 4 to 5</title>

  <important>
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    packages versions on all the &ceph; nodes you want to upgrade before you
    can start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      &storage; 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Note that you need to update the <systemitem>deepsea</systemitem> package on
    the &storage; 4 &smaster; to include &deepsea; from &storage; 5 before
    running the upgrade procedure.
   </para>
  </important>

  <warning>
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
    <para>
     Check whether the &aa; service is running and disable it on each cluster
     node. Start &yast; &aa; module, and select <guimenu>Settings</guimenu> and
     then deactivate the <guimenu>Enable Apparmor</guimenu> checkbox. Confirm
     with <guimenu>Done</guimenu>.
    </para>
    <para>
     Note that &storage; will <emphasis>not</emphasis> work with &aa; enabled.
    </para>
    </listitem>
    <listitem>
     <para>
      You cannot proceed with the upgrade procedure if there are OSDs assigned
      to the &smaster;. You can check for it by running
     </para>
<screen>&prompt.smaster;salt '<replaceable>salt_master_name</replaceable>' pillar.get roles</screen>
     <para>
      If the output of the previous command lists the word 'storage', stop the
      upgrade and merge the OSDs away from &smaster;. Refer to
      <xref
       linkend="salt.adding.nodes"/> for more details on how to add
      a new cluster node.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the cluster is fully functional during the upgrade, &deepsea;
      sets the 'noout' flag which prevents &ceph; from rebalancing data during
      downtime and therefore avoids unnecessary data transfers.
     </para>
    </listitem>
    <listitem>
     <para>
      To optimize the upgrade process, &deepsea; upgrades your nodes in the
      order based on their assigned role as recommended by &ceph; upstream
      recommendation: MONs, OSDs, MDS, RGW, IGW, and NFS Ganesha.
     </para>
     <para>
      Note that &deepsea; cannot prevent from violating the prescribed order if
      a node runs multiple services.
     </para>
    </listitem>
    <listitem>
     <para>
      Although the &ceph; cluster is operational during the upgrade, nodes may
      get rebooted in order to apply for example new kernel versions. To reduce
      waiting I/O operations, we recommend declining incoming requests for the
      duration of the upgrade process.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   To upgrade the &storage; 4 cluster to version 5, follow these steps:
  </para>

  <procedure>
   <step>
    <substeps>
     <step>
      <para>
       If you are using repositories provided by SCC/SMT, make sure to get the
       latest channels via <command>SUSEConnect</command> on all your
       &sminion;s and &smaster;. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: zypper-migration</screen>
     </step>
     <step>
      <para>
       If you are not using SCC/SMT but a Media-ISO or other package source,
       change your Pillar data in order to use a different strategy. Edit
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       and add the following line:
      </para>
<screen>upgrade_init: default</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     To update your Pillar, please re-run Stage 2:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Verify that you successfully wrote to the Pillar:
    </para>
<screen>&prompt.smaster;salt '*' pillar.get upgrade_init</screen>
    <para>
     The command's output should mirror the entry you added.
    </para>
   </step>
   <step>
    <para>
     Update &deepsea; on the &smaster; to include the &storage; 5 version:
    </para>
    <screen>&prompt.smaster;zypper refresh &amp;&amp; zypper update deepsea</screen>
   </step>
   <step>
    <para>
     Upgrade the salt-minion and salt-master service, and Deepsea on the
     &smaster;:
    </para>
<screen>&prompt.smaster;salt '<replaceable>salt_master_name</replaceable>' state.apply ceph.updates.master</screen>
   </step>
   <step>
    <para>
     Upgrade &sminion;s:
    </para>
<screen>&prompt.smaster;salt '*' state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verify that all &sminion;s are upgraded:
    </para>
<screen>&prompt.smaster;salt '*' test.version</screen>
   </step>
   <step>
    <para>
     Start the upgrade of &sls; and &ceph;:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>Re-run on Reboot</title>
     <para>
      If the process results in a reboot of the &smaster;, re-run the command
      to start the upgrade process for the &sminion;s again.
     </para>
    </tip>
   </step>
  </procedure>

  <important>
   <title>Upgrade Failure</title>
   <para>
    If the cluster is in 'HEALTH_ERR' state for more than 300 seconds, or one
    of the services for each assigned role is down for more than 900 seconds,
    the upgrade failed. In that case, try to find the problem, resolve it, and
    re-run the upgrade procedure. Note that in virtualized environments, the
    timeouts are shorter.
   </para>
  </important>

  <important>
   <title>Rebooting OSDs</title>
   <para>
    After upgrade to &storage; 5, FileStore OSDs need approximately 5 minutes
    longer to start as the OSD will do a one-off conversion of its on-disk files.
   </para>
  </important>

  <tip>
   <title>Check for the Version of Cluster Components/Nodes</title>
   <para>
    When you need to find out the versions of individual cluster components and
    nodes&mdash;for example to find out if all your nodes are actually on the
    same patch level after the upgrade, you can run
   </para>
<screen>&prompt.smaster;salt-run status.report</screen>
   <para>
    The command goes through the connected &sminion;s and scans for the version
    numbers of &ceph;, &salt;, &sls;, and gives you a report displaying the
    version that the majority of nodes have and shows nodes whose version is
    different from the majority.
   </para>
  </tip>
 </sect1>
</chapter>
