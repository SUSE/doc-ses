<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Upgrading from &prevproductnumber; to
  &productnumber; means the following:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrading from &ceph; &prevcephname; to &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    Switching from installing and running &ceph; via RPM packages to running in
    containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Complete removal of &deepsea; and replacing with &cephsalt; and &cephadm;.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   The upgrade information in this chapter <emphasis>only</emphasis> applies to
   upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
   instructions if you want to deploy &productname; on the &casp; platform.
  </para>
 </warning>
 <important>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Before Upgrading</title>

  <para>
   Before upgrading, ensure you read through the following sections to ensure
   you understand all tasks that need to be executed.
  </para>

  <sect2 xml:id="upgrade-consider-points">
   <title>Points to Consider</title>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Read the release notes</emphasis> - there you can find
      additional information on changes since the previous release of
      &productname;. Check the release notes to see whether:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Your hardware needs special considerations.
       </para>
      </listitem>
      <listitem>
       <para>
        Any used software packages have changed significantly.
       </para>
      </listitem>
      <listitem>
       <para>
        Special precautions are necessary for your installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </para>
     <para>
      After having installed the package <package>release-notes-ses</package>,
      find the release notes locally in the directory
      <filename>/usr/share/doc/release-notes</filename> or online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a long time&mdash;approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      You have to upgrade the &smaster; first, and replace &deepsea; with
      &cephsalt; and &cephadm; after. You will <emphasis>not</emphasis> be able
      to start using the &cephadm; orchestrator module until at least all
      &mgr;'s are upgraded.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade from using &prevcephname; RPMs to &cephname; containers needs
      to happen all in one step. This means upgrade an entire node at a time,
      not one daemon at a time.
     </para>
    </listitem>
    <listitem>
     <para>
      The OSD migration from &filestore; to &bluestore;
      <emphasis>needs</emphasis> to happen before the upgrade as it is
      unsupported in &productname; &productnumber;. Find more details about
      &bluestore; and how to migrate from &filestore; at
      <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are running an older cluster that still uses
      <literal>ceph-disk</literal> OSDs, you <emphasis>need</emphasis> to
      switch to <literal>ceph-volume</literal> before the upgrade. Find more
      detials in
      <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Backing Up Cluster Configuration and Data</title>
   <para>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to &productname; &productnumber;. For instruction on
    how to back up all your data, see
    <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verify Steps From Previous Upgrade</title>
   <para>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </para>
   <para>
    Check for the existence of the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
    file.
   </para>
   <para>
    This file is created by the engulf process during the upgrade from
    &productname; 5 to 6. The <option>configuration_init:
    default-import</option> option is set in
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    If <option>configuration_init</option> is still set to
    <option>default-import</option>, the cluster is using
    <filename>ceph.conf.import</filename> as its configuration file and not
    &deepsea;'s default <filename>ceph.conf</filename> which is compiled from
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Therefore, you need to inspect <filename>ceph.conf.import</filename> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Then remove the <option>configuration_init: default-import</option> line
    from
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
<!-- Notes: This section should also contain information on:
* Ensuring you have enough disk space -->
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Update Cluster Nodes and Verify Cluster Health</title>
   <para>
    Verify that all latest updates of &prevcephos; and &productname;
    &prevproductnumber; are applied to all cluster nodes:
   </para>
<screen>&prompt.root;zypper refresh &amp;&amp;zypper patch</screen>
   <para>
    After updates are applied, check the cluster health:
   </para>
<screen>&prompt.cephuser;ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verify Access to Software Repositories and Container Images</title>
   <para>
    Verify that each cluster node has access to &prevcephos; and &productname;
    &prevproductnumber; software repositories as well as registry of container
    images.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Software Repositories</title>
    <para>
     If all nodes are registered with SCC, you will be able to use the
     <command>zypper migration</command> command to upgrade.
    </para>
    <para>
     If nodes are <emphasis role="bold">not</emphasis> registered with SCC,
     disable all existing software repositories and add bot
     <literal>Pool</literal> and <literal>Updates</literal> repositories for
     each of the following extensions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Container Images</title>
    <para>
     All cluster nodes need access to the container image registry. In most
     cases, you will use the public SUSE registry at
     <literal>registry.suse.com</literal>. You need the following images:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Alternatively&mdash;for example, for air-gapped
     deployments&mdash;configure a local registry and verify that you have the
     correct set of container images available.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrade the &smaster;</title>

  <para>
   The following procedure describes the process of upgrading the &smaster;:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For cluster whose all nodes are registered with SCC, run <command>zypper
       migrate</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       For cluster whose nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Disable &deepsea; stages to avoid anyone accidentally using it. Add the
     following content to
     <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     If you are not using container images from
     <literal>registry.suse.com</literal> but rather the locally configured
     registry, edit <filename>/srv/pillar/ceph/stack/global.yml</filename> to
     inform &deepsea; which &ceph; container image and registry to use. For
     example, to use <literal>192.168.121.1:5000/my/ceph/image</literal> from
     an insecure (HTTP) registry, add the following lines:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
  - insecure: true
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;alt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Assimilate existing configuration:
    </para>
<screen>&prompt.cephuser;ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verify the upgrade status. Your output may differ depending on your
     cluster configuration:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrade the MON, MGR, and OSD Nodes</title>

  <para>
   Upgrade MON, MGR, and OSD nodes subsequently, one at a time. For each of
   them, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     If the node you are upgrading is an OSD node, avoid having the OSD marked
     out during the upgrade by running
    </para>
<screen>&prompt.cephuser;ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Replace <replaceable>SHORT_NODE_NAME</replaceable> with the short name of
     the node as it appears in the output of the <command>ceph osd
     tree</command> command.
    </para>
   </step>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For cluster whose all nodes are registered with SCC, run <command>zypper
       migrate</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       For cluster whose nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     After the node is rebooted, containerize all existing MON, MGR, and OSD
     daemons on that node by running the following command on the &smaster;:
    </para>
<screen>&prompt.smaster;salt <replaceable>HOST_NAME</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Replace <replaceable>HOST_NAME</replaceable> with the host name of the
     node that you are upgrading.
    </para>
    <tip>
     <para>
      To see the status and progress of the <emphasis>adoption</emphasis>,
      check the &dashboard; or run one of the following commands on the
      &smaster;:
     </para>
<screen>
ceph status
ceph versions
salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     After the adoption has successfully finished, unset the
     <literal>noout</literal> flag if the node you are upgrading is an OSD
     node:
    </para>
<screen>&cephuser;ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Install &cephsalt; on the &smaster;</title>

  <para></para>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Upgrade the Gateways</title>

  <sect2 xml:id="upgrade-mds">
   <title>Upgrade the &mds;</title>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrade the &igw;</title>
  </sect2>

  <sect2 xml:id="upgrade-ogw">
   <title>Upgrade the &ogw;</title>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrade &ganesha;</title>
   <para>
    The following demonstrates how to migrate an existing &ganesha; service
    running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
   </para>
   <warning>
    <para>
     The following documentation requires you to have already successfully
     upgraded the core &ceph; services.
    </para>
   </warning>
   <para>
    &ganesha; stores additional per-daemon config and export configs in a
    &rados; pool. The configured &rados; pool can be found on the
    <literal>watch_url</literal> line of the <literal>RADOS_URLS</literal>
    block in the <filename>ganesha.conf</filename> file.
   </para>
   <para>
    Before attempting any migration, we strongly recommend to make a copy of
    the export and daemon config objects located in the RADOS pool To locate
    the configured RADOS pool, run the following command:
   </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    To list the contents of the RADOS pool:
   </para>
<screen>&prompt.cephuser;rados --pool cephfs_data --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    To copy the RADOS objects:
   </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    On a per node basis, an existing &ganesha; service needs to be stopped and
    then replaced with a container managed by &cephadm;.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &ganesha; service:
     </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      After the existing &ganesha; service has been stopped, a new one can be
      deployed in a container using &cephadm;. To do so, you need to create a
      service specification that contains a <literal>service_id</literal> that
      will be used to identify this new NFS cluster, the hostname of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
     </para>
     <para>
      Deploy a new &ganesha; container by creating a new placement
      specification. For information on creating a placement specification see
      <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Apply the placement specification:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirm the &ganesha; daemon is running on the host:
     </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
   </procedure>
   <para>
    The existing exports can be migrated in two different ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Manually re-created or re-assigned using the &dashboard;.
     </para>
<!-- depends on https://github.com/ceph/ceph/pull/36948 -->
    </listitem>
    <listitem>
     <para>
      Manually copy the contents of each per-daemon &rados; object into the
      newly created &ganesha; common configuration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manually Copying Exports to &ganesha; Common Configuration File</title>
    <step>
     <para>
      Determine the list of per-daemon RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Make a copy of the per-daemon &rados; objects:
     </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.foo
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sort and merge into a single list of exports:
     </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.foo
&prompt.cephuser;cat conf-nfs.foo
%url "rados://cephfs_data/ganesha/export-1"
%url "rados://cephfs_data/ganesha/export-2"
%url "rados://cephfs_data/ganesha/export-3"
%url "rados://cephfs_data/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Write the new &ganesha; common configuration file:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.foo conf-nfs.foo</screen>
    </step>
    <step>
     <para>
      Notify the &ganesha; daemon:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.foo conf-nfs.foo</screen>
     <note>
      <para>
       This action will cause the daemon to reload the configuration.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    After the service has been successfully migrated, the Nautilus based
    &ganesha; service can be removed.
   </para>
   <procedure>
    <step>
     <para>
      Remove &ganesha;:
     </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remove the legacy cluster settings from the &dashboard;:
     </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard set-ganesha-clusters-rados-pool-namespace cluster2:pool2/ns2
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Post-upgrade Clean-up</title>

<screen>
  # ceph versions
  # ceph osd require-osd-release octopus
  # ceph mgr module enable pg_autoscaler
  # ceph mgr module enable telemetry
  # ceph telemetry on
  # ceph osd set require-min-compat-client luminous
  # ceph balancer mode upmap
  # ceph balancer on
  [...]
</screen>
 </sect1>
</chapter>
