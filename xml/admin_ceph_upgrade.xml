<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade from a Previous Release</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;.
 </para>
 <para>
  The upgrade includes the following tasks:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrading from &ceph; &prevcephname; to &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    Switching from installing and running &ceph; via RPM packages to running in
    containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Complete removal of &deepsea; and replacing with &cephsalt; and &cephadm;.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   The upgrade information in this chapter <emphasis>only</emphasis> applies to
   upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
   instructions if you want to deploy &productname; on the &caasp; platform.
  </para>
 </warning>
 <important>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Before Upgrading</title>

  <para>
    The following tasks <emphasis>must</emphasis> be completed before you
    start the upgrade. This can be done at any time during the &productname;
    &prevproductnumber; life time.
  </para>

  <itemizedlist>
    <listitem>
     <para>
      The OSD migration from &filestore; to &bluestore;
      <emphasis>must</emphasis> happen before the upgrade as &filestore;
      unsupported in &productname; &productnumber;. Find more details about
      &bluestore; and how to migrate from &filestore; at
      <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      If you are running an older cluster that still uses
      <literal>ceph-disk</literal> OSDs, you <emphasis>need</emphasis> to
      switch to <literal>ceph-volume</literal> before the upgrade. Find more
      details in
      <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
     </para>
    </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Points to Consider</title>

   <para>
    Before upgrading, ensure you read through the following sections to ensure
    you understand all tasks that need to be executed.
   </para>

   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Read the release notes</emphasis> - there you can find
      additional information on changes since the previous release of
      &productname;. Check the release notes to see whether:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Your hardware needs special considerations.
       </para>
      </listitem>
      <listitem>
       <para>
        Any used software packages have changed significantly.
       </para>
      </listitem>
      <listitem>
       <para>
        Special precautions are necessary for your installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </para>
     <para>
      After having installed the package <package>release-notes-ses</package>,
      find the release notes locally in the directory
      <filename>/usr/share/doc/release-notes</filename> or online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a long time&mdash;approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      You have to upgrade the &smaster; first, and replace &deepsea; with
      &cephsalt; and &cephadm; after. You will <emphasis>not</emphasis> be able
      to start using the &cephadm; orchestrator module until at least all
      &mgr;'s are upgraded.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade from using &prevcephname; RPMs to &cephname; containers needs
      to happen all in one step. This means upgrade an entire node at a time,
      not one daemon at a time.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Backing Up Cluster Configuration and Data</title>
   <para>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to &productname; &productnumber;. For instruction on
    how to back up all your data, see
    <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verifying Steps From the Previous Upgrade</title>
   <para>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </para>
   <para>
    Check for the existence of the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
    file.
   </para>
   <para>
    This file is created by the engulf process during the upgrade from
    &productname; 5 to 6. The <option>configuration_init:
    default-import</option> option is set in
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    If <option>configuration_init</option> is still set to
    <option>default-import</option>, the cluster is using
    <filename>ceph.conf.import</filename> as its configuration file and not
    &deepsea;'s default <filename>ceph.conf</filename> which is compiled from
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Therefore, you need to inspect <filename>ceph.conf.import</filename> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Then remove the <option>configuration_init: default-import</option> line
    from
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Updating Cluster Nodes and Verifying Cluster Health</title>
   <para>
    Verify that all latest updates of &prevcephos; and &productname;
    &prevproductnumber; are applied to all cluster nodes:
   </para>
<screen>&prompt.root;zypper refresh &amp;&amp; zypper patch</screen>
   <para>
    After updates are applied, check the cluster health:
   </para>
<screen>&prompt.cephuser;ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verifying Access to Software Repositories and Container Images</title>
   <para>
    Verify that each cluster node has access to &cephos; and &productname;
    &productnumber; software repositories as well as registry of container
    images.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Software Repositories</title>
    <para>
     If all nodes are registered with SCC, you will be able to use the
     <command>zypper migration</command> command to upgrade. Refer to <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/>
      for more details.
    </para>
    <para>
     If nodes are <emphasis role="bold">not</emphasis> registered with SCC,
     disable all existing software repositories and add both the
     <literal>Pool</literal> and <literal>Updates</literal> repositories for
     each of the following extensions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Container Images</title>
    <para>
     All cluster nodes need access to the container image registry. In most
     cases, you will use the public SUSE registry at
     <literal>registry.suse.com</literal>. You need the following images:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Alternatively&mdash;for example, for air-gapped
     deployments&mdash;configure a local registry and verify that you have the
     correct set of container images available. Refer to <xref
     linkend="deploy-cephadm-configure-registry"/> for more details about
     configuring a local container image registry.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrading the &smaster;</title>

  <para>
   The following procedure describes the process of upgrading the &smaster;:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For cluster whose all nodes are registered with SCC, run <command>zypper
       migrate</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       For cluster whose nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Disable the &deepsea; stages to avoid accidental use. Add the
     following content to
     <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the locally
     configured registry, edit
     <filename>/srv/pillar/ceph/stack/global.yml</filename> to inform &deepsea;
     which &ceph; container image and registry to use. For example, to use
     <literal>192.168.121.1:5000/my/ceph/image</literal> from an insecure
     (HTTP) registry, add the following lines:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
  - insecure: true
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Assimilate existing configuration:
    </para>
<screen>&prompt.cephuser;ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verify the upgrade status. Your output may differ depending on your
     cluster configuration:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrading the MON, MGR, and OSD Nodes</title>

  <para>
   Upgrade the &mon;, &mgr;, and OSD nodes one at a time. For each service,
   follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     If the node you are upgrading is an OSD node, avoid having the OSD marked
     <literal>out</literal> during the upgrade by running the following command:
    </para>
<screen>&prompt.cephuser;ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Replace <replaceable>SHORT_NODE_NAME</replaceable> with the short name of
     the node as it appears in the output of the <command>ceph osd
     tree</command> command.
    In the following input, the short host names are <literal>ses-min1</literal> and <literal>ses-min2</literal>
    </para>
<screen>
&prompt.smaster;ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the cluster's nodes are all registered with SCC, run <command>zypper
       migrate</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       If the cluster's nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     After the node is rebooted, containerize all existing MON, MGR, and OSD
     daemons on that node by running the following command on the &smaster;:
    </para>
<screen>&prompt.smaster;salt <replaceable>HOST_NAME</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Replace <replaceable>HOST_NAME</replaceable> with the host name of the
     node that you are upgrading.
    </para>
    <tip>
     <para>
      To see the status and progress of the <emphasis>adoption</emphasis>,
      check the &dashboard; or run one of the following commands on the
      &smaster;:
     </para>
<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     After the adoption has successfully finished, unset the
     <literal>noout</literal> flag if the node you are upgrading is an OSD
     node:
    </para>
<screen>&prompt.cephuser;ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Installing &cephsalt; and Applying the Cluster Configuration</title>

  <para>
   Before you start the procedure of installing &cephsalt; and applying the cluster
   configuration, check the cluster and upgrade status by running the following
   commands:
  </para>

<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Export cluster configuration from &deepsea;, by running the following
     commands:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.ceph_salt_config > ceph-salt-config.json
&prompt.smaster;salt-run upgrade.generate_service_specs > specs.yaml
</screen>
   </step>
   <step>
    <para>
     Uninstall &deepsea; and install &cephsalt; on the &smaster;:
    </para>
<screen>
&prompt.smaster;zypper remove deepsea
&prompt.smaster;zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Import &deepsea;'s cluster configuration into &cephsalt;:
    </para>
<screen>&prompt.smaster;ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Generate SSH keys for cluster node communication:
    </para>
<screen>&prompt.smaster;ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verify that the cluster configuration was imported from &deepsea; and
      specify potentially missed options:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
     <para>
      For a complete description of cluster configuration, refer to
      <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Apply the configuration and enable &cephadm;:
    </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the locally
     configured registry, inform &ceph; which container image to use by running
    </para>
<screen>&prompt.smaster;ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.smaster;ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Upgrading and Adopting the Monitoring Stack</title>

  <para>
   This following procedure adopts all components of the monitoring stack (see
   <xref linkend="monitoring-alerting"/> for more details).
  </para>

  <procedure>
   <step>
    <para>
     Pause the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch pause</screen>
   </step>
   <step>
    <para>
     On whichever node is running &prometheus;, &grafana; and &alertmanager;
     (the &smaster; by default), run the following commands:
    </para>
<screen>
&prompt.cephuser;cephadm adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      If you are <emphasis role="bold">not</emphasis> running the default
      container image registry <literal>registry.suse.com</literal>, you need
      to specify the image to use, for example:
     </para>
<screen>
&prompt.cephuser;cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-server:2.18.0 \
  adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-alertmanager:0.16.2 \
  adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.0.3 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      For more details on using custom or local container images, refer to
      <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Remove the Node-Exporter. It does not need to be migrated and will be
     re-installed as a container when the <filename>specs.yaml</filename> file is
     applied.
    </para>
<screen>&prompt.sudo;zypper rm golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Apply the service specifications that you previously exported from
     &deepsea;:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i specs.yaml</screen>
   </step>
   <step>
    <para>
     Resume the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Gateway Upgrades</title>
<!-- 2020-10-12 tbazant: no upgrade path yet
  <sect2 xml:id="upgrade-mds">
   <title>Upgrade the &mds;</title>
   <para><remark>TBD</remark></para>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrade the &igw;</title>
   <para><remark>TBD</remark></para>
  </sect2>
-->
  <sect2 xml:id="upgrade-ogw">
   <title>Upgrading the &ogw;</title>
   <para>
    In &productname; &productnumber;, the &ogw;s are always configured with a realm,
    which allows for multi-site (see <xref linkend="ceph-rgw-fed"/> for more details) in the future. If you
    used a single-site &ogw; configuration in &productname;
    &prevproductnumber;, follow these steps to migrate it to multi-site:
   </para>
   <procedure>
    <step>
     <para>
      Create a new realm:
     </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Optionally, rename the default zone and &zgroup;.
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
&prompt.cephuser;radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure the master &zgroup;:
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure the master zone:
     </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Commit the updated configuration:
     </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    To have the &ogw; service containerized, restart it with the &ceph;
    orchestrator:
   </para>
<screen>&prompt.cephuser;ceph orch restart rgw</screen>
  </sect2>
  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrading &ganesha;</title>
   <para>
    The following demonstrates how to migrate an existing &ganesha; service
    running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
   </para>
   <warning>
    <para>
     The following documentation requires you to have already successfully
     upgraded the core &ceph; services.
    </para>
   </warning>
   <para>
    &ganesha; stores additional per-daemon config and export configs in a
    &rados; pool. The configured &rados; pool can be found on the
    <literal>watch_url</literal> line of the <literal>RADOS_URLS</literal>
    block in the <filename>ganesha.conf</filename> file.
   </para>
   <para>
    Before attempting any migration, we strongly recommend to make a copy of
    the export and daemon config objects located in the RADOS pool To locate
    the configured RADOS pool, run the following command:
   </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    To list the contents of the RADOS pool:
   </para>
<screen>&prompt.cephuser;rados --pool cephfs_data --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    To copy the RADOS objects:
   </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    On a per node basis, an existing &ganesha; service needs to be stopped and
    then replaced with a container managed by &cephadm;.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &ganesha; service:
     </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      After the existing &ganesha; service has been stopped, a new one can be
      deployed in a container using &cephadm;. To do so, you need to create a
      service specification that contains a <literal>service_id</literal> that
      will be used to identify this new NFS cluster, the hostname of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
     </para>
     <para>
      Deploy a new &ganesha; container by creating a new placement
      specification. For information on creating a placement specification see
      <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Apply the placement specification:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirm the &ganesha; daemon is running on the host:
     </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
   </procedure>
   <para>
    The existing exports can be migrated in two different ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Manually re-created or re-assigned using the &dashboard;.
     </para>
<!-- depends on https://github.com/ceph/ceph/pull/36948 -->
    </listitem>
    <listitem>
     <para>
      Manually copy the contents of each per-daemon &rados; object into the
      newly created &ganesha; common configuration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manually Copying Exports to &ganesha; Common Configuration File</title>
    <step>
     <para>
      Determine the list of per-daemon RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Make a copy of the per-daemon &rados; objects:
     </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.foo
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sort and merge into a single list of exports:
     </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.foo
&prompt.cephuser;cat conf-nfs.foo
%url "rados://cephfs_data/ganesha/export-1"
%url "rados://cephfs_data/ganesha/export-2"
%url "rados://cephfs_data/ganesha/export-3"
%url "rados://cephfs_data/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Write the new &ganesha; common configuration file:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.foo conf-nfs.foo</screen>
    </step>
    <step>
     <para>
      Notify the &ganesha; daemon:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.foo conf-nfs.foo</screen>
     <note>
      <para>
       This action will cause the daemon to reload the configuration.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    After the service has been successfully migrated, the Nautilus based
    &ganesha; service can be removed.
   </para>
   <procedure>
    <step>
     <para>
      Remove &ganesha;:
     </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remove the legacy cluster settings from the &dashboard;:
     </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard set-ganesha-clusters-rados-pool-namespace cluster2:pool2/ns2
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
<!-- 2020-10-12 tbazant: need description etc
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Post-upgrade Clean-up</title>

<screen>
  # ceph versions
  # ceph osd require-osd-release octopus
  # ceph mgr module enable pg_autoscaler
  # ceph mgr module enable telemetry
  # ceph telemetry on
  # ceph osd set require-min-compat-client luminous
  # ceph balancer mode upmap
  # ceph balancer on
  [...]
</screen>
 </sect1>
-->
</chapter>
