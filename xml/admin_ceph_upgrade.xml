<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade from a previous release</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;.
 </para>
 <para>
  The upgrade includes the following tasks:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrading from &ceph; &prevcephname; to &cephname;.
   </para>
  </listitem>
  <listitem>
   <para>
    Switching from installing and running &ceph; via RPM packages to running in
    containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Complete removal of &deepsea; and replacing with &cephsalt; and &cephadm;.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   The upgrade information in this chapter <emphasis>only</emphasis> applies to
   upgrades from &deepsea; to &cephadm;. Do not attempt to follow these
   instructions if you want to deploy &productname; on the &caasp; platform.
  </para>
 </warning>
 <important>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Before upgrading</title>

  <para>
   The following tasks <emphasis>must</emphasis> be completed before you start
   the upgrade. This can be done at any time during the &productname;
   &prevproductnumber; life time.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The OSD migration from &filestore; to &bluestore;
     <emphasis>must</emphasis> happen before the upgrade as &filestore;
     unsupported in &productname; &productnumber;. Find more details about
     &bluestore; and how to migrate from &filestore; at
     <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     If you are running an older cluster that still uses
     <literal>ceph-disk</literal> OSDs, you <emphasis>need</emphasis> to switch
     to <literal>ceph-volume</literal> before the upgrade. Find more details in
     <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Points to consider</title>
   <para>
    Before upgrading, ensure you read through the following sections to ensure
    you understand all tasks that need to be executed.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Read the release notes</emphasis> - there you can find
      additional information on changes since the previous release of
      &productname;. Check the release notes to see whether:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Your hardware needs special considerations.
       </para>
      </listitem>
      <listitem>
       <para>
        Any used software packages have changed significantly.
       </para>
      </listitem>
      <listitem>
       <para>
        Special precautions are necessary for your installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      The release notes also provide information that could not make it into
      the manual on time. They also contain notes about known issues.
     </para>
     <para>
      You can find SES &productnumber; release notes online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Additionally, after having installed the package
      <package>release-notes-ses</package> from the SES &productnumber;
      repository, find the release notes locally in the directory
      <filename>/usr/share/doc/release-notes</filename> or online at
      <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Read <xref linkend="deploy-cephadm"/> to familiarise yourself with
      &cephsalt; and the &ceph; orchestrator, and in particular the information
      on service specifications.
     </para>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a long time&mdash;approximately the time it
      takes to upgrade one machine multiplied by the number of cluster nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      You have to upgrade the &smaster; first, and replace &deepsea; with
      &cephsalt; and &cephadm; after. You will <emphasis>not</emphasis> be able
      to start using the &cephadm; orchestrator module until at least all
      &mgr;'s are upgraded.
     </para>
    </listitem>
    <listitem>
     <para>
      The upgrade from using &prevcephname; RPMs to &cephname; containers needs
      to happen all in one step. This means upgrade an entire node at a time,
      not one daemon at a time.
     </para>
    </listitem>
    <listitem>
     <para>
      Upgrade of core services (MON, MGR, OSD) happens in an orderly fashion. 
      Each service is available during the upgrade. The gateway services (&mds;,
      &ogw;, &ganesha;, &igw;) need to be redeployed after the core services are
      upgraded. There is a certain amount of downtime for each of the following services:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        &mds;s and &ogw;s are down from the time the nodes are upgraded
        from &prevcephos; to &cephos; until the services are redeployed at
        the end of the upgrade procedure. This is particularly important to
        bear in mind if these services are colocated with MONs, MGRs or OSDs,
        because in this case they may be down for the entire duration of the
        cluster upgrade. If this is going to be a problem, consider deploying
        these services separately on additional nodes before upgrading, so that
        they are down for the least amount of time possible. This is the
        duration of the upgrade of the gateway nodes, not the duration of the
        upgrade of the entire cluster.
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha; and &igw;s are down only while nodes are rebooting during
        upgrade from &prevcephos; to &cephos; and again briefly when each
        service is redeployed in containerized mode.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Backing Up cluster configuration and data</title>
   <para>
    We strongly recommend backing up all cluster configuration and data before
    starting your upgrade to &productname; &productnumber;. For instruction on
    how to back up all your data, see
    <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verifying steps from the previous upgrade</title>
   <para>
    In case you previously upgraded from version 5, verify that the upgrade to
    version 6 was completed successfully:
   </para>
   <para>
    Check for the existence of the
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>
    file.
   </para>
   <para>
    This file is created by the engulf process during the upgrade from
    &productname; 5 to 6. The <option>configuration_init:
    default-import</option> option is set in
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    If <option>configuration_init</option> is still set to
    <option>default-import</option>, the cluster is using
    <filename>ceph.conf.import</filename> as its configuration file and not
    &deepsea;'s default <filename>ceph.conf</filename> which is compiled from
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Therefore, you need to inspect <filename>ceph.conf.import</filename> for
    any custom configuration, and possibly move the configuration to one of the
    files in
    <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Then remove the <option>configuration_init: default-import</option> line
    from
    <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Updating cluster nodes and verifying cluster health</title>
   <para>
    Verify that all latest updates of &prevcephos; and &productname;
    &prevproductnumber; are applied to all cluster nodes:
   </para>
<screen>&prompt.root;zypper refresh &amp;&amp; zypper patch</screen>
   <para>
    After updates are applied, check the cluster health:
   </para>
<screen>&prompt.cephuser;ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verifying access to software repositories and container images</title>
   <para>
    Verify that each cluster node has access to &cephos; and &productname;
    &productnumber; software repositories as well as registry of container
    images.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Software repositories</title>
    <para>
     If all nodes are registered with SCC, you will be able to use the
     <command>zypper migration</command> command to upgrade. Refer to
     <link
      xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/>
     for more details.
    </para>
    <para>
     If nodes are <emphasis role="bold">not</emphasis> registered with SCC,
     disable all existing software repositories and add both the
     <literal>Pool</literal> and <literal>Updates</literal> repositories for
     each of the following extensions:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Container images</title>
    <para>
     All cluster nodes need access to the container image registry. In most
     cases, you will use the public SUSE registry at
     <literal>registry.suse.com</literal>. You need the following images:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Alternatively&mdash;for example, for air-gapped
     deployments&mdash;configure a local registry and verify that you have the
     correct set of container images available. Refer to
     <xref
     linkend="deploy-cephadm-configure-registry"/> for more details
     about configuring a local container image registry.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrading the &smaster;</title>

  <para>
   The following procedure describes the process of upgrading the &smaster;:
  </para>

  <procedure>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       For cluster whose all nodes are registered with SCC, run <command>zypper
       migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       For cluster whose nodes have software repositories assigned manually,
       run <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Disable the &deepsea; stages to avoid accidental use. Add the following
     content to <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the locally
     configured registry, edit
     <filename>/srv/pillar/ceph/stack/global.yml</filename> to inform &deepsea;
     which &ceph; container image and registry to use. For example, to use
     <literal>192.168.121.1:5000/my/ceph/image</literal> add the following
     lines:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Save the file and apply the changes:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Assimilate existing configuration:
    </para>
<screen>&prompt.cephuser;ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verify the upgrade status. Your output may differ depending on your
     cluster configuration:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrading the MON, MGR, and OSD nodes</title>

  <para>
   Upgrade the &mon;, &mgr;, and OSD nodes one at a time. For each service,
   follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     If the node you are upgrading is an OSD node, avoid having the OSD marked
     <literal>out</literal> during the upgrade by running the following
     command:
    </para>
<screen>&prompt.cephuser;ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Replace <replaceable>SHORT_NODE_NAME</replaceable> with the short name of
     the node as it appears in the output of the <command>ceph osd
     tree</command> command. In the following input, the short host names are
     <literal>ses-min1</literal> and <literal>ses-min2</literal>
    </para>
<screen>
&prompt.smaster;ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Upgrade the underlying OS to &cephos;:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the cluster's nodes are all registered with SCC, run <command>zypper
       migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       If the cluster's nodes have software repositories assigned manually, run
       <command>zypper dup</command> followed by <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     After the node is rebooted, containerize all existing MON, MGR, and OSD
     daemons on that node by running the following command on the &smaster;:
    </para>
<screen>&prompt.smaster;salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Replace <replaceable>MINION_ID</replaceable> with the ID of the minion
     that you are upgrading. You can get the list of minion IDs by running the
     <command>salt-key -L</command> command on the &smaster;.
    </para>
    <tip>
     <para>
      To see the status and progress of the <emphasis>adoption</emphasis>,
      check the &dashboard; or run one of the following commands on the
      &smaster;:
     </para>
<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     After the adoption has successfully finished, unset the
     <literal>noout</literal> flag if the node you are upgrading is an OSD
     node:
    </para>
<screen>&prompt.cephuser;ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Upgrading gateway nodes</title>
  <para>
   Upgrade your separate gateway nodes (&mds;, &ogw;, &ganesha;, or &igw;) next.
   For each of these nodes, upgrade the underlying OS to &cephos;:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     If the cluster's nodes are all registered with &scc;, run <command>zypper
     If the cluster's nodes are all registered with &scc;, run the <command>zypper
     migration</command> command.
    </para>
   </listitem>
   <listitem>
    <para>
     If the cluster's nodes have software repositories assigned manually, run the
     <command>zypper dup</command> followed by the <command>reboot</command> commands.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   This step also applies for any nodes that are part of the
   cluster, but do not yet have any roles assigned (if in doubt, check the list
   of hosts on the &smaster; provided by the <command>salt-key -L</command> command and
   compare it to the output of the <command>salt-run upgrade.status</command> command).
  </para>
  <para>
   Once the OS is upgraded on all nodes in the cluster, the next step is to
   install the <package>ceph-salt</package> package and apply the cluster configuration.
   The actual gateway services are redeployed in a containerized mode at the end
   of the upgrade procedure.
  </para>
  <note>
   <para>
    &mds; and &ogw; services are unavailable from the time of upgrading to
    &cephos; until they are redeployed at the end of the upgrade procedure.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Installing &cephsalt; and applying the cluster configuration</title>

  <para>
   Before you start the procedure of installing &cephsalt; and applying the
   cluster configuration, check the cluster and upgrade status by running the
   following commands:
  </para>

<screen>
&prompt.smaster;ceph status
&prompt.smaster;ceph versions
&prompt.smaster;salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Remove the &deepsea;-created <literal>rbd_exporter</literal> and <literal>rgw_exporter</literal> cron jobs.
     On the &smaster; as the &rootuser; run the <command>crontab -e</command> command to edit the crontab. Delete the following items if present:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh > \
 /var/lib/prometheus/node-exporter/rbd.prom 2> /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py > \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2> /dev/null
</screen>
   </step>
   <step>
    <para>
     Export cluster configuration from &deepsea;, by running the following
     commands:
    </para>
<screen>
&prompt.smaster;salt-run upgrade.ceph_salt_config > ceph-salt-config.json
&prompt.smaster;salt-run upgrade.generate_service_specs > specs.yaml
</screen>
   </step>
   <step>
    <para>
     Uninstall &deepsea; and install &cephsalt; on the &smaster;:
    </para>
<screen>
&prompt.smaster;zypper remove 'deepsea*'
&prompt.smaster;zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Restart the &smaster; and synchronize &salt; modules:
    </para>
<screen>
&prompt.smaster;systemctl restart salt-master.service
&prompt.smaster;salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Import &deepsea;'s cluster configuration into &cephsalt;:
    </para>
<screen>&prompt.smaster;ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Generate SSH keys for cluster node communication:
    </para>
<screen>&prompt.smaster;ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verify that the cluster configuration was imported from &deepsea; and
      specify potentially missed options:
     </para>
<screen>&prompt.smaster;ceph-salt config ls</screen>
     <para>
      For a complete description of cluster configuration, refer to
      <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Apply the configuration and enable &cephadm;:
    </para>
<screen>&prompt.smaster;ceph-salt apply</screen>
   </step>
   <step>
    <para>
     If you need to supply local container registry URL and access credentials,
     follow the steps described in
     <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     If you are <emphasis role="bold">not</emphasis> using container images
     from <literal>registry.suse.com</literal> but rather the locally
     configured registry, inform &ceph; which container image to use by running
    </para>
<screen>&prompt.smaster;ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.smaster;ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Stop and disable the &productname; 6 <systemitem
     class="daemon">ceph-crash</systemitem> daemons. New containerized forms of
     these daemons are started later automatically.
    </para>
<screen>
&prompt.smaster;salt '*' service.stop ceph-crash
&prompt.smaster;salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Upgrading and adopting the monitoring stack</title>

  <para>
   This following procedure adopts all components of the monitoring stack (see
   <xref linkend="monitoring-alerting"/> for more details).
  </para>

  <procedure>
   <step>
    <para>
     Pause the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch pause</screen>
   </step>
   <step>
    <para>
     On whichever node is running &prometheus;, &grafana; and &alertmanager;
     (the &smaster; by default), run the following commands:
    </para>
<screen>
&prompt.cephuser;cephadm adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      If you are <emphasis role="bold">not</emphasis> running the default
      container image registry <literal>registry.suse.com</literal>, you need
      to specify the image to use, for example:
     </para>
<screen>
&prompt.cephuser;cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-server:2.18.0 \
  adopt --style=legacy --name prometheus.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-alertmanager:0.16.2 \
  adopt --style=legacy --name alertmanager.$(hostname)
&prompt.cephuser;cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.0.3 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      For more details on using custom or local container images, refer to
      <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Remove the Node-Exporter. It does not need to be migrated and will be
     re-installed as a container when the <filename>specs.yaml</filename> file
     is applied.
    </para>
<screen>&prompt.sudo;zypper rm golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Apply the service specifications that you previously exported from
     &deepsea;:
    </para>
<screen>&prompt.cephuser;ceph orch apply -i specs.yaml</screen>
   </step>
   <step>
    <para>
     Resume the orchestrator:
    </para>
<screen>&prompt.cephuser;ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Gateway service redeployment</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Upgrading the &ogw;</title>
   <para>
    In &productname; &productnumber;, the &ogw;s are always configured with a
    realm, which allows for multi-site (see <xref linkend="ceph-rgw-fed"/> for
    more details) in the future. If you used a single-site &ogw; configuration
    in &productname; &prevproductnumber;, follow these steps to migrate it to
    multi-site:
   </para>
   <procedure>
    <step>
     <para>
      Create a new realm:
     </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Optionally, rename the default zone and &zgroup;.
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
&prompt.cephuser;radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure the master &zgroup;:
     </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure the master zone:
     </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Commit the updated configuration:
     </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    To have the &ogw; service containerized, create its specification file as
    described in <xref linkend="deploy-cephadm-day2-service-ogw"/>, apply it,
    and restart the service:
   </para>
<screen>
&prompt.cephuser;ceph orch apply -i <replaceable>RGW</replaceable>.yml
&prompt.cephuser;ceph orch restart rgw
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrading &ganesha;</title>
   <para>
    The following demonstrates how to migrate an existing &ganesha; service
    running &ceph; Nautilus to an &ganesha; container running &ceph; Octopus.
   </para>
   <warning>
    <para>
     The following documentation requires you to have already successfully
     upgraded the core &ceph; services.
    </para>
   </warning>
   <para>
    &ganesha; stores additional per-daemon config and export configs in a
    &rados; pool. The configured &rados; pool can be found on the
    <literal>watch_url</literal> line of the <literal>RADOS_URLS</literal>
    block in the <filename>ganesha.conf</filename> file.
   </para>
   <para>
    Before attempting any migration, we strongly recommend to make a copy of
    the export and daemon config objects located in the RADOS pool To locate
    the configured RADOS pool, run the following command:
   </para>
<screen>&prompt.cephuser;grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    To list the contents of the RADOS pool:
   </para>
<screen>&prompt.cephuser;rados --pool cephfs_data --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    To copy the RADOS objects:
   </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;OBJS=$(rados $RADOS_ARGS ls)
&prompt.cephuser;for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    On a per node basis, an existing &ganesha; service needs to be stopped and
    then replaced with a container managed by &cephadm;.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &ganesha; service:
     </para>
<screen>&prompt.cephuser;systemctl stop nfs-ganesha
&prompt.cephuser;systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      After the existing &ganesha; service has been stopped, a new one can be
      deployed in a container using &cephadm;. To do so, you need to create a
      service specification that contains a <literal>service_id</literal> that
      will be used to identify this new NFS cluster, the hostname of the node
      we are migrating listed as a host in the placement specification, and the
      RADOS pool and namespace that contains the configured NFS export objects.
     </para>
     <para>
      Deploy a new &ganesha; container by creating a new placement
      specification. For information on creating a placement specification see
      <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Apply the placement specification:
     </para>
<screen>&prompt.cephuser;ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirm the &ganesha; daemon is running on the host:
     </para>
<screen>&prompt.cephuser;ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
   </procedure>
   <para>
    The existing exports can be migrated in two different ways:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Manually re-created or re-assigned using the &dashboard;.
     </para>
    </listitem>
    <listitem>
     <para>
      Manually copy the contents of each per-daemon &rados; object into the
      newly created &ganesha; common configuration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manually copying exports to &ganesha; common configuration file</title>
    <step>
     <para>
      Determine the list of per-daemon RADOS objects:
     </para>
<screen>&prompt.cephuser;RADOS_ARGS="--pool cephfs_data --namespace ganesha"
&prompt.cephuser;DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Make a copy of the per-daemon &rados; objects:
     </para>
<screen>&prompt.cephuser;for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
&prompt.cephuser;ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.foo
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sort and merge into a single list of exports:
     </para>
<screen>&prompt.cephuser;cat conf-* | sort -u > conf-nfs.foo
&prompt.cephuser;cat conf-nfs.foo
%url "rados://cephfs_data/ganesha/export-1"
%url "rados://cephfs_data/ganesha/export-2"
%url "rados://cephfs_data/ganesha/export-3"
%url "rados://cephfs_data/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Write the new &ganesha; common configuration file:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS put conf-nfs.foo conf-nfs.foo</screen>
    </step>
    <step>
     <para>
      Notify the &ganesha; daemon:
     </para>
<screen>&prompt.cephuser;rados $RADOS_ARGS notify conf-nfs.foo conf-nfs.foo</screen>
     <note>
      <para>
       This action will cause the daemon to reload the configuration.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    After the service has been successfully migrated, the Nautilus based
    &ganesha; service can be removed.
   </para>
   <procedure>
    <step>
     <para>
      Remove &ganesha;:
     </para>
<screen>&prompt.cephuser;zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remove the legacy cluster settings from the &dashboard;:
     </para>
<screen>&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard reset-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard get-ganesha-clusters-rados-pool-namespace
&prompt.cephuser;ceph dashboard set-ganesha-clusters-rados-pool-namespace cluster2:pool2/ns2
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Upgrading the &mds;</title>
   <para>
    Unlike MONs, MGRs and OSDs, &mds; cannot be adopted in-place.
    Instead, you need to redeploy them in containers using the &ceph; orchestrator.
   </para>
   <procedure>
    <step>
     <para>
      Run the <command>ceph fs ls</command> command to obtain the name of your file system, for example:
     </para>
<screen>
&prompt.cephuser;ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Create a new service specification file <filename>mds.yml</filename> as
      described in <xref linkend="deploy-cephadm-day2-service-mds"/> by using
      the file system name as the <option>service_id</option> and specifying
      the hosts that will run the MDS daemons. For example:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Run <command>ceph orch apply -i mds.yml</command> to apply the service
      specification and start the MDS daemons.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrading the &igw;</title>
   <para>
    To upgrade the &igw;, you need to redeploy it in containers using the &ceph;
    orchestrator. If you have multiple &igw;s, you need to redeploy them one-by-one
    to reduce the service downtime.
   </para>
   <procedure>
    <step>
     <para>
      Stop and disable the existing &iscsi; daemons on each &igw; node:
     </para>
<screen>
&prompt.sudo;systemctl stop rbd-target-gw
&prompt.sudo;systemctl disable rbd-target-gw
&prompt.sudo;systemctl stop rbd-target-api
&prompt.sudo;systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Create a service specification for the &igw; as described in <xref
      linkend="deploy-cephadm-day2-service-igw"/>. For this, you need the
      <option>pool</option>, <option>trusted_ip_list</option>, and
      <option>api_*</option> settings from the existing
      <filename>/etc/ceph/iscsi-gateway.cfg</filename> file. If you have SSL
      support enabled (<literal>api_secure = true</literal>), you also need the
      SSL certificate (<filename>/etc/ceph/iscsi-gateway.crt</filename>) and
      key (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      For example, if <filename>/etc/ceph/iscsi-gateway.cfg</filename> contains
      the following:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Then you need to create the following service specification file
      <filename>iscsi.yml</filename>:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       The <option>pool</option>, <option>trusted_ip_list</option>,
       <option>api_port</option>, <option>api_user</option>,
       <option>api_password</option>, <option>api_secure</option> settings are
       identical to the ones from the
       <filename>/etc/ceph/iscsi-gateway.cfg</filename> file. The
       <option>ssl_cert</option> and <option>ssl_key</option> values can be
       copied in from the existing SSL certificate and key files. Verify that
       they are indented correctly and the <emphasis>pipe</emphasis> character
       <literal>|</literal> appears at the end of the
       <literal>ssl_cert:</literal> and <literal>ssl_key:</literal> lines.
      </para>
     </note>
    </step>
    <step>
     <para>
      Run the <command>ceph orch apply -i iscsi.yml</command> command to apply the
      service specification and start the &igw; daemons.
     </para>
    </step>
    <step>
     <para>
      Remove the old <package>ceph-iscsi</package> package from each of the existing iSCSI gateway nodes:
     </para>
<screen>&prompt.cephuser;zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Post-upgrade Clean-up</title>

  <para>
   After the upgrade, perform the following clean-up steps:
  </para>

  <procedure>
   <step>
    <para>
     Verify that the cluster was successfully upgraded by checking the current
     &ceph; version:
    </para>
<screen>&prompt.cephuser;ceph versions</screen>
   </step>
   <step>
    <para>
     Make sure that no old OSDs will join the cluster:
    </para>
<screen>&prompt.cephuser;ceph osd require-osd-release octopus</screen>
   </step>
   <step>
    <para>
     Enable the autoscaler module:
    </para>
<screen>&prompt.cephuser;ceph mgr module enable pg_autoscaler</screen>
    <important>
     <para>
      Pools in SES 6 had the <option>pg_autoscale_mode</option> set to
      <option>warn</option> by default.  This resulted in a warning message in
      case of suboptimal number of PGs, but autoscaling did not actually
      happen.  The default in SES 7 is <option>pg_autoscale_mode</option> set
      to <option>on</option> for new pools, therefore PGs will actually
      autoscale.  The upgrade process will not automatically change the
      <option>pg_autoscale_mode</option> of existing pools. If you want to
      change it to <option>on</option> to get the full benefit of the
      autoscaler, do so by following the instructions in <xref
      linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Find more details in <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Prevent pre-Luminous clients:
    </para>
<screen>&prompt.cephuser;ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Enable the balancer module:
    </para>
<screen>
&prompt.cephuser;ceph balancer mode upmap
&prompt.cephuser;ceph balancer on
</screen>
    <para>
     Find more details in <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Optionally, enable the telemetry module:
    </para>
<screen>
&prompt.cephuser;ceph mgr module enable telemetry
&prompt.cephuser;ceph telemetry on
 </screen>
    <para>
     Find more details in <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
