<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; &prevproductnumber; to
  version &productnumber;. Note that version &prevproductnumber; is basically 5
  with all latest patches applied.
 </para>
 <note>
  <title>Upgrade from Older Releases Not Supported</title>
  <para>
   Upgrading from &productname; versions older than &prevproductnumber; is not
   supported. You first need to upgrade to the latest version of &productname;
   &prevproductnumber; and then follow the steps in this chapter.
  </para>
 </note>
 <sect1 xml:id="upgrade-consider-points">
  <title>Points to Consider before the Upgrade</title>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>Read the release notes</emphasis> - there you can find
     additional information on changes since the previous release of
     &productname;. Check the release notes to see whether:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       your hardware needs special considerations.
      </para>
     </listitem>
     <listitem>
      <para>
       any used software packages have changed significantly.
      </para>
     </listitem>
     <listitem>
      <para>
       special precautions are necessary for your installation.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The release notes also provide information that could not make it into the
     manual on time. They also contain notes about known issues.
    </para>
    <para>
     After having installed the package <package>release-notes-ses</package>,
     find the release notes locally in the directory
     <filename>/usr/share/doc/release-notes</filename> or online at
     <link xlink:href="https://www.suse.com/releasenotes/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     In case you previously upgraded from version 4, verify that the upgrade to
     version 5 was completed successfully:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Check for the existence of the file
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
      <para>
       It is created by the engulf process during the upgrade from SES 4 to 5.
       Also, the <option>configuration_init: default-import</option> option is
       set in the file
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <para>
       If <option>configuration_init</option> is still set to
       <option>default-import</option>, the cluster is using
       <filename>ceph.conf.import</filename> as its configuration file and not
       &deepsea;'s default <filename>ceph.conf</filename> which is compiled
       from files in
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       Therefore you need to inspect <filename>ceph.conf.import</filename> for
       any custom configuration, and possibly move the configuration to one of
       the files in
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       Then remove the <option>configuration_init: default-import</option> line
       from
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <warning>
       <title>Default &deepsea; Configuration</title>
       <para>
        If you <emphasis role="bold">do not</emphasis> merge the configuration
        from <filename>ceph.conf.import</filename> and remove the
        <option>configuration_init: default-import</option> option, any default
        configuration settings we ship as part of &deepsea; (stored in
        <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>)
        will not be applied to the cluster.
       </para>
      </warning>
     </listitem>
     <listitem>
      <para>
       Run the <command>salt-run upgrade.check</command> command to verify that
       the cluster uses the new bucket type <literal>straw2</literal> and that
       the &adm; is not a storage node.
      </para>
     </listitem>
     <listitem>
      <para>
       Check that &ceph; 'jewel' profile is used:
      </para>
<screen>
&prompt.cephuser;ceph osd crush dump | grep profile
</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     In case old RBD kernel clients (older than &prevcephos;) are being used,
     refer to <xref linkend="rbd-old-clients-map" />. We recommend upgrading
     old RBD kernel clients if possible.
    </para>
   </listitem>
   <listitem>
    <para>
     If &oa; is located on the &adm;, it will be unavailable after you upgrade
     the node. The new &dashboard; will not be available until you deploy it by
     using &deepsea;.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster upgrade may take a long time&mdash;approximately the time it
     takes to upgrade one machine multiplied by the number of cluster nodes.
    </para>
   </listitem>
   <listitem>
    <para>
     A single node cannot be upgraded while running the previous &sls; release,
     but needs to be rebooted into the new version's installer. Therefore the
     services that the node provides will be unavailable for some time. The
     core cluster services will still be available&mdash;for example if one MON
     is down during upgrade, there are still at least two active MONs.
     Unfortunately, single instance services, such as a single &igw;, will be
     unavailable.
    </para>
   </listitem>
   <listitem>
    <para>
     Certain types of daemons depend upon others. For example, &ceph; &rgw;s
     depend upon &ceph; MON and OSD daemons. We recommend upgrading in this
     order:
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       &adm;
      </para>
     </listitem>
     <listitem>
      <para>
       &mon;s/&mgr;s
      </para>
     </listitem>
     <listitem>
      <para>
       &mds;s
      </para>
     </listitem>
     <listitem>
      <para>
       &osd;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ogw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &igw;s
      </para>
     </listitem>
     <listitem>
      <para>
       &ganesha;
      </para>
     </listitem>
     <listitem>
      <para>
       &sgw;s
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     If you used &aa; in either 'complain' or 'enforce' mode, you need to set a
     &spillar; variable before upgrading. Because &cephos; ships with &aa; by
     default, &aa; management was integrated into &deepsea; stage 0. The
     default behavior in &productname; &productnumber; is to remove &aa; and
     related profiles. If you want to retain the behavior configured in
     &productname; &prevproductnumber;, verify that one of the following lines
     is present in the <filename>/srv/pillar/ceph/stack/global.yml</filename>
     file before starting the upgrade:
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     or
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     From &productname; &productnumber;, MDS names starting with a digit are no
     longer allowed and MDS daemons will refuse to start. You can check whether
     your daemons have such names either by running the <command>ceph fs
     status</command> command, or by restarting an MDS and checking its logs
     for the following message:
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     If you see the above message, the MDS names will need to be migrated
     before attempting to upgrade to &productname; &productnumber;. &deepsea;
     provides an orchestration to automate such a migration. MDS names starting
     with a digit will be prepended with 'mds.':
    </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <tip>
     <title>Custom Configuration Bound to MDS Names</title>
     <para>
      If you have configuration settings that are bound to MDS names and your
      MDS daemons have names starting with a digit, verify that your
      configuration settings apply to the new names as well (with the 'mds.'
      prefix). Consider the following example section in the
      <filename>/etc/ceph/ceph.conf</filename> file:
     </para>
<screen>
[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
 mds cache memory limit = 1073741824
 mds standby for name = 456-another-mds
</screen>
     <para>
      The <command>ceph.mds.migrate-numerical-names</command> orchestrator will
      change the MDS daemon name '123-my-mds' to 'mds.123-my-mds'. You need to
      adjust the configuration to reflect the new name:
     </para>
<screen>
[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds
</screen>
    </tip>
    <para>
     This will add MDS daemons with the new names before removing the old MDS
     daemons. The number of MDS daemons will double for a short time. Clients
     will be able to access &cephfs; only after a short pause to failover.
     Therefore plan the migration for times when you expect little or no
     &cephfs; load.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-backup">
  <title>Back Up Cluster Data</title>

  <para>
   Although creating backups of a cluster's configuration and data is not
   mandatory, we strongly recommend backing up important configuration files
   and cluster data. Refer to <xref linkend="cha-deployment-backup"/> for more
   details.
  </para>
 </sect1>
 <sect1 xml:id="upgrade-ntp">
  <title>Migrate from <systemitem class="daemon">ntpd</systemitem> to <systemitem class="daemon">chronyd</systemitem></title>

  <para>
   &cephos; no longer uses <systemitem class="daemon">ntpd</systemitem> to
   synchronize the local host time. Instead,
   <systemitem class="daemon">chronyd</systemitem> is used. You need to migrate
   the time synchronization daemon on each cluster node. You can migrate to
   <systemitem>chronyd</systemitem> either
   <emphasis role="bold">before</emphasis> the cluster, or upgrade the cluster
   and migrate to <systemitem class="daemon">chronyd</systemitem>
   <emphasis role="bold">afterward</emphasis>.
  </para>

  <procedure>
   <title>Migrate to <systemitem class="daemon">chronyd</systemitem> <emphasis>before</emphasis> the Cluster Upgrade</title>
   <step>
    <para>
     Install the <package>chrony</package> package:
    </para>
<screen>&prompt.sminion;zypper install chrony</screen>
   </step>
   <step>
    <para>
     Edit the <systemitem class="daemon">chronyd</systemitem> configuration
     file <filename>/etc/chrony.conf</filename> and add NTP sources from the
     current <systemitem class="daemon">ntpd</systemitem> configuration in
     <filename>/etc/ntp.conf</filename>.
    </para>
    <tip>
     <title>More Details on <systemitem class="daemon">chronyd</systemitem> Configuration</title>
     <para>
      Refer to
      <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>
      to find more details about how to include time sources in
      <systemitem class="daemon">chronyd</systemitem> configuration.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Disable and stop the <systemitem class="daemon">ntpd</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     Start and enable the <systemitem class="daemon">chronyd</systemitem>
     service:
    </para>
<screen>&prompt.sminion;systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     Verify the status of chrony:
    </para>
<screen>&prompt.sminion;chronyc tracking</screen>
   </step>
  </procedure>

  <procedure>
   <title>Migrate to <systemitem class="daemon">chronyd</systemitem> <emphasis>after</emphasis> the Cluster Upgrade</title>
   <step>
    <para>
     During cluster upgrade, add the following software repositories:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Upgrade the cluster to version &productnumber;.
    </para>
   </step>
   <step>
    <para>
     Edit the <systemitem class="daemon">chronyd</systemitem> configuration
     file <filename>/etc/chrony.conf</filename> and add NTP sources from the
     current <systemitem class="daemon">ntpd</systemitem> configuration in
     <filename>/etc/ntp.conf</filename>.
    </para>
    <tip>
     <title>More Details on <systemitem class="daemon">chronyd</systemitem> Configuration</title>
     <para>
      Refer to
      <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>
      to find more details about how to include time sources in
      <systemitem class="daemon">chronyd</systemitem> configuration.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Disable and stop the <systemitem class="daemon">ntpd</systemitem> service:
    </para>
<screen>&prompt.sminion;systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     Start and enable the <systemitem class="daemon">chronyd</systemitem>
     service:
    </para>
<screen>&prompt.sminion;systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     Migrate from <systemitem class="daemon">ntpd</systemitem> to
     <systemitem class="daemon">chronyd</systemitem>.
    </para>
   </step>
   <step>
    <para>
     Verify the status of chrony:
    </para>
<screen>&prompt.sminion;chronyc tracking</screen>
   </step>
   <step>
    <para>
     Remove the legacy software repositories that you added to keep
     <systemitem class="daemon">ntpd</systemitem> in the system during the
     upgrade process.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-prepare">
  <title>Patch Cluster Prior to Upgrade</title>

  <para>
   Apply the latest patches to all cluster nodes prior to upgrade.
  </para>

  <sect2 xml:id="upgrade-prepare-repos">
   <title>Required Software Repositories</title>
   <para>
    Check that required repositories are configured on each cluster's node. To
    list all available repositories, run
   </para>
   <tip>
    <title>Upgrade Without Using SCC, SMT, or RMT</title>
    <para>
     If your nodes are not subscribed to one of the supported software channel
     providers that handle automatic channel adjustment&mdash;such as SMT, RMT,
     or SCC&mdash;you may need to enable additional software modules and
     channels.
    </para>
   </tip>
<screen>
&prompt.sminion;zypper lr
</screen>
   <para>
    &productname; &prevproductnumber; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    NFS/SMB Gateway on SLE-HA on &prevcephos; requires:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-staging">
   <title>Repository Staging Systems</title>
   <para>
    If you are using one of the repository staging systems&mdash;SMT, RMT;, or
    &susemgr;&mdash;create a new frozen patch level for the current and the new
    &productname; version.
   </para>
   <para>
    Find more information in:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-smt/#book-smt"/>,
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#book-rmt"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://documentation.suse.com/suma/3.2/"/>,
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-patch">
   <title>Patch the Whole Cluster to the Latest Patches</title>
   <procedure>
    <step>
     <para>
      Apply the latest patches of &productname; &prevproductnumber; and
      &prevcephos; to each &ceph; cluster node. Verify that correct software
      repositories are connected to each cluster node (see
      <xref linkend="upgrade-prepare-repos" />) and run &deepsea; stage 0:
     </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      After stage 0 completes, verify that each cluster node's status includes
      'HEALTH_OK'. If not, resolve the problem before possible reboots in next
      steps.
     </para>
    </step>
    <step>
     <para>
      Run <command>zypper ps</command> to check for processes that may run with
      outdated libraries or binaries, and reboot if there are any.
     </para>
    </step>
    <step>
     <para>
      Verify that the running kernel is the latest available and reboot if not.
      Check outputs of the following commands:
     </para>
<screen>
&prompt.cephuser;uname -a
&prompt.cephuser;rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      Verify that the <package>ceph</package> package is version 12.2.12 or
      newer. Verify that the <package>deepsea</package> package is version
      0.8.9 or newer.
     </para>
    </step>
    <step>
     <para>
      If you previously used any of the <option>bluestore_cache</option>
      settings, they are no longer effective from <package>ceph</package>
      version 12.2.10. The new setting
      <option>bluestore_cache_autotune</option> which is set to 'true' by
      default disables manual cache sizing. To turn on the old behavior, you
      need to set <option>bluestore_cache_autotune=false</option>. Refer to
      <xref linkend="config-auto-cache-sizing" /> for details.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-verify-current">
  <title>Verify the Current Environment</title>

  <itemizedlist>
   <listitem>
    <para>
     If the system has obvious problems, fix them before starting the upgrade.
     Upgrading never fixes existing system problems.
    </para>
   </listitem>
   <listitem>
    <para>
     Check cluster performance. You can use commands such as <command>rados
     bench</command>, <command>ceph tell osd.* bench</command>, or
     <command>iperf3</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify access to gateways (such as &igw; or &ogw;) and &rbd;.
    </para>
   </listitem>
   <listitem>
    <para>
     Document specific parts of the system setup, such as network setup,
     partitioning, or installation details.
    </para>
   </listitem>
   <listitem>
    <para>
     Use <command>supportconfig</command> to collect important system
     information and save it outside cluster nodes. Find more information in
     <link xlink:href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-admsupport-supportconfig"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure there is enough free disk space on each cluster node. Check free
     disk space with <command>df -h</command>. When needed, free disk space by
     removing unneeded files/directories or removing obsolete OS snapshots. If
     there is not enough free disk space, do not continue with the upgrade
     until having freed enough disk space.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-verify-state">
  <title>Check the Cluster's State</title>

  <itemizedlist>
   <listitem>
    <para>
     Check the <command>cluster health</command> command before starting the
     upgrade procedure. Do not start the upgrade unless each cluster node
     reports 'HEALTH_OK'.
    </para>
   </listitem>
   <listitem>
    <para>
     Verify that all services are running:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &smaster; and &smaster; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &mon; and &mgr; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &mds; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &osd; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &rgw; daemons.
      </para>
     </listitem>
     <listitem>
      <para>
       &igw; daemons.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   The following commands provide details of the cluster state and specific
   configuration:
  </para>

  <variablelist>
   <varlistentry>
    <term><command>ceph -s</command></term>
    <listitem>
     <para>
      Prints a brief summary of &ceph; cluster health, running services, data
      usage, and I/O statistics. Verify that it reports 'HEALTH_OK' before
      starting the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph health detail</command></term>
    <listitem>
     <para>
      Prints details if &ceph; cluster health is not OK.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph versions</command></term>
    <listitem>
     <para>
      Prints versions of running &ceph; daemons.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph df</command></term>
    <listitem>
     <para>
      Prints total and free disk space on the cluster. Do not start the upgrade
      if the cluster's free disk space is less than 25% of the total disk
      space.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>salt '*' cephprocesses.check results=true</command></term>
    <listitem>
     <para>
      Prints running &ceph; processes and their PIDs sorted by &sminion;s.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph osd dump | grep ^flags</command></term>
    <listitem>
     <para>
      Verify that 'recovery_deletes' and 'purged_snapdirs' flags are present.
      If not, you can force a scrub on all placement groups by running the
      following command. Be aware that this forced scrub may possibly have a
      negative impact on your &ceph; clients’ performance.
     </para>
<screen>
&prompt.cephuser;ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>Offline Upgrade of CTDB Clusters</title>

  <para>
   CTDB provides a clustered database used by &sgw;s. The CTDB protocol is very
   simple and does not support clusters of nodes communicating with different
   protocol versions. Therefore CTDB nodes need to be taken offline prior to
   performing an upgrade.
  </para>
 </sect1>
 <sect1 xml:id="upgrade-one-node">
  <title>Per Node Upgrade&mdash;Basic Procedure</title>

  <para>
   To ensure the core cluster services are available during the upgrade, you
   need to upgrade the cluster nodes sequentially one by one. There are two
   ways you can perform the upgrade of a node: either using the <emphasis>
   installer DVD</emphasis> or using the <emphasis>distribution migration
   system</emphasis>.
  </para>

  <para>
   After upgrading each node, we recommend running
   <command>rpmconfigcheck</command> to check for any updated configuration
   files that have been edited locally. If the command returns a list of file
   names with a suffix <filename>.rpmnew</filename>,
   <filename>.rpmorig</filename>, or <filename>.rpmsave</filename>, compare
   these files against the current configuration files to ensure that no local
   changes have been lost. If necessary, update the affected files. For more
   information on working with <filename>.rpmnew</filename>,
   <filename>.rpmorig</filename>, and <filename>.rpmsave</filename> files,
   refer to
   <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-rpm-packages-manage"/>.
  </para>

  <tip>
   <title>Orphaned Packages</title>
   <para>
    After a node is upgraded, a number of packages will be in an 'orphaned'
    state without a parent repository. This happens because python3 related
    packages do not make python2 packages obsolete.
   </para>
   <para>
    Find more information about listing orphaned packages in
    <link xlink:href="https://documentation.suse.com/sles/12-SP5/single-html/SLES-admin/#sec-zypper-softup-orphaned"/>.
   </para>
  </tip>

  <sect2 xml:id="upgrade-one-node-manual">
   <title>Manual Node Upgrade Using the Installer DVD</title>
   <procedure>
    <step>
     <para>
      Reboot the node from the &cephos; installer DVD/image.
     </para>
    </step>
    <step>
     <para>
      Select <guimenu>Upgrade</guimenu> from the boot menu.
     </para>
    </step>
    <step>
     <para>
      On the <guimenu>Select the Migration Target</guimenu> screen, verify that
      '&cephos;' is selected and activate the <guimenu>Manually Adjust the
      Repositories for Migration</guimenu> check box.
     </para>
     <figure>
      <title>Select the Migration Target</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the following modules to install:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SUSE Enterprise Storage 6 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Basesystem Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Desktop Applications Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Legacy Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Server Applications Module 15 SP1 x86_64
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      On the <guimenu>Previously Used Repositories</guimenu> screen, verify
      that the correct repositories are selected. If the system is not
      registered with SCC/SMT, you need to add the repositories manually.
     </para>
     <para>
      &productname; &productnumber; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      If you intend to migrate <systemitem>ntpd</systemitem> to
      <systemitem class="daemon">chronyd</systemitem> after SES migration
      (refer to <xref linkend="upgrade-ntp"/>), include the following
      repositories:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      NFS/SMB Gateway on SLE-HA on &cephos; requires:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      Review the <guimenu>Installation Settings</guimenu> and start the
      installation procedure by clicking <guimenu>Update</guimenu>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-one-node-auto">
   <title>Node Upgrade Using the &suse; Distribution Migration System</title>
   <para>
    The <emphasis>Distribution Migration System</emphasis> (DMS) provides an
    upgrade path for an installed &sle; system from one major version to
    another. The following procedure utilizes DMS to upgrade &productname;
    &prevproductnumber; to version &productnumber;, including the underlying
    &prevcephos; to &cephos; migration.
   </para>
   <para>
    Refer to
    <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>
    to find both general and detailed information about DMS.
   </para>
   <procedure>
    <step>
     <para>
      Install the migration RPM packages. They adjust the &grub; boot loader to
      automatically trigger the upgrade on next reboot. Install the
      <package>SLES15-SES-Migration</package> and
      <package>suse-migration-sle15-activation</package> packages:
     </para>
<screen>&prompt.sminion;zypper install SLES15-SES-Migration suse-migration-sle15-activation</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        If the node being upgraded <emphasis role="bold">is</emphasis>
        registered with a repository staging system such as SCC, SMT, RMT, or
        &susemgr;, create the
        <filename>/etc/sle-migration-service.yml</filename> with the following
        content:
       </para>
<screen>
use_zypper_migration: true
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
      </step>
      <step>
       <para>
        If the node being upgraded is <emphasis role="bold">not</emphasis>
        registered with a repository staging system such as SCC, SMT, RMT, or
        &susemgr;, perform the following changes:
       </para>
       <substeps>
        <step>
         <para>
          Create the <filename>/etc/sle-migration-service.yml</filename> with
          the following content:
         </para>
<screen>
use_zypper_migration: false
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
        </step>
        <step>
         <para>
          Disable or remove the SLE 12 SP3 and SES 5 repos, and add the SLE 15
          SP1 and SES6 repos. Find the list of related repositories in
          <xref linkend="upgrade-prepare-repos"/>.
         </para>
        </step>
       </substeps>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Reboot to start the upgrade. While the upgrade is running, you can log in
      to the upgraded node via <command>ssh</command> as the migration user
      using the existing SSH key from the host system as described in
      <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>.
      For &productname;, if you have physical access or direct console access
      to the machine, you can also log in as &rootuser; on the system console
      using the password <literal>sesupgrade</literal>. The node will reboot
      automatically after the upgrade.
     </para>
     <tip>
      <title>Upgrade Failure</title>
      <para>
       If the upgrade fails, inspect
       <filename>/var/log/distro_migration.log</filename>. Fix the problem,
       re-install the migration RPM packages, and reboot the node.
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-adm">
  <title>Upgrade the &adm;</title>

  <itemizedlist>
   <listitem>
    <para>
     The following commands will still work, although &sminion;s are running
     old versions of &ceph; and &salt;: <command>salt '*' test.ping</command>
     and <command>ceph status</command>
    </para>
   </listitem>
   <listitem>
    <para>
     After the upgrade of the &adm;, &oa; will no longer be installed.
    </para>
   </listitem>
   <listitem>
    <para>
     If the &adm; hosted SMT, complete its migration to RMT (refer to
     <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#cha-rmt-migrate"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Status of Cluster Nodes</title>
   <para>
    After the &adm; is upgraded, you can run the <command>salt-run
    upgrade.status</command> command to view useful information about cluster
    nodes. The command lists the &ceph; and OS versions of all nodes, and
    recommends the order in which to upgrade any nodes that are still running
    old versions.
   </para>
<screen>&prompt.smaster;salt-run upgrade.status
The newest installed software versions are:
  ceph: ceph version 14.2.1-468-g994fd9e0cc (994fd9e0ccc50c2f3a55a3b7a3d4e0ba74786d50) nautilus (stable)
  os: SUSE Linux Enterprise Server 15 SP1

Nodes running these software versions:
  admin.ceph (assigned roles: master)
  mon2.ceph (assigned roles: admin, mon, mgr)

Nodes running older software versions must be upgraded in the following order:
   1: mon1.ceph (assigned roles: admin, mon, mgr)
   2: mon3.ceph (assigned roles: admin, mon, mgr)
   3: data1.ceph (assigned roles: storage)
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="upgrade-mons">
  <title>Upgrade &mon;/&mgr; Nodes</title>

  <itemizedlist>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">does not use</emphasis> MDS roles,
     upgrade MON/MGR nodes one by one.
    </para>
   </listitem>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">uses</emphasis> MDS roles, and
     MON/MGR and MDS roles are co-located, you need to shrink the MDS cluster
     and then upgrade the co-located nodes. Refer to
     <xref linkend="upgrade-mds"/> for more details.
    </para>
   </listitem>
   <listitem>
    <para>
     If your cluster <emphasis role="bold">uses</emphasis> MDS roles and they
     run on <emphasis role="bold">dedicated</emphasis> servers, upgrade all
     MON/MGR nodes one by one, then shrink the MDS cluster and upgrade it.
     Refer to <xref linkend="upgrade-mds"/> for more details.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>&mon; Upgrade</title>
   <para>
    Due to a limitation in the &mon; design, once two MONs have been upgraded
    to &productname; &productnumber; and have formed a quorum, the third MON
    (while still on &productname; &prevproductnumber;) will not rejoin the MON
    cluster if it restarted for any reason, including a node reboot. Therefore,
    when two MONs have been upgraded it is best to upgrade the rest as soon as
    possible.
   </para>
  </note>

  <para>
   <emphasis role="bold">Use the procedure described in
   <xref linkend="upgrade-one-node"/>.</emphasis>
  </para>
 </sect1>
 <sect1 xml:id="upgrade-mds">
  <title>Upgrade &mds;s</title>

  <para>
   You need to shrink the &mds; (MDS) cluster. Because of incompatible features
   between the &productname; &prevproductnumber; and &productnumber; versions,
   the older MDS daemons will shut down as soon as they see a single SES
   &productnumber; level MDS join the cluster. Therefore it is necessary to
   shrink the MDS cluster to a single active MDS (and no standbys) for the
   duration of the MDS node upgrades. As soon as the second node is upgraded,
   you can extend the MDS cluster again.
  </para>

  <tip>
   <para>
    On a heavily loaded MDS cluster, you may need to reduce the load (for
    example by stopping clients) so that a single active MDS is able to handle
    the workload.
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     Note the current value of the <option>max_mds</option> option:
    </para>
<screen>
&prompt.cephuser;ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     Shrink the MDS cluster if you have more then 1 active MDS daemon, i.e.
     <option>max_mds</option> is &gt; 1. To shrink the MDS cluster, run
    </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     where <replaceable>FS_NAME</replaceable> is the name of your &cephfs;
     instance ('cephfs' by default).
    </para>
   </step>
   <step>
    <para>
     Find the node hosting one of the standby MDS daemons. Consult the output
     of the <command>ceph fs status</command> command and start the upgrade of
     the MDS cluster on this node.
    </para>
<screen>
&prompt.cephuser;ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     In this example, you need to start the upgrade procedure either on node
     'mon3-6' or 'mon2-6'.
    </para>
   </step>
   <step>
    <para>
     Upgrade the node with the standby MDS daemon. After the upgraded MDS node
     starts, the outdated MDS daemons will shut down automatically. At this
     point, clients may experience a short downtime of the &cephfs; service.
    </para>
    <para>
     <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </step>
   <step>
    <para>
     Upgrade the remaining MDS nodes.
    </para>
   </step>
   <step>
    <para>
     Reset <option>max_mds</option> to the desired configuration:
    </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-main-osd">
  <title>Upgrade &osd;s</title>

  <para>
   For each storage node, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Identify which OSD daemons are running on a particular node:
    </para>
<screen>
&prompt.cephuser;ceph osd tree
</screen>
   </step>
   <step>
    <para>
     Set the 'noout' flag for each OSD daemon on the node that is being
     upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</screen>
    <para>
     Verify with:
    </para>
<screen>&prompt.cephuser;ceph health detail | grep noout</screen>
    <para>
     or
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</screen>
   </step>
   <step>
    <para>
     Create <filename>/etc/ceph/osd/*.json</filename> files for all existing
     OSDs by running the following command on the node that is going to be
     upgraded:
    </para>
<screen>
&prompt.cephuser.osd;ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     Upgrade the OSD node. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </step>
   <step>
    <para>
     Activate all OSDs found in the system:
    </para>
<screen>
&prompt.cephuser.osd;ceph-volume simple activate --all
</screen>
    <tip>
     <title>Activating Data Partitions Individually</title>
     <para>
      If you want to activate data partitions individually, you need to find
      the correct <command>ceph-volume</command> command for each partition to
      activate it. Replace <replaceable>X1</replaceable> with the partition's
      correct letter/number:
     </para>
<screen>
 &prompt.cephuser.osd;ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      For example:
     </para>
<screen>
&prompt.cephuser.osd;ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      The last line of the output contains the command to activate the
      partition:
     </para>
<screen>
&prompt.cephuser.osd;ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--> All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--> Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     Verify that the OSD node will start properly after the reboot.
    </para>
   </step>
   <step>
    <para>
     Address the 'Legacy BlueStore stats reporting detected on XX OSD(s)'
     message:
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
    <para>
     The warning is normal when upgrading &ceph; to 14.2.2. You can disable it
     by setting:
    </para>
<screen>bluestore_warn_on_legacy_statfs = false</screen>
    <para>
     The proper fix is to run the following command on all OSDs while they are
     stopped:
    </para>
<screen>&prompt.cephuser.osd;ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</screen>
    <para>
     Following is a helper script that runs the <command>ceph-bluestore-tool
     repair</command> for all OSDs on the <replaceable>NODE_NAME</replaceable>
     node:
    </para>
<screen>&prompt.cephuser;OSDNODE=<replaceable>OSD_NODE_NAME</replaceable>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE* cmd.run "systemctl stop ceph-osd@$OSD";\
 salt $OSDNODE* cmd.run "ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD";\
 salt $OSDNODE* cmd.run "systemctl start ceph-osd@$OSD";\
 done</screen>
   </step>
   <step>
    <para>
     Unset the 'noout' flag for each OSD daemon on the node that is upgraded:
    </para>
<screen>
&prompt.cephuser;ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</screen>
    <para>
     Verify with:
    </para>
<screen>&prompt.cephuser;ceph health detail | grep noout</screen>
    <para>
     Note:
    </para>
<screen>&prompt.cephuser;ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
   </step>
   <step>
    <para>
     Verify the cluster status. It will be similar to the following output:
    </para>
<screen>
&prompt.cephuser;ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
   </step>
   <step>
    <para>
     Verify that all OSD nodes were rebooted and that OSDs started
     automatically after the reboot.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="filestore2bluestore">
  <title>OSD Migration to &bluestore;</title>

  <para>
   OSD &bluestore; is a new back-end for the OSD daemons. It is the default
   option since &productname; 5. Compared to &filestore;, which stores objects
   as files in an XFS file system, &bluestore; can deliver increased
   performance because it stores objects directly on the underlying block
   device. &bluestore; also enables other features, such as built-in
   compression and EC overwrites, that are unavailable with &filestore;.
  </para>

  <para>
   Specifically for &bluestore;, an OSD has a 'wal' (Write Ahead Log) device
   and a 'db' (RocksDB database) device. The RocksDB database holds the
   metadata for a &bluestore; OSD. These two devices will reside on the same
   device as an OSD by default, but either can be placed on different, for
   example faster, media.
  </para>

  <para>
   In &productname; 5, both &filestore; and &bluestore; are supported and it is
   possible for &filestore; and &bluestore; OSDs to co-exist in a single
   cluster. During the &productname; upgrade procedure, &filestore; OSDs are
   not automatically converted to &bluestore;. Be aware that the
   &bluestore;-specific features will not be available on OSDs that have not
   been migrated to &bluestore;.
  </para>

  <para>
   Before converting to &bluestore;, the OSDs need to be running &productname;
   5. The conversion is a slow process as all data gets re-written twice.
   Though the migration process can take a long time to complete, there is no
   cluster outage and all clients can continue accessing the cluster during
   this period. However, do expect lower performance for the duration of the
   migration. This is caused by rebalancing and backfilling of cluster data.
  </para>

  <para>
   Use the following procedure to migrate &filestore; OSDs to &bluestore;:
  </para>

  <tip>
   <title>Turn Off Safety Measures</title>
   <para>
    &salt; commands needed for running the migration are blocked by safety
    measures. In order to turn these precautions off, run the following
    command:
   </para>
<screen>
 &prompt.smaster;salt-run disengage.safety
 </screen>
   <para>
    Rebuild the nodes before continuing:
   </para>
<screen>
 &prompt.smaster; salt-run rebuild.node <replaceable>TARGET</replaceable>
 </screen>
   <para>
    You can also choose to rebuild each node individually. For example:
   </para>
<screen>
&prompt.smaster; salt-run rebuild.node data1.ceph
 </screen>
   <para>
    The <literal>rebuild.node</literal> always removes and recreates all OSDs
    on the node.
   </para>
   <important>
    <para>
     If one OSD fails to convert, re-running the rebuild destroys the already
     converted &bluestore; OSDs. Instead of re-running the rebuild, you can
     run:
    </para>
<screen>
&prompt.smaster;salt-run disks.deploy <replaceable>TARGET</replaceable>
 </screen>
   </important>
  </tip>

  <para>
   After the migration to &bluestore;, the object count will remain the same
   and disk usage will be nearly the same.
  </para>
 </sect1>
 <sect1 xml:id="upgrade-appnodes-order">
  <title>Upgrade Application Nodes</title>

  <para>
   Upgrade application nodes in the following order:
  </para>

  <orderedlist>
   <listitem>
    <para>
     &ogw;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If the &ogw;s are fronted by a load balancer, then a rolling upgrade of
       the &ogw;s should be possible without an outage.
      </para>
     </listitem>
     <listitem>
      <para>
       Validate that the &ogw; daemons are running after each upgrade, and test
       with S3/&swift; client.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Use the procedure described in
       <xref linkend="upgrade-one-node"/>.</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &igw;s
    </para>
    <itemizedlist>
     <listitem>
      <para>
       If &iscsi; initiators are configured with multipath, then a rolling
       upgrade of the &igw;s should be possible without an outage.
      </para>
     </listitem>
     <listitem>
      <para>
       Validate that the <systemitem class="daemon">lrbd</systemitem> daemon is
       running after each upgrade, and test with initiator.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">Use the procedure described in
       <xref linkend="upgrade-one-node"/>.</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     &ganesha;. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     &sgw;s. <emphasis role="bold">Use the procedure described in
     <xref linkend="upgrade-one-node"/>.</emphasis>
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="upgrade-main-policy">
  <title>Update <filename>policy.cfg</filename> and Deploy &dashboard; Using &deepsea;</title>

  <para>
   On the &adm;, edit
   <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and apply the
   following changes:
  </para>

  <important>
   <title>No New Services</title>
   <para>
    During cluster upgrade, do not add new services to the
    <filename>policy.cfg</filename> file. Change the cluster architecture only
    after the upgrade is completed.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Remove <literal>role-openattic</literal>.
    </para>
   </step>
   <step>
    <para>
     Add <literal>role-prometheus</literal> and <literal>role-grafana</literal>
     to the node that had &prometheus; and &grafana; installed, usually the
     &adm;.
    </para>
   </step>
   <step>
    <para>
     Role <literal>profile-<replaceable>PROFILE_NAME</replaceable></literal> is
     now ignored. Add new corresponding role, <literal>role-storage</literal>
     line. For example, for existing
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     add
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     Synchronize all &salt; modules:
    </para>
<screen>&prompt.smaster;salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     Update the &spillar; by running &deepsea; stage 1 and stage 2:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Clean up &oa;:
    </para>
<screen>&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
&prompt.smaster;salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic</screen>
   </step>
   <step>
    <para>
     Unset the 'restart_igw' grain to prevent stage 0 from restarting &igw;,
     which is not installed yet:
    </para>
<screen>&smaster;salt '*' grains.delkey restart_igw</screen>
   </step>
   <step>
    <para>
     Finally, run through &deepsea; stages 0-4:
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4</screen>
    <tip>
     <title>'subvolume missing' Errors during Stage 3</title>
     <para>
      &deepsea; stage 3 may fail with an error similar to the following:
     </para>
<screen>subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</screen>
     <para>
      In this case, you need to edit
      <filename role="bold">/srv/pillar/ceph/stack/global.yml</filename> and
      add the following line:
     </para>
<screen>subvolume_init: disabled</screen>
     <para>
      Then refresh the &spillar; and re-run &deepsea; stage.3:
     </para>
<screen>&prompt.smaster;salt '*' saltutil.refresh_pillar
 &prompt.smaster;salt-run state.orch ceph.stage.3</screen>
     <para>
      After &deepsea; successfully finished stage.3, the &dashboard; will be
      running. Refer to <xref linkend="ceph-dashboard"/> for a detailed
      overview of &dashboard; features.
     </para>
     <para>
      To list nodes running dashboard, run:
     </para>
<screen>&prompt.cephuser;ceph mgr services | grep dashboard</screen>
     <para>
      To list admin credentials, run:
     </para>
<screen>&prompt.smaster;salt-call grains.get dashboard_creds</screen>
    </tip>
   </step>
   <step>
    <para>
     Sequentially restart the &ogw; services to use 'beast' Web server instead
     of the outdated 'civetweb':
    </para>
<screen>&prompt.smaster;salt-run state.orch ceph.restart.rgw.force</screen>
   </step>
   <step>
    <para>
     Before you continue, we strongly recommend enabling the &ceph; telemetry
     module. For more information, see <xref linkend="mgr-modules-telemetry"/>
     for information and instructions.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-drive-groups">
  <title>Migration from Profile-based Deployments to &drvgrps;</title>

  <para>
   In &productname; &prevproductnumber;, &deepsea; offered so called 'profiles'
   to describe the layout of your OSDs. Starting with &productname;
   &productnumber;, we moved to a different approach called
   <emphasis>&drvgrps;</emphasis> (find more details in
   <xref linkend="ds-drive-groups" />).
  </para>

  <note>
   <para>
    Migrating to the new approach is not immediately mandatory. Destructive
    operations, such as <command>salt-run osd.remove</command>,
    <command>salt-run osd.replace</command>, or <command>salt-run
    osd.purge</command> are still available. However, adding new OSDs will
    require your action.
   </para>
  </note>

  <para>
   Because of the different approach of these implementations, we do not offer
   an automated migration path. However, we offer a variety of
   tools&mdash;&salt; runners&mdash;to make the migration as simple as
   possible.
  </para>

  <sect2>
   <title>Analyze the Current Layout</title>
   <para>
    To view information about the currently deployed OSDs, use the following
    command:
   </para>
<screen>
&prompt.smaster;salt-run disks.discover
</screen>
   <para>
    Alternatively, you can inspect the content of the files in the
    <filename>/srv/pillar/ceph/proposals/profile-*/</filename> directories.
    They have a similar structure to the following:
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>Create &drvgrps; Matching the Current Layout</title>
   <para>
    Refer to <xref linkend="ds-drive-groups-specs" /> for more details on
    &drvgrps; specification.
   </para>
   <para>
    The difference between a fresh deployment and upgrade scenario is that the
    drives to be migrated are already 'used'. Because
   </para>
<screen>
&prompt.smaster;salt-run disks.list
</screen>
   <para>
    looks for unused disks only, use
   </para>
<screen>
&prompt.smaster;salt-run disks.list include_unavailable=True
</screen>
   <para>
    Adjust &drvgrps; until you match your current setup. For a more visual
    representation of what will be happening, use the following command. Note
    that it has no output if there are no free disks:
   </para>
<screen>
&prompt.smaster;salt-run disks.report bypass_pillar=True
</screen>
   <para>
    If you verified that your &drvgrps; are properly configured and want to
    apply the new approach, remove the files from the
    <filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename>
    directory, remove the corresponding
    <literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal>
    lines from the <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>
    file, and run &deepsea; stage 2 to refresh the &spillar;.
   </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
   <para>
    Verify the result by running the following commands:
   </para>
<screen>
&prompt.smaster;salt target_node pillar.get ceph:storage
&prompt.smaster;salt-run disks.report
</screen>
   <warning>
    <title>Incorrect &drvgrps; Configuration</title>
    <para>
     If your &drvgrps; are not properly configured and there are spare disks in
     your setup, they will be deployed in the way you specified them. We
     recommend running:
    </para>
<screen>
&prompt.smaster;salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2 xml:id="upgrade-osd-deployment">
   <title>OSD Deployment</title>
   <para>
    For simple cases such as stand-alone OSDs, the migration will happen over
    time. Whenever you remove or replace an OSD from the cluster, it will be
    replaced by a new, LVM-based OSD.
   </para>
   <tip>
    <title>Migrate to LVM Format</title>
    <para>
     Whenever a single 'legacy' OSD needs to be replaced on a node, all OSDs
     that share devices with it need to be migrated to the LVM-based format.
    </para>
    <para>
     For completeness, consider migrating OSDs on the whole node.
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>More Complex Setups</title>
   <para>
    If you have a more sophisticated setup than just stand-alone OSDs, for
    example dedicated WAL/DBs or encrypted OSDs, the migration can only happen
    when all OSDs assigned to that WAL/DB device are removed. This is due to
    the <command>ceph-volume</command> command that creates Logical Volumes on
    disks before deployment. This prevents the user from mixing partition based
    deployments with LV based deployments. In such cases it is best to manually
    remove all OSDs that are assigned to a WAL/DB device and re-deploy them
    using the &drvgrps; approach.
   </para>
  </sect2>
 </sect1>
</chapter>
