<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Upgrading from Previous Releases</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces steps to upgrade &productname; from the previous
  release(s) to version &productnumber;.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Read the Release Notes</title>

  <para>
   In the release notes you can find additional information on changes since
   the previous release of &productname;. Check the release notes to see
   whether:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     your hardware needs special considerations.
    </para>
   </listitem>
   <listitem>
    <para>
     any used software packages have changed significantly.
    </para>
   </listitem>
   <listitem>
    <para>
     special precautions are necessary for your installation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The release notes also provide information that could not make it into the
   manual on time. They also contain notes about known issues.
  </para>

  <para>
   After having installed the package <package>release-notes-ses</package>,
   find the release notes locally in the directory
   <filename>/usr/share/doc/release-notes</filename> or online at
   <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>General Upgrade Procedure</title>

  <para>
   Consider the following items before starting the upgrade procedure:
  </para>

  <variablelist>
   <varlistentry xml:id="upgrade.order">
    <term>Upgrade Order</term>
    <listitem>
     <para>
      Before upgrading the &ceph; cluster, you need to have both the underlying
      &sls; and &productname; correctly registered against SCC or SMT. You can
      upgrade daemons in your cluster while the cluster is online and in
      service. Certain types of daemons depend upon others. For example &ceph;
      &rgw;s depend upon &ceph; monitors and &ceph; OSD daemons. We recommend
      upgrading in this order:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        &smaster;
       </para>
      </listitem>
      <listitem>
       <para>
        &mon;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mgr;s
       </para>
      </listitem>
      <listitem>
       <para>
        &osd;s
       </para>
      </listitem>
      <listitem>
       <para>
        &mds;s
       </para>
      </listitem>
      <listitem>
       <para>
        &rgw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &igw;s
       </para>
      </listitem>
      <listitem>
       <para>
        &ganesha;
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Delete Unnecessary Operating System Snapshots</term>
    <listitem>
     <para>
      Remove not needed file system snapshots on the operating system
      partitions of nodes. This ensures that there is enough free disk space
      during the upgrade.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Check Cluster Health</term>
    <listitem>
     <para>
      Check the &ceph; cluster health before starting the upgrade procedure.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Upgrade One by One</term>
    <listitem>
     <para>
      Upgrade all the daemons of a specific type&mdash;for example all monitor
      daemons or all OSD daemons&mdash;one by one to ensure that they are all
      on the same release version. Also, upgrade all the daemons in your
      cluster before trying to exercise new functionality in a release.
     </para>
     <para>
      After all the daemons of a specific type are upgraded, check their
      status.
     </para>
     <para>
      Ensure that each monitor has rejoined the quorum after all monitors are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph mon stat</screen>
     <para>
      Ensure each &osd; daemon has rejoined the cluster after all OSDs are
      upgraded:
     </para>
<screen>&prompt.cephuser;ceph osd stat</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.upgrade.prev2curr">
  <title>Upgrade from &productname; &prevproductnumber; to &productnumber;</title>

  <para>
   This section provides details on upgrading &productname; from version
   &prevproductnumber; to &productnumber;.
  </para>

  <sect2 xml:id="upgrade.software_requirements">
   <title>Software Requirements</title>
   <para>
    You need to have the following software installed and updated to the latest
    package versions on all the &ceph; nodes you want to upgrade before you can
    start with the upgrade procedure:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &sls; &prevcephos;
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; &prevproductnumber;
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.consider_points">
   <title>Points to Consider before the Upgrade</title>
   <itemizedlist>
    <listitem>
     <para>
      Because the migration of the underlying &prevcephos; to &cephos; cannot
      be done online, each node needs to be stopped for the duration of the OS
      upgrade. Therefore the services that the node provides will be
      unavailable for some time. The core cluster services will still be
      available&mdash;for example if one MON is down during upgrade, there are
      still at least two active MONs. Unfortunately, single instance services
      such as &igw;, &ogw; or &ganesha; will be unavailable.
     </para>
    </listitem>
    <listitem>
     <para>
      If &oa; is located on the &smaster;, it will be unavailable after you
      upgrade the node. The new &dashboard; will not be available until you
      deploy it by using &deepsea;.
     </para>
    </listitem>
    <listitem>
     <para>
      Before upgrading storage nodes, set the 'noout' flag which prevents
      &ceph; from rebalancing data during downtime and therefore avoids
      unnecessary data transfers:
     </para>
<screen>
&prompt.cephuser;ceph osd set noout
</screen>
    </listitem>
    <listitem>
     <para>
      The cluster upgrade may take a very long time&mdash;approximately the
      time it takes to upgrade one machine multiplied by the number of cluster
      nodes.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade.one_node">
   <title>Single Node Upgrade</title>
   <para>
    You need to upgrade the cluster nodes one by one manually.
<!-- There are two ways you can perform the upgrade of a node: either using the installer DVD, or using the upgrade RPM package. -->
   </para>
   <sect3 xml:id="upgrade.one_node.manual">
    <title>Manual Node Upgrade using the Installer DVD</title>
    <procedure>
     <step>
      <para>
       Reboot the node from the &cephos; installer DVD/image.
      </para>
     </step>
     <step>
      <para>
       Select <guimenu>Upgrade</guimenu> from the boot menu.
      </para>
     </step>
     <step>
      <para>
       On the <guimenu>Previously Used Repositories</guimenu> screen, verify
       that the correct repositories are selected. If the system is not
       registered with SCC/SMT, you need to add the repositories manually. The
       list of corresponding repositories looks as follows:
      </para>
<screen>
SLE-Product-SLES/15-SP1
SLE-Module-Basesystem/15-SP1
SLE-Module-Server-Applications/15-SP1
Storage/6
</screen>
     </step>
     <step>
      <para>
       Review the <guimenu>Installation Settings</guimenu> and start the
       installation procedure by clicking <guimenu>Update</guimenu>.
      </para>
     </step>
    </procedure>
   </sect3>
<!--
   <sect3 xml:id="upgrade.one_node.auto">
    <title>Automated Node Upgrade using the <emphasis>SLE15 Migration</emphasis> Package</title>
    <procedure>
     <step>
      <para>
       Disable or remove all existing software repositories:
      </para>
<screen>
&prompt.root;zypper mr -d -a
</screen>
     </step>
     <step>
      <para>
       Manually add the following software repositories:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         SLE-Product-SLES/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Basesystem/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         SLE-Module-Server-Applications/15-SP1
        </para>
       </listitem>
       <listitem>
        <para>
         Storage/6
        </para>
       </listitem>
      </itemizedlist>
      <para>
       You can add them by finding out the right repository URL and running:
      </para>
<screen>
zypper ar -f <replaceable>REPO_URL</replaceable>
</screen>
      <para>
       for each required repository. Refresh them with <command>zypper
       ref</command>.
      </para>
     </step>
     <step>
      <para>
       Install the migration RPM packages. They adjust the &grub; boot loader
       to automatically trigger the upgrade on next reboot. Download the
       <package>SLES15-Migration</package> package from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/images/x86_64/"/>
       and the <package>suse-migration-activation</package> from
       <link xlink:href="http://download.suse.de/ibs/home:/tserong/SLE_12_SP3/noarch/"/>.
      </para>
     </step>
     <step>
      <para>
       Reboot. The migration system will start automatically and upgrade the
       packages by using the <command>zypper dup</command> command. The node
       will reboot automatically after the upgrade.
      </para>
      <tip>
       <title>Upgrade Failure</title>
       <para>
        If the upgrade fails, inspect
        <filename>/var/log/distro_migration.log</filename>. Then fix the
        problem, re-install the migration RPM packages, and reboot the node.
       </para>
      </tip>
     </step>
    </procedure>
   </sect3>
   -->
  </sect2>

  <sect2 xml:id="upgrade.procedure">
   <title>The Upgrade Procedure</title>
   <para>
    To upgrade the &productname; &prevproductnumber; cluster to version
    &productnumber;, follow instructions and procedures in this section.
   </para>
   <para>
    First upgrade the &smaster; (described in
    <xref linkend="upgrade.one_node" />). After the upgrade is complete, &oa;
    will no longer be installed. The following commands will still work,
    although &sminion;s are running old version of &ceph; and &salt;:
   </para>
<screen>
&prompt.cephuser;salt '*' test.ping
&prompt.cephuser;ceph status
</screen>
   <sect3>
    <title>Upgrade &mon;s</title>
    <procedure>
     <step>
      <para>
       Upgrade one &mon; node (described in
       <xref linkend="upgrade.one_node" />. The cluster will be running with
       mixed &ceph; versions.
      </para>
     </step>
     <step>
      <para>
       Upgrade the second &mon;.
      </para>
      <note>
       <title>Out of Quorum</title>
       <para>
        In your cluster contains exactly three &mon;s, during the second &mon;
        upgrade the two remaining &mon;s (the already upgraded one running
        &ceph; Nautilus, while the old one still running &ceph; Luminous) will
        be out of quorum. To fix it, run the following command on the &mon;
        node running &ceph; Luminous:
       </para>
<screen>
&prompt.cephuser;ceph daemon mon.<replaceable>LUMINOUS_MON_HOST_NAME</replaceable> quorum enter
</screen>
      </note>
     </step>
     <step>
      <para>
       Upgrade the rest of &mon;s, one by one.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Upgrade &osd;s</title>
    <tip>
     <para>
      Before upgrading the storage nodes, do not forget to run:
     </para>
<screen>
&prompt.cephuser;ceph osd set noout
</screen>
    </tip>
    <para>
     For each storage node, follow these steps:
    </para>
    <procedure>
     <step>
      <para>
       Upgrade the OSD node (described in <xref linkend="upgrade.one_node" />.
      </para>
     </step>
     <step>
      <para>
       For each OSD's data partition, you need to find the correct
       <command>ceph-volume</command> command to activate it. Replace
       <replaceable>X1</replaceable> with the partition's correct
       letter/number:
      </para>
<screen>
 &prompt.cephuser;ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
      <para>
       For example:
      </para>
<screen>
&prompt.cephuser;ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     </step>
     <step>
      <para>
       The last line of the output contains the command to activate the
       partition:
      </para>
<screen>
&prompt.cephuser;ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--> All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--> Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     </step>
    </procedure>
    <tip>
     <para>
      After all OSD nodes are upgraded and their data partitions activated,
      run:
     </para>
<screen>
&prompt.cephuser;ceph osd unset noout
</screen>
    </tip>
    <para>
     Verify the cluster status. It will be similar to the following output:
    </para>
<screen>
&prompt.cephuser;ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
    <tip>
     <title>Check for the Version of Cluster Components/Nodes</title>
     <para>
      When you need to find out the versions of individual cluster components
      and nodes&mdash;for example to find out if all your nodes are actually on
      the same patch level after the upgrade&mdash;you can run
     </para>
<screen>&prompt.smaster;salt-run status.report</screen>
     <para>
      The command goes through the connected &sminion;s and scans for the
      version numbers of &ceph;, &salt;, and &sls;, and gives you a report
      displaying the version that the majority of nodes have and showing nodes
      whose version is different from the majority.
     </para>
    </tip>
   </sect3>
   <sect3>
    <title>Update <filename>policy.cfg</filename> and Deploy &dashboard; using &deepsea;</title>
    <procedure>
     <step>
      <para>
       On the &smaster;, edit
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> and apply the
       following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Remove <literal>role-openattic</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Add <literal>role-storage</literal> for each storage node.
        </para>
       </listitem>
       <listitem>
        <para>
         Add <literal>role-prometheus</literal> and
         <literal>role-grafana</literal> to the &smaster; node.
        </para>
       </listitem>
       <listitem>
        <para>
         Remove obsolete <literal>profile-*</literal> lines.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Synchronize &deepsea; modules:
      </para>
<screen>
&prompt.smaster;salt '*' saltutil.sync_all
</screen>
     </step>
     <step>
      <para>
       Run &deepsea; stage 2 to update the roles:
      </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
</screen>
     </step>
     <step>
      <para>
       Cleanup &oa;:
      </para>
<screen>
&prompt.smaster;salt '*' state.apply ceph.rescind.openattic
&prompt.smaster;salt -I 'roles:master' state.apply ceph.remove.openattic
</screen>
     </step>
     <step>
      <para>
       Finally, run through &deepsea; stages 0-3:
      </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.0
&prompt.smaster;salt-run state.orch ceph.stage.1
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
</screen>
     </step>
    </procedure>
    <para>
     After &deepsea; successfully finished Stage 3, the &dashboard; will be
     running. Refer to <xref linkend="ceph.dashboard" /> for a detailed
     overview of &dashboard; features.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
