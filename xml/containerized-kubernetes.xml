<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0"
 xml:id="cha-container-kubernetes">
<!-- ============================================================== -->
 <title>&productname; &productnumber; on Top of &casp; 4 &kube; Cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <warning>
  <title>Technology Preview</title>
  <para>
   Running containerized &ceph; cluster on &casp; is a technology preview. Do
   not deploy on a production &kube; cluster. This is not a supported version.
  </para>
 </warning>
 <para>
  This chapter describes how to deploy containerized &productname;
  &productnumber; on top of &casp; 4 &kube; cluster.
 </para>
 <sect1 xml:id="kube-consider">
  <title>Considerations</title>
  <para>
   Before you start deploying, consider the following points:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     To run &ceph; in &kube;, &productname; &productnumber; uses an upstream
     project called &rook; (<link xlink:href="https://rook.io/"/>).
    </para>
   </listitem>
   <listitem>
    <para>
     Depending on the configuration, &rook; may consume
     <emphasis>all</emphasis> unused disks on all nodes in a &kube; cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     The setup requires privileged containers.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="kube-prereq">
  <title>Prerequisites</title>
  <para>
   The minimum requirements and prerequisites to deploy &productname; &productnumber;
   on top of &casp; 4 &kube; cluster are as follows:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     A running &casp; 4 cluster. You need to have an account with a
     &casp; subscription. You can activate a 60-day free evaluation
     here <link xlink:href="https://www.suse.com/products/caas-platform/download/MkpwEt3Ub98~/?campaign_name=Eval:_CaaSP_4"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     At least 3 &casp; worker nodes with a number of extra disks attached as
     storage for the &ceph; cluster. We recommend 4 &casp; worker nodes.
    </para>
   </listitem>
   <listitem>
     <para>
       At least one OSD per worker node, with a minimum disk size of 5&nbsp;GB.
     </para>
   </listitem>
   <listitem>
     <para>
       Access to &productname; &productnumber;. You can get a trial
       subscription from here <link xlink:href="https://www.suse.com/products/suse-enterprise-storage/download/"/>.
     </para>
   </listitem>
   <listitem>
     <para>
       Access to a workstation that has access via <literal>kubectl</literal> to the
       &casp; cluster. We recommend using the &casp; master node as the workstation.
     </para>
   </listitem>
   <listitem>
      <para>
        Ensure that the <literal>SUSE-Enterprise-Storage-6-Pool</literal> and
        <literal>SUSE-Enterprise-Storage-6-Updates</literal> repositories are configured on
        the management node to install the <package>rook-k8s-yaml</package> RPM package.
      </para>
    </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="kube-rook-manifests">
  <title>Get &rook; Manifests</title>

  <para>
   The &rook; orchestrator uses configuration files in YAML format called
   <emphasis>manifests</emphasis>. The manifests you need are included in the
   <package>rook-k8s-yaml</package> RPM package. You can find this package
   in the &productname; &productnumber; repository. Install it by running
   the following:
  </para>
<screen>&prompt.root;zypper install rook-k8s-yaml</screen>
 </sect1>
 <sect1 xml:id="kube-install">
  <title>Installation</title>
  <para>
   &rookceph; includes two main components: the 'operator' which is run by
   &kube; and allows creation of &ceph; clusters, and the &ceph; 'cluster'
   itself which is created and partially managed by the operator.
  </para>

  <sect2 xml:id="kube-configure">
   <title>Configuration</title>
   <sect3 xml:id="kube-configure-global">
    <title>Global Configuration</title>
    <para>
     The manifests used in this setup install all &rook; and &ceph; components
     in the 'rook-ceph' namespace. If you need to change it, adopt all
     references to the namespace in the &kube; manifests accordingly.
    </para>
    <para>
     Depending on which features of &rook; you intend to use, alter the 'Pod
     Security Policy' configuration in <filename>common.yaml</filename> to
     limit &rook;'s security requirements. Follow the comments in the manifest
     file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-operator">
    <title>Operator Configuration</title>
    <para>
     The manifest <filename>operator.yaml</filename> configures the &rook;
     operator. Normally, you do not need to change it. Find more information
     following the comments in the manifest file.
    </para>
   </sect3>
   <sect3 xml:id="kube-config-ceph">
    <title>&ceph; Cluster Configuration</title>
    <para>
     The manifest <filename>cluster.yaml</filename> is responsible for
     configuring the actual &ceph; cluster which will run in &kube;. Find
     detailed description of all available options in the upstream &rook;
     documentation at
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html"/>.
    </para>
    <para>
     By default, &rook; is configured to use all nodes that are not tainted
     with <option>node-role.kubernetes.io/master:NoSchedule</option> and will
     obey configured placement settings (see
     <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-cluster-crd.html#placement-configuration-settings"/>).
     The following example disables such behavior and only uses the nodes
     explicitly listed in the nodes section:
    </para>
<screen>
storage:
  useAllNodes: false
  nodes:
    - name: caasp4-worker-0
    - name: caasp4-worker-1
    - name: caasp4-worker-2
</screen>
    <note>
     <para>
      By default, &rook; is configured to use all free and empty disks on each
      node for use as &ceph; storage.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="kube-config-docs">
    <title>Documentation</title>
    <itemizedlist>
     <listitem>
      <para>
       The &rookceph; upstream documentation at
       <link xlink:href="https://rook.github.io/docs/rook/master/ceph-storage.html"/>
       contains more detailed information about configuring more advanced
       deployments. Use it as a reference for understanding the basics of
       &rookceph; before doing more advanced configurations.
      </para>
     </listitem>
     <listitem>
      <para>
       Find more details about the &casp; product at
       <link xlink:href="https://documentation.suse.com/suse-caasp/4.0/"/>.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="kube-config-rook-operator">
   <title>Create the &rook; Operator</title>
   <para>
    Install the &rookceph; common components, CSI roles, and the &rookceph;
    operator by executing the following command on the &casp; master node:
   </para>
<screen>&prompt.root;kubectl apply -f common.yaml -f operator.yaml</screen>
   <para>
    <filename>common.yaml</filename> will create the 'rook-ceph' namespace,
    &ceph; Custom Resource Definitions (CRDs) (see
    <link xlink:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"/>)
    to make &kube; aware of &ceph; Objects (for example, 'CephCluster'), and the
    RBAC roles and Pod Security Policies (see
    <link xlink:href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/"/>)
    which are necessary for allowing &rook; to manage the cluster-specific
    resources.
   </para>
   <tip>
    <title><option>hostNetwork</option> and <option>hostPorts</option> Usage</title>
    <para>
     Allowing the usage of <option>hostNetwork</option> is required when using
     <option>hostNetwork: true</option> in the Cluster Resource Definition.
     Allowing the usage of <option>hostPorts</option> in the
     <literal>PodSecurityPolicy</literal> is also required.
    </para>
   </tip>
   <para>
    Verify the installation by running <command>kubectl get pods -n
    rook-ceph</command> on &casp; master node, for example:
   </para>
<screen>&prompt.root;kubectl get pods -n rook-ceph
NAME                                     READY   STATUS      RESTARTS   AGE
rook-ceph-agent-57c9j                    1/1     Running     0          22h
rook-ceph-agent-b9j4x                    1/1     Running     0          22h
rook-ceph-operator-cf6fb96-lhbj7         1/1     Running     0          22h
rook-discover-mb8gv                      1/1     Running     0          22h
rook-discover-tztz4                      1/1     Running     0          22h</screen>
  </sect2>

  <sect2 xml:id="kube-create-ceph-cluster">
   <title>Create the &ceph; Cluster</title>
   <para>
    After you modify <filename>cluster.yaml</filename> according to your needs,
    you can create the &ceph; cluster. Run the following command on the &casp;
    master node:
   </para>
<screen>&prompt.root;kubectl apply -f cluster.yaml</screen>
   <para>
    Watch the 'rook-ceph' namespace to see the &ceph; cluster being created.
    You will see as many &mon;s as configured in the
    <filename>cluster.yaml</filename> manifest (default is 3), one &mgr;, and
    as many &osd;s as you have free disks.
   </para>
   <tip>
    <title>Temporary OSD Pods</title>
    <para>
     While bootstrapping the &ceph; cluster, you will see some pods with the
     name
     <literal>rook-ceph-osd-prepare-<replaceable>NODE-NAME</replaceable></literal>
     run for a while and then terminate with the status 'Completed'. As their
     name implies, these pods provision &osd;s. They are left without being
     deleted so that you can inspect their logs after their termination. For
     example:
    </para>
<screen>&prompt.root;kubectl get pods --namespace rook-ceph
NAME                                         READY  STATUS     RESTARTS  AGE
rook-ceph-agent-57c9j                        1/1    Running    0         22h
rook-ceph-agent-b9j4x                        1/1    Running    0         22h
rook-ceph-mgr-a-6d48564b84-k7dft             1/1    Running    0         22h
rook-ceph-mon-a-cc44b479-5qvdb               1/1    Running    0         22h
rook-ceph-mon-b-6c6565ff48-gm9wz             1/1    Running    0         22h
rook-ceph-operator-cf6fb96-lhbj7             1/1    Running    0         22h
rook-ceph-osd-0-57bf997cbd-4wspg             1/1    Running    0         22h
rook-ceph-osd-1-54cf468bf8-z8jhp             1/1    Running    0         22h
rook-ceph-osd-prepare-caasp4-worker-0-f2tmw  0/2    Completed  0         9m35s
rook-ceph-osd-prepare-caasp4-worker-1-qsfhz  0/2    Completed  0         9m33s
rook-ceph-tools-76c7d559b6-64rkw             1/1    Running    0         22h
rook-discover-mb8gv                          1/1    Running    0         22h
rook-discover-tztz4                          1/1    Running    0         22h</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="kube-using-rook">
  <title>Using &rook; as Storage for Kubernetes Workload</title>

  <para>
   &rook; allows you to use three different types of storage:
  </para>

  <variablelist>
   <varlistentry>
    <term><emphasis role="bold">Object Storage</emphasis></term>
    <listitem>
     <para>
      Object storage exposes an S3 API to the storage cluster for applications
      to put and get data. Refer to
      <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-object.html"/> for
      a detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Shared File System</term>
    <listitem>
     <para>
      A shared file system can be mounted with read/write permission from
      multiple pods. This is useful for applications that are clustered using a
      shared file system. Refer to
      <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-filesystem.html"/>
      for a detailed description.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Block Storage</term>
    <listitem>
     <para>
      Block storage allows you to mount storage to a single pod. Refer to
      <link xlink:href="https://rook.io/docs/rook/v1.0/ceph-block.html"/> for a
      detailed description.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="kube-uninstall">
  <title>Uninstalling &rook;</title>

  <para>
   To uninstall &rook;, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Delete any &kube; applications that are consuming &rook; storage.
    </para>
   </step>
   <step>
    <para>
     Delete all object, file, and/or block storage artifacts that you created
     by following <xref linkend="kube-using-rook"/>.
    </para>
   </step>
   <step>
    <para>
     Delete the &ceph; cluster, operator, and related resources:
    </para>
<screen>
&prompt.root;kubectl delete -f cluster.yaml
&prompt.root;kubectl delete -f operator.yaml
&prompt.root;kubectl delete -f common.yaml
</screen>
   </step>
   <step>
    <para>
     Delete the data on hosts:
    </para>
<screen>&prompt.root;rm -rf /var/lib/rook</screen>
   </step>
   <step>
    <para>
     If necessary, wipe the disks that were used by &rook;. Refer to
     <link xlink:href="https://rook.io/docs/rook/master/ceph-teardown.html"/>
     for more details.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
