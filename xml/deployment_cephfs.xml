<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.as.cephfs">
<!-- ============================================================== -->
 <title>Installation of &cephfs;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES5</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that uses
  a &ceph; storage cluster to store its data. &cephfs; uses the same cluster
  system as &ceph; block devices, &ceph; object storage with its S3 and Swift
  APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <sect1 xml:id="ceph.cephfs.limitations">
  <title>Supported &cephfs; Scenarios and Guidance</title>

  <para>
   With &productname;, SUSE introduces official support for many scenarios in
   which the scale-out and distributed component &cephfs; is used. This entry
   describes hard limits and provides guidance for the suggested use cases.
  </para>

  <para>
   A supported &cephfs; deployment must meet these requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A minimum of one &mds;. SUSE recommends to deploy several nodes with the
     MDS role. Only one will be 'active' and the rest will be 'passive'.
     Remember to mention all the MDS nodes in the <command>mount</command>
     command when mounting the &cephfs; from a client.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; snapshots are disabled (default) and not supported in this
     version.
    </para>
   </listitem>
   <listitem>
    <para>
     Clients are SUSE Linux Enterprise Server 12 SP2 or SP3 based, using the
     <literal>cephfs</literal> kernel module driver. The FUSE module is not
     supported.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; supports file layout changes as documented in
     <link
    xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>.
     However, while the file system is mounted by any client, new data pools
     may not be added to an existing &cephfs; file system (<literal>ceph mds
     add_data_pool</literal>). They may only be added while the file system is
     unmounted.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>&ceph; Metadata Server</title>

  <para>
   &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph; block
   devices and &ceph; object storage <emphasis>do not</emphasis> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adding a Metadata Server</title>
   <para>
    You can deploy MDS either during the initial cluster deployment process as
    described in <xref linkend="ceph.install.stack"/>, or add it to an already
    deployed cluster as described in <xref linkend="salt.adding.nodes"/>.
   </para>
   <para>
    After you deploy your MDS, allow the <literal>&ceph; OSD/MDS</literal>
    service in the firewall setting of the server where MDS is deployed. Start
    <literal>yast</literal>, navigate to <menuchoice> <guimenu>Security and
    Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed
    Services</guimenu> </menuchoice> and in the <guimenu>Service to
    Allow</guimenu> drop&ndash;down menu select <guimenu>Ceph
    OSD/MDS</guimenu>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mdf.config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file. For detailed list of MDS
    related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
  <title>&cephfs;</title>

  <para>
   When you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you may create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>. When
    configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When assigning a <literal>role-mds</literal> in the <filename>policy.cfg</filename>,
    the required pools are automatically created. You can manually create
    the pools <literal>cephfs_data</literal> and <literal>cephfs_metadata</literal>
    for manual performance tuning before setting up the &mds;. &deepsea;
    will not create these pools if they already exist.
   </para>
   <para>
    For more information on managing pools, see <xref linkend="ceph.pools"/>.
   </para>
   <para>
    To create the two required pools&mdash;for example 'cephfs_data' and
    'cephfs_metadata'&mdash;with default settings for use with &cephfs;, run
    the following commands:
   </para>
<screen>&prompt.root;ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
&prompt.root;ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    When the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>&prompt.root;ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.root;ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    &cephfs;'s:
   </para>
<screen>&prompt.root;<command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    When the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>&prompt.root;<command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>More Topics</title>
    <para>
     You can find more information of specific tasks&mdash;for example
     mounting, unmounting, and advanced &cephfs; setup&mdash;in
     <xref
     linkend="cha.ceph.cephfs"/>.
    </para>
   </tip>
  </sect2>
 </sect1>
</chapter>
