<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-as-cephfs">
<!-- ============================================================== -->
 <title>Installation of &cephfs;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that uses
  a &ceph; storage cluster to store its data. &cephfs; uses the same cluster
  system as &ceph; block devices, &ceph; object storage with its S3 and Swift
  APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Supported &cephfs; Scenarios and Guidance</title>

  <para>
   With &productname; &productnumber;, SUSE introduces official support for
   many scenarios in which the scale-out and distributed component &cephfs; is
   used. This entry describes hard limits and provides guidance for the
   suggested use cases.
  </para>

  <para>
   A supported &cephfs; deployment must meet these requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Clients are &sls; 12 SP3 or newer, or &sls; 15 or newer, using the
     <literal>cephfs</literal> kernel module driver. The FUSE module is not
     supported.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; quotas are supported in &productname; &productnumber; and can be
     set on any subdirectory of the &ceph; file system. The quota restricts
     either the number of <literal>bytes</literal> or <literal>files</literal>
     stored beneath the specified point in the directory hierarchy. For more
     information, see <xref linkend="cephfs-quotas"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; supports file layout changes as documented in
     <xref linkend="cephfs-layouts"/>. However, while the file system is
     mounted by any client, new data pools may not be added to an existing
     &cephfs; file system (<literal>ceph mds add_data_pool</literal>). They may
     only be added while the file system is unmounted.
    </para>
   </listitem>
   <listitem>
     <para>
       A minimum of one &mds;. &suse; recommends deploying several
       nodes with the MDS role. By default, additional MDS daemons start
       as <literal>standby</literal> daemons, acting as backups for the active
       MDS. Multiple active MDS daemons are also supported
       (refer to section <xref linkend="ceph-cephfs-multimds"/>).
     </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>&ceph; Metadata Server</title>

  <para>
   &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph; block
   devices and &ceph; object storage <emphasis>do not</emphasis> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Adding and Removing a Metadata Server</title>
   <para>
    You can deploy MDS either during the initial cluster deployment process as
    described in DEAD LINK, or add it to an already
    deployed cluster as described in <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    After you deploy your MDS, allow the <literal>&ceph; OSD/MDS</literal>
    service in the firewall setting of the server where MDS is deployed: Start
    <literal>yast</literal>, navigate to <menuchoice> <guimenu>Security and
    Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed
    Services</guimenu> </menuchoice> and in the <guimenu>Service to
    Allow</guimenu> drop&ndash;down menu select <guimenu>Ceph
    OSD/MDS</guimenu>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </para>
   <para>
     You can remove a metadata server in your cluster as described in
     <xref linkend="ex-ds-mignode"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file.
   </para>
   <variablelist>
    <title>&mds; Settings</title>
    <varlistentry>
     <term>mon force standby active</term>
     <listitem>
      <para>
       If set to 'true' (default), monitors force standby-replay to be active.
       Set under <literal>[mon]</literal> or <literal>[global]</literal>
       sections.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache memory limit</option></term>
     <listitem>
      <para>
       The soft memory limit (in bytes) that the MDS will enforce for its
       cache. Administrators should use this instead of the old <option>mds
       cache size</option> setting. Defaults to 1&nbsp;GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option></term>
     <listitem>
      <para>
       The cache reservation (memory or inodes) for the MDS cache to maintain.
       When the MDS begins touching its reservation, it will recall client
       state until its cache size shrinks to restore the reservation. Defaults
       to 0.05.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache size</term>
     <listitem>
      <para>
       The number of inodes to cache. A value of 0 (default) indicates an
       unlimited number. It is recommended to use <option>mds cache memory
       limit</option> to limit the amount of memory the MDS cache uses.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache mid</term>
     <listitem>
      <para>
       The insertion point for new items in the cache LRU (from the top).
       Default is 0.7.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir commit ratio</term>
     <listitem>
      <para>
       The fraction of directory that is dirty before &ceph; commits using a
       full update instead of partial update. Default is 0.5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir max commit size</term>
     <listitem>
      <para>
       The maximum size of a directory update before &ceph; breaks it into
       smaller transactions. Default is 90 MB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds decay halflife</term>
     <listitem>
      <para>
       The half-life of MDS cache temperature. Default is 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon interval</term>
     <listitem>
      <para>
       The frequency in seconds of beacon messages sent to the monitor. Default
       is 4.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon grace</term>
     <listitem>
      <para>
       The interval without beacons before &ceph; declares an MDS laggy and
       possibly replaces it. Default is 15.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds blacklist interval</term>
     <listitem>
      <para>
       The blacklist duration for failed MDSs in the OSD map. This setting
       controls how long failed MDS daemons will stay in the OSD map blacklist.
       It has no effect on how long something is blacklisted when the
       administrator blacklists it manually. For example, the <command>ceph osd
       blacklist add</command> command will still use the default blacklist
       time. Default is 24 * 60.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds reconnect timeout</term>
     <listitem>
      <para>
       The interval in seconds to wait for clients to reconnect during MDS
       restart. Default is 45.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds tick interval</term>
     <listitem>
      <para>
       How frequently the MDS performs internal periodic tasks. Default is 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dirstat min interval</term>
     <listitem>
      <para>
       The minimum interval in seconds to try to avoid propagating recursive
       stats up the tree. Default is 1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds scatter nudge interval</term>
     <listitem>
      <para>
       How quickly dirstat changes propagate up. Default is 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds client prealloc inos</term>
     <listitem>
      <para>
       The number of inode numbers to preallocate per client session. Default
       is 1000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds early reply</term>
     <listitem>
      <para>
       Determines whether the MDS should allow clients to see request results
       before they commit to the journal. Default is 'true'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds use tmap</term>
     <listitem>
      <para>
       Use trivial map for directory updates. Default is 'true'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds default dir hash</term>
     <listitem>
      <para>
       The function to use for hashing files across directory fragments.
       Default is 2 (that is 'rjenkins').
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log skip corrupt events</term>
     <listitem>
      <para>
       Determines whether the MDS should try to skip corrupt journal events
       during journal replay. Default is 'false'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max events</term>
     <listitem>
      <para>
       The maximum events in the journal before we initiate trimming. Set to -1
       (default) to disable limits.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max segments</term>
     <listitem>
      <para>
       The maximum number of segments (objects) in the journal before we
       initiate trimming. Set to -1 to disable limits. Default is 30.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max expiring</term>
     <listitem>
      <para>
       The maximum number of segments to expire in parallels. Default is 20.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log eopen size</term>
     <listitem>
      <para>
       The maximum number of inodes in an EOpen event. Default is 100.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal sample interval</term>
     <listitem>
      <para>
       Determines how frequently to sample directory temperature for
       fragmentation decisions. Default is 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal replicate threshold</term>
     <listitem>
      <para>
       The maximum temperature before &ceph; attempts to replicate metadata to
       other nodes. Default is 8000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal unreplicate threshold</term>
     <listitem>
      <para>
       The minimum temperature before &ceph; stops replicating metadata to
       other nodes. Default is 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split size</term>
     <listitem>
      <para>
       The maximum directory size before the MDS will split a directory
       fragment into smaller bits. Default is 10000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split rd</term>
     <listitem>
      <para>
       The maximum directory read temperature before &ceph; splits a directory
       fragment. Default is 25000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split wr</term>
     <listitem>
      <para>
       The maximum directory write temperature before Ceph splits a directory
       fragment. Default is 10000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split bits</term>
     <listitem>
      <para>
       The number of bits by which to split a directory fragment. Default is 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal merge size</term>
     <listitem>
      <para>
       The minimum directory size before Ceph tries to merge adjacent directory
       fragments. Default is 50.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal interval</term>
     <listitem>
      <para>
       The frequency in seconds of workload exchanges between MDSs. Default is
       10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment interval</term>
     <listitem>
      <para>
       The delay in seconds between a fragment being capable of splitting or
       merging, and execution of the fragmentation change. Default is 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment fast factor</term>
     <listitem>
      <para>
       The ratio by which fragments may exceed the split size before a split is
       executed immediately, skipping the fragment interval. Default is 1.5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment size max</term>
     <listitem>
      <para>
       The maximum size of a fragment before any new entries are rejected with
       ENOSPC. Default is 100000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal idle threshold</term>
     <listitem>
      <para>
       The minimum temperature before &ceph; migrates a subtree back to its
       parent. Default is 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal mode</term>
     <listitem>
      <para>
       The method for calculating MDS load:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         0 = Hybrid.
        </para>
       </listitem>
       <listitem>
        <para>
         1 = Request rate and latency.
        </para>
       </listitem>
       <listitem>
        <para>
         2 = CPU load.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Default is 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min rebalance</term>
     <listitem>
      <para>
       The minimum subtree temperature before &ceph; migrates. Default is 0.1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min start</term>
     <listitem>
      <para>
       The minimum subtree temperature before &ceph; searches a subtree.
       Default is 0.2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need min</term>
     <listitem>
      <para>
       The minimum fraction of target subtree size to accept. Default is 0.8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need max</term>
     <listitem>
      <para>
       The maximum fraction of target subtree size to accept. Default is 1.2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal midchunk</term>
     <listitem>
      <para>
       &ceph; will migrate any subtree that is larger than this fraction of the
       target subtree size. Default is 0.3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal minchunk</term>
     <listitem>
      <para>
       &ceph; will ignore any subtree that is smaller than this fraction of the
       target subtree size. Default is 0.001.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal min</term>
     <listitem>
      <para>
       The minimum number of balancer iterations before &ceph; removes an old
       MDS target from the MDS map. Default is 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal max</term>
     <listitem>
      <para>
       The maximum number of balancer iteration before &ceph; removes an old
       MDS target from the MDS map. Default is 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds replay interval</term>
     <listitem>
      <para>
       The journal poll interval when in standby-replay mode ('hot standby').
       Default is 1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds shutdown check</term>
     <listitem>
      <para>
       The interval for polling the cache during MDS shutdown. Default is 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds thrash fragments</term>
     <listitem>
      <para>
       &ceph; will randomly fragment or merge directories. Default is 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache on map</term>
     <listitem>
      <para>
       &ceph; will dump the MDS cache contents to a file on each MDS map.
       Default is 'false'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache after rejoin</term>
     <listitem>
      <para>
       &ceph; will dump MDS cache contents to a file after rejoining the cache
       during recovery. Default is 'false'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for name</term>
     <listitem>
      <para>
       An MDS daemon will standby for another MDS daemon of the name specified
       in this setting.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for rank</term>
     <listitem>
      <para>
       An MDS daemon will standby for an MDS daemon of this rank. Default is
       -1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby replay</term>
     <listitem>
      <para>
       Determines whether a &ceph; MDS daemon should poll and replay the log of
       an active MDS ('hot standby'). Default is 'false'.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds min caps per client</term>
     <listitem>
      <para>
       Set the minimum number of capabilities a client may hold. Default is
       100.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds max ratio caps per client</term>
     <listitem>
      <para>
       Set the maximum ratio of current caps that may be recalled during MDS
       cache pressure. Default is 0.8.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>&mds; Journaler Settings</title>
    <varlistentry>
     <term>journaler write head interval</term>
     <listitem>
      <para>
       How frequently to update the journal head object. Default is 15.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler prefetch periods</term>
     <listitem>
      <para>
       How many stripe periods to read ahead on journal replay. Default is 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journal prezero periods</term>
     <listitem>
      <para>
       How many stripe periods to zero ahead of write position. Default 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch interval</term>
     <listitem>
      <para>
       Maximum additional latency in seconds we incur artificially. Default is
       0.001.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch max</term>
     <listitem>
      <para>
       Maximum number of bytes by which we will delay flushing. Default is 0.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>&cephfs;</title>

  <para>
   When you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you can create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>. When
    configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When assigning a <literal>role-mds</literal> in the
    <filename>policy.cfg</filename>, the required pools are automatically
    created. You can manually create the pools <literal>cephfs_data</literal>
    and <literal>cephfs_metadata</literal> for manual performance tuning before
    setting up the &mds;. &deepsea; will not create these pools if they already
    exist.
   </para>
   <para>
    For more information on managing pools, see <xref linkend="ceph-pools"/>.
   </para>
   <para>
    To create the two required pools&mdash;for example, 'cephfs_data' and
    'cephfs_metadata'&mdash;with default settings for use with &cephfs;, run
    the following commands:
   </para>
<screen>&prompt.cephuser;ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
&prompt.cephuser;ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    It is possible to use EC pools instead of replicated pools. We recommend to
    only use EC pools for low performance requirements and infrequent random
    access, for example cold storage, backups, archiving. &cephfs; on EC pools
    requires BlueStore to be enabled and the pool must have the
    <literal>allow_ec_overwrite</literal> option set. This option can be set by
    running <command>ceph osd pool set ec_pool allow_ec_overwrites
    true</command>.
   </para>
   <para>
    Erasure coding adds significant overhead to file system operations,
    especially small updates. This overhead is inherent to using erasure coding
    as a fault tolerance mechanism. This penalty is the trade off for
    significantly reduced storage space overhead.
   </para>
   <para>
    When the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>&prompt.cephuser;ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    &cephfs;s:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    When the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>More Topics</title>
    <para>
     You can find more information of specific tasks&mdash;for example
     mounting, unmounting, and advanced &cephfs; setup&mdash;in
     <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>MDS Cluster Size</title>
   <para>
    A &cephfs; instance can be served by multiple active MDS daemons. All
    active MDS daemons that are assigned to a &cephfs; instance will distribute
    the file system's directory tree between themselves, and thus spread the
    load of concurrent clients. In order to add an active MDS daemon to a
    &cephfs; instance, a spare standby is needed. Either start an additional
    daemon or use an existing standby instance.
   </para>
   <para>
    The following command will display the current number of active and passive
    MDS daemons.
   </para>
<screen>&prompt.cephuser;ceph mds stat</screen>
   <para>
    The following command sets the number of active MDSs to two in a file
    system instance.
   </para>
<screen>&prompt.cephuser;ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    In order to shrink the MDS cluster prior to an update, two steps are
    necessary. First, set <option>max_mds</option> so that only one instance
    remains:
   </para>
<screen>&prompt.cephuser;ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    and after that, explicitly deactivate the other active MDS daemons:
   </para>
<screen>&prompt.cephuser;ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    where <replaceable>rank</replaceable> is the number of an active MDS daemon
    of a file system instance, ranging from 0 to <option>max_mds</option>-1.
   </para>
   <para>
    We recommend at least one MDS is left as a standby daemon.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>MDS Cluster and Updates</title>
   <para>
    During &ceph; updates, the feature flags on a file system instance may
    change (usually by adding new features). Incompatible daemons (such as the
    older versions) are not able to function with an incompatible feature set
    and will refuse to start. This means that updating and restarting one
    daemon can cause all other not yet updated daemons to stop and refuse to
    start. For this reason, we recommend shrinking the active MDS cluster to
    size one and stopping all standby daemons before updating &ceph;. The
    manual steps for this update procedure are as follows:
   </para>
   <procedure>
    <step>
     <para>
      Update the &ceph; related packages using <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Shrink the active MDS cluster as described above to one instance and stop
      all standby MDS daemons using their &systemd; units on all other nodes:
     </para>
<screen>&prompt.cephuser.mds;systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Only then restart the single remaining MDS daemon, causing it to restart
      using the updated binary.
     </para>
<screen>&prompt.cephuser.mds;systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Restart all other MDS daemons and reset the desired
      <option>max_mds</option> setting.
     </para>
<screen>&prompt.cephuser.mds;systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    If you use &deepsea;, it will follow this procedure in case the
    <package>ceph</package> package was updated during stages 0 and 4. It is
    possible to perform this procedure while clients have the &cephfs; instance
    mounted and I/O is ongoing. Note however that there will be a very brief
    I/O pause while the active MDS restarts. Clients will recover
    automatically.
   </para>
   <para>
    It is good practice to reduce the I/O load as much as possible before
    updating an MDS cluster. An idle MDS cluster will go through this update
    procedure quicker. Conversely, on a heavily loaded cluster with multiple
    MDS daemons it is essential to reduce the load in advance to prevent a
    single MDS daemon from being overwhelmed by ongoing I/O.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-layouts">
   <title>File Layouts</title>
   <para>
    The layout of a file controls how its contents are mapped to &ceph; &rados;
    objects. You can read and write a file’s layout using <emphasis>virtual
    extended attributes</emphasis> or <emphasis>xattrs</emphasis> for shortly.
   </para>
   <para>
    The name of the layout xattrs depends on whether a file is a regular file
    or a directory. Regular files’ layout xattrs are called
    <literal>ceph.file.layout</literal>, while directories’ layout xattrs are
    called <literal>ceph.dir.layout</literal>. Where examples refer to
    <literal>ceph.file.layout</literal>, substitute the
    <literal>.dir.</literal> part as appropriate when dealing with directories.
   </para>
   <sect3>
    <title>Layout Fields</title>
    <para>
     The following attribute fields are recognized:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        ID or name of a &rados; pool in which a file’s data objects will be
        stored.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pool_namespace</term>
      <listitem>
       <para>
        RADOS namespace within a data pool to which the objects will be
        written. It is empty by default, meaning the default namespace.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_unit</term>
      <listitem>
       <para>
        The size in bytes of a block of data used in the RAID 0 distribution of
        a file. All stripe units for a file have equal size. The last stripe
        unit is typically incomplete&mdash;it represents the data at the end of
        the file as well as the unused 'space' beyond it up to the end of the
        fixed stripe unit size.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_count</term>
      <listitem>
       <para>
        The number of consecutive stripe units that constitute a RAID 0
        'stripe' of file data.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>object_size</term>
      <listitem>
       <para>
        The size in bytes of &rados; objects into which the file data is
        chunked.
       </para>
       <tip>
        <title>Object Sizes</title>
        <para>
         &rados; enforces a configurable limit on object sizes. If you increase
         &cephfs; object sizes beyond that limit, then writes may not succeed.
         The OSD setting is <option>osd_max_object_size</option>, which is
         128&nbsp;MB by default. Very large &rados; objects may prevent smooth
         operation of the cluster, so increasing the object size limit past the
         default is not recommended.
        </para>
       </tip>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Reading Layout with <command>getfattr</command></title>
    <para>
     Use the <command>getfattr</command> command to read the layout information
     of an example file <filename>file</filename> as a single string:
    </para>
<screen>
&prompt.root;touch file
&prompt.root;getfattr -n ceph.file.layout file
# file: file
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=419430
</screen>
    <para>
     Read individual layout fields:
    </para>
<screen>
&prompt.root;getfattr -n ceph.file.layout.pool file
# file: file
ceph.file.layout.pool="cephfs_data"
&prompt.root;getfattr -n ceph.file.layout.stripe_unit file
# file: file
ceph.file.layout.stripe_unit="4194304"
</screen>
    <tip>
     <title>Pool ID or Name</title>
     <para>
      When reading layouts, the pool will usually be indicated by name.
      However, in rare cases when pools have only just been created, the ID may
      be output instead.
     </para>
    </tip>
    <para>
     Directories do not have an explicit layout until it is customized.
     Attempts to read the layout will fail if it has never been modified: this
     indicates that the layout of the next ancestor directory with an explicit
     layout will be used.
    </para>
<screen>
&prompt.root;mkdir dir
&prompt.root;getfattr -n ceph.dir.layout dir
dir: ceph.dir.layout: No such attribute
&prompt.root;setfattr -n ceph.dir.layout.stripe_count -v 2 dir
&prompt.root;getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>Writing Layouts with <command>setfattr</command></title>
    <para>
     Use the <command>setfattr</command> command to modify the layout fields of
     an example file <command>file</command>:
    </para>
<screen>
&prompt.cephuser;ceph osd lspools
0 rbd
1 cephfs_data
2 cephfs_metadata
&prompt.root;setfattr -n ceph.file.layout.stripe_unit -v 1048576 file
&prompt.root;setfattr -n ceph.file.layout.stripe_count -v 8 file
# Setting pool by ID:
&prompt.root;setfattr -n ceph.file.layout.pool -v 1 file
# Setting pool by name:
&prompt.root;setfattr -n ceph.file.layout.pool -v cephfs_data file
</screen>
    <note>
     <title>Empty File</title>
     <para>
      When the layout fields of a file are modified using
      <command>setfattr</command>, this file needs to be empty otherwise an
      error will occur.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Clearing Layouts</title>
    <para>
     If you want to remove an explicit layout from an example directory
     <filename>mydir</filename> and revert back to inheriting the layout of its
     ancestor, run the following:
    </para>
<screen>
&prompt.root;setfattr -x ceph.dir.layout mydir
</screen>
    <para>
     Similarly, if you have set the 'pool_namespace' attribute and wish to
     modify the layout to use the default namespace instead, run:
    </para>
<screen>
# Create a directory and set a namespace on it
&prompt.root;mkdir mydir
&prompt.root;setfattr -n ceph.dir.layout.pool_namespace -v foons mydir
&prompt.root;getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a pool_namespace=foons"

# Clear the namespace from the directory's layout
&prompt.root;setfattr -x ceph.dir.layout.pool_namespace mydir
&prompt.root;getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a"
</screen>
   </sect3>
   <sect3>
    <title>Inheritance of Layouts</title>
    <para>
     Files inherit the layout of their parent directory at creation time.
     However, subsequent changes to the parent directory’s layout do not
     affect children:
    </para>
<screen>
&prompt.root;getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# file1 inherits its parent's layout
&prompt.root;touch dir/file1
&prompt.root;getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# update the layout of the directory before creating a second file
&prompt.root;setfattr -n ceph.dir.layout.stripe_count -v 4 dir
&prompt.root;touch dir/file2

# file1's layout is unchanged
&prompt.root;getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# ...while file2 has the parent directory's new layout
&prompt.root;getfattr -n ceph.file.layout dir/file2
# file: dir/file2
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
    <para>
     Files created as descendants of the directory also inherit its layout if
     the intermediate directories do not have layouts set:
    </para>
<screen>
&prompt.root;getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
&prompt.root;mkdir dir/childdir
&prompt.root;getfattr -n ceph.dir.layout dir/childdir
dir/childdir: ceph.dir.layout: No such attribute
&prompt.root;touch dir/childdir/grandchild
&prompt.root;getfattr -n ceph.file.layout dir/childdir/grandchild
# file: dir/childdir/grandchild
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>Adding a Data Pool to the &mds;</title>
    <para>
     Before you can use a pool with &cephfs;, you need to add it to the &mds;:
    </para>
<screen>
&prompt.cephuser;ceph fs add_data_pool cephfs cephfs_data_ssd
&prompt.cephuser;ceph fs ls  # Pool should now show up
.... data pools: [cephfs_data cephfs_data_ssd ]
</screen>
    <tip>
     <title>cephx Keys</title>
     <para>
      Make sure that your cephx keys allow the client to access this new pool.
     </para>
    </tip>
    <para>
     You can then update the layout on a directory in &cephfs; to use the pool
     you added:
    </para>
<screen>
&prompt.root;mkdir /mnt/cephfs/myssddir
&prompt.root;setfattr -n ceph.dir.layout.pool -v cephfs_data_ssd /mnt/cephfs/myssddir
</screen>
    <para>
     All new files created within that directory will now inherit its layout
     and place their data in your newly added pool. You may notice that the
     number of objects in your primary data pool continues to increase, even if
     files are being created in the pool you newly added. This is normal: the
     file data is stored in the pool specified by the layout, but a small
     amount of metadata is kept in the primary data pool for all files.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
