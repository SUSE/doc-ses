<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-as-cephfs">
<!-- ============================================================== -->
 <title>Installation of &cephfs;</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses5.5/xml/</dm:editurl>
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editing</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that uses
  a &ceph; storage cluster to store its data. &cephfs; uses the same cluster
  system as &ceph; block devices, &ceph; object storage with its S3 and Swift
  APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Supported &cephfs; Scenarios and Guidance</title>

  <para>
   With &productname;, SUSE introduces official support for many scenarios in
   which the scale-out and distributed component &cephfs; is used. This entry
   describes hard limits and provides guidance for the suggested use cases.
  </para>

  <para>
   A supported &cephfs; deployment must meet these requirements:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A minimum of one &mds;. SUSE recommends to deploy several nodes with the
     MDS role. Only one will be 'active' and the rest will be 'passive'.
     Remember to mention all the MDS nodes in the <command>mount</command>
     command when mounting the &cephfs; from a client.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; snapshots are disabled (default) and not supported in this
     version.
    </para>
   </listitem>
   <listitem>
    <para>
     Clients are SUSE Linux Enterprise Server 12 SP2 or SP3 based, using the
     <literal>cephfs</literal> kernel module driver. The FUSE module is not
     supported.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; quotas are not supported in &productname;, as support for quotas
     is implemented in the FUSE client only.
    </para>
   </listitem>
   <listitem>
    <para>
     &cephfs; supports file layout changes as documented in
     <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>.
     However, while the file system is mounted by any client, new data pools
     may not be added to an existing &cephfs; file system (<literal>ceph mds
     add_data_pool</literal>). They may only be added while the file system is
     unmounted.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>&ceph; Metadata Server</title>

  <para>
   &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph; block
   devices and &ceph; object storage <emphasis>do not</emphasis> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Adding a Metadata Server</title>
   <para>
    You can deploy MDS either during the initial cluster deployment process as
    described in <xref linkend="ceph-install-stack"/>, or add it to an already
    deployed cluster as described in <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    After you deploy your MDS, allow the <literal>&ceph; OSD/MDS</literal>
    service in the firewall setting of the server where MDS is deployed: Start
    <literal>yast</literal>, navigate to <menuchoice> <guimenu>Security and
    Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed
    Services</guimenu> </menuchoice> and in the <guimenu>Service to
    Allow</guimenu> drop&ndash;down menu select <guimenu>Ceph
    OSD/MDS</guimenu>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file.
   </para>
   <variablelist>
    <title>MDS Cache Size</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       The soft memory limit (in bytes) that the MDS will enforce for its
       cache. Administrators should use this instead of the old <option>mds
       cache size</option> setting. Defaults to 1GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       The cache reservation (memory or inodes) for the MDS cache to maintain.
       When the MDS begins touching its reservation, it will recall client
       state until its cache size shrinks to restore the reservation. Defaults
       to 0.05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    For a detailed list of MDS related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For a detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>&cephfs;</title>

  <para>
   When you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you can create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>. When
    configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    When assigning a <literal>role-mds</literal> in the
    <filename>policy.cfg</filename>, the required pools are automatically
    created. You can manually create the pools <literal>cephfs_data</literal>
    and <literal>cephfs_metadata</literal> for manual performance tuning before
    setting up the &mds;. &deepsea; will not create these pools if they already
    exist.
   </para>
   <para>
    For more information on managing pools, see <xref linkend="ceph-pools"/>.
   </para>
   <para>
    To create the two required pools&mdash;for example, 'cephfs_data' and
    'cephfs_metadata'&mdash;with default settings for use with &cephfs;, run
    the following commands:
   </para>
<screen>&prompt.cephuser;ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
&prompt.cephuser;ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    It is possible to use EC pools instead of replicated pools. We recommend to
    only use EC pools for low performance requirements and infrequent random
    access, for example cold storage, backups, archiving. &cephfs; on EC pools
    requires BlueStore to be enabled and the pool must have the
    <literal>allow_ec_overwrite</literal> option set. This option can be set by
    running <command>ceph osd pool set ec_pool allow_ec_overwrites
    true</command>.
   </para>
   <para>
    Erasure coding adds significant overhead to file system operations,
    especially small updates. This overhead is inherent to using erasure coding
    as a fault tolerance mechanism. This penalty is the trade off for
    significantly reduced storage space overhead.
   </para>
   <para>
    When the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>&prompt.cephuser;ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>&prompt.cephuser;ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    &cephfs;s:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    When the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>More Topics</title>
    <para>
     You can find more information of specific tasks&mdash;for example
     mounting, unmounting, and advanced &cephfs; setup&mdash;in
     <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>MDS Cluster Size</title>
   <para>
    A &cephfs; instance can be served by multiple active MDS daemons. All
    active MDS daemons that are assigned to a &cephfs; instance will distribute
    the file system's directory tree between themselves, and thus spread the
    load of concurrent clients. In order to add an active MDS daemon to a
    &cephfs; instance, a spare standby is needed. Either start an additional
    daemon or use an existing standby instance.
   </para>
   <para>
    The following command will display the current number of active and passive
    MDS daemons.
   </para>
<screen>&prompt.cephuser;ceph mds stat</screen>
   <para>
    The following command sets the number of active MDS's to two in a file
    system instance.
   </para>
<screen>&prompt.cephuser;ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    In order to shrink the MDS cluster prior to an update, two steps are
    necessary. First set <option>max_mds</option> so that only one instance
    remains:
   </para>
<screen>&prompt.cephuser;ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    and after that explicitly deactivate the other active MDS daemons:
   </para>
<screen>&prompt.cephuser;ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    where <replaceable>rank</replaceable> is the number of an active MDS daemon
    of a file system instance, ranging from 0 to <option>max_mds</option>-1.
    See
    <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>
    for additional information.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>MDS Cluster and Updates</title>
   <para>
    During &ceph; updates, the feature flags on a file system instance may
    change (usually by adding new features). Incompatible daemons (such as the
    older versions) are not able to function with an incompatible feature set
    and will refuse to start. This means that updating and restarting one
    daemon can cause all other not yet updated daemons to stop and refuse to
    start. For this reason we, recommend shrinking the active MDS cluster to
    size one and stopping all standby daemons before updating &ceph;. The
    manual steps for this update procedure are as follows:
   </para>
   <procedure>
    <step>
     <para>
      Update the &ceph; related packages using <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Shrink the active MDS cluster as described above to 1 instance and stop
      all standby MDS daemons using their &systemd; units on all other nodes:
     </para>
<screen>&prompt.root;systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Only then restart the single remaining MDS daemon, causing it to restart
      using the updated binary.
     </para>
<screen>&prompt.root;systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Restart all other MDS daemons and re-set the desired
      <option>max_mds</option> setting.
     </para>
<screen>&prompt.root;systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    If you use &deepsea;, it will follow this procedure in case the
    <package>ceph</package> package was updated during Stages 0 and 4. It is
    possible to perform this procedure while clients have the &cephfs; instance
    mounted and I/O is ongoing. Note however that there will be a very brief
    I/O pause while the active MDS restarts. Clients will recover
    automatically.
   </para>
   <para>
    It is good practice to reduce the I/O load as much as possible before
    updating an MDS cluster. An idle MDS cluster will go through this update
    procedure quicker. Conversely, on a heavily loaded cluster with multiple
    MDS daemons it is essential to reduce the load in advance to prevent a
    single MDS daemon from being overwhelmed by ongoing I/O.
   </para>
  </sect2>
 </sect1>
</chapter>
