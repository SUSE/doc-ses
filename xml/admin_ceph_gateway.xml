<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.gw">
<!-- ============================================================== -->
 <title>&ceph; &rgw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; &rgw; is an object storage interface built on top of
  <systemitem>librgw</systemitem> to provide applications with a RESTful
  gateway to &ceph; Storage Clusters. &ceph; Object Storage supports two
  interfaces:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the Amazon S3 RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset
    of the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  &ceph; Object Storage uses the &ceph; &rgw; daemon
  (<systemitem>radosgw</systemitem>), which uses an embedded HTTP server
  (CivetWeb) for interacting with a &ceph; Storage Cluster. Since it
  provides interfaces compatible with OpenStack Swift and Amazon S3, the
  &ceph; &rgw; has its own user management. &ceph; &rgw; can store data in
  the same &ceph; Storage Cluster used to store data from &ceph; File System
  clients or &ceph; Block Device clients. The S3 and Swift APIs share a
  common name space, so you may write data with one API and retrieve it with
  the other.
 </para>
 <para>
  This section helps you install and manage the &ceph; &rgw; (&rgw;). You
  can either choose to use the <command>ceph-deploy</command> tool, or do
  the installation and management manually.
 </para>
 <important>
  <para>
   Before installing &rgw;, you need to have the &ceph; cluster installed
   first (see <xref linkend="cha.ceph.install"/> for more information).
  </para>
 </important>
 <sect1 xml:id="ceph.rgw.cephdeploy">
  <title>Managing &rgw; with <command>ceph-deploy</command></title>

  <para>
   This section describes how to install and configure &rgw; with
   <command>ceph-deploy</command>.
  </para>

  <sect2 xml:id="ceph.rgw.cephdeploy.install">
   <title>Installation</title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <command>rgw</command> component that helps you manage the &rgw;
    creation and operation.
   </para>
   <important>
    <title>Install &ceph;</title>
    <para>
     Before running <command>ceph-deploy rgw</command> as suggested in the
     following step, make sure that &ceph; together with the object gateway
     package are correctly installed on the node where you want to setup &rgw;:
    </para>
    <screen>ceph-deploy install --rgw <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
</screen>
   </important>
   <para>
    Prepare and activate the nodes in one step. You can specify several
    pairs of
    <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
    to install &rgw; on a required number of nodes.
   </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw create ceph-node1:rgw.gateway1</screen>
   <para>
    You now have a working &rgw; on the specified nodes, and you need to
    give access to a client. For more information, see
    <xref linkend="ceph.rgw.access"/>.
   </para>
  </sect2>

  <sect2>
   <title>Listing &rgw; Installations</title>
   <para>
    To list all &rgw; instances within the &ceph; cluster, run:
   </para>
<screen>ceph-deploy rgw list</screen>
  </sect2>

  <sect2>
   <title>Removing &rgw; from a Node</title>
   <para>
    To remove a &rgw; installation from the node where it was previously
    installed, run:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete  \
  <replaceable>short_hostname</replaceable>:<replaceable>gatewayname</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete ceph-node1:rgw.gateway1</screen>
   <tip>
    <para>
     You need a copy of the local <command>ceph.conf</command> file, in your
     current working directory. If you do not have a copy of it, copy it
     from your cluster.
    </para>
   </tip>
  </sect2>

<!-- ============================================================== -->
 </sect1>
 <sect1 xml:id="ceph.rgw.manual">
  <title>Managing &rgw; Manually</title>

  <para>
   This section describes how to install and configure &rgw; manually.
  </para>

  <sect2>
   <title>Installation</title>
   <procedure>
    <step>
     <para>
      Install &rgw;. The following command installs all required components:
     </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
    </step>
    <step>
     <para>
      Disable automatic start of the Apache server:
     </para>
<screen>sudo systemctl disable apache2.service</screen>
    </step>
    <step>
     <para>
      If the Apache server from the previous &rgw; instance is running, stop
      it and disable the relevant service:
     </para>
<screen>sudo systemctl stop disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.rgw.gateway]
 rgw frontends = "civetweb port=80"</screen>
     <tip>
      <para>
       If you want to configure &rgw;/CivetWeb for use with SSL encryption,
       modify the line accordingly:
      </para>
<screen>rgw frontends = civetweb port=7480s ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Restart the &rgw; service. See <xref linkend="ceph.rgw.operating"/>
      for more information.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ses.rgw.config">
   <title>Configuring &rgw;</title>
   <para>
    Several steps are required to configure a &rgw;.
   </para>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a &ceph; &rgw; requires a running &ceph; Storage Cluster.
     The &ceph; &rgw; is a client of the &ceph; Storage Cluster. As a &ceph;
     Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions in a
       keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the &ceph; Configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a
     &ceph; Storage Cluster. In the following steps, we use an admin node to
     create a keyring. Then, we create a client user name and key. Next, we
     add the key to the &ceph; Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a &ceph; &rgw; user name and key for each instance. As an
       example, we will use the name <systemitem>gateway</systemitem> after
       <systemitem>client.radosgw</systemitem>:
      </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>sudo ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the &ceph; Object
       Gateway with access to the &ceph; Storage Cluster, add the key to
       your &ceph; Storage Cluster. For example:
      </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>sudo scp /etc/ceph/ceph.client.rgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Create Pools (Optional)</title>
    <para>
     &ceph; &rgw;s require &ceph; Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the
     gateway will create the pools automatically. However, ensure that you
     have set an appropriate default number of placement groups per pool in
     the &ceph; configuration file.
    </para>
    <para>
     When configuring a gateway with the default region and zone, the naming
     convention for pools typically uses 'default' for region and zone
     naming, but you can use any naming convention you prefer:
    </para>
<screen>.rgw.root
default.rgw.control
default.rgw.data.root
default.rgw.gc
default.rgw.log
default.rgw.users.uid
default.rgw.users.email
default.rgw.users.keys
default.rgw.meta
default.rgw.users.swift</screen>
    <para>
     To create the pools manually, see
     <xref linkend="ceph.pools.operate.add_pool"/>.
    </para>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to &ceph;</title>
    <para>
     Add the &ceph; &rgw; configuration to the &ceph; Configuration file. The
     &ceph; &rgw; configuration requires you to identify the &ceph; &rgw;
     instance. Then, specify the host name where you installed the &ceph; &rgw;
     daemon, a keyring (for use with cephx), and optionally a log file. For example:
    </para>
<screen>[client.rgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
<tip>
 <title>&rgw; Log File</title>
 <para>
  To override the default &rgw; log file, include the follwing:
 </para>
 <screen>log file = /var/log/radosgw/client.rgw.<replaceable>instance-name</replaceable>.log</screen>
</tip>
    <para>
     The <literal>[client.rgw.*]</literal> portion of the gateway
     instance identifies this portion of the &ceph; configuration file as
     configuring a &ceph; Storage Cluster client where the client type is a
     &ceph; &rgw; (radosgw). The instance name follows. For example:
    </para>
<screen>[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
    <para>
     To use a &ceph; &rgw; with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the &ceph;
     &rgw; DNS name under the <literal>[client.rgw.gateway]</literal>
     section of the &ceph; configuration file:
    </para>
<screen>[client.rgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on
     your client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address
     as the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Redeploy &ceph; Configuration</title>
    <para>
     Use <command>ceph-deploy</command> to push a new copy of the
     configuration to the hosts in your cluster:
    </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default &ceph; &rgw; data
     directory. Create data directories for each instance of a radosgw
     daemon if not already done. The <literal>host</literal> variables in
     the &ceph; configuration file determine which host runs each instance
     of a radosgw daemon. The typical form specifies the radosgw daemon, the
     cluster name and the daemon ID.
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the exemplary ceph.conf settings above, you would execute the
     following:
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your &ceph; Storage Cluster service. Then, start
     up the <systemitem>radosgw</systemitem> service. For more information,
     see <xref linkend="cha.ceph.operating"/> and
     <xref linkend="ceph.rgw.operating"/>.
    </para>
    <para>
     After the service is up and running, you can make an anonymous GET
     request to see if the gateway returns a response. A simple HTTP request
     to the domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Operating the &rgw; Service</title>

  <para>
   &rgw; service is operated with the <command>systemctl</command> command.
   You need to have &rootuser; privileges to operate the &rgw; service. Note
   that <replaceable>gateway_host</replaceable> is the host name of the
   server whose &rgw; instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the &rgw; service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Managing &rgw; Access</title>

  <para>
   You can communicate with &rgw; using either S3- or &swift;-compatible
   interface. Both interfaces require you to create a specific user, and
   install the relevant client software to communicate with the gateway
   using the user's secret key.
  </para>

  <para>
   For an introduction and a few practical examples on &rgw; access, see
   <xref linkend="storage.bp.inst.rgw_client"/>.
  </para>

  <sect2>
   <title>Managing S3 Access</title>
   <para>
    S3 interface is compatible with a large subset of the Amazon S3 RESTful
    API.
   </para>
   <tip>
    <para>
     S3cmd is a command line S3 client. You can find it in the
     <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both &sle;
     and &opensuse; based distributions.
    </para>
   </tip>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3add"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3rm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing User Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Setting Quotas</title>
    <para>
     See <xref linkend="storage.bp.account.s3quota"/>.
    </para>
   </sect3>
  </sect2>

  <sect2>
   <title>Managing &swift; Access</title>
   <para>
    &swift; interface is compatible with a large subset of the &ostack;
    &swift; API.
   </para>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftadd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftrm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.fed">
<!-- https://github.com/theanalyst/ceph/blob/doc/multisite-rgw/doc/radosgw/multisite.rst -->

  <title>Multisite Object Storage Gateways</title>

  <warning>
   <title>Technology Preview</title>
   <para>
    As of &storage; 3, multisite object storage gateways are considered a
    technology preview and are not supported.
   </para>
  </warning>

  <para>
   Starting from &storage; 3, you may configure each &rgw; to participate in
   a federated architecture, working in an active zone configuration while
   allowing for writes to non-master zones.
  </para>

  <sect2 xml:id="ceph.rgw.fed.term">
   <title>Terminology</title>
   <para>
    A description of terms specific to a federated architecture follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>Zone</term>
     <listitem>
      <para>
       A logical grouping of one or more &rgw; instances. There must be one
       zone designated as the <emphasis>master</emphasis> zone in a
       <emphasis>zonegroup</emphasis>, which handles all bucket and user
       creation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup</term>
     <listitem>
      <para>
       A zonegroup consists of multiple zones. There should be a master
       zonegroup that will handle changes to the system configuration.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup map</term>
     <listitem>
      <para>
       A configuration structure that holds the map of the entire system,
       for example which zonegroup is the master, relationships between
       different zonegroups, and certain configuration options such as
       storage policies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Realm</term>
     <listitem>
      <para>
       A container for zonegroups. This allows for separation of zonegroups
       between clusters. It is possible to create multiple realms, making it
       easier to run completely different configurations in the same
       cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Period</term>
     <listitem>
      <para>
       A period holds the configuration structure for the current state of
       the realm. Every period contains a unique ID and an epoch. Every
       realm has an associated current period, holding the current state of
       configuration of the zonegroups and storage policies. Any
       configuration change for a non-master zone will increment the
       period's epoch. Changing the master zone to a different zone will
       trigger the following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         A new period is generated with a new period ID and epoch of 1.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's current period is updated to point to the newly generated
         period ID.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's epoch is incremented.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.intro">
   <title>Example Cluster Setup</title>
   <para>
    In this example, we will focus on creating a single zone group with
    three separate zones, which actively synchronize their data. Two zones
    belong to the same cluster, while the third belongs to a different one.
    There is no synchronization agent involved in mirroring data changes
    between the &rgw;s. This allows for a much simpler configuration scheme
    and active-active configurations. Note that metadata
    operations&mdash;such as creating a new user&mdash;still need to go
    through the master zone. However, data operations&mdash;such as creation
    of buckets and objects&mdash;can be handled by any of the zones.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.keys">
   <title>System Keys</title>
   <para>
    While configuring zones, &rgw; expects creation of an S3-compatible
    system user together with their access and secret keys. This allows
    another &rgw; instance to pull the configuration remotely with the
    access and secret keys. For more information on creating S3 users, see
    <xref linkend="storage.bp.account.s3add"/>.
   </para>
   <tip>
    <para>
     It is useful to generate the access and secret keys before the zone
     creation itself because it makes scripting and use of configuration
     management tools easier later on.
    </para>
   </tip>
   <para>
    For the purpose of this example, let us assume that the access and
    secret keys are set in the environment variables:
   </para>
<screen># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</screen>
   <para>
    Generally, access keys consist of 20 alphanumeric characters, while
    secret keys consist of 40 alphanumeric characters (they can contain +/=
    characters as well). You can generate these keys in the command line:
   </para>
<screen># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.naming">
   <title>Naming Conventions</title>
   <para>
    This example describes the process of setting up a master zone. We will
    assume a zonegroup called <literal>us</literal> spanning the United
    States, which will be our master zonegroup. This will contain two zones
    written in a
    <replaceable>zonegroup</replaceable>-<replaceable>zone</replaceable>
    format. This is our convention only and you can choose a format that you
    prefer. In summary:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Master zonegroup: United States <literal>us</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Master zone: United States, East Region 1:
      <literal>us-east-1</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, East Region 2:
      <literal>us-east-2</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, West Region: <literal>us-west</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This will be a part of a larger realm named <literal>gold</literal>. The
    <literal>us-east-1</literal> and <literal>us-east-2</literal> zones are
    part of the same &ceph; cluster, <literal>us-east-1</literal> being the
    primary one. <literal>us-west</literal> is in a different &ceph;
    cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.pools">
   <title>Default Pools</title>
   <para>
    When configured with the appropriate permissions, &rgw; creates default
    pools on its own. The <literal>pg_num</literal> and
    <literal>pgp_num</literal> values are taken from the
    <filename>ceph.conf</filename> configuration file. Pools related to a
    zone by default follow the convention of
    <replaceable>zone-name</replaceable>.<replaceable>pool-name</replaceable>.
    For example for the <literal>us-east-1</literal> zone, it will be the
    following pools:
   </para>
<screen>.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</screen>
   <para>
    These pools can be created in other zones as well, by replacing
    <literal>us-east-1</literal> with the appropriate zone name.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.realm">
   <title>Creating a Realm</title>
   <para>
    Configure a realm called <literal>gold</literal> and make it the default
    realm:
   </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <para>
    Note that every realm has an ID, which allows for flexibility such as
    renaming the realm later if needed. The
    <literal>current_period</literal> changes whenever we change anything in
    the master zone. The <literal>epoch</literal> is incremented when there
    is a change in the master zone's configuration which results in a change
    of the current period.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.deldefzonegrp">
   <title>Deleting the Default Zonegroup</title>
   <para>
    The default installation of &rgw; creates the default zonegroup called
    <literal>default</literal>. Because we no longer need the default
    zonegroup, remove it.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.createmasterzonegrp">
   <title>Creating a Master Zonegroup</title>
   <para>
    Create a master zonegroup called <literal>us</literal>. The zonegroup
    will manage the zonegroup map and propagate changes to the rest of the
    system. By marking the zonegroup as default, you allow explicitly
    mentioning the rgw-zonegroup switch for later commands.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Alternatively, you can mark a zonegroup as default with the following
    command:
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.masterzone">
   <title>Creating a Master Zone</title>
   <para>
    Now create a default zone and add it to the default zonegroup. Note that
    you will use this zone for metadata operations such as user creation:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Note that the <option>--rgw-zonegroup</option> and
    <option>--default</option> switches add the zone to a zonegroup and make
    it the default zone. Alternatively, the same can also be done with the
    following commands:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone default --rgw-zone=us-east-1
&prompt.cephuser;radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</screen>
   <sect3 xml:id="ceph.rgw.fed.masterzone.createuser">
    <title>Creating System Users</title>
    <para>
     To access zone pools, you need to create a system user. Note that you
     will need these keys when configuring the secondary zone as well.
    </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> --system</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Because you changed the master zone configuration, you need to commit
     the changes for them to take effect in the realm configuration
     structure. Initially, the period looks like this:
    </para>
<screen>&prompt.cephuser;radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
    <para>
     Update the period and commit the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     You need to mention the &rgw; zone and port options in the
     configuration file before starting the &rgw;. For more information on
     &rgw; and its configuration, see <xref linkend="cha.ceph.gw"/>. The
     configuration section of &rgw; should look similar to this:
    </para>
<screen>[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</screen>
    <para>
     Start the &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-east-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.secondaryzone">
   <title>Creating a Secondary Zone</title>
   <para>
    In the same cluster, create and configure the secondary zone named
    <literal>us-east-2</literal>. You can execute all the following commands
    in the node hosting the master zone itself.
   </para>
   <para>
    To create the secondary zone, use the same command as when you created
    the primary zone, except dropping the master flag:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <sect3 xml:id="ceph.rgw.fed.secondzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Inform all the gateways of the new change in the system map by doing a
     period update and committing the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.secondzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     Adjust the configuration of the &rgw; for the secondary zone, and start
     it:
    </para>
<screen>[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</screen>
<screen>&prompt.cephuser;sudo systemctl start ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.seccluster">
   <title>Adding &rgw; to the Second Cluster</title>
   <para>
    The second &ceph; cluster belongs to the same zonegroup as the initial
    one, but may be geographically located elsewhere.
   </para>
   <sect3 xml:id="ceph.rgw.fed.seccluster.realm">
    <title>Default Realm and Zonegroup</title>
    <para>
     Since you already created the realm for the first gateway, pull the
     realm here and make it the default here:
    </para>
<screen>&prompt.cephuser;radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
&prompt.cephuser;radosgw-admin realm default --rgw-realm=gold</screen>
    <para>
     Get the configuration from the master zone by pulling the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable></screen>
    <para>
     Set the default zonegroup to the already created <literal>us</literal>
     zonegroup:
    </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.seczone">
    <title>Secondary Zone Configuration</title>
    <para>
     Create a new zone named <literal>us-west</literal> with the same system
     keys:
    </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.period">
    <title>Update the Period</title>
    <para>
     To propagate the zonegroup map changes, we update and commit the
     period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
    <para>
     Note that the period epoch number has incremented, indicating a change
     in the configuration.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.rgwstart">
    <title>Start the &rgw;</title>
    <para>
     This is similar to starting the &rgw; in the first zone. The only
     difference is that the &rgw; zone configuration should reflect the
     <literal>us-west</literal> zone name:
    </para>
<screen>[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</screen>
    <para>
     Start the second &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-west</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
