<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.gw">
<!-- ============================================================== -->
 <title>&ceph; &rgw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; &rgw; is an object storage interface built on top of
  <systemitem>librgw</systemitem> to provide applications with a RESTful
  gateway to &ceph; Storage Clusters. &ceph; Object Storage supports two
  interfaces:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>S3-compatible</emphasis>: Provides object storage functionality
    with an interface that is compatible with a large subset of the Amazon S3
    RESTful API.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Swift-compatible</emphasis>: Provides object storage
    functionality with an interface that is compatible with a large subset of
    the OpenStack Swift API.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  &ceph; Object Storage uses the &ceph; &rgw; daemon
  (<systemitem>radosgw</systemitem>), which uses an embedded HTTP server
  (CivetWeb) for interacting with a &ceph; Storage Cluster. Since it provides
  interfaces compatible with OpenStack Swift and Amazon S3, the &ceph; &rgw;
  has its own user management. &ceph; &rgw; can store data in the same &ceph;
  Storage Cluster used to store data from &ceph; File System clients or &ceph;
  Block Device clients. The S3 and Swift APIs share a common name space, so you
  may write data with one API and retrieve it with the other.
 </para>
 <para>
  This section helps you install and manage the &ceph; &rgw; (&rgw;). You can
  either choose to use the <command>ceph-deploy</command> tool, or do the
  installation and management manually.
 </para>
 <important>
  <para>
   Before installing &rgw;, you need to have the &ceph; cluster installed first
   (see <xref linkend="cha.ceph.install"/> for more information).
  </para>
 </important>
 <sect1 xml:id="ceph.rgw.cephdeploy">
  <title>Managing &rgw; with <command>ceph-deploy</command></title>

  <para>
   This section describes how to install and configure &rgw; with
   <command>ceph-deploy</command>.
  </para>

  <sect2 xml:id="ceph.rgw.cephdeploy.install">
   <title>Installation</title>
   <para>
    The <command>ceph-deploy</command> script includes the
    <command>rgw</command> component that helps you manage the &rgw; creation
    and operation.
   </para>
   <important>
    <title>Install &ceph;</title>
    <para>
     Before running <command>ceph-deploy rgw</command> as suggested in the
     following step, make sure that &ceph; together with the object gateway
     package are correctly installed on the node where you want to setup &rgw;:
    </para>
    <screen>ceph-deploy install --rgw <replaceable>short_rgw_hostname</replaceable>
</screen>
   </important>
   <para>
    Prepare and activate the nodes in one step. You can specify several pairs
    of
    <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable>
    to install &rgw; on a required number of nodes.
   </para>
<screen>ceph-deploy --overwrite-conf rgw create \
 <replaceable>short_hostname</replaceable>:<replaceable>gateway_name</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw create ceph-node1:rgw.gateway1</screen>
   <para>
    You now have a working &rgw; on the specified nodes, and you need to give
    access to a client. For more information, see
    <xref linkend="ceph.rgw.access"/>.
   </para>
  </sect2>

  <sect2>
   <title>Listing &rgw; Installations</title>
   <para>
    To list all &rgw; instances within the &ceph; cluster, run:
   </para>
<screen>ceph-deploy rgw list</screen>
  </sect2>

  <sect2>
   <title>Removing &rgw; from a Node</title>
   <para>
    To remove a &rgw; installation from the node where it was previously
    installed, run:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete  \
  <replaceable>short_hostname</replaceable>:<replaceable>gatewayname</replaceable> ...</screen>
   <para>
    For example:
   </para>
<screen>ceph-deploy --overwrite-conf rgw delete ceph-node1:rgw.gateway1</screen>
   <tip>
    <para>
     You need a copy of the local <command>ceph.conf</command> file, in your
     current working directory. If you do not have a copy of it, copy it from
     your cluster.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.manual">
  <title>Managing &rgw; Manually</title>

  <para>
   This section describes how to install and configure &rgw; manually.
  </para>

  <sect2>
   <title>Installation</title>
   <procedure>
    <step>
     <para>
      Install &rgw;. The following command installs all required components:
     </para>
<screen>sudo zypper ref &amp;&amp; sudo zypper in ceph-radosgw</screen>
    </step>
    <step>
     <para>
      If the Apache server from the previous &rgw; instance is running, stop it
      and disable the relevant service:
     </para>
<screen>sudo systemctl stop disable apache2.service</screen>
    </step>
    <step>
     <para>
      Edit <filename>/etc/ceph/ceph.conf</filename> and add the following
      lines:
     </para>
<screen>[client.rgw.gateway]
 rgw frontends = "civetweb port=80"</screen>
     <tip>
      <para>
       If you want to configure &rgw;/CivetWeb for use with SSL encryption,
       modify the line accordingly:
      </para>
<screen>rgw frontends = civetweb port=7480s ssl_certificate=<replaceable>path_to_certificate.pem</replaceable></screen>
     </tip>
    </step>
    <step>
     <para>
      Restart the &rgw; service. See <xref linkend="ceph.rgw.operating"/> for
      more information.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ses.rgw.config">
   <title>Configuring &rgw;</title>
   <para>
    Several steps are required to configure a &rgw;.
   </para>
   <sect3>
    <title>Basic Configuration</title>
    <para>
     Configuring a &ceph; &rgw; requires a running &ceph; Storage Cluster. The
     &ceph; &rgw; is a client of the &ceph; Storage Cluster. As a &ceph;
     Storage Cluster client, it requires:
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       A host name for the gateway instance, for example
       <systemitem>gateway</systemitem>.
      </para>
     </listitem>
     <listitem>
      <para>
       A storage cluster user name with appropriate permissions and a keyring.
      </para>
     </listitem>
     <listitem>
      <para>
       Pools to store its data.
      </para>
     </listitem>
     <listitem>
      <para>
       A data directory for the gateway instance.
      </para>
     </listitem>
     <listitem>
      <para>
       An instance entry in the &ceph; Configuration file.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Each instance must have a user name and key to communicate with a &ceph;
     storage cluster. In the following steps, we use a monitor node to create a
     bootstrap keyring, then create the &rgw; instance user keyring based on
     the bootstrap one. Then, we create a client user name and key. Next, we
     add the key to the &ceph; Storage Cluster. Finally, we distribute the
     keyring to the node containing the gateway instance.
    </para>
    <procedure>
     <step>
      <para>
       Create a keyring for the gateway:
      </para>
<screen>sudo ceph-authtool --create-keyring /etc/ceph/ceph.client.rgw.keyring
sudo chmod +r /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Generate a &ceph; &rgw; user name and key for each instance. As an
       example, we will use the name <systemitem>gateway</systemitem> after
       <systemitem>client.radosgw</systemitem>:
      </para>
<screen>sudo ceph-authtool /etc/ceph/ceph.client.rgw.keyring \
  -n client.rgw.gateway --gen-key</screen>
     </step>
     <step>
      <para>
       Add capabilities to the key:
      </para>
<screen>sudo ceph-authtool -n client.rgw.gateway --cap osd 'allow rwx' \
  --cap mon 'allow rwx' /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Once you have created a keyring and key to enable the &ceph; Object
       Gateway with access to the &ceph; Storage Cluster, add the key to your
       &ceph; Storage Cluster. For example:
      </para>
<screen>sudo ceph -k /etc/ceph/ceph.client.admin.keyring auth add client.rgw.gateway \
  -i /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
     <step>
      <para>
       Distribute the keyring to the node with the gateway instance:
      </para>
<screen>sudo scp /etc/ceph/ceph.client.rgw.keyring  ceph@<replaceable>hostname</replaceable>:/home/ceph
ssh <replaceable>hostname</replaceable>
sudo mv ceph.client.rgw.keyring /etc/ceph/ceph.client.rgw.keyring</screen>
     </step>
    </procedure>
    <tip>
     <title>Use Bootstrap Keyring</title>
     <para>
      An alternative way is to create the &rgw; bootstrap keyring, and then
      create the &rgw; keyring from it:
     </para>
     <procedure>
      <step>
       <para>
        Create a &rgw; bootstrap keyring on one of the monitor nodes:
       </para>
<screen>sudo ceph \
 auth get-or-create client.bootstrap-rgw mon 'allow profile bootstrap-rgw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name mon. \
 --keyring=/var/lib/ceph/mon/ceph-<replaceable>node_host</replaceable>/keyring \
 -o /var/lib/ceph/bootstrap-rgw/keyring</screen>
      </step>
      <step>
       <para>
        Create the
        <filename>/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></filename>
        directory for storing the bootstrap keyring:
       </para>
<screen>sudo mkdir \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable></screen>
      </step>
      <step>
       <para>
        Create a &rgw; keyring from the newly created bootstrap keyring:
       </para>
<screen>sudo ceph \
 auth get-or-create client.rgw.<replaceable>rgw_name</replaceable> osd 'allow rwx' mon 'allow rw' \
 --connect-timeout=25 \
 --cluster=ceph \
 --name client.bootstrap-rgw \
 --keyring=/var/lib/ceph/bootstrap-rgw/keyring \
 -o /var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
      <step>
       <para>
        Copy the &rgw; keyring to the &rgw; host:
       </para>
<screen>sudo scp \
/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring \
<replaceable>rgw_host</replaceable>:/var/lib/ceph/radosgw/ceph-<replaceable>rgw_name</replaceable>/keyring</screen>
      </step>
     </procedure>
    </tip>
   </sect3>
   <sect3>
    <title>Create Pools (Optional)</title>
    <para>
     &ceph; &rgw;s require &ceph; Storage Cluster pools to store specific
     gateway data. If the user you created has proper permissions, the gateway
     will create the pools automatically. However, ensure that you have set an
     appropriate default number of placement groups per pool in the &ceph;
     configuration file.
    </para>
    <para>
     When configuring a gateway with the default region and zone, the naming
     convention for pools typically uses 'default' for region and zone naming,
     but you can use any naming convention you prefer:
    </para>
<screen>.rgw.root
default.rgw.control
default.rgw.data.root
default.rgw.gc
default.rgw.log
default.rgw.users.uid
default.rgw.users.email
default.rgw.users.keys
default.rgw.meta
default.rgw.users.swift</screen>
    <para>
     To create the pools manually, see
     <xref linkend="ceph.pools.operate.add_pool"/>.
    </para>
   </sect3>
   <sect3>
    <title>Adding Gateway Configuration to &ceph;</title>
    <para>
     Add the &ceph; &rgw; configuration to the &ceph; Configuration file. The
     &ceph; &rgw; configuration requires you to identify the &ceph; &rgw;
     instance. Then, specify the host name where you installed the &ceph; &rgw;
     daemon, a keyring (for use with cephx), and optionally a log file. For
     example:
    </para>
<screen>[client.rgw.<replaceable>instance-name</replaceable>]
host = <replaceable>hostname</replaceable>
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <tip>
     <title>&rgw; Log File</title>
     <para>
      To override the default &rgw; log file, include the following:
     </para>
<screen>log file = /var/log/radosgw/client.rgw.<replaceable>instance-name</replaceable>.log</screen>
    </tip>
    <para>
     The <literal>[client.rgw.*]</literal> portion of the gateway instance
     identifies this portion of the &ceph; configuration file as configuring a
     &ceph; Storage Cluster client where the client type is a &ceph; &rgw;
     (radosgw). The instance name follows. For example:
    </para>
<screen>[client.rgw.gateway]
host = ceph-gateway
keyring = /etc/ceph/ceph.client.rgw.keyring</screen>
    <note>
     <para>
      The <replaceable>host</replaceable> must be your machine host name,
      excluding the domain name.
     </para>
    </note>
    <para>
     Then turn off <literal>print continue</literal>. If you have it set to
     true, you may encounter problems with PUT operations:
    </para>
<screen>rgw print continue = false</screen>
<!-- Enabling Subdomain S3 Calls -->
    <para>
     To use a &ceph; &rgw; with subdomain S3 calls (for example
     <literal>http://bucketname.hostname</literal>), you must add the &ceph;
     &rgw; DNS name under the <literal>[client.rgw.gateway]</literal> section
     of the &ceph; configuration file:
    </para>
<screen>[client.rgw.gateway]
...
rgw dns name = <replaceable>hostname</replaceable></screen>
    <para>
     You should also consider installing a DNS server such as Dnsmasq on your
     client machine(s) when using the
     <literal>http://<replaceable>bucketname</replaceable>.<replaceable>hostname</replaceable></literal>
     syntax. The <filename>dnsmasq.conf</filename> file should include the
     following settings:
    </para>
<screen>address=/<replaceable>hostname</replaceable>/<replaceable>host-ip-address</replaceable>
listen-address=<replaceable>client-loopback-ip</replaceable></screen>
    <para>
     Then, add the <replaceable>client-loopback-ip</replaceable> IP address as
     the first DNS server on the client machine(s).
    </para>
   </sect3>
   <sect3>
    <title>Redeploy &ceph; Configuration</title>
    <para>
     Use <command>ceph-deploy</command> to push a new copy of the configuration
     to the hosts in your cluster:
    </para>
<screen>ceph-deploy config push <replaceable>host-name [host-name]...</replaceable></screen>
   </sect3>
   <sect3>
    <title>Create Data Directory</title>
    <para>
     Deployment scripts may not create the default &ceph; &rgw; data directory.
     Create data directories for each instance of a radosgw daemon if not
     already done. The <literal>host</literal> variables in the &ceph;
     configuration file determine which host runs each instance of a radosgw
     daemon. The typical form specifies the radosgw daemon, the cluster name
     and the daemon ID.
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/<replaceable>cluster</replaceable>-<replaceable>id</replaceable></screen>
    <para>
     Using the exemplary ceph.conf settings above, you would execute the
     following:
    </para>
<screen>sudo mkdir -p /var/lib/ceph/radosgw/ceph-radosgw.gateway</screen>
   </sect3>
   <sect3>
    <title>Restart Services and Start the Gateway</title>
    <para>
     To ensure that all components have reloaded their configurations, we
     recommend restarting your &ceph; Storage Cluster service. Then, start up
     the <systemitem>radosgw</systemitem> service. For more information, see
     <xref linkend="cha.ceph.operating"/> and
     <xref linkend="ceph.rgw.operating"/>.
    </para>
    <para>
     After the service is up and running, you can make an anonymous GET request
     to see if the gateway returns a response. A simple HTTP request to the
     domain name should return the following:
    </para>
<screen>&lt;ListAllMyBucketsResult&gt;
      &lt;Owner&gt;
              &lt;ID&gt;anonymous&lt;/ID&gt;
              &lt;DisplayName/&gt;
      &lt;/Owner&gt;
      &lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Operating the &rgw; Service</title>

  <para>
   &rgw; service is operated with the <command>systemctl</command> command. You
   need to have &rootuser; privileges to operate the &rgw; service. Note that
   <replaceable>gateway_host</replaceable> is the host name of the server whose
   &rgw; instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the &rgw; service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Managing &rgw; Access</title>

  <para>
   You can communicate with &rgw; using either S3- or &swift;-compatible
   interface. Both interfaces require you to create a specific user, and
   install the relevant client software to communicate with the gateway using
   the user's secret key.
  </para>

  <para>
   For an introduction and a few practical examples on &rgw; access, see
   <xref linkend="storage.bp.inst.rgw_client"/>.
  </para>

  <sect2>
   <title>Managing S3 Access</title>
   <para>
    S3 interface is compatible with a large subset of the Amazon S3 RESTful
    API.
   </para>
   <tip>
    <para>
     S3cmd is a command line S3 client. You can find it in the
     <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both &sle; and
     &opensuse; based distributions.
    </para>
   </tip>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3add"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.s3rm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing User Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Setting Quotas</title>
    <para>
     See <xref linkend="storage.bp.account.s3quota"/>.
    </para>
   </sect3>
  </sect2>

  <sect2>
   <title>Managing &swift; Access</title>
   <para>
    &swift; interface is compatible with a large subset of the &ostack; &swift;
    API.
   </para>
   <sect3>
    <title>Adding Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftadd"/>.
    </para>
   </sect3>
   <sect3>
    <title>Removing Users</title>
    <para>
     See <xref linkend="storage.bp.account.swiftrm"/>.
    </para>
   </sect3>
   <sect3>
    <title>Changing Passwords</title>
    <para>
     See <xref linkend="storage.bp.account.user_pwd"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.fed">
<!-- https://github.com/theanalyst/ceph/blob/doc/multisite-rgw/doc/radosgw/multisite.rst -->

  <title>Multi-site Object Storage Gateways</title>

  <para>
   You can configure each &rgw; to participate in a federated architecture,
   working in an active zone configuration while allowing for writes to
   non-master zones.
  </para>

  <sect2 xml:id="ceph.rgw.fed.term">
   <title>Terminology</title>
   <para>
    A description of terms specific to a federated architecture follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>Zone</term>
     <listitem>
      <para>
       A logical grouping of one or more &rgw; instances. There must be one
       zone designated as the <emphasis>master</emphasis> zone in a
       <emphasis>zonegroup</emphasis>, which handles all bucket and user
       creation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup</term>
     <listitem>
      <para>
       A zonegroup consists of multiple zones. There should be a master
       zonegroup that will handle changes to the system configuration.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup map</term>
     <listitem>
      <para>
       A configuration structure that holds the map of the entire system, for
       example which zonegroup is the master, relationships between different
       zonegroups, and certain configuration options such as storage policies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Realm</term>
     <listitem>
      <para>
       A container for zonegroups. This allows for separation of zonegroups
       between clusters. It is possible to create multiple realms, making it
       easier to run completely different configurations in the same cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Period</term>
     <listitem>
      <para>
       A period holds the configuration structure for the current state of the
       realm. Every period contains a unique ID and an epoch. Every realm has
       an associated current period, holding the current state of configuration
       of the zonegroups and storage policies. Any configuration change for a
       non-master zone will increment the period's epoch. Changing the master
       zone to a different zone will trigger the following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         A new period is generated with a new period ID and epoch of 1.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's current period is updated to point to the newly generated
         period ID.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's epoch is incremented.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.intro">
   <title>Example Cluster Setup</title>
   <para>
    In this example, we will focus on creating a single zone group with three
    separate zones, which actively synchronize their data. Two zones belong to
    the same cluster, while the third belongs to a different one. There is no
    synchronization agent involved in mirroring data changes between the
    &rgw;s. This allows for a much simpler configuration scheme and
    active-active configurations. Note that metadata operations&mdash;such as
    creating a new user&mdash;still need to go through the master zone.
    However, data operations&mdash;such as creation of buckets and
    objects&mdash;can be handled by any of the zones.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.keys">
   <title>System Keys</title>
   <para>
    While configuring zones, &rgw; expects creation of an S3-compatible system
    user together with their access and secret keys. This allows another &rgw;
    instance to pull the configuration remotely with the access and secret
    keys. For more information on creating S3 users, see
    <xref linkend="storage.bp.account.s3add"/>.
   </para>
   <tip>
    <para>
     It is useful to generate the access and secret keys before the zone
     creation itself because it makes scripting and use of configuration
     management tools easier later on.
    </para>
   </tip>
   <para>
    For the purpose of this example, let us assume that the access and secret
    keys are set in the environment variables:
   </para>
<screen># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</screen>
   <para>
    Generally, access keys consist of 20 alphanumeric characters, while secret
    keys consist of 40 alphanumeric characters (they can contain +/= characters
    as well). You can generate these keys in the command line:
   </para>
<screen># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.naming">
   <title>Naming Conventions</title>
   <para>
    This example describes the process of setting up a master zone. We will
    assume a zonegroup called <literal>us</literal> spanning the United States,
    which will be our master zonegroup. This will contain two zones written in
    a <replaceable>zonegroup</replaceable>-<replaceable>zone</replaceable>
    format. This is our convention only and you can choose a format that you
    prefer. In summary:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Master zonegroup: United States <literal>us</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Master zone: United States, East Region 1: <literal>us-east-1</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, East Region 2:
      <literal>us-east-2</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, West Region: <literal>us-west</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This will be a part of a larger realm named <literal>gold</literal>. The
    <literal>us-east-1</literal> and <literal>us-east-2</literal> zones are
    part of the same &ceph; cluster, <literal>us-east-1</literal> being the
    primary one. <literal>us-west</literal> is in a different &ceph; cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.pools">
   <title>Default Pools</title>
   <para>
    When configured with the appropriate permissions, &rgw; creates default
    pools on its own. The <literal>pg_num</literal> and
    <literal>pgp_num</literal> values are taken from the
    <filename>ceph.conf</filename> configuration file. Pools related to a zone
    by default follow the convention of
    <replaceable>zone-name</replaceable>.<replaceable>pool-name</replaceable>.
    For example for the <literal>us-east-1</literal> zone, it will be the
    following pools:
   </para>
<screen>.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</screen>
   <para>
    These pools can be created in other zones as well, by replacing
    <literal>us-east-1</literal> with the appropriate zone name.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.realm">
   <title>Creating a Realm</title>
   <para>
    Configure a realm called <literal>gold</literal> and make it the default
    realm:
   </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <para>
    Note that every realm has an ID, which allows for flexibility such as
    renaming the realm later if needed. The <literal>current_period</literal>
    changes whenever we change anything in the master zone. The
    <literal>epoch</literal> is incremented when there is a change in the
    master zone's configuration which results in a change of the current
    period.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.deldefzonegrp">
   <title>Deleting the Default Zonegroup</title>
   <para>
    The default installation of &rgw; creates the default zonegroup called
    <literal>default</literal>. Because we no longer need the default
    zonegroup, remove it.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.createmasterzonegrp">
   <title>Creating a Master Zonegroup</title>
   <para>
    Create a master zonegroup called <literal>us</literal>. The zonegroup will
    manage the zonegroup map and propagate changes to the rest of the system.
    By marking the zonegroup as default, you allow explicitly mentioning the
    rgw-zonegroup switch for later commands.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Alternatively, you can mark a zonegroup as default with the following
    command:
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.masterzone">
   <title>Creating a Master Zone</title>
   <para>
    Now create a default zone and add it to the default zonegroup. Note that
    you will use this zone for metadata operations such as user creation:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Note that the <option>--rgw-zonegroup</option> and
    <option>--default</option> switches add the zone to a zonegroup and make it
    the default zone. Alternatively, the same can also be done with the
    following commands:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone default --rgw-zone=us-east-1
&prompt.cephuser;radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</screen>
   <sect3 xml:id="ceph.rgw.fed.masterzone.createuser">
    <title>Creating System Users</title>
    <para>
     To access zone pools, you need to create a system user. Note that you will
     need these keys when configuring the secondary zone as well.
    </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> --system</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Because you changed the master zone configuration, you need to commit the
     changes for them to take effect in the realm configuration structure.
     Initially, the period looks like this:
    </para>
<screen>&prompt.cephuser;radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
    <para>
     Update the period and commit the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     You need to mention the &rgw; zone and port options in the configuration
     file before starting the &rgw;. For more information on &rgw; and its
     configuration, see <xref linkend="cha.ceph.gw"/>. The configuration
     section of &rgw; should look similar to this:
    </para>
<screen>[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</screen>
    <para>
     Start the &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-east-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.secondaryzone">
   <title>Creating a Secondary Zone</title>
   <para>
    In the same cluster, create and configure the secondary zone named
    <literal>us-east-2</literal>. You can execute all the following commands in
    the node hosting the master zone itself.
   </para>
   <para>
    To create the secondary zone, use the same command as when you created the
    primary zone, except dropping the master flag:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <sect3 xml:id="ceph.rgw.fed.secondzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Inform all the gateways of the new change in the system map by doing a
     period update and committing the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.secondzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     Adjust the configuration of the &rgw; for the secondary zone, and start
     it:
    </para>
<screen>[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</screen>
<screen>&prompt.cephuser;sudo systemctl start ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.seccluster">
   <title>Adding &rgw; to the Second Cluster</title>
   <para>
    The second &ceph; cluster belongs to the same zonegroup as the initial one,
    but may be geographically located elsewhere.
   </para>
   <sect3 xml:id="ceph.rgw.fed.seccluster.realm">
    <title>Default Realm and Zonegroup</title>
    <para>
     Since you already created the realm for the first gateway, pull the realm
     here and make it the default here:
    </para>
<screen>&prompt.cephuser;radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
&prompt.cephuser;radosgw-admin realm default --rgw-realm=gold</screen>
    <para>
     Get the configuration from the master zone by pulling the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable></screen>
    <para>
     Set the default zonegroup to the already created <literal>us</literal>
     zonegroup:
    </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.seczone">
    <title>Secondary Zone Configuration</title>
    <para>
     Create a new zone named <literal>us-west</literal> with the same system
     keys:
    </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.period">
    <title>Update the Period</title>
    <para>
     To propagate the zonegroup map changes, we update and commit the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
    <para>
     Note that the period epoch number has incremented, indicating a change in
     the configuration.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.rgwstart">
    <title>Start the &rgw;</title>
    <para>
     This is similar to starting the &rgw; in the first zone. The only
     difference is that the &rgw; zone configuration should reflect the
     <literal>us-west</literal> zone name:
    </para>
<screen>[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</screen>
    <para>
     Start the second &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-west</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
