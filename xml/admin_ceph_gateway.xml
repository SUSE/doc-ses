<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.gw">
<!-- ============================================================== -->
 <title>&ceph; &rgw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces details about administration tasks related to &rgw;,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </para>
 <sect1 xml:id="ceph.rgw.operating">
  <title>Operating the &rgw; Service</title>

  <para>
   &rgw; service is operated with the <command>systemctl</command> command. You
   need to have &rootuser; privileges to operate the &rgw; service. Note that
   <replaceable>gateway_host</replaceable> is the host name of the server whose
   &rgw; instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the &rgw; service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>gateway_host</replaceable>
    </term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph.rgw.https">
  <title>Enable HTTPS</title>
  <sect2 xml:id="ceph.rgw.https.ceph_conf">
   <title>&rgw; Configuration</title>
   <para>
    To enable HTTPS for the default &rgw; role, a certificate file has to be created on
    the &rgw; nodes and port 443 needs to be enabled in the <filename>ceph.conf</filename>.
    Edit the <literal>[rgw]</literal> section in the <filename>/etc/ceph/ceph.conf</filename>
    to contain the following lines:
   </para>
<screen>[client.rgw.<replaceable>NODE_NAME</replaceable>]
rgw_frontends = civetweb port=443s ssl_certificate=/etc/ceph/private/keyandcert.pem</screen>
   <para>
    The <filename>keyandcert.pem</filename> must contain the key and the certificate.
    Replace <replaceable>NODE_NAME</replaceable> with the name of the node.
   </para>
  </sect2>
  <sect2 xml:id="ceph.rgw.https.deepsea">
   <title>Configure with &deepsea;</title>
   <para>
    This example procedure describes how to generate the certificate on the
    &smaster; and distribute all required changes via &deepsea;.
   </para>
   <procedure>
    <step>
     <para>
      Create the the certificates on the &smaster;. Enter all data you need
      in your certificate when running the <command>openssl</command> command.
      Especially entering the FQDN as the common name is useful.
     </para>
<screen>&prompt.smaster;<command>mkdir</command> /srv/salt/ceph/rgw/cert
&prompt.smaster;<command>cd</command> /srv/salt/ceph/rgw/cert
&prompt.smaster;<command>openssl</command> req -x509 -nodes -days 1095 -newkey rsa:4096 -keyout rgw.key -out rgw.crt
&prompt.smaster;<command>cat</command> rgw.key > rgw.pem &amp;&amp; <command>cat</command> rgw.crt >> rgw.pem</screen>
     <para>
      It is also possible to use a certificate signed by a CA instead and place
      it in the folder <filename>/srv/salt/ceph/rgw/cert</filename>. The
      <filename>rgw.pem</filename> must contain the key and certificate.
     </para>
    </step>
    <step>
     <para>
      Edit the <filename>/srv/salt/ceph/configuration/files/ceph.conf.rgw</filename>
      and change the content to:
     </para>
<screen>[ client.{{ client }} ]
rgw frontends = "civetweb port=443s ssl_certificate=/etc/ceph/rgw.pem"
rgw dns name = {{ grains['host'] }}</screen>
    </step>
    <step>
     <para>
      &deepsea; needs to distribute the <filename>rgw.pem</filename>. Therefore
      edit the <filename>/srv/salt/ceph/rgw/init.sls</filename> and change it to:
     </para>
<screen>include:
  - .{{ salt['pillar.get']('rgw_init', 'default') }}
  - .cert</screen>
    </step>
    <step>
     <para>
      Create the <filename>/srv/salt/ceph/rgw/cert/init.sls</filename> and
      add the following content:
     </para>
<screen>deploy the rgw.pem file:
  file.managed:
    - name: /etc/ceph/rgw.pem
    - source: salt://ceph/rgw/cert/rgw.pem
</screen>
    </step>
    <step>
     <para>Execute &deepsea; stages 0 to 4:</para>
<screen>&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.0
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.1
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.2
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.3
&prompt.smaster;<command>salt-run</command> state.orch ceph.stage.4</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.access">
  <title>Managing &rgw; Access</title>

  <para>
   You can communicate with &rgw; using either S3- or &swift;-compatible
   interface. S3 interface is compatible with a large subset of the Amazon S3
   RESTful API. &swift; interface is compatible with a large subset of the
   &ostack; &swift; API.
  </para>

  <para>
   Both interfaces require you to create a specific user, and install the
   relevant client software to communicate with the gateway using the user's
   secret key.
  </para>

  <sect2 xml:id="accessing.ragos.gateway">
   <title>Accessing &rgw;</title>
   <sect3>
    <title>S3 Interface Access</title>
    <tip>
     <para>
      S3cmd is a command line S3 client. You can find it in the
      <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
      Build Service</link>. The repository contains versions for both &sle; and
      &opensuse; based distributions.
     </para>
    </tip>
    <para>
     To access the S3 interface, you need a REST client.
     <command>S3cmd</command> is a command line S3 client. You can find it in
     the
     <link xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both &sle; and
     &opensuse; based distributions.
    </para>
    <para>
     If you want to test your access to the S3 interface, you can also write a
     small a Python script. The script will connect to &rgw;, create a new
     bucket, and list all buckets. The values for
     <option>aws_access_key_id</option> and
     <option>aws_secret_access_key</option> are taken from the values of
     <option>access_key</option> and <option>secret_key</option> returned by
     the <command>radosgw_admin</command> command from
     <xref linkend="adding.s3.swift.users"/>.
    </para>
    <procedure>
     <step>
      <para>
       Install the <systemitem>python-boto</systemitem> package:
      </para>
<screen>sudo zypper in python-boto</screen>
     </step>
     <step>
      <para>
       Create a new Python script called <filename>s3test.py</filename> with
       the following content:
       <remark role="fixme">Provide script in RPM? Is it really necessary to create pool? This script is not necessary at all, remove it from documentation?</remark>
      </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '{hostname}',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
print "{name}\t{created}".format(
name = bucket.name,
created = bucket.creation_date,
)</screen>
      <para>
       Replace <literal>{hostname}</literal> with the host name of the host
       where you configured &rgw; service, for example
       <literal>gateway_host</literal>.
      </para>
     </step>
     <step>
      <para>
       Run the script:
      </para>
<screen>python s3test.py</screen>
      <para>
       The script outputs something like the following:
      </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Swift Interface Access</title>
    <para>
     To access &rgw; via Swift interface, you need the <command>swift</command>
     command line client. Its manual page <command>man 1 swift</command> tells
     you more about its command line options.
    </para>
    <para>
     To install <command>swift</command>, run the following:
    </para>
<screen>sudo zypper in python-swiftclient</screen>
    <para>
     The swift access uses the following syntax:
    </para>
<screen>swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>swift_secret_key</replaceable>' list</screen>
    <para>
     Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
     gateway server, and <replaceable>swift_secret_key</replaceable> with its
     value from the output of the <command>radosgw-admin key create</command>
     command executed for the <systemitem>swift</systemitem> user in
     <xref linkend="adding.s3.swift.users"/>.
    </para>
    <para>
     For example:
    </para>
<screen>swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
    <para>
     The output is:
    </para>
<screen>my-new-bucket</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="s3.swift.accounts.managment">
   <title>Managing S3 and &swift; Accounts</title>
   <sect3 xml:id="adding.s3.swift.users">
    <title>Adding S3 and &swift; Users</title>
    <para>
     You need to create a user, access key and secret to enable end users to
     interact with the gateway. There are two types of users: a
     <emphasis>user</emphasis> and <emphasis>subuser</emphasis>. While
     <emphasis>users</emphasis> are used when interacting with the S3
     interface, <emphasis>subusers</emphasis> are users of the &swift;
     interface. Each subuser is associated to a user. To create a &swift; user,
     follow the steps:
    </para>
    <procedure>
     <step>
      <para>
       To create a &swift; user&mdash;which is a <emphasis>subuser</emphasis>
       in our terminology&mdash;you need to create the associated
       <emphasis>user</emphasis> first.
      </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
      <para>
       For example:
      </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
     </step>
     <step>
      <para>
       To create a subuser (&swift; interface) for the user, you must specify
       the user ID (--uid=<replaceable>username</replaceable>), a subuser ID,
       and the access level for the subuser.
      </para>
<screen>sudo radosgw-admin subuser create --uid=<replaceable>uid</replaceable> \
 --subuser=<replaceable>uid</replaceable> \
 --access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
      <para>
       For example:
      </para>
<screen>sudo radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</screen>
     </step>
     <step>
      <para>
       Generate a secret key for the user.
      </para>
<screen>sudo radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</screen>
     </step>
     <step>
      <para>
       Both commands will output JSON-formatted data showing the user state.
       Notice the following lines, and remember the
       <literal>secret_key</literal> value:
      </para>
<screen>"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
     </step>
    </procedure>
    <para/>
    <para>
     When accessing &rgw; through the S3 interface you need to create a S3 user
     by running:
    </para>
<screen>sudo radosgw-admin user create --uid=<replaceable>username</replaceable> \
 --display-name="<replaceable>display-name</replaceable>" --email=<replaceable>email</replaceable></screen>
    <para>
     For example:
    </para>
<screen>sudo radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
    <para>
     The command also creates the user's access and secret key. Check its
     output for <literal>access_key</literal> and <literal>secret_key</literal>
     keywords and their values:
    </para>
<screen>[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</screen>
   </sect3>
   <sect3 xml:id="removing.s3.swift.users">
    <title>Removing S3 and &swift; Users</title>
    <para>
     The procedure for deleting users is similar for S3 and &swift; users. But
     in case of &swift; users you may need to delete the user including its
     subusers.
    </para>
    <para>
     To remove a S3 or &swift; user (including all its subusers), specify
     <option>user rm</option> and the user ID in the following command:
    </para>
<screen>sudo radosgw-admin user rm --uid=example_user</screen>
    <para>
     To remove just a subuser, specify <option>subuser rm</option> and the
     subuser ID.
     <remark role="fixme">Until this line all commands are beginning with sudo, the following are executed as normal user. Intentional?</remark>
    </para>
<screen>radosgw-admin subuser rm --uid=example_user:swift</screen>
    <para>
     You can make use of the following options:
    </para>
    <variablelist>
     <varlistentry>
      <term>--purge-data</term>
      <listitem>
       <para>
        Purges all data associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>--purge-keys</term>
      <listitem>
       <para>
        Purges all keys associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <tip>
     <title>Removing a Subuser</title>
     <para>
      When you remove a subuser, you are removing access to the Swift
      interface. The user will remain in the system.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="changing.s3.swift.users.password">
    <title>Changing S3 and &swift; User Access and Secret Keys</title>
    <para>
     The <literal>access_key</literal> and <literal>secret_key</literal>
     parameters identify the &rgw; user when accessing the gateway. Changing
     the existing user keys is the same as creating new ones, as the old keys
     get overwritten.
    </para>
    <para>
     For S3 users, run the following:
    </para>
<screen>radosgw-admin key create --uid=<replaceable>example_user</replaceable> --key-type=s3 --gen-access-key --gen-secret</screen>
    <para>
     For &swift; users, run the following:
    </para>
<screen>radosgw-admin key create --subuser=<replaceable>example_user</replaceable>:swift --key-type=swift --gen-secret</screen>
    <variablelist>
     <varlistentry>
      <term><option>--key-type=<replaceable>type</replaceable></option>
      </term>
      <listitem>
       <para>
        Specifies the type of key. Either <literal>swift</literal> or
        <literal>s3</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-access-key</option>
      </term>
      <listitem>
       <para>
        Generates a random access key (for S3 user by default).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-secret</option>
      </term>
      <listitem>
       <para>
        Generates a random secret key.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--secret=<replaceable>key</replaceable></option>
      </term>
      <listitem>
       <para>
        Specifies a secret key, for example manually generated.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="user.quota.managment">
    <title>User Quota Management</title>
    <para>
     The &ceph; &rgw; enables you to set quotas on users and buckets owned by
     users. Quotas include the maximum number of objects in a bucket and the
     maximum storage size in megabytes.
    </para>
    <para>
     Before you enable a user quota, you first need to set its parameters:
    </para>
<screen>radosgw-admin quota set --quota-scope=user --uid=<replaceable>example_user</replaceable> \
 --max-objects=1024 --max-size=1024</screen>
    <variablelist>
     <varlistentry>
      <term><option>--max-objects</option>
      </term>
      <listitem>
       <para>
        Specifies the maximum number of objects. A negative value disables the
        check.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--max-size</option>
      </term>
      <listitem>
       <para>
        Specifies the maximum number of bytes. A negative value disables the
        check.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--quota-scope</option>
      </term>
      <listitem>
       <para>
        Sets the scope for the quota. The options are <literal>bucket</literal>
        and <literal>user</literal>. Bucket quotas apply to buckets a user
        owns. User quotas apply to a user.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Once you set a user quota, you may enable it:
    </para>
<screen>radosgw-admin quota enable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
    <para>
     To disable a quota:
    </para>
<screen>radosgw-admin quota disable --quota-scope=user --uid=<replaceable>example_user</replaceable></screen>
    <para>
     To list quota settings:
    </para>
<screen>radosgw-admin user info --uid=<replaceable>example_user</replaceable></screen>
    <para>
     To update quota statistics:
    </para>
<screen>radosgw-admin user stats --uid=<replaceable>example_user</replaceable> --sync-stats</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.fed">
<!-- https://github.com/theanalyst/ceph/blob/doc/multisite-rgw/doc/radosgw/multisite.rst -->

  <title>Multisite Object Storage Gateways</title>

  <para>
   You can configure each &rgw; to participate in a federated architecture,
   working in an active zone configuration while allowing for writes to
   non-master zones.
   <remark role="fixme">What is a master zone? Explained in next section but maybe it is better to first explain the term before using it. Explanation of federated architecture is also not provided. Meaning, benefits, setup, etc. This section needs some sort of introduction.</remark>
  </para>

  <sect2 xml:id="ceph.rgw.fed.term">
   <title>Terminology</title>
   <para>
    A description of terms specific to a federated architecture follows:
   </para>
   <variablelist>
    <varlistentry>
     <term>Zone</term>
     <listitem>
      <para>
       A logical grouping of one or more &rgw; instances. There must be one
       zone designated as the <emphasis>master</emphasis> zone in a
       <emphasis>zonegroup</emphasis>, which handles all bucket and user
       creation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup</term>
     <listitem>
      <para>
       A zonegroup consists of multiple zones. There should be a master
       zonegroup that will handle changes to the system configuration.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zonegroup map</term>
     <listitem>
      <para>
       A configuration structure that holds the map of the entire system, for
       example which zonegroup is the master, relationships between different
       zonegroups, and certain configuration options such as storage policies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Realm</term>
     <listitem>
      <para>
       A container for zonegroups. This allows for separation of zonegroups
       between clusters. It is possible to create multiple realms, making it
       easier to run completely different configurations in the same cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Period</term>
     <listitem>
      <para>
       A period holds the configuration structure for the current state of the
       realm. Every period contains a unique ID and an epoch. Every realm has
       an associated current period, holding the current state of configuration
       of the zonegroups and storage policies. Any configuration change for a
       non-master zone will increment the period's epoch. Changing the master
       zone to a different zone will trigger the following changes:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         A new period is generated with a new period ID and epoch of 1.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's current period is updated to point to the newly generated
         period ID.
        </para>
       </listitem>
       <listitem>
        <para>
         Realm's epoch is incremented.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.intro">
   <title>Example Cluster Setup</title>
   <para>
    In this example, we will focus on creating a single zone group with three
    separate zones, which actively synchronize their data. Two zones belong to
    the same cluster, while the third belongs to a different one. There is no
    synchronization agent involved in mirroring data changes between the
    &rgw;s. This allows for a much simpler configuration scheme and
    active-active configurations. Note that metadata operations&mdash;such as
    creating a new user&mdash;still need to go through the master zone.
    However, data operations&mdash;such as creation of buckets and
    objects&mdash;can be handled by any of the zones.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.keys">
   <title>System Keys</title>
   <para>
    While configuring zones, &rgw; expects creation of an S3-compatible system
    user together with their access and secret keys. This allows another &rgw;
    instance to pull the configuration remotely with the access and secret
    keys. For more information on creating S3 users, see
    <xref linkend="adding.s3.swift.users"/>.
   </para>
   <tip>
    <para>
     It is useful to generate the access and secret keys before the zone
     creation itself because it makes scripting and use of configuration
     management tools easier later on.
    </para>
   </tip>
   <para>
    For the purpose of this example, let us assume that the access and secret
    keys are set in the environment variables:
   </para>
<screen># SYSTEM_ACCESS_KEY=1555b35654ad1656d805
# SYSTEM_SECRET_KEY=h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q==</screen>
   <para>
    Generally, access keys consist of 20 alphanumeric characters, while secret
    keys consist of 40 alphanumeric characters (they can contain +/= characters
    as well). You can generate these keys in the command line:
   </para>
<screen># SYSTEM_ACCESS_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 20 | head -n 1)
# SYSTEM_SECRET_KEY=$(cat /dev/urandom | tr -dc 'a-zA-Z0-9' | fold -w 40 | head -n 1)</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.naming">
   <title>Naming Conventions</title>
   <para>
    This example describes the process of setting up a master zone. We will
    assume a zonegroup called <literal>us</literal> spanning the United States,
    which will be our master zonegroup. This will contain two zones written in
    a <replaceable>zonegroup</replaceable>-<replaceable>zone</replaceable>
    format. This is our convention only and you can choose a format that you
    prefer. In summary:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Master zonegroup: United States <literal>us</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Master zone: United States, East Region 1: <literal>us-east-1</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, East Region 2:
      <literal>us-east-2</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      Secondary zone: United States, West Region: <literal>us-west</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    This will be a part of a larger realm named <literal>gold</literal>. The
    <literal>us-east-1</literal> and <literal>us-east-2</literal> zones are
    part of the same &ceph; cluster, <literal>us-east-1</literal> being the
    primary one. <literal>us-west</literal> is in a different &ceph; cluster.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.pools">
   <title>Default Pools</title>
   <para>
    When configured with the appropriate permissions, &rgw; creates default
    pools on its own. The <literal>pg_num</literal> and
    <literal>pgp_num</literal> values are taken from the
    <filename>ceph.conf</filename> configuration file. Pools related to a zone
    by default follow the convention of
    <replaceable>zone-name</replaceable>.<replaceable>pool-name</replaceable>.
    For example for the <literal>us-east-1</literal> zone, it will be the
    following pools:
   </para>
<screen>.rgw.root
us-east-1.rgw.control
us-east-1.rgw.data.root
us-east-1.rgw.gc
us-east-1.rgw.log
us-east-1.rgw.intent-log
us-east-1.rgw.usage
us-east-1.rgw.users.keys
us-east-1.rgw.users.email
us-east-1.rgw.users.swift
us-east-1.rgw.users.uid
us-east-1.rgw.buckets.index
us-east-1.rgw.buckets.data
us-east-1.rgw.meta</screen>
   <para>
    These pools can be created in other zones as well, by replacing
    <literal>us-east-1</literal> with the appropriate zone name.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.realm">
   <title>Creating a Realm</title>
   <para>
    Configure a realm called <literal>gold</literal> and make it the default
    realm:
   </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=gold --default
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <para>
    Note that every realm has an ID, which allows for flexibility such as
    renaming the realm later if needed. The <literal>current_period</literal>
    changes whenever we change anything in the master zone. The
    <literal>epoch</literal> is incremented when there is a change in the
    master zone's configuration which results in a change of the current
    period.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.deldefzonegrp">
   <title>Deleting the Default Zonegroup</title>
   <para>
    The default installation of &rgw; creates the default zonegroup called
    <literal>default</literal>. Because we no longer need the default
    zonegroup, remove it.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.createmasterzonegrp">
   <title>Creating a Master Zonegroup</title>
   <para>
    Create a master zonegroup called <literal>us</literal>. The zonegroup will
    manage the zonegroup map and propagate changes to the rest of the system.
    By marking the zonegroup as default, you allow explicitly mentioning the
    rgw-zonegroup switch for later commands.
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default
{
  "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "name": "us",
  "api_name": "us",
  "is_master": "true",
  "endpoints": [
      "http:\/\/rgw1:80"
  ],
  "hostnames": [],
  "hostnames_s3website": [],
  "master_zone": "",
  "zones": [],
  "placement_targets": [],
  "default_placement": "",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Alternatively, you can mark a zonegroup as default with the following
    command:
   </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.masterzone">
   <title>Creating a Master Zone</title>
   <para>
    Now create a default zone and add it to the default zonegroup. Note that
    you will use this zone for metadata operations such as user creation:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "name": "us-east-1",
  "domain_root": "us-east-1/gc.rgw.data.root",
  "control_pool": "us-east-1/gc.rgw.control",
  "gc_pool": "us-east-1/gc.rgw.gc",
  "log_pool": "us-east-1/gc.rgw.log",
  "intent_log_pool": "us-east-1/gc.rgw.intent-log",
  "usage_log_pool": "us-east-1/gc.rgw.usage",
  "user_keys_pool": "us-east-1/gc.rgw.users.keys",
  "user_email_pool": "us-east-1/gc.rgw.users.email",
  "user_swift_pool": "us-east-1/gc.rgw.users.swift",
  "user_uid_pool": "us-east-1/gc.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-1/gc.rgw.buckets.index",
              "data_pool": "us-east-1/gc.rgw.buckets.data",
              "data_extra_pool": "us-east-1/gc.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-1/gc.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   <para>
    Note that the <option>--rgw-zonegroup</option> and
    <option>--default</option> switches add the zone to a zonegroup and make it
    the default zone. Alternatively, the same can also be done with the
    following commands:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone default --rgw-zone=us-east-1
&prompt.cephuser;radosgw-admin zonegroup add --rgw-zonegroup=us --rgw-zone=us-east-1</screen>
   <sect3 xml:id="ceph.rgw.fed.masterzone.createuser">
    <title>Creating System Users</title>
    <para>
     To access zone pools, you need to create a system user. Note that you will
     need these keys when configuring the secondary zone as well.
    </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> --system</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Because you changed the master zone configuration, you need to commit the
     changes for them to take effect in the realm configuration structure.
     Initially, the period looks like this:
    </para>
<screen>&prompt.cephuser;radosgw-admin period get
{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
    <para>
     Update the period and commit the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 1,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.masterzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     You need to mention the &rgw; zone and port options in the configuration
     file before starting the &rgw;. For more information on &rgw; and its
     configuration, see <xref linkend="cha.ceph.gw"/>. The configuration
     section of &rgw; should look similar to this:
    </para>
<screen>[client.rgw.us-east-1]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-1</screen>
    <para>
     Start the &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-east-1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.secondaryzone">
   <title>Creating a Secondary Zone</title>
   <para>
    In the same cluster, create and configure the secondary zone named
    <literal>us-east-2</literal>. You can execute all the following commands in
    the node hosting the master zone itself.
   </para>
   <para>
    To create the secondary zone, use the same command as when you created the
    primary zone, except dropping the master flag:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <sect3 xml:id="ceph.rgw.fed.secondzone.updateperiod">
    <title>Update the Period</title>
    <para>
     Inform all the gateways of the new change in the system map by doing a
     period update and committing the changes:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.secondzone.startrgw">
    <title>Start the &rgw;</title>
    <para>
     Adjust the configuration of the &rgw; for the secondary zone, and start
     it:
    </para>
<screen>[client.rgw.us-east-2]
rgw_frontends="civetweb port=80"
rgw_zone=us-east-2</screen>
<screen>&prompt.cephuser;sudo systemctl start ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.fed.seccluster">
   <title>Adding &rgw; to the Second Cluster</title>
   <para>
    The second &ceph; cluster belongs to the same zonegroup as the initial one,
    but may be geographically located elsewhere.
   </para>
   <sect3 xml:id="ceph.rgw.fed.seccluster.realm">
    <title>Default Realm and Zonegroup</title>
    <para>
     Since you already created the realm for the first gateway, pull the realm
     here and make it the default here:
    </para>
<screen>&prompt.cephuser;radosgw-admin realm pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2
}
&prompt.cephuser;radosgw-admin realm default --rgw-realm=gold</screen>
    <para>
     Get the configuration from the master zone by pulling the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period pull --url=http://rgw1:80 \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable></screen>
    <para>
     Set the default zonegroup to the already created <literal>us</literal>
     zonegroup:
    </para>
<screen>&prompt.cephuser;radosgw-admin zonegroup default --rgw-zonegroup=us</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.seczone">
    <title>Secondary Zone Configuration</title>
    <para>
     Create a new zone named <literal>us-west</literal> with the same system
     keys:
    </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-west \
--access-key=<replaceable>$SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>$SYSTEM_SECRET_KEY</replaceable> \
--endpoints=http://rgw3:80 --default
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-west",
  "domain_root": "us-west.rgw.data.root",
  "control_pool": "us-west.rgw.control",
  "gc_pool": "us-west.rgw.gc",
  "log_pool": "us-west.rgw.log",
  "intent_log_pool": "us-west.rgw.intent-log",
  "usage_log_pool": "us-west.rgw.usage",
  "user_keys_pool": "us-west.rgw.users.keys",
  "user_email_pool": "us-west.rgw.users.email",
  "user_swift_pool": "us-west.rgw.users.swift",
  "user_uid_pool": "us-west.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-west.rgw.buckets.index",
              "data_pool": "us-west.rgw.buckets.data",
              "data_extra_pool": "us-west.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-west.rgw.meta",
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.period">
    <title>Update the Period</title>
    <para>
     To propagate the zonegroup map changes, we update and commit the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit --rgw-zone=us-west
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 3,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [
      "", # truncated
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "d9522067-cb7b-4129-8751-591e45815b16",
                      "name": "us-west",
                      "endpoints": [
                          "http:\/\/rgw3:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }
              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          },
          {
              "key": "d9522067-cb7b-4129-8751-591e45815b16",
              "val": 329470157
          }
      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
    <para>
     Note that the period epoch number has incremented, indicating a change in
     the configuration.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.fed.seccluster.rgwstart">
    <title>Start the &rgw;</title>
    <para>
     This is similar to starting the &rgw; in the first zone. The only
     difference is that the &rgw; zone configuration should reflect the
     <literal>us-west</literal> zone name:
    </para>
<screen>[client.rgw.us-west]
rgw_frontends="civetweb port=80"
rgw_zone=us-west</screen>
    <para>
     Start the second &rgw;:
    </para>
<screen>sudo systemctl start ceph-radosgw@rgw.us-west</screen>
   </sect3>
  </sect2>
  <sect2 xml:id="ceph.rgw.fed.failover">
   <title>Failover and Disaster Recovery</title>
   <para>
    If the master zone should fail, failover to the secondary zone for disaster recovery.
   </para>
   <procedure>
    <step>
     <para>
      Make the secondary zone the master and default zone. For example:
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default
     </screen>
     <para>
      By default, Ceph Object Gateway will run in an active-active configuration.
      If the cluster was configured to run in an active-passive configuration,
      the secondary zone is a read-only zone. Remove the --read-only status to allow
      the zone to receive write operations. For example:
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default \
--read-only=False
     </screen>
    </step>
    <step>
     <para>
      Update the period to make the changes take effect.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Finally, restart the Ceph Object Gateway.
     </para>
     <screen>
&prompt.root;<command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
   </procedure>
   <para>
    If the former master zone recovers, revert the operation.
   </para>
   <procedure>
    <step>
     <para>
      From the recovered zone, pull the period from the current master zone.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> period pull --url={url-to-master-zone-gateway} \
--access-key={access-key} --secret={secret}
     </screen>
    </step>
    <step>
     <para>
      Make the recovered zone the master and default zone.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --master --default
     </screen>
    </step>
    <step>
     <para>
      Update the period to make the changes take effect.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Then, restart the Ceph Object Gateway in the recovered zone.
     </para>
     <screen>
&prompt.root;<command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
    <step>
     <para>
      If the secondary zone needs to be a read-only configuration, update the secondary zone.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --read-only
     </screen>
    </step>
    <step>
     <para>
      Update the period to make the changes take effect.
     </para>
     <screen>
&prompt.root;<command>radosgw-admin</command> period update --commit
     </screen>
    </step>
    <step>
     <para>
      Finally, restart the Ceph Object Gateway in the secondary zone.
     </para>
     <screen>
&prompt.root;<command>systemctl</command> restart ceph-radosgw@rgw.`hostname -s`
     </screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.sync">
  <title>Sync Modules</title>
  <para>
   The <emphasis>multisite</emphasis> functionality of &rgw; introduced in Jewel allowes to
   create multiple zones and mirror data and metadata between them.
   <emphasis>Sync Modules</emphasis> are built atop of the multisite
   framework that allows for forwarding data and metadata to a different
   external tier. A sync module allows for a set of actions
   to be performed whenever a change in data occurs (metadata ops like bucket or
   user creation etc. are also regarded as changes in data). As the rgw multisite
   changes are eventually consistent at remote sites, changes are propagated
   asynchronously. This would allow for unlocking use cases such as backing up the
   object storage to an external cloud cluster or a custom backup solution using
   tape drives, indexing metadata in Elasticsearch etc.
  </para>

  <sect2 xml:id="ceph.rgw.sync.zones">
   <title>Synchronizing Zones</title>
   <para>
    A sync module configuration is local to a zone. The sync module determines
    whether the zone exports data or can only consume data that was modified in
    another zone. As of luminous the supported sync plug-ins are <literal>elasticsearch</literal>,
    <literal>rgw</literal>, which is the default sync plug-in that synchronises data between the
    zones and <literal>log</literal> which is a trivial sync plug-in that logs the metadata
    operation that happens in the remote zones. The following sections are written with
    the example of a zone using <literal>elasticsearch</literal> sync module. The process would
    be similar for configuring any other sync plug-in.
   </para>
   <note>
    <title>Default Sync Plugin</title>
    <para>
     <literal>rgw</literal> is the default sync plug-in and there is no need to explicitly
     configure this.
    </para>
   </note>
   <sect3 xml:id="ceph.rgw.sync.zones.req">
   <title>Requirements and Assumptions</title>
   <para>
    Let us assume a simple multisite configuration as described in <xref linkend="ceph.rgw.fed" />
    consisting of the 2 zones <literal>us-east</literal> and <literal>us-west</literal>. Now we add
    a third zone <literal>us-east-es</literal> which is a zone that only processes metadata from
    the other sites. This zone can be in the same or a different &ceph; cluster than
    <literal>us-east</literal>. This zone would only consume metadata from other zones and
    &rgw;s in this zone will not serve any end user requests directly.
    </para>
   </sect3>
   <sect3 xml:id="ceph.rgw.sync.zones.configure">
    <title>Configuring Sync Modules</title>
    <procedure>
     <step>
      <para>
       Create the third zone similar to the ones described in
       <xref linkend="ceph.rgw.fed" />, for example
      </para>
      <screen>
&prompt.root;<command>radosgw-admin</command> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key={system-key} --secret={secret} --endpoints=http://rgw-es:80
      </screen>
     </step>
     <step>
      <para>
       A sync module can be configured for this zone via the following
      </para>
      <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --tier-type={tier-type} \
--tier-config={set of key=value pairs}
      </screen>
     </step>
     <step>
      <para>
       For example in the <literal>elasticsearch</literal> sync module
      </para>
      <screen>
&prompt.root;<command>radosgw-admin</command> zone modify --rgw-zone={zone-name} --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1
      </screen>
      <para>
       For the various supported tier-config options refer to <xref linkend="ceph.rgw.sync.elastic" />.
      </para>
     </step>
     <step>
      <para>
       Finally update the period
      </para>
      <screen>
&prompt.root;<command>radosgw-admin</command> period update --commit
      </screen>
     </step>
     <step>
      <para>
       Now start the radosgw in the zone
      </para>
      <screen>
&prompt.root;<command>systemctl</command> start ceph-radosgw@rgw.`hostname -s`
&prompt.root;<command>systemctl</command> enable ceph-radosgw@rgw.`hostname -s`
      </screen>
     </step>
    </procedure>
   </sect3>
  </sect2>
  <sect2 xml:id="ceph.rgw.sync.elastic">
   <title>Storing Metadata in Elasticsearch</title>
   <para>
    This sync module writes the metadata from other zones to Elasticsearch. As of
    luminous this is JSON of data fields we currently store in Elasticsearch.
   </para>
   <screen>
{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}
   </screen>
   <sect3 xml:id="ceph.rgw.sync.elastic.config">
    <title>Elasticsearch Tier Type Configuration Parameters</title>
    <variablelist>
     <varlistentry>
      <term>endpoint</term>
      <listitem>
       <para>
        Specifies the Elasticsearch server endpoint to access.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_shards</term>
      <listitem>
       <para>
        <emphasis>(integer)</emphasis>
        The number of shards that Elasticsearch will be configured with on
        data sync initialization. Note that this cannot be changed after init.
        Any change here requires rebuild of the Elasticsearch index and reinit
        of the data sync process.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_replicas</term>
      <listitem>
       <para>
        <emphasis>(integer)</emphasis>
        The number of the replicas that Elasticsearch will be configured with
        on data sync initialization.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>explicit_custom_meta</term>
      <listitem>
       <para>
        <emphasis>(true | false)</emphasis>
        Specifies whether all user custom metadata will be indexed, or whether
        user will need to configure (at the bucket level) what custome
        metadata entries should be indexed. This is false by default
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>index_buckets_list</term>
      <listitem>
       <para>
        <emphasis>(comma separated list of strings)</emphasis>
        If empty, all buckets will be indexed. Otherwise, only buckets
        specified here will be indexed. It is possible to provide bucket
        prefixes (e.g., foo*), or bucket suffixes (e.g., *bar).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>approved_owners_list</term>
      <listitem>
       <para>
        <emphasis>(comma separated list of strings)</emphasis>
        If empty, buckets of all owners will be indexed (subject to other
        restrictions), otherwise, only buckets owned by specified owners will
        be indexed. Suffixes and prefixes can also be provided.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>override_index_path</term>
      <listitem>
       <para>
        <emphasis>(string)</emphasis>
        if not empty, this string will be used as the elasticsearch index
        path. Otherwise the index path will be determined and generated on
        sync initialization.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph.rgw.sync.elastic.query">
    <title>Metadata Queries</title>
    <para>
     Since the Elasticsearch cluster now stores object metadata, it is important that
     the Elasticsearch endpoint is not exposed to the public and only accessible to
     the cluster administrators. For exposing metadata queries to the end user itself
     this poses a problem since we'd want the user to only query their metadata and
     not of any other users, this would require the Elasticsearch cluster to
     authenticate users in a way similar to RGW does which poses a problem.
    </para>
    <para>
     As of Luminous RGW in the metadata master zone can now service end user
     requests. This allows for not exposing the Elasticsearch endpoint in public and
     also solves the authentication and authorization problem since RGW itself can
     authenticate the end user requests. For this purpose RGW introduces a new query
     in the bucket APIs that can service Elasticsearch requests. All these requests
     must be sent to the metadata master zone.
    </para>
    <variablelist>
     <varlistentry>
      <term>Get an Elasticsearch Query</term>
      <listitem>
       <screen>
GET /<replaceable>BUCKET</replaceable>?query={query-expr}
       </screen>
       <para>
        request params:
       </para>
        <itemizedlist>
         <listitem>
          <para>
           max-keys: max number of entries to return
          </para>
         </listitem>
         <listitem>
          <para>
           marker: pagination marker
          </para>
         </listitem>
        </itemizedlist>
       <screen>
expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]
       </screen>
       <para>
         op is one of the following: &lt;, &lt;=, ==, &gt;=, &gt;
       </para>
       <para>
         For example:
       </para>
       <screen>
GET /?query=name==foo
       </screen>
       <para>
        Will return all the indexed keys that user has read permission to, and
        are named 'foo'. The output will be a list of keys in XML that is similar
        to the S3 list buckets response.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Configure custom metadata fields</term>
      <listitem>
       <para>
        Define which custom metadata entries should be indexed (under the
        specified bucket), and what are the types of these keys. If explicit
        custom metadata indexing is configured, this is needed so that rgw
        will index the specified custom metadata values. Otherwise it is
        needed in cases where the indexed metadata keys are of a type other
        than string.
       </para>
       <screen>
POST /<replaceable>BUCKET</replaceable>?mdsearch
x-amz-meta-search: &lt;key &#91;&#59; type&#93;&gt; &#91;, ...&#93;
       </screen>
       <para>
        Multiple metadata fields must be comma seperated, a type can be forced for a
        field with a `;`. The currently allowed types are string(default), integer and
        date eg. if you want to index a custom object metadata x-amz-meta-year as int,
        x-amz-meta-date as type date and x-amz-meta-title as string, you'd do
       </para>
       <screen>
POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Delete custom metadata configuration</term>
      <listitem>
       <para>
        Delete custom metadata bucket configuration.
       </para>
       <screen>
DELETE /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Get custom metadata configuration</term>
      <listitem>
       <para>
        Retrieve custom metadata bucket configuration.
       </para>
       <screen>
GET /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.rgw.ldap">
  <title>LDAP Authentication</title>

  <para>
   Apart form the default local user authentication, &rgw; can utilize LDAP
   server services to authenticate users as well.
  </para>

  <sect2 xml:id="ceph.rgw.ldap.how_works">
   <title>Authentication Mechanism</title>
   <para>
    The &rgw; extracts the user's LDAP credentials from a token. A search
    filter is constructed from the user name. The &rgw; uses the configured
    service account to search the directory for a matching entry. If an entry
    is found, the &rgw; attempts to bind to the found distinguished name with
    the password from the token. If the credentials are valid, the bind will
    succeed, and the &rgw; grants access.
   </para>
   <para>
    You can limit the allowed users by setting the base for the search to a
    specific organizational unit or by specifying a custom search filter, for
    example requiring specific group membership, custom object classes, or
    attributes.
   </para>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.reqs">
   <title>Requirements</title>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>LDAP or Active Directory</emphasis>: A running LDAP instance
      accessible by the &rgw;.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Service account</emphasis>: LDAP credentials to be used by the
      &rgw; with search permissions.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>User account</emphasis>: At least one user account in the LDAP
      directory.
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <title>Do Not Overlap LDAP and Local Users</title>
    <para>
     You should not use the same user names for local users and for users being
     authenticated by using LDAP. The &rgw; cannot distinguish them and it
     treats them as the same user.
    </para>
   </important>
   <tip>
    <title>Sanity Checks</title>
    <para>
     Use the <command>ldapsearch</command> utility to verify the service
     account or the LDAP connection. For example:
    </para>
<screen>ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</screen>
    <para>
     Make sure to use the same LDAP parameters as in the &ceph; configuration
     file to eliminate possible problems.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.config">
   <title>Configure &rgw; to Use LDAP Authentication</title>
   <para>
    The following parameters in the <filename>/etc/ceph/ceph.conf</filename>
    configuration file are related to the LDAP authentication:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>rgw_ldap_uri</option>
     </term>
     <listitem>
      <para>
       Specifies the LDAP server to use. Make sure to use the
       <literal>ldaps://<replaceable>fqdn</replaceable>:<replaceable>port</replaceable></literal>
       parameter to avoid transmitting the plain text credentials openly.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_binddn</option>
     </term>
     <listitem>
      <para>
       The Distinguished Name (DN) of the service account used by the &rgw;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_secret</option>
     </term>
     <listitem>
      <para>
       The password for the service account.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw_ldap_searchdn</term>
     <listitem>
      <para>
       Specifies the base in the directory information tree for searching
       users. This might be your users organizational unit or some more
       specific Organizational Unit (OU).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_dnattr</option>
     </term>
     <listitem>
      <para>
       The attribute being used in the constructed search filter to match a
       user name. Depending on your Directory Information Tree (DIT) this would
       probably be <literal>uid</literal> or <literal>cn</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_search_filter</option>
     </term>
     <listitem>
      <para>
       If not specified, the &rgw; automatically constructs the search filter
       with the <option>rgw_ldap_dnattr</option> setting. Use this parameter to
       narrow the list of allowed users in very flexible ways. Consult
       <xref
        linkend="ceph.rgw.ldap.filter"/> for details.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.filter">
   <title>Using a Custom Search Filter to Limit User Access</title>
   <para>
    There are two ways you can use the <option>rgw_search_filter</option>
    parameter.
   </para>
   <sect3>
    <title>Partial Filter to Further Limit the Constructed Search Filter</title>
    <para>
     An example of a partial filter:
    </para>
<screen>"objectclass=inetorgperson"</screen>
    <para>
     The &rgw; will generate the search filter as usual with the user name from
     the token and the value of <option>rgw_ldap_dnattr</option>. The
     constructed filter is then combined with the partial filter from the
     <option>rgw_search_filter</option> attribute. Depending on the user name
     and the settings the final search filter may become:
    </para>
<screen>"(&amp;(uid=hari)(objectclass=inetorgperson))"</screen>
    <para>
     In that case, user 'hari' will only be granted access if he is found in
     the LDAP directory, has an object class of 'inetorgperson', and did
     specify a valid password.
    </para>
   </sect3>
   <sect3>
    <title>Complete Filter</title>
    <para>
     A complete filter must contain a <option>USERNAME</option> token which
     will be substituted with the user name during the authentication attempt.
     The <option>rgw_ldap_dnattr</option> parameter is not used anymore in this
     case. For example, to limit valid users to a specific group, use the
     following filter:
    </para>
<screen>"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</screen>
    <note>
     <title><literal>memberOf</literal> Attribute</title>
     <para>
      Using the <literal>memberOf</literal> attribute in LDAP searches requires
      server side support from you specific LDAP server implementation.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.rgw.ldap.token">
   <title>Generating an Access Token for LDAP authentication</title>
   <para>
    The <command>radosgw-token</command> utility generates the access token
    based on the LDAP user name and password. It outputs a base-64 encoded
    string which is the actual access token. Use your favorite S3 client (refer
    to <xref linkend="accessing.ragos.gateway"/>) and
    specify the token as the access key and use an empty secret key.
   </para>
<screen>&prompt.sminion;export RGW_ACCESS_KEY_ID="<replaceable>username</replaceable>"
&prompt.sminion;export RGW_SECRET_ACCESS_KEY="<replaceable>password</replaceable>"
&prompt.sminion;radosgw-token --encode --ttype=ldap</screen>
   <important>
    <title>Clear Text Credentials</title>
    <para>
     The access token is a base-64 encoded JSON structure and contains the LDAP
     credentials as a clear text.
    </para>
   </important>
   <note>
    <title>Active Directory</title>
    <para>
     For Active Directory, use the <option>--ttype=ad</option> parameter.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.rgw.configuration">
  <title>Configuration Parameters</title>
  <para>
   A full list of configuration parameters in the <filename>ceph.conf</filename>
   for &rgw; is available at
   <link xlink:href="http://docs.ceph.com/docs/luminous/radosgw/config-ref/"/>.
  </para>
  <sect2 xml:id="sec.ceph.rgw.configuration.notes">
   <title>Additional Notes</title>
   <variablelist>
    <varlistentry>
     <term>rgw dns name</term>
     <listitem>
      <para>
       If the parameter <literal>rgw dns name</literal> is added to the
       <filename>ceph.conf</filename>, make sure that the S3 client is configured
       to direct requests at the endpoint specified by <literal>rgw dns name</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
