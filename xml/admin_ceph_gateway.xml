<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-gw">
<!-- ============================================================== -->
 <title>&cogw;</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter introduces details about administration tasks related to &ogw;,
  such as checking status of the service, managing accounts, multisite
  gateways, or LDAP authentication.
 </para>
 <sect1 xml:id="sec-ceph-rgw-limits">
  <title>&ogw; Restrictions and Naming Limitations</title>

  <para>
   Following is a list of important &ogw; limits:
  </para>

  <sect2 xml:id="ogw-limits-bucket">
   <title>Bucket Limitations</title>
   <para>
    When approaching &ogw; via the S3 API, bucket names are limited to
    DNS-compliant names with a dash character '-' allowed. When approaching
    &ogw; via the &swift; API, you may use any combination of UTF-8 supported
    characters except for a slash character '/'. The maximum length of a bucket
    name is 255 characters. Bucket names must be unique.
   </para>
   <tip>
    <title>Use DNS-compliant Bucket Names</title>
    <para>
     Although you may use any UTF-8 based bucket name via the &swift; API, it
     is recommended to name buckets with regard to the S3 naming limitations to
     avoid problems accessing the same bucket via the S3 API.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ogw-limits-object">
   <title>Stored Object Limitations</title>
   <variablelist>
    <varlistentry>
     <term>Maximum number of objects per user</term>
     <listitem>
      <para>
       No restriction by default (limited by ~ 2^63).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Maximum number of objects per bucket</term>
     <listitem>
      <para>
       No restriction by default (limited by ~ 2^63).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Maximum size of an object to upload/store</term>
     <listitem>
      <para>
       Single uploads are restricted to 5 GB. Use multipart for larger object
       sizes. The maximum number of multipart chunks is 10000.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ogw-limits-http">
   <title>HTTP Header Limitations</title>
   <para>
    HTTP header and request limitation depend on the Web front-end used. The
    default Beast restricts the size of the HTTP header to 16 kB.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw-deploy">
  <title>Deploying the &ogw;</title>

  <para>
   The recommended way of deploying the &cogw; is via the &deepsea;
   infrastructure by adding the relevant <literal>role-rgw [...]</literal>
   line(s) into the <filename>policy.cfg</filename> file on the &smaster;, and
   running the required &deepsea; stages.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     To include the &ogw; during the &ceph; cluster deployment process, refer
     to <xref linkend="ceph-install-stack"/> and
     <xref
    linkend="policy-configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     To add the &ogw; role to an already deployed cluster, refer to
     <xref linkend="salt-adding-services"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-rgw-operating">
  <title>Operating the &ogw; Service</title>

  <para>
   The &ogw; service is operated with the <command>systemctl</command> command.
   You need to have &rootuser; privileges to operate the &ogw; service. Note
   that <replaceable>GATEWAY_HOST</replaceable> is the host name of the server
   whose &ogw; instance you need to operate.
  </para>

  <para>
   The following subcommands are supported for the &ogw; service:
  </para>

  <variablelist>
   <varlistentry>
    <term>systemctl status ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Prints the status information of the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl start ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Starts the service if it is not already running.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl restart ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Restarts the service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl stop ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Stops the running service.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl enable ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Enables the service so that it is automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>systemctl disable ceph-radosgw@rgw.<replaceable>GATEWAY_HOST</replaceable></term>
    <listitem>
     <para>
      Disables the service so that it is not automatically started on system
      start-up.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ogw-config-parameters">
  <title>Configuration Options</title>

  <para>
   Refer to <xref linkend="config-ogw" /> for a list of &ogw; configuration
   options.
  </para>
 </sect1>
 <sect1 xml:id="ceph-rgw-access">
  <title>Managing &ogw; Access</title>

  <para>
   You can communicate with &ogw; using either S3- or &swift;-compatible
   interface. S3 interface is compatible with a large subset of the Amazon S3
   RESTful API. &swift; interface is compatible with a large subset of the
   &ostack; &swift; API.
  </para>

  <para>
   Both interfaces require you to create a specific user, and install the
   relevant client software to communicate with the gateway using the user's
   secret key.
  </para>

  <sect2 xml:id="accessing-ragos-gateway">
   <title>Accessing &ogw;</title>
   <sect3>
    <title>S3 Interface Access</title>
    <para>
     To access the S3 interface, you need a REST client.
     <command>S3cmd</command> is a command line S3 client. You can find it in
     the
     <link
      xlink:href="https://build.opensuse.org/package/show/Cloud:Tools/s3cmd">OpenSUSE
     Build Service</link>. The repository contains versions for both &sle; and
     &opensuse; based distributions.
    </para>
    <para>
     If you want to test your access to the S3 interface, you can also write a
     small a Python script. The script will connect to &ogw;, create a new
     bucket, and list all buckets. The values for
     <option>aws_access_key_id</option> and
     <option>aws_secret_access_key</option> are taken from the values of
     <option>access_key</option> and <option>secret_key</option> returned by
     the <command>radosgw_admin</command> command from
     <xref
      linkend="adding-s3-swift-users"/>.
    </para>
    <procedure>
     <step>
      <para>
       Install the <systemitem>python-boto</systemitem> package:
      </para>
<screen>&prompt.root;zypper in python-boto</screen>
     </step>
     <step>
      <para>
       Create a new Python script called <filename>s3test.py</filename> with
       the following content:
       <remark role="fixme">Provide script in RPM? Is it really necessary to create pool? This script is not necessary at all, remove it from documentation?</remark>
      </para>
<screen>import boto
import boto.s3.connection
access_key = '11BS02LGFB6AL6H1ADMW'
secret_key = 'vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY'
conn = boto.connect_s3(
aws_access_key_id = access_key,
aws_secret_access_key = secret_key,
host = '<replaceable>HOSTNAME</replaceable>',
is_secure=False,
calling_format = boto.s3.connection.OrdinaryCallingFormat(),
)
bucket = conn.create_bucket('my-new-bucket')
for bucket in conn.get_all_buckets():
  print "<replaceable>NAME</replaceable>\t<replaceable>CREATED</replaceable>".format(
  name = bucket.name,
  created = bucket.creation_date,
  )</screen>
      <para>
       Replace <literal><replaceable>HOSTNAME</replaceable></literal> with the
       host name of the host where you configured the &ogw; service, for
       example <literal>gateway_host</literal>.
      </para>
     </step>
     <step>
      <para>
       Run the script:
      </para>
<screen>python s3test.py</screen>
      <para>
       The script outputs something like the following:
      </para>
<screen>my-new-bucket 2015-07-22T15:37:42.000Z</screen>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Swift Interface Access</title>
    <para>
     To access &ogw; via Swift interface, you need the <command>swift</command>
     command line client. Its manual page <command>man 1 swift</command> tells
     you more about its command line options.
    </para>
    <para>
     The package is included in the 'Public Cloud' module for &sle; 12 from SP3
     and &sle; 15. Before installing the package, you need to activate the
     module and refresh the software repository:
    </para>
<screen>&prompt.root;SUSEConnect -p sle-module-public-cloud/12/<replaceable>SYSTEM-ARCH</replaceable>
sudo zypper refresh</screen>
    <para>
     Or
    </para>
<screen>&prompt.root;SUSEConnect -p sle-module-public-cloud/15/<replaceable>SYSTEM-ARCH</replaceable>
&prompt.root;zypper refresh</screen>
    <para>
     To install the <command>swift</command> command, run the following:
    </para>
<screen>&prompt.root;zypper in python-swiftclient</screen>
    <para>
     The swift access uses the following syntax:
    </para>
<screen>&prompt.user;swift -A http://<replaceable>IP_ADDRESS</replaceable>/auth/1.0 \
-U example_user:swift -K '<replaceable>SWIFT_SECRET_KEY</replaceable>' list</screen>
    <para>
     Replace <replaceable>IP_ADDRESS</replaceable> with the IP address of the
     gateway server, and <replaceable>SWIFT_SECRET_KEY</replaceable> with its
     value from the output of the <command>radosgw-admin key create</command>
     command executed for the <systemitem>swift</systemitem> user in
     <xref linkend="adding-s3-swift-users"/>.
    </para>
    <para>
     For example:
    </para>
<screen>&prompt.user;swift -A http://gateway.example.com/auth/1.0 -U example_user:swift \
-K 'r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h' list</screen>
    <para>
     The output is:
    </para>
<screen>my-new-bucket</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="s3-swift-accounts-managment">
   <title>Managing S3 and &swift; Accounts</title>
   <sect3 xml:id="adding-s3-swift-users">
    <title>Adding S3 and &swift; Users</title>
    <para>
     You need to create a user, access key and secret to enable end users to
     interact with the gateway. There are two types of users: a
     <emphasis>user</emphasis> and <emphasis>subuser</emphasis>. While
     <emphasis>users</emphasis> are used when interacting with the S3
     interface, <emphasis>subusers</emphasis> are users of the &swift;
     interface. Each subuser is associated to a user.
    </para>
    <para>
     Users can also be added via the &deepsea; file
     <filename>rgw.sls</filename>. For an example, see
     <xref linkend="ceph-nfsganesha-customrole-rgw-multiusers"/>.
    </para>
    <para>
     To create a &swift; user, follow the steps:
    </para>
    <procedure>
     <step>
      <para>
       To create a &swift; user&mdash;which is a <emphasis>subuser</emphasis>
       in our terminology&mdash;you need to create the associated
       <emphasis>user</emphasis> first.
      </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=<replaceable>USERNAME</replaceable> \
 --display-name="<replaceable>DISPLAY-NAME</replaceable>" --email=<replaceable>EMAIL</replaceable></screen>
      <para>
       For example:
      </para>
<screen>&prompt.cephuser;radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
     </step>
     <step>
      <para>
       To create a subuser (&swift; interface) for the user, you must specify
       the user ID (--uid=<replaceable>USERNAME</replaceable>), a subuser ID,
       and the access level for the subuser.
      </para>
<screen>&prompt.cephuser;radosgw-admin subuser create --uid=<replaceable>UID</replaceable> \
 --subuser=<replaceable>UID</replaceable> \
 --access=[ <replaceable>read | write | readwrite | full</replaceable> ]</screen>
      <para>
       For example:
      </para>
<screen>&prompt.cephuser;radosgw-admin subuser create --uid=example_user \
 --subuser=example_user:swift --access=full</screen>
     </step>
     <step>
      <para>
       Generate a secret key for the user.
      </para>
<screen>&prompt.cephuser;radosgw-admin key create \
   --gen-secret \
   --subuser=example_user:swift \
   --key-type=swift</screen>
     </step>
     <step>
      <para>
       Both commands will output JSON-formatted data showing the user state.
       Notice the following lines, and remember the
       <literal>secret_key</literal> value:
      </para>
<screen>"swift_keys": [
   { "user": "example_user:swift",
     "secret_key": "r5wWIxjOCeEO7DixD1FjTLmNYIViaC6JVhi3013h"}],</screen>
     </step>
    </procedure>
    <para/>
    <para>
     When accessing &ogw; through the S3 interface you need to create an S3
     user by running:
    </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=<replaceable>USERNAME</replaceable> \
 --display-name="<replaceable>DISPLAY-NAME</replaceable>" --email=<replaceable>EMAIL</replaceable></screen>
    <para>
     For example:
    </para>
<screen>&prompt.cephuser;radosgw-admin user create \
   --uid=example_user \
   --display-name="Example User" \
   --email=penguin@example.com</screen>
    <para>
     The command also creates the user's access and secret key. Check its
     output for <literal>access_key</literal> and <literal>secret_key</literal>
     keywords and their values:
    </para>
<screen>[...]
 "keys": [
       { "user": "example_user",
         "access_key": "11BS02LGFB6AL6H1ADMW",
         "secret_key": "vzCEkuryfn060dfee4fgQPqFrncKEIkh3ZcdOANY"}],
 [...]</screen>
   </sect3>
   <sect3 xml:id="removing-s3-swift-users">
    <title>Removing S3 and &swift; Users</title>
    <para>
     The procedure for deleting users is similar for S3 and &swift; users. But
     in case of &swift; users you may need to delete the user including its
     subusers.
    </para>
    <para>
     To remove a S3 or &swift; user (including all its subusers), specify
     <option>user rm</option> and the user ID in the following command:
    </para>
<screen>&prompt.cephuser;radosgw-admin user rm --uid=example_user</screen>
    <para>
     To remove a subuser, specify <option>subuser rm</option> and the subuser
     ID.
    </para>
<screen>&prompt.cephuser;radosgw-admin subuser rm --uid=example_user:swift</screen>
    <para>
     You can make use of the following options:
    </para>
    <variablelist>
     <varlistentry>
      <term>--purge-data</term>
      <listitem>
       <para>
        Purges all data associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>--purge-keys</term>
      <listitem>
       <para>
        Purges all keys associated to the user ID.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <tip>
     <title>Removing a Subuser</title>
     <para>
      When you remove a subuser, you are removing access to the Swift
      interface. The user will remain in the system.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="changing-s3-swift-users-password">
    <title>Changing S3 and &swift; User Access and Secret Keys</title>
    <para>
     The <literal>access_key</literal> and <literal>secret_key</literal>
     parameters identify the &ogw; user when accessing the gateway. Changing
     the existing user keys is the same as creating new ones, as the old keys
     get overwritten.
    </para>
    <para>
     For S3 users, run the following:
    </para>
<screen>&prompt.cephuser;radosgw-admin key create --uid=<replaceable>EXAMPLE_USER</replaceable> --key-type=s3 --gen-access-key --gen-secret</screen>
    <para>
     For &swift; users, run the following:
    </para>
<screen>&prompt.cephuser;radosgw-admin key create --subuser=<replaceable>EXAMPLE_USER</replaceable>:swift --key-type=swift --gen-secret</screen>
    <variablelist>
     <varlistentry>
      <term><option>--key-type=<replaceable>TYPE</replaceable></option></term>
      <listitem>
       <para>
        Specifies the type of key. Either <literal>swift</literal> or
        <literal>s3</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-access-key</option></term>
      <listitem>
       <para>
        Generates a random access key (for S3 user by default).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--gen-secret</option></term>
      <listitem>
       <para>
        Generates a random secret key.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--secret=<replaceable>KEY</replaceable></option></term>
      <listitem>
       <para>
        Specifies a secret key, for example manually generated.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="user-quota-managment">
    <title>User Quota Management</title>
    <para>
     The &cogw; enables you to set quotas on users and buckets owned by
     users. Quotas include the maximum number of objects in a bucket and the
     maximum storage size in megabytes.
    </para>
    <para>
     Before you enable a user quota, you first need to set its parameters:
    </para>
<screen>&prompt.cephuser;radosgw-admin quota set --quota-scope=user --uid=<replaceable>EXAMPLE_USER</replaceable> \
 --max-objects=1024 --max-size=1024</screen>
    <variablelist>
     <varlistentry>
      <term><option>--max-objects</option></term>
      <listitem>
       <para>
        Specifies the maximum number of objects. A negative value disables the
        check.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--max-size</option></term>
      <listitem>
       <para>
        Specifies the maximum number of bytes. A negative value disables the
        check.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>--quota-scope</option></term>
      <listitem>
       <para>
        Sets the scope for the quota. The options are <literal>bucket</literal>
        and <literal>user</literal>. Bucket quotas apply to buckets a user
        owns. User quotas apply to a user.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Once you set a user quota, you may enable it:
    </para>
<screen>&prompt.cephuser;radosgw-admin quota enable --quota-scope=user --uid=<replaceable>EXAMPLE_USER</replaceable></screen>
    <para>
     To disable a quota:
    </para>
<screen>&prompt.cephuser;radosgw-admin quota disable --quota-scope=user --uid=<replaceable>EXAMPLE_USER</replaceable></screen>
    <para>
     To list quota settings:
    </para>
<screen>&prompt.cephuser;radosgw-admin user info --uid=<replaceable>EXAMPLE_USER</replaceable></screen>
    <para>
     To update quota statistics:
    </para>
<screen>&prompt.cephuser;radosgw-admin user stats --uid=<replaceable>EXAMPLE_USER</replaceable> --sync-stats</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw-http-frontends">
  <title>HTTP Front-ends</title>

  <para>
   The &cogw; supports two embedded HTTP front-ends: <emphasis>Beast</emphasis>
   and <emphasis>Civetweb</emphasis>.
  </para>

  <para>
   The Beast front-end uses the Boost.Beast library for HTTP parsing and the
   Boost.Asio library for asynchronous network I/O.
  </para>

  <para>
   The Civetweb front-end uses the Civetweb HTTP library, which is a fork of
   Mongoose.
  </para>

  <para>
   You can configure them with the <option>rgw_frontends</option> option in the
   <filename>/etc/ceph/ceph.conf</filename> file. Refer to
   <xref linkend="config-ogw" /> for a list of configuration options.
  </para>
 </sect1>
 <sect1 xml:id="ceph-rgw-https">
  <title>Enabling HTTPS/SSL for &ogw;s</title>

  <para>
   To enable the default &ogw; role to communicate securely using SSL, you need
   to either have a CA issued certificate or create a self-signed one. There
   are two ways to configure &ogw; with HTTPS enabled&mdash;a simple way that
   makes use of the default settings, and an advanced way that lets you fine
   tune HTTPS related settings.
  </para>

  <sect2 xml:id="ogw-selfcert">
   <title>Create a Self-Signed Certificate</title>
   <tip>
    <para>
     Skip this section if you already have a valid certificate signed by CA.
    </para>
   </tip>
   <para>
    By default, &deepsea; expects the certificate file in
    <filename>/srv/salt/ceph/rgw/cert/rgw.pem</filename> on the &smaster;. It
    will then distribute the certificate to
    <filename>/etc/ceph/rgw.pem</filename> on the &sminion; with the &ogw;
    role, where &ceph; reads it.
   </para>
   <para>
    The following procedure describes how to generate a self-signed SSL
    certificate on the &smaster;.
   </para>
   <procedure>
    <step>
     <para>
      If you need your &ogw; to be known by additional subject identities, add
      them to the <option>subjectAltName</option> option in the
      <literal>[v3_req]</literal> section of the
      <filename>/etc/ssl/openssl.cnf</filename> file:
     </para>
<screen>
[...]
[ v3_req ]
subjectAltName = DNS:server1.example.com DNS:server2.example.com
[...]
</screen>
     <tip>
      <title>IP Addresses in <option>subjectAltName</option></title>
      <para>
       To use IP addresses instead of domain names in the
       <option>subjectAltName</option> option, replace the example line with
       the following:
      </para>
<screen>
subjectAltName = IP:10.0.0.10 IP:10.0.0.11
</screen>
     </tip>
    </step>
    <step>
     <para>
      Create the key and the certificate using <command>openssl</command>.
      Enter all data you need to include in your certificate. We recommend
      entering the FQDN as the common name. Before signing the certificate,
      verify that 'X509v3 Subject Alternative Name:' is included in requested
      extensions, and that the resulting certificate has "X509v3 Subject
      Alternative Name:" set.
     </para>
<screen>
&prompt.smaster;openssl req -x509 -nodes -days 1095 \
 -newkey rsa:4096 -keyout rgw.key -out /srv/salt/ceph/rgw/cert/rgw.pem
</screen>
    </step>
    <step>
     <para>
      Append the key to the certificate file:
     </para>
<screen>
&prompt.smaster;cat rgw.key >> /srv/salt/ceph/rgw/cert/rgw.pem
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw-ssl-simple">
   <title>Simple HTTPS Configuration</title>
   <para>
    By default, &ceph; on the &ogw; node reads the
    <filename>/etc/ceph/rgw.pem</filename> certificate, and uses port 443 for
    secure SSL communication. If you do not need to change these values, follow
    these steps:
   </para>
   <procedure>
    <step>
     <para>
      Edit <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
      following line:
     </para>
<screen>
rgw_init: default-ssl
</screen>
    </step>
    <step>
     <para>
      Copy the default &ogw; SSL configuration to the
      <filename>ceph.conf.d</filename> subdirectory:
     </para>
<screen>
&prompt.smaster;cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf
</screen>
    </step>
    <step>
     <para>
      Run &deepsea; stages 2, 3, and 4 to apply the changes:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.2
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw-ssl-advanced">
   <title>Advanced HTTPS Configuration</title>
   <para>
    If you need to change the default values for SSL settings of the &ogw;,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Edit <filename>/srv/pillar/ceph/stack/global.yml</filename> and add the
      following line:
     </para>
<screen>
rgw_init: default-ssl
</screen>
    </step>
    <step>
     <para>
      Copy the default &ogw; SSL configuration to the
      <filename>ceph.conf.d</filename> subdirectory:
     </para>
<screen>
&prompt.smaster;cp /srv/salt/ceph/configuration/files/rgw-ssl.conf \
 /srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf
</screen>
    </step>
    <step>
     <para>
      Edit
      <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/rgw.conf</filename>
      and change the default options, such as port number or path to the SSL
      certificate, to reflect your setup.
     </para>
    </step>
    <step>
     <para>
      Run &deepsea; stage 3 and 4 to apply the changes:
     </para>
<screen>
&prompt.smaster;salt-run state.orch ceph.stage.3
&prompt.smaster;salt-run state.orch ceph.stage.4
</screen>
    </step>
   </procedure>
   <tip xml:id="rgw-webserver-multiport">
    <title>Binding to Multiple Ports</title>
    <para>
     The Beast server can bind to multiple ports. This is useful if you need to
     access a single &ogw; instance with both SSL and non-SSL connections. A
     two-port configuration line example follows:
    </para>
<screen>[client.{{ client }}]
rgw_frontends = beast port=80 ssl_port=443 ssl_certificate=/etc/ceph/rgw.pem</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rgw-sync">
  <title>Synchronization Modules</title>

  <para>
   The <emphasis>multisite</emphasis> functionality of &ogw; allows you to
   create multiple zones and mirror data and metadata between them.
   <emphasis>Synchronization modules</emphasis> are built atop of the multisite
   framework that allows for forwarding data and metadata to a different
   external tier. A synchronization module allows for a set of actions to be
   performed whenever a change in data occurs (for example, metadata operations
   such as bucket or user creation). As the &ogw; multisite changes are
   eventually consistent at remote sites, changes are propagated
   asynchronously. This covers use cases such as backing up the object storage
   to an external cloud cluster, a custom backup solution using tape drives, or
   indexing metadata in ElasticSearch.
  </para>

  <sect2 xml:id="ogw-sync-general-config">
   <title>General Configuration</title>
   <para>
    All synchronization modules are configured in a similar way. You need to
    create a new zone (refer to <xref linkend="ceph-rgw-fed"/> for more
    details) and set its <option>--tier_type</option> option, for example
    <option>--tier-type=cloud</option> for the cloud synchronization module:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE-NAME</replaceable> \
 --endpoints=http://endpoint1.example.com,http://endpoint2.example.com, [...] \
 --tier-type=cloud
</screen>
   <para>
    You can configure the specific tier by using the following command:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE-NAME</replaceable> \
 --tier-config=<replaceable>KEY1</replaceable>=<replaceable>VALUE1</replaceable>,<replaceable>KEY2</replaceable>=<replaceable>VALUE2</replaceable>
</screen>
   <para>
    The <replaceable>KEY</replaceable> in the configuration specifies the
    configuration variable that you want to update, and the
    <replaceable>VALUE</replaceable> specifies its new value. Nested values can
    be accessed using period. For example:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE-NAME</replaceable> \
 --tier-config=connection.access_key=<replaceable>KEY</replaceable>,connection.secret=<replaceable>SECRET</replaceable>
</screen>
   <para>
    You can access array entries by appending square brackets '[]' with the
    referenced entry. You can add a new array entry by using square brackets
    '[]'. Index value of -1 references the last entry in the array. It is not
    possible to create a new entry and reference it again in the same command.
    For example, a command to create a new profile for buckets starting with
    <replaceable>PREFIX</replaceable> follows:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zone modify --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE-NAME</replaceable> \
 --tier-config=profiles[].source_bucket=<replaceable>PREFIX</replaceable>'*'
&prompt.cephuser;radosgw-admin zone modify --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE-NAME</replaceable> \
 --tier-config=profiles[-1].connection_id=<replaceable>CONNECTION_ID</replaceable>,profiles[-1].acls_id=<replaceable>ACLS_ID</replaceable>
</screen>
   <tip>
    <title>Adding and Removing Configuration Entries</title>
    <para>
     You can add a new tier configuration entry by using the
     <option>--tier-config-add=<replaceable>KEY</replaceable>=<replaceable>VALUE</replaceable></option>
     parameter.
    </para>
    <para>
     You can remove an existing entry by using
     <option>--tier-config-rm=<replaceable>KEY</replaceable></option>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rgw-sync-zones">
   <title>Synchronizing Zones</title>
   <para>
    A synchronization module configuration is local to a zone. The
    synchronization module determines whether the zone exports data or can only
    consume data that was modified in another zone. As of Luminous the
    supported synchronization plug-ins are <literal>ElasticSearch</literal>,
    <literal>rgw</literal>, which is the default synchronization plug-in that
    synchronizes data between the zones and <literal>log</literal> which is a
    trivial synchronization plug-in that logs the metadata operation that
    happens in the remote zones. The following sections are written with the
    example of a zone using <literal>ElasticSearch</literal> synchronization
    module. The process would be similar for configuring any other
    synchronization plug-in.
   </para>
   <note>
    <title>Default Synchronization Plug-in</title>
    <para>
     <literal>rgw</literal> is the default synchronization plug-in and there is
     no need to explicitly configure this.
    </para>
   </note>
   <sect3 xml:id="ceph-rgw-sync-zones-req">
    <title>Requirements and Assumptions</title>
    <para>
     Let us assume a simple multisite configuration as described in
     <xref linkend="ceph-rgw-fed"/> consists of 2 zones:
     <literal>us-east</literal> and <literal>us-west</literal>. Now we add a
     third zone <literal>us-east-es</literal> which is a zone that only
     processes metadata from the other sites. This zone can be in the same or a
     different &ceph; cluster than <literal>us-east</literal>. This zone would
     only consume metadata from other zones and &ogw;s in this zone will not
     serve any end user requests directly.
    </para>
   </sect3>
   <sect3 xml:id="ceph-rgw-sync-zones-configure">
    <title>Configuring Synchronization Modules</title>
    <procedure>
     <step>
      <para>
       Create the third zone similar to the ones described in
       <xref linkend="ceph-rgw-fed"/>, for example
      </para>
<screen>
&prompt.cephuser;<command>radosgw-admin</command> zone create --rgw-zonegroup=us --rgw-zone=us-east-es \
--access-key=<replaceable>SYSTEM-KEY</replaceable> --secret=<replaceable>SECRET</replaceable> --endpoints=http://rgw-es:80
      </screen>
     </step>
     <step>
      <para>
       A synchronization module can be configured for this zone via the
       following
      </para>
<screen>
&prompt.cephuser;<command>radosgw-admin</command> zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --tier-type=<replaceable>TIER-TYPE</replaceable> \
--tier-config={set of key=value pairs}
      </screen>
     </step>
     <step>
      <para>
       For example in the <literal>ElasticSearch</literal> synchronization
       module
      </para>
<screen>
&prompt.cephuser;<command>radosgw-admin</command> zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --tier-type=elasticsearch \
--tier-config=endpoint=http://localhost:9200,num_shards=10,num_replicas=1
      </screen>
      <para>
       For the various supported tier-config options refer to
       <xref linkend="ceph-rgw-sync-elastic"/>.
      </para>
     </step>
     <step>
      <para>
       Finally update the period
      </para>
<screen>
&prompt.cephuser;<command>radosgw-admin</command> period update --commit
      </screen>
     </step>
     <step>
      <para>
       Now start the &ogw; in the zone
      </para>
<screen>
&prompt.root;<command>systemctl</command> start ceph-radosgw@rgw.`hostname -s`
&prompt.root;<command>systemctl</command> enable ceph-radosgw@rgw.`hostname -s`
      </screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-rgw-sync-elastic">
   <title>ElasticSearch Synchronization Module</title>
   <para>
    This synchronization module writes the metadata from other zones to
    ElasticSearch. As of Luminous this is JSON of data fields we currently
    store in ElasticSearch.
   </para>
<screen>
{
  "_index" : "rgw-gold-ee5863d6",
  "_type" : "object",
  "_id" : "34137443-8592-48d9-8ca7-160255d52ade.34137.1:object1:null",
  "_score" : 1.0,
  "_source" : {
    "bucket" : "testbucket123",
    "name" : "object1",
    "instance" : "null",
    "versioned_epoch" : 0,
    "owner" : {
      "id" : "user1",
      "display_name" : "user1"
    },
    "permissions" : [
      "user1"
    ],
    "meta" : {
      "size" : 712354,
      "mtime" : "2017-05-04T12:54:16.462Z",
      "etag" : "7ac66c0f148de9519b8bd264312c4d64"
    }
  }
}
   </screen>
   <sect3 xml:id="ceph-rgw-sync-elastic-config">
    <title>ElasticSearch Tier Type Configuration Parameters</title>
    <variablelist>
     <varlistentry>
      <term>endpoint</term>
      <listitem>
       <para>
        Specifies the ElasticSearch server endpoint to access.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_shards</term>
      <listitem>
       <para>
        <emphasis>(integer)</emphasis> The number of shards that ElasticSearch
        will be configured with on data synchronization initialization. Note
        that this cannot be changed after initialization. Any change here
        requires rebuild of the ElasticSearch index and reinitialization of the
        data synchronization process.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>num_replicas</term>
      <listitem>
       <para>
        <emphasis>(integer)</emphasis> The number of replicas that
        ElasticSearch will be configured with on data synchronization
        initialization.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>explicit_custom_meta</term>
      <listitem>
       <para>
        <emphasis>(true | false)</emphasis> Specifies whether all user custom
        metadata will be indexed, or whether user will need to configure (at
        the bucket level) what customer metadata entries should be indexed.
        This is false by default
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>index_buckets_list</term>
      <listitem>
       <para>
        <emphasis>(comma separated list of strings)</emphasis> If empty, all
        buckets will be indexed. Otherwise, only buckets specified here will be
        indexed. It is possible to provide bucket prefixes (for example
        'foo*'), or bucket suffixes (for example '*bar').
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>approved_owners_list</term>
      <listitem>
       <para>
        <emphasis>(comma separated list of strings)</emphasis> If empty,
        buckets of all owners will be indexed (subject to other restrictions),
        otherwise, only buckets owned by specified owners will be indexed.
        Suffixes and prefixes can also be provided.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>override_index_path</term>
      <listitem>
       <para>
        <emphasis>(string)</emphasis> if not empty, this string will be used as
        the ElasticSearch index path. Otherwise the index path will be
        determined and generated on synchronization initialization.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>username</term>
      <listitem>
       <para>
        Specifies a user name for ElasticSearch if authentication is required.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>password</term>
      <listitem>
       <para>
        Specifies a password for ElasticSearch if authentication is required.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-rgw-sync-elastic-query">
    <title>Metadata Queries</title>
    <para>
     Since the ElasticSearch cluster now stores object metadata, it is
     important that the ElasticSearch endpoint is not exposed to the public and
     only accessible to the cluster administrators. For exposing metadata
     queries to the end user itself this poses a problem since we'd want the
     user to only query their metadata and not of any other users, this would
     require the ElasticSearch cluster to authenticate users in a way similar
     to RGW does which poses a problem.
    </para>
    <para>
     As of Luminous RGW in the metadata master zone can now service end user
     requests. This allows for not exposing the ElasticSearch endpoint in
     public and also solves the authentication and authorization problem since
     RGW itself can authenticate the end user requests. For this purpose RGW
     introduces a new query in the bucket APIs that can service ElasticSearch
     requests. All these requests must be sent to the metadata master zone.
    </para>
    <variablelist>
     <varlistentry>
      <term>Get an ElasticSearch Query</term>
      <listitem>
<screen>
GET /<replaceable>BUCKET</replaceable>?query=<replaceable>QUERY-EXPR</replaceable>
       </screen>
       <para>
        request params:
       </para>
       <itemizedlist>
        <listitem>
         <para>
          max-keys: max number of entries to return
         </para>
        </listitem>
        <listitem>
         <para>
          marker: pagination marker
         </para>
        </listitem>
       </itemizedlist>
<screen>
expression := [(]&lt;arg&gt; &lt;op&gt; &lt;value&gt; [)][&lt;and|or&gt; ...]
       </screen>
       <para>
        op is one of the following: &lt;, &lt;=, ==, &gt;=, &gt;
       </para>
       <para>
        For example:
       </para>
<screen>
GET /?query=name==foo
       </screen>
       <para>
        Will return all the indexed keys that user has read permission to, and
        are named 'foo'. The output will be a list of keys in XML that is
        similar to the S3 list buckets response.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Configure custom metadata fields</term>
      <listitem>
       <para>
        Define which custom metadata entries should be indexed (under the
        specified bucket), and what are the types of these keys. If explicit
        custom metadata indexing is configured, this is needed so that rgw will
        index the specified custom metadata values. Otherwise it is needed in
        cases where the indexed metadata keys are of a type other than string.
       </para>
<screen>
POST /<replaceable>BUCKET</replaceable>?mdsearch
x-amz-meta-search: &lt;key [; type]&gt; [, ...]
       </screen>
       <para>
        Multiple metadata fields must be comma separated, a type can be forced
        for a field with a `;`. The currently allowed types are
        string(default), integer and date, for example, if you want to index a
        custom object metadata x-amz-meta-year as int, x-amz-meta-date as type
        date and x-amz-meta-title as string, you would do
       </para>
<screen>
POST /mybooks?mdsearch
x-amz-meta-search: x-amz-meta-year;int, x-amz-meta-release-date;date, x-amz-meta-title;string
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Delete custom metadata configuration</term>
      <listitem>
       <para>
        Delete custom metadata bucket configuration.
       </para>
<screen>
DELETE /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Get custom metadata configuration</term>
      <listitem>
       <para>
        Retrieve custom metadata bucket configuration.
       </para>
<screen>
GET /<replaceable>BUCKET</replaceable>?mdsearch
       </screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>

  <sect2 xml:id="ogw-cloud-sync">
   <title>Cloud Synchronization Module</title>
   <para>
    This section introduces a module that synchronizes the zone data to a
    remote cloud service. The synchronization is only unidirectional&mdash;the
    date is not synchronized back from the remote zone. The main goal of this
    module is to enable synchronizing data to multiple cloud service providers.
    Currently it supports cloud providers that are compatible with AWS (S3).
   </para>
   <para>
    To synchronize data to a remote cloud service, you need to configure user
    credentials. Because many cloud services introduce limits on the number of
    buckets that each user can create, you can configure the mapping of source
    objects and buckets, different targets to different buckets and bucket
    prefixes. Note that source access lists (ACLs) will not be preserved. It is
    possible to map permissions of specific source users to specific
    destination users.
   </para>
   <para>
    Because of API limitations, there is no way to preserve original object
    modification time and HTTP entity tag (ETag). The cloud synchronization
    module stores these as metadata attributes on the destination objects.
   </para>
   <sect3>
    <title>General Configuration</title>
    <para>
     Following are examples of a trivial and non-trivial configuration for the
     cloud synchronization module. Note that the trivial configuration can
     collide with the non-trivial one.
    </para>
    <example>
     <title>Trivial Configuration</title>
<screen>
{
  "connection": {
    "access_key": <replaceable>ACCESS</replaceable>,
    "secret": <replaceable>SECRET</replaceable>,
    "endpoint": <replaceable>ENDPOINT</replaceable>,
    "host_style": <replaceable>path | virtual</replaceable>,
  },
  "acls": [ { "type": <replaceable>id | email | uri</replaceable>,
    "source_id": <replaceable>SOURCE_ID</replaceable>,
    "dest_id": <replaceable>DEST_ID</replaceable> } ... ],
  "target_path": <replaceable>TARGET_PATH</replaceable>,
}
</screen>
    </example>
    <example>
     <title>Non-trivial Configuration</title>
<screen>
{
  "default": {
    "connection": {
      "access_key": <replaceable>ACCESS</replaceable>,
      "secret": <replaceable>SECRET</replaceable>,
      "endpoint": <replaceable>ENDPOINT</replaceable>,
      "host_style" <replaceable>path | virtual</replaceable>,
    },
    "acls": [
    {
      "type": <replaceable>id | email | uri</replaceable>,   #  optional, default is id
      "source_id": <replaceable>ID</replaceable>,
      "dest_id": <replaceable>ID</replaceable>
    } ... ]
    "target_path": <replaceable>PATH</replaceable> # optional
  },
  "connections": [
  {
    "connection_id": <replaceable>ID</replaceable>,
    "access_key": <replaceable>ACCESS</replaceable>,
    "secret": <replaceable>SECRET</replaceable>,
    "endpoint": <replaceable>ENDPOINT</replaceable>,
    "host_style": <replaceable>path | virtual</replaceable>,  # optional
  } ... ],
  "acl_profiles": [
  {
    "acls_id": <replaceable>ID</replaceable>, # acl mappings
    "acls": [ {
      "type": <replaceable>id | email | uri</replaceable>,
      "source_id": <replaceable>ID</replaceable>,
      "dest_id": <replaceable>ID</replaceable>
    } ... ]
  }
  ],
  "profiles": [
  {
   "source_bucket": <replaceable>SOURCE</replaceable>,
   "connection_id": <replaceable>CONNECTION_ID</replaceable>,
   "acls_id": <replaceable>MAPPINGS_ID</replaceable>,
   "target_path": <replaceable>DEST</replaceable>,          # optional
  } ... ],
}
</screen>
    </example>
    <para>
     Explanation of used configuration terms follows:
    </para>
    <variablelist>
     <varlistentry>
      <term>connection</term>
      <listitem>
       <para>
        Represents a connection to the remote cloud service. Contains
        'connection_id', 'access_key', 'secret', 'endpoint', and 'host_style'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>access_key</term>
      <listitem>
       <para>
        The remote cloud access key that will be used for the specific
        connection.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>secret</term>
      <listitem>
       <para>
        The secret key for the remote cloud service.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>endpoint</term>
      <listitem>
       <para>
        URL of remote cloud service endpoint.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>host_style</term>
      <listitem>
       <para>
        Type of host style ('path' or 'virtual') to be used when accessing
        remote cloud endpoint. Default is 'path'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>acls</term>
      <listitem>
       <para>
        Array of access list mappings.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>acl_mapping</term>
      <listitem>
       <para>
        Each 'acl_mapping' structure contains 'type', 'source_id', and
        'dest_id'. These will define the ACL mutation for each object. An ACL
        mutation allows converting source user ID to a destination ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>type</term>
      <listitem>
       <para>
        ACL type: 'id' defines user ID, 'email' defines user by email, and
        'uri' defines user by uri (group).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>source_id</term>
      <listitem>
       <para>
        ID of user in the source zone.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>dest_id</term>
      <listitem>
       <para>
        ID of user in the destination.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>target_path</term>
      <listitem>
       <para>
        A string that defines how the target path is created. The target path
        specifies a prefix to which the source object name is appended. The
        target path configurable can include any of the following variables:
       </para>
       <variablelist>
        <varlistentry>
         <term>SID</term>
         <listitem>
          <para>
           A unique string that represents the synchronization instance ID.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>ZONEGROUP</term>
         <listitem>
          <para>
           Zonegroup name.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>ZONEGROUP_ID</term>
         <listitem>
          <para>
           Zonegroup ID.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>ZONE</term>
         <listitem>
          <para>
           Zone name.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>ZONE_ID</term>
         <listitem>
          <para>
           Zone ID.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>BUCKET</term>
         <listitem>
          <para>
           Source bucket name.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>OWNER</term>
         <listitem>
          <para>
           Source bucket owner ID.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
       <para>
        For example: target_path =
        rgwx-<replaceable>ZONE</replaceable>-<replaceable>SID</replaceable>/<replaceable>OWNER</replaceable>/<replaceable>BUCKET</replaceable>
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>acl_profiles</term>
      <listitem>
       <para>
        An array of access list profiles.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>acl_profile</term>
      <listitem>
       <para>
        Each profile contains 'acls_id' that represents the profile, and an
        'acls' array that holds a list of 'acl_mappings'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>profiles</term>
      <listitem>
       <para>
        A list of profiles. Each profile contains the following:
       </para>
       <variablelist>
        <varlistentry>
         <term>source_bucket</term>
         <listitem>
          <para>
           Either a bucket name, or a bucket prefix (if ends with *) that
           defines the source bucket(s) for this profile.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>target_path</term>
         <listitem>
          <para>
           See above for the explanation.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>connection_id</term>
         <listitem>
          <para>
           ID of the connection that will be used for this profile.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>acls_id</term>
         <listitem>
          <para>
           ID of ACL's profile that will be used for this profile.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>S3 Specific Configurables</title>
    <para>
     The cloud synchronization module will only work with back-ends that are
     compatible with AWS S3. There are a few configurables that can be used to
     tweak its behavior when accessing S3 cloud services:
    </para>
<screen>
{
  "multipart_sync_threshold": <replaceable>OBJECT_SIZE</replaceable>,
  "multipart_min_part_size": <replaceable>PART_SIZE</replaceable>
}
</screen>
    <variablelist>
     <varlistentry>
      <term>multipart_sync_threshold</term>
      <listitem>
       <para>
        Objects whose size is equal to or larger than this value will be
        synchronized with the cloud service using multipart upload.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>multipart_min_part_size</term>
      <listitem>
       <para>
        Minimum parts size to use when synchronizing objects using multipart
        upload.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>

  <sect2 xml:id="archive-sync-module">
   <title>Archive Synchronization Module</title>
   <para>
    The <emphasis>archive sync module</emphasis> utilizes the versioning
    feature of S3 objects in &ogw;. You can configure an <emphasis>archive
    zone</emphasis> that captures the different versions of S3 objects as they
    occur over time in other zones. The history of versions that the archive
    zone keeps can only be eliminated via gateways associated with the archive
    zone.
   </para>
   <para>
    With such an architecture, several non-versioned zones can mirror their
    data and metadata via their zone gateways providing high availability to
    the end users, while the archive zone captures all the data updates to
    consolidate them as versions of S3 objects.
   </para>
   <para>
    By including the archive zone in a multi-zone configuration, you gain the
    flexibility of an S3 object history in one zone while saving the space that
    the replicas of the versioned S3 objects would consume in the remaining
    zones.
   </para>
   <sect3 xml:id="archive-sync-module-configuration">
    <title>Configuration</title>
    <tip>
     <title>More Information</title>
     <para>
      Refer to <xref linkend="ceph-rgw-fed"/> for details on configuring
      multisite gateways.
     </para>
     <para>
      Refer to <xref linkend="ceph-rgw-sync"/> for details on configuring
      synchronization modules.
     </para>
    </tip>
    <para>
     To use the archive module, you need to create a new zone whose tier type
     is set to <literal>archive</literal>:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=<replaceable>ZONE_GROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>OGW_ZONE_NAME</replaceable> \
 --endpoints=<replaceable>http://OGW_ENDPOINT1_URL[,http://OGW_ENDPOINT2_URL,...]</replaceable>
 --tier-type=archive
</screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rgw-ldap">
  <title>LDAP Authentication</title>

  <para>
   Apart from the default local user authentication, &ogw; can use LDAP server
   services to authenticate users as well.
  </para>

  <sect2 xml:id="ceph-rgw-ldap-how-works">
   <title>Authentication Mechanism</title>
   <para>
    The &ogw; extracts the user's LDAP credentials from a token. A search
    filter is constructed from the user name. The &ogw; uses the configured
    service account to search the directory for a matching entry. If an entry
    is found, the &ogw; attempts to bind to the found distinguished name with
    the password from the token. If the credentials are valid, the bind will
    succeed, and the &ogw; grants access.
   </para>
   <para>
    You can limit the allowed users by setting the base for the search to a
    specific organizational unit or by specifying a custom search filter, for
    example requiring specific group membership, custom object classes, or
    attributes.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rgw-ldap-reqs">
   <title>Requirements</title>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>LDAP or Active Directory</emphasis>: A running LDAP instance
      accessible by the &ogw;.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Service account</emphasis>: LDAP credentials to be used by the
      &ogw; with search permissions.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>User account</emphasis>: At least one user account in the LDAP
      directory.
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <title>Do Not Overlap LDAP and Local Users</title>
    <para>
     You should not use the same user names for local users and for users being
     authenticated by using LDAP. The &ogw; cannot distinguish them and it
     treats them as the same user.
    </para>
   </important>
   <tip>
    <title>Sanity Checks</title>
    <para>
     Use the <command>ldapsearch</command> utility to verify the service
     account or the LDAP connection. For example:
    </para>
<screen>&prompt.user;ldapsearch -x -D "uid=ceph,ou=system,dc=example,dc=com" -W \
-H ldaps://example.com -b "ou=users,dc=example,dc=com" 'uid=*' dn</screen>
    <para>
     Make sure to use the same LDAP parameters as in the &ceph; configuration
     file to eliminate possible problems.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rgw-ldap-config">
   <title>Configure &ogw; to Use LDAP Authentication</title>
   <para>
    The following parameters in the <filename>/etc/ceph/ceph.conf</filename>
    configuration file are related to the LDAP authentication:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>rgw_ldap_uri</option></term>
     <listitem>
      <para>
       Specifies the LDAP server to use. Make sure to use the
       <literal>ldaps://<replaceable>FQDN</replaceable>:<replaceable>PORT</replaceable></literal>
       parameter to avoid transmitting the plain text credentials openly.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_binddn</option></term>
     <listitem>
      <para>
       The Distinguished Name (DN) of the service account used by the &ogw;.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_secret</option></term>
     <listitem>
      <para>
       The password for the service account.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rgw_ldap_searchdn</term>
     <listitem>
      <para>
       Specifies the base in the directory information tree for searching
       users. This might be your users organizational unit or some more
       specific Organizational Unit (OU).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_ldap_dnattr</option></term>
     <listitem>
      <para>
       The attribute being used in the constructed search filter to match a
       user name. Depending on your Directory Information Tree (DIT) this would
       probably be <literal>uid</literal> or <literal>cn</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rgw_search_filter</option></term>
     <listitem>
      <para>
       If not specified, the &ogw; automatically constructs the search filter
       with the <option>rgw_ldap_dnattr</option> setting. Use this parameter to
       narrow the list of allowed users in very flexible ways. Consult
       <xref linkend="ceph-rgw-ldap-filter"/> for details.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-rgw-ldap-filter">
   <title>Using a Custom Search Filter to Limit User Access</title>
   <para>
    There are two ways you can use the <option>rgw_search_filter</option>
    parameter.
   </para>
   <sect3>
    <title>Partial Filter to Further Limit the Constructed Search Filter</title>
    <para>
     An example of a partial filter:
    </para>
<screen>"objectclass=inetorgperson"</screen>
    <para>
     The &ogw; will generate the search filter as usual with the user name from
     the token and the value of <option>rgw_ldap_dnattr</option>. The
     constructed filter is then combined with the partial filter from the
     <option>rgw_search_filter</option> attribute. Depending on the user name
     and the settings the final search filter may become:
    </para>
<screen>"(&amp;(uid=hari)(objectclass=inetorgperson))"</screen>
    <para>
     In that case, user 'hari' will only be granted access if he is found in
     the LDAP directory, has an object class of 'inetorgperson', and did
     specify a valid password.
    </para>
   </sect3>
   <sect3>
    <title>Complete Filter</title>
    <para>
     A complete filter must contain a <option>USERNAME</option> token which
     will be substituted with the user name during the authentication attempt.
     The <option>rgw_ldap_dnattr</option> parameter is not used anymore in this
     case. For example, to limit valid users to a specific group, use the
     following filter:
    </para>
<screen>"(&amp;(uid=USERNAME)(memberOf=cn=ceph-users,ou=groups,dc=mycompany,dc=com))"</screen>
    <note>
     <title><literal>memberOf</literal> Attribute</title>
     <para>
      Using the <literal>memberOf</literal> attribute in LDAP searches requires
      server side support from you specific LDAP server implementation.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-rgw-ldap-token">
   <title>Generating an Access Token for LDAP authentication</title>
   <para>
    The <command>radosgw-token</command> utility generates the access token
    based on the LDAP user name and password. It outputs a base-64 encoded
    string which is the actual access token. Use your favorite S3 client (refer
    to <xref linkend="accessing-ragos-gateway"/>) and specify the token as the
    access key and use an empty secret key.
   </para>
<screen>&prompt.user;export RGW_ACCESS_KEY_ID="<replaceable>USERNAME</replaceable>"
&prompt.user;export RGW_SECRET_ACCESS_KEY="<replaceable>PASSWORD</replaceable>"
&prompt.cephuser;radosgw-token --encode --ttype=ldap</screen>
   <important>
    <title>Clear Text Credentials</title>
    <para>
     The access token is a base-64 encoded JSON structure and contains the LDAP
     credentials as a clear text.
    </para>
   </important>
   <note>
    <title>Active Directory</title>
    <para>
     For Active Directory, use the <option>--ttype=ad</option> parameter.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw-bucket-sharding">
  <title>Bucket Index Sharding</title>

  <para>
   The &ogw; stores bucket index data in an index pool, which defaults to
   <literal>.rgw.buckets.index</literal>. If you put too many (hundreds of
   thousands) objects into a single bucket and the quota for maximum number of
   objects per bucket (<option>rgw bucket default quota max objects</option>)
   is not set, the performance of the index pool may degrade. <emphasis>Bucket
   index sharding</emphasis> prevents such performance decreases and allows a
   high number of objects per bucket.
  </para>

  <sect2 xml:id="ogw-bucket-reshard">
   <title>Bucket Index Resharding</title>
   <para>
    If a bucket has grown large and its initial configuration is not sufficient
    anymore, the bucket's index pool needs to be resharded. You can either use
    automatic online bucket index resharding (refer to
    <xref linkend="ogw-bucket-sharding-dyn"/>), or reshard the bucket index
    offline manually (refer to <xref linkend="ogw-bucket-sharding-re"/>).
   </para>
   <sect3 xml:id="ogw-bucket-sharding-dyn">
    <title>Dynamic Resharding</title>
    <para>
     From &productname; 5, we support online bucket resharding. This detects if
     the number of objects per bucket reaches a certain threshold, and
     automatically increases the number of shards used by the bucket index.
     This process reduces the number of entries in each bucket index shard.
    </para>
    <para>
     The detection process runs:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       When new objects are added to the bucket.
      </para>
     </listitem>
     <listitem>
      <para>
       In a background process that periodically scans all the buckets. This is
       needed in order to deal with existing buckets that are not being
       updated.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     A bucket that requires resharding is added to the
     <option>reshard_log</option> queue and will be scheduled to be resharded
     later. The reshard threads run in the background and execute the scheduled
     resharding, one at a time.
    </para>
    <variablelist>
     <title>Configuring Dynamic Resharding</title>
     <varlistentry>
      <term><option>rgw_dynamic_resharding</option></term>
      <listitem>
       <para>
        Enables or disables dynamic bucket index resharding. Possible values
        are 'true' or 'false'. Defaults to 'true'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_num_logs</option></term>
      <listitem>
       <para>
        Number of shards for the resharding log. Defaults to 16.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_bucket_lock_duration</option></term>
      <listitem>
       <para>
        Duration of lock on the bucket object during resharding. Defaults to
        120 seconds.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_max_objs_per_shard</option></term>
      <listitem>
       <para>
        Maximum number of objects per bucket index shard. Defaults to 100000
        objects.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw_reshard_thread_interval</option></term>
      <listitem>
       <para>
        Maximum time between rounds of reshard thread processing. Defaults to
        600 seconds.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <important>
     <title>Multisite Configurations</title>
     <para>
      Dynamic resharding is not supported in multisite environments. It is
      disabled by default from &ceph; 12.2.2 onward, but we recommend you to
      double check the setting.
     </para>
    </important>
    <variablelist>
     <title>Commands to Administer the Resharding Process</title>
     <varlistentry>
      <term>Add a bucket to the resharding queue:</term>
      <listitem>
<screen>
&prompt.cephuser;radosgw-admin reshard add \
 --bucket <replaceable>BUCKET_NAME</replaceable> \
 --num-shards <replaceable>NEW_NUMBER_OF_SHARDS</replaceable>
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>List resharding queue:</term>
      <listitem>
<screen>
&prompt.cephuser;radosgw-admin reshard list
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Process/schedule a bucket resharding:</term>
      <listitem>
<screen>
&prompt.cephuser;radosgw-admin reshard process
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Display the bucket resharding status:</term>
      <listitem>
<screen>
&prompt.cephuser;radosgw-admin reshard status --bucket <replaceable>BUCKET_NAME</replaceable>
</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Cancel pending bucket resharding:</term>
      <listitem>
<screen>
&prompt.cephuser;radosgw-admin reshard cancel --bucket <replaceable>BUCKET_NAME</replaceable>
</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ogw-bucket-sharding-re">
    <title>Manual Resharding</title>
    <para>
     Dynamic resharding as mentioned in
     <xref linkend="ogw-bucket-sharding-dyn"/> is supported only for simple
     &ogw; configurations. For multisite configurations, use manual resharding
     as described in this section.
    </para>
    <para>
     To reshard the bucket index manually offline, use the following command:
    </para>
<screen>
&prompt.cephuser;radosgw-admin bucket reshard
</screen>
    <para>
     The <command>bucket reshard</command> command performs the following:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Creates a new set of bucket index objects for the specified object.
      </para>
     </listitem>
     <listitem>
      <para>
       Spreads all entries of these index objects.
      </para>
     </listitem>
     <listitem>
      <para>
       Creates a new bucket instance.
      </para>
     </listitem>
     <listitem>
      <para>
       Links the new bucket instance with the bucket so that all new index
       operations go through the new bucket indexes.
      </para>
     </listitem>
     <listitem>
      <para>
       Prints the old and the new bucket ID to the standard output.
      </para>
     </listitem>
    </itemizedlist>
    <procedure>
     <title>Resharding the Bucket Index Pool</title>
     <step>
      <para>
       Make sure that all operations to the bucket are stopped.
      </para>
     </step>
     <step>
      <para>
       Back up the original bucket index:
      </para>
<screen>
&prompt.cephuser;radosgw-admin bi list \
 --bucket=<replaceable>BUCKET_NAME</replaceable> \
 &gt; <replaceable>BUCKET_NAME</replaceable>.list.backup
</screen>
     </step>
     <step>
      <para>
       Reshard the bucket index:
      </para>
<screen>
 &prompt.cephuser;radosgw-admin reshard \
 --bucket=<replaceable>BUCKET_NAME</replaceable> \
 --num-shards=<replaceable>NEW_SHARDS_NUMBER</replaceable>
</screen>
      <tip>
       <title>Old Bucket ID</title>
       <para>
        As part of its output, this command also prints the new and the old
        bucket ID. Note the old bucket ID down; you will need it to purge the
        old bucket index objects.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Verify that the objects are listed correctly by comparing the old bucket
       index listing with the new one. Then purge the old bucket index objects:
      </para>
<screen>
&prompt.cephuser;radosgw-admin bi purge
 --bucket=<replaceable>BUCKET_NAME</replaceable>
 --bucket-id=<replaceable>OLD_BUCKET_ID</replaceable>
</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="ogw-bucket-sharding-new">
   <title>Bucket Index Sharding for New Buckets</title>
   <para>
    There are two options that affect bucket index sharding:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Use the <option>rgw_override_bucket_index_max_shards</option> option for
      simple configurations.
     </para>
    </listitem>
    <listitem>
     <para>
      Use the <option>bucket_index_max_shards</option> option for multisite
      configurations.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Setting the options to <literal>0</literal> disables bucket index sharding.
    A value greater than <literal>0</literal> enables bucket index sharding and
    sets the maximum number of shards.
   </para>
   <para>
    The following formula helps you calculate the recommended number of shards:
   </para>
<screen>
number_of_objects_expected_in_a_bucket / 100000
</screen>
   <para>
    Be aware that the maximum number of shards is 7877.
   </para>
   <sect3>
    <title>Simple Configurations</title>
    <procedure>
     <step>
      <para>
       Open the &ceph; configuration file and add or modify the following
       option:
      </para>
<screen>
rgw_override_bucket_index_max_shards = 12
</screen>
      <tip>
       <title>All or One &ogw; Instances</title>
       <para>
        To configure bucket index sharding for all instances of the &ogw;,
        include <option>rgw_override_bucket_index_max_shards</option> in the
        <literal>[global]</literal> section.
       </para>
       <para>
        To configure bucket index sharding only for a particular instance of
        the &ogw;, include
        <option>rgw_override_bucket_index_max_shards</option> in the related
        instance section.
       </para>
      </tip>
     </step>
     <step>
      <para>
       Restart the &ogw;. See <xref linkend="ceph-rgw-operating"/> for more
       details.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3>
    <title>Multisite Configurations</title>
    <para>
     Multisite configurations can have a different index pool to manage
     failover. To configure a consistent shard count for zones in one zone
     group, set the <option>bucket_index_max_shards</option> option in the zone
     group's configuration:
    </para>
    <procedure>
     <step>
      <para>
       Export the &zgroup; configuration to the
       <filename>zonegroup.json</filename> file:
      </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup get &gt; zonegroup.json
</screen>
     </step>
     <step>
      <para>
       Edit the <filename>zonegroup.json</filename> file and set the
       <option>bucket_index_max_shards</option> option for each named zone.
      </para>
     </step>
     <step>
      <para>
       Reset the &zgroup;:
      </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup set &lt; zonegroup.json
</screen>
     </step>
     <step>
      <para>
       Update the period. See <xref linkend="ceph-rgw-fed-masterzone-updateperiod"/>.
     </para>
     </step>
    </procedure>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw-keystone">
  <title>Integrating &okeystone;</title>

  <para>
   &okeystone; is an identity service for the &ostack; product. You can
   integrate the &ogw; with &keystone; to set up a gateway that accepts a
   &keystone; authentication token. A user authorized by &keystone; to access
   the gateway will be verified on the &cogw; side and automatically created if
   needed. The &ogw; queries &keystone; periodically for a list of revoked
   tokens.
  </para>

  <sect2 xml:id="ogw-keystone-ostack">
   <title>Configuring &ostack;</title>
   <para>
    Before configuring the &cogw;, you need to configure the &okeystone; to
    enable the &swift; service and point it to the &cogw;:
   </para>
   <procedure>
    <step>
     <para>
      <emphasis>Set the &swift; service.</emphasis> To use &ostack; to validate
      &swift; users, first create the &swift; service:
     </para>
<screen>
&prompt.user;openstack service create \
 --name=swift \
 --description="Swift Service" \
 object-store
</screen>
    </step>
    <step>
     <para>
      <emphasis>Set the endpoints.</emphasis> After you create the &swift;
      service, point to the &cogw;. Replace
      <replaceable>REGION_NAME</replaceable> with the name of the gateway’s
      &zgroup; name or region name.
     </para>
<screen>
&prompt.user;openstack endpoint create --region <replaceable>REGION_NAME</replaceable> \
 --publicurl   "http://radosgw.example.com:8080/swift/v1" \
 --adminurl    "http://radosgw.example.com:8080/swift/v1" \
 --internalurl "http://radosgw.example.com:8080/swift/v1" \
 swift
</screen>
    </step>
    <step>
     <para>
      <emphasis>Verify the settings.</emphasis> After you create the &swift;
      service and set the endpoints, show the endpoints to verify that all the
      settings are correct.
     </para>
<screen>
&prompt.user;openstack endpoint show object-store
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ogw-keystone-ogw">
   <title>Configuring the &cogw;</title>
   <sect3>
    <title>Configure SSL Certificates</title>
    <para>
     The &cogw; queries &keystone; periodically for a list of revoked tokens.
     These requests are encoded and signed. &keystone; may be also configured
     to provide self-signed tokens, which are also encoded and signed. You need
     to configure the gateway so that it can decode and verify these signed
     messages. Therefore, the OpenSSL certificates that &keystone; uses to
     create the requests need to be converted to the 'nss db' format:
    </para>
<screen>
&prompt.root;mkdir /var/ceph/nss
&prompt.root;openssl x509 -in /etc/keystone/ssl/certs/ca.pem \
 -pubkey | certutil -d /var/ceph/nss -A -n ca -t "TCu,Cu,Tuw"
&rootuser;openssl x509 -in /etc/keystone/ssl/certs/signing_cert.pem \
 -pubkey | certutil -A -d /var/ceph/nss -n signing_cert -t "P,P,P"
</screen>
    <para>
     To allow &cogw; to interact with &okeystone;, &okeystone; can use a
     self-signed SSL certificate. Either install &keystone;’s SSL certificate
     on the node running the &cogw;, or alternatively set the value of the
     option <option>rgw keystone verify ssl</option> to 'false'. Setting
     <option>rgw keystone verify ssl</option> to 'false' means that the gateway
     will not attempt to verify the certificate.
    </para>
   </sect3>
   <sect3>
    <title>Configure the &ogw;'s Options</title>
    <para>
     You can configure &keystone; integration using the following options:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone api version</option></term>
      <listitem>
       <para>
        Version of the &keystone; API. Valid options are 2 or 3. Defaults to 2.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone url</option></term>
      <listitem>
       <para>
        The URL and port number of the administrative RESTful API on the
        &keystone; server. Follows the pattern
        <replaceable>SERVER_URL:PORT_NUMBER</replaceable>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin token</option></term>
      <listitem>
       <para>
        The token or shared secret that is configured internally in &keystone;
        for administrative requests.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone accepted roles</option></term>
      <listitem>
       <para>
        The roles required to serve requests. Defaults to 'Member, admin'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone accepted admin roles</option></term>
      <listitem>
       <para>
        The list of roles allowing a user to gain administrative privileges.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone token cache size</option></term>
      <listitem>
       <para>
        The maximum number of entries in the &keystone; token cache.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone revocation interval</option></term>
      <listitem>
       <para>
        The number of seconds before checking revoked tokens. Defaults to 15 *
        60.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone implicit tenants</option></term>
      <listitem>
       <para>
        Create new users in their own tenants of the same name. Defaults to
        'false'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw s3 auth use keystone</option></term>
      <listitem>
       <para>
        If set to 'true', the &cogw; will authenticate users using &keystone;.
        Defaults to 'false'.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>nss db path</option></term>
      <listitem>
       <para>
        The path to the NSS database.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     It is also possible to configure the &keystone; service tenant, user, and
     password for Keystone (for version 2.0 of the &ostack; Identity API),
     similar to the way &ostack; services tend to be configured. This way you
     can avoid setting the shared secret <option>rgw keystone admin
     token</option> in the configuration file, which should be disabled in
     production environments. The service tenant credentials should have admin
     privileges. For more details refer to the
     <link
      xlink:href="https://docs.openstack.org/keystone/latest/#setting-up-projects-users-and-roles">official
     &okeystone; documentation</link>. The related configuration options
     follow:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone admin user</option></term>
      <listitem>
       <para>
        The &keystone; administrator user name.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin password</option></term>
      <listitem>
       <para>
        The keystone administrator user password.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin tenant</option></term>
      <listitem>
       <para>
        The &keystone; version 2.0 administrator user tenant.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     A &cogw; user is mapped to a &keystone; tenant. A &keystone; user has
     different roles assigned to it, possibly on more than one tenant. When the
     &cogw; gets the ticket, it looks at the tenant and the user roles that are
     assigned to that ticket, and accepts or rejects the request according to
     the setting of the <option>rgw keystone accepted roles</option> option.
    </para>
    <tip>
     <title>Mapping to &ostack; Tenants</title>
     <para>
      Although &swift; tenants are mapped to the &ogw; user by default, they
      can be also mapped to &ostack; tenants via the <option>rgw keystone
      implicit tenants</option> option. This will make containers use the
      tenant namespace instead of the S3 like global namespace that the &ogw;
      defaults to. We recommend deciding on the mapping method at the planning
      stage to avoid confusion. The reason for this is that toggling the option
      later affects only newer requests which get mapped under a tenant, while
      older buckets created before still continue to be in a global namespace.
     </para>
    </tip>
    <para>
     For version 3 of the &ostack; Identity API, you should replace the
     <option>rgw keystone admin tenant</option> option with:
    </para>
    <variablelist>
     <varlistentry>
      <term><option>rgw keystone admin domain</option></term>
      <listitem>
       <para>
        The &keystone; administrator user domain.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><option>rgw keystone admin project</option></term>
      <listitem>
       <para>
        The &keystone; administrator user project.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ogw-storage-classes">
<!-- http://docs.ceph.com/docs/nautilus/radosgw/placement/ -->

  <title>Pool Placement and Storage Classes</title>

  <sect2 xml:id="ogw-storage-classes-placement-targets">
   <title>Placement Targets</title>
   <para>
    Placement targets control which pools are associated with a particular
    bucket. A bucket’s placement target is selected on creation, and cannot
    be modified. You can display its 'placement_rule' by running the following
    command:
   </para>
<screen>
&prompt.cephuser;radosgw-admin bucket stats
</screen>
   <para>
    The &zgroup; configuration contains a list of placement targets with an
    initial target named 'default-placement'. The zone configuration then maps
    each &zgroup; placement target name onto its local storage. This zone
    placement information includes the 'index_pool' name for the bucket index,
    the 'data_extra_pool' name for metadata about incomplete multipart uploads,
    and a 'data_pool' name for each storage class.
   </para>
  </sect2>

  <sect2 xml:id="ogw-storage-classes-itself">
   <title>Storage Classes</title>
   <para>
    Storage classes help customizing the placement of object data. S3 Bucket
    Lifecycle rules can automate the transition of objects between storage
    classes.
   </para>
   <para>
    Storage classes are defined in terms of placement targets. Each &zgroup;
    placement target lists its available storage classes with an initial class
    named 'STANDARD'. The zone configuration is responsible for providing a
    'data_pool' pool name for each of the &zgroup;’s storage classes.
   </para>
  </sect2>

  <sect2 xml:id="ogw-storage-classes-zone-config">
   <title>Zonegroup/Zone Configuration</title>
   <para>
    Use the <command>radosgw-admin</command> command on the &zgroup;s and
    zones to configure their placement. You can query the &zgroup; placement
    configuration using the following command:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup get
{
    "id": "ab01123f-e0df-4f29-9d71-b44888d67cd5",
    "name": "default",
    "api_name": "default",
    ...
    "placement_targets": [
        {
            "name": "default-placement",
            "tags": [],
            "storage_classes": [
                "STANDARD"
            ]
        }
    ],
    "default_placement": "default-placement",
    ...
}
</screen>
   <para>
    To query the zone placement configuration, run:
   </para>
<screen>
&prompt.cephuser;radosgw-admin zone get
{
    "id": "557cdcee-3aae-4e9e-85c7-2f86f5eddb1f",
    "name": "default",
    "domain_root": "default.rgw.meta:root",
    ...
    "placement_pools": [
        {
            "key": "default-placement",
            "val": {
                "index_pool": "default.rgw.buckets.index",
                "storage_classes": {
                    "STANDARD": {
                        "data_pool": "default.rgw.buckets.data"
                    }
                },
                "data_extra_pool": "default.rgw.buckets.non-ec",
                "index_type": 0
            }
        }
    ],
    ...
}
</screen>
   <note>
    <title>No Previous Multisite Configuration</title>
    <para>
     If you have not done any previous multisite configuration, a 'default'
     zone and &zgroup; are created for you, and changes to the zone/zonegroup
     will not take effect until you restart the &cogw;s. If you have created a
     realm for multisite, the zone/zonegroup changes will take effect after you
     commit the changes with the <command>radosgw-admin period update
     --commit</command> command.
    </para>
   </note>
   <sect3>
    <title>Adding a Placement Target</title>
    <para>
     To create a new placement target named 'temporary', start by adding it to
     the &zgroup;:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id temporary
</screen>
    <para>
     Then provide the zone placement info for that target:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id temporary \
      --data-pool default.rgw.temporary.data \
      --index-pool default.rgw.temporary.index \
      --data-extra-pool default.rgw.temporary.non-ec
</screen>
   </sect3>
   <sect3>
    <title>Adding a Storage Class</title>
    <para>
     To add a new storage class named 'COLD' to the 'default-placement' target,
     start by adding it to the &zgroup;:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup placement add \
      --rgw-zonegroup default \
      --placement-id default-placement \
      --storage-class COLD
</screen>
    <para>
     Then provide the zone placement info for that storage class:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zone placement add \
      --rgw-zone default \
      --placement-id default-placement \
      --storage-class COLD \
      --data-pool default.rgw.cold.data \
      --compression lz4
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ogw-storage-classes-customizing-placement">
   <title>Customizing Placement</title>
   <sect3>
    <title>Default Placement</title>
    <para>
     By default, new buckets will use the &zgroup;’s 'default_placement'
     target. You can change this &zgroup; setting with:
    </para>
<screen>
&prompt.cephuser;radosgw-admin zonegroup placement default \
      --rgw-zonegroup default \
      --placement-id new-placement
</screen>
   </sect3>
   <sect3>
    <title>User Placement</title>
    <para>
     A &cogw; user can override the &zgroup;’s default placement target by
     setting a non-empty 'default_placement' field in the user info. Similarly,
     the 'default_storage_class' can override the 'STANDARD' storage class
     applied to objects by default.
    </para>
<screen>
&prompt.cephuser;radosgw-admin user info --uid testid
{
    ...
    "default_placement": "",
    "default_storage_class": "",
    "placement_tags": [],
    ...
}
</screen>
    <para>
     If a &zgroup;’s placement target contains any tags, users will be
     unable to create buckets with that placement target unless their user info
     contains at least one matching tag in its 'placement_tags' field. This can
     be useful to restrict access to certain types of storage.
    </para>
    <para>
     The <command>radosgw-admin</command> command cannot modify these fields
     directly, therefore you need to edit the JSON format manually:
    </para>
<screen>
&prompt.cephuser;radosgw-admin metadata get user:<replaceable>USER-ID</replaceable> > user.json
&prompt.user;vi user.json     # edit the file as required
&prompt.cephuser;radosgw-admin metadata put user:<replaceable>USER-ID</replaceable> &lt; user.json
</screen>
   </sect3>
   <sect3>
    <title>S3 Bucket Placement</title>
    <para>
     When creating a bucket with the S3 protocol, a placement target can be
     provided as part of the <option>LocationConstraint</option> to override
     the default placement targets from the user and &zgroup;.
    </para>
    <para>
     Normally, the <option>LocationConstraint</option> needs to match the
     &zgroup;’s <option>api_name</option>:
    </para>
<screen>
&lt;LocationConstraint>default&lt;/LocationConstraint>
</screen>
    <para>
     You can add a custom placement target to the <option>api_name</option>
     following a colon:
    </para>
<screen>
&lt;LocationConstraint>default:new-placement&lt;/LocationConstraint>
</screen>
   </sect3>
   <sect3>
    <title>&swift; Bucket Placement</title>
    <para>
     When creating a bucket with the Swift protocol, you can provide a
     placement target in the HTTP header's 'X-Storage-Policy':
    </para>
<screen>
 X-Storage-Policy: <replaceable>NEW-PLACEMENT</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ogw-storage-classes-usage">
   <title>Using Storage Classes</title>
   <para>
    All placement targets have a 'STANDARD' storage class which is applied to
    new objects by default. You can override this default with its
    'default_storage_class'.
   </para>
   <para>
    To create an object in a non-default storage class, provide that storage
    class name in an HTTP header with the request. The S3 protocol uses the
    'X-Amz-Storage-Class' header, while the &swift; protocol uses the
    'X-Object-Storage-Class' header.
   </para>
   <para>
    You can use <emphasis>S3 Object Lifecycle Management</emphasis> to move
    object data between storage classes using 'Transition' actions.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="ceph-rgw-fed">
  <title>Multisite &ogw;s</title>
  <para>&ceph; supports several multi-site configuration options for the
    &cogw;:</para>
  <variablelist>
   <varlistentry>
    <term>Multi-zone</term>
    <listitem>
     <para>
      A configuration consisting of one &zgroup; and multiple
      zones, each zone with one or more <systemitem class="daemon">ceph-radosgw</systemitem> instances. Each zone
      is backed by its own &ceph; Storage Cluster. Multiple zones in a zone
      group provide disaster recovery for the &zgroup; should one of
      the zones experience a significant failure. Each zone is
      active and may receive write operations. In addition to disaster
      recovery, multiple active zones may also serve as a foundation for
      content delivery networks.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multi-zone-group</term>
    <listitem>
     <para>
      &cogw; supports multiple &zgroup;s, each &zgroup; with one or
      more zones. Objects stored to zones in one &zgroup; within the same
      realm as another &zgroup; share a global object namespace, ensuring
      unique object IDs across &zgroup;s and zones.
     </para>
     <note>
       <para>It is important to note that &zgroup;s <emphasis>only</emphasis>
       sync metadata amongst themselves. Data and metadata are replicated
       between the zones within the &zgroup;. No data or metadata is shared
       across a realm.</para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Multiple Realms</term>
    <listitem>
     <para>
      &cogw; supports the notion of realms; a globally unique
      namespace. Multiple realms are supported which may
      encompass single or multiple &zgroup;s.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
  <para>
   You can configure each &ogw; to work in an active-active zone
   configuration, allowing for writes to non-master zones.
   The multi-site configuration is stored within a container called a realm.
   The realm stores &zgroup;s, zones, and a time period with multiple
   epochs for tracking changes to the configuration. The
   <systemitem class="daemon">ceph-radosgw</systemitem> daemons handle the synchronization, eliminating the
   need for a separate synchronization agent. This approach
   to synchronization allows the &cogw; to operate with an
   active-active configuration instead of active-passive.
  </para>
  <sect2 xml:id="ceph-rgw-multi-req-assump">
   <title>Requirements and Assumptions</title>
   <para>
     A multi-site configuration requires at least two &ceph; storage clusters, and
     at least two &cogw; instances, one for each &ceph; storage cluster.
     The following configuration assumes at least two &ceph; storage clusters
     are in geographically separate locations. However, the configuration can
     work on the same site. For example, named <literal>rgw1</literal> and
     <literal>rgw2</literal>.
   </para>
   <para>A multi-site configuration requires a master &zgroup; and a
     master zone. A master zone is the source of truth with respect to all
     metadata operations in a multisite cluster. Additionally, each
     &zgroup; requires a master zone.
     &zgroup;s may have one or more secondary or non-master zones.
     In this guide, the <literal>rgw1</literal> host serves as the master zone
     of the master &zgroup; and the <literal>rgw2</literal> host serves as the
     secondary zone of the master &zgroup;.</para>
  </sect2>

  <sect2 xml:id="ceph-rgw-config-master-zone">
   <title>Configuring a Master Zone</title>
   <para>
     All gateways in a multi-site configuration retrieve their configuration
     from a <systemitem class="daemon">ceph-radosgw</systemitem> daemon on a host within the master &zgroup; and master
     zone. To configure your gateways in a multi-site configuration, choose
     a <systemitem class="daemon">ceph-radosgw</systemitem> instance to configure the master &zgroup; and master zone.
   </para>
  <sect3 xml:id="ceph-rgw-fed-realm">
   <title>Creating a Realm</title>
   <para>A realm represents a globally unique namespace consisting of one or
     more &zgroup;s containing one or more zones. Zones contain buckets,
     which in turn contain objects. A realm enables the &cogw; to
     support multiple namespaces and their configuration on the same hardware.
     A realm contains the notion of periods. Each period represents the state of
     the &zgroup; and zone configuration in time. Each time you make a change
     to a &zgroup; or zone, update the period and commit it.
     By default, the &cogw; does not create a realm for backward
     compatibility. As a best practice, we recommend creating realms for new clusters.</para>
   <para>
    Create a new realm called <literal>gold</literal> for the multi-site
    configuration by opening a command line interface on a host identified
    to serve in the master &zgroup; and zone. Then, execute the following:
   </para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=gold --default</screen>
   <para>If the cluster has a single realm, specify the <option>--default</option>
   flag. If <option>--default</option> is specified, <command>radosgw-admin</command> uses this realm by default.
   If <option>--default</option> is not specified, adding zone-groups and zones requires specifying
   either the <option>--rgw-realm</option> flag or the <option>--realm-id</option> flag to identify the realm when
   adding &zgroup;s and zones.</para>
   <para>After creating the realm, <command>radosgw-admin</command> returns the realm
     configuration:</para>
<screen>
{
  "id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "name": "gold",
  "current_period": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "epoch": 1
}</screen>
   <note>
     <para>&ceph; generates a unique ID for the realm, which allows the
       renaming of a realm if the need arises.</para>
   </note>
 </sect3>

 <sect3 xml:id="ceph-rgw-fed-createmasterzonegrp">
  <title>Creating a Master &zgroup;</title>
  <para>
    A realm must have at least one &zgroup; to serve as the
    master &zgroup; for the realm. Create a new master &zgroup; for
    the multi-site configuration by opening a command line interface
    on a host identified to serve in the master &zgroup; and zone.
    Create a master &zgroup; called <literal>us</literal> by executing the following:</para>
<screen>&prompt.cephuser;radosgw-admin zonegroup create --rgw-zonegroup=us \
--endpoints=http://rgw1:80 --master --default</screen>
  <para>If the realm only has a single &zgroup;, specify the
    <option>--default</option> flag. If <option>--default</option> is specified,
    <command>radosgw-admin</command> uses this &zgroup; by default when adding new zones.
    If <option>--default</option> is not specified, adding zones requires
    either the <option>--rgw-zonegroup</option> flag or the <option>--zonegroup-id</option> flag to identify the
    &zgroup; when adding or modifying zones.</para>
  <para>After creating the master &zgroup;, <command>radosgw-admin</command> returns
  the &zgroup; configuration. For example:</para>
<screen>{
 "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
 "name": "us",
 "api_name": "us",
 "is_master": "true",
 "endpoints": [
     "http:\/\/rgw1:80"
 ],
 "hostnames": [],
 "hostnames_s3website": [],
 "master_zone": "",
 "zones": [],
 "placement_targets": [],
 "default_placement": "",
 "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
}</screen>
</sect3>
 <sect3 xml:id="ceph-rgw-fed-masterzone">
  <title>Creating a Master Zone</title>
  <important>
    <para>
       Zones need to be created on a &cogw; node that will be within the zone.
    </para>
  </important>
  <para>
   Create a new master zone for the multi-site configuration by opening
   a command line interface on a host identified to serve in the master
   &zgroup; and zone. Execute the following:
  </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --rgw-zone=us-east-1 \
--endpoints=http://rgw1:80 --access-key=<replaceable>SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>SYSTEM_SECRET_KEY</replaceable></screen>
  <note>
    <para>
      The <option>--access-key</option> and <option>--secret</option> options are not
      specified in the above example. These settings are added to the zone once the
      user is created in the next section.</para>
  </note>
  <para>After creating the master zone, <command>radosgw-admin</command> returns
  the zone configuration. For example:</para>
<screen>
  {
      "id": "56dfabbb-2f4e-4223-925e-de3c72de3866",
      "name": "us-east-1",
      "domain_root": "us-east-1.rgw.meta:root",
      "control_pool": "us-east-1.rgw.control",
      "gc_pool": "us-east-1.rgw.log:gc",
      "lc_pool": "us-east-1.rgw.log:lc",
      "log_pool": "us-east-1.rgw.log",
      "intent_log_pool": "us-east-1.rgw.log:intent",
      "usage_log_pool": "us-east-1.rgw.log:usage",
      "reshard_pool": "us-east-1.rgw.log:reshard",
      "user_keys_pool": "us-east-1.rgw.meta:users.keys",
      "user_email_pool": "us-east-1.rgw.meta:users.email",
      "user_swift_pool": "us-east-1.rgw.meta:users.swift",
      "user_uid_pool": "us-east-1.rgw.meta:users.uid",
      "otp_pool": "us-east-1.rgw.otp",
      "system_key": {
          "access_key": "1555b35654ad1656d804",
          "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
      },
      "placement_pools": [
          {
              "key": "us-east-1-placement",
              "val": {
                  "index_pool": "us-east-1.rgw.buckets.index",
                  "storage_classes": {
                      "STANDARD": {
                          "data_pool": "us-east-1.rgw.buckets.data"
                      }
                  },
                  "data_extra_pool": "us-east-1.rgw.buckets.non-ec",
                  "index_type": 0
              }
          }
      ],
      "metadata_heap": "",
      "realm_id": ""
  }</screen>
</sect3>
  <sect3 xml:id="ceph-rgw-fed-deldefzonegrp">
   <title>Deleting the Default Zone and Group</title>
   <para>
    The default installation of &ogw; creates the default &zgroup; called
    <literal>default</literal>. Delete the default zone if it exists.
    Make sure to remove it from the default &zgroup; first.
   </para>
   <important>
     <para>The following steps assume a multi-site configuration using newly
       installed systems that are not storing data yet. <emphasis role="bold">Do not delete</emphasis>
       the default zone and its pools if you are already using it to store data,
       or the data will be deleted and unrecoverable.</para>
   </important>
<screen>&prompt.cephuser;radosgw-admin zonegroup delete --rgw-zonegroup=default</screen>
   <para>Delete the default pools in your &ceph; storage cluster if they exist:</para>
   <important>
     <para>The following step assumes a multi-site configuration using newly
       installed systems that are not currently storing data. <emphasis role="bold">Do not delete</emphasis>
       the default &zgroup; if you are already using it to store data.</para>
   </important>
<screen>&prompt.cephuser;ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.meta default.rgw.meta --yes-i-really-really-mean-it</screen>
   <warning>
     <para>
       If you delete the default &zgroup;, you are also deleting the system user.
       If your admin user keys are not propagated, the &ogw; management functionality of the &dashboard;
       will fail. Follow on to the next section to re-create your system user if
       you go ahead with this step.
     </para>
   </warning>
</sect3>
   <sect3 xml:id="ceph-rgw-fed-masterzone-createuser">
    <title>Creating System Users</title>
    <para>
     The <systemitem class="daemon">ceph-radosgw</systemitem> daemons must authenticate before pulling realm and
     period information. In the master zone, create a system user to
     facilitate authentication between daemons:
    </para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=zone.user \
--display-name="Zone User" --access-key=<replaceable>SYSTEM_ACCESS_KEY</replaceable> \
--secret=<replaceable>SYSTEM_SECRET_KEY</replaceable> --system</screen>
    <para>Make a note of the <option>access_key</option> and <option>secret_key</option>
    as the secondary zones require them to authenticate with the master zone.</para>
    <para>Add the system user to the master zone:</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-zone=us-east --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET</replaceable>
&prompt.cephuser;radosgw-admin period update --commit</screen>
  <para>Update the period to make the changes take effect:</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
</sect3>

   <sect3 xml:id="ceph-rgw-fed-masterzone-updateperiod">
    <title>Update the Period</title>
    <para>
     After updating the master zone configuration, update the period:
    </para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
   <para>After updating the period, <command>radosgw-admin</command> returns
         the period configuration. For example:</para>
<screen>{
  "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "epoch": 1, "predecessor_uuid": "", "sync_status": [], "period_map":
  {
    "id": "09559832-67a4-4101-8b3f-10dfcd6b2707", "zonegroups": [], "short_zone_ids": []
  }, "master_zonegroup": "", "master_zone": "", "period_config":
  {
     "bucket_quota": {
     "enabled": false, "max_size_kb": -1, "max_objects": -1
     }, "user_quota": {
       "enabled": false, "max_size_kb": -1, "max_objects": -1
     }
  }, "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7", "realm_name": "gold", "realm_epoch": 1
}</screen>
   <note>
     <para>Updating the period changes the epoch and ensures that other
       zones receive the updated configuration.</para>
   </note>
</sect3>
   <sect3 xml:id="update-ceph-config-file">
     <title>Update the &ceph; Configuration File</title>
     <para>Update the &ceph; configuration file on master zone hosts by adding
       the <literal>rgw_zone</literal> configuration option and the name
       of the master zone to the instance entry.</para>
<screen>[client.rgw.<replaceable>INSTANCE-NAME</replaceable>]
...
rgw_zone=<replaceable>ZONE-NAME</replaceable></screen>
     <para>For example:</para>
<screen>[client.rgw.rgw1]
host = rgw1
rgw frontends = "beast port=80"
rgw_zone=us-east</screen>
</sect3>

   <sect3 xml:id="ceph-rgw-fed-masterzone-startrgw">
    <title>Start the Gateway</title>
    <para>On the &ogw; host, start and enable the &cogw; service:
    </para>
<screen>&prompt.cephuser.ogw;systemctl start ceph-radosgw@rgw.`hostname -s`
&prompt.cephuser.ogw;systemctl enable ceph-radosgw@rgw.`hostname -s`</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-rgw-config-secondaryzone">
   <title>Configure Secondary Zones</title>
   <para>Zones within a &zgroup; replicate all data to ensure that each
     zone has the same data. When creating the secondary zone, execute
     all of the following operations on a host identified to serve the secondary zone.
   </para>
   <note>
     <para>To add a third zone, follow the same procedures as for adding
       the secondary zone. Use different zone name.</para>
   </note>
   <important>
     <para>You must execute metadata operations, such as user creation,
       on a host within the master zone. The master zone and the secondary
       zone can receive bucket operations, but the secondary zone redirects
       bucket operations to the master zone. If the master zone is down,
       bucket operations will fail.</para>
   </important>

   <sect3 xml:id="ceph-rgw-pull-realm">
     <title>Pull The Realm</title>
     <para>Using the URL path, access key, and secret of the master zone
       in the master &zgroup;, pull the realm configuration to the
       host. To pull a non-default realm, specify the realm
       using the <option>--rgw-realm</option> or <option>--realm-id</option>
       configuration options.</para>
<screen>&prompt.cephuser;radosgw-admin realm pull --url=<replaceable>url-to-master-zone-gateway</replaceable> --access-key=<replaceable>access-key</replaceable> --secret=<replaceable>secret</replaceable></screen>
     <note>
       <para>Pulling the realm also retrieves the remote’s current period
         configuration, and makes it the current period on this host as well.</para>
     </note>
     <para>If this realm is the default realm or the only realm, make the
       realm the default realm.</para>
<screen>&prompt.cephuser;radosgw-admin realm default --rgw-realm=<replaceable>REALM-NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="cceph-rgw-create-secondaryzone">
     <title>Create a Secondary Zone</title>
   <para>Create a secondary zone for the multi-site configuration by opening
     a command line interface on a host identified to serve the secondary
     zone. Specify the &zgroup; ID, the new zone name and an endpoint for
     the zone. <emphasis>Do not</emphasis> use the <option>--master</option> flag.
     All zones run in an active-active configuration by default. If the secondary
     zone should not accept write operations, specify the <option>--read-only</option> flag
     to create an active-passive configuration between the master zone and
     the secondary zone. Additionally, provide the <option>access_key</option> and <option>secret_key</option>
     of the generated system user stored in the master zone of the master
     &zgroup;. Execute the following:
   </para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=<replaceable>ZONE-GROUP-NAME</replaceable>\
                            --rgw-zone=<replaceable>ZONE-NAME</replaceable> --endpoints=<replaceable>URL</replaceable> \
                            --access-key=<replaceable>SYSTEM-KEY</replaceable> --secret=<replaceable>SECRET</replaceable>\
                            --endpoints=http://<replaceable>FQDN</replaceable>:80 \
                            [--read-only]</screen>
   <para>For example:</para>
<screen>&prompt.cephuser;radosgw-admin zone create --rgw-zonegroup=us --endpoints=http://rgw2:80 \
--rgw-zone=us-east-2 --access-key=<replaceable>SYSTEM_ACCESS_KEY</replaceable> --secret=<replaceable>SYSTEM_SECRET_KEY</replaceable>
{
  "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
  "name": "us-east-2",
  "domain_root": "us-east-2.rgw.data.root",
  "control_pool": "us-east-2.rgw.control",
  "gc_pool": "us-east-2.rgw.gc",
  "log_pool": "us-east-2.rgw.log",
  "intent_log_pool": "us-east-2.rgw.intent-log",
  "usage_log_pool": "us-east-2.rgw.usage",
  "user_keys_pool": "us-east-2.rgw.users.keys",
  "user_email_pool": "us-east-2.rgw.users.email",
  "user_swift_pool": "us-east-2.rgw.users.swift",
  "user_uid_pool": "us-east-2.rgw.users.uid",
  "system_key": {
      "access_key": "1555b35654ad1656d804",
      "secret_key": "h7GhxuBLTrlhVUyxSPUKUV8r\/2EI4ngqJxD7iBdBYLhwluN30JaT3Q=="
  },
  "placement_pools": [
      {
          "key": "default-placement",
          "val": {
              "index_pool": "us-east-2.rgw.buckets.index",
              "data_pool": "us-east-2.rgw.buckets.data",
              "data_extra_pool": "us-east-2.rgw.buckets.non-ec",
              "index_type": 0
          }
      }
  ],
  "metadata_heap": "us-east-2.rgw.meta",
  "realm_id": "815d74c2-80d6-4e63-8cfc-232037f7ff5c"
}</screen>
   <important>
     <para>The following steps assume a multi-site configuration
       using newly installed systems that are not storing data.
       <emphasis role="bold">Do not delete</emphasis> the default zone and its pools if you are
       already using it to store data, or the data will be lost and
       unrecoverable.</para>
   </important>
   <para>Delete the default zone if needed:</para>
<screen>&prompt.cephuser;radosgw-admin zone rm --rgw-zone=default</screen>
   <para>Delete the default pools in your &ceph; storage cluster if needed:</para>
<screen>&prompt.cephuser;ceph osd pool rm default.rgw.control default.rgw.control --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.data.root default.rgw.data.root --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.gc default.rgw.gc --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.log default.rgw.log --yes-i-really-really-mean-it
&prompt.cephuser;ceph osd pool rm default.rgw.users.uid default.rgw.users.uid --yes-i-really-really-mean-it</screen>
</sect3>
  <sect3 xml:id="ceph-rgw-secondzone-update-config">
    <title>Update the &ceph; Configuration File</title>
    <para>Update the &ceph; configuration file on the secondary zone
      hosts by adding the <literal>rgw_zone</literal> configuration option and the
      name of the secondary zone to the instance entry.</para>
<screen>[client.rgw.<replaceable>INSTANCE-NAME</replaceable>]
...
rgw_zone=<replaceable>ZONE-NAME</replaceable></screen>
    <para>For example:</para>
<screen>[client.rgw.rgw2]
host = rgw2
rgw frontends = "civetweb port=80"
rgw_zone=us-west</screen>
</sect3>

   <sect3 xml:id="ceph-rgw-fed-secondzone-updateperiod">
    <title>Update the Period</title>
    <para>After updating the master zone configuration, update the period:</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit
{
  "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
  "epoch": 2,
  "predecessor_uuid": "09559832-67a4-4101-8b3f-10dfcd6b2707",
  "sync_status": [ "[...]"
  ],
  "period_map": {
      "id": "b5e4d3ec-2a62-4746-b479-4b2bc14b27d1",
      "zonegroups": [
          {
              "id": "d4018b8d-8c0d-4072-8919-608726fa369e",
              "name": "us",
              "api_name": "us",
              "is_master": "true",
              "endpoints": [
                  "http:\/\/rgw1:80"
              ],
              "hostnames": [],
              "hostnames_s3website": [],
              "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "zones": [
                  {
                      "id": "83859a9a-9901-4f00-aa6d-285c777e10f0",
                      "name": "us-east-1",
                      "endpoints": [
                          "http:\/\/rgw1:80"
                      ],
                      "log_meta": "true",
                      "log_data": "false",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  },
                  {
                      "id": "950c1a43-6836-41a2-a161-64777e07e8b8",
                      "name": "us-east-2",
                      "endpoints": [
                          "http:\/\/rgw2:80"
                      ],
                      "log_meta": "false",
                      "log_data": "true",
                      "bucket_index_max_shards": 0,
                      "read_only": "false"
                  }

              ],
              "placement_targets": [
                  {
                      "name": "default-placement",
                      "tags": []
                  }
              ],
              "default_placement": "default-placement",
              "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7"
          }
      ],
      "short_zone_ids": [
          {
              "key": "83859a9a-9901-4f00-aa6d-285c777e10f0",
              "val": 630926044
          },
          {
              "key": "950c1a43-6836-41a2-a161-64777e07e8b8",
              "val": 4276257543
          }

      ]
  },
  "master_zonegroup": "d4018b8d-8c0d-4072-8919-608726fa369e",
  "master_zone": "83859a9a-9901-4f00-aa6d-285c777e10f0",
  "period_config": {
      "bucket_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      },
      "user_quota": {
          "enabled": false,
          "max_size_kb": -1,
          "max_objects": -1
      }
  },
  "realm_id": "4a367026-bd8f-40ee-b486-8212482ddcd7",
  "realm_name": "gold",
  "realm_epoch": 2
}</screen>
   <note>
     <para>Updating the period changes the epoch and ensures that other zones
       receive the updated configuration.</para>
   </note>
   </sect3>
   <sect3 xml:id="ceph-rgw-fed-secondzone-startrgw">
    <title>Start the &ogw;</title>
    <para>On the &ogw; host, start and enable the &cogw; service:</para>
<screen>&prompt.cephuser.ogw;systemctl start ceph-radosgw@rgw.us-east-2
&prompt.cephuser.ogw;systemctl enable ceph-radosgw@rgw.us-east-2</screen>
   </sect3>
   <sect3 xml:id="ceph-rgw-check-sync-status">
    <title>Check Synchronization Status</title>
    <para>Once the secondary zone is up and running, check the synchronization
      status. Synchronization copies users and buckets created in the
      master zone to the secondary zone.</para>
<screen>&prompt.cephuser;radosgw-admin sync status</screen>
    <para>The output provides the status of synchronization operations.
      For example:</para>
<screen>realm f3239bc5-e1a8-4206-a81d-e1576480804d (gold)
    zonegroup c50dbb7e-d9ce-47cc-a8bb-97d9b399d388 (us)
         zone 4c453b70-4a16-4ce8-8185-1893b05d346e (us-west)
metadata sync syncing
              full sync: 0/64 shards
              metadata is caught up with master
              incremental sync: 64/64 shards
    data sync source: 1ee9da3e-114d-4ae3-a8a4-056e8a17f532 (us-east)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source</screen>
    <note>
      <para>Secondary zones accept bucket operations; however, secondary zones
        redirect bucket operations to the master zone and then synchronize with
        the master zone to receive the result of the bucket operations.
        If the master zone is down, bucket operations executed on the
        secondary zone will fail, but object operations should succeed.</para>
     </note>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-rgw-maintenance">
   <title>Maintenance</title>

   <sect3 xml:id="ceph-rgw-check-sync">
     <title>Checking the Sync Status</title>
     <para>Information about the replication status of a zone can be
       queried with:</para>
<screen>&prompt.cephuser;radosgw-admin sync status
        realm b3bc1c37-9c44-4b89-a03b-04c269bea5da (gold)
    zonegroup f54f9b22-b4b6-4a0e-9211-fa6ac1693f49 (us)
         zone adce11c9-b8ed-4a90-8bc5-3fc029ff0816 (us-west)
        metadata sync syncing
              full sync: 0/64 shards
              incremental sync: 64/64 shards
              metadata is behind on 1 shards
              oldest incremental change not applied: 2017-03-22 10:20:00.0.881361s
    data sync source: 341c2d81-4574-4d08-ab0f-5a2a7b168028 (us-east)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source
              source: 3b5d1a3f-3f27-4e4a-8f34-6072d4bb1275 (us-3)
                      syncing
                      full sync: 0/128 shards
                      incremental sync: 128/128 shards
                      data is caught up with source</screen>
    </sect3>
    <sect3 xml:id="ceph-rgw-metadata-master">
      <title>Changing the Metadata Master Zone</title>
      <important>
        <para>Be careful when changing which zone is the metadata master.
          If a zone has not finished syncing metadata from the current master
          zone, it is unable to serve any remaining entries when promoted
          to master and those changes will be lost. For this reason, we recommend waiting for
          a zone’s <command>radosgw-admin</command> sync status to catch up on metadata sync before
          promoting it to master.
          Similarly, if changes to metadata are being processed by the current master zone
          while another zone is being promoted to master, those changes are likely to be
          lost. To avoid this, we recommend shutting down any &ogw; instances on the previous
          master zone. After promoting another zone, its new period
          can be fetched with <command>radosgw-admin</command> period pull and the gateway(s) can be restarted.</para>
      </important>
      <para>To promote a zone (for example, zone <literal>us-west</literal> in &zgroup; <literal>us</literal>) to
        metadata master, run the following commands on that zone:</para>
<screen>&prompt.cephuser.ogw;radosgw-admin zone modify --rgw-zone=us-west --master
&prompt.cephuser.ogw;radosgw-admin zonegroup modify --rgw-zonegroup=us --master
&prompt.cephuser.ogw;radosgw-admin period update --commit</screen>
      <para>This generates a new period, and the &ogw; instance(s)
        in zone <literal>us-west</literal> sends this period to other zones.</para>
    </sect3>
   </sect2>
   <sect2 xml:id="ceph-rgw-failover-dr">
     <title>Failover and Disaster Recovery</title>
     <para>If the master zone should fail, failover to the secondary
       zone for disaster recovery.</para>
     <procedure>
       <step>
         <para>Make the secondary zone the master and default zone.
           For example:</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --master --default</screen>
         <para>By default, &cogw; runs in an active-active
           configuration. If the cluster was configured to run in an active-passive
           configuration, the secondary zone is a read-only zone. Remove the
           <option>--read-only</option> status to allow the zone to receive
           write operations. For example:</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --master --default \
                                                   --read-only=false
</screen>
       </step>
       <step>
         <para>Update the period to make the changes take effect:</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
       </step>
       <step>
         <para>Restart the &cogw;:</para>
<screen>&prompt.cephuser;systemctl restart ceph-radosgw@rgw.`hostname -s`</screen>
       </step>
     </procedure>
     <para>If the former master zone recovers, revert the operation.</para>
     <procedure>
       <step>
         <para>From the recovered zone, pull the latest realm configuration
           from the current master zone.</para>
<screen>&prompt.cephuser;radosgw-admin realm pull --url=<replaceable>URL-TO-MASTER-ZONE-GATEWAY</replaceable> \
                           --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET</replaceable>
</screen>
       </step>
       <step>
         <para>Make the recovered zone the master and default zone:</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --master --default</screen>
       </step>
       <step>
         <para>Update the period to make the changes take effect:</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
       </step>
       <step>
         <para>Restart the &cogw; in the recovered zone:</para>
<screen>&prompt.cephuser.ogw;systemctl restart ceph-radosgw@rgw.`hostname -s`</screen>
       </step>
       <step>
         <para>If the secondary zone needs to be a read-only configuration, update the secondary zone:</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-zone=<replaceable>ZONE-NAME</replaceable> --read-only</screen>
       </step>
       <step>
         <para>Update the period to make the changes take effect:</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
       </step>
       <step>
         <para>Restart the &cogw; in the secondary zone:</para>
<screen>&prompt.cephuser.ogw;systemctl restart ceph-radosgw@rgw.`hostname -s`</screen>
       </step>
     </procedure>
   </sect2>
   <sect2 xml:id="ceph-rgw-single-site-multi">
     <title>Migrating a Single Site System to Multi-Site</title>
     <para>To migrate from a single site system with a default &zgroup;
       and zone to a multi-site system, use the following steps:</para>
     <procedure>
       <step>
         <para>Create a realm. Replace <replaceable>NAME</replaceable> with
         the realm name.</para>
<screen>&prompt.cephuser;radosgw-admin realm create --rgw-realm=<replaceable>NAME</replaceable> --default</screen>
       </step>
       <step>
         <para>Create a system user. Replace <replaceable>USER-ID</replaceable> with the username.
           Replace <replaceable>DISPLAY-NAME</replaceable> with a display name.
           Only the display name may contain spaces.</para>
<screen>&prompt.cephuser;radosgw-admin user create --uid=<replaceable>USER-ID</replaceable> --display-name="<replaceable>DISPLAY-NAME</replaceable>"\
                            --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET-KEY</replaceable> --system</screen>
       </step>
       <step>
         <para>Rename the default zone and &zgroup;. Replace
           <replaceable>NAME</replaceable> with the &zgroup; or zone name.</para>
<screen>&prompt.cephuser;radosgw-admin zonegroup rename --rgw-zonegroup default --zonegroup-new-name=<replaceable>NAME</replaceable>
&prompt.cephuser;radosgw-admin zone rename --rgw-zone default --zone-new-name us-east-1 --rgw-zonegroup=<replaceable>NAME</replaceable></screen>
       </step>
       <step>
         <para>Configure the master &zgroup;. Replace <replaceable>NAME</replaceable> with the realm
           or &zgroup; name. Replace <replaceable>FQDN</replaceable> with the fully
           qualified domain name(s) in the &zgroup;.</para>
<screen>&prompt.cephuser;radosgw-admin zonegroup modify --rgw-realm=<replaceable>NAME</replaceable> --rgw-zonegroup=<replaceable>NAME</replaceable> --endpoints http://<replaceable>FQDN</replaceable>:80 --master --default</screen>
       </step>
       <step>
         <para>Configure the master zone. Replace <replaceable>NAME</replaceable> with the realm, &zgroup;
           or zone name. Replace <replaceable>FQDN</replaceable> with the fully
           qualified domain name(s) in the &zgroup;.</para>
<screen>&prompt.cephuser;radosgw-admin zone modify --rgw-realm=<replaceable>NAME</replaceable> --rgw-zonegroup=<replaceable>NAME</replaceable> \
                            --rgw-zone=<replaceable>NAME</replaceable> --endpoints http://<replaceable>FQDN</replaceable>:80 \
                            --access-key=<replaceable>ACCESS-KEY</replaceable> --secret=<replaceable>SECRET-KEY</replaceable> \
                            --master --default</screen>
       </step>
       <step>
         <para>Commit the updated configuration.</para>
<screen>&prompt.cephuser;radosgw-admin period update --commit</screen>
       </step>
       <step>
         <para>Restart the &cogw;.</para>
<screen>&prompt.cephuser.ogw;systemctl restart ceph-radosgw@rgw.`hostname -s`</screen>
       </step>
     </procedure>
     <para>After completing this procedure, configure a
       secondary zone to create a secondary zone in the master &zgroup;.</para>
   </sect2>
</sect1>
 <sect1 xml:id="ogw-haproxy">
  <title>Load Balancing the &ogw; Servers with &haproxy;</title>

  <para>
   You can use the &haproxy; load balancer to distribute all requests across
   multiple &ogw; back-end servers. Refer to
   <link
    xlink:href="https://documentation.suse.com/sle-ha/15-SP1/single-html/SLE-HA-guide/#sec-ha-lb-haproxy"/>
   for more details on configuring &haproxy;.
  </para>

  <para>
   Following is a simple configuration of &haproxy; for balancing &ogw; nodes
   using round robin as the balancing algorithm:
  </para>

<screen>
&prompt.user;cat /etc/haproxy/haproxy.cfg
[...]
frontend <replaceable>HTTPS_FRONTEND</replaceable>
bind *:443 crt <replaceable>path-to-cert.pem</replaceable> [ciphers: ... ]
default_backend rgw

backend rgw
mode http
balance roundrobin
server rgw_server1 <replaceable>RGW-ENDPOINT1</replaceable> weight 1 maxconn 100 check
server rgw_server2 <replaceable>RGW-ENDPOINT2</replaceable> weight 1 maxconn 100 check
[...]
</screen>
 </sect1>
</chapter>
