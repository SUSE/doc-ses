<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="tuning-ceph">
 <title>&ceph; Tuning</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
   &ceph; includes a telemetry module that provides anonymized information
   back to the &ceph; developer community. The information contained in
   the telemetry report provides information that helps the developers
   prioritize efforts and identify areas where more work may be needed.
   It may be necessary to enable the telemetry module before turning it on.
   To enable the module:
 </para>
<screen>
ceph mgr module enable telemetry
</screen>
 <para>
   To turn on telemetry reporting use the following command:
 </para>
<screen>
ceph telemetry on
</screen>
 <para>
   Additional information about the &ceph; telemetry module may be
   found in the <xref linkend="book-storage-admin"/>.
 </para>
 <sect1 xml:id="tuning-obtaining-metrics">
   <title>Obtaining Ceph Metrics</title>
   <para>
     Before adjusting &ceph; tunables, it is helpful to have an understanding
     of the critical metrics to monitor and what they indicate. Many of
     these parameters are found by dumping raw data from the daemons.
     This is done by means of the <command>ceph daemon dump</command> command. The following
     example shows the dump command being utilized for <literal>osd.104</literal>.
   </para>
<screen>
ceph --admin-daemon /var/run/ceph/ceph-osd.104.asok perf dump
</screen>
   <para>
     Starting with the &ceph; Nautilus release, the following command may be
     used as well:
   </para>
<screen>
ceph daemon osd.104 perf dump
</screen>
   <para>
     The output of the command is quite lengthy and may benefit from being
     redirected to a file.
   </para>
 </sect1>
 <sect1 xml:id="tuning-tuning-persistent">
   <title>Making Tuning Persistent</title>
   <para>
     To make parameter adjustment persistent requires modifying the
     <filename>/etc/ceph/ceph.conf</filename> file. This is best done
     through modifying the source component files that &deepsea; uses
     to manage the cluster. Each section is identified with a header such as:
   </para>
<screen>
  [global]
  [osd]
  [mds]
  [mon]
  [mgr]
  [client]
</screen>
   <para>
     The section of the configuration is tuned by modifying the correct
     <filename>[sectionname].conf</filename> in the <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>
     directory. After modifying the configuration file, replace <literal>master</literal>
     with the master minion-name (usually the admin node). The result is that
     the changes are pushed to all cluster nodes.
   </para>
<screen>
  salt 'master' state.apply ceph.configuration.create
  salt '*' state.apply ceph.configuration
</screen>
   <para>
     Changes made in this way require the affected services to be restarted
     before taking effect. It is also possible to deploy these files before
     running stage 2 of the &productname; deployment process. It
     is especially desirable to do so if changing the settings that
     require node or device re-deployment.
   </para>
 </sect1>
 <sect1 xml:id="tuning-core">
   <title>Core</title>
   <sect2>
     <title>Logging</title>
     <para>
       It is possible to disable all logging to reduce latency
       in the various codepaths.
     </para>
     <warning>
       <para>
         This tuning should be used with caution and understanding that
         logging <emphasis>will</emphasis> need to be re-enabled should support be required.
         This implies that an incident would need to be reproduced
         <emphasis>after</emphasis> logging is re-enabled.
       </para>
     </warning>
<screen>
  debug ms=0
  debug mds=0
  debug osd=0
  debug optracker=0
  debug auth=0
  debug asok=0
  debug bluestore=0
  debug bluefs=0
  debug bdev=0
  debug kstore=0
  debug rocksdb=0
  debug eventtrace=0
  debug default=0
  debug rados=0
  debug client=0
  debug perfcounter=0
  debug finisher=0
</screen>
   </sect2>
   <sect2>
     <title>Authentication Parameters</title>
     <para>
       Under certain conditions where the cluster is physically secure
       and isolated inside a secured network with no external exposure,
       it is possible to disable &cephx;.
       There are two levels at which &cephx; can be disabled. The first
       is to disable signing of authentication traffic. This can be
       accomplished with the following settings:
     </para>
<screen>
cephx_require_signatures = <replaceable>false</replaceable>
cephx_cluster_require_signatures = <replaceable>false</replaceable>
cephx_sign_messages = <replaceable>false</replaceable>
</screen>
     <para>
       The second level of tuning completely disables &cephx; authentication.
       This should only be done on networks that are isolated from public
       network infrastructure.
       This change is achieved by adding the following three lines in the
       global section:
     </para>
<screen>
auth cluster required = none
auth service required = none
auth client required = none
</screen>
   </sect2>
   <sect2>
     <title>RADOS Operations</title>
     <para>
       The backend processes for performing RADOS operations show up
       in <literal>throttle-*objector_ops</literal> when dumping various daemons. If
       there is too much time being spent in <literal>wait</literal>, there may be some
       performance to gain by increasing the memory for in-flight
       ops or by increasing the total number of inflight operations overall.
     </para>
<screen>
objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576
</screen>
   </sect2>
   <sect2>
     <title>OSD parameters</title>
     <para>
       Increasing the number of <literal>op threads</literal> may be helpful with SSD
       and NVMe devices, as it provides more work queues for operations.
     </para>
<screen>
osd_op_num_threads_per_shard = 4
</screen>
   </sect2>
   <sect2>
     <title>RocksDB or WAL device</title>
     <para>
       In checking the performance of &bluestore;, it is important to
       understand if your metadata is spilling over from the high-speed
       device, if defined, to the bulk data storage device. The parameters
       useful in this case are found under bluefs <literal>slow_used_bytes</literal>.
       If <literal>slow_used_bytes</literal> is greater than zero, the cluster is using
       the storage device instead of the RocksDB/WAL device. This
       is an indicator that more space needs to be allocated to RocksDB/WAL.
     </para>
     <para>
       Starting with the &ceph; Nautilus release, spillover is shown
       in the output of the <command>ceph health</command> command.
     </para>
     <para>
       The process of allocating more space depends on how the OSD
       was deployed. If it was deployed by a version prior to
       &productname; 6, the OSD will need to be re-deployed.
       If it was deployed with version 6 or after, it may be possible to
       expand the LVM that the RocksDB and WAL reside on, subject to available space.
     </para>
   </sect2>
   <sect2>
     <title>&bluestore; parameters</title>
     <para>
       &ceph; is thin provisioned, including the
       Write-Ahead Log (WAL) files. By pre-extending the files for the WAL,
       time is saved by not having to engage the allocator. It also
       potentially reduces the likelihood of fragmentation of the WAL files.
       This likely only provides benefit during the early life of the cluster.
     </para>
<screen>
bluefs_preextend_wal_files=1
</screen>
     <para>
       &bluestore; has the ability to perform buffered writes. Buffered writes
       enable populating the read cache during the write process. This
       setting, in effect, changes the &bluestore; cache into a write-through cache.
     </para>
<screen>
bluestore_default_buffered_write = true
</screen>
     <para>
       To prevent writes to the WAL when using a fast device, such as
       SSD and NVMe, set:
     </para>
<screen>
prefer_deferred_size_ssd=0 (pre-deployment)
</screen>
   </sect2>
   <sect2>
     <title>&bluestore; Allocation Size</title>
     <important>
       <para>
         The following settings are not necessary for fresh deployments. Apply
         only to upgraded deployments or early &productname; 6
         deployments as they may still benefit.
       </para>
     </important>
     <para>
       The following settings have been shown to slightly improve small
       object write performance under mixed workload conditions.
       Reducing the <literal>alloc_size</literal> to 4&nbsp;kB helps reduce write
       amplification for small objects and with erasure coded pools
       of smaller objects. This change needs to be done before OSD
       deployment. If done after the fact, the OSDs will need to be
       re-deployed for it to take effect.
     </para>
     <para>
       It is advised that spinning media continue to use 64&nbsp;kB while
       SSD/NVMe are likely to benefit from setting to 4&nbsp;kB.
     </para>
<screen>
min_alloc_size_ssd=4096
min_alloc_size_hdd=65536
</screen>
      <warning>
        <para>
          Setting the <literal>alloc_size_ssd</literal> to 64&nbsp;kB may reduce maximum
          throughput capability of the OSD.
        </para>
      </warning>
   </sect2>
   <sect2>
     <title>BlueStore Cache</title>
     <para>
       With many workloads, increasing the OSD cache can improve
       performance. This cache is a read cache that is populated
       on object read by default. The memory target is inclusive of
       the base RAM required for the OSD process and the total cache.
       By setting the cache minimum higher than the default, it is
       guaranteed that the value specified will be the minimum cache
       available for each OSD. The idea being that more low-probability
       cache hits may occur.
     </para>
<screen>
osd_memory_target = 6442450944
osd_memory_cache_min = 4294967296
</screen>
     <para>
       By default, &bluestore; automatically tunes cache ratios between
       data and key-value data. In some cases it may be helpful to
       manually tune the ratios or even increase the cache size.
       There are several relevant counters for the
        cache:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_onode_hits</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_misses</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_shard_hits</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_shard_misses</literal>
         </para>
       </listitem>
     </itemizedlist>
     <para>
       If the misses are high, it is worth experimenting with increasing
       the cache settings or adjusting the ratios.
     </para>
     <para>
       Adjusting the &bluestore; cache size above default has the
       potential to improve performance of small-block workloads.
       This can be done globally by adjusting the <option>_cache_size</option>
       value. By default, the cluster utilizes different values
       for HDD and SSD/NVMe devices. The best practice would be to
       increase the specific media cache you are interested in tuning:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_cache_size_hdd</literal> (default 1073741824 - 1&nbsp;GB)
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_cache_size_ssd</literal> (default 3221225472 - 3&nbsp;GB)
         </para>
       </listitem>
     </itemizedlist>
     <note>
       <para>
         If the cache size parameters are adjusted and auto mode is utilized,
         <option>osd_memory_target</option> should be adjusted to accomodate
         the OSD base RAM and cache allocation.
       </para>
     </note>
     <para>
       In some cases, manually tuning the cache allocation percentage
       may improve performance. This is achieved by modifying the
       disabling autotuning of the cache with this configuration line:
     </para>
<screen>
bluestore_cache_autotune=0
</screen>
     <para>
       Changing this value invalidates tuning of the
       <option>osd_memory_cache_min</option> value.
     </para>
     <para>
       The cache allocations are modified by adjusting the following:
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_cache_kv_ratio</literal> (default .4)
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_cache_meta_ratio values</literal> (default .4)
         </para>
       </listitem>
     </itemizedlist>
     <para>
       Any unspecified portion is used for caching the objects themselves.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-rbd">
   <title>RBD</title>
   <sect2>
     <title>RBD Cluster</title>
     <para>
       As RBD is a native protocol, the tuning is directly related to
       OSD or general &ceph; core options that are covered in previous sections.
     </para>
   </sect2>
   <sect2>
     <title>RBD Client</title>
     <para>
       Read ahead cache defaults to 512&nbsp;kB; test by tuning up and down
       on the client nodes.
     </para>
<screen>
echo {bytes} > /sys/block/rbd0/queue/read_ahead_kb
</screen>
     <para>
       If your workload performs large sequential reads such as
       backup and restore, then this can make a significant difference
       in restore performance.
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-cephfs">
   <title>CephFS</title>
   <para>
     Most of the performance tuning covered in this section pertains to
     the CephFS Metadata Servers. Because CephFS is a native protocol,
     much of the performance tuning is handled at the operating system,
     OSD and &bluestore; layers. Being a file system that is mounted by
     a client, there are some client options that are covered in the
     client section.
   </para>
   <sect2>
     <title>MDS Tuning</title>
     <para>
       In filesystems with millions of files, there is some advantage to
       utilizing very low-latency media, such as NVMe, for the cephfs metadata pool.
     </para>
     <para>
       Utlizing the <command>ceph-daemon perf dump</command> command, there is a significant
       amount of data that can be examined for the &ceph; Metadata Servers.
       It should be noted that the MDS perf counters only apply to metadata
       operations. The actual IO path is from clients straight to OSDs.
     </para>
     <para>
       CephFS supports multiple metadata servers. These servers can operate
       in a multiple-active mode to provide load balancing of the metadata
       operation requests. To identify whether the MDS infrastructure is
       under-performing, one would examine the MDS data for request count
       and reply latencies. This should be done during an idle period on
       the cluster to form a baseline and then compared when under load.
       If the average time for reply latencies climbs too high, the MDS
       server needs to be examined further to identify whether the number
       of active metadata servers should be augmented, or whether simply
       increasing the metadata server cache may be sufficient.
       A sample of the output from the general MDS data for count and
       reply latency are below:
     </para>
<screen>
  "mds": {
    # request count, interesting to get a sense of MDS load
           "request": 0,
           "reply": 0,
    # reply and the latencies of replies can point to load issues
           "reply_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           }
          }
</screen>
     <para>
       Examining the <literal>mds_mem</literal> section of the output can help with understanding
       how the cache is utilized. High inode counters can indicate that
       a large number of files are open concurrently. This generally
       indicates that more memory may need to be provided to the MDS.
       If MDS memory cannot be increased, additional active MDS daemons
       should be deployed.
     </para>
<screen>
  "mds_mem": {

           "ino": 13,
           "ino+": 13,
           "ino-": 0,
           "dir": 12,
           "dir+": 12,
           "dir-": 0,
           "dn": 10,
           "dn+": 10,
           "dn-": 0,
</screen>
     <para>
       A high <literal>cap</literal> count can indicate misbehaving clients. For example, clients
       that do not hand back caps. This may indicate that some clients
       need to be upgraded to a more recent version, or that the client
       needs to be investigated for possible issues.
     </para>
<screen>
  "cap": 0,
  "cap+": 0,
  "cap-": 0,
</screen>
     <para>
       This final section shows memory utilization. The RSS value
       is the current memory size used. If this is roughly equal
       to the <literal>mds_cache_memory_limit</literal>, the MDS could
       probably use more memory.
     </para>
<screen>
  "rss": 41524,
  "heap": 314072
},
</screen>
     <para>
       Another important aspect of tuning a distributed file system
       is recognizing problematic workloads. The output values
       below provide some insight to what the MDS daemon is spending
       its time on. Each heading has the same three attributes as the
       <literal>req_create_latency</literal>. With this information, it may be possible
       to better tune the workloads.
     </para>
<screen>
  "mds_server": {
           "dispatch_client_request": 0,
           "dispatch_server_request": 0,
           "handle_client_request": 0,
           "handle_client_session": 0,
           "handle_slave_request": 0,
           "req_create_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           },
           "req_getattr_latency": {},
           "req_getfilelock_latency": {},
           "req_link_latency": {},
           "req_lookup_latency": {},
           "req_lookuphash_latency": {},
           "req_lookupino_latency": {},
           "req_lookupname_latency": {},
           "req_lookupparent_latency": {},
           "req_lookupsnap_latency": {},
           "req_lssnap_latency": {},
           "req_mkdir_latency": {},
           "req_mknod_latency": {},
           "req_mksnap_latency": {},
           "req_open_latency": {},
           "req_readdir_latency": {},
           "req_rename_latency": {},
           "req_renamesnap_latency": {},
           "req_rmdir_latency": {},
           "req_rmsnap_latency": {},
           "req_rmxattr_latency": {},
           "req_setattr_latency": {},
           "req_setdirlayout_latency": {},
           "req_setfilelock_latency": {},
           "req_setlayout_latency": {},
           "req_setxattr_latency": {},
           "req_symlink_latency": {},
           "req_unlink_latency": {},
       }
</screen>
     <para>
       Tuning the metadata server cache allows for more metadata operations
       to come from RAM, resulting in improved performance. The example
       below sets the cache to 16&nbsp;GB.
     </para>
<screen>
mds_cache_memory_limit=17179869184
</screen>
   </sect2>
   <sect2>
     <title>CephFS - Client</title>
     <para>
       From the client side, there are a number of performance affecting
       mount options that can be employed. It is important to understand
       the potential impact on the applications being utilized before
       employing these options.</para>
      <para>
       The following mount options may be adjusted to improve
       performance, but we recommend that their impact is clearly
       understood prior to implementation in a production environment.
     </para>
     <variablelist>
       <varlistentry>
         <term>noacl</term>
         <listitem>
           <para>
             Setting this mount option disables POSIX Access Control Lists
             for the CephFS mount, lowering potential metadata overhead.
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>noatime</term>
         <listitem>
           <para>
             This option prevents the access time metadata for files
             from being updated.
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>nodiratime</term>
         <listitem>
           <para>
             Setting this option prevents the metadata for access time of
             a directory from being updated.
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>nocrc</term>
         <listitem>
           <para>
             This disables CephFS CRCs, thus relying on TCP Checksums for
             the correctness of the data to be verified.
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>rasize</term>
         <listitem>
           <para>
             Setting a larger read-ahead for the mount may increase
             performance for large, sequential operations. Default is 8&nbsp;MiB.
           </para>
         </listitem>
       </varlistentry>
     </variablelist>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-rgw">
   <title>RGW</title>
   <para>
     There are a large number of tunables for the Rados GateWay (RGW).
     These may be specific to the types of workloads being handled
     by the gateway and it may make sense to have different gateways
     handling distictly different workloads.
   </para>
   <sect2>
     <title>Sharding</title>
     <para>
       The ideal situation is to understand how many total
       objects a bucket will host as this allows a bucket to be created
       with an appropriate number of shards at the outset. To gather
       information on bucket sharding, issue:
     </para>
<screen>
radosgw-admin bucket limit check
</screen>
     <para>
       The output of this command appears like the following format:
     </para>
<screen>
  "user_id": "myusername",
          "buckets": [
              {
                  "bucket": "mybucketname",
                  "tenant": "",
                  "num_objects": 611493,
                  "num_shards": 50,
                  "objects_per_shard": 12229,
                  "fill_status": "OK"
              }
          ]
</screen>
     <para>
       By default, &ceph; reshards buckets to try and maintain
       reasonable performance. If it is known ahead of time how
       many shards a bucket may need, based on a ratio of 1
       shard per 100&nbsp;000 objects, it may be pre-sharded. This
       reduces contention and potential latency issues when resharding
       will occur. To pre-shard the bucket, it should be created and
       then submitted for sharding with the <command>rgw-admin</command> command.
       For example:
     </para>
<screen>
radosgw-admin bucket reshard --bucket={bucket name} --num-shards={prime number}
</screen>
     <para>
       Where the <literal>num-shards</literal> is a prime number. Each shard should
       represent about 100&nbsp;000 objects.
     </para>
   </sect2>
   <sect2>
     <title>Limiting Bucket Listing Results</title>
     <para>
       If a process relies on listing the buckets on a frequent basis
       to iterate through results, yet only uses a small number of
       results for each iteration it is useful to set the
       <literal>rgw_max_listing_results</literal> parameter.
     </para>
   </sect2>
   <sect2>
     <title>Parallel I/O Requests</title>
     <para>
       By default, the &rgw; process is limited to eight simultaneous I/O
       operations for the index. This can be adjusted with the
       <literal>rgw_bucket_index_max_aio</literal> parameter.
     </para>
     </sect2>
     <sect2>
       <title>Window Size</title>
     <para>
       When working with larger objects, increasing the size of the &rgw;
       windows for <literal>put</literal> and get can help with performance.
       Modify the following values in the &rgw; section of the configuration:
     </para>
<screen>
rgw put obj min window size = [size in bytes, 16MiB default]
rgw get obj min window size = [size in bytes, 16MiB default]
</screen>
     </sect2>
     <sect2>
       <title>Nagle's Algorithm</title>
     <para>
       Nagle's algorithm was introduced to maximize the use of buffers and
       attempt to reduce the number of small packets transmitted over the
       network. While this is helpful in lower bandwdith environments, it
       can represent a performance degredation in high-bandwidth environments.
       Disabling it from RGW nodes can improve performance. Including the
       following in the &ceph;configuation RGW section:
     </para>
<screen>
tcp_nodelay=1
</screen>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-admin-usage">
   <title>Administrative and Usage Choices</title>
   <sect2>
     <title>Data Protection Schemes</title>
     <para>
       The default replication setting keeps three total copies of every
       object written. The provides a high level of data protection by
       allowing up to two devices or nods to fail while still proecting
       the data.</para>
      <para>
       There are use cases where protecting the data is not important,
       but where performance is. In these cases, such as HPC scratch
       storage, it may be worthwhile to lower the replication count.
       This can be achieved by issuing a command such as:
     </para>
<screen>
ceph osd pool set rbd size 2
</screen>
   </sect2>
   <sect2>
     <title>Erasure Coding</title>
     <para>
       When using erasure coding, it is best to utilize optimized coding
       pool sizes. Experimental data suggests that the optimial pool sizes
       have either four or eight data chunks. It is also important to map
       this in relation to your failure domain model. If your cluster
       failure domain is at the node level, you will need at least <literal>k+m</literal>
       number of nodes. Similarly, if your failure domain it at the rack
       level, then your cluster needs to be spread over <literal>k+m</literal> racks. The
       key consideration is that distribution of the data in relation
       to the failure domain should be taken into consideration.
     </para>
     <para>
       When using erasure coding schemes with failure domains larger than
       a single node, the use of Local Reconstruction Codes (LRC) may
       be beneficial due to lowered utilization of the network backbone,
       especially during failure and recovery scenatios.
     </para>
     <para>
       There are particular use cases where erasure coding may even
       increase performance. These are mostly limited to large block (1&nbsp;MB+)
       sequential read/write workloads. This is due to the parallelization
       of I/O requests that occurs when splitting objects into chunks to
       write to multiple OSDs.
     </para>
   </sect2>
 </sect1>
</chapter>
