<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "generic-entities.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="tuning-ceph">
 <title>&ceph; tuning</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  &ceph; includes a telemetry module that provides anonymized information back
  to the &ceph; developer community. The information contained in the telemetry
  report provides information that helps the developers prioritize efforts and
  identify areas where more work may be needed. It may be necessary to enable
  the telemetry module before turning it on. To enable the module:
 </para>
<screen>&prompt.cephuser;ceph mgr module enable telemetry</screen>
 <para>
  To turn on telemetry reporting use the following command:
 </para>
<screen>&prompt.cephuser;ceph telemetry on</screen>
 <para>
  Additional information about the &ceph; telemetry module may be found in the
  <xref linkend="book-storage-admin"/>.
 </para>
 <sect1 xml:id="tuning-obtaining-metrics">
  <title>Obtaining &ceph; metrics</title>

  <para>
   Before adjusting &ceph; tunables, it is helpful to have an understanding of
   the critical metrics to monitor and what they indicate. Many of these
   parameters are found by dumping raw data from the daemons. This is done by
   means of the <command>ceph daemon dump</command> command. The following
   example shows the dump command being utilized for
   <literal>osd.104</literal>.
  </para>

<screen>
&prompt.cephuser;ceph --admin-daemon /var/run/ceph/ceph-osd.104.asok perf dump
</screen>

  <para>
   Starting with the &ceph; Nautilus release, the following command may be used
   as well:
  </para>

<screen>
&prompt.cephuser;ceph daemon osd.104 perf dump
</screen>

  <para>
   The output of the command is quite lengthy and may benefit from being
   redirected to a file.
  </para>
 </sect1>
 <sect1 xml:id="tuning-tuning-persistent">
  <title>Making tuning persistent</title>

  <para>
   To make parameter adjustment persistent requires modifying the configuration
   database. Refer to <xref linkend="cha-ceph-configuration"/> for more details
   about making persistent changes to &ceph; configuration.
  </para>
 </sect1>
 <sect1 xml:id="tuning-core">
  <title>Core</title>

  <sect2 xml:id="tuning-core-logging">
   <title>Disabling logging</title>
   <para>
    It is possible to disable all logging to reduce latency in the various
    codepaths.
   </para>
   <warning>
    <para>
     This tuning should be used with caution and understanding that logging
     <emphasis>will</emphasis> need to be re-enabled should support be
     required. This implies that an incident would need to be reproduced
     <emphasis>after</emphasis> logging is re-enabled.
    </para>
   </warning>
<screen>
  debug ms=0
  debug mds=0
  debug osd=0
  debug optracker=0
  debug auth=0
  debug asok=0
  debug bluestore=0
  debug bluefs=0
  debug bdev=0
  debug kstore=0
  debug rocksdb=0
  debug eventtrace=0
  debug default=0
  debug rados=0
  debug client=0
  debug perfcounter=0
  debug finisher=0
</screen>
  </sect2>

  <sect2 xml:id="tuning-core-auth-params">
   <title>Authenticating parameters</title>
   <para>
    Under certain conditions where the cluster is physically secure and
    isolated inside a secured network with no external exposure, it is possible
    to disable &cephx;. There are two levels at which &cephx; can be disabled.
    The first is to disable signing of authentication traffic. This can be
    accomplished with the following settings:
   </para>
<screen>
cephx_require_signatures = <replaceable>false</replaceable>
cephx_cluster_require_signatures = <replaceable>false</replaceable>
cephx_sign_messages = <replaceable>false</replaceable>
</screen>
   <para>
    The second level of tuning completely disables &cephx; authentication. This
    should only be done on networks that are isolated from public network
    infrastructure. This change is achieved by adding the following three lines
    in the global section:
   </para>
<screen>
auth cluster required = none
auth service required = none
auth client required = none
</screen>
  </sect2>

  <sect2 xml:id="tuning-core-rados-ops">
   <title>RADOS operations</title>
   <para>
    The backend processes for performing RADOS operations show up in
    <literal>throttle-*objector_ops</literal> when dumping various daemons. If
    there is too much time being spent in <literal>wait</literal>, there may be
    some performance to gain by increasing the memory for in-flight ops or by
    increasing the total number of inflight operations overall.
   </para>
<screen>
objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576
</screen>
  </sect2>

  <sect2 xml:id="tuning-core-osd-params">
   <title>OSD parameters</title>
   <para>
    Increasing the number of <literal>op threads</literal> may be helpful with
    SSD and NVMe devices, as it provides more work queues for operations.
   </para>
<screen>
osd_op_num_threads_per_shard = 4
</screen>
  </sect2>

  <sect2 xml:id="tuning-core-rocksdb-wal-device">
   <title>RocksDB or WAL device</title>
   <para>
    In checking the performance of &bluestore;, it is important to understand
    if your metadata is spilling over from the high-speed device, if defined,
    to the bulk data storage device. The parameters useful in this case are
    found under bluefs <literal>slow_used_bytes</literal>. If
    <literal>slow_used_bytes</literal> is greater than zero, the cluster is
    using the storage device instead of the RocksDB/WAL device. This is an
    indicator that more space needs to be allocated to RocksDB/WAL.
   </para>
   <para>
    Starting with the &ceph; Nautilus release, spillover is shown in the output
    of the <command>ceph health</command> command.
   </para>
   <para>
    The process of allocating more space depends on how the OSD was deployed.
    If it was deployed by a version prior to &productname; 6, the OSD will need
    to be re-deployed. If it was deployed with version 6 or after, it may be
    possible to expand the LVM that the RocksDB and WAL reside on, subject to
    available space.
   </para>
  </sect2>

  <sect2 xml:id="bluestore-parameters">
   <title>&bluestore; parameters</title>
   <para>
    &ceph; is thin provisioned, including the Write-Ahead Log (WAL) files. By
    pre-extending the files for the WAL, time is saved by not having to engage
    the allocator. It also potentially reduces the likelihood of fragmentation
    of the WAL files. This likely only provides benefit during the early life
    of the cluster.
   </para>
<screen>
bluefs_preextend_wal_files=1
</screen>
   <para>
    &bluestore; has the ability to perform buffered writes. Buffered writes
    enable populating the read cache during the write process. This setting, in
    effect, changes the &bluestore; cache into a write-through cache.
   </para>
<screen>
bluestore_default_buffered_write = true
</screen>
   <para>
    To prevent writes to the WAL when using a fast device, such as SSD and
    NVMe, set:
   </para>
<screen>
prefer_deferred_size_ssd=0 (pre-deployment)
</screen>
  </sect2>

  <sect2 xml:id="bluestore-alloc-size">
   <title>&bluestore; allocation size</title>
   <important>
    <para>
     The following settings are not necessary for fresh deployments. Apply only
     to upgraded deployments or early &productname; 6 deployments as they may
     still benefit.
    </para>
   </important>
   <para>
    The following settings have been shown to slightly improve small object
    write performance under mixed workload conditions. Reducing the
    <literal>alloc_size</literal> to 4&nbsp;kB helps reduce write amplification
    for small objects and with erasure coded pools of smaller objects. This
    change needs to be done before OSD deployment. If done after the fact, the
    OSDs will need to be re-deployed for it to take effect.
   </para>
   <para>
    It is advised that spinning media continue to use 64&nbsp;kB while SSD/NVMe
    are likely to benefit from setting to 4&nbsp;kB.
   </para>
<screen>
min_alloc_size_ssd=4096
min_alloc_size_hdd=65536
</screen>
   <warning>
    <para>
     Setting the <literal>alloc_size_ssd</literal> to 64&nbsp;kB may reduce
     maximum throughput capability of the OSD.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="bluestore-cache">
   <title>BlueStore cache</title>
   <para>
    Increasing the &bluestore; cache size can improve performance with many
    workloads. While &filestore; OSDs cache data in the kernel's page cache,
    &bluestore; OSDs cache data within the memory allocated by the OSD daemon
    itself. The OSD daemon will allocate memory up to its memory target (as
    controlled by the <literal>osd_memory_target</literal> parameter), and this
    determines the potential size of the &bluestore; cache. The &bluestore;
    cache is a read cache that by default is populated when objects are read.
    By setting the cache's minimum size higher than the default, it is
    guaranteed that the value specified will be the minimum cache available for
    each OSD. The idea being that more low-probability cache hits may occur.
   </para>
   <para>
    The default <literal>osd_memory_target</literal> value is 4&nbsp;GB: For
    example, each OSD daemon running on a node can be expected to consume that
    much memory. If a node's total RAM is significantly higher than
    <literal>number of OSDs &times; 4&nbsp;GB</literal> and there are no other
    daemons running on the node, performance can be increased by increasing the
    value of <literal>osd_memory_target</literal>. This should be done with
    care to ensure that the operating system will still have enough memory for
    its needs, while leaving a safety margin.
   </para>
   <para>
    If you want to ensure that the &bluestore; cache will not fall below a
    certain minimum, use the <literal>osd_memory_cache_min</literal> parameter.
    Here is an example (the values are expressed in bytes):
   </para>
<screen>
osd_memory_target = 6442450944
osd_memory_cache_min = 4294967296
</screen>
   <tip>
    <para>
     As a best practice, start with the full memory of the node. Deduct
     16&nbsp;GB or 32&nbsp;GB for the OS and then deduct appropriate amounts
     for any other workloads running on the node. For example, MDS cache if the
     MDS is colocated. Divide the remainder by the number of OSDs on that host.
     Ensure you leave room for improvement. For example:
    </para>
<screen>
(256 GB - 32 GB ) / 20 OSDs = 11,2 GB/OSD (max)
</screen>
    <para>
     Using this example, configure approximately 8 or 10&nbsp;GB per OSD.
    </para>
   </tip>
   <para>
    By default, &bluestore; automatically tunes cache ratios between data and
    key-value data. In some cases it may be helpful to manually tune the ratios
    or even increase the cache size. There are several relevant counters for
    the cache:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>bluestore_onode_hits</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>bluestore_onode_misses</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>bluestore_onode_shard_hits</literal>
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>bluestore_onode_shard_misses</literal>
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If the misses are high, it is worth experimenting with increasing the cache
    settings or adjusting the ratios.
   </para>
   <para>
    Adjusting the &bluestore; cache size above default has the potential to
    improve performance of small-block workloads. This can be done globally by
    adjusting the <option>_cache_size</option> value. By default, the cluster
    utilizes different values for HDD and SSD/NVMe devices. The best practice
    would be to increase the specific media cache you are interested in tuning:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>bluestore_cache_size_hdd</literal> (default 1073741824 -
      1&nbsp;GB)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>bluestore_cache_size_ssd</literal> (default 3221225472 -
      3&nbsp;GB)
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     If the cache size parameters are adjusted and auto mode is utilized,
     <option>osd_memory_target</option> should be adjusted to accomodate the
     OSD base RAM and cache allocation.
    </para>
   </note>
   <para>
    In some cases, manually tuning the cache allocation percentage may improve
    performance. This is achieved by modifying the disabling autotuning of the
    cache with this configuration line:
   </para>
<screen>
bluestore_cache_autotune=0
</screen>
   <para>
    Changing this value invalidates tuning of the
    <option>osd_memory_cache_min</option> value.
   </para>
   <para>
    The cache allocations are modified by adjusting the following:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <literal>bluestore_cache_kv_ratio</literal> (default .4)
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>bluestore_cache_meta_ratio values</literal> (default .4)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Any unspecified portion is used for caching the objects themselves.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-rbd">
  <title>RBD</title>

  <sect2 xml:id="tuning-rbd-cluster">
   <title>RBD cluster</title>
   <para>
    As RBD is a native protocol, the tuning is directly related to OSD or
    general &ceph; core options that are covered in previous sections.
   </para>
  </sect2>

  <sect2 xml:id="tuning-rbd-client">
   <title>RBD client</title>
   <para>
    Read ahead cache defaults to 512&nbsp;kB; test by tuning up and down on the
    client nodes.
   </para>
<screen>
echo {bytes} > /sys/block/rbd0/queue/read_ahead_kb
</screen>
   <para>
    If your workload performs large sequential reads such as backup and
    restore, then this can make a significant difference in restore
    performance.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-cephfs">
  <title>&cephfs;</title>

  <para>
   Most of the performance tuning covered in this section pertains to the
   &cephfs; Metadata Servers. Because &cephfs; is a native protocol, much of
   the performance tuning is handled at the operating system, OSD and
   &bluestore; layers. Being a file system that is mounted by a client, there
   are some client options that are covered in the client section.
  </para>

  <sect2 xml:id="tuning-cephfs-mds-tuning">
   <title>MDS tuning</title>
   <para>
    In filesystems with millions of files, there is some advantage to utilizing
    very low-latency media, such as NVMe, for the &cephfs; metadata pool.
   </para>
   <para>
    Utlizing the <command>ceph-daemon perf dump</command> command, there is a
    significant amount of data that can be examined for the &cephfs; Metadata
    Servers. It should be noted that the MDS perf counters only apply to
    metadata operations. The actual IO path is from clients straight to OSDs.
   </para>
   <para>
    &cephfs; supports multiple metadata servers. These servers can operate in a
    multiple-active mode to provide load balancing of the metadata operation
    requests. To identify whether the MDS infrastructure is under-performing,
    one would examine the MDS data for request count and reply latencies. This
    should be done during an idle period on the cluster to form a baseline and
    then compared when under load. If the average time for reply latencies
    climbs too high, the MDS server needs to be examined further to identify
    whether the number of active metadata servers should be augmented, or
    whether simply increasing the metadata server cache may be sufficient. A
    sample of the output from the general MDS data for count and reply latency
    are below:
   </para>
<screen>
  "mds": {
    # request count, interesting to get a sense of MDS load
           "request": 0,
           "reply": 0,
    # reply and the latencies of replies can point to load issues
           "reply_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           }
          }
</screen>
   <para>
    Examining the <literal>mds_mem</literal> section of the output can help
    with understanding how the cache is utilized. High inode counters can
    indicate that a large number of files are open concurrently. This generally
    indicates that more memory may need to be provided to the MDS. If MDS
    memory cannot be increased, additional active MDS daemons should be
    deployed.
   </para>
<screen>
  "mds_mem": {

           "ino": 13,
           "ino+": 13,
           "ino-": 0,
           "dir": 12,
           "dir+": 12,
           "dir-": 0,
           "dn": 10,
           "dn+": 10,
           "dn-": 0,
</screen>
   <para>
    A high <literal>cap</literal> count can indicate misbehaving clients. For
    example, clients that do not hand back caps. This may indicate that some
    clients need to be upgraded to a more recent version, or that the client
    needs to be investigated for possible issues.
   </para>
<screen>
  "cap": 0,
  "cap+": 0,
  "cap-": 0,
</screen>
   <para>
    This final section shows memory utilization. The RSS value is the current
    memory size used. If this is roughly equal to the
    <literal>mds_cache_memory_limit</literal>, the MDS could probably use more
    memory.
   </para>
<screen>
  "rss": 41524,
  "heap": 314072
},
</screen>
   <para>
    Another important aspect of tuning a distributed file system is recognizing
    problematic workloads. The output values below provide some insight to what
    the MDS daemon is spending its time on. Each heading has the same three
    attributes as the <literal>req_create_latency</literal>. With this
    information, it may be possible to better tune the workloads.
   </para>
<screen>
  "mds_server": {
           "dispatch_client_request": 0,
           "dispatch_server_request": 0,
           "handle_client_request": 0,
           "handle_client_session": 0,
           "handle_slave_request": 0,
           "req_create_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           },
           "req_getattr_latency": {},
           "req_getfilelock_latency": {},
           "req_link_latency": {},
           "req_lookup_latency": {},
           "req_lookuphash_latency": {},
           "req_lookupino_latency": {},
           "req_lookupname_latency": {},
           "req_lookupparent_latency": {},
           "req_lookupsnap_latency": {},
           "req_lssnap_latency": {},
           "req_mkdir_latency": {},
           "req_mknod_latency": {},
           "req_mksnap_latency": {},
           "req_open_latency": {},
           "req_readdir_latency": {},
           "req_rename_latency": {},
           "req_renamesnap_latency": {},
           "req_rmdir_latency": {},
           "req_rmsnap_latency": {},
           "req_rmxattr_latency": {},
           "req_setattr_latency": {},
           "req_setdirlayout_latency": {},
           "req_setfilelock_latency": {},
           "req_setlayout_latency": {},
           "req_setxattr_latency": {},
           "req_symlink_latency": {},
           "req_unlink_latency": {},
       }
</screen>
   <para>
    Tuning the metadata server cache allows for more metadata operations to
    come from RAM, resulting in improved performance. The example below sets
    the cache to 16&nbsp;GB.
   </para>
<screen>
mds_cache_memory_limit=17179869184
</screen>
  </sect2>

  <sect2 xml:id="tuning-cephfs-client">
   <title>Understanding the &cephfs; client</title>
   <para>
    From the client side, there are a number of performance affecting mount
    options that can be employed. It is important to understand the potential
    impact on the applications being utilized before employing these options.
   </para>
   <para>
    The following mount options may be adjusted to improve performance, but we
    recommend that their impact is clearly understood prior to implementation
    in a production environment.
   </para>
   <variablelist>
    <varlistentry>
     <term>noacl</term>
     <listitem>
      <para>
       Setting this mount option disables POSIX Access Control Lists for the
       &cephfs; mount, lowering potential metadata overhead.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noatime</term>
     <listitem>
      <para>
       This option prevents the access time metadata for files from being
       updated.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodiratime</term>
     <listitem>
      <para>
       Setting this option prevents the metadata for access time of a directory
       from being updated.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nocrc</term>
     <listitem>
      <para>
       This disables &cephfs; CRCs, thus relying on TCP Checksums for the
       correctness of the data to be verified.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>rasize</term>
     <listitem>
      <para>
       Setting a larger read-ahead for the mount may increase performance for
       large, sequential operations. Default is 8&nbsp;MiB.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-rgw">
  <title>&ogw;</title>

  <para>
   There are a large number of tunables for the &ogw; (RGW). These may be
   specific to the types of workloads being handled by the gateway and it may
   make sense to have different gateways handling distictly different
   workloads.
  </para>

  <sect2 xml:id="tuning-rgw-sharding">
   <title>Sharding</title>
   <para>
    The ideal situation is to understand how many total objects a bucket will
    host as this allows a bucket to be created with an appropriate number of
    shards at the outset. To gather information on bucket sharding, issue:
   </para>
<screen>&prompt.cephuser;radosgw-admin bucket limit check</screen>
   <para>
    The output of this command appears like the following format:
   </para>
<screen>
  "user_id": "myusername",
          "buckets": [
              {
                  "bucket": "mybucketname",
                  "tenant": "",
                  "num_objects": 611493,
                  "num_shards": 50,
                  "objects_per_shard": 12229,
                  "fill_status": "OK"
              }
          ]
</screen>
   <para>
    By default, &ceph; reshards buckets to try and maintain reasonable
    performance. If it is known ahead of time how many shards a bucket may
    need, based on a ratio of 1 shard per 100&nbsp;000 objects, it may be
    pre-sharded. This reduces contention and potential latency issues when
    resharding will occur. To pre-shard the bucket, it should be created and
    then submitted for sharding with the <command>rgw-admin</command> command.
    For example:
   </para>
<screen>&prompt.cephuser;radosgw-admin bucket reshard --bucket={bucket name} --num-shards={prime number}</screen>
   <para>
    Where the <literal>num-shards</literal> is a prime number. Each shard
    should represent about 100&nbsp;000 objects.
   </para>
  </sect2>

  <sect2 xml:id="tuning-rgw-limit-bucket-listing-results">
   <title>Limiting bucket listing results</title>
   <para>
    If a process relies on listing the buckets on a frequent basis to iterate
    through results, yet only uses a small number of results for each iteration
    it is useful to set the <literal>rgw_max_listing_results</literal>
    parameter.
   </para>
  </sect2>

  <sect2 xml:id="tuning-rgw-parallel-io-requests">
   <title>Parallel I/O requests</title>
   <para>
    By default, the &ogw; process is limited to eight simultaneous I/O
    operations for the index. This can be adjusted with the
    <literal>rgw_bucket_index_max_aio</literal> parameter.
   </para>
  </sect2>

  <sect2 xml:id="tuning-rgw-window-size">
   <title>Increasing window size</title>
   <para>
    When working with larger objects, increasing the size of the &ogw; windows
    for <literal>put</literal> and get can help with performance. Modify the
    following values in the &ogw; section of the configuration:
   </para>
<screen>
&prompt.cephuser;rgw put obj min window size = [size in bytes, 16MiB default]
&prompt.cephuser;rgw get obj min window size = [size in bytes, 16MiB default]
</screen>
  </sect2>

  <sect2 xml:id="tuning-rgw-nagles-algorithm">
   <title>Adding Nagle's algorithm</title>
   <para>
    Nagle's algorithm was introduced to maximize the use of buffers and attempt
    to reduce the number of small packets transmitted over the network. While
    this is helpful in lower bandwdith environments, it can represent a
    performance degredation in high-bandwidth environments. Disabling it from
    RGW nodes can improve performance. Including the following in the &ceph;
    configuation RGW section:
   </para>
<screen>
tcp_nodelay=1
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="tuning-admin-usage">
  <title>Administrative and usage choices</title>

  <sect2 xml:id="tuning-admin-usage-data-protect-schemes">
   <title>Data protection schemes</title>
   <para>
    The default replication setting keeps three total copies of every object
    written. The provides a high level of data protection by allowing up to two
    devices or nods to fail while still proecting the data.
   </para>
   <para>
    There are use cases where protecting the data is not important, but where
    performance is. In these cases, such as HPC scratch storage, it may be
    worthwhile to lower the replication count. This can be achieved by issuing
    a command such as:
   </para>
<screen>&prompt.cephuser;ceph osd pool set rbd size 2</screen>
  </sect2>

  <sect2 xml:id="tuning-admin-usage-erasure-coding">
   <title>Erasure coding</title>
   <para>
    When using erasure coding, it is best to utilize optimized coding pool
    sizes. Experimental data suggests that the optimial pool sizes have either
    four or eight data chunks. It is also important to map this in relation to
    your failure domain model. If your cluster failure domain is at the node
    level, you will need at least <literal>k+m</literal> number of nodes.
    Similarly, if your failure domain it at the rack level, then your cluster
    needs to be spread over <literal>k+m</literal> racks. The key consideration
    is that distribution of the data in relation to the failure domain should
    be taken into consideration.
   </para>
   <para>
    When using erasure coding schemes with failure domains larger than a single
    node, the use of Local Reconstruction Codes (LRC) may be beneficial due to
    lowered utilization of the network backbone, especially during failure and
    recovery scenatios.
   </para>
   <para>
    There are particular use cases where erasure coding may even increase
    performance. These are mostly limited to large block (1&nbsp;MB+)
    sequential read/write workloads. This is due to the parallelization of I/O
    requests that occurs when splitting objects into chunks to write to
    multiple OSDs.
   </para>
  </sect2>
 </sect1>
</chapter>
