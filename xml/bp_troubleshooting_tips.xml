<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="storage-tips">
 <title>Hints and Tips</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The chapter provides information to help you enhance performance of your
  &ceph; cluster and provides tips how to set the cluster up.
 </para>
 <sect1 xml:id="tips-orphaned-partitions">
  <title>Identifying Orphaned Partitions</title>

  <para>
   To identify possibly orphaned journal/WAL/DB devices, follow these steps:
  </para>

  <procedure>
   <step>
    <para>
     Pick the device that may have orphaned partitions and save the list of its
     partitions to a file:
    </para>
<screen>
&prompt.sminion;ls /dev/sdd?* &gt; /tmp/partitions
</screen>
   </step>
   <step>
    <para>
     Run <command>readlink</command> against all block.wal, block.db, and
     journal devices, and compare the output to the previously saved list of
     partitions:
    </para>
<screen>
&prompt.sminion;readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
    <para>
     The output is the list of partitions that are <emphasis>not</emphasis>
     used by &ceph;.
    </para>
   </step>
   <step>
    <para>
     Remove the orphaned partitions that do not belong to &ceph; with your
     preferred command (for example, <command>fdisk</command>,
     <command>parted</command>, or <command>sgdisk</command>).
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="tips-scrubbing">
  <title>Adjusting Scrubbing</title>

  <para>
   By default, &ceph; performs light scrubbing daily (find more details in
   <xref linkend="scrubbing"/>) and deep scrubbing weekly.
   <emphasis>Light</emphasis> scrubbing checks object sizes and checksums to
   ensure that placement groups are storing the same object data.
   <emphasis>Deep</emphasis> scrubbing checks an objectâ€™s content with that
   of its replicas to ensure that the actual contents are the same. The price
   for checking data integrity is increased I/O load on the cluster during the
   scrubbing procedure.
  </para>

  <para>
   The default settings allow &osd;s to initiate scrubbing at inappropriate
   times, such as during periods of heavy loads. Customers may experience
   latency and poor performance when scrubbing operations conflict with their
   operations. &ceph; provides several scrubbing settings that can limit
   scrubbing to periods with lower loads or during off-peak hours.
  </para>

  <para>
   If the cluster experiences high loads during the day and low loads late at
   night, consider restricting scrubbing to night time hours, such as 11pm till
   6am:
  </para>

<screen>
[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6
</screen>

  <para>
   If time restriction is not an effective method of determining a scrubbing
   schedule, consider using the <option>osd_scrub_load_threshold</option>
   option. The default value is 0.5, but it could be modified for low load
   conditions:
  </para>

<screen>
[osd]
osd_scrub_load_threshold = 0.25
</screen>
 </sect1>
 <sect1 xml:id="tips-stopping-osd-without-rebalancing">
  <title>Stopping OSDs without Rebalancing</title>

  <para>
   You may need to stop OSDs for maintenance periodically. If you do not want
   CRUSH to automatically rebalance the cluster in order to avoid huge data
   transfers, set the cluster to <literal>noout</literal> first:
  </para>

<screen>
&prompt.sminion;ceph osd set noout
</screen>

  <para>
   When the cluster is set to <literal>noout</literal>, you can begin stopping
   the OSDs within the failure domain that requires maintenance work. To identify the
   unique FSID of the cluster, run <command>ceph fsid</command>. To identify
   the &ogw; daemon name, run <command>ceph orch ps ---hostname <replaceable>HOSTNAME</replaceable></command>.
  </para>

<screen>systemctl stop ceph-<replaceable>FSID</replaceable>@<replaceable>DAEMON_NAME</replaceable></screen>

  <para>
   Find more information about operating &ceph; services and identifying their
   names in <xref linkend="cha-ceph-operating"/>.
  </para>

  <para>
   After you complete the maintenance, start OSDs again:
  </para>

<screen>
  &prompt.sminion;systemctl start ceph-<replaceable>FSID</replaceable>@osd.<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   After OSD services are started, unset the cluster from
   <literal>noout</literal>:
  </para>

<screen>
&prompt.cephuser;ceph osd unset noout
</screen>
 </sect1>
 <sect1 xml:id="Cluster-Time-Setting">
  <title>Time Synchronization of Nodes</title>

  <para>
   &ceph; requires precise time synchronization between all nodes.
  </para>

  <para>
   We recommend synchronizing all &ceph; cluster nodes with at least three
   reliable time sources that are located on the internal network. The internal
   time sources can point to a public time server or have their own time
   source.
  </para>

  <important>
   <title>Public Time Servers</title>
   <para>
    Do not synchronize all &ceph; cluster nodes directly with remote public
    time servers. With such a configuration, each node in the cluster has its
    own NTP daemon that communicates continually over the Internet with a set
    of three or four time servers that may provide slightly different times.
    This solution introduces a large degree of latency variability that makes
    it difficult or impossible to keep the clock drift under 0.05 seconds,
    which is what the &ceph; monitors require.
   </para>
  </important>

  <para>
   For details how to set up the NTP server refer to
   <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-ntp">&sls;
   Administration Guide</link>.
  </para>

  <para>
   Then to change the time on your cluster, do the following:
  </para>

  <important>
   <title>Setting Time</title>
   <para>
    You may face a situation when you need to set the time back, for example if
    the time changes from the summer to the standard time. We do not recommend
    to move the time backward for a longer period than the cluster is down.
    Moving the time forward does not cause any trouble.
   </para>
  </important>

  <procedure>
   <title>Time Synchronization on the Cluster</title>
   <step>
    <para>
     Stop all clients accessing the &ceph; cluster, especially those using
     iSCSI.
    </para>
   </step>
   <step>
    <para>
     Shut down your &ceph; cluster. On each node run:
    </para>
<screen>&prompt.root;systemctl stop ceph-<replaceable>FSID</replaceable>@<replaceable>DAEMON_NAME</replaceable></screen>
    <note>
     <para>
      If you use &ceph; and &ocloud;, stop also the &ocloud;.
     </para>
    </note>
   </step>
   <step>
    <para>
     Verify that your NTP server is set up correctly&mdash;all
     <systemitem class="daemon">chronyd</systemitem> daemons get their time
     from a source or sources in the local network.
    </para>
   </step>
   <step>
    <para>
     Set the correct time on your NTP server.
    </para>
   </step>
   <step>
    <para>
     Verify that NTP is running and working properly, on all nodes run:
    </para>
<screen>&prompt.root;systemctl status chronyd.service</screen>
   </step>
   <step>
    <para>
     Start all monitoring nodes and verify that there is no clock skew:
    </para>
<screen>&prompt.root;systemctl start <replaceable>target</replaceable></screen>
   </step>
   <step>
    <para>
     Start all OSD nodes.
    </para>
   </step>
   <step>
    <para>
     Start other &ceph; services.
    </para>
   </step>
   <step>
    <para>
     Start the &ocloud; if you have it.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage-bp-cluster-mntc-unbalanced">
  <title>Checking for Unbalanced Data Writing</title>

  <para>
   When data is written to OSDs evenly, the cluster is considered balanced.
   Each OSD within a cluster is assigned its <emphasis>weight</emphasis>. The
   weight is a relative number and tells &ceph; how much of the data should be
   written to the related OSD. The higher the weight, the more data will be
   written. If an OSD has zero weight, no data will be written to it. If the
   weight of an OSD is relatively high compared to other OSDs, a large portion
   of the data will be written there, which makes the cluster unbalanced.
  </para>

  <para>
   Unbalanced clusters have poor performance, and in the case that an OSD with
   a high weight suddenly crashes, a lot of data needs to be moved to other
   OSDs, which slows down the cluster as well.
  </para>

  <para>
   To avoid this, you should regularly check OSDs for the amount of data
   writing. If the amount is between 30% and 50% of the capacity of a group of
   OSDs specified by a given ruleset, you need to reweight the OSDs. Check for
   individual disks and find out which of them fill up faster than the others
   (or are generally slower), and lower their weight. The same is valid for
   OSDs where not enough data is written&mdash;you can increase their weight to
   have &ceph; write more data to them. In the following example, you will find
   out the weight of an OSD with ID 13, and reweight it from 3 to 3.05:
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>OSD Reweight by Utilization</title>
   <para>
    The <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable> command automates the process of
    reducing the weight of OSDs which are heavily overused. By default it will
    adjust the weights downward on OSDs which reached 120% of the average
    usage, but if you include threshold it will use that percentage instead.
   </para>
  </tip>
 </sect1>

<!--Removed the Btrfs Subvolume for <filename>/var/lib/ceph</filename> on &mon; Nodes
section. A.S -->

 <sect1 xml:id="storage-bp-srv-maint-fds-inc">
  <title>Increasing File Descriptors</title>

  <para>
   For OSD daemons, the read/write operations are critical to keep the &ceph;
   cluster balanced. They often need to have many files open for reading and
   writing at the same time. On the OS level, the maximum number of
   simultaneously open files is called 'maximum number of file descriptors'.
  </para>

  <para>
   To prevent OSDs from running out of file descriptors, you can override the
   OS default value and specify the number in
   <filename>/etc/ceph/ceph.conf</filename>, for example:
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   After you change <option>max_open_files</option>, you need to restart the
   OSD service on the relevant &ceph; node.
  </para>
 </sect1>
 <sect1 xml:id="storage-admin-integration">
  <title>Integration with Virtualization Software</title>

  <sect2 xml:id="storage-bp-integration-kvm">
   <title>Storing &kvm; Disks in &ceph; Cluster</title>
   <para>
    You can create a disk image for &kvm;-driven virtual machine, store it in a
    &ceph; pool, optionally convert the content of an existing image to it, and
    then run the virtual machine with <command>qemu-kvm</command> making use of
    the disk image stored in the cluster. For more detailed information, see
    <xref linkend="cha-ceph-kvm"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-integration-libvirt">
   <title>Storing &libvirt; Disks in &ceph; Cluster</title>
   <para>
    Similar to &kvm; (see <xref linkend="storage-bp-integration-kvm"/>), you
    can use &ceph; to store virtual machines driven by &libvirt;. The advantage
    is that you can run any &libvirt;-supported virtualization solution, such
    as &kvm;, &xen;, or LXC. For more information, see
    <xref linkend="cha-ceph-libvirt"/>.
   </para>
  </sect2>

  <sect2 xml:id="storage-bp-integration-xen">
   <title>Storing &xen; Disks in &ceph; Cluster</title>
   <para>
    One way to use &ceph; for storing &xen; disks is to make use of &libvirt;
    as described in <xref linkend="cha-ceph-libvirt"/>.
   </para>
   <para>
    Another option is to make &xen; talk to the <systemitem>rbd</systemitem>
    block device driver directly:
   </para>
   <procedure>
    <step>
     <para>
      If you have no disk image prepared for &xen;, create a new one:
     </para>
<screen>&prompt.cephuser;rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      List images in the pool <literal>mypool</literal> and check if your new
      image is there:
     </para>
<screen>&prompt.cephuser;rbd list mypool</screen>
    </step>
    <step>
     <para>
      Create a new block device by mapping the <literal>myimage</literal> image
      to the <systemitem>rbd</systemitem> kernel module:
     </para>
<screen>&prompt.cephuser;rbd map --pool mypool myimage</screen>
     <tip>
      <title>User Name and Authentication</title>
      <para>
       To specify a user name, use <option>--id
       <replaceable>user-name</replaceable></option>. Moreover, if you use
       <systemitem>cephx</systemitem> authentication, you must also specify a
       secret. It may come from a keyring or a file containing the secret:
      </para>
<screen>&prompt.cephuser;rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
      <para>
       or
      </para>
<screen>&cephuser;rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      List all mapped devices:
     </para>
<screen><command>rbd showmapped</command>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      Now you can configure &xen; to use this device as a disk for running a
      virtual machine. You can for example add the following line to the
      <command>xl</command>-style domain configuration file:
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-net-firewall">
  <title>Firewall Settings for &ceph;</title>

  <para>
   We recommend protecting the network cluster communication with SUSE
   Firewall. You can edit its configuration by selecting
   <menuchoice><guimenu>&yast;</guimenu><guimenu>Security and
   Users</guimenu><guimenu>Firewall</guimenu><guimenu>Allowed
   Services</guimenu></menuchoice>.
  </para>

  <para>
   Following is a list of &ceph; related services and numbers of the ports that
   they normally use:
  </para>

  <variablelist>
    <varlistentry>
     <term>&dashboard;</term>
     <listitem>
      <para>
       The &dashboard; binds to a specific TCP/IP address and TCP port. By
       default, the currently active &mgr; that hosts the dashboard binds to
       TCP port 8443 (or 8080 when SSL is disabled).
      </para>
     </listitem>
    </varlistentry>
   <varlistentry>
    <term>&mon;</term>
    <listitem>
     <para>
      Enable the <guimenu>Ceph MON</guimenu> service or port 6789 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&osd; or &mds;</term>
    <listitem>
     <para>
      Enable the <guimenu>Ceph OSD/MDS</guimenu> service or ports 6800-7300
      (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&igw;</term>
    <listitem>
     <para>
      Open port 3260 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&ogw;</term>
    <listitem>
     <para>
      Open the port where &ogw; communication occurs. It is set in
      <filename>/etc/ceph.conf</filename> on the line starting with
      <literal>rgw frontends =</literal>. Default is 80 for HTTP and 443 for
      HTTPS (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&ganesha;</term>
    <listitem>
     <para>
      By default, &ganesha; uses ports 2049 (NFS service, TCP) and 875 (rquota
      support, TCP). Refer to <xref linkend="ganesha-nfsport"/> for more
      information on changing the default &ganesha; ports.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>
      Open port 22 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      Open port 123 (UDP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&salt;</term>
    <listitem>
     <para>
      Open ports 4505 and 4506 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&grafana;</term>
    <listitem>
     <para>
      Open port 3000 (TCP).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&prometheus;</term>
    <listitem>
     <para>
      Open port 9095 (TCP).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-bp-network-test">
  <title>Testing Network Performance</title>
  <para>
    Intermittent and complete network failures will impact a &ceph; cluster.
    These two utilities can help in tracking down the cause and verifying expectations.
  </para>
  <sect2 xml:id="network-test-basic-diagnostics">
    <title>Basic Diagnostics</title>
    <para>
      Try the runner <literal>network.ping</literal> to ping between cluster
      nodes to see if individual interface can reach to specific interface and
      the average response time. Any specific response time much slower then
      average will also be reported. For example:
    </para>
<screen>&prompt.root;salt-run network.ping
Succeeded: 8 addresses from 7 minions average rtt 0.15 ms
</screen>
    <para>
      Or, for IPv6:
    </para>
<screen>&prompt.root;salt-run network.ping6
Succeeded: 8 addresses from 7 minions average rtt 0.15 ms
</screen>
    <para>
      Try validating all interfaces with JumboFrame enabled:
    </para>
<screen>&prompt.root;salt-run network.jumbo_ping
Succeeded: 8 addresses from 7 minions average rtt 0.26 ms
</screen>
  </sect2>
  <sect2 xml:id="network-test-throughput-benchmark">
    <title>Throughput Benchmark</title>
    <para>
      Try the runner <literal>network.iperf</literal> to test network bandwidth
      between each pair of interfaces. On a given cluster node, a number of
      <literal>iperf</literal> processes (according to the number of CPU cores) are started as
      servers. The remaining cluster nodes will be used as clients to generate
      network traffic. The accumulated bandwidth of all per-node <literal>iperf</literal> processes
      is reported. This should reflect the maximum achievable network throughput
      on all cluster nodes. For example:
    </para>
<screen>&prompt.root;salt-run network.iperf
Fastest 2 hosts:
    |_
      - 192.168.31.25
      - 11443 Mbits/sec
    |_
      - 172.16.31.25
      - 10363 Mbits/sec

Slowest 2 hosts:
    |_
      - 172.16.32.14
      - 10217 Mbits/sec
    |_
      - 192.168.121.164
      - 10113 Mbits/sec
</screen>
  </sect2>
  <sect2 xml:id="network-test-useful-options">
    <title>Useful Options</title>
    <para>
      The <option>output=full</option> option will list the results of each
      interface rather than the summary of the two slowest and fastest.
    </para>
<screen>&prompt.root;salt-run network.iperf output=full
192.168.128.1:
    8644.0 Mbits/sec
192.168.128.2:
    10360.0 Mbits/sec
192.168.128.3:
    9336.0 Mbits/sec
192.168.128.4:
    9588.56 Mbits/sec
192.168.128.5:
    10187.0 Mbits/sec
192.168.128.6:
    10465.0 Mbits/sec
</screen>
    <para>
      The <option>remove=network</option> where network is a comma delimited
      list of subnets that should not be included.
    </para>
<screen>&prompt.root;salt-run network.ping remove="192.168.121.0/24,192.168.1.0/24"
Succeeded: 20 addresses from 10 minions average rtt 14.16 ms
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="bp-flash-led-lights">
  <title>How to Locate Physical Disks Using LED Lights</title>

  <para>
    &ceph; tracks which daemons manage which hardware storage devices (HDDs, SSDs), and collects health metrics about those devices in
    order to provide tools to predict and automatically respond to hardware failure.
  </para>
  <para>
    You can blink the drive LEDs on hardware enclosures to make the replacement
    of failed disks easy and less error-prone. Use the following command:
  </para>
<screen>&prompt.cephuser;ceph device light --enable=on --devid=string --light_type=ident --force</screen>
  <para>
    The <option>DEVID</option> parameter is the device identification. You
    can obtain it by running the following command:
  </para>
<screen>&prompt.cephuser;ceph device ls</screen>

 </sect1>

 <sect1 xml:id="storage-bp-cluster-mntc-rados-striping">
  <title>Sending Large Objects with <command>rados</command> Fails with Full OSD</title>

  <para>
   <command>rados</command> is a command line utility to manage RADOS object
   storage. For more information, see <command>man 8 rados</command>.
  </para>

  <para>
   If you send a large object to a &ceph; cluster with the
   <command>rados</command> utility, such as
  </para>

<screen>&prompt.cephuser;rados -p mypool put myobject /file/to/send</screen>

  <para>
   it can fill up all the related OSD space and cause serious trouble to the
   cluster performance.
  </para>
 </sect1>

 <sect1 xml:id="storage-bp-recover-toomanypgs">
  <title>'Too Many PGs per OSD' Status Message</title>
  <para>
   If you receive a <literal>Too Many PGs per OSD</literal> message after
   running <command>ceph status</command>, it means that the
   <option>mon_pg_warn_max_per_osd</option> value (300 by default) was
   exceeded. This value is compared to the number of PGs per OSD ratio. This
   means that the cluster setup is not optimal.
  </para>

  <para>
   The number of PGs cannot be reduced after the pool is created. Pools that do
   not yet contain any data can safely be deleted and then re-created with a
   lower number of PGs. Where pools already contain data, the only solution is
   to add OSDs to the cluster so that the ratio of PGs per OSD becomes lower.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-stuckinactive">
  <title>'<emphasis>nn</emphasis> pg stuck inactive' Status Message</title>

  <para>
   If you receive a <literal>stuck inactive</literal> status message after
   running <command>ceph status</command>, it means that &ceph; does not know
   where to replicate the stored data to fulfill the replication rules. It can
   happen shortly after the initial &ceph; setup and fix itself automatically.
   In other cases, this may require a manual interaction, such as bringing up a
   broken OSD, or adding a new OSD to the cluster. In very rare cases, reducing
   the replication level may help.
  </para>

  <para>
   If the placement groups are stuck perpetually, you need to check the output
   of <command>ceph osd tree</command>. The output should look tree-structured,
   similar to the example in <xref linkend="storage-bp-recover-osddown"/>.
  </para>

  <para>
   If the output of <command>ceph osd tree</command> is rather flat as in the
   following example
  </para>

<screen>&prompt.cephuser;ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</screen>

  <para>
   You should check that the related CRUSH map has a tree structure. If it is
   also flat, or with no hosts as in the above example, it may mean that host
   name resolution is not working correctly across the cluster.
  </para>

  <para>
   If the hierarchy is incorrect&mdash;for example the root contains hosts, but
   the OSDs are at the top level and are not themselves assigned to
   hosts&mdash;you will need to move the OSDs to the correct place in the
   hierarchy. This can be done using the <command>ceph osd crush move</command>
   and/or <command>ceph osd crush set</command> commands. For further details
   see <xref linkend="op-crush"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-bp-recover-clockskew">
  <title>Fixing Clock Skew Warnings</title>
  <para>
   As a general rule, time synchronization must be configured and running on
   all nodes. Once time synchronization is set up, the clocks should not get
   skewed. However, if a clock skew is to occur, this is likely due to the
   <literal>chronyd.service</literal> not running on one or more hosts.
  </para>
  <note>
    <para>
      It is also possible that the battery on the motherboard has died, and
      the clock skew will be more pronounced. If this is the case, be aware
      that it will take quite some time for chronyd to re-sync the clocks.
    </para>
  </note>
  <para>
    If you receive a clock skew warning, confirm that the <literal>chronyd.service</literal>
    is running on all cluster nodes. If not, restart the service and wait for
    chronyd to re-sync the clock.
  </para>
  <para>
    To do so, review the <command>ceph-salt config ls</command> output and
    verify that <option>/time_server</option> is not set to <literal>disable</literal>.
  </para>
  <para>
    If in the case it is, run <command>ceph-salt apply</command> to completion
    to reconfigure the <literal>chronyd.service</literal>.
  </para>

 </sect1>
 <sect1 xml:id="storage-bp-performance-net-issues">
  <title>Poor Cluster Performance Caused by Network Problems</title>

  <para>
   There are more reasons why the cluster performance may become weak. One of
   them can be network problems. In such case, you may notice the cluster
   reaching quorum, OSD and monitor nodes going offline, data transfers taking
   a long time, or a lot of reconnect attempts.
  </para>

  <para>
   To check whether cluster performance is degraded by network problems,
   inspect the &ceph; log files under the <filename>/var/log/ceph</filename>
   directory.
  </para>

  <para>
   To fix network issues on the cluster, focus on the following points:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Basic network diagnostics. Try diagnostics tools runner <literal>net.ping</literal>
     to ping between cluster nodes to see if individual interface can reach to
     specific interface and the average response time. Any specific response
     time much slower then average will also be reported. See <xref linkend="network-test-basic-diagnostics"/>
     for more information.
   </para>
   </listitem>
   <listitem>
    <para>
     Check firewall settings on cluster nodes. Make sure they do not block
     ports/protocols required by &ceph; operation. See
     <xref linkend="storage-bp-net-firewall"/> for more information on firewall
     settings.
    </para>
   </listitem>
   <listitem>
    <para>
     Check the networking hardware, such as network cards, cables, or switches,
     for proper operation.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Separate Network</title>
   <para>
    To ensure fast and safe network communication between cluster nodes, set up
    a separate network used exclusively by the cluster OSD and monitor nodes.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="trouble-jobcache">
  <title><filename>/var</filename> Running Out of Space</title>

  <para>
   By default, the &smaster; saves every minion's return for every job in its
   <emphasis>job cache</emphasis>. The cache can then be used later to look up
   results from previous jobs. The cache directory defaults to
   <filename>/var/cache/salt/master/jobs/</filename>.
  </para>

  <para>
   Each job return from every minion is saved in a single file. Over time this
   directory can grow very large, depending on the number of published jobs and
   the value of the <option>keep_jobs</option> option in the
   <filename>/etc/salt/master</filename> file. <option>keep_jobs</option> sets
   the number of hours (24 by default) to keep information about past minion
   jobs.
  </para>

<screen>keep_jobs: 24</screen>

  <important>
   <title>Do Not Set <option>keep_jobs: 0</option></title>
   <para>
    Setting <option>keep_jobs</option> to '0' will cause the job cache cleaner
    to <emphasis>never</emphasis> run, possibly resulting in a full partition.
   </para>
  </important>

  <para>
   If you want to disable the job cache, set <option>job_cache</option> to
   'False':
  </para>

<screen>job_cache: False</screen>

  <tip>
   <title>Restoring Partition Full because of Job Cache</title>
   <para>
    When the partition with job cache files gets full because of wrong
    <option>keep_jobs</option> setting, follow these steps to free disk space
    and improve the job cache settings:
   </para>
   <procedure>
    <step>
     <para>
      Stop the &smaster; service:
     </para>
<screen>&prompt.smaster;systemctl stop salt-master</screen>
    </step>
    <step>
     <para>
      Change the &smaster; configuration related to job cache by editing
      <filename>/etc/salt/master</filename>:
     </para>
<screen>job_cache: False
keep_jobs: 1</screen>
    </step>
    <step>
     <para>
      Clear the &smaster; job cache:
     </para>
<screen>&prompt.root;rm -rfv /var/cache/salt/master/jobs/*</screen>
    </step>
    <step>
     <para>
      Start the &smaster; service:
     </para>
<screen>&prompt.smaster;systemctl start salt-master</screen>
    </step>
   </procedure>
  </tip>
 </sect1>
</chapter>
