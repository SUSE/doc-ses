<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Clustered File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  This chapter describes administration tasks that are normally performed after
  the cluster is set up and &cephfs; exported. If you need more information on
  setting up &cephfs;, refer to <xref linkend="cha-ceph-as-cephfs"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Mounting &cephfs;</title>

  <para>
   When the file system is created and the MDS is active, you are ready to
   mount the file system from a client host.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Client Preparation</title>
   <para>
    If the client host is running &sle; 12 SP2 or SP3, you can skip this
    section as the system is ready to mount &cephfs; 'out of the box'.
   </para>
   <para>
    If the client host is running &sle; 12 SP1, you need to apply all the
    latest patches before mounting &cephfs;.
   </para>
   <para>
    In any case, everything needed to mount &cephfs; is included in &sle;. The
    &productname; &productnumber; product is not needed.
   </para>
   <para>
    To support the full <command>mount</command> syntax, the
    <package>ceph-common</package> package (which is shipped with &sle;) should
    be installed before trying to mount &cephfs;.
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Create a Secret File</title>
   <para>
    The &ceph; cluster runs with authentication turned on by default. You
    should create a file that stores your secret key (not the keyring itself).
    To obtain the secret key for a particular user and then create the file, do
    the following:
   </para>
   <procedure>
    <title>Creating a Secret Key</title>
    <step>
     <para>
      View the key for the particular user in a keyring file:
     </para>
<screen>&prompt.cephuser;cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copy the key of the user who will be using the mounted Ceph FS file
      system. Usually, the key looks similar to the following:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Create a file with the user name as a file name part, for example
      <filename>/etc/ceph/admin.secret</filename> for the user
      <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Paste the key value to the file created in the previous step.
     </para>
    </step>
    <step>
     <para>
      Set proper access rights to the file. The user should be the only one who
      can read the file&mdash;others may not have any access rights.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Mount &cephfs;</title>
   <para>
    You can mount &cephfs; with the <command>mount</command> command. You need
    to specify the monitor host name or IP address. Because the
    <systemitem>cephx</systemitem> authentication is enabled by default in
    &productname;, you need to specify a user name and their related secret as
    well:
   </para>
<screen>&prompt.root;mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    As the previous command remains in the shell history, a more secure
    approach is to read the secret from a file:
   </para>
<screen>&prompt.root;mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Note that the secret file should only contain the actual keyring secret. In
    our example, the file will then contain only the following line:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>Specify Multiple Monitors</title>
    <para>
     It is a good idea to specify multiple monitors separated by commas on the
     <command>mount</command> command line in case one monitor happens to be
     down at the time of mount. Each monitor address takes the form
     <literal>host[:port]</literal>. If the port is not specified, it defaults
     to 6789.
    </para>
   </tip>
   <para>
    Create the mount point on the local host:
   </para>
<screen>&prompt.root;mkdir /mnt/cephfs</screen>
   <para>
    Mount the &cephfs;:
   </para>
<screen>&prompt.root;mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    A subdirectory <filename>subdir</filename> may be specified if a subset of
    the file system is to be mounted:
   </para>
<screen>&prompt.root;mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    You can specify more than one monitor host in the <command>mount</command>
    command:
   </para>
<screen>&prompt.root;mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>Read Access to the Root Directory</title>
    <para>
     If clients with path restriction are used, the MDS capabilities need to
     include read access to the root directory. For example, a keyring may look
     as follows:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     The <literal>allow r path=/</literal> part means that path-restricted
     clients are able to see the root volume, but cannot write to it. This may
     be an issue for use cases where complete isolation is a requirement.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Unmounting &cephfs;</title>

  <para>
   To unmount the &cephfs;, use the <command>umount</command> command:
  </para>

<screen>&prompt.root;umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>&cephfs; in <filename>/etc/fstab</filename></title>

  <para>
   To mount &cephfs; automatically upon client start-up, insert the
   corresponding line in its file systems table
   <filename>/etc/fstab</filename>:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Multiple Active MDS Daemons (Active-Active MDS)</title>

  <para>
   &cephfs; is configured for a single active MDS daemon by default. To scale
   metadata performance for large-scale systems, you can enable multiple active
   MDS daemons, which will share the metadata workload with one another.
  </para>

  <sect2>
   <title>When to Use Active-Active MDS</title>
   <para>
    Consider using multiple active MDS daemons when your metadata performance
    is bottlenecked on the default single MDS.
   </para>
   <para>
    Adding more daemons does not increase performance on all workload types.
    For example, a single application running on a single client will not
    benefit from an increased number of MDS daemons unless the application is
    doing a lot of metadata operations in parallel.
   </para>
   <para>
    Workloads that typically benefit from a larger number of active MDS daemons
    are those with many clients, perhaps working on many separate directories.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Increasing the MDS Active Cluster Size</title>
   <para>
    Each &cephfs; file system has a <option>max_mds</option> setting, which
    controls how many ranks will be created. The actual number of ranks in the
    file system will only be increased if a spare daemon is available to take
    on the new rank. For example, if there is only one MDS daemon running and
    <option>max_mds</option> is set to two, no second rank will be created.
   </para>
   <para>
    In the following example, we set the <option>max_mds</option> option to 2
    to create a new rank apart from the default one. To see the changes, run
    <command>ceph status</command> before and after you set
    <option>max_mds</option>, and watch the line containing
    <literal>fsmap</literal>:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
&prompt.cephuser;<command>ceph</command> fs set cephfs max_mds 2
&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    The newly created rank (1) passes through the 'creating' state and then
    enter its 'active' state.
   </para>
   <important>
    <title>Standby Daemons</title>
    <para>
     Even with multiple active MDS daemons, a highly available system still
     requires standby daemons to take over if any of the servers running an
     active daemon fail.
    </para>
    <para>
     Consequently, the practical maximum of <option>max_mds</option> for highly
     available systems is one less than the total number of MDS servers in your
     system. To remain available in the event of multiple server failures,
     increase the number of standby daemons in the system to match the number
     of server failures you need to survive.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Decreasing the Number of Ranks</title>
   <para>
    All ranks&mdash;including the ranks to be removed&mdash;must first be
    active. This means that you need to have at least <option>max_mds</option>
    MDS daemons available.
   </para>
   <para>
    First, set <option>max_mds</option> to a lower number. For example, go back
    to having a single active MDS:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
&prompt.cephuser;<command>ceph</command> fs set cephfs max_mds 1
&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    Note that we still have two active MDSs. The ranks still exist even though
    we have decreased <option>max_mds</option>, because
    <option>max_mds</option> only restricts the creation of new ranks.
   </para>
   <para>
    Next, use the <command>ceph mds deactivate
    <replaceable>rank</replaceable></command> command to remove the unneeded
    rank:
   </para>
<screen>&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
&prompt.cephuser;<command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

&prompt.cephuser;<command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    The deactivated rank will first enter the stopping state, for a period of
    time while it hands off its share of the metadata to the remaining active
    daemons. This phase can take from seconds to minutes. If the MDS appears to
    be stuck in the stopping state then that should be investigated as a
    possible bug.
   </para>
   <para>
    If an MDS daemon crashes or is terminated while in the 'stopping' state, a
    standby will take over and the rank will go back to 'active'. You can try
    to deactivate it again when it has come back up.
   </para>
   <para>
    When a daemon finishes stopping, it will start again and go back to being a
    standby.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Manually Pinning Directory Trees to a Rank</title>
   <para>
    In multiple active metadata server configurations, a balancer runs, which
    works to spread metadata load evenly across the cluster. This usually works
    well enough for most users, but sometimes it is desirable to override the
    dynamic balancer with explicit mappings of metadata to particular ranks.
    This can allow the administrator or users to evenly spread application load
    or limit impact of users' metadata requests on the entire cluster.
   </para>
   <para>
    The mechanism provided for this purpose is called an 'export pin'. It is an
    extended attribute of directories. The name of this extended attribute is
    <literal>ceph.dir.pin</literal>. Users can set this attribute using
    standard commands:
   </para>
<screen>&prompt.root;setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    The value (<option>-v</option>) of the extended attribute is the rank to
    assign the directory sub-tree to. A default value of -1 indicates that the
    directory is not pinned.
   </para>
   <para>
    A directory export pin is inherited from its closest parent with a set
    export pin. Therefore, setting the export pin on a directory affects all of
    its children. However, the parent's pin can be overridden by setting the
    child directory export pin. For example:
   </para>
<screen>&prompt.root;mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Managing Failover</title>

  <para>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <option>mds_beacon_grace</option> seconds (default 15 seconds) before
   marking the daemon as <emphasis>laggy</emphasis>. You can configure one or
   more 'standby' daemons that will take over during the MDS daemon failover.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configuring Standby Daemons</title>
   <para>
    There are several configuration settings that control how a daemon will
    behave while in standby. You can specify them in the
    <filename>ceph.conf</filename> on the host where the MDS daemon runs. The
    daemon loads these settings when it starts, and sends them to the monitor.
   </para>
   <para>
    By default, if none of these settings are used, all MDS daemons which do
    not hold a rank will be used as 'standbys' for any rank.
   </para>
   <para>
    The settings which associate a standby daemon with a particular name or
    rank do not guarantee that the daemon will only be used for that rank. They
    mean that when several standbys are available, the associated standby
    daemon will be used. If a rank is failed, and a standby is available, it
    will be used even if it is associated with a different rank or named
    daemon.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       If set to true, then the standby daemon will continuously read the
       metadata journal of an up rank. This will give it a warm metadata cache,
       and speed up the process of failing over if the daemon serving the rank
       fails.
      </para>
      <para>
       An up rank may only have one standby replay, daemon assigned to it. If
       two daemons are both set to be standby replay then one of them will
       arbitrarily win, and the other will become a normal non-replay standby.
      </para>
      <para>
       When a daemon has entered the standby replay state, it will only be used
       as a standby for the rank that it is following. If another rank fails,
       this standby replay daemon will not be used as a replacement, even if no
       other standbys are available.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over a failed rank if the
       last daemon to hold it matches this name.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over the specified rank.
       If another rank fails, this daemon will not be used to replace it.
      </para>
      <para>
       Use in conjunction with<option>mds_standby_for_fscid</option> to be
       specific about which file system's rank you are targeting in case of
       multiple file systems.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       If <option>mds_standby_for_rank</option> is set, this is simply a
       qualifier to say which file system's rank is being referred to.
      </para>
      <para>
       If <option>mds_standby_for_rank</option> is not set, then setting FSCID
       will cause this daemon to target any rank in the specified FSCID. Use
       this if you have a daemon that you want to use for any rank, but only
       within a particular file system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       This setting is used on monitor hosts. It defaults to true.
      </para>
      <para>
       If it is false, then daemons configured with
       <option>standby_replay=true</option> will only become active if the
       rank/name that they have been configured to follow fails. On the other
       hand, if this setting is true, then a daemon configured with
       <option>standby_replay=true</option> may be assigned some other rank.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>Examples</title>
   <para>
    Several example <filename>ceph.conf</filename> configurations follow. You
    can either copy a <filename>ceph.conf</filename> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains that server's daemon configuration.
   </para>
   <sect3>
    <title>Simple Pair</title>
    <para>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>
<!-- 2016-11-28 tbazant: commenting for now...
   <sect3>
    <title>Floating Standby</title>
    <para>
     Three MDS daemons 'a', 'b' and 'c', in a file system that has
     <option>max_mds</option> set to 2.
    </para>
    <para>
     For this setup, no explicit configuration is required. Whichever daemon is
     not assigned a rank will go into 'standby' and take over for whichever
     other daemon fails.
    </para>
   </sect3>
   <sect3>
    <title>Two MDS Clusters</title>
    <para>
     To two file systems, there are four MDS daemons. Two act as a pair for one
     file system while the other two act as a pair for the other file system.
    </para>
<screen>[mds.a]
mds standby for fscid = 1

[mds.b]
mds standby for fscid = 1

[mds.c]
mds standby for fscid = 2

[mds.d]
mds standby for fscid = 2</screen>
   </sect3>
   -->
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Setting &cephfs; Quotas</title>

  <para>
   You can set quotas on any subdirectory of the &ceph; file system. The quota
   restricts either the number of <emphasis role="bold">bytes</emphasis> or
   <emphasis role="bold">files</emphasis> stored beneath the specified point in
   the directory hierarchy.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limitations</title>
   <para>
    Using quotas with &cephfs; has the following limitations:
   </para>
   <variablelist>
    <varlistentry>
     <term>Quotas are cooperative and non-competing.</term>
     <listitem>
      <para>
       &ceph; quotas rely on the client that is mounting the file system to
       stop writing to it when a limit is reached. The server part cannot
       prevent a malicious client from writing as much data as it needs. Do not
       use quotas to prevent filling the file system in environments where the
       clients are fully untrusted.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Quotas are imprecise.</term>
     <listitem>
      <para>
       Processes that are writing to the file system will be stopped shortly
       after the quota limit is reached. They will inevitably be allowed to
       write some amount of data over the configured limit. Client writers will
       be stopped within tenths of seconds after crossing the configured limit.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Quotas are implemented in the kernel client from version 4.17.</term>
     <listitem>
      <para>
       Quotas are supported by the user space client (libcephfs, ceph-fuse).
       Linux kernel clients 4.17 and higher support &cephfs; quotas on
       &productname; &productnumber; clusters. Kernel clients (even recent
       versions) will fail to handle quotas on older clusters, even if they are
       able to set the quotas extended attributes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configure quotas carefully when used with path-based mount restrictions.</term>
     <listitem>
      <para>
       The client needs to have access to the directory inode on which quotas
       are configured in order to enforce them. If the client has restricted
       access to a specific path (for example <filename>/home/user</filename>)
       based on the MDS capability, and a quota is configured on an ancestor
       directory they do not have access to (<filename>/home</filename>), the
       client will not enforce it. When using path-based access restrictions,
       be sure to configure the quota on the directory where the client is
       restricted too (<filename>/home/user</filename>) or something nested
       beneath it.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configuration</title>
   <para>
    You can configure &cephfs; quotas by using virtual extended attributes:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configures a <emphasis>file</emphasis> limit.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configures a <emphasis>byte</emphasis> limit.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    If the attributes appear on a directory inode, a quota is configured there.
    If they are not present then no quota is set on that directory (although
    one may still be configured on a parent directory).
   </para>
   <para>
    To set a 100 MB quota, run:
   </para>
<screen>
&prompt.cephuser.mds;setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    To set a 10,000 files quota, run:
   </para>
<screen>
&prompt.cephuser.mds;setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    To view quota setting, run:
   </para>
<screen>
&prompt.cephuser.mds;getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
&prompt.cephuser.mds;getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>Quota Not Set</title>
    <para>
     If the value of the extended attribute is '0', the quota is not set.
    </para>
   </note>
   <para>
    To remove a quota, run:
   </para>
<screen>
&prompt.cephuser.mds;setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
&prompt.cephuser.mds;setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Managing &cephfs; Snapshots</title>

  <para>
   &cephfs; snapshots create a read-only view of the file system at the point
   in time they are taken. You can create a snapshot in any directory. The
   snapshot will cover all data in the file system under the specified
   directory. After creating a snapshot, the buffered data is flushed out
   asynchronously from various clients. As a result, creating a snapshot is
   very fast.
  </para>

  <important>
   <title>Multiple File Systems</title>
   <para>
    If you have multiple &cephfs; file systems sharing a single pool (via name
    spaces), their snapshots will collide, and deleting one snapshot will
    result in missing file data for other snapshots sharing the same pool.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Creating Snapshots</title>
   <para>
    The &cephfs; snapshot feature is enabled by default on new file systems. To
    enable it on existing file systems, run:
   </para>
<screen>
&prompt.cephuser;ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    After you enable snapshots, all directories in the &cephfs; will have a
    special <filename>.snap</filename> subdirectory.
   </para>
   <para>
    &cephfs; kernel clients have a limitation: they cannot handle more than 400
    snapshots in a directory tree. The number of snapshots should always be
    kept below this limit, regardless of which client you are using. If using
    older &cephfs; clients, such as SLE12-SP3, keep in mind that going above
    400 snapshots is harmful to operations as the client will crash.
   </para>
   <tip>
    <title>Custom Snapshot Subdirectory Name</title>
    <para>
     You may configure a different name for the snapshots subdirectory by
     setting the <option>client snapdir</option> setting.
    </para>
   </tip>
   <para>
    To create a snapshot, create a subdirectory under the
    <filename>.snap</filename> directory with a custom name. For example, to
    create a snapshot of the directory
    <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, run:
   </para>
<screen>
&prompt.user;mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Deleting Snapshots</title>
   <para>
    To delete a snapshot, remove its subdirectory inside the
    <filename>.snap</filename> directory:
   </para>
<screen>
&prompt.user;rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
