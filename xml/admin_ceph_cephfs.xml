<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.cephfs">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>Clustered File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES4</dm:release>
  </dm:docmanager>
 </info>
 <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that uses
  a &ceph; storage cluster to store its data. &cephfs; uses the same cluster
  system as &ceph; block devices, &ceph; object storage with its S3 and Swift
  APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <warning>
  <para>
   &cephfs; file layout changes can be performed as documented in
   <link
    xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>.
   However, a data pool must not be added to an existing &cephfs; file system
   (via <command>ceph mds add_data_pool</command>) while the file system is
   mounted by any clients.
  </para>
 </warning>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>&ceph; Metadata Server</title>

  <para>
   &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph; block
   devices and &ceph; object storage <emphasis>do not</emphasis> use MDS. MDSs
   make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adding a Metadata Server</title>
   <para>
    After you deploy OSDs and monitors, you can deploy metadata servers.
    Although MDS service can share a node with an OSD and/or monitor service,
    you are encouraged to deploy it on a separate cluster node for performance
    reasons.
   </para>
<screen>&prompt.cephuser;ceph-deploy install <replaceable>mds-host-name</replaceable>
&prompt.cephuser;ceph-deploy mds create <replaceable>host-name</replaceable>:<replaceable>daemon-name</replaceable></screen>
   <para>
    You may optionally specify a daemon instance name if you need to run
    multiple daemons on a single server.
   </para>
   <para>
    After you deploy your MDS, allow the <literal>Ceph OSD/MDS</literal>
    service in the firewall setting of the server where MDS is deployed. Start
    <literal>yast</literal>, navigate to <menuchoice> <guimenu>Security and
    Users</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed
    Services</guimenu> </menuchoice> and in the <guimenu>Service to
    Allow</guimenu> drop&ndash;down menu select <guimenu>Ceph
    OSD/MDS</guimenu>. If the Ceph MDS node is not allowed full traffic,
    mounting of a file system fails, even though other operations may work
    properly.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mdf.config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine-tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file. For detailed list of MDS
    related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
<!-- http://docs.ceph.com/docs/master/cephfs/createfs/ -->

  <title>&cephfs;</title>

  <para>
   When you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you may create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>. When
    configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data loss
      in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as this
      will improve the observed latency of file system operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information on managing pools, see <xref linkend="ceph.pools"/>.
   </para>
   <para>
    To create the two required pools&mdash;for example 'cephfs_data' and
    'cephfs_metadata'&mdash;with default settings for use with &cephfs;, run
    the following commands:
   </para>
<screen>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    When the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example:
   </para>
<screen>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    &cephfs;'s:
   </para>
<screen>ceph fs ls
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    When the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>ceph mds stat
 e5: 1/1/1 up</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.mount">
   <title>Mounting &cephfs;</title>
   <para>
    Once the file system is created and the MDS is active, you are ready to
    mount the file system from a client host.
    </para>
    <sect3 xml:id="cephfs.client_preparation">
    <title>Client Preparation</title>
    <para>
     If the client host is running &sle; 12 SP2, you can skip this section as
     the system is ready to mount &cephfs; 'out-of-the-box'.
    </para>
    <para>
     If the client host is running &sle; 12 SP1, you need to apply all the
     latest patches before mounting &cephfs;.
    </para>
    <para>
     In either case (the client is &sle; 12 SP2 or &sle; 12 SP1), neither the
     &storage; product itself nor the <systemitem>ceph-common</systemitem>
     package need be installed on the client host, and it is always
     recommended to run fully updated systems.
    </para>
   </sect3>
   <sect3 xml:id="Creating_Secret_File">
    <title>Create a Secret File</title>
    <para>
     The &ceph; cluster runs with authentication turned on by default. You
     should create a file that stores your secret key (not the keyring itself).
     To obtain the secret key for a particular user and then create the file,
     do the following:
    </para>
    <procedure>
     <title>Creating a Secret Key</title>
     <step>
      <para>
       View the key for the particular user in a keyring file:
      </para>
<screen>cat /etc/ceph/ceph.client.admin.keyring</screen>
     </step>
     <step>
      <para>
       Copy the key of the user who will be using the mounted Ceph FS
       file system. Usually the key looks similar like the following:
      </para>
<screen>[client.admin]
   key = AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
     </step>
     <step>
      <para>
       Create a file with the user name as a filename part, e.g.
       <filename>/etc/ceph/admin.secret</filename> for the user
       <emphasis>admin</emphasis>
      </para>
     </step>
     <step>
      <para>
       Paste the key value to the file created in the previous step.
      </para>
     </step>
     <step>
      <para>
       Set proper access rights to the file. The user should be the only one
       who can read the file, others may not have any access rights.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="ceph.cephfs.krnldrv">
    <title>Mount &cephfs; with the Kernel Driver</title>
    <para>
     You can mount &cephfs;, normally with the <command>mount</command>
     command. You need to specify the monitor host name or IP address.
    </para>
    <tip>
     <title>Specify Multiple Monitors</title>
     <para>
      It is a good idea to specify multiple monitors separated by commas on the
      <command>mount</command> command line in case one monitor happens to be
      down at the time of mount. Each monitor address takes the form<literal>
      host[:port]</literal>. If the port is not specified, it defaults to 6789.
     </para>
    </tip>
    <para>
     Create the mount point on the local host:
    </para>
<screen>sudo mkdir /mnt/cephfs</screen>
    <para>
     Mount the &cephfs;:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs</screen>
    <para>
     A subdirectory <filename>subdir</filename> may be specified if a subset of
     the file system is to be mounted:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs</screen>
    <para>
     You can specify more than one monitor host in the <command>mount</command>
     command:
    </para>
<screen>sudo mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs</screen>
    <tip>
     <title>&cephfs; and <systemitem>cephx</systemitem> Authentication</title>
     <para>
      To mount &cephfs; with <systemitem>cephx</systemitem> authentication
      enabled, you need to specify a user name and a secret:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
     <para>
      As the previous command remains in the shell history, a more secure
      approach is to read the secret from a file:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
    </tip>
    <important>
     <title>Read Access to the Root Directory</title>
     <para>
      If clients with path restriction are used, the mds capabilities need to
      include read access to the root directory. For example a keyring may
      look as follows:
     </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
     <para>
      The <literal>allow r path=/</literal> part means that path-restricted clients
      are able to see the root volume, but cannot write to it. This may be an issue for
      use cases where complete isolation is a requirement.
     </para>
    </important>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.unmount">
   <title>Unmounting &cephfs;</title>
   <para>
    To unmount the &cephfs;, use the <command>umount</command> command:
   </para>
<screen>sudo umount /mnt/cephfs</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.fstab">
   <title>&cephfs; in <filename>/etc/fstab</filename></title>
   <para>
    To mount &cephfs; automatically on the client start-up, insert the
    corresponding line in its file systems table
    <filename>/etc/fstab</filename>:
   </para>
<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.failover">
  <title>Managing Failover</title>

  <para>
   If an MDS daemon stops communicating with the monitor, the monitor will wait
   <option>mds_beacon_grace</option> seconds (default 15 seconds) before
   marking the daemon as <emphasis>laggy</emphasis>. You can configure one or
   more 'standby' daemons that can will take over during the MDS daemon
   failover.
  </para>

  <sect2 xml:id="ceph.cephfs.failover.standby">
   <title>Configuring Standby Daemons</title>
   <para>
    There are several configuration settings that control how a daemon will
    behave while in standby. You can specify them in the
    <filename>ceph.conf</filename> on the host where the MDS daemon runs. The
    daemon loads these settings when it starts, and sends them to the monitor.
   </para>
   <para>
    By default, if none of these settings are used, all MDS daemons which do
    not hold a rank will be used as 'standbys' for any rank.
   </para>
   <para>
    The settings which associate a standby daemon with a particular name or
    rank do not guarantee that the daemon will only be used for that rank. They
    mean that when several standbys are available, the associated standby
    daemon will be used. If a rank is failed, and a standby is available, it
    will be used even if it is associated with a different rank or named
    daemon.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       If set to true, then the standby daemon will continuously read the
       metadata journal an up rank. This will give it a warm metadata cache,
       and speed up the process of failing over if the daemon serving the rank
       fails.
      </para>
      <para>
       An up rank may only have one standby replay daemon assigned to it. If
       two daemons are both set to be standby replay then one of them will
       arbitrarily win, and the other will become a normal non-replay standby.
      </para>
      <para>
       Once a daemon has entered the standby replay state, it will only be used
       as a standby for the rank that it is following. If another rank fails,
       this standby replay daemon will not be used as a replacement, even if no
       other standbys are available.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over a failed rank if the
       last daemon to hold it matches this name.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Set this to make the standby daemon only take over the specified rank.
       If another rank fails, this daemon will not be used to replace it.
      </para>
      <para>
       Use in conjunction with<option>mds_standby_for_fscid</option> to be
       specific about which file system's rank you are targeting in case of
       multiple file systems.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       If <option>mds_standby_for_rank</option> is set, this is simply a
       qualifier to say which file system's rank is referred to.
      </para>
      <para>
       If <option>mds_standby_for_rank</option> is not set, then setting FSCID
       will cause this daemon to target any rank in the specified FSCID. Use
       this if you have a daemon that you want to use for any rank, but only
       within a particular file system.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       This setting is used on monitor hosts. It defaults to true.
      </para>
      <para>
       If it is false, then daemons configured with
       <option>standby_replay=true</option> will only become active if the
       rank/name that they have been configured to follow fails. On the other
       hand, if this setting is true, then a daemon configured with
       <option>standby_replay=true</option> may be assigned some other rank.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.cephfs.failover.examples">
   <title>Examples</title>
   <para>
    Several example <filename>ceph.conf</filename> configurations follow. You
    can either copy a <filename>ceph.conf</filename> with the configuration of
    all daemons to all your servers, or you can have a different file on each
    server that contains just that server's daemons configuration.
   </para>
   <sect3>
    <title>Simple Pair</title>
    <para>
     Two MDS daemons 'a' and 'b' acting as a pair. Whichever one is not
     currently assigned a rank will be the standby replay follower of the
     other.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>
   <!-- 2016-11-28 tbazant: commenting for now...
   <sect3>
    <title>Floating Standby</title>
    <para>
     Three MDS daemons 'a', 'b' and 'c', in a file system that has
     <option>max_mds</option> set to 2.
    </para>
    <para>
     For this setup, no explicit configuration is required. Whichever daemon is
     not assigned a rank will go into 'standby' and take over for whichever
     other daemon fails.
    </para>
   </sect3>
   <sect3>
    <title>Two MDS Clusters</title>
    <para>
     To two file systems, there are four MDS daemons. Two act as a pair for one
     file system while the other two act as a pair for the other file system.
    </para>
<screen>[mds.a]
mds standby for fscid = 1

[mds.b]
mds standby for fscid = 1

[mds.c]
mds standby for fscid = 2

[mds.d]
mds standby for fscid = 2</screen>
   </sect3>
   -->
  </sect2>
 </sect1>
</chapter>
