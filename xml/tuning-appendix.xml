<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE appendix
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<appendix xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="tuning-appendix">
 <title>Appendicies</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   There are a significant number of items that can be tuned to provide
   improved performance of a &ceph; cluster. While the material provided
   in previous sections attempts to showcase important items, it should
   not be considered exhaustive. It is highly recommended that use of
   the information contained herein be done so with a scientific
   approach to understanding the performance needs and outcomes of
   specific tuning in comparison with baseline performance.
 </para>
 <sect1 xml:id="tuning-appendix-a">
   <title>Appendix A: &salt; State for Kernel Tuning</title>
   <para>
     To utilize this &salt; state follow these steps:
   </para>
   <procedure>
     <step>
       <para>
         Set <option>mitigations=off</option> in the <filename>grub2.cfg</filename> file
         by adjusting <filename>/etc/default/grub</filename> and executing
         <literal>grub2-install</literal>. For more information, see
         <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-deployment/#sec-boot-parameters-advanced-cpu-mitigations"/>.
       </para>
     </step>
     <step>
       <para>
         Reboot.
       </para>
     </step>
     <step>
       <para>
         Create directory named <filename>my_kerntune</filename> in <filename>/srv/salt/</filename>.
       </para>
     </step>
     <step>
       <para>
         Create <filename>/srv/salt/my_kerntune/init.sls</filename> with the
         following contents:
       </para>
<screen>
  my_kern_tune:
    file.replace:
      - name: /etc/default/grub
      - pattern: showopts.*
      - repl: showopts intel_idle.max_cstate=0 processor.max_cstate=0 idle=poll scsi_mod.use_blk_mq=1 nospec spectre_v2=off pti=off spec_store_bypass_disable=off l1tf=off"

  grub2_mkconfig:
    cmd.run:
      - runas: root
      - name: grub2-mkconfig -o /boot/grub2/grub.cfg
      - onchanges:
        - file: my_kern_tune
</screen>
     </step>
     <step>
       <para>
         Issue the following command to set the state:
       </para>
<screen>
salt '*' state.apply my_kerntune
</screen>
     </step>
     <step>
       <para>
         Reboot the nodes.
       </para>
     </step>
   </procedure>
   <warning>
     <para>
       Verify the <command>grub</command> kernel command line and ensure the pattern match
       specified by the pattern: parameter is appropriate. As is,
       this will overwrite anything after the <option>showopts</option> kernel command
       line argument.
     </para>
   </warning>
 </sect1>
 <sect1 xml:id="tuning-appendix-b">
   <title>Appendix B: Ring Buffer Max Value Script</title>
   <para>
     This script is intended to be run on a single host at a time.
     It will set all interfaces on the host to the maximum ring
     buffer values. It may be used with some sort of orchestration,
     for example &salt;, to apply to all hosts in a cluster.
   </para>
<screen>
  for i in `ls /etc/sysconfig/network/ifcfg-*`;do
  	nicname=`echo $i|cut -f2 -d"-"`
  	echo nicname=$nicname
  	if [ `ethtool -g $nicname 2>/dev/null |wc -l ` -gt 6 ]; then
  		ethtoolcmd="-G $nicname rx `ethtool -g $nicname|head -6|grep RX:|cut -f2 -d":"|xargs` tx `ethtool -g $nicname|head -6|grep TX:|cut -f2 -d":"|xargs`"
  		echo ethtoolcmd=$ethtoolcmd
  		sed -i "s/ETHTOOL_OPTIONS=''/ETHTOOL_OPTIONS='$ethtoolcmd'/g" $i
  	fi
  	done
</screen>
 </sect1>
 <sect1 xml:id="tuning-appendix-c">
   <title>Appendix C: Network Tuning</title>
   <para>
     This script uses &salt; to apply a number of settings to a cluster.
     This was used during testing so that parameters could be rapidly
     adjusted. It is intended to be an example only and is not intended
     for use without modification.
   </para>
<screen>
  #!/bin/bash
  #These commands tune the write size of the pcie interface for the NIC cards.
  salt '*' cmd.run 'setpci -s 5b:00.0 68.w=5936'
  salt '*' cmd.run 'setpci -s 5b:00.1 68.w=5936'

  #Set Jumbo Frames
  salt '*' cmd.run 'ip link set bond0 mtu 9000'

  #Set the rx queue for rx-0 to use all CPU cores local to the device.
  salt '*' cmd.run 'for j in `cat /sys/class/net/bond0/bonding/slaves`;do LOCAL_CPUS=`cat /sys/class/net/$j/device/local_cpus`;echo $LOCAL_CPUS > /sys/class/net/$j/queues/rx-0/rps_cpus;done'

  #Set send and recieve buffers for both network interfaces involved in the bond
  salt '*' cmd.run 'ethtool -G eth4 rx 8192 tx 8192'
  salt '*' cmd.run 'ethtool -G eth5 rx 8192 tx 8192'

  #Ensure SACK is on
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_sack=1'
  #Ensure timestamps are on to prevent possible drops
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_timestamps=1'

  #Set the max conections to 2048
  salt '*' cmd.run 'sysctl -w net.core.somaxconn=2048'

  #Set TCP to low latency
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_low_latency=1'
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_fastopen=1'

  #Set min, default, and max send and receive buffers
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_rmem="10240 87380 2147483647"'
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_wmem="10240 87380 2147483647"'

  salt '*' cmd.run 'sysctl -w net.core.netdev_max_backlog=250000'
</screen>
 </sect1>
 </appendix>
