<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph.pools">
 <title>Gestión de repositorios de almacenamiento</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sí</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph almacena los datos en repositorios. Los repositorios son grupos lógicos en los que se almacenan objetos. Al distribuir un clúster por primera vez sin crear un repositorio, Ceph utiliza los repositorios por defecto para almacenar los datos. Los repositorios aportan lo siguiente:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Capacidad de recuperación:</emphasis> puede establecer cuántos OSD pueden fallar sin que se pierda ningún dato. En el caso de los repositorios replicados, es el número de copias o réplicas deseadas de un objeto. Los repositorios nuevos se crean con un número predeterminado por defecto de réplicas establecido en 3. Dado que en una configuración típica se almacenan el objeto y una copia adicional, debe definir un valor de número de réplicas igual a 2. En los repositorios codificados de borrado, se trata del número de fragmentos de codificación (es decir <emphasis>m = 2</emphasis> en el perfil de código de borrado).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Grupos de colocación:</emphasis> estructuras de datos internas para almacenar datos en un repositorio entre los OSD. La forma en que ceph almacena datos en los grupos de colocación se define en una asignación CRUSH. Puede establecer el número de grupos de colocación del repositorio. En una configuración típica, se utilizan aproximadamente 100 grupos de colocación por OSD para proporcionar el equilibrio óptimo sin necesidad de utilizar demasiados recursos informáticos. Al configurar varios repositorios, asegúrese de que ha definido un número razonable de grupos de colocación para el conjunto de repositorio y clúster.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Reglas de CRUSH:</emphasis> cuando almacena datos en un repositorio, un conjunto de reglas de CRUSH asignada al repositorio permite que CRUSH identifique una regla para la colocación del objeto y sus réplicas (o fragmentos en repositorios de repositorios codificados de borrado) en el clúster. Puede crear una regla de CRUSH personalizada para el repositorio.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Instantáneas:</emphasis> al crear instantáneas con <command>ceph osd pool mksnap</command>, se realiza una instantánea de un repositorio concreto.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Definición de la propiedad:</emphasis> puede configurar un ID de usuario como propietario de un repositorio.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Para organizar los datos en repositorios, puede enumerar, crear y eliminar repositorios. También puede ver las estadísticas de uso para cada repositorio.
 </para>
 <sect1 xml:id="ceph.pools.associate">
  <title>Asociación de repositorios a una aplicación</title>

  <para>
   Para utilizar los grupos, debe asociarlos a una aplicación. Los repositorios que se utilizarán con CephFS y los creados automáticamente por Object Gateway se asocian de forma automática. Los repositorios que se vayan a utilizar con RBD deben inicializarse mediante la herramienta <command>rbd</command> (consulte la <xref linkend="ceph.rbd.commands"/> para obtener más información).
  </para>

  <para>
   En otros casos, se puede asociar manualmente un nombre de aplicación de formato libre al repositorio:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>nombres de las aplicaciones por defecto</title>
   <para>
    CephFS utiliza el nombre de aplicación <literal>cephfs</literal>, el dispositivo de bloques RADOS utiliza <literal>rbd</literal> y Object Gateway utiliza <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   Un repositorio se puede asociar a varias aplicaciones y cada aplicación puede tener sus propios metadatos. Para mostrar los metadatos de la aplicación para un determinado repositorio, emplee el siguiente comando:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.pools.operate">
  <title>Gestión de los repositorios</title>

  <para>
   En esta sección encontrará información práctica para realizar tareas básicas relacionadas con los repositorios. Descubrirá cómo enumerar, crear y eliminar repositorios, así como visualizar las estadísticas de los repositorios o gestionar sus instantáneas.
  </para>

  <sect2>
   <title>Enumeración de repositorios</title>
   <para>
    Para enumerar los repositorios de su clúster, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Creación de repositorios</title>
   <para>
    Para crear un repositorio replicado, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Para crear un repositorio codificado de borrado, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    El comando <command>ceph osd pool create</command> puede fallar si supera el límite de grupos de colocación por OSD. El límite se establece con la opción <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       Nombre del repositorio. Debe ser único. Esta opción es obligatoria.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       El número total de grupos de colocación para el repositorio. Esta opción es obligatoria. El valor por defecto es 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       El número total de grupos de colocación para fines de colocación. Debe ser idéntico al número total de grupos de colocación, salvo en situaciones en las que se dividan los grupos de colocación. Esta opción es obligatoria. El valor por defecto es 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       El tipo de repositorio, que puede ser <emphasis>replicated</emphasis> (replicado) para recuperar los OSD perdidos manteniendo varias copias de los objetos o <emphasis>erasure</emphasis> (de borrado) para obtener un tipo de capacidad RAID5 generalizada. Los repositorios replicados requieren más espacio de almacenamiento en bruto, pero implementan todas las operaciones de Ceph. Los repositorios codificados de borrado requieren menos espacio almacenamiento en bruto, pero solo implementan un subconjunto de las operaciones disponibles. El valor por defecto es "replicated" (replicado).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       El nombre de conjunto de reglas de CRUSH para el repositorio. Si el conjunto de reglas especificado no existe, se producirá un error -ENOENT al crear el repositorio replicado. No obstante, el repositorio replicado creará un nuevo conjunto de reglas de borrado con el nombre que se indique. El valor por defecto es "erasure-code" (código de borrado) para un repositorio codificado de borrado. Recoge la variable de configuración de Ceph <option>osd_pool_default_crush_replicated_ruleset</option> para el repositorio replicado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=perfil</term>
     <listitem>
      <para>
       Solo para los repositorios codificados de borrado. Utiliza el perfil de código de borrado. Debe ser un perfil existente, según se haya definido mediante <command>osd erasure-code-profile set</command>.
      </para>
      <para>
       Al crear un repositorio, establezca un valor razonable de número de grupos de colocación (por ejemplo, 100). Tenga en cuenta también el número total de grupos de colocación por OSD. Los grupos de colocación consumen muchos recursos, por lo que el rendimiento se degradará si tiene repositorios con muchos grupos (por ejemplo, 50 repositorios con 100 grupos de colocación cada uno). El punto de rendimiento decreciente depende de la capacidad del host OSD.
      </para>
      <para>
       Consulte <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement Groups</link> (Grupos de colocación) para obtener información sobre cómo calcular el número adecuado de grupos de colocación para su repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       El número previsto de objetos para este repositorio. Si define este valor, la división de la carpeta de grupos de colocación se producirá al crear el repositorio. Esto evita el impacto de latencia cuando se divide una carpeta en tiempo de ejecución.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Definición de las cuotas de repositorio</title>
   <para>
    Puede definir cuotas de repositorio para el número máximo de bytes o el número máximo de objetos por repositorio.
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Para eliminar una cuota, establezca su valor en 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Supresión de un repositorio</title>
   <warning>
    <title>la supresión de un repositorio no se puede deshacer</title>
    <para>
     Los repositorios pueden contener datos importantes. Si suprime un repositorio, desaparecerán todos sus datos y no habrá forma de recuperarlo.
    </para>
   </warning>
   <para>
    Dado que la supresión accidental de un repositorio es un peligro real, Ceph implementa dos mecanismos que impiden suprimir los repositorios. Para suprimir un repositorio, primero es necesario inhabilitar ambos mecanismos.
   </para>
   <para>
    El primero es el indicador <literal>NODELETE</literal>. Todos los repositorios lo tienen y el valor por defecto es "false". Para averiguar el valor de este indicador en un repositorio, ejecute el siguiente comando:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Si la salida es <literal>nodelete: true</literal>, no podrá suprimir el repositorio hasta que cambie el indicador mediante el siguiente comando:
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    El segundo mecanismo es el parámetro de configuración para todo el clúster <option>mon allow pool delete</option>, que es "false" por defecto. Esto significa que, por defecto, no es posible suprimir un repositorio. El mensaje de error que se muestra es:
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Para suprimir el repositorio a pesar de la configuración de seguridad, puede definir temporalmente <option>mon allow pool delete</option> como "true", suprimir el repositorio y, a continuación, volver a definir el parámetro como "false":
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    El comando <command>injectargs</command> muestra el siguiente mensaje:
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Es una simple confirmación de que el comando se ha ejecutado correctamente. No es un error.
   </para>
   <para>
    Si ha creado sus propios conjuntos de reglas y reglas para un repositorio que ha creado, plantéese la posibilidad de eliminar estos elementos cuando ya no necesite el repositorio. Si ha creado usuarios con permisos estrictamente para un repositorio que ya no existe, plantéese también la posibilidad de suprimir dichos usuarios.
   </para>
  </sect2>

  <sect2>
   <title>Cambios en el nombre de un repositorio</title>
   <para>
    Para renombrar un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Si ha renombrado de un repositorio y existen permisos por repositorio para los usuarios autenticados, debe actualizar los permisos con el nuevo nombre de repositorio.
   </para>
  </sect2>

  <sect2>
   <title>Visualización de estadísticas del repositorio</title>
   <para>
    Para mostrar las estadísticas de uso de un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Definición de los valores de repositorio</title>
   <para>
    Para definir un valor para un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    Puede definir valores para las siguientes claves:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Define el número de réplicas para los objetos del repositorio. Consulte la <xref linkend="ceph.pools.options.num_of_replicas"/> para obtener más información. Solo para repositorios replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Define el número mínimo de réplicas obligatorias para E/S. Consulte la <xref linkend="ceph.pools.options.num_of_replicas"/> para obtener más información. Solo para repositorios replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       El número de segundos que se debe permitir a los clientes que reproduzcan peticiones reconocidas, pero no confirmadas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       El número de grupos de colocación del repositorio. Si añade OSD a su clúster, deberá aumentar el valor de los grupos de colocación. Consulte la <xref linkend="storage.bp.cluster_mntc.add_pgnum"/> para obtener más información.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       El número efectivo de grupos de colocación que se deben utilizar para calcular la colocación de los datos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       El conjunto de reglas que se debe utilizar para asignar la colocación en el clúster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Establece (1) o anula (0) el indicador HASHPSPOOL en un repositorio. Cuando se habilita este indicador, el algoritmo cambia para mejorar la distribución de los grupos de colocación a los OSD. Después de habilitar el indicador en un repositorio cuyo indicador HASHPSPOOL se ha definido como 0, el clúster inicia la reposición para que todos los grupos vuelvan a la colocación correcta. Tenga en cuenta que esto puede crear una carga considerable de E/S en un clúster, por lo que debe llevarse a cabo una buena planificación en los clústeres de producción con una carga elevada.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Impide la eliminación del repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Impide que se modifiquen los valores <option>pg_num</option> y <option>pgp_num</option> del repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Impide que se modifique el tamaño del repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Establece o anula el indicador <literal>WRITE_FADVISE_DONTNEED</literal> en un repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Se inhabilita el borrado seguro (profundo) de los datos para el repositorio específico, a fin de resolver un pico temporal de carga de E/S.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Habilita el seguimiento de conjuntos de resultados para repositorios de caché. Consulte <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">este artículo sobre los filtros de Bloom</link> para obtener más información. Esta opción puede tener los valores siguientes: <literal>bloom</literal>, <literal>explicit_hash</literal> o <literal>explicit_object</literal>. El valor por defecto es <literal>bloom</literal>, los demás solo se emplean para realizar pruebas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       El número de conjuntos de resultados que se deben almacenar en los repositorios de caché. Cuanto mayor sea el número, más RAM consumirá el daemon <systemitem>ceph-osd</systemitem>. El valor por defecto es <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       La duración en segundos de un periodo de conjunto de resultados para los repositorios de caché. Cuanto mayor sea el número, más RAM consumirá el daemon <systemitem>ceph-osd</systemitem>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       La probabilidad de falsos positivos para el tipo de conjunto de resultados del filtro de Bloom. Consulte <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">este artículo sobre los filtros de Bloom</link> para obtener más información. El intervalo válido es de 0.0 a 1.0; el valor predeterminado es <literal>0.05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Forzar a los OSD para que utilicen marcas horarias GMT (hora del meridiano de Greenwich) al crear un conjunto de resultados para los niveles de caché. Esto garantiza que los nodos de distintas zonas horarias devuelvan el mismo resultado. El valor por defecto es <literal>1</literal>. Este valor no debe cambiarse.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       El porcentaje del repositorio de caché que debe contener objetos modificados (sucios) para que el agente de niveles de caché los vacíe en el repositorio de almacenamiento. El valor por defecto es <literal>.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       El porcentaje del repositorio de caché que debe contener objetos modificados (sucios) para que el agente de niveles de caché los vacíe en el repositorio de almacenamiento a mayor velocidad. El valor por defecto es <literal>.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       El porcentaje del repositorio de caché que debe contener objetos no modificados (limpios) para que el agente de niveles de caché los expulse del repositorio de caché. El valor por defecto es <literal>.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph comenzará a expulsar o limpiar objetos cuando se active el umbral establecido en <option>max_bytes</option>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph comenzará a expulsar o limpiar objetos cuando se active el umbral establecido en <option>max_objects</option>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Velocidad de caída de temperatura entre dos <literal>hit_set</literal>s sucesivos. El valor por defecto es <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Deben contarse como máximo <literal>N</literal> apariciones en <literal>hit_set</literal>s para calcular la temperatura. El valor por defecto es <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       El tiempo (en segundos) que debe transcurrir para que el agente de niveles de caché mueva un objeto del repositorio de caché al de almacenamiento.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       El tiempo (en segundos) que debe transcurrir para que el agente de niveles de caché expulse un objeto del repositorio de caché.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Si este indicador está habilitado en los repositorios codificados de borrado, las peticiones de lectura emite sublecturas a todos los shards y espera a recibir suficientes shards que descodificar para ofrecer servicio al cliente. En el caso de los complementos de borrado <emphasis>jerasure</emphasis> e <emphasis>isa</emphasis>, cuando vuelve la respuesta <literal>K</literal>, la petición del cliente se atiende de inmediato con los datos descodificados a partir de estas respuestas. Esto permite obtener algunos recursos para mejorar el rendimiento. Actualmente, este indicador solo se admite para los repositorios codificados de borrado. El valor por defecto es <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       El intervalo mínimo en segundos para el borrado seguro de datos del repositorio cuando la carga del clúster es reducida. El valor <literal>0</literal> por defecto significa que se utiliza el valor <option>osd_scrub_min_interval</option> del archivo de configuración de Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       El intervalo máximo en segundos para el borrado seguro de datos del repositorio, independientemente de la carga del clúster. El valor <literal>0</literal> por defecto significa que se utiliza el valor <option>osd_scrub_max_interval</option> del archivo de configuración de Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       El intervalo en segundos para el borrado seguro <emphasis>profundo</emphasis> del repositorio. El valor <literal>0</literal> por defecto significa que se utiliza el valor <option>osd_deep_scrub</option> del archivo de configuración de Ceph.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Obtención de los valores del repositorio</title>
   <para>
    Para obtener un valor de un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    Puede obtener los valores de las claves indicadas en la <xref linkend="ceph.pools.values"/>, además de las siguientes:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       El número de grupos de colocación del repositorio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       El número efectivo de grupos de colocación que se deben utilizar para calcular la colocación de los datos. El intervalo válido es igual o menor que <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Definición del número de réplicas de objetos</title>
   <para>
    Para establecer el número de réplicas de objetos en un repositorio replicado, ejecute lo siguiente:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    El valor <replaceable>num-replicas</replaceable> incluye el objeto en sí. Si por ejemplo, quiere que existan el objeto y dos copias (tres instancias en total), especifique 3.
   </para>
   <para>
    Si establece un valor de 2 en <replaceable>num-replicas</replaceable>, solo habrá <emphasis>una</emphasis> copia de los datos. Si pierde una instancia de un objeto, para recuperarlo debe poder confiar en que la otra copia no ha sufrido daños, por ejemplo, desde el último <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">borrado seguro</link>.
   </para>
   <para>
    Si establece solo una réplica en el repositorio, significa que habrá exactamente <emphasis>una</emphasis> única instancia del objeto de datos. Si el OSD falla, se perderán los datos. Un posible uso de un repositorio con una réplica es el almacenamiento temporal de datos durante poco tiempo.
   </para>
   <para>
    La configuración de más de tres réplicas para un repositorio implica solo un pequeño incremento en la fiabilidad, pero puede ser adecuado en raras ocasiones. Recuerde que a mayor número de réplicas, más espacio de disco necesitará para almacenar las copias de los objetos. Si necesita un nivel de seguridad máximo para sus datos, recomendamos utilizar repositorios codificados de borrado. Para obtener más información, consulte: <xref linkend="cha.ceph.erasure"/>.
   </para>
   <warning>
    <title>se recomienda usar más de dos réplicas</title>
    <para>
     Es muy recomendable no utilizar solo 2 réplicas. En caso de que falle un OSD, es muy probable que se produzca un fallo del segundo debido a una gran carga de trabajo durante la recuperación.
    </para>
   </warning>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    Puede ejecutar este comando para cada repositorio.
   </para>
   <note>
    <para>
     Un objeto puede aceptar operaciones de E/S en modo degradado con menos réplicas de las indicadas en <literal>pool size</literal>. Para configurar un número mínimo de réplicas necesarias para las operaciones de E/S, debe utilizar el valor <literal>min_size</literal>. Por ejemplo:
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Esto garantiza que ningún objeto del repositorio de datos recibirá operaciones de E/S con menos réplicas de las indicadas en <literal>min_size</literal>.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Obtención del número de réplicas de objetos</title>
   <para>
    Para obtener el número de réplicas de objetos, ejecute lo siguiente:
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph enumerará los grupos con el atributo <literal>replicated size</literal> destacado. Ceph crea por defecto dos réplicas de cada objeto (un total de tres copias o un tamaño de 3).
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pgnum">
   <title>Aumento del número de grupos de colocación</title>
   <para>
    Al crear un repositorio nuevo, debe especificar el número de grupos de colocación del repositorio (consulte la <xref linkend="ceph.pools.operate.add_pool"/>). Después de añadir más OSD al clúster, normalmente deberá aumentar también el número de grupos de colocación por razones de rendimiento y de durabilidad de los datos. Para cada grupo de colocación, los nodos de OSD y de Monitor necesitan recursos de memoria, red y CPU en todo momento, y aún más durante la recuperación. Por lo tanto, reducir el número de grupos de colocación permite ahorrar grandes cantidades de recursos.
   </para>
   <warning>
    <title>valor demasiado elevado de <option>pg_num</option></title>
    <para>
     Al modificar el valor de <option>pg_num</option> de un repositorio, puede ocurrir que el nuevo número de grupos de colocación supere el límite permitido. Por ejemplo
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     El límite impide una división extrema de los grupos de colocación y se deriva del valor de <option>mon_osd_max_split_count</option>.
    </para>
   </warning>
   <para>
    Determinar el número adecuado de grupos de colocación para un clúster redimensionado es una tarea compleja. Un método consiste en aumentar continuamente el número de grupos de colocación hasta un estado que ofrezca un rendimiento óptimo del clúster. Para determinar el nuevo número de grupos de colocación, debe obtener el valor del parámetro <option>mon_osd_max_split_count</option> y sumarlo al número actual de grupos de colocación. Para hacerse una idea básica, eche un vistazo al siguiente guion:
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    Después de calcular el siguiente número de grupos de colocación, auméntelo con
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pool">
   <title>Adición de un repositorio</title>
   <para>
    Después de distribuir un clúster por primera vez, Ceph utiliza los repositorios por defecto para almacenar datos. Posteriormente, podrá crear un nuevo repositorio con
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    Para obtener más información sobre la creación de repositorios en el clúster, consulte la <xref linkend="ceph.pools.operate.add_pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools.migration">
  <title>Migración de repositorios</title>

  <para>
   Al crear un repositorio (consulte la <xref linkend="ceph.pools.operate.add_pool"/>) debe especificar sus parámetros iniciales, como el tipo de repositorio o el número de grupos de colocación. Si posteriormente decide cambiar alguno de estos parámetros después de añadir datos al repositorio, deberá migrar los datos del repositorio a otro cuyos parámetros se adapten a su distribución.
  </para>

  <para>
   Existen varios métodos para migrar repositorios. Recomendamos utilizar los <emphasis>niveles de caché,</emphasis> ya que es un método transparente, reduce el tiempo de inactividad del clúster y evita la duplicación de todos los datos del repositorio.
  </para>

  <sect2 xml:id="pool.migrate.cache_tier">
   <title>Migración mediante niveles de caché</title>
   <para>
    El principio básico es simple: incluir el repositorio que quiere migrar en un nivel de caché en orden inverso. Encontrará más información sobre los niveles de caché en el <xref linkend="cha.ceph.tiered"/>. Por ejemplo, para migrar un repositorio replicado llamado "testpool" a un repositorio codificado de borrado, siga estos pasos:
   </para>
   <procedure>
    <title>Migración de un repositorio replicado a un repositorio codificado de borrado</title>
    <step>
     <para>
      Cree un nuevo repositorio codificado de borrado y llámelo "newpool":
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      Ahora tiene dos repositorios: el original replicado y lleno de datos ("testpool") y el nuevo repositorio codificado de borrado vacío ("newpool"):
     </para>
     <figure>
      <title>Repositorios antes de la migración</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Configure los niveles de caché y establezca el repositorio replicado "testpool" como repositorio de caché:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      A partir de ahora, todos los objetos nuevos se crearán en el nuevo repositorio:
     </para>
     <figure>
      <title>Configuración de los niveles de caché</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Ejecute un traslado forzoso de todos los objetos del repositorio de caché al nuevo repositorio:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Limpieza de datos</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Traslade todos los clientes al nuevo repositorio. Hasta que todos los datos se hayan movido al nuevo repositorio codificado de borrado, es preciso especificar una superposición para que los objetos se busquen en el repositorio antiguo:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Con la superposición, todas las operaciones se reenvían al repositorio replicado antiguo ("testpool"):
     </para>
     <figure>
      <title>Configuración de la superposición</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Ahora puede configurar todos los clientes para que accedan a los objetos en el nuevo repositorio.
     </para>
    </step>
    <step>
     <para>
      Una vez migrados todos los datos al repositorio codificado de borrado "newpool", elimine la superposición y el repositorio de caché antiguo ("testpool"):
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migración completada</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Instantáneas de repositorios</title>

  <para>
   Las instantáneas de repositorios son instantáneas del estado del repositorio Ceph completo. Con las instantáneas de repositorios, es posible conservar el historial de estado del repositorio. Según el tamaño del repositorio, la creación de instantáneas de repositorios puede requerir mucho espacio de almacenamiento. Compruebe siempre que hay espacio de almacenamiento suficiente antes de crear la instantánea de un repositorio.
  </para>

  <sect2>
   <title>Creación de una instantánea de un repositorio</title>
   <para>
    Para crear una instantánea de un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Eliminación de una instantánea de un repositorio</title>
   <para>
    Para eliminar una instantánea de un repositorio, ejecute:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.pool.compression">
  <title>Compresión de datos</title>

  <para>
   A partir de SUSE Enterprise Storage 5, BlueStore proporciona compresión de datos sobre la marcha para ahorrar espacio de disco.
  </para>

  <sect2 xml:id="sec.ceph.pool.compression.enable">
   <title>Cómo habilitar la compresión</title>
   <para>
    La compresión de datos de un repositorio puede habilitarse con:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    Sustituya <replaceable>POOL_NAME</replaceable> por el repositorio para el que desea habilitar la compresión.
   </para>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.compression.options">
   <title>Opciones de compresión de repositorio</title>
   <para>
    Este es un listado completo de las opciones de compresión:
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Valores: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>. Por defecto: <literal>snappy</literal>.
      </para>
      <para>
       El algoritmo de compresión utilizado depende del caso de uso específico. Estas son algunas de nuestras recomendaciones:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         No utilice <literal>zlib</literal>: los otros algoritmos son mejores.
        </para>
       </listitem>
       <listitem>
        <para>
         Si necesita un buen índice de compresión, utilice <literal>zstd</literal>. Tenga en cuenta que <literal>zstd</literal> no se recomienda para BlueStore debido a la alta sobrecarga de CPU al comprimir pequeñas cantidades de datos.
        </para>
       </listitem>
       <listitem>
        <para>
         Si necesita reducir el uso de CPU, use <literal>lz4</literal> o <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Ejecute una comparativa de estos algoritmos en una muestra de sus datos reales y observe el uso de CPU y memoria en su clúster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       Valor: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Por defecto: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: no comprimir nunca
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: comprimir si se sugiere <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: comprimir a menos que se sugiera <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: comprimir siempre
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Para obtener información acerca de cómo definir los indicadores <literal>COMPRESSIBLE</literal> o <literal>INCOMPRESSIBLE</literal>, consulte <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valor: doble, proporción = SIZE_COMPRESSED / SIZE_ORIGINAL. Por defecto: <literal>.875</literal>
      </para>
      <para>
       Los objetos por encima de esta proporción no se almacenarán comprimidos, ya que el beneficio neto es bajo.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>0</literal>
      </para>
      <para>
       El tamaño mínimo de los objetos comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>0</literal>
      </para>
      <para>
       El tamaño máximo de los objetos comprimidos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.bluestore_compression.options">
   <title>Opciones de compresión global</title>
   <para>
    Las siguientes opciones de configuración se pueden definir en la configuración de Ceph y se aplican a todos los OSD y no a un solo repositorio. La configuración específica de repositorio indicada en la <xref linkend="sec.ceph.pool.compression.options"/> tendrá prioridad.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Valores: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>, <literal>zlib</literal>. Por defecto: <literal>snappy</literal>.
      </para>
      <para>
       El algoritmo de compresión utilizado depende del caso de uso específico. Estas son algunas de nuestras recomendaciones:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         No utilice <literal>zlib</literal>: los otros algoritmos son mejores.
        </para>
       </listitem>
       <listitem>
        <para>
         Si necesita un buen índice de compresión, utilice <literal>zstd</literal>. Tenga en cuenta que <literal>zstd</literal> no se recomienda para BlueStore debido a la alta sobrecarga de CPU al comprimir pequeñas cantidades de datos.
        </para>
       </listitem>
       <listitem>
        <para>
         Si necesita reducir el uso de CPU, use <literal>lz4</literal> o <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Ejecute una comparativa de estos algoritmos en una muestra de sus datos reales y observe el uso de CPU y memoria en su clúster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Valor: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Por defecto: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: no comprimir nunca
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: comprimir si se sugiere <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: comprimir a menos que se sugiera <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: comprimir siempre
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Para obtener información acerca de cómo definir los indicadores <literal>COMPRESSIBLE</literal> o <literal>INCOMPRESSIBLE</literal>, consulte <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Valor: doble, proporción = SIZE_COMPRESSED / SIZE_ORIGINAL. Por defecto: <literal>.875</literal>
      </para>
      <para>
       Los objetos por encima de esta proporción no se almacenarán comprimidos, ya que el beneficio neto es bajo.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>0</literal>
      </para>
      <para>
       El tamaño mínimo de los objetos comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>0</literal>
      </para>
      <para>
       El tamaño máximo de los objetos comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>8K</literal>
      </para>
      <para>
       Tamaño mínimo de los objetos comprimidos y almacenados en la unidad de estado sólido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>64K</literal>
      </para>
      <para>
       Tamaño máximo de los objetos comprimidos y almacenados en la unidad de estado sólido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>128K</literal>
      </para>
      <para>
       Tamaño mínimo de los objetos comprimidos y almacenados en los discos duros.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: entero sin firmar, tamaño en bytes. Por defecto: <literal>512K</literal>
      </para>
      <para>
       Tamaño máximo de los objetos comprimidos y almacenados en los discos duros.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
