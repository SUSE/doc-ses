<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha: exportación de datos de Ceph a través de NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editar</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sí</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha es un servidor NFS (consulte <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Uso compartido de sistemas de archivos con NFS</link>) que se ejecuta en un espacio de dirección de usuario, en lugar de hacerlo como parte del kernel del sistema operativo. Con NFS Ganesha, puede poner en marcha su propio mecanismo de almacenamiento, como Ceph, y acceder a él desde cualquier cliente NFS.
 </para>
 <para>
  Los depósitos S3 se exportan a NFS en base a cada usuario; por ejemplo, mediante la vía <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
 </para>
 <para>
  CephFS se exporta por defecto a través de la vía <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename>.
 </para>
 <note>
  <title>rendimiento de NFS Ganesha</title>
  <para>
   Debido al aumento de la sobrecarga de protocolo y a la latencia adicional causados por saltos de red adicionales entre el cliente y el almacenamiento, el acceso a Ceph a través de una pasarela de NFS puede reducir de forma significativa el rendimiento de la aplicación en comparación con los clientes nativos de CephFS u Object Gateway.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Instalación</title>

  <para>
   Para obtener instrucciones sobre la instalación, consulte <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configuración</title>

  <para>
   Para obtener una lista de todos los parámetros disponibles en el archivo de configuración, consulte:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> para las opciones de capa de abstracción del sistema de archivos (FSAL) CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> para las opciones de FSAL de Object Gateway.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Esta sección incluye información que le ayudará a configurar el servidor NFS Ganesha a fin de exportar los datos del clúster a los que se puede acceder a través de Object Gateway y CephFS.
  </para>

  <para>
   La configuración de NFS Ganesha consta de dos partes: configuración del servicio y configuración de las exportaciones. La configuración del servicio se controla mediante <filename>/etc/ganesha/ganesha.conf</filename>. Tenga en cuenta que los cambios que realice en este archivo se sobrescribirán cuando se ejecute la fase 4 de DeepSea. Para cambiar de forma persistente los valores, edite el archivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> ubicado en el master de Salt. La configuración de las exportaciones se almacena en el clúster de Ceph como objetos RADOS.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Configuración del servicio</title>
   <para>
    La configuración del servicio se almacena en <filename>/etc/ganesha/ganesha.conf</filename> y controla todos los valores del daemon de NFS Ganesha, incluido donde se almacena la configuración de las exportaciones en el clúster de Ceph. Tenga en cuenta que los cambios que realice en este archivo se sobrescribirán cuando se ejecute la fase 4 de DeepSea. Para cambiar de forma persistente los valores, edite el archivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> ubicado en el master de Salt.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>Sección RADOS_URLS</title>
    <para>
     La sección <literal>RADOS_URLS</literal> permite configurar el acceso al clúster de Ceph para leer la configuración de NFS Ganesha desde objetos RADOS.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        La ubicación de la vía del archivo de configuración de Ceph.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        El ID de usuario de cephx.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        La URL del objeto RADOS donde se puede ver si hay notificaciones de recarga.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>Sección RGW</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Señala al archivo <filename>ceph.conf</filename>. Si se distribuye con DeepSea, no es necesario cambiar este valor.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        El nombre del usuario del cliente de Ceph que utiliza NFS Ganesha.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        El nombre del clúster de Ceph. SUSE Enterprise Storage 6 solo admite actualmente un nombre de clúster, que es <literal>ceph</literal> por defecto.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>URL del objeto RADOS</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     NFS Ganesha admite la lectura de la configuración desde un objeto RADOS. La directiva <literal>%url</literal> permite especificar una URL de RADOS que identifica la ubicación del objeto RADOS.
    </para>
    <para>
     Una URL de RADOS puede tener dos formatos: <literal>rados://&lt;REPOSITORIO&gt;/&lt;OBJETO&gt;</literal> o <literal>rados://&lt;REPOSITORIO&gt;/&lt;ESPACIODENOMBRES&gt;/&lt;OBJETO&gt;</literal>, donde <literal>REPOSITORIO</literal> es el repositorio de RADOS donde se almacena el objeto, <literal>ESPACIODENOMBRE</literal> es el espacio de nombres del repositorio donde se almacena el objeto y <literal>OBJETO</literal> es el nombre del objeto.
    </para>
    <para>
     Para admitir las capacidades de gestión de NFS Ganesha de Ceph Dashboard, el nombre del objeto RADOS debe seguir una convención para cada daemon de servicio. El nombre del objeto debe tener el formato <literal>conf-<replaceable>ID_MINION</replaceable></literal>, donde ID_MINION corresponde al ID de minion de Salt del nodo donde se ejecuta este servicio.
    </para>
    <para>
     DeepSea ya se encarga de generar correctamente esta URL y no es necesario realizar ningún cambio.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Cambio de los puertos por defecto de NFS Ganesha</title>
    <para>
     NFS Ganesha usa por defecto el puerto 2049 para NFS y el 875 para la compatibilidad con rquota. Para cambiar los números de puerto por defecto, utilice las opciones <option>NFS_Port</option> y <option>RQUOTA_Port</option> dentro de la sección <literal>NFS_CORE_PARAM</literal>, por ejemplo:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Configuración de las exportaciones</title>
   <para>
    La configuración de las exportaciones se almacena como objetos RADOS en el clúster de Ceph. Cada bloque de exportación se almacena en su propio objeto RADOS denominado <literal>export-&lt;id&gt;</literal>, donde <literal>&lt;id&gt;</literal> debe coincidir con el atributo <literal>Export_ID</literal> de la configuración de la exportación. La asociación entre las exportaciones y los servicios de NFS Ganesha se realiza a través de los objetos <literal>conf-MINION_ID</literal>. Cada objeto de servicio contiene una lista de direcciones URL de RADOS para cada exportación exportada por ese servicio. Un bloque de exportación tiene el siguiente aspecto:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    Para crear el objeto RADOS para el bloque de exportación anterior, primero es necesario almacenar el código de bloque de exportación en un archivo. A continuación, se puede usar la herramienta de línea de comandos de RADOS para almacenar el contenido del archivo guardado previamente en un objeto RADOS.
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    Después de crear el objeto de exportación, es posible asociar la exportación a una instancia de servicio añadiendo la URL de RADOS correspondiente del objeto de exportación al objeto de servicio. En las secciones siguientes se describe cómo configurar un bloque de exportación.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Sección Export principal</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Cada exportación debe tener un valor "Export_Id" exclusivo (obligatorio).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        La vía de exportación en el repositorio de CephFS relacionado (obligatorio). Esto permite exportar los subdirectorios desde CephFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Vía de exportación de NFS de destino (obligatorio para NFSv4). Define en qué vía de exportación de NFS estarán disponibles los datos exportados.
       </para>
       <para>
        Ejemplo: con el valor <literal>/cephfs/</literal> y después de ejecutar
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        Los datos de CephFS están disponibles en el directorio <filename>/mnt/cephfs/</filename> del cliente.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        "RO" para el acceso de solo lectura, "RW" para el acceso de lectura y escritura y "None" para que no haya ningún acceso.
       </para>
       <tip>
        <title>límite del acceso a los clientes</title>
        <para>
         Si se deja <literal>Access_Type = RW</literal> en la sección principal <literal>EXPORT</literal> y se limita el acceso a un cliente específico en la sección <literal>CLIENT</literal>, otros clientes podrán conectarse de cualquier modo. Para inhabilitar el acceso a todos los clientes y habilitar el acceso solo a clientes específicos, defina <literal>Access_Type = None</literal> en la sección <literal>EXPORT</literal> y, a continuación, especifique el modo de acceso menos restrictivo para uno o más clientes en la sección <literal>CLIENT</literal>:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        Opción squash de NFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exportación de la "Capa de abstracción del sistema de archivos". Consulte la <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>Subsección FSAL</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Define qué procesador final utiliza NFS Ganesha. Los valores permitidos son <literal>CEPH</literal> para CephFS o <literal>RGW</literal> para Object Gateway. Dependiendo de lo que elija, habrá que definir un valor para <literal>role-mds</literal> o <literal>role-rgw</literal> en <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Funciones personalizadas de NFS Ganesha</title>

  <para>
   Es posible definir funciones personalizadas de NFS Ganesha para los nodos del clúster. Estas funciones se asignan posteriormente a los nodos en <filename>policy.cfg</filename>. Las funciones permiten:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Nodos de NFS Ganesha separados para acceder a Object Gateway y CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     Asignar distintos usuarios de Object Gateway a nodos de NFS Ganesha.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Contar con usuarios de Object Gateway distintos permite a los nodos de NFS Ganesha acceder a distintos depósitos S3. Los depósitos S3 se pueden utilizar para controlar el acceso. Nota: los depósitos S3 no se deben confundir con los depósitos de Ceph utilizados en el mapa de CRUSH.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Usuarios distintos de Object Gateway para NFS Ganesha</title>
   <para>
    El siguiente procedimiento de ejemplo para el master de Salt muestra cómo crear dos funciones de NFS Ganesha con diferentes usuarios de Object Gateway. En este ejemplo se utilizan las funciones <literal>gold</literal> y <literal>silver</literal>. DeepSea ya proporciona archivos de configuración de ejemplo para ellas.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Abra el archivo <filename>/srv/pillar/ceph/stack/global.yml</filename> con el editor que quiera. Cree el archivo si no existe.
     </para>
    </step>
    <step>
     <para>
      El archivo debe contener las siguientes líneas:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      Estas funciones se pueden asignar más tarde en <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Cree un archivo <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> y añada el siguiente contenido:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Cree un archivo <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> y añada el siguiente contenido:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Ahora, hay que crear plantillas para <filename>ganesha.conf</filename> para cada función. La plantilla original de DeepSea es un buen punto de partida. Cree dos copias:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      Las nuevas funciones requieren anillos de claves para acceder al clúster. Para proporcionar el acceso, copie la línea <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copie el anillo de claves para Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway también necesita la configuración de las distintas funciones:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Asigne las funciones recién creadas a los nodos del clúster en <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Sustituya <replaceable>NODE1</replaceable> y <replaceable>NODE2</replaceable> por los nombres de los nodos a los que desea asignar las funciones.
     </para>
    </step>
    <step>
     <para>
      Ejecute las fases 0 a 4 de DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Separación de FSAL de CephFS y Object Gateway</title>
   <para>
    El siguiente procedimiento de ejemplo para el master de Salt muestra cómo crear dos funciones distintas nuevas que utilizan CephFS y Object Gateway:
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Abra el archivo <filename>/srv/pillar/ceph/rgw.sls</filename> con el editor que quiera. Cree el archivo si no existe.
     </para>
    </step>
    <step>
     <para>
      El archivo debe contener las siguientes líneas:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      Estas funciones se pueden asignar más tarde en <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Ahora, hay que crear plantillas para <filename>ganesha.conf</filename> para cada función. La plantilla original de DeepSea es un buen punto de partida. Cree dos copias:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Edite <filename>ganesha_rgw.conf.j2</filename> y elimine la sección:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Edite <filename>ganesha_cfs.conf.j2</filename> y elimine la sección:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Las nuevas funciones requieren anillos de claves para acceder al clúster. Para proporcionar el acceso, copie la línea <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      La línea <literal>caps mds = "allow *"</literal> se puede eliminar de <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copie el anillo de claves para Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway necesita la configuración de la nueva función:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Asigne las funciones recién creadas a los nodos del clúster en <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Sustituya <replaceable>NODE1</replaceable> y <replaceable>NODE2</replaceable> por los nombres de los nodos a los que desea asignar las funciones.
     </para>
    </step>
    <step>
     <para>
      Ejecute las fases 0 a 4 de DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Operaciones admitidas</title>
   <para>
    La interfaz NFS de RGW admite la mayoría de las operaciones en archivos y directorios, con las siguientes restricciones:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>No se admiten enlaces que incluyan enlaces simbólicos.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>No se admiten listas de control de acceso de NFS.</emphasis> <emphasis>Se admiten</emphasis> la propiedad y los permisos de usuario y grupo de Unix.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Los directorios no se pueden mover ni cambiar de nombre.</emphasis> <emphasis>Se pueden</emphasis> mover archivos entre directorios.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Solo se admite E/S de escritura secuencial completa.</emphasis> Por lo tanto, se impone que las operaciones de escritura sean cargas. Muchas operaciones de E/S típicas, como la edición de archivos locales producirán siempre un error porque se realizan almacenamientos no secuenciales. Hay utilidades de archivo que aparentemente realizan escrituras secuenciales (por ejemplo, algunas versiones <command>tar</command> de GNU), pero pueden fallar porque realicen almacenamientos no secuenciales poco frecuentes. Al montar mediante NFS, la E/S secuencial de una aplicación generalmente se ve obligada a realizar escrituras secuenciales en el servidor NFS a través del montaje sincrónico (la opción <option>-o sync</option>). Los clientes de NFS que no pueden realizar el montaje de forma sincrónica (por ejemplo, Microsoft Windows*) no podrán cargar archivos.
     </para>
    </listitem>
    <listitem>
     <para>
      RGW de NFS admite operaciones de lectura y escritura solo para bloques de menos de 4 MB de tamaño.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Inicio o reinicio de NFS Ganesha</title>

  <para>
   Para habilitar e iniciar el servicio de NFS Ganesha, ejecute:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Reinicie NFS Ganesha con:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   Cuando se inicia o se reinicia NFS Ganesha, tiene un tiempo límite de gracia de 90 segundos para NFS v4. Durante el período de gracia, las peticiones nuevas de los clientes se rechazan de forma activa. Por lo tanto, los clientes pueden observar una ralentización de las peticiones cuando NFS se encuentra en período de gracia.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Definición del nivel de registro</title>

  <para>
   Para cambiar el nivel de depuración por defecto <literal>NIV_EVENT</literal>, debe editar el archivo <filename>/etc/sysconfig/nfs-ganesha</filename>. Sustituya <literal>NIV_EVENT</literal> por <literal>NIV_DEBUG</literal> o <literal>NIV_FULL_DEBUG</literal>. Si se aumenta el nivel de detalle del registro, se pueden producir grandes cantidades de datos en los archivos de registro.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   Es preciso reiniciar el servicio cuando se cambia el nivel del registro.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verificación del recurso compartido NFS exportado</title>

  <para>
   Cuando se utiliza NFS v3, puede verificar si los recursos compartidos NFS se exportan en el nodo del servidor NFS Ganesha:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Montaje del recurso compartido NFS exportado</title>

  <para>
   Para montar el recurso compartido de NFS exportado (según se haya configurado en la <xref linkend="ceph-nfsganesha-config"/>) en un host de cliente, ejecute:
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
