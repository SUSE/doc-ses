<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage.salt.cluster">
 <title>Administración de un clúster de Salt</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sí</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Después de distribuir el clúster de Ceph, probablemente tendrá que realizar modificaciones ocasionalmente. Por ejemplo, añadir o eliminar nuevos nodos, discos o servicios. En este capítulo se describe cómo llevar a cabo estas tareas de administración.
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>Adición de nuevos nodos de clúster</title>

  <para>
   El procedimiento de añadir nodos nuevos al clúster es casi idéntico a la distribución inicial de nodos del clúster que se describe en el <xref linkend="ceph.install.saltstack"/>:
  </para>

  <procedure>
   <step>
    <para>
     Instale SUSE Linux Enterprise Server 12 SP3 en el nuevo nodo, configure la red de modo que resuelva correctamente el nombre del master de Salt e instale el paquete <systemitem>salt-minion</systemitem>:
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Si el nombre del master de Salt no es <literal>salt</literal>, edite <filename>/etc/salt/minion</filename> y añada lo siguiente:
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     Si ha realizado cambios en los archivos de configuración mencionados anteriormente, reinicie el servicio <systemitem>salt.minion</systemitem>:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Acepte todas las claves de Salt en el master de Salt:
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Compruebe que <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> también tiene como objetivo el nuevo minion de Salt. Consulte el <xref linkend="ds.minion.targeting.name"/> del <xref linkend="ds.depl.stages"/> para obtener más detalles.
    </para>
   </step>
   <step>
    <para>
     Ejecute la etapa de preparación. Se sincronizarán los módulos y grains para que el nuevo minion pueda proporcionar toda la información que espera DeepSea:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     Ejecute la etapa de descubrimiento. Se escribirán nuevas entradas de archivos en el directorio <filename>/srv/pillar/ceph/proposals</filename>, donde podrá editar los archivos .yml relevantes:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Si lo desea, cambie <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> si el host recién añadido no coincide con el esquema de denominación existente. Para obtener información detallada, consulte el <xref linkend="policy.configuration"/>.
    </para>
   </step>
   <step>
    <para>
     Ejecute la etapa de configuración. Se leerá todo el contenido de <filename>/srv/pillar/ceph</filename> y el pillar se actualizará en consecuencia:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     El pillar almacenará los datos, a los que podrá acceder con el siguiente comando:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     Las etapas de distribución y configuración incluyen los nodos recién añadidos:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>Adición de nuevas funciones a los nodos</title>

  <para>
   Puede distribuir cualquier tipo de función compatible con DeepSea. Consulte el <xref linkend="policy.role.assignment"/> para obtener más información sobre los tipos de funciones compatibles y ejemplos de coincidencias.
  </para>

  <tip>
   <title>etapas y funciones obligatorias y opcionales</title>
   <para>
    Generalmente, recomendamos ejecutar todas las etapas de distribución de 0 a 5 al añadir una nueva función a un nodo del clúster. Para ahorrar tiempo, puede omitir las etapas 3 o 4, dependiendo del tipo de función que vaya a distribuir. Aunque las funciones OSD y MON incluyen servicios esenciales y son obligatorias para usar Ceph, otras funciones, como Object Gateway, son opcionales. Las etapas de distribución de DeepSea son jerárquicas: mientras que en la etapa 3 se distribuyen servicios esenciales, en la etapa 4 se distribuye los opcionales.
   </para>
   <para>
    Por lo tanto, debe ejecutar la etapa 3 para distribuir las funciones esenciales, como MON, en un nodo OSD existente, y puede omitir la etapa 4.
   </para>
   <para>
    De igual forma, puede omitir la etapa 3 al distribuir servicios opcionales, como Object Gateway, pero en ese caso, debe ejecutar la etapa 4.
   </para>
  </tip>

  <para>
   Para añadir un nuevo servicio a un nodo existente, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Adapte <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> para que se adapte al host existente con una función nueva. Para obtener más información, consulte el <xref linkend="policy.configuration"/>. Por ejemplo, si necesita ejecutar un Object Gateway en un nodo MON, la línea será similar a la siguiente:
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     Ejecute la etapa 2 para actualizar el pillar:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     Ejecute la etapa 3 para distribuir los servicios esenciales o la etapa 4 para los opcionales. No es perjudicial ejecutar ambas etapas.
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    Al añadir un OSD al clúster existente, tenga en cuenta que el clúster tardará un tiempo en reequilibrarse. Para minimizar los períodos de reequilibrio, recomendamos añadir al mismo tiempo todos los OSD que tenga previstos.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>eliminación y reinstalación de nodos del clúster</title>

  <para>
   Para eliminar una función de un clúster, edite <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> y elimine las líneas correspondientes. A continuación, ejecute las etapas 2 y 5, tal como se describe en el <xref linkend="ceph.install.stack"/>.
  </para>

  <note>
   <title>Eliminación de OSD de su clúster</title>
   <para>
    En caso de que deba eliminar un nodo OSD en concreto del clúster, asegúrese de que el clúster tenga más espacio de disco disponible que el disco que pretende eliminar. Tenga en cuenta que la eliminación de un OSD da lugar al reequilibrado de todo el clúster.
   </para>
  </note>

  <para>
   Al eliminar una función de un minion, el objetivo es deshacer todos los cambios relacionados con dicha función. Para la mayoría de las funciones, la tarea es sencilla, pero puede haber algunos problemas con las dependencias de paquetes. Al desinstalar un paquete, no se desinstalan sus dependencias.
  </para>

  <para>
   Los OSD eliminados se muestran como unidades vacías. Las tareas relacionadas sobrescriben el principio de los sistemas de archivos y eliminan las particiones de copia de seguridad, además de borrar las tablas de particiones.
  </para>

  <note>
   <title>conservación de particiones creadas mediante otros métodos</title>
   <para>
    Las unidades de disco configuradas previamente mediante otros métodos, como <command>ceph-deploy</command>, aún podrían contener particiones. DeepSea no las destruirá automáticamente. El administrador debe recuperar estas unidades.
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>Eliminación de un minion de Salt del clúster</title>
   <para>
    Si sus minions de almacenamiento se denominan, por ejemplo, "data1.ceph", "data2.ceph" ... "data6.ceph", y las líneas relacionadas del archivo <filename>policy.cfg</filename> son similares a las siguientes:
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    Para eliminar el minion de Salt "data2.ceph", cambie las líneas por las siguientes:
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    A continuación, ejecute las etapas 2 y 5:
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>Migración de nodos</title>
   <para>
    Supongamos que se da la siguiente situación: durante la instalación de un nuevo clúster, usted (el administrador) asigna uno de los nodos de almacenamiento como Object Gateway mientras espera la llegada del hardware del gateway. Ahora que ha llegado el hardware permanente para el gateway, por fin puede asignar la función deseada al nodo de almacenamiento de copia de seguridad y eliminar la función de gateway.
   </para>
   <para>
    Después de ejecutar las etapas 0 y 1 (consulte <xref linkend="ds.depl.stages"/>) en el nuevo hardware, ha asignado el nombre <literal>rgw1</literal> al nuevo gateway. Si el nodo <literal>data8</literal> necesita que se añada la función de almacenamiento y se elimine la de Object Gateway y el archivo <filename>policy.cfg</filename> tiene una configuración semejante a esta:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    Cámbielo a:
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    Ejecute las etapas 2 a 5. En la etapa 3, se añadirá <literal>data8</literal> como nodo de almacenamiento. Durante un momento, <literal>data8</literal> tendrá ambas funciones. En la etapa 4, se añadirá la función de Object Gateway a <literal>rgw1</literal>. En la etapa 5, se eliminará la función de Object Gateway de <literal>data8</literal>.
   </para>
  </example>
 </sect1>
 <sect1 xml:id="ds.mon">
  <title>Redistribución de nodos de Monitor</title>

  <para>
   Si uno o varios nodos de Monitor presentan errores y no responden, debe eliminar los monitores fallidos del clúster y, si es posible, volver a añadirlos a continuación.
  </para>

  <important>
   <title>debe haber al menos tres nodos de Monitor</title>
   <para>
    El número de nodos de Monitor no debe ser inferior a tres. Si se produce un error en un nodo de Monitor y el clúster solo cuenta con uno o dos nodos de Monitor, deberá asignar temporalmente la función de Monitor a otros nodos del clúster antes de redistribuir los nodos fallidos. Después de redistribuir los nodos fallidos, puede desinstalar las funciones de Monitor temporales.
   </para>
   <para>
    Para obtener más información acerca de cómo añadir nodos o funciones al clúster de Ceph, consulte la <xref linkend="salt.adding.nodes"/> y la <xref linkend="salt.adding.services"/>.
   </para>
   <para>
    Para obtener más información sobre la eliminación de nodos del clúster, consulte la <xref linkend="salt.node.removing"/>.
   </para>
  </important>

  <para>
   Existen dos grados básicos de fallo de un nodo de Ceph:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     El host del minion de Salt está averiado físicamente o a nivel de sistema operativo y no responde a la llamada <command>salt '<replaceable>nombre_minion</replaceable>' test.ping</command>. En ese caso, debe volver a distribuir el servidor completamente siguiendo las instrucciones que encontrará en <xref linkend="ceph.install.stack"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Los servicios relacionados con el monitor fallan y no se recuperan, pero el host responde a la llamada <command>salt '<replaceable>nombre_minion</replaceable>' test.ping</command>. Si se da esta situación, siga estos pasos:
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Edite <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> en el master de Salt y elimine o actualice las líneas correspondientes a los nodos de Monitor fallidos, de modo que lleven a nodos de Monitor en funcionamiento.
    </para>
   </step>
   <step>
    <para>
     Ejecute las etapas 2 a 5 de DeepSea para aplicar los cambios:
    </para>
<screen>
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.4
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>Adición de un OSD a un nodo</title>

  <para>
   Para añadir un disco a un nodo OSD existente, compruebe que todas las particiones del disco se hayan eliminado y borrado. Consulte el <xref linkend="deploy.wiping.disk"/> de <xref linkend="ceph.install.stack"/> para obtener más información. Cuando el disco esté vacío, añádalo al archivo YAML del nodo. La vía al archivo es <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>nombre_nodo</replaceable>.yml</filename>. Después de guardar el archivo, ejecute las etapas 2 y 3 de DeepSea:
  </para>

<screen><prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>

  <tip>
   <title>actualización automática de los perfiles</title>
   <para>
    En lugar de editar manualmente el archivo YAML, DeepSea puede crear nuevos perfiles automáticamente. Para que DeepSea pueda crear nuevos perfiles, es necesario mover los existentes:
   </para>
<screen><prompt>root@master # </prompt><command>old</command> /srv/pillar/ceph/proposals/profile-default/
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.1
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
  </tip>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>Eliminación de un OSD</title>

  <para>
   Puede eliminar un OSD de Ceph del clúster ejecutando el siguiente comando:
  </para>

<screen><prompt>root@master # </prompt><command>salt-run</command> disengage.safety
<prompt>root@master # </prompt><command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> debe ser el número del OSD sin el término <literal>osd</literal>. Por ejemplo, para <literal>osd.3</literal>, utilice solo el dígito <literal>3</literal>.
  </para>

  <tip>
   <title>eliminación de varios OSD</title>
   <para>
    No es posible eliminar varios OSD en paralelo con el comando <command>salt-run remove.osd</command>. Para automatizar la eliminación de varios OSD, puede emplear el siguiente bucle (5, 21, 33 y 19 son números de ID de OSD que se deben eliminar):
   </para>
<screen>
for i in 5 21 33 19
do
 echo $i
 salt-run disengage.safety
 salt-run remove.osd $i
done
</screen>
  </tip>

  <sect2 xml:id="osd.forced.removal">
   <title>Eliminación forzosa de OSD dañados</title>
   <para>
    Existen casos en que el procedimiento de eliminación ordenada de un OSD (consulte la <xref linkend="salt.removing.osd"/>) produce un error. Por ejemplo, esto puede ocurrir si el OSD o su caché están dañados, cuando hay operaciones de E/S pendientes o si no es posible desmontar el disco del OSD. En ese caso, debe forzar la eliminación del OSD:
   </para>
<screen><prompt>root@master # </prompt><replaceable>target</replaceable> osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <para>
    Este comando permite eliminar la partición de datos y el diario transaccional o las particiones WAL/DB.
   </para>
   <para>
    Para identificar los posibles dispositivos de diario transaccional/WAL/DB huérfanos, lleve a cabo estos pasos:
   </para>
   <procedure>
    <step>
     <para>
      Elija el dispositivo que puede tener particiones huérfanas y guarde la lista de particiones en un archivo:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ls /dev/sdd?* &gt; /tmp/partitions
</screen>
    </step>
    <step>
     <para>
      Ejecute <command>readlink</command> para todos los dispositivos block.wal, block.db y de diario transaccional y compare la salida con la lista de particiones guardada anteriormente:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
     <para>
      La salida es la lista de particiones que Ceph <emphasis>no</emphasis> utiliza.
     </para>
    </step>
    <step>
     <para>
      Elimine las particiones huérfanas que no pertenecen a Ceph con el comando que prefiera (por ejemplo, <command>fdisk</command>, <command>parted</command> o <command>sgdisk</command>).
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ds.osd.recover">
  <title>Recuperación de un nodo OSD reinstalado</title>

  <para>
   Si el sistema operativo se interrumpe y no se puede recuperar en uno de los nodos OSD, lleve a cabo estos pasos para recuperarlo y volver a distribuir su función de OSD con los datos del clúster intactos:
  </para>

  <procedure>
   <step>
    <para>
     Vuelva a instalar el sistema operativo en el nodo.
    </para>
   </step>
   <step>
    <para>
     Instale los paquetes <package>salt-minion</package> en el nodo OSD, suprima la antigua clave del minion de Salt en el master de Salt y registre la nueva. Para obtener más información acerca de la distribución de minions de Salt, consulte <xref linkend="ceph.install.stack"/>.
    </para>
   </step>
   <step>
    <para>
     En lugar de ejecutar la etapa 0 completa, ejecute los siguientes elementos:
    </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     Ejecute las etapas 1 a 5 de DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     Ejecute la etapa 0 de DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     Reinicie el nodo OSD relevante. Todos los discos del OSD se redescubrirán y se reutilizarán.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Instalación automatizada mediante Salt</title>

  <para>
   La instalación se puede automatizar mediante el reactor de Salt. En entornos virtuales o entornos de hardware coherentes, esta configuración permitirá la creación de un clúster de Ceph con el comportamiento especificado.
  </para>

  <warning>
   <para>
    Salt no puede realizar comprobaciones de dependencias basadas en eventos del reactor. Existe un riesgo real de introducir al master Salt en una espiral potencialmente letal.
   </para>
  </warning>

  <para>
   La instalación automatizada requiere lo siguiente:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Un archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> creado correctamente.
    </para>
   </listitem>
   <listitem>
    <para>
     Una configuración personalizada preparada situada en el directorio <filename>/srv/pillar/ceph/stack</filename>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   La configuración por defecto del reactor solo ejecutará las etapas 0 y 1. Esto permite probar el reactor sin esperar a que se completen las etapas siguientes.
  </para>

  <para>
   Cuando se inicie el primer salt-minion, comenzará la etapa 0. Un bloqueo impide que existan varias instancias. Cuando todos los minions completen la etapa 0, empezará la etapa 1.
  </para>

  <para>
   Si la operación se lleva a cabo correctamente, cambie la última línea en el archivo <filename>/etc/salt/master.d/reactor.conf</filename>:
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   por
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>Actualización de los nodos del clúster</title>

  <para>
   Es recomendable aplicar las actualizaciones periódicas a los nodos del clúster regularmente. Para aplicar las actualizaciones, ejecute la etapa 0:
  </para>

<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>

  <para>
   Si DeepSea detecta un clúster de Ceph en ejecución, aplicará las actualizaciones y reiniciará los nodos secuencialmente. DeepSea sigue la recomendación oficial de Ceph de actualizar primero los monitores, luego los OSD y, por último, los servicios adicionales, como MDS, Object Gateway, iSCSI Gateway o NFS Ganesha. Si DeepSea detecta algún problema en el clúster, detendrá el proceso de actualización. Un activador potencial puede ser el siguiente:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Ceph informa del error "HEALTH_ERR" durante más de 300 segundos.
    </para>
   </listitem>
   <listitem>
    <para>
     Se envía una consulta a los minions de Salt para que sus servicios asignados sigan ejecutándose después de una actualización. La actualización falla si los servicios están inactivos durante más de 900 segundos.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Esta configuración permite que incluso en caso de que haya actualizaciones fallidas o dañadas, el clúster de Ceph permanezca operativo.
  </para>

  <para>
   La etapa 0 de DeepSea actualiza el sistema mediante <command>zypper update</command> y vuelve a arrancar el sistema si se actualiza el kernel. Si desea eliminar la posibilidad de un posible reinicio forzoso de todos los nodos, asegúrese de que el kernel más reciente esté instalado y en ejecución antes de iniciar la etapa 0 de DeepSea.
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    Si prefiere actualizar el sistema mediante el comando <command>zypper patch</command>, edite <filename>/srv/pillar/ceph/stack/global.yml</filename> y añada la siguiente línea:
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   Puede cambiar el comportamiento de reinicio predeterminado de la etapa 0 de DeepSea añadiendo las siguientes líneas al archivo <filename>/srv/pillar/ceph/stack/global.yml</filename>:
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal> define el comportamiento de la etapa 0 del master Salt y <literal>stage_prep_minion</literal> define el comportamiento de todos los minions. Los parámetros disponibles son los siguientes:
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      Instalar las actualizaciones y reiniciar después de la actualización.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      Instalar las actualizaciones sin reiniciar.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      Reiniciar sin instalar las actualizaciones.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      No instalar las actualizaciones ni reiniciar.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>Detención o reinicio del clúster</title>

  <para>
   En algunos casos, puede ser necesario detener o reiniciar el clúster completo. Recomendamos comprobar atentamente las dependencias de los servicios en ejecución. Los siguientes pasos se pueden usar como esquema de inicio y detención del clúster:
  </para>

  <procedure>
   <step>
    <para>
     Indique al clúster de Ceph que no marque los OSD como "out":
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Detenga los daemons y los nodos en el siguiente orden:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clientes de almacenamiento
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, por ejemplo, NFS Ganesha u Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de metadatos
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Si es necesario, realice las tareas de mantenimiento.
    </para>
   </step>
   <step>
    <para>
     Inicie los nodos y los servidores en el orden inverso al del proceso de apagado:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de metadatos
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, por ejemplo, NFS Ganesha u Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Clientes de almacenamiento
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Elimine el indicador de noout:
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>Archivo <filename>ceph.conf</filename> personalizado</title>

  <para>
   Si es necesario añadir una configuración personalizada al archivo <filename>ceph.conf</filename>, puede hacerlo modificando los archivos de configuración del directorio <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>archivo <filename>rgw.conf</filename> único</title>
   <para>
    Object Gateway ofrece mucha flexibilidad y es único en comparación con el resto de secciones de <filename>ceph.conf</filename>. Todos los demás componentes de Ceph tienen encabezados estáticos, como <literal>[mon]</literal> u <literal>[osd]</literal>. El Object Gateway tiene encabezados únicos, como <literal>[client.rgw.rgw1]</literal>. Esto significa que el archivo <filename>rgw.conf</filename> necesita una entrada de encabezado. Consulte <filename>/srv/salt/ceph/configuration/files/rgw.conf</filename> para ver un ejemplo.
   </para>
  </note>

  <important>
   <title>ejecute la etapa 3</title>
   <para>
    Después de hacer cambios personalizados en los archivos de configuración mencionados, ejecute la etapa 3 para aplicar dichos cambios a los nodos del clúster:
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   Estos archivos se incluyen desde el archivo de plantilla <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> y se corresponden con las diferentes secciones que acepta el archivo de configuración de Ceph. Un fragmento de configuración en el archivo correcto permite que DeepSea lo coloque en la sección adecuada. No es necesario que añada ninguno de los encabezados de sección.
  </para>

  <tip>
   <para>
    Para aplicar las opciones de configuración solo a instancias específicas de un daemon, añada un encabezado como <literal>[osd.1]</literal>. Las siguientes opciones de configuración solo se aplicarán al daemon OSD con el ID 1.
   </para>
  </tip>

  <sect2>
   <title>Sustitución de los valores por defecto</title>
   <para>
    Las declaraciones posteriores de una sección sustituyen a las anteriores. Por lo tanto, es posible anular la configuración por defecto especificada en la plantilla <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>. Por ejemplo, para desactivar la autenticación de cephx, añade las tres líneas siguientes al archivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>:
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>Inclusión de los archivos de configuración</title>
   <para>
    Si necesita aplicar muchas configuraciones personalizadas, utilice las siguientes declaraciones include dentro de los archivos de configuración personalizados para facilitar la gestión de archivos. A continuación encontrará un ejemplo del archivo <filename>osd.conf</filename>:
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    En el ejemplo anterior, los archivos <filename>osd1.conf</filename>, <filename>osd2.conf</filename>, <filename>osd3.conf</filename> y <filename>osd4.conf</filename> contienen las opciones de configuración específicas del OSD relacionado.
   </para>
   <tip>
    <title>configuración de tiempo de ejecución</title>
    <para>
     Los cambios realizados en los archivos de configuración de Ceph surten efecto después de reiniciar los daemons de Ceph relacionados. Consulte la <xref linkend="ceph.config.runtime"/> para obtener más información sobre cómo cambiar la configuración del tiempo de ejecución de Ceph.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.config.runtime">
  <title>Configuración del tiempo de ejecución de Ceph</title>

  <para>
   En la <xref linkend="ds.custom.cephconf"/> se describe cómo hacer cambios en el archivo de configuración de Ceph <filename>ceph.conf</filename>. Sin embargo, el comportamiento real del clúster no está determinado por el estado actual del archivo <filename>ceph.conf</filename>, sino por la configuración de los daemons de Ceph en ejecución, que se almacena en la memoria.
  </para>

  <para>
   Puede consultar a un daemon individual de Ceph un valor de configuración en concreto a través del <emphasis>zócalo de administración</emphasis> en el nodo en el que se esté ejecutando el daemon. Por ejemplo, el siguiente comando obtiene el valor del parámetro de configuración <option>osd_max_write_size</option> para el daemon denominado <literal>osd.0</literal>:
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   También puede <emphasis>cambiar</emphasis> la configuración de los daemons durante el tiempo de ejecución. Recuerde que este cambio es temporal y se perderá después del siguiente reinicio del daemon. Por ejemplo, el siguiente comando cambia el parámetro <option>osd_max_write_size</option> a "50" para todos los OSD del clúster:
  </para>

<screen><prompt>root # </prompt>ceph tell osd.* injectargs --osd_max_write_size 50</screen>

  <warning>
   <title><command>injectargs</command> no es fiable</title>
   <para>
    Lamentablemente, cambiar la configuración del clúster mediante el comando <command>injectargs</command> no es un proceso fiable al 100 %. Si necesita estar seguro de que el parámetro modificado está activo, cámbielo en el archivo de configuración y reinicie todos los daemons del clúster.
   </para>
  </warning>
 </sect1>
</chapter>
