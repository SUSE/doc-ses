<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha.ceph.upgrade">
 <title>Actualización desde versiones anteriores</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editar</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sí</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Este capítulo presenta los pasos necesarios para actualizar SUSE Enterprise Storage desde las versiones anteriores a la actual.
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>Lectura de las notas de la versión</title>

  <para>
   En las notas de la versión puede encontrar información adicional sobre los cambios realizados desde la versión previa de SUSE Enterprise Storage. Consulte las notas de versión para comprobar lo siguiente:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     si el hardware necesita consideraciones especiales,
    </para>
   </listitem>
   <listitem>
    <para>
     si los paquetes de software usados han cambiado de forma significativa,
    </para>
   </listitem>
   <listitem>
    <para>
     si es necesario tomar precauciones especiales para la instalación.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Las notas de la versión también proporcionan información que no pudo publicarse en el manual a tiempo y notas acerca de problemas conocidos.
  </para>

  <para>
   Después de instalar el paquete <package>release-notes-ses</package>, encontrará las notas de la versión en el directorio <filename>/usr/share/doc/release-notes</filename> o en línea en <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>Procedimiento de actualización general</title>

  <para>
   Antes de iniciar el procedimiento de actualización, tenga en cuenta los siguientes elementos:
  </para>

  <variablelist>
   <varlistentry>
    <term>Orden de la actualización</term>
    <listitem>
     <para>
      Antes de actualizar el clúster de Ceph, debe tener debidamente registrados en el SCC o en SMT tanto SUSE Linux Enterprise Server como SUSE Enterprise Storage. Puede actualizar los daemons del clúster mientras este esté en línea y en servicio. Algunos tipos de daemons dependen de otros. Por ejemplo, los daemons de Ceph Object Gateway dependen de los daemons de los Ceph Monitor y los Ceph OSD. Se recomienda actualizar en este orden:
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Monitores Ceph Monitor
       </para>
      </listitem>
      <listitem>
       <para>
        Gestores Ceph Manager
       </para>
      </listitem>
      <listitem>
       <para>
        Daemons Ceph OSD
       </para>
      </listitem>
      <listitem>
       <para>
        Servidores de metadatos
       </para>
      </listitem>
      <listitem>
       <para>
        Pasarelas Object Gateway
       </para>
      </listitem>
      <listitem>
       <para>
        Pasarelas iSCSI Gateway
       </para>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Supresión de las instantáneas innecesarias del sistema operativo</term>
    <listitem>
     <para>
      Elimine las instantáneas del sistema de archivos que no se necesiten de las particiones del sistema operativo de nodos. De esta forma se garantiza que haya suficiente espacio libre en disco durante la actualización.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Comprobación del estado del clúster</term>
    <listitem>
     <para>
      Se recomienda comprobar el estado del clúster antes de iniciar el procedimiento de actualización.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Actualización de uno en uno</term>
    <listitem>
     <para>
      Se recomienda actualizar todos los daemons de un tipo específico, por ejemplo todos los daemons de monitor o todos los daemons de OSD, uno a uno para asegurarse de que todos tienen la misma versión. También se recomienda actualizar todos los daemons del clúster antes de intentar utilizar las nuevas funciones de una versión.
     </para>
     <para>
      Después de actualizar todos los daemons de un tipo concreto, compruebe su estado.
     </para>
     <para>
      Asegúrese de que cada monitor se ha vuelto a unir al quórum después de que se actualicen todos los monitores:
     </para>
<screen><prompt>root # </prompt>ceph mon stat</screen>
     <para>
      Asegúrese de que cada daemon Ceph OSD se ha vuelto a unir al clúster después de que se actualicen todos los OSD:
     </para>
<screen><prompt>root # </prompt>ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     Definición del indicador <option>require-osd-release luminous</option>
    </term>
    <listitem>
     <para>
      Cuando se actualice el último OSD a SUSE Enterprise Storage 5, los nodos de monitor detectarán que todos los OSD tienen la versión "luminous" de Ceph y podrían mostrar la advertencia de que el indicador osdmap <option>require-osd-release luminous</option> no está definido. En tal caso, deberá definir este indicador manualmente para confirmar que, ahora que el clúster se ha actualizado a la versión "luminous", no es posible volver a la versión anterior de Ceph: "jewel". Defina el indicador ejecutando el comando siguiente:
     </para>
<screen><prompt>root@minion &gt; </prompt>sudo ceph osd require-osd-release luminous</screen>
     <para>
      Después de que se complete el comando, la advertencia desaparece.
     </para>
     <para>
      En las instalaciones nuevas de SUSE Enterprise Storage 5, este indicador se define automáticamente cuando los Ceph Monitor crean el osdmap inicial, por lo que no se necesita acción alguna por parte del usuario final.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds.migrate.osd.encrypted">
  <title>Cifrado de OSD durante la actualización</title>

  <para>
   A partir de SUSE Enterprise Storage 5, los OSD se distribuyen por defecto mediante BlueStore, en lugar de mediante FileStore. Aunque BlueStore admite el cifrado, los Ceph OSD se distribuyen sin cifrar por defecto. El procedimiento siguiente describe los pasos necesarios para cifrar los OSD durante el proceso de actualización. En él se presupone que tanto los datos como los discos WAL/DB que se van a usar para la distribución de los OSD están limpios y no tienen particiones. Si el disco se ha utilizado anteriormente, bórrelo con el procedimiento descrito en el <xref linkend="deploy.wiping.disk"/>.
  </para>

  <important>
   <title>un OSD cada vez</title>
   <para>
    Debe distribuir los OSD cifrados de uno en uno, no de forma simultánea. El motivo es que los datos del OSD se borran y que el clúster pasa por varias repeticiones de reequilibrio.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Determine los valores de <option>bluestore block db size</option> y <option>bluestore block wal size</option> para la distribución y añádalos en el archivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> en el master de Salt. Es preciso especificar los valores en bytes.
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     Para obtener más información sobre cómo personalizar el archivo <filename>ceph.conf</filename>, consulte el <xref linkend="ds.custom.cephconf"/>.
    </para>
   </step>
   <step>
    <para>
     Ejecute la fase 3 de DeepSea para distribuir los cambios:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Verifique que el archivo <filename>ceph.conf</filename> está actualizado en los nodos de OSD pertinentes:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     Edite los archivos *.yml del directorio <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename> que sean relevantes para los OSD que va a cifrar. Vuelva a comprobar que la vía es la definida en el archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> y asegúrese de que modifica los archivos *.yml correctos.
    </para>
    <important>
     <title>identificadores de disco largos</title>
     <para>
      A la hora de identificar los discos de OSD en los archivos <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>, use identificadores de disco largos.
     </para>
    </important>
    <para>
     A continuación se muestra un ejemplo de una configuración de OSD. Tenga en cuenta que debido a que es necesario el cifrado, las opciones <option>db_size</option> y <option>wal_size</option> se han eliminado:
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     Distribuya los nuevos OSD de almacenamiento de bloques con cifrado ejecutando las fases 2 y 3 de DeepSea:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    <para>
     Puede observar el progreso con los comandos <command>ceph -s</command> o <command>ceph osd tree</command>. Es fundamental que se permita un reequilibrio del clúster antes de repetir el proceso en el nodo de OSD siguiente.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>Actualización desde SUSE Enterprise Storage 4 (distribución de DeepSea) a la versión 5</title>

  <important xml:id="u4to5.softreq">
   <title>requisitos de software</title>
   <para>
    Debe tener el siguiente software instalado y actualizado con las últimas versiones de los paquetes en todos los nodos de Ceph que desee actualizar antes de iniciar el procedimiento de actualización:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Además, antes de iniciar la actualización, debe actualizar el nodo master de Salt a SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5 ejecutando <command>zypper migration</command> (o su método de actualización preferido).
   </para>
  </important>

  <warning>
   <title>puntos que se deben tener en cuenta antes de la actualización</title>
   <itemizedlist>
    <listitem>
     <para>
      Compruebe si se está ejecutando el servicio AppArmor e inhabilítelo en todos los nodos del clúster. Inicie el módulo AppArmor de YaST, seleccione <guimenu>Settings</guimenu> (Configuración) y desactive la casilla de verificación <guimenu>Enable Apparmor</guimenu> (Habilitar Apparmor). Confirme haciendo clic en <guimenu>Done</guimenu> (Terminado).
     </para>
     <para>
      Tenga en cuenta que SUSE Enterprise Storage <emphasis>no</emphasis> funcionará si AppArmor está habilitado.
     </para>
    </listitem>
    <listitem>
     <para>
      Aunque el clúster sea completamente funcional durante la actualización, DeepSea establece el indicador "noout", que impide a Ceph reequilibrar los datos durante el tiempo de inactividad y, por lo tanto, impide las transferencias de datos innecesarias.
     </para>
    </listitem>
    <listitem>
     <para>
      Para optimizar el proceso de actualización, DeepSea actualiza los nodos en este orden basado en la función que tiene asignada según recomiende Ceph en fases anteriores: Monitor, Manager, OSD, servidores de metadatos, Object Gateway, ISCSI Gateway y NFS Ganesha.
     </para>
     <para>
      Tenga en cuenta que DeepSea no puede impedir que el orden prescrito se infrinja si un nodo ejecuta varios servicios.
     </para>
    </listitem>
    <listitem>
     <para>
      Aunque el clúster de Ceph esté en funcionamiento durante la actualización, los nodos podrían rearrancarse a fin de aplicar, por ejemplo, nuevas versiones del kernel. Para reducir las operaciones de E/S en espera, se recomienda rechazar las peticiones entrantes durante el proceso de actualización.
     </para>
    </listitem>
    <listitem>
     <para>
      La actualización del clúster puede tardar mucho tiempo, aproximadamente el que se tarda en actualizar un equipo multiplicado por el número de nodos del clúster.
     </para>
    </listitem>
    <listitem>
     <para>
      A partir de la versión Ceph Luminous, la opción de configuración <option>osd crush location</option> ya no se admite. Actualice los archivos de configuración de DeepSea para que usen <command>crush location</command> antes de proceder con la actualización.
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   Para actualizar el clúster de SUSE Enterprise Storage 4 a la versión 5, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Defina el nuevo orden de clasificación de objetos interno. Para ello, ejecute:
    </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      Para verificar que el comando se ha ejecutado correctamente, se recomienda ejecutar:
     </para>
<screen><prompt>root # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Con el comando <command>rpm -q deepsea</command>, verifique que la versión del paquete de DeepSea en el nodo master de Salt empieza con al menos <literal>0.7</literal>. Por ejemplo:
    </para>
<screen><prompt>root # </prompt>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     Si el número de versión del paquete de DeepSea empieza con 0.6, vuelva a comprobar si el nodo master de Salt se ha migrado correctamente a SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5 (consulte <xref linkend="u4to5.softreq"/> al principio de esta sección). Este es un requisito previo que debe completarse antes de iniciar el procedimiento de actualización.
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Si ha registrado los sistemas con SUSEConnect y usa el SCC/SMT, no necesita llevar a cabo ninguna otra acción. Continúe con el <xref linkend="step.updatepillar"/>.
      </para>
     </step>
     <step>
      <para>
       Si <emphasis role="bold">no</emphasis> usa el SCC/SMT, sino una imagen ISO de medios u otro origen de paquete, añada los siguientes repositorios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base y SES5 Update. Puede hacerlo con el comando <command>zypper</command>. Antes, elimine todos los repositorios de software existentes y, a continuación, añada los nuevos repositorios necesarios. Por último, actualice los orígenes de los repositorios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       Cambie los datos de Pillar para utilizar una estrategia diferente. Edite:
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       Y añada la línea siguiente:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        La estrategia <literal>zypper-dup</literal> requiere que se añadan manualmente los repositorios de software más recientes, mientras que la opción por defecto <literal>zypper-migration</literal> se basa en los repositorios proporcionados por el SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     Actualice Pillar:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     Consulte la <xref linkend="ds.minion.targeting"/> para obtener información sobre cómo asignar destinos a los minions de Salt.
    </para>
   </step>
   <step>
    <para>
     Verifique si se ha escrito correctamente en Pillar:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     El resultado del comando debe reflejar la entrada que ha añadido.
    </para>
   </step>
   <step>
    <para>
     Actualice los minions de Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     Verifique que todos los minions de Salt se han actualizado:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     Incluya los minions de Salt del clúster. Consulte la <xref linkend="ds.minion.targeting"/> de <xref linkend="ds.depl.stages"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     Inicie la actualización de SUSE Linux Enterprise Server y Ceph:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>nueva ejecución durante el rearranque</title>
     <para>
      Si en el proceso se produce un reinicio del master de Salt, vuelva a ejecutar el comando para iniciar de nuevo el proceso de actualización de los minions de Salt.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Compruebe que AppArmor está inhabilitado y detenga todos los nodos después de la actualización:
    </para>
<screen><prompt>root # </prompt>systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     Después de la actualización, los gestores Ceph Manager aún no están instalados. Para que el estado del clúster sea el correcto, haga lo siguiente:
    </para>
    <substeps>
     <step>
      <para>
       Ejecute la fase 0 para habilitar la API REST de Salt:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       Ejecute la fase 1 para crear el subdirectorio <filename>role-mgr/</filename>:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       Edite el archivo <guimenu>policy.cfg</guimenu> como se describe en la <xref linkend="policy.configuration"/> y añada una función de Ceph Manager a los nodos a los que va a distribuir los monitores Ceph Monitor. Asimismo, puede añadir la función openATTIC a uno de los nodos del clúster. Consulte el <xref linkend="ceph.oa"/> para obtener más información.
      </para>
     </step>
     <step>
      <para>
       Ejecute la fase 2 para actualizar Pillar:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       DeepSea utiliza ahora un enfoque diferente para generar el archivo de configuración <filename>ceph.conf</filename>, consulte el <xref linkend="ds.custom.cephconf"/> para obtener más información.
      </para>
     </step>
     <step>
      <para>
       Ejecute la fase 3 para distribuir los gestores Ceph Manager:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       Ejecute la fase 4 para configurar openATTIC correctamente:
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>error de coincidencia de capacidades de clave de Ceph</title>
     <para>
      Si <literal>ceph.stage.3</literal> falla con el "Error EINVAL: entity client.bootstrap-osd exists but caps do not match" (Error EINVAL: la entidad client.bootstrap-osd existe, pero las capacidades de clave no coinciden), significa que las capacidades de clave (caps) de la clave <literal>client.bootstrap.osd</literal> del clúster existente no coinciden con las caps que DeepSea está intentando establecer. Encima del mensaje de error, en texto rojo, verá un volcado del comando <command>ceph auth</command> que ha fallado. Consulte en este comando el ID de clave y el archivo que se están utilizando. En el caso de <literal>client.bootstrap-osd</literal>, el comando será:
     </para>
<screen><prompt>root # </prompt>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      Para solucionar los errores de coincidencia de capacidades de clave, compruebe el contenido del archivo de anillo de claves que DeepSea está intentando distribuir; por ejemplo:
     </para>
<screen><prompt>cephadm &gt; </prompt>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Compárelo con el resultado de <command>ceph auth get client.bootstrap-osd</command>:
     </para>
<screen><prompt>root # </prompt>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      Fíjese en que falta la última clave <literal>caps mgr = "allow r"</literal>. Para solucionar el problema, ejecute:
     </para>
<screen><prompt>root # </prompt>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      Si se ejecuta <literal>ceph.stage.3</literal> ahora, el problema se debe haber solucionado.
     </para>
     <para>
      El mismo problema puede producirse con los anillos de claves del servidor de metadatos y de Object Gateway cuando se ejecuta <literal>ceph.stage.4</literal>. Se aplica el mismo procedimiento anterior: compruebe qué comando ha fallado, el archivo de anillo de claves que se está distribuyendo y las capacidades de la clave existente. A continuación, ejecute <command>ceph auth caps</command> para actualizar las caps de claves existentes a fin de que coincidan con las que DeepSea está distribuyendo.
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>fallo de actualización</title>
   <para>
    Si el clúster está en estado "HEALTH_ERR" durante más de 300 segundos o uno de los servicios para cada función asignada está inactivo durante más de 900 segundos, la actualización falla. En tal caso, busque el problema, resuélvalo y vuelva a ejecutar el procedimiento de actualización. Tenga en cuenta que en entornos virtualizados, los tiempos límite son más cortos.
   </para>
  </important>

  <important>
   <title>rearranque de los OSD</title>
   <para>
    Después de actualizar a SUSE Enterprise Storage 5, los OSD de FileStore necesitan aproximadamente cinco minutos más para iniciarse, ya que el OSD realizará una conversión una sola vez de sus archivos en disco.
   </para>
  </important>

  <tip>
   <title>comprobación de la versión de los componentes y nodos del clúster</title>
   <para>
    Si necesita averiguar las versiones de los componentes y nodos individuales del clúster (por ejemplo, para comprobar si todos los nodos se encuentran realmente en el mismo nivel de parches después de la actualización), puede ejecutar:
   </para>
<screen><prompt>root@master # </prompt>salt-run status.report</screen>
   <para>
    El comando recorre los minions de Salt conectados y busca los números de versión de Ceph, Salt y SUSE Linux Enterprise Server. A continuación, ofrece un informe donde se muestra la versión que tienen la mayoría de los nodos y también los nodos cuya versión es diferente a los de esa mayoría.
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>Migración de OSD a BlueStore</title>
   <para>
    OSD BlueStore es un nuevo procesador final para los daemons de OSD. Es la opción por defecto desde SUSE Enterprise Storage 5. En comparación con FileStore, que almacena los objetos como archivos en un sistema de archivos XFS, BlueStore puede ofrecer un mayor rendimiento debido a que almacena los objetos directamente en el dispositivo de bloques subyacente. BlueStore también permite otras funciones, como la compresión integrada y la sobrescritura de codificación de borrado, que no están disponibles con FileStore.
   </para>
   <para>
    Específicamente para BlueStore, un OSD dispone de un dispositivo "wal" (Write Ahead Log, registro de escritura predictiva) y un dispositivo "db" (base de datos RocksDB). La base de datos RocksDB almacena los metadatos de un OSD BlueStore. Estos dos dispositivos se encuentran por defecto en el mismo dispositivo que un OSD, pero cualquiera de ellos se puede colocar en un medio más rápido o en uno distinto.
   </para>
   <para>
    En SES5, se admite tanto FileStore como BlueStore y es posible que ambos sistemas coexistan en un mismo clúster. Durante el procedimiento de actualización de SUSE Enterprise Storage, los OSD de FileStore no se convierten automáticamente a BlueStore. Tenga en cuenta que las funciones específicas de BlueStore no estarán disponibles en los OSD que no se hayan migrado a BlueStore.
   </para>
   <para>
    Antes de convertirlos a BlueStore, los OSD deben disponer ya de SUSE Enterprise Storage en ejecución. La conversión es un proceso lento, ya que todos los datos se reescriben dos veces. Aunque el proceso de migración puede tardar mucho tiempo en completarse, no hay ninguna interrupción del clúster y todos los clientes pueden seguir accediendo a él durante ese período. No obstante, el rendimiento será inferior durante la migración. Esto es debido al reequilibrio de la carga y la reposición de datos del clúster.
   </para>
   <para>
    Utilice el procedimiento siguiente para migrar los OSD de FileStore a BlueStore:
   </para>
   <tip>
    <title>desactivación de las medidas de seguridad</title>
    <para>
     Los comandos de Salt necesarios para ejecutar la migración se bloquean por motivos de seguridad. Para desactivar estas medidas de seguridad, ejecute el siguiente comando:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      Migre los perfiles de hardware:
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.policy</screen>
     <para>
      Este servicio migra los perfiles de hardware que usa actualmente el archivo <filename>policy.cfg</filename>. Procesa <filename>policy.cfg</filename>, encuentra cualquier perfil de hardware que use la estructura de datos original y lo convierte a la nueva estructura de datos. El resultado es un perfil de hardware nuevo denominado "migrated-<replaceable>nombre_original</replaceable>". También se actualiza <filename>policy.cfg</filename>.
     </para>
     <para>
      Si la configuración original tenía diarios independientes, la configuración de BlueStore utilizará el mismo dispositivo para los dispositivos "wal" y "db" de ese OSD.
     </para>
    </step>
    <step>
     <para>
      DeepSea migra los OSD definiendo su peso en 0, lo que "vacía" los datos hasta que el OSD está vacío. Los OSD se pueden migrar uno por uno o todos a la vez. En cualquier caso, cuando el OSD está vacío, la organización lo elimina y lo vuelve a crear con la nueva configuración.
     </para>
     <tip>
      <title>método recomendado</title>
      <para>
       Use <command>ceph.migrate.nodes</command> si tiene un gran número de nodos de almacenamiento físicos o casi ningún dato. Si un nodo representa menos de 10 % de su capacidad, <command>ceph.migrate.nodes</command> puede ser ligeramente más rápido que mover todos los datos de esos OSD en paralelo.
      </para>
      <para>
       Si no tiene claro qué método debe utilizar, o si el sitio tiene menos nodos de almacenamiento (por ejemplo, cada nodo tiene más del 10 % de los datos del clúster), seleccione <command>ceph.migrate.osds</command>.
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        Para migrar los OSD a la vez, ejecute:
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        Para migrar todos los OSD de cada nodo en paralelo, ejecute:
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       Como la organización no aporta información sobre el progreso de la migración, utilice
      </para>
<screen><prompt>root # </prompt>ceph osd tree</screen>
      <para>
       para ver qué OSD tiene periódicamente un peso cero.
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    Después de la migración a BlueStore, el recuento de objetos seguirá siendo el mismo y el uso de disco será prácticamente igual.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>Actualización desde SUSE Enterprise Storage 4 (distribución de <command>ceph-deploy</command>) a la versión 5</title>

  <important>
   <title>requisitos de software</title>
   <para>
    Debe tener el siguiente software instalado y actualizado con las últimas versiones de los paquetes en todos los nodos de Ceph que desee actualizar antes de iniciar el procedimiento de actualización:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Seleccione al master de Salt para el clúster. Si el clúster tiene Calamari distribuido, el nodo de Calamari ya <emphasis>es</emphasis> el master de Salt. Como alternativa, el nodo de administración desde el que se ha ejecutado el comando <command>ceph-deploy</command> se convertirá en el master de Salt.
   </para>
   <para>
    Antes de iniciar el proceso siguiente, debe actualizar el nodo master de Salt a SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5 ejecutando <command>zypper migration</command> (o su método de actualización preferido).
   </para>
  </important>

  <para>
   Para actualizar el clúster de SUSE Enterprise Storage 4 que se ha distribuido con <command>ceph-deploy</command> a la versión 5, siga estos pasos:
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>Pasos que se deben aplicar a todos los nodos del clúster (incluido el nodo de Calamari)</title>
   <step>
    <para>
     Instale el paquete <systemitem>salt</systemitem> desde SLE-12-SP2/SES4:
    </para>
<screen><prompt>root # </prompt>zypper install salt</screen>
   </step>
   <step>
    <para>
     Instale el paquete <systemitem>salt-minion</systemitem> desde SLE-12-SP2/SES4 y habilite e inicie el servicio relacionado:
    </para>
<screen><prompt>root # </prompt>zypper install salt-minion
<prompt>root # </prompt>systemctl enable salt-minion
<prompt>root # </prompt>systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     Asegúrese de que el nombre de host "salt" se resuelve en la dirección IP del nodo master de Salt. Si el nombre de host <literal>salt</literal> no puede acceder al master de Salt, edite el archivo <filename>/etc/salt/minion</filename> o cree un archivo <filename>/etc/salt/minion.d/master.conf</filename> nuevo con el contenido siguiente:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      Los minions de Salt existentes tienen la opción <option>master:</option> ya definida en <filename>/etc/salt/minion.d/calamari.conf</filename>. No importa el nombre de archivo de configuración que se use, pero el directorio <filename>/etc/salt/minion.d/</filename> sí es importante.
     </para>
    </tip>
    <para>
     Si realiza cambios en los archivos de configuración mencionados anteriormente, reinicie el servicio Salt en todos los minions de Salt:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Si ha registrado los sistemas con SUSEConnect y usa el SCC/SMT, no necesita llevar a cabo ninguna otra acción.
      </para>
     </step>
     <step>
      <para>
       Si <emphasis role="bold">no</emphasis> usa el SCC/SMT, sino una imagen ISO de medios u otro origen de paquete, añada los siguientes repositorios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base y SES5 Update. Puede hacerlo con el comando <command>zypper</command>. Antes, elimine todos los repositorios de software existentes y, a continuación, añada los nuevos repositorios necesarios. Por último, actualice los orígenes de los repositorios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Pasos que se deben aplicar en el nodo master de Salt</title>
   <step>
    <para>
     Defina el nuevo orden de clasificación de objetos interno. Para ello, ejecute:
    </para>
<screen><prompt>root@master # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      Para verificar que el comando se ha ejecutado correctamente, se recomienda ejecutar:
     </para>
<screen><prompt>root@master # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Actualice el nodo master de Salt a SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5. Para sistemas registrados en el SCC, use <command>zypper migration</command>. Si proporciona manualmente los repositorios de software necesarios, use <command>zypper dup</command>. Después de la actualización, asegúrese de que solo los repositorios de SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5 estén activos (y actualizados) en el nodo master de Salt antes de continuar.
    </para>
   </step>
   <step>
    <para>
     Si no están ya presentes, instale el paquete <systemitem>salt-master</systemitem> y habilite e inicie el servicio relacionado:
    </para>
<screen><prompt>root@master # </prompt>zypper install salt-master
<prompt>root@master # </prompt>systemctl enable salt-master
<prompt>root@master # </prompt>systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Verifique que todos los minions de Salt están presentes mostrando sus claves:
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     Añada todas las claves de los minions de Salt al master de Salt, incluido el master de los minions:
    </para>
<screen><prompt>root@master # </prompt>salt-key -A -y</screen>
   </step>
   <step>
    <para>
     Asegúrese de que se han aceptado todas las claves de los minions de Salt:
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     Asegúrese de que el software del nodo master de Salt está actualizado:
    </para>
<screen><prompt>root@master # </prompt>zypper migration</screen>
   </step>
   <step>
    <para>
     Instale el paquete <systemitem>deepsea</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     Incluya los minions de Salt del clúster. Consulte la <xref linkend="ds.minion.targeting"/> de <xref linkend="ds.depl.stages"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     Importe el clúster instalado <command>ceph-deploy</command> existente:
    </para>
<screen><prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster</screen>
    <para>
     El comando hará lo siguiente:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Distribuir todos los módulos necesarios de Salt y DeepSea a todos los minions de Salt.
      </para>
     </listitem>
     <listitem>
      <para>
       Inspeccionar el clúster de Ceph en ejecución y completar <filename>/srv/pillar/ceph/proposals</filename> con un diseño del clúster.
      </para>
      <para>
       Se creará <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> con funciones que coincidan con todos los servicios de Ceph en ejecución detectados. Consulte este archivo para verificar que todos los nodos de Monitor, OSD, Object Gateway y servidor de metadatos existentes tienen las funciones adecuadas. Los nodos de OSD se importarán en el subdirectorio <filename>profile-import/</filename>, por lo que puede examinar los archivos de <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename> y <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename> para confirmar que los OSD se han recogido correctamente.
      </para>
      <note>
       <para>
        El archivo <filename>policy.cfg</filename> generado solo aplicará funciones a los servicios de Ceph detectados "role-mon", "role-mgr", "role-mds", "role-rgw", "role-admin" y "role-master" para el nodo master de Salt. Las otras funciones que desee deberán añadirse manualmente al archivo (consulte la <xref linkend="policy.role.assignment"/>).
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       El archivo <filename>ceph.conf</filename> del clúster existente se guardará en <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> incluirá las redes fsid, de clúster y pública del clúster, y también especificará la opción <option>configuration_init: default-import</option>, por lo que DeepSea usará el archivo de configuración <filename>ceph.conf.import</filename> mencionado anteriormente, en lugar de utilizar la plantilla por defecto de DeepSea <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>.
      </para>
      <note>
       <title>archivo <filename>ceph.conf</filename> personalizado</title>
       <para>
        Si necesita integrar el archivo <filename>ceph.conf</filename> con cambios personalizados, espere a que el proceso de actualización o importación finalice correctamente. A continuación, modifique el archivo <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> y convierta en comentario la línea siguiente:
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        Guarde el archivo y siga las instrucciones del <xref linkend="ds.custom.cephconf"/>.
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       Los distintos anillos de claves del clúster se guardan en los directorios siguientes:
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       Verifique que existen los archivos de anillos de claves y que <emphasis>no</emphasis> hay ningún archivo de anillo de claves en el siguiente directorio (Ceph Manager no existía en versiones anteriores a SUSE Enterprise Storage 5):
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     El comando <command>salt-run populate.engulf_existing_cluster</command> no gestiona la importación de la configuración de openATTIC. Tendrá que editar manualmente el archivo <filename>policy.cfg</filename> y añadir una línea <literal>role-openattic</literal>. Consulte la <xref linkend="policy.configuration"/> para obtener más información.
    </para>
   </step>

   <step>
    <para>
     El comando <command>salt-run populate.engulf_existing_cluster</command> no gestiona la importación de las configuraciones de iSCSI Gateway. Si el clúster incluye instancias de iSCSI Gateway, importe manualmente sus configuraciones:
    </para>
    <substeps>
     <step>
      <para>
       En uno de los nodos de iSCSI Gateway, exporte el archivo <filename>lrbd.conf</filename> actual y cópielo en el nodo master de Salt:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt;/tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       En el nodo master de Salt, añada la configuración por defecto de iSCSI Gateway en la configuración de DeepSea:
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Añada las funciones de iSCSI Gateway en <filename>policy.cfg</filename> y guarde el archivo:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Ejecute la fase 1 para crear todas las funciones posibles:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Genere los subdirectorios necesarios en <filename>/srv/pillar/ceph/stack</filename>:
    </para>
<screen><prompt>root@master # </prompt>salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     Verifique que hay un clúster gestionado por DeepSea en funcionamiento con las funciones asignadas correctamente:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     Compare el resultado con el diseño real del clúster.
    </para>
   </step>
   <step>
    <para>
     Calamari deja un trabajo de Salt programado en ejecución y comprueba el estado del clúster. Elimine el trabajo:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
   </step>
   <step>
    <para>
     A partir de este punto, siga el procedimiento que se describe en la <xref linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5crowbar">
  <title>Actualización desde SUSE Enterprise Storage 4 (distribución de Crowbar) a la versión 5</title>

  <important>
   <title>requisitos de software</title>
   <para>
    Debe tener el siguiente software instalado y actualizado con las últimas versiones de los paquetes en todos los nodos de Ceph que desee actualizar antes de iniciar el procedimiento de actualización:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Para actualizar la instancia de SUSE Enterprise Storage 4 distribuida mediante Crowbar a la versión 5, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Para cada nodo de Ceph (incluido el nodo de Calamari), detenga e inhabilite todos los servicios relacionados con Crowbar:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl stop chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_join
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     Para cada nodo de Ceph (incluido el nodo de Calamari), verifique que los repositorios de software señalan a los productos SUSE Enterprise Storage 5 y SUSE Linux Enterprise Server 12 SP3. Si todavía hay repositorios que señalen a versiones anteriores del producto, inhabilítelos.
    </para>
   </step>
   <step>
    <para>
     Para cada nodo de Ceph (incluido el nodo de Calamari), verifique que
     <package>salt-minion</package> está instalado. Si no lo está, instálelo:
    </para>
<screen><prompt>root@minion &gt; </prompt>sudo zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     Para los nodos de Ceph que no tengan el paquete <package>salt-minion</package>
     instalado, cree el archivo <filename>/etc/salt/minion.d/master.conf</filename> y haga que la opción <option>master</option> señale al nombre de host completo del nodo de Calamari:
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      Los minions de Salt existentes tienen la opción <option>master:</option> ya definida en <filename>/etc/salt/minion.d/calamari.conf</filename>. No importa el nombre de archivo de configuración que se use, pero el directorio <filename>/etc/salt/minion.d/</filename> sí es importante.
     </para>
    </tip>
    <para>
     Habilite e inicie el servicio <systemitem class="daemon">salt-minion</systemitem>:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl enable salt-minion
<prompt>root@minion &gt; </prompt>sudo systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     En el nodo de Calamari, acepte las claves de los minions de Salt restantes:
    </para>
<screen>
<prompt>root@master # </prompt>salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

<prompt>root@master # </prompt>salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     Si se ha distribuido Ceph en la red pública y no existe ninguna interfaz VLAN, añada una interfaz VLAN en la red pública de Crowbar al nodo de Calamari.
    </para>
   </step>
   <step>
    <para>
     Actualice el nodo de Calamari a SUSE Linux Enterprise Server 12 SP3 y SUSE Enterprise Storage 5, ya sea mediante <command>zypper migration</command> o con el método que prefiera. A partir de este punto, el nodo de Calamari se convierte en el <emphasis>master de Salt</emphasis>. Después de la actualización, reinicie el master de Salt.
    </para>
   </step>
   <step>
    <para>
     Instale DeepSea en el master de Salt:
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Especifique la opción <option>deepsea_minions</option> para incluir el grupo correcto de minions de Salt en fases de distribución. Consulte <xref linkend="ds.minion.targeting.dsminions"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     DeepSea espera que todos los nodos de Ceph tengan un archivo <filename>/etc/ceph/ceph.conf</filename> idéntico. Crowbar distribuye una versión ligeramente distinta de <filename>ceph.conf</filename> a cada nodo, por lo que deberá consolidarlas:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Elimine la opción <option>osd crush location hook</option> que incluyó Calamari.
      </para>
     </listitem>
     <listitem>
      <para>
       Elimine la opción <option>public addr</option> de la sección <literal>[mon]</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Elimine los números de puerto de la opción <option>mon host</option>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Si se estaba ejecutando Object Gateway, Crowbar habrá distribuido un archivo <filename>/etc/ceph/ceph.conf.radosgw</filename> independiente para mantener los secretos de Keystone separados del archivo <filename>ceph.conf</filename> normal. Crowbar también habrá añadido un archivo <filename>/etc/systemd/system/ceph-radosgw@.service</filename> personalizado. Dado que DeepSea no lo admite, deberá eliminarlo:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Añada al final de todas las secciones <literal>[client.rgw...]</literal> del archivo <filename>ceph.conf.radosgw</filename> a <filename>/etc/ceph/ceph.conf</filename> en todos los nodos.
      </para>
     </listitem>
     <listitem>
      <para>
       En el nodo de Object Gateway, ejecute lo siguiente:
      </para>
<screen><prompt>root@minion &gt; </prompt>rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Asegúrese de que <command>ceph status</command> funciona cuando se ejecuta desde el master de Salt:
    </para>
<screen><prompt>root@master # </prompt>ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     Importe el clúster existente:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run push.proposal
</screen>
   </step>

   <step>
    <para>
     El comando <command>salt-run populate.engulf_existing_cluster</command> no gestiona la importación de las configuraciones de iSCSI Gateway. Si el clúster incluye instancias de iSCSI Gateway, importe manualmente sus configuraciones:
    </para>
    <substeps>
     <step>
      <para>
       En uno de los nodos de iSCSI Gateway, exporte el archivo <filename>lrbd.conf</filename> actual y cópielo en el nodo master de Salt:
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt; /tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       En el nodo master de Salt, añada la configuración por defecto de iSCSI Gateway en la configuración de DeepSea:
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       Añada las funciones de iSCSI Gateway en <filename>policy.cfg</filename> y guarde el archivo:
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       Si ha registrado los sistemas con SUSEConnect y usa el SCC/SMT, no necesita llevar a cabo ninguna otra acción.
      </para>
     </step>
     <step>
      <para>
       Si <emphasis role="bold">no</emphasis> usa el SCC/SMT, sino una imagen ISO de medios u otro origen de paquete, añada los siguientes repositorios manualmente: SLE12-SP3 Base, SLE12-SP3 Update, SES5 Base y SES5 Update. Puede hacerlo con el comando <command>zypper</command>. Antes, elimine todos los repositorios de software existentes y, a continuación, añada los nuevos repositorios necesarios. Por último, actualice los orígenes de los repositorios:
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       Cambie los datos de Pillar para utilizar una estrategia diferente. Editar
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       Y añada la línea siguiente:
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        La estrategia <literal>zypper-dup</literal> requiere que se añadan manualmente los repositorios de software más recientes, mientras que la opción por defecto <literal>zypper-migration</literal> se basa en los repositorios proporcionados por el SCC/SMT.
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Arregle los elementos grain del host para hacer que DeepSea utilice nombres de host cortos en la red pública para los ID de instancia del daemon de Ceph. Para cada nodo, debe ejecutar <command>grains.set</command> con el nombre de host (corto) nuevo. Antes de ejecutar <command>grains.set</command>, verifique las instancias del monitor actual ejecutando <command>ceph status</command>. Se muestra un ejemplo del antes y el después:
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
<prompt>root@master # </prompt>salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
<prompt>root@master # </prompt>salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
<prompt>root@master # </prompt>salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     Ejecute la actualización:
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version
<prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     Se reiniciarán todos los nodos. El clúster se volverá a activar con una advertencia de que no hay ninguna instancia de Ceph Manager activa. Esto es normal. Calamari no debe estar instalado ni en ejecución ya en este punto.
    </para>
   </step>
   <step>
    <para>
     Ejecute todas las fases de distribución necesarias para hacer que el clúster tenga un estado correcto:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     Para distribuir openATTIC (consulte el <xref linkend="ceph.oa"/>), añada una línea <literal>role-openattic</literal> adecuada (consulte la <xref linkend="policy.role.assignment"/>) a <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> y ejecute:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     Durante la actualización, es posible que reciba errores de tipo "Error EINVAL: entity [...] exists but caps do not match" (Error EINVAL: la entidad [...] existe pero las caps no coinciden). Para solucionarlos, consulte la <xref linkend="ceph.upgrade.4to5"/>.
    </para>
   </step>
   <step>
    <para>
     Lleve a cabo la limpieza restante:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Crowbar crea entradas en <filename>/etc/fstab</filename> para cada OSD. No son necesarias, así que suprímalas.
      </para>
     </listitem>
     <listitem>
      <para>
       Calamari deja un trabajo de Salt programado en ejecución y comprueba el estado del clúster. Elimine el trabajo:
      </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       Aún hay algunos paquetes innecesarios instalados, principalmente gemas de ruby y relacionados con chef. Su eliminación no es obligatoria, pero puede hacerlo ejecutando <command>zypper rm <replaceable>nombre_paquete</replaceable></command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>Actualización desde SUSE Enterprise Storage 3 a la versión 5</title>

  <important>
   <title>requisitos de software</title>
   <para>
    Debe tener el siguiente software instalado y actualizado con las últimas versiones de los paquetes en todos los nodos de Ceph que desee actualizar antes de iniciar el procedimiento de actualización:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Para actualizar el clúster de SUSE Enterprise Storage 3 a la versión 5, siga los pasos descritos en el <xref linkend="upgrade4to5cephdeploy.all"/> y, a continuación, los descritos en el <xref linkend="upgrade4to5cephdeploy.admin"/>.
  </para>
 </sect1>
</chapter>
