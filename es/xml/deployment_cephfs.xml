<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha.ceph.as.cephfs">

 <title>Instalación de CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editar</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sí</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  El sistema de archivos de Ceph (CephFS) es un sistema de archivos conforme con POSIX que utiliza un clúster de almacenamiento de Ceph para almacenar sus datos. CephFS utiliza el mismo sistema de clúster para los dispositivos de bloques de Ceph, el almacenamiento de objetos de Ceph con sus API S3 y Swift o los enlaces nativos (<systemitem>librados</systemitem>).
 </para>
 <para>
  Para utilizar CephFS, debe disponer de un clúster de almacenamiento de Ceph en ejecución y al menos un <emphasis>servidor de metadatos de Ceph</emphasis> en ejecución.
 </para>
 <sect1 xml:id="ceph.cephfs.limitations">
  <title>Escenarios admitidos de CephFS y directrices</title>

  <para>
   Con SUSE Enterprise Storage, SUSE incluye compatibilidad oficial con muchos escenarios en los que se utiliza el componente de ampliación horizontal y distribuido CephFS. En esta entrada se describen los límites rígidos y se ofrecen directrices para los casos de uso sugeridos.
  </para>

  <para>
   Una distribución de CephFS compatible debe cumplir estos requisitos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Un mínimo de un servidor de metadatos. SUSE recomienda distribuir varios nodos con la función de servidor de metadatos. Solo uno estará "activo", mientras el resto estarán "pasivos". No olvide mencionar todos los nodos del servidor de metadatos en el comando <command>mount</command> cuando monte CephFS desde un cliente.
    </para>
   </listitem>
   <listitem>
    <para>
     Las instantáneas de CephFS están inhabilitadas (por defecto) y no se admiten en esta versión.
    </para>
   </listitem>
   <listitem>
    <para>
     Los clientes se basan en SUSE Linux Enterprise Server 12 SP2 o SP3 y usan el controlador de módulo de kernel <literal>cephfs</literal>. No se admite el módulo FUSE.
    </para>
   </listitem>
   <listitem>
    <para>
     Las cuotas de CephFS no se admiten en SUSE Enterprise Storage, ya que la compatibilidad con las cuotas se implementa únicamente en el cliente de FUSE.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS admite cambios de diseño de archivos, como se describe en <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/>. Sin embargo, aunque el sistema de archivos se puede montar mediante cualquier cliente, no es posible añadir nuevos repositorios de datos a un sistema de archivos CephFS existente (<literal>ceph mds add_data_pool</literal>). Solo se pueden añadir mientras el sistema de archivos está desmontado.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>Servidor de metadatos de Ceph</title>

  <para>
   El servidor de metadatos de Ceph (MDS) almacena los metadatos para CephFS. Los dispositivos de bloques de Ceph y el almacenamiento de objetos de Ceph <emphasis>no</emphasis> utilizan un servidor de metadatos. Gracias a los servidores de metadatos, los usuarios del sistema de archivos POSIX pueden ejecutar comandos básicos, como <command>ls</command> o <command>find</command>, sin que ello suponga una carga de trabajo enorme al clúster de almacenamiento de Ceph.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adición de un servidor de metadatos</title>
   <para>
    Es posible distribuir el servidor de metadatos durante el proceso de distribución inicial del clúster, tal como se describe en la <xref linkend="ceph.install.stack"/>, o añadirlos a un clúster ya distribuido como se describe en el <xref linkend="salt.adding.nodes"/>.
   </para>
   <para>
    Después de distribuir el servidor de metadatos, permita el servicio <literal>Ceph OSD/MDS</literal> en la configuración del cortafuegos del servidor donde se vaya a distribuir dicho servidor: inicie <literal>yast</literal>, acceda a <menuchoice> <guimenu>Seguridad y usuarios</guimenu> <guimenu>Cortafuegos</guimenu> <guimenu>Servicios autorizados</guimenu> </menuchoice> y, en el menú desplegable <guimenu>Servicio que se va a autorizar</guimenu> seleccione <guimenu>Ceph OSD/MDS</guimenu>. Si no se permite el tráfico completo del nodo del servidor de metadatos de Ceph, el montaje de un sistema de archivos falla, aunque otras operaciones pueden funcionar correctamente.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mds.config">
   <title>Configuración de un servidor de metadatos</title>
   <para>
    Puede ajustar con más detalle el comportamiento del servidor de metadatos insertando las opciones relevantes en el archivo de configuración <filename>ceph.conf</filename>.
   </para>
   <variablelist>
    <title>Tamaño de caché del servidor de metadatos</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       La cantidad máxima de memoria (en bytes) que el servidor de metadatos aplicará para su caché. Los administradores deben utilizar este valor en lugar del antiguo ajuste <option>mds cache size</option>. Por defecto es 1 GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       La reserva de caché (memoria o inodos) que debe conservar el caché del servidor de metadatos. Cuando el servidor de metadatos empieza a utilizar su reserva, retendrá temporalmente el estado del cliente hasta que el tamaño de caché se reduzca para restaurar la reserva. El valor por defecto es 0,05.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para obtener una lista detallada de opciones de configuración relacionadas con el servidor de metadatos, consulte <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    Para obtener una lista detallada de opciones de configuración del creador de diarios del servidor de metadatos, consulte <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
  <title>CephFS</title>

  <para>
   Si dispone de un clúster de almacenamiento de Ceph en buen estado con al menos un servidor de metadatos de Ceph, puede crear y montar el sistema de archivos de Ceph. Asegúrese de que su cliente cuenta con conectividad de red y de un anillo de claves de autenticación adecuado.
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>Creación de CephFS</title>
   <para>
    CephFS requiere al menos dos repositorios RADOS: uno para <emphasis>datos</emphasis> y otro para <emphasis>metadatos</emphasis>. A la hora de configurar estos repositorios, tenga en cuenta lo siguiente:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Use un nivel de réplica mayor para el repositorio de metadatos, ya que cualquier pérdida de datos en este repositorio puede producir que no sea posible acceder al sistema de archivos completo.
     </para>
    </listitem>
    <listitem>
     <para>
      Use un almacenamiento de baja latencia, como discos SSD, para el repositorio de metadatos, ya que así mejorará la latencia observada de las operaciones del sistema de archivos en los clientes.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Los repositorios necesarios se crean automáticamente asignando una entrada <literal>role-mds</literal> en el archivo <filename>policy.cfg</filename>. Puede crear manualmente los repositorios <literal>cephfs_data</literal> y <literal>cephfs_metadata</literal> para ajustar el rendimiento de forma manual antes de configurar el servidor de metadatos. DeepSea no creará estos repositorios si ya existen.
   </para>
   <para>
    Para obtener más información sobre cómo gestionar repositorios, consulte el <xref linkend="ceph.pools"/>.
   </para>
   <para>
    Para crear los dos repositorios obligatorios (por ejemplo, "cephfs_data" y "cephfs_metadata") con los valores por defecto para usarlos con CephFS, ejecute los comandos siguientes:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    Es posible utilizar repositorios codificados de borrado en lugar de los repositorios replicados. Se recomienda usar los repositorios codificados de borrado solo si se requiere un rendimiento bajo y acceso aleatorio poco frecuente; por ejemplo, para el almacenamiento en frío, las copias de seguridad y el archivado. En los repositorios codificados de borrado, CephFS requiere que BlueStore esté habilitado y el repositorio debe tener la opción <literal>allow_ec_overwrite</literal> definida. Esta opción puede definirse ejecutando <command>ceph osd pool set ec_pool allow_ec_overwrites true</command>.
   </para>
   <para>
    La codificación de borrado añade una sobrecarga considerable a las operaciones del sistema de archivos, especialmente en pequeñas actualizaciones. Esta sobrecarga es inherente al uso de la codificación de borrado como mecanismo de tolerancia a fallos. Este problema es un inconveniente necesario si se quiere reducir de forma significativa la sobrecarga del espacio de almacenamiento.
   </para>
   <para>
    Cuando se crean los repositorios, puede habilitar el sistema de archivos con el comando <command>ceph fs new</command>:
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    Para comprobar que se ha creado el sistema de archivos, puede mostrar todos los sistemas de archivos CephFS disponibles:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Cuando se haya creado el sistema de archivos, el servidor de metadatos podrá entrar en un estado <emphasis>activo</emphasis>. Por ejemplo, en un único sistema de servidor de metadatos:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>otros temas</title>
    <para>
     Encontrará más información sobre tareas específicas; por ejemplo, el montaje, el desmontaje y la configuración avanzada de CephFS, en el <xref linkend="cha.ceph.cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.cephfs.multimds">
   <title>Tamaño del clúster del servidor de metadatos</title>
   <para>
    Varios daemons activos de servidor de metadatos pueden dar servicio a una instancia de CephFS. Todos los daemons activos del servidor de metadatos que se asignan a una instancia de CephFS distribuirán el árbol del directorio del sistema de archivos entre sí y, por lo tanto, distribuirán la carga de los clientes simultáneos. Para poder añadir un daemon activo del servidor de metadatos a una instancia de CephFS, se necesita una reserva de repuesto. Es posible iniciar un daemon adicional o utilizar una instancia de reserva existente.
   </para>
   <para>
    El comando siguiente muestra el número actual de daemons del servidor de metadatos activos y pasivos.
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    El siguiente comando define el número de servidores de metadatos activos a dos en una instancia del sistema de archivos.
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Con el fin de reducir el tamaño del clúster del servidor de metadatos antes de una actualización, es preciso llevar a cabo dos pasos. Primero, defina <option>max_mds</option> para que solo quede una instancia:
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    y, después, desactive de forma explícita los demás daemons activos del servidor de metadatos:
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    donde <replaceable>rank</replaceable> es el número de un daemon activo del servidor de metadatos de una instancia del sistema de archivos, entre 0 y <option>max_mds</option>-1. Consulte <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/> para obtener más información.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.multimds_updates">
   <title>Clúster de servidor de metadatos y actualizaciones</title>
   <para>
    Durante las actualizaciones de Ceph, los indicadores de función de una instancia del sistema de archivos pueden cambiar (normalmente, al añadir nuevas funciones). Los daemons incompatibles (por ejemplo, de versiones anteriores) no funcionan con un conjunto de funciones incompatible y no se iniciarán. Esto significa que actualizar y reiniciar un daemon pueden provocar que todos los otros daemons que aún no se han actualizado se detengan y no se puedan iniciar. Por este motivo, se recomienda reducir el clúster activo del servidor de metadatos a una sola instancia y detener todos los daemons en espera antes de actualizar Ceph. Los pasos manuales para este procedimiento de actualización son los siguientes:
   </para>
   <procedure>
    <step>
     <para>
      Actualice los paquetes relacionados con Ceph mediante <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Reduzca el tamaño del clúster activo del servidor de metadatos como se describe anteriormente a una sola instancia y detenga todos los daemons en espera del servidor de metadatos con sus unidades <systemitem class="daemon">systemd</systemitem> en todos los otros nodos:
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Solo entonces, reinicie el único daemon del servidor de metadatos de los que quedan, de forma que se reinicie con archivo binario actualizado.
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Reinicie todos los demás daemons del servidor de metadatos y vuelva a definir el valor de <option>max_mds</option> que desee.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Si utiliza DeepSea, se seguirá este procedimiento en caso de que el paquete
    <package>ceph</package> se haya actualizado durante las fase de la 0 a la 4. Es posible llevar a cabo este procedimiento mientras los clientes tienen la instancia de CephFS montada y hay operaciones de E/S en curso. Sin embargo, tenga en cuenta que habrá una breve pausa de E/S mientras se reinicie el servidor de metadatos activo. Los clientes se recuperarán automáticamente.
   </para>
   <para>
    Es recomendable reducir la carga de E/S tanto como sea posible antes de actualizar un clúster del servidor de metadatos. Este procedimiento es más rápido en los clústeres del servidor de metadatos inactivos. Por el contrario, en un clúster con mucha carga con varios daemons del servidor de metadatos es fundamental reducir la carga de antemano para evitar que un solo daemon del servidor de metadatos se vea desbordado por operaciones de E/S continuas.
   </para>
  </sect2>
 </sect1>
</chapter>
