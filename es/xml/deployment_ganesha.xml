<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha.as.ganesha">

 <title>Instalación de NFS Ganesha</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>editar</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>sí</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  NFS Ganesha proporciona acceso NFS para Object Gateway o CephFS. En SUSE Enterprise Storage 5, se admiten las versiones 3 y 4 de NFS. NFS Ganesha se ejecuta en el espacio del usuario, en lugar de en el espacio del kernel, e interactúa directamente con Object Gateway o CephFS.
 </para>
 <sect1 xml:id="sec.as.ganesha.preparation">
  <title>Preparación</title>

  <sect2 xml:id="sec.as.ganesha.preparation.general">
   <title>Información general</title>
   <para>
    Para distribuir correctamente NFS Ganesha, debe añadir <literal>role-ganesha</literal> al archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Para obtener información, consulte: <xref linkend="policy.configuration"/>. NFS Ganesha también necesita que <literal>role-rgw</literal> o <literal>role-mds</literal> estén presentes en <filename>policy.cfg</filename>.
   </para>
   <para>
    Aunque es posible instalar y ejecutar el servidor de NFS Ganesha en un nodo de Ceph ya existente, se recomienda ejecutarlo en un host dedicado con acceso al clúster de Ceph. Los hosts del cliente por lo general no forman parte del clúster, pero deben tener acceso de red al servidor de NFS Ganesha.
   </para>
   <para>
    Para habilitar el servidor de NFS Ganesha en cualquier momento tras la instalación inicial, añada <literal>role-ganesha</literal> a <filename>policy.cfg</filename> y vuelva a ejecutar al menos las fases 2 y 4 de DeepSea. Para obtener información, consulte: <xref linkend="ceph.install.stack"/>.
   </para>
   <para>
    NFS Ganesha se configura mediante el archivo <filename>/etc/ganesha/ganesha.conf</filename>, presente en el nodo de NFS Ganesha. Sin embargo, este archivo se sobrescribe cada vez que se ejecuta la fase 4 de DeepSea. Por lo tanto, se recomienda editar la plantilla que utiliza Salt, que es el archivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> del master de Salt. Para obtener información sobre el archivo de configuración, consulte el <xref linkend="ceph.nfsganesha.config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.preparation.requirements">
   <title>Resumen de requisitos</title>
   <para>
    Los siguientes requisitos deben cumplirse antes de ejecutar las fases 2 y 4 de DeepSea a fin de instalar NFS Ganesha:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Debe haber al menos un nodo asignado a <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Solo puede definir un <literal>role-ganesha</literal> por minion.
     </para>
    </listitem>
    <listitem>
     <para>
      Para funcionar, NFS Ganesha necesita una instancia de Object Gateway o CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Si se espera que NFS Ganesha utilice Object Gateway como interfaz con el clúster, se debe completar el archivo <filename>/srv/pillar/ceph/rgw.sls</filename> en el master de Salt.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.basic_example">
  <title>Instalación de ejemplo</title>

  <para>
   Este procedimiento proporciona una instalación de ejemplo que usa tanto Object Gateway como capas de abstracción del sistema de archivos (FSAL) de CephFS de NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Si aún no lo ha hecho, ejecute las fases 0 y 1 de DeepSea antes de continuar con este procedimiento.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Después de ejecutar la fase 1 de DeepSea, edite el archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> y añada la línea:
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Sustituya <replaceable>NODENAME</replaceable> con el nombre de un nodo del clúster.
    </para>
    <para>
     Asegúrese también de que hay asignados un <literal>role-mds</literal> y un <literal>role-rgw</literal>.
    </para>
   </step>
   <step>
    <para>
     Cree el archivo <filename>/srv/pillar/ceph/rgw.sls</filename> e inserte el contenido siguiente:
    </para>
<screen>rgw_configurations:
  rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
      - { uid: "demo1", name: "Demo1", email: "demo1@demo.nil" }</screen>
    <para>
     Estos usuarios se crean más adelante como usuarios de Object Gateway y se generan claves de API. En el nodo de Object Gateway, puede ejecutar más tarde <command>radosgw-admin user list</command> para mostrar todos los usuarios creados y <command>radosgw-admin user info --uid=demo</command> para obtener información acerca de usuarios individuales.
    </para>
    <para>
     DeepSea garantiza que Object Gateway y NFS Ganesha reciben las credenciales de todos los usuarios mostrados en la sección <literal>rgw</literal> del archivo <filename>rgw.sls</filename>.
    </para>
    <para>
     El NFS exportado utiliza los nombres de usuario en el primer nivel del sistema de archivos; en este ejemplo, las vías <filename>/demo</filename> y <filename>/demo1</filename> se exportarán.
    </para>
   </step>
   <step>
    <para>
     Ejecute al menos las fases 2 y 4 de DeepSea. Se recomienda ejecutar la fase 3 en medio.
    </para>
<screen><prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verifique que NFS Ganesha funciona montando el recurso compartido NFS desde un nodo de cliente:
    </para>
<screen><prompt>root # </prompt><command>mount</command> -o sync -t nfs <replaceable>GANESHA_NODE</replaceable>:/ /mnt
<prompt>root # </prompt><command>ls</command> /mnt
cephfs  demo  demo1</screen>
    <para>
     <filename>/mnt</filename> debe contener todas las vías exportadas. Deben existir directorios para CephFS y para ambos usuarios de Object Gateway. Para cada depósito que posea un usuario, se exportará una vía <filename>/mnt/<replaceable>NOMBREUSUARIO</replaceable>/<replaceable>NOMBREDEPÓSITO</replaceable></filename>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.ha-ap">
  <title>Configuración activa-pasiva de alta disponibilidad</title>

  <para>
   En esta sección se proporciona un ejemplo de cómo establecer una configuración activa-pasiva de dos nodos de los servidores de NFS Ganesha. El programa de instalación requiere SUSE Linux Enterprise High Availability Extension. Los dos nodos se denominan <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem>.
  </para>

  <para>
   Para obtener información sobre SUSE Linux Enterprise High Availability Extension, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>.
  </para>

  <sect2 xml:id="sec.as.ganesha.ha-ap.basic">
   <title>Instalación básica</title>
   <para>
    En esta configuración <systemitem class="domainname">earth</systemitem> tiene la dirección IP <systemitem class="ipaddress">192.168.1.1</systemitem> y <systemitem class="domainname">mars</systemitem> tiene la dirección <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Asimismo, se usan dos direcciones IP virtuales flotantes que permiten a los clientes conectarse al servicio independientemente del nodo físico en el que se estén ejecutando. <systemitem class="ipaddress">192.168.1.10</systemitem> se usa para la administración del clúster con Hawk2 y <systemitem class="ipaddress">192.168.2.1</systemitem> se usa exclusivamente para las exportaciones NFS. De esta forma es más fácil aplicar más tarde restricciones de seguridad.
   </para>
   <para>
    El procedimiento siguiente describe la instalación de ejemplo. Encontrará más información en <link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/>.
   </para>
   <procedure xml:id="proc.as.ganesha.ha-ap">
    <step>
     <para>
      Prepare los nodos de NFS Ganesha en el master de Salt:
     </para>
     <substeps>
      <step>
       <para>
        Ejecute las fases 0 y 1 de DeepSea en el master de Salt.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Asigne a los nodos <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem> la función <literal>role-ganesha</literal> en el archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Ejecute las fases 3 y 4 de DeepSea en el master de Salt.
       </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registre SUSE Linux Enterprise High Availability Extension en <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Instalación <package>ha-cluster-bootstrap</package> en ambos nodos:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Inicialice el clúster en <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Deje que <systemitem class="domainname">mars</systemitem> se una al clúster:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Compruebe el estado del clúster. Debería observar que se han añadido dos nodos al clúster:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      En ambos nodos, inhabilite el inicio automático del servicio NFS Ganesha durante el arranque:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Inicie la shell <command>crm</command> en <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Los comandos siguientes se ejecutan en la shell crm.
     </para>
    </step>
    <step>
     <para>
      En <systemitem class="domainname">earth</systemitem>, ejecute la shell crm para ejecutar los comandos siguientes a fin de configurar el recurso para los daemons de NFS Ganesha como clones de tipo de recurso systemd:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Cree una IPAddr2 primitiva con la shell crm:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Para configurar una relación entre el servidor de NFS Ganesha y la dirección IP virtual flotante, utilice la colocación y el orden.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilice el comando <command>mount</command> desde el cliente para asegurarse de que la configuración del clúster está completa:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.ha-ap.cleanup">
   <title>Limpieza de recursos</title>
   <para>
    En caso de que se produzca un fallo en NFS Ganesha en uno de los nodos, por ejemplo en <systemitem class="domainname">earth</systemitem>, solucione el problema y limpie el recurso. Solo después de que el recurso se haya limpiado, el recurso podrá recuperarse tras el fallo en <systemitem class="domainname">earth</systemitem>, en caso de que NFS Ganesha falle en <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Para limpiar el recurso:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec.as.ganesha.ha-ap.ping-resource">
   <title>Configuración del recurso de Ping</title>
   <para>
    Puede suceder que el servidor no pueda acceder al cliente debido a un problema de red. Un recurso de ping puede detectar y solucionar este problema. La configuración de este recurso es opcional.
   </para>
   <procedure>
    <step>
     <para>
      Defina el recurso de ping:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> es una lista de direcciones IP separadas por caracteres de espacio. Se hará ping a las direcciones IP con regularidad para comprobar si se producen interrupciones de la red. Si un cliente debe tener acceso continuo al servidor de NFS, añádalo a <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Cree un clon:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      El siguiente comando crea una restricción para el servicio NFS Ganesha. Fuerza al servicio a moverse a otro nodo cuando no sea posible acceder a <literal>host_list</literal>.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha_ha_deepsea">
   <title>Alta disponibilidad de NFS Ganesha y DeepSea</title>
   <para>
    DeepSea no admite la configuración de alta disponibilidad de NFS Ganesha. Para evitar que DeepSea falle después de configurar la alta disponibilidad de NFS Ganesha, excluya el inicio y la detención del servicio NFS Ganesha de la fase 4 de DeepSea:
   </para>
   <procedure>
    <step>
     <para>
      Copie <filename>/srv/salt/ceph/ganesha/default.sls</filename> en <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Elimine la entrada <literal>.service</literal> de <filename>/srv/salt/ceph/ganesha/ha.sls</filename> de forma que quede como sigue:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Añada la línea siguiente a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.as.ganesha.info">
  <title>Más información</title>

  <para>
   Encontrará más información en <xref linkend="cha.ceph.nfsganesha"/>.
  </para>
 </sect1>
</chapter>
