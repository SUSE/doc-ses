<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Instalación de NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editar</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sí</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha proporciona acceso NFS para Object Gateway o CephFS. En SUSE Enterprise Storage 6, se admiten las versiones 3 y 4 de NFS. NFS Ganesha se ejecuta en el espacio del usuario, en lugar de en el espacio del kernel, e interactúa directamente con Object Gateway o CephFS.
 </para>
 <warning>
  <title>acceso con varios protocolos</title>
  <para>
   Los clientes nativos de CephFS y NFS no están restringidos por los bloqueos de archivos obtenidos a través de Samba, y viceversa. Las aplicaciones que se basan en el bloqueo de archivos con varios protocolos pueden sufrir daños en los datos si se accede a las vías compartidas de Samba respaldadas por CephFS a través de otros medios.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparación</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Información general</title>
   <para>
    Para distribuir correctamente NFS Ganesha, debe añadir <literal>role-ganesha</literal> al archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Para obtener información, consulte: <xref linkend="policy-configuration"/>. NFS Ganesha también necesita que <literal>role-rgw</literal> o <literal>role-mds</literal> estén presentes en <filename>policy.cfg</filename>.
   </para>
   <para>
    Aunque es posible instalar y ejecutar el servidor de NFS Ganesha en un nodo de Ceph ya existente, se recomienda ejecutarlo en un host dedicado con acceso al clúster de Ceph. Los hosts del cliente por lo general no forman parte del clúster, pero deben tener acceso de red al servidor de NFS Ganesha.
   </para>
   <para>
    Para habilitar el servidor de NFS Ganesha en cualquier momento tras la instalación inicial, añada <literal>role-ganesha</literal> a <filename>policy.cfg</filename> y vuelva a ejecutar al menos las fases 2 y 4 de DeepSea. Para obtener información, consulte: <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha se configura mediante el archivo <filename>/etc/ganesha/ganesha.conf</filename>, presente en el nodo de NFS Ganesha. Sin embargo, este archivo se sobrescribe cada vez que se ejecuta la fase 4 de DeepSea. Por lo tanto, se recomienda editar la plantilla que utiliza Salt, que es el archivo <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> del master de Salt. Para obtener información sobre el archivo de configuración, consulte el <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Resumen de requisitos</title>
   <para>
    Los siguientes requisitos deben cumplirse antes de ejecutar las fases 2 y 4 de DeepSea a fin de instalar NFS Ganesha:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Debe haber al menos un nodo asignado a <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Solo puede definir un <literal>role-ganesha</literal> por minion.
     </para>
    </listitem>
    <listitem>
     <para>
      Para funcionar, NFS Ganesha necesita una instancia de Object Gateway o CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      El protocolo NFS basado en kernel debe inhabilitarse en los minions con la función <literal>role-ganesha</literal>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Instalación de ejemplo</title>

  <para>
   Este procedimiento proporciona una instalación de ejemplo que usa tanto Object Gateway como capas de abstracción del sistema de archivos (FSAL) de CephFS de NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Si aún no lo ha hecho, ejecute las fases 0 y 1 de DeepSea antes de continuar con este procedimiento.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Después de ejecutar la fase 1 de DeepSea, edite el archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> y añada la línea:
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Sustituya <replaceable>NODENAME</replaceable> con el nombre de un nodo del clúster.
    </para>
    <para>
     Asegúrese también de que hay asignados un <literal>role-mds</literal> y un <literal>role-rgw</literal>.
    </para>
   </step>
   <step>
    <para>
     Ejecute al menos las fases 2 y 4 de DeepSea. Se recomienda ejecutar la fase 3 en medio.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Verifique que NFS Ganesha funciona comprobando que el servicio NFS Ganesha se está ejecutando en el nodo de minion:
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configuración activa-pasiva de alta disponibilidad</title>

  <para>
   En esta sección se proporciona un ejemplo de cómo establecer una configuración activa-pasiva de dos nodos de los servidores de NFS Ganesha. El programa de instalación requiere SUSE Linux Enterprise High Availability Extension. Los dos nodos se denominan <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem>.
  </para>

  <important>
   <title>coubicación de servicios</title>
   <para>
    Los servicios que tienen su propia tolerancia a errores y su propio equilibrio de carga no deben ejecutarse en nodos de clúster que se aíslan para los servicios de failover. Por lo tanto, no ejecute los servicios de Ceph Monitor, el servidor de metadatos, iSCSI ni Ceph OSD en configuraciones de alta disponibilidad.
   </para>
  </important>

  <para>
   Para obtener información sobre SUSE Linux Enterprise High Availability Extension, consulte <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Instalación básica</title>
   <para>
    En esta configuración <systemitem class="domainname">earth</systemitem> tiene la dirección IP <systemitem class="ipaddress">192.168.1.1</systemitem> y <systemitem class="domainname">mars</systemitem> tiene la dirección <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Asimismo, se usan dos direcciones IP virtuales flotantes que permiten a los clientes conectarse al servicio independientemente del nodo físico en el que se estén ejecutando. <systemitem class="ipaddress">192.168.1.10</systemitem> se usa para la administración del clúster con Hawk2 y <systemitem class="ipaddress">192.168.2.1</systemitem> se usa exclusivamente para las exportaciones NFS. De esta forma es más fácil aplicar más tarde restricciones de seguridad.
   </para>
   <para>
    El procedimiento siguiente describe la instalación de ejemplo. Encontrará más información en <link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Prepare los nodos de NFS Ganesha en el master de Salt:
     </para>
     <substeps>
      <step>
       <para>
        Ejecute las fases 0 y 1 de DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Asigne a los nodos <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem> la función <literal>role-ganesha</literal> en el archivo <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Ejecute las fases 2 a 4 de DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registre SUSE Linux Enterprise High Availability Extension en <systemitem class="domainname">earth</systemitem> y <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Instale <package>ha-cluster-bootstrap</package> en ambos nodos:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Inicialice el clúster en <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Deje que <systemitem class="domainname">mars</systemitem> se una al clúster:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Compruebe el estado del clúster. Debería observar que se han añadido dos nodos al clúster:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      En ambos nodos, inhabilite el inicio automático del servicio NFS Ganesha durante el arranque:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Inicie la shell <command>crm</command> en <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Los comandos siguientes se ejecutan en la shell crm.
     </para>
    </step>
    <step>
     <para>
      En <systemitem class="domainname">earth</systemitem>, ejecute la shell crm para ejecutar los comandos siguientes a fin de configurar el recurso para los daemons de NFS Ganesha como clones de tipo de recurso systemd:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Cree una IPAddr2 primitiva con la shell crm:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Para configurar una relación entre el servidor de NFS Ganesha y la dirección IP virtual flotante, utilice la colocación y el orden.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilice el comando <command>mount</command> desde el cliente para asegurarse de que la configuración del clúster está completa:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Limpieza de recursos</title>
   <para>
    En caso de que se produzca un fallo en NFS Ganesha en uno de los nodos, por ejemplo en <systemitem class="domainname">earth</systemitem>, solucione el problema y limpie el recurso. Solo después de que el recurso se haya limpiado, el recurso podrá recuperarse tras el fallo en <systemitem class="domainname">earth</systemitem>, en caso de que NFS Ganesha falle en <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Para limpiar el recurso:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Configuración del recurso de Ping</title>
   <para>
    Puede suceder que el servidor no pueda acceder al cliente debido a un problema de red. Un recurso de ping puede detectar y solucionar este problema. La configuración de este recurso es opcional.
   </para>
   <procedure>
    <step>
     <para>
      Defina el recurso de ping:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> es una lista de direcciones IP separadas por caracteres de espacio. Se hará ping a las direcciones IP con regularidad para comprobar si se producen interrupciones de la red. Si un cliente debe tener acceso continuo al servidor de NFS, añádalo a <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Cree un clon:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      El siguiente comando crea una restricción para el servicio NFS Ganesha. Fuerza al servicio a moverse a otro nodo cuando no sea posible acceder a <literal>host_list</literal>.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>Alta disponibilidad de NFS Ganesha y DeepSea</title>
   <para>
    DeepSea no admite la configuración de alta disponibilidad de NFS Ganesha. Para evitar que DeepSea falle después de configurar la alta disponibilidad de NFS Ganesha, excluya el inicio y la detención del servicio NFS Ganesha de la fase 4 de DeepSea:
   </para>
   <procedure>
    <step>
     <para>
      Copie <filename>/srv/salt/ceph/ganesha/default.sls</filename> en <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Elimine la entrada <literal>.service</literal> de <filename>/srv/salt/ceph/ganesha/ha.sls</filename> de forma que quede como sigue:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Añada la línea siguiente a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Configuración activa-activa</title>

  <para>
   Esta sección proporciona un ejemplo de configuración activa-activa sencilla de NFS Ganesha. El objetivo es distribuir dos servidores de NFS Ganesha en capas sobre el mismo CephFS existente. Los servidores serán dos nodos de clúster de Ceph con direcciones independientes. Los clientes deben distribuirse entre ellos manualmente. En esta configuración, <quote>failover</quote> significa desmontar y volver a montar manualmente el otro servidor en el cliente.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Requisitos previos</title>
   <para>
    Para nuestra configuración de ejemplo, necesita lo siguiente:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      El clúster de Ceph en ejecución. Consulte la <xref linkend="ceph-install-stack"/> para obtener más información sobre la distribución y configuración del clúster de Ceph mediante DeepSea.
     </para>
    </listitem>
    <listitem>
     <para>
      Al menos un CephFS configurado. Consulte el <xref linkend="cha-ceph-as-cephfs"/> para obtener más información sobre la distribución y configuración de CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Dos nodos de clúster de Ceph con NFS Ganesha distribuido. Consulte el <xref linkend="cha-as-ganesha"/> para obtener más información sobre la distribución de NFS Ganesha.
     </para>
     <tip>
      <title>uso de servidores dedicados</title>
      <para>
       Aunque los nodos de NFS Ganesha pueden compartir recursos con otros servicios relacionados con Ceph, se recomienda usar servidores dedicados para mejorar el rendimiento.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    Después de distribuir los nodos de NFS Ganesha, verifique que el clúster está operativo y que los repositorios CephFS por defecto están allí:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Configuración de NFS Ganesha</title>
   <para>
    Compruebe que ambos nodos de NFS Ganesha tengan instalado el archivo <filename>/etc/ganesha/ganesha.conf</filename>. Añada los bloques siguientes, si aún no existen, al archivo de configuración para habilitar RADOS como back-end de recuperación de NFS Ganesha.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   Puede averiguar los valores para <replaceable>rados_pool</replaceable> y <replaceable>pool_namespace</replaceable> comprobando la línea ya existente en la configuración del formulario:</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   El valor para la opción <replaceable>nodeid</replaceable> corresponde al nombre completo de la máquina. Los valores de las opciones <replaceable>UserId</replaceable> y <replaceable>Ceph_Conf</replaceable> se pueden encontrar en el bloque <replaceable>RADOS_URLS</replaceable> ya existente.
   </para>
   <para>
    Debido a que las versiones heredadas de NFS impiden levantar el período de gracia antes de tiempo y, por lo tanto, prolongan el reinicio del servidor, se han inhabilitado las opciones correspondientes a versiones de NFS anteriores a la 4.2. También se ha inhabilitado la mayor parte del almacenamiento en caché de NFS Ganesha, ya que las bibliotecas de Ceph ya realizan un almacenamiento en caché agresivo.
   </para>
   <para>
    El back-end de recuperación "rados_cluster" almacena su información en objetos RADOS. Aunque no es una gran cantidad de datos, queremos que tenga alta disponibilidad. Con este objetivo se usa el repositorio de metadatos de cephFS y se declara en él un nuevo espacio de nombres "ganesha" para mantenerlo diferenciado de los objetos CephFS.
   </para>
   <note>
    <title>IDs de nodo de clúster</title>
    <para>
     La mayor parte de la configuración es idéntica entre los dos hosts, sin embargo, la opción <option>nodeid</option> del bloque "RADOS_KV" debe ser una cadena exclusiva para cada nodo. Por defecto, NFS Ganesha define en <option>nodeid</option> el nombre de host del nodo.
    </para>
    <para>
     Si necesita utilizar valores fijos diferentes a los nombres de host, puede por ejemplo definir <option>nodeid = 'a'</option> en un nodo y <option>nodeid = 'b'</option> en el otro.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Relleno de la base de datos de gracia del clúster</title>
   <para>
    Necesitamos verificar que todos los nodos del clúster son conscientes de los demás. Eso se hace mediante un objeto RADOS que se comparte entre los hosts. NFS Ganesha utiliza este objeto para comunicar el estado actual en relación a un período de gracia.
   </para>
   <para>
    El paquete <package>nfs-ganesha-rados-grace</package> contiene una herramienta de línea de comandos para consultar y manipular esta base de datos. Si el paquete no está instalado en al menos uno de los nodos, instálelo con:
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    El comando se usa para crear la base de datos y añadir ambos <option>nodeid</option>. En nuestro ejemplo, los dos nodos NFS Ganesha se denominan <literal>ses6min1.example.com</literal> y <literal>ses6min2.example.com</literal>. En uno de los hosts de NFS Ganesha, ejecute:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    Esto crea la base de datos de gracia y le añade "ses6min1.example.com" y "ses6min2.example.com". El último comando vuelca el estado actual. Los hosts recién añadidos siempre se consideran que aplican el período de gracia, por lo que ambos tienen el indicador "E" definido. Los valores "cur" y "rec" muestran las épocas actuales y de recuperación, que es cómo se realiza un seguimiento de qué hosts pueden realizar la recuperación y cuándo.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Reinicio de servicios de NFS Ganesha</title>
   <para>
    En ambos nodos de NFS Ganesha, reinicie los servicios relacionados:
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    Después de reiniciar los servicios, compruebe la base de datos de gracia:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>se ha borrado el indicador "E"</title>
    <para>
     Tenga en cuenta que en ambos nodos se ha borrado el indicador "E", lo que indica que ya no están aplicando el período de gracia y ahora están en modo de funcionamiento normal.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Conclusión</title>
   <para>
    Después de completar todos los pasos anteriores, puede montar el NFS exportado desde cualquiera de los dos servidores de NFS Ganesha y realizar operaciones NFS normales en ellos.
   </para>
   <para>
    En la configuración de ejemplo se supone que si uno de los dos servidores de NFS Ganesha se apaga, se reiniciará manualmente en 5 minutos. Después de 5 minutos, el servidor de metadatos puede cancelar la sesión del cliente de NFS Ganesha y todo el estado asociado a él. Si las capacidades de la sesión se cancelan antes de que el resto del clúster entre en el período de gracia, es posible que los clientes del servidor no puedan recuperar todo su estado.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>Más información</title>

  <para>
   Encontrará más información en <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
