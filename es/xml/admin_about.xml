<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha-storage-about">
 <title>SUSE Enterprise Storage 6 y Ceph</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editar</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sí</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SUSE Enterprise Storage 6 es un sistema de almacenamiento distribuido basado en la tecnología Ceph diseñado para que tenga capacidad de ampliación, sea fiable y aporte alto rendimiento. Los clústeres de Ceph se pueden ejecutar en servidores básicos en una red común como Ethernet. Es posible ampliar fácilmente el clúster hasta miles de servidores (a partir de ahora denominados nodos) y almacenar petabytes de información. A diferencia de los sistemas convencionales que cuentan con tablas de asignación para almacenar y recuperar datos, Ceph utiliza un algoritmo determinista para asignar el almacenamiento de datos y no tiene ninguna estructura de información centralizada. Ceph entiende que en los clústeres de almacenamiento la adición o eliminación de hardware es la norma, no la excepción. El clúster de Ceph automatiza las tareas de gestión, como la distribución, redistribución y réplica de datos; la detección de fallos y la recuperación. Ceph se autorrepara y se autoadministra, con lo que se consigue una reducción de las tareas administrativas y del presupuesto necesario.
 </para>
 <para>
  Este capítulo proporciona una visión general de SUSE Enterprise Storage 6 y describe brevemente los componentes más importantes.
 </para>
 <tip>
  <para>
   A partir de SUSE Enterprise Storage 5, el único método de distribución de clústeres admitido es DeepSea. Consulte el <xref linkend="ceph-install-saltstack"/> para obtener más información sobre el proceso de distribución.
  </para>
 </tip>
 <sect1 xml:id="storage-intro-features">
  <title>Características de Ceph</title>

  <para>
   El entorno Ceph incluye las siguientes características:
  </para>

  <variablelist>
   <varlistentry>
    <term>Capacidad de ampliación</term>
    <listitem>
     <para>
      Ceph puede ampliarse a miles de nodos y gestionar petabytes de almacenamiento.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hardware básico</term>
    <listitem>
     <para>
      Para ejecutar un clúster de Ceph no se requiere ningún hardware especial. Para obtener información, consulte el <xref linkend="storage-bp-hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Autoadministración</term>
    <listitem>
     <para>
      El clúster de Ceph se gestiona automáticamente. Cuando se añaden o se eliminan nodos, o cuando estos fallan, el clúster redistribuye los datos automáticamente. También es consciente de los discos sobrecargados.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Sin un punto único de error</term>
    <listitem>
     <para>
      Ningún nodo de un clúster almacena información importante por sí solo. El número de redundancias se puede configurar.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Software de código abierto</term>
    <listitem>
     <para>
      Ceph es una solución de software de código abierto e independiente de hardware o proveedores específicos.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-intro-core">
  <title>Componentes básicos</title>

  <para>
   Para aprovechar al máximo las ventajas de Ceph, es necesario comprender algunos de sus componentes y conceptos básicos. Esta sección presenta algunas partes de Ceph a las que se hace referencia con frecuencia en otros capítulos.
  </para>

  <sect2 xml:id="storage-intro-core-rados">
   <title>RADOS</title>
   <para>
    El componente básico de Ceph se denomina <emphasis>RADOS</emphasis> <emphasis>(Reliable Autonomic Distributed Object Store, almacén de objetos distribuido autónomo fiable)</emphasis>. Es el encargado de gestionar los datos almacenados en el clúster. Los datos de Ceph se almacenan normalmente como objetos. Cada objeto se compone de un identificador y datos.
   </para>
   <para>
    RADOS proporciona los siguientes métodos de acceso para los objetos almacenados que abarcan muchos casos de uso:
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway es una pasarela REST HTTP para el almacén de objetos RADOS. Permite el acceso directo a los objetos almacenados en el clúster de Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Dispositivo de bloques RADOS</term>
     <listitem>
      <para>
       Es posible acceder a los dispositivos de bloques RADOS (RBD) igual que a cualquier otro dispositivo de bloques. Por ejemplo, se pueden usar en combinación con <systemitem class="library">libvirt</systemitem> con fines de virtualización.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       El sistema de archivos de Ceph tiene conformidad con POSIX.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem></term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> es una biblioteca que se puede utilizar con varios lenguajes de programación a fin de crear una aplicación capaz de interactuar directamente con el clúster de almacenamiento.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> se utiliza en Object Gateway y RBD, mientras que CephFS interactúa directamente con RADOS (<xref linkend="storage-intro-core-rados-figure"/>).
   </para>
   <figure xml:id="storage-intro-core-rados-figure">
    <title>Interfaces con el almacén de objetos de Ceph</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage-intro-core-crush">
   <title>CRUSH</title>
   <para>
    En el núcleo de un clúster de Ceph se encuentra el algoritmo <emphasis>CRUSH</emphasis>. CRUSH es el acrónimo de <emphasis>Controlled Replication Under Scalable Hashing</emphasis> (réplica controlada bajo hash escalable). Es una función que gestiona la asignación del almacenamiento y, en comparación con otros algoritmos, necesita pocos parámetros. Es decir, que solo necesita una pequeña cantidad de información para calcular la posición de almacenamiento de un objeto. Los parámetros son un mapa actual del clúster, incluido el estado de actividad, algunas reglas de colocación definidas por el administrador y el nombre del objeto que se va a almacenar o recuperar. Con esta información, todos los nodos del clúster de Ceph pueden calcular dónde está almacenado un objeto y sus réplicas. De esta forma, la escritura o lectura de los datos se realiza de forma muy eficaz. CRUSH intenta distribuir homogéneamente los datos por todos los nodos del clúster.
   </para>
   <para>
    El <emphasis>mapa de CRUSH</emphasis> contiene todos los nodos de almacenamiento y las reglas de colocación definidas por el administrador para almacenar los objetos en el clúster. Define una estructura jerárquica que se suele corresponder con la estructura física del clúster. Por ejemplo, los discos que contienen los datos están en hosts, los hosts están en bastidores, los bastidores en filas y las filas en centros de datos. Esta estructura se puede utilizar para definir <emphasis>dominios de fallo</emphasis>. Ceph se asegura de que las réplicas se almacenan en diferentes ramas de un dominio de fallo concreto.
   </para>
   <para>
    Si el dominio de fallo se define en el bastidor, las réplicas de los objetos se distribuyen en distintos bastidores. Esto puede mitigar las interrupciones causadas por un conmutador que falle en un bastidor. Si una unidad de distribución de alimentación da energía a una fila de bastidores, el dominio de fallo se puede definir en la fila. Cuando se produce un fallo en la unidad de distribución de alimentación, los datos replicados siguen disponibles en las demás filas.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-core-nodes">
   <title>Nodos y daemons de Ceph</title>
   <para>
    En Ceph, los nodos son servidores que trabajan para el clúster. Pueden ejecutar distintos tipos de daemons. Se recomienda ejecutar solo un tipo de daemon en cada nodo, excepto los daemons de Ceph Manager, que se pueden colocar junto con los monitores Ceph Monitor. Cada clúster requiere al menos los daemons de Ceph Monitor, Ceph Manager y Ceph OSD:
   </para>
   <variablelist>
    <varlistentry>
     <term>Nodo de administración</term>
     <listitem>
      <para>
       El <emphasis>nodo de administración</emphasis> es un nodo de clúster de Ceph donde se ejecuta el servicio master de Salt. El nodo de administración es un punto central del clúster de Ceph porque gestiona el resto de los nodos del clúster consultando y dando instrucciones a los servicios minion de Salt. Normalmente también incluye otros servicios como la interfaz de usuario Web de Ceph Dashboard con la consola <emphasis>Grafana</emphasis> respaldada por el kit de herramientas de supervisión <emphasis>Prometheus</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       Los nodos de <emphasis>Ceph Monitor</emphasis> (a menudo abreviado como <emphasis>MON</emphasis>) guardan información sobre el estado de la actividad del clúster: un mapa de todos los nodos y las reglas de distribución de los datos (consulte la <xref linkend="storage-intro-core-crush"/>).
      </para>
      <para>
       Si se producen fallos o conflictos, los nodos de Ceph Monitor del clúster deciden por mayoría qué información es la correcta. Para formar una mayoría cualificada, se recomienda tener un número impar de nodos de Ceph Monitor, y tres como mínimo.
      </para>
      <para>
       Si se utiliza más de un sitio, los nodos de Ceph Monitor deben distribuirse en un número impar de sitios. El número de nodos de Ceph Monitor por sitio debe ser superior al 50 % de los nodos de Ceph Monitor que permanecen operativos si se produce un fallo en un sitio.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       Ceph Manager (MGR) recopila la información de estado de todo el clúster. El daemon de Ceph Manager se ejecuta junto con los daemons de monitor. Proporciona supervisión adicional y sirve de interfaz entre los sistemas de supervisión y gestión externos.
      </para>
      <para>
       Ceph Manager no requiere configuración adicional, aparte de asegurarse de que está en ejecución. Se puede distribuir como una función independiente con DeepSea.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       Un <emphasis>Ceph OSD</emphasis> es un daemon que gestiona <emphasis>dispositivos de almacenamiento de objetos</emphasis>, que son unidades de almacenamiento físicas o lógicas (discos duros o particiones). Los dispositivos de almacenamiento de objetos pueden ser discos físicos/particiones o volúmenes lógicos. El daemon se encarga también de la réplica de datos y del reequilibrio de la carga en caso de que se añadan o se eliminen nodos.
      </para>
      <para>
       Los daemons Ceph OSD se comunican con los daemons de monitor y les proporcionan el estado de los demás daemons de OSD.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para utilizar CephFS, Object Gateway, NFS Ganesha o iSCSI Gateway se necesitan nodos adicionales:
   </para>
   <variablelist>
    <varlistentry>
     <term>Servidor de metadatos (MDS)</term>
     <listitem>
      <para>
       Los servidores de metadatos almacenan metadatos para CephFS. Mediante un servidor de metadatos es posible ejecutar comandos básicos del sistema de archivos como <command>ls</command> sin sobrecargar el clúster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway es una pasarela REST HTTP para el almacén de objetos RADOS. Es compatible con OpenStack Swift y Amazon S3 y tiene su propia de gestión de usuarios.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       NFS Ganesha proporciona acceso NFS para Object Gateway o CephFS. Se ejecuta en el espacio del usuario, en lugar de en el espacio del kernel, e interactúa directamente con Object Gateway o CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI Gateway</term>
     <listitem>
      <para>
       iSCSI es un protocolo de red de almacenamiento que permite a los clientes enviar comandos SCSI a dispositivos de almacenamiento SCSI (destinos) en servidores remotos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Samba Gateway</term>
     <listitem>
      <para>
       Samba Gateway proporciona un acceso SAMBA a los datos almacenados en CephFS.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-intro-structure">
  <title>Estructura de almacenamiento</title>

  <sect2 xml:id="storage-intro-structure-pool">
   <title>Repositorio</title>
   <para>
    Los objetos que se almacenan en un clúster de Ceph se colocan en <emphasis>repositorios</emphasis>. Los repositorios representan particiones lógicas del clúster hacia el mundo exterior. Por cada repositorio, es posible definir un conjunto de reglas; por ejemplo, cuántas réplicas de cada objeto deben existir. La configuración estándar de los repositorios se denomina <emphasis>repositorio replicado</emphasis>.
   </para>
   <para>
    Los repositorios suelen contener objetos, pero también pueden configurarse para actuar como si fueran un sistema RAID 5. En esta configuración, los objetos se almacenan en porciones junto con porciones de código adicionales. Las porciones de código contienen información redundante. El administrador puede definir el número de datos y de porciones de código. En esta configuración, los repositorios se denominan <emphasis>repositorios codificados de borrado</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-pg">
   <title>Grupo de colocación</title>
   <para>
    Los <emphasis>grupos de colocación</emphasis> (PG) se utilizan para la distribución de datos dentro de un repositorio. Cuando se crea un repositorio, se define un número determinado de grupos de colocación. Los grupos de colocación se utilizan de modo interno para agrupar objetos y son un factor importante para el rendimiento de un clúster de Ceph. El grupo de colocación para un objeto se determina según el nombre del objeto.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-example">
   <title>Ejemplo</title>
   <para>
    Esta sección proporciona un ejemplo de cómo gestiona Ceph los datos (consulte la <xref linkend="storage-intro-structure-example-figure"/>). El ejemplo no representa una configuración recomendada de un clúster de Ceph. El hardware está formado por tres nodos de almacenamiento o tres Ceph OSD (<literal>Host 1</literal>, <literal>Host 2</literal> y <literal>Host 3</literal>). Cada nodo tiene tres discos duros que se utilizan como OSD (<literal>osd.1</literal> a <literal>osd.9</literal>). Los nodos de Ceph Monitor no se tratan en este ejemplo.
   </para>
   <note>
    <title>diferencia entre Ceph OSD y OSD</title>
    <para>
     <emphasis>Ceph OSD</emphasis> o <emphasis>daemon Ceph OSD</emphasis> hace referencia a un daemon que se ejecuta en un nodo, mientras que el término <emphasis>OSD</emphasis> hace referencia al disco lógico con el que interactúa el daemon.
    </para>
   </note>
   <para>
    El clúster tiene dos repositorios, <literal>Repositorio A</literal> y <literal>Repositorio B</literal>. Mientras Repositorio A replica objetos solo dos veces, la capacidad de recuperación de Repositorio B es más importante y tiene tres réplicas para cada objeto.
   </para>
   <para>
    Cuando una aplicación coloca un objeto en un repositorio, por ejemplo, mediante la API REST, se selecciona un grupo de colocación (<literal>PG1</literal> a <literal>PG4</literal>) según el repositorio y el nombre del objeto. El algoritmo CRUSH calcula en qué OSD se almacena el objeto, según el grupo de colocación que contiene el objeto.
   </para>
   <para>
    En este ejemplo, el dominio de fallo está definido en el host. Esto garantiza que las réplicas de los objetos se almacenan en distintos hosts. Según el nivel de réplica que se defina para un repositorio, el objeto se almacenará en dos o en tres de los OSD que usa el grupo de colocación.
   </para>
   <para>
    Una aplicación que escribe un objeto solo interactúa con un Ceph OSD: el Ceph OSD primario. El Ceph OSD primario se encarga de la réplica y confirma que el proceso de escritura ha terminado después de que todos los demás OSD hayan almacenado el objeto.
   </para>
   <para>
    Si falla <literal>osd.5</literal>, todos los objetos de <literal>PG1</literal> siguen estando disponibles en <literal>osd.1</literal>. En cuanto el clúster reconoce que un OSD ha fallado, otro OSD toma su lugar. En este ejemplo, <literal>osd.4</literal> se utiliza como sustituto de <literal>osd.5</literal>. Los objetos almacenados en <literal>osd.1</literal> se replican a <literal>osd.4</literal> para restaurar el nivel de réplica.
   </para>
   <figure xml:id="storage-intro-structure-example-figure">
    <title>Ejemplo de Ceph a pequeña escala</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Si se añade un nuevo nodo con nuevos OSD al clúster, el mapa del clúster cambia. La función CRUSH devuelve ubicaciones diferentes para los objetos. los objetos que reciben las nuevas ubicaciones se reubican. Este proceso da como resultado un uso equilibrado de todos los OSD.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about-bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore es el procesador final de almacenamiento por defecto para Ceph desde SUSE Enterprise Storage 5. Su rendimiento es mejor que el de FireStore y cuenta con suma de comprobación de los datos completos y compresión integrada.
  </para>

  <para>
   BlueStore puede gestionar uno, dos o tres dispositivos de almacenamiento. En el caso más sencillo, BlueStore consume un único dispositivo de almacenamiento primario. Normalmente, el dispositivo de almacenamiento se divide en dos partes:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Una pequeña partición denominada BlueFS que implementa las funciones de sistema de archivos requeridas por RocksDB.
    </para>
   </listitem>
   <listitem>
    <para>
     El resto del dispositivo suele estar ocupado por una partición de gran tamaño. Se gestiona directamente mediante BlueStore y contiene todos los datos reales. Este dispositivo primario normalmente se identifica mediante un enlace simbólico de bloque en el directorio de datos.
    </para>
   </listitem>
  </orderedlist>

  <para>
   También es posible distribuir BlueStore en dos dispositivos adicionales:
  </para>

  <para>
   Un <emphasis>dispositivo WAL</emphasis> que se puede usar para el diario interno o el registro de escritura anticipada de BlueStore. Se identifica mediante el enlace simbólico <literal>block.wal</literal> en el directorio de datos. Solo resulta útil emplear un dispositivo WAL independiente si el dispositivo es más rápido que el dispositivo primario o que el dispositivo DB, por ejemplo en estos casos:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Si el dispositivo WAL es un NVMe, el dispositivo DB es una unidad SSD y el dispositivo de datos es una unidad SSD o un disco duro.
    </para>
   </listitem>
   <listitem>
    <para>
     Los dispositivos WAL y DB están en unidades SSD independientes, y el dispositivo de datos está en un SSD o un disco duro.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Es posible utilizar un <emphasis>dispositivo DB</emphasis> para almacenar los metadatos internos de BlueStore. BlueStore (o en su lugar, la instancia incrustada de RocksDB) colocará todos los metadatos que pueda en el dispositivo DB para mejorar el rendimiento. De nuevo, provisionar un dispositivo DB compartido solo resulta útil si es más rápido que el dispositivo primario.
  </para>

  <tip>
   <title>planificación del tamaño del dispositivo DB</title>
   <para>
    Debe realizar una planificación exhaustiva para garantizar un tamaño suficiente al dispositivo DB. Si el dispositivo DB se llena, los metadatos se diseminarán por todo el dispositivo primario, lo que afectará muy negativamente al rendimiento del OSD.
   </para>
   <para>
    Puede comprobar si una partición WAL/BD se está llenando y desbordándose con el comando <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command>. El valor <option>slow_used_bytes</option> muestra la cantidad de datos que se han desbordado:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage-moreinfo">
  <title>Información adicional</title>

  <itemizedlist>
   <listitem>
    <para>
     Ceph es un proyecto comunitario que cuenta con su propia documentación en línea completa. Para obtener información sobre temas que no encuentre en este manual, consulte <link xlink:href="http://docs.ceph.com/docs/master/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     La publicación original <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> (CRUSH: colocación controlada, ampliable y descentralizada de datos replicados) de <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller y C. Maltzahn</emphasis> proporciona información útil sobre el funcionamiento interno de Ceph. Se recomienda su lectura sobre todo al distribuir clústeres a gran escala. Encontrará la publicación en <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
   <listitem>
     <para>
       SUSE Enterprise Storage se puede utilizar con distribuciones OpenStack que no sean de SUSE. Los clientes de Ceph deben estar en un nivel que sea compatible con SUSE Enterprise Storage.
     </para>
     <note>
       <para>
         SUSE admite el componente de servidor de la distribución de Ceph y el proveedor de distribución de OpenStack debe admitir el cliente.
       </para>
     </note>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
