<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage.salt.cluster">
 <title>Saltクラスタの管理</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>○</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Cephクラスタの展開後、いくつか変更を行わなければならない場合があります。新しいノード、ディスク、またはサービスの追加や削除などです。この章では、これらの管理タスクを実行する方法について説明します。
 </para>
 <sect1 xml:id="salt.adding.nodes">
  <title>新しいクラスタノードの追加</title>

  <para>
   クラスタに新しいノードを追加する手順は、<xref linkend="ceph.install.saltstack"/>で説明されているクラスタノードの初期展開手順とほぼ同じです。
  </para>

  <procedure>
   <step>
    <para>
     SUSE Linux Enterprise Server 12 SP3を新しいノードにインストールし、Salt Masterのホスト名が正しく解決されるようにネットワークを設定してから、<systemitem>salt-minion</systemitem>パッケージをインストールします。
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Salt Masterのホスト名が<literal>salt</literal>と異なる場合は、<filename>/etc/salt/minion</filename>を編集して次の内容を追加します。
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     上で説明した設定ファイルを変更した場合は、<systemitem>salt.minion</systemitem>サービスを再起動します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Salt Master上のすべてのSaltキーを受諾します。
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>で新しいSalt Minionもターゲットに設定されていることを確認します。詳細については、<xref linkend="ds.depl.stages"/>の<xref linkend="ds.minion.targeting.name"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     準備ステージを実行します。モジュールとグレインが同期され、新しいMinionがDeepSeaに必要なすべての情報を提供できるようになります。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
   </step>
   <step>
    <para>
     ディスカバリステージを実行します。<filename>/srv/pillar/ceph/proposals</filename>ディレクトリに新しいファイルエントリが書き込まれます。このディレクトリで、関連する.ymlファイルを編集できます。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     オプションで、新しく追加したホストが既存の命名スキームに一致しない場合は、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を変更します。詳細については、<xref linkend="policy.configuration"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     設定ステージを実行します。<filename>/srv/pillar/ceph</filename>にあるすべてのファイルが読み込まれ、それに従ってPillarが更新されます。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillarに保存されているデータには次のコマンドでアクセスできます。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.items</screen>
   </step>
   <step>
    <para>
     設定および展開のステージで、新しく追加したノードを組み込みます。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.adding.services">
  <title>ノードへの新しい役割の追加</title>

  <para>
   DeepSeaを使用して、サポートされているすべてのタイプの役割を展開できます。サポートされている役割のタイプの詳細と、それらに合う例については、<xref linkend="policy.role.assignment"/>を参照してください。
  </para>

  <tip>
   <title>必須およびオプションの役割とステージ</title>
   <para>
    一般的に、クラスタノードに新しい役割を追加する場合、0～5のすべての展開ステージを実行することをお勧めします。時間を若干節約するには、展開予定の役割のタイプに応じてステージ3～4をスキップできます。OSDおよびMONの役割にはコアサービスが含まれるためCephで必要ですが、Object Gatewayなどその他の役割はオプションです。DeepSeaの展開ステージは階層状になっており、ステージ3はコアサービスを展開し、ステージ4はオプションのサービスを展開します。
   </para>
   <para>
    したがって、既存のOSDノードにMONなどのコアの役割を展開する場合は、ステージ3を実行する必要があり、ステージ4はスキップできます。
   </para>
   <para>
    同様に、Object Gatewayなどのオプションのサービスを展開する場合、ステージ3はスキップできますが、ステージ4は実行する必要があります。
   </para>
  </tip>

  <para>
   既存のノードに新しいサービスを追加するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     既存のホストが新しい役割に一致するように、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を変更します。詳細については、<xref linkend="policy.configuration"/>を参照してください。たとえば、MONノードでObject Gatewayを実行する必要がある場合、次のような行になります。
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     ステージ2を実行してPillarを更新します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     ステージ3を実行してコアサービスを展開するか、ステージ4を実行してオプションのサービスを展開します。両方のステージを実行しても問題はありません。
    </para>
   </step>
  </procedure>

  <tip>
   <para>
    既存のクラスタにOSDを追加すると、後でしばらくの時間、クラスタがリバランスを実行することに注意してください。リバランス期間を最小限に抑えるには、追加予定のOSDをすべて同時に追加することをお勧めします。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="salt.node.removing">
  <title>クラスタノードの削除と再インストール</title>

  <para>
   クラスタから役割を削除するには、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を編集して、対応する行を削除します。その後、<xref linkend="ceph.install.stack"/>の説明に従ってステージ2と5を実行します。
  </para>

  <note>
   <title>クラスタからのOSDの削除</title>
   <para>
    クラスタから特定のOSDノードを削除する必要がある場合、削除予定のディスクよりも多くのディスク領域がクラスタにあることを確認してください。OSDを削除すると、クラスタ全体のリバランスが発生することに注意してください。
   </para>
  </note>

  <para>
   Minionから役割を削除する場合、その役割に関連する変更をすべて元に戻すことが目的です。ほとんどの役割では、これは簡単なタスクですが、パッケージ依存関係の問題が発生する可能性があります。パッケージをアンインストールしても、その依存関係は削除されません。
  </para>

  <para>
   削除されたOSDは空のドライブとして表示されます。関連するタスクによってファイルシステムの先頭が上書きされ、パーティションテーブルが消去されるほか、バックアップパーティションも削除されます。
  </para>

  <note>
   <title>他の方法で作成されたパーティションの維持</title>
   <para>
    以前に他の方法(<command>ceph-deploy</command>など)で設定されたディスクドライブには、引き続きパーティションが含まれています。DeepSeaはこれらを自動的には破棄しません。管理者がこれらのドライブを解放する必要があります。
   </para>
  </note>

  <example xml:id="ex.ds.rmnode">
   <title>クラスタからのSalt Minionの削除</title>
   <para>
    ストレージMinionに「data1.ceph」「data2.ceph」...「data6.ceph」のように名前が付けられている場合、<filename>policy.cfg</filename>の関連する行は次のようになります。
   </para>
<screen>[...]
# Hardware Profile
profile-default/cluster/data*.sls
profile-default/stack/default/ceph/minions/data*.yml
[...]</screen>
   <para>
    この場合にSalt Minion「data2.ceph」を削除するには、これらの行を次のように変更します。
   </para>
<screen>
[...]
# Hardware Profile
profile-default/cluster/data[1,3-6]*.sls
profile-default/stack/default/ceph/minions/data[1,3-6]*.yml
[...]</screen>
   <para>
    その後、ステージ2と5を実行します。
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex.ds.mignode">
   <title>ノードの移行</title>
   <para>
    次のような状況を想定します。クラスタの新規インストール中に、ゲートウェイ用のハードウェアが到着するまでの間、管理者がストレージノードの1つをスタンドアロンのObject Gatewayとして割り当てました。ゲートウェイ用の常設ハードウェアが到着したので、ようやく目的の役割をバックアップストレージノードに割り当てて、ゲートウェイの役割を削除できます。
   </para>
   <para>
    新しいハードウェアに対してステージ0と1 (<xref linkend="ds.depl.stages"/>を参照)を実行した後、この新しいゲートウェイに<literal>rgw1</literal>という名前を付けました。ノード<literal>data8</literal>でObject Gatewayの役割を削除し、ストレージの役割を追加する必要があります。現在の<filename>policy.cfg</filename>は次のようになっています。
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-7]*.sls
profile-default/stack/default/ceph/minions/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    これを次のように変更します。
   </para>
<screen># Hardware Profile
profile-default/cluster/data[1-8]*.sls
profile-default/stack/default/ceph/minions/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    ステージ2～5を実行します。ステージ3で<literal>data8</literal>をストレージノードとして追加します。しばらくの間、<literal>data8</literal>は両方の役割を持ちます。ステージ4で<literal>rgw1</literal>にObject Gatewayの役割を追加し、ステージ5で<literal>data8</literal>からObject Gatewayの役割を削除します。
   </para>
  </example>
 </sect1>
 <sect1 xml:id="ds.mon">
  <title>Monitorノードの再展開</title>

  <para>
   1つ以上のMonitorノードに障害が発生し、応答していない場合、障害が発生したMonitorをクラスタから削除してから、可能であればクラスタに再度追加する必要があります。
  </para>

  <important>
   <title>3つのMonitorノードが最小</title>
   <para>
    Monitorノードの数を3つ未満にすることはできません。1つのMonitorに障害が発生し、その結果クラスタのMonitorノードの数が1つまたは2つのみになった場合、障害が発生したMonitorノードを再展開する前に、そのMonitorノードを一時的に他のMonitorノードに割り当てる必要があります。障害が発生したMonitorノードを再展開した後、一時的なMonitorノードをアンインストールできます。
   </para>
   <para>
    Cephクラスタへの新しいノード/役割の追加の詳細については、<xref linkend="salt.adding.nodes"/>および<xref linkend="salt.adding.services"/>を参照してください。
   </para>
   <para>
    クラスタノードの削除の詳細については、<xref linkend="salt.node.removing"/>を参照してください。
   </para>
  </important>

  <para>
   Cephノードの障害の程度には、基本的に次の2つがあります。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Salt Minionホストが物理的にまたはOSレベルで壊れ、<command>salt '<replaceable>minion_name</replaceable>' test.ping</command>の呼び出しに応答しない。この場合、<xref linkend="ceph.install.stack"/>の関連する手順に従って、サーバを完全に再展開する必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     Monitor関連サービスに障害が発生して回復できないものの、ホストは<command>salt '<replaceable>minion_name</replaceable>' test.ping</command>の呼び出しに応答する。この場合は、次の手順に従います。
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     Salt Masterの<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を編集して、障害が発生したMonitorノードに対応する行を削除または更新し、動作中のMonitorノードを指すようにします。
    </para>
   </step>
   <step>
    <para>
     DeepSeaステージ2～5を実行して変更を適用します。
    </para>
<screen>
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.4
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.node.add-disk">
  <title>ノードへのOSDの追加</title>

  <para>
   既存のOSDノードにディスクを追加するには、ディスク上のパーティションを削除および消去する必要があります。詳細については、<xref linkend="ceph.install.stack"/>の<xref linkend="deploy.wiping.disk"/>を参照してください。ディスクが空になったら、そのディスクをノードのYAMLファイルに追加します。ファイルのパスは、<filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/<replaceable>node_name</replaceable>.yml</filename>です。ファイルを保存した後、DeepSeaステージ2と3を実行します。
  </para>

<screen><prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>

  <tip>
   <title>ファイルの自動更新</title>
   <para>
    YAMLファイルを手動で編集する代わりに、DeepSeaで新しいプロファイルを作成できます。DeepSeaが新しいプロファイルを作成できるようにするには、既存のプロファイルを削除する必要があります。
   </para>
<screen><prompt>root@master # </prompt><command>old</command> /srv/pillar/ceph/proposals/profile-default/
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.1
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
  </tip>
 </sect1>
 <sect1 xml:id="salt.removing.osd">
  <title>OSDの削除</title>

  <para>
   次のコマンドを実行することによって、クラスタからCeph OSDを削除できます。
  </para>

<screen><prompt>root@master # </prompt><command>salt-run</command> disengage.safety
<prompt>root@master # </prompt><command>salt-run</command> remove.osd <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable>は、<literal>osd</literal>という用語を除く、OSDの番号にする必要があります。たとえば、<literal>osd.3</literal>の場合、数字<literal>3</literal>のみを使用します。
  </para>

  <tip>
   <title>複数のOSDの削除</title>
   <para>
    <command>salt-run remove.osd</command>コマンドで複数のOSDを並行して削除することはできません。複数のOSDの削除を自動化するには、次のループを使用できます(5、21、33、19は、削除するOSDのID番号です)。
   </para>
<screen>
for i in 5 21 33 19
do
 echo $i
 salt-run disengage.safety
 salt-run remove.osd $i
done
</screen>
  </tip>

  <sect2 xml:id="osd.forced.removal">
   <title>壊れたOSDの強制削除</title>
   <para>
    OSDを正常に削除できない場合があります(<xref linkend="salt.removing.osd"/>を参照してください)。これは、たとえば、OSDまたはそのキャッシュが壊れた場合、I/O操作がハングする問題が発生している場合、OSDディスクをアンマウントできない場合などに発生することがあります。このような場合、OSDを強制的に削除する必要があります。
   </para>
<screen><prompt>root@master # </prompt><replaceable>target</replaceable> osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <para>
    このコマンドは、データパーティションと、ジャーナルまたはWAL/DBパーティションの両方を削除します。
   </para>
   <para>
    孤立している可能性があるジャーナル/WAL/DBデバイスを特定するには、次の手順に従います。
   </para>
   <procedure>
    <step>
     <para>
      孤立パーティションが存在する可能性があるデバイスを選択して、そのパーティションのリストをファイルに保存します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ls /dev/sdd?* &gt; /tmp/partitions
</screen>
    </step>
    <step>
     <para>
      block.wal、block.db、およびjournalのすべてのデバイスに対して<command>readlink</command>を実行し、その出力を、先ほど保存したパーティションリストと比較します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>readlink -f /var/lib/ceph/osd/ceph-*/{block.wal,block.db,journal} \
 | sort | comm -23 /tmp/partitions -
</screen>
     <para>
      この出力は、Cephによって使用されて「いない」<emphasis/>パーティションのリストです。
     </para>
    </step>
    <step>
     <para>
      好みのコマンド(たとえば、<command>fdisk</command>、または<command>parted</command>、<command>sgdisk</command>)を使用して、Cephに属していない孤立パーティションを削除します。
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ds.osd.recover">
  <title>再インストールしたOSDノードの回復</title>

  <para>
   OSDノードの1つでオペレーティングシステムが壊れて回復不可能な場合、クラスタデータを変更せずにノードを回復してそのOSDの役割を再展開するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     ノードにオペレーティングシステムを再インストールします。
    </para>
   </step>
   <step>
    <para>
     OSDノードに <package>salt-minion</package> パッケージをインストールして、Salt Master上にある古いSalt Minionキーを削除し、新しいSalt MinionのキーをSalt Masterに登録します。Salt Minionの展開の詳細については、<xref linkend="ceph.install.stack"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     ステージ0全体を実行するのではなく、次の部分を実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     DeepSeaステージ1～5を実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     DeepSeaステージ0を実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     関連するOSDノードを再起動します。すべてのOSDディスクが再検出されて再使用されます。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt.automated.installation">
  <title>Saltを使用したインストールの自動化</title>

  <para>
   Salt Reactorを使用することにより、インストールを自動化できます。仮想環境または一貫性のあるハードウェア環境では、この設定により、指定した動作でCephクラスタを作成できます。
  </para>

  <warning>
   <para>
    Saltは、Reactorのイベントに基づいて依存関係の確認を実行することはできません。Salt Masterが過負荷になり、応答しなくなる現実の危険があります。
   </para>
  </warning>

  <para>
   自動インストールには以下が必要です。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     適切に作成された<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。
    </para>
   </listitem>
   <listitem>
    <para>
     <filename>/srv/pillar/ceph/stack</filename>ディレクトリに配置された、準備済みのカスタム設定。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Reactorのデフォルトの設定では、ステージ0と1のみが実行されます。このため、以降のステージが完了するまで待つことなくReactorをテストできます。
  </para>

  <para>
   最初のSalt Minionが起動すると、ステージ0が開始されます。ロックされるため、複数のインスタンスが起動することはありません。すべてのMinionがステージ0を完了すると、ステージ1が開始されます。
  </para>

  <para>
   操作が適切に実行されたら、<filename>/etc/salt/master.d/reactor.conf</filename>の最後の行を変更します。
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   変更後:
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>
 </sect1>
 <sect1 xml:id="deepsea.rolling_updates">
  <title>クラスタノードの更新</title>

  <para>
   クラスタのノードに定期的にローリング更新を適用することをお勧めします。更新を適用するには、ステージ0を実行します。
  </para>

<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>

  <para>
   実行中のCephクラスタが検出されると、DeepSeaは各ノードに順次更新を適用してノードを再起動します。DeepSeaは、Cephの公式な推奨に従って、最初にMonitor、次にOSD、最後に追加のサービス(MDS、Object Gateway、iSCSI Gateway、NFS Ganeshaなど)の順に更新します。クラスタで問題が検出された場合、DeepSeaは更新プロセスを停止します。そのトリガになる可能性がある条件は次のとおりです。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Cephが300秒以上「HEALTH_ERR」をレポートする。
    </para>
   </listitem>
   <listitem>
    <para>
     割り当てられているサービスが更新後も引き続き稼働中かどうかをSalt Minionに問い合わせる。サービスが900秒以上ダウンしている場合、更新は失敗します。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   これらの手段を講じておくことにより、更新が壊れていたり更新に失敗したりしても、Cephクラスタが継続して動作するようにします。
  </para>

  <para>
   DeepSeaステージ0は、<command>zypper update</command>によってシステムを更新し、カーネルが更新されている場合、システムを再起動します。すべてのノードが強制的に再起動される可能性を排除するには、DeepSeaステージ0を開始する前に、最新のカーネルがインストールおよび実行されていることを確認します。
  </para>

  <tip>
   <title><command>zypper patch</command></title>
   <para>
    <command>zypper patch</command>コマンドを使用してシステムを更新する場合は、<filename>/srv/pillar/ceph/stack/global.yml</filename>を編集して次の行を追加します。
   </para>
<screen>update_method_init: zypper-patch</screen>
  </tip>

  <para>
   <filename>/srv/pillar/ceph/stack/global.yml</filename>に次の行を追加することによって、DeepSeaステージ0でデフォルトで再起動される動作を変更できます。
  </para>

<screen>stage_prep_master: default-update-no-reboot
stage_prep_minion: default-update-no-reboot</screen>

  <para>
   <literal>stage_prep_master</literal>はSalt Masterのステージ0の動作を設定し、<literal>stage_prep_minion</literal>はすべてのMinionの動作を設定します。利用可能なすべてのパラメータは次のとおりです。
  </para>

  <variablelist>
   <varlistentry>
    <term>default</term>
    <listitem>
     <para>
      更新をインストールして、更新後に再起動します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-update-no-reboot</term>
    <listitem>
     <para>
      再起動せずに更新をインストールします。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-reboot</term>
    <listitem>
     <para>
      更新をインストールせずに再起動します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>default-no-update-no-reboot</term>
    <listitem>
     <para>
      更新のインストールまたは再起動を行いません。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.salt.cluster.reboot">
  <title>クラスタの停止または再起動</title>

  <para>
   場合によっては、クラスタ全体を停止または再起動しなければならないことがあります。実行中のサービスの依存関係を入念に確認することをお勧めします。次の手順では、クラスタの停止と起動の概要を説明します。
  </para>

  <procedure>
   <step>
    <para>
     OSDにoutのマークを付けないようCephクラスタに指示します。
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     次の順序でデーモンとノードを停止します。
    </para>
    <orderedlist>
     <listitem>
      <para>
       ストレージクライアント
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイ(たとえば、NFS Ganesha、Object Gateway)
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     必要に応じて、保守タスクを実行します。
    </para>
   </step>
   <step>
    <para>
     ノードとサーバをシャットダウンプロセスの逆の順序で起動します。
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイ(たとえば、NFS Ganesha、Object Gateway)
      </para>
     </listitem>
     <listitem>
      <para>
       ストレージクライアント
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     nooutフラグを削除します。
    </para>
<screen><prompt>root # </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds.custom.cephconf">
  <title>カスタムの<filename>ceph.conf</filename>ファイル</title>

  <para>
   <filename>ceph.conf</filename>ファイルにカスタム設定を挿入する必要がある場合、<filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename>ディレクトリにある次の設定ファイルを変更できます。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>固有の<filename>rgw.conf</filename></title>
   <para>
    Object Gatewayは非常に柔軟で、他の<filename>ceph.conf</filename>セクションと比較して独特です。他のすべてのCephコンポーネントには、<literal>[mon]</literal>や<literal>[osd]</literal>のような静的なヘッダがあります。Object Gatewayには、<literal>[client.rgw.rgw1]</literal>のような固有のヘッダがあります。つまり、<filename>rgw.conf</filename>ファイルにはヘッダエントリが必要です。例については、<filename>/srv/salt/ceph/configuration/files/rgw.conf</filename>を参照してください。
   </para>
  </note>

  <important>
   <title>ステージ3を実行します。</title>
   <para>
    上で説明されている設定ファイルにカスタムの変更を加えた後、ステージ3を実行し、これらの変更をクラスタノードに適用します。
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
  </important>

  <para>
   これらのファイルは、<filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>テンプレートファイルからインクルードされ、Ceph設定ファイルで使用できるさまざまなセクションに対応します。設定スニペットを正しいファイルに配置することにより、DeepSeaはその設定を正しいセクションに配置できます。セクションヘッダを追加する必要はありません。
  </para>

  <tip>
   <para>
    設定オプションをデーモンの特定のインスタンスにのみ適用するには、<literal>[osd.1]</literal>のようなヘッダを追加します。そのヘッダより後の設定オプションは、IDが1のOSDデーモンにのみ適用されます。
   </para>
  </tip>

  <sect2>
   <title>デフォルト値の上書き</title>
   <para>
    セクションの後にあるステートメントの方が前にあるステートメントよりも優先されます。したがって、<filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>テンプレートで指定されているデフォルト設定を上書きできます。たとえば、cephx認証をオフにするには、<filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>ファイルに次の3行を追加します。
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
  </sect2>

  <sect2>
   <title>設定ファイルのインクルード</title>
   <para>
    大量のカスタム設定を適用する必要がある場合、カスタム設定ファイル内で次のincludeステートメントを使用すると、ファイル管理が容易になります。次に、<filename>osd.conf</filename>ファイルの例を示します。
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    上の例では、<filename>osd1.conf</filename>、<filename>osd2.conf</filename>、<filename>osd3.conf</filename>、および<filename>osd4.conf</filename>の各ファイルに、関連するOSDに固有の設定ファイルが含まれています。
   </para>
   <tip>
    <title>ランタイム設定</title>
    <para>
     Ceph設定ファイルに加えた変更は、関連するCephデーモンの再起動後に有効になります。Cephランタイム設定の変更の詳細については、<xref linkend="ceph.config.runtime"/>を参照してください。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.config.runtime">
  <title>Cephランタイム設定</title>

  <para>
   <xref linkend="ds.custom.cephconf"/>は、Ceph設定ファイル<filename>ceph.conf</filename>に変更を加える方法について説明しています。しかし、クラスタの実際の動作は、<filename>ceph.conf</filename>ファイルの現在の状態ではなく、メモリに格納されている、実行中のCephデーモンの設定によって決まります。
  </para>

  <para>
   デーモンが実行されているノードで「admin socket」<emphasis/>を使用して、特定の設定をCephの個々のデーモンに問い合わせることができます。たとえば、次のコマンドは、<literal>osd.0</literal>という名前のデーモンから<option>osd_max_write_size</option>設定パラメータの値を取得します。
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/ceph-osd.0.asok \
config get osd_max_write_size
{
  "osd_max_write_size": "90"
}</screen>

  <para>
   デーモンの設定をランタイム時に「変更」<emphasis/>することもできます。この変更は一時的で、デーモンを次回再起動すると失われることに注意してください。たとえば、次のコマンドは、クラスタ内にあるすべてのOSDの<option>osd_max_write_size</option>パラメータを「50」に変更します。
  </para>

<screen><prompt>root # </prompt>ceph tell osd.* injectargs --osd_max_write_size 50</screen>

  <warning>
   <title><command>injectargs</command>は信頼性が低い</title>
   <para>
    残念ながら、<command>injectargs</command>コマンドでのクラスタ設定の変更は完全には信頼できません。変更したパラメータを確実にアクティブにする必要がある場合は、設定ファイルでパラメータを変更して、クラスタ内のすべてのデーモンを再起動してください。
   </para>
  </warning>
 </sect1>
</chapter>
