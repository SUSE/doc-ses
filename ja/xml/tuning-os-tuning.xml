<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="tuning-os-tuning.xml" version="5.0" xml:id="tuning-os">
 <title>オペレーティングシステムレベルのチューニング</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   Cephクラスタのパフォーマンスチューニングの大部分は、オペレーティングシステム(OS)階層で実行できます。このチューニングでは、不要なサービスが実行されていないことの確認が含まれ、またバッファがオーバーランされないようにするまで拡張し、割り込みが適切に分散されるようにします。統計的に重要でないパフォーマンスの変更により、またはこの作業時にパフォーマンスが大幅に向上する候補と見なされなかったため、OSには多数の追加のチューニングオプションがあります。
 </para>
 <sect1 xml:id="tuning-sles">
   <title>SUSE Linux Enterpriseのインストールと基本パフォーマンスの検証</title>
   <para>
     OSインストール中に、Xサーバを含むインストールパターンを選択「しない」でください。<emphasis/>そうすることにより、ストレージ関連のデーモンのチューニングに、より適切に割り当てられるRAMおよびCPUリソースが利用されます。<literal>YaST管理</literal>パターンを追加して、最小パターンを含むパターンをお勧めします。</para>
    <para>
     OSをインストールした後で、ストレージクラスタの全体的なパフォーマンスに重要なさまざまなコンポーネントを個別に評価することが適切です。
   </para>
   <note>
     <para>
       SUSE Enterprise Storageをセットアップする前に、個々のコンポーネントのパフォーマンスを確認して、コンポーネントが期待どおりに動作していることを確認します。
     </para>
   </note>
   <sect2>
     <title>ネットワークパフォーマンス</title>
     <para>
       ネットワークパフォーマンスについて<command>iperf3</command>テストを実行するには、ウィンドウサイズ(<option>-w</option>)を増やして、複数のストリームを実行して、帯域幅機能を完全にテストすることを検討してください。標準のMTUの場合、NICはアドバタイズされた帯域幅の約70〜80％で実行できる必要があります。ジャンボフレームまで上に移動すると、NICは回線を飽和させることができるはずです。
     </para>
     <para>
       これは、100Gbなどの高速トポロジには必ずしも当てはまりません。これらのトポロジでは、NICを飽和させるには、ハードウェアに適切なCPUクロック速度と設定があることと、かなりの量のOSとドライバのチューニングが必要になる場合があります。
     </para>
     <para>
       これは、100Gbネットワークで使用される<command>iperf3</command>コマンドのサンプルです。コマンドラインで、<option>-N</option>はNagleのバッファリングアルゴリズムを無効にし、<option>-l</option>はバッファ長をデフォルトの128kよりも大きく設定するため、わずかにスループットが向上します。
     </para>
<screen>
  server# iperf3 -s

  client# iperf3   -c server -N -l 256k
  Connecting to host sr650-1, port 5201
  [  4] local 172.16.227.22 port 36628 connected to 172.16.227.21 port 5201
  [ ID] Interval           Transfer     Bandwidth       Retr  Cwnd
  [  4]   0.00-1.00   sec  4.76 GBytes  40.9 Gbits/sec    0   1.48 MBytes
  [  4]   1.00-2.00   sec  4.79 GBytes  41.1 Gbits/sec    0   2.52 MBytes
  [  4]   2.00-3.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   3.00-4.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   4.00-5.00   sec  4.74 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   5.00-6.00   sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   6.00-7.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   7.00-8.00   sec  4.72 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  [  4]   8.00-9.00   sec  4.73 GBytes  40.7 Gbits/sec    0   2.52 MBytes
  [  4]   9.00-10.00  sec  4.73 GBytes  40.6 Gbits/sec    0   2.52 MBytes
  - - - - - - - - - - - - - - - - - - - - - - - - -
  [ ID] Interval           Transfer     Bandwidth       Retr
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec    0             sender
  [  4]   0.00-10.00  sec  47.4 GBytes  40.7 Gbits/sec                  receiver
</screen>
   </sect2>
   <sect2>
     <title>ストレージパフォーマンス</title>
     <para>
       <command>fio</command>を使用して個々のストレージデバイスをテストし、デバイスごとのパフォーマンスの最大値を理解します。これをすべてのデバイスに対して実行して、仕様から外れたり、帯域幅を低下させるエクスパンダに接続されたりすることのないようにします。さまざまなI/Oサイズとパターンを徹底的に調査することで、パフォーマンスの期待に関するほとんどの情報が得られますが、これはこのドキュメントの範囲を超えています。
     </para>
     <para>
       少なくともランダム4kB、ランダム64kB、シーケンシャル64kBおよび1MBをテストすることをお勧めします。これにより、デバイスのパフォーマンス特性を全体的に適切に把握できます。テストする際には、rawデバイス(<literal>/dev/sd<replaceable>X</replaceable></literal>)を使用し、複数のジョブで<option>direct=1</option>オプションを使用して、負荷下でデバイスのパフォーマンスを最大化することが重要です。
     </para>
     <para>
       テストサイズ(データセット)が、適用される可能性のあるキャッシュをオーバーランするのに十分な大きさであることを確認します。パフォーマンス測定が行われる前に、キャッシュオーバーランが発生するのに十分な時間を確保できるように<literal>ramp-time</literal>パラメータを使用することをお勧めします。これにより、キャッシュのみのパフォーマンスによってパフォーマンス値が損なわれないようにします。
     </para>
     <para>
       OSDノードのすべてのデバイスに対して<command>fio</command>を同時に実行して、ボトルネックを特定します。ほぼ線形にスケールするはずです。そうでない場合は、コントローラのファームウェア、スロットの配置を確認し、必要に応じてデバイスを複数のコントローラに分割します。これは、負荷の高いノードをシミュレートしています。
     </para>
     <para>
       個々のデバイスをテストしたときと同じI/Oパターンとブロックサイズを使用することをお勧めします。ジョブ数は、すべてのデバイスおよびバスに均等に分配できるように、システム内のデバイスの合計数の倍数である必要があります。
     </para>
     <sect3>
       <title>レイテンシバウンドと最大</title>
       <para>
         レイテンシ·バウンドとワースト·ケース·シナリオのテストの両方を実行することには価値があります。特定の値を変更してもレイテンシは改善されない可能性がありますが、レイテンシが増加し続けている場合でも、クラスタはより多くの合計負荷を処理できるようになります。変化が測定可能な方法で操作のレイテンシに影響を及ぼす場合に、逆のことが当てはまる場合もあります。両方の可能性を特定するために、両方の位置を表すテストを実行することをお勧めします。<command>fio</command>でのレイテンシ-バウンドテストには次のセットがあります。
       </para>
<screen>
  latency_target=10ms
  latency_window=5s
  latency_percentile=99
</screen>
       <para>
         上記の設定により、<command>fio</command>は5秒のスライドウィンドウで平均10ミリ秒を維持しなくなったときに、IOPSの1%以上になるまでI/Oキューの深さを増やします。次に、レイテンシの平均が維持されるまで、キューの深さを減らします。
       </para>
     </sect3>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-kernel-tuning">
   <title>カーネルチューニング</title>
   <para>
     カーネルには、クラスタと一部のクライアントの両方で調整できるいくつかの側面があります。ほとんどのチューニングは、全体として測定可能であり、できればパフォーマンスの意味のある改善を示す、非常に小さな増分の向上を実現することを理解することが重要です。チューニングに関する情報は、この作業のさまざまなソースからのものです。プライマリソースは、<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/book-sle-tuning.html"/>です。ハードウェアベンダのドキュメントを含む、多数の他の参照が利用されました。
   </para>
   <sect2>
     <title>CPU緩和策</title>
     <para>
       パフォーマンスチューニングのためのカーネルの重要な領域の1つは、カーネルに存在するサイドチャネル攻撃の緩和策を無効にすることです。このチューニングから得られる多くのメリットは、サイズが4kB～64kBの小さいI/Oで発生します。特に、2つのクライアントノードのみを使用する限定的なテスト環境では、64kBランダム読み取りと順次書き込みのパフォーマンスが2倍になりました。
     </para>
     <para>
       これらのオプションを変更するには、一部のCPUに対するサイドチャネル攻撃の緩和策を無効にすることが含まれるため、セキュリティの影響を明確に理解する必要があります。新しいプロセッサでは、これらの緩和策を無効にする必要性を最小限に抑えることができます。これが、使用されている特定のハードウェアに必要なものであるどうかを慎重に評価します。テスト設定では、これらの変更を適用するためにsalt状態が使用されました。Salt状態は、Saltマスタの<filename>/srv/salt</filename>のサブディレクトリにあり、次のような<command>salt state.apply</command>コマンドを使用することで適用されます。
     </para>
<screen>
salt '*' state.apply my_kerntune
</screen>
     <para>
       このテストで使用されるSalt状態と手順については、<xref linkend="tuning-appendix-a"/>を参照してください。これは、各お客様の環境で機能するように調整する必要があります。<literal>grub2</literal>環境設定ファイルの調整例については、付録を参照してください。
     </para>
   </sect2>
   <sect2>
     <title>I/Oチューニング - マルチキューブロックI/O</title>
     <para>
       調整する最初の側面は、I/Oが最適なパターンで流れているかどうかを確認することです。このテストで使用するテストクラスタでは、これはマルチキューブロックI/Oを有効にすることを意味します。これは、セクション12.4 (<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-tuning-io.html#cha-tuning-io-barrier"/>)で示すように、ブート時のカーネルパラメータを追加することによって行われます。これは、HDDデバイスを含むクラスタでは、これらのデバイスのパフォーマンスが低下する可能性があるため、一方的に実行する必要があるアクションではありません。一般的な結果は、デバイスには複数のI/Oキューが割り当てられているため、多数の要求を処理できるNVMeやSSDなどのジョブで同時に処理できるジョブが増えます。
     </para>
   </sect2>
   <sect2>
     <title>SSDチューニング</title>
     <para>
       最適な読み取りパフォーマンスを得るために、SSDデバイスの<option>read_ahead</option>および書き込みキャッシュ設定を調整する必要がある場合があります。特定のテスト環境では、書き込みキャッシュを無効にして、 <option>read_ahead</option>を2MBに強制すると最適な全体的パフォーマンスを得られます。
     </para>
     <para>
       チューニングの前に、デフォルト値を確認し、ベースラインと比較してパフォーマンスの違いを測定することが重要です。
     </para>
     <para>
       <filename>/etc/udev/rules.d</filename>に次のファイルを配置すると、デバイスは<filename>/sys/block/{devname}/device/model</filename>に示されているモデル名で検出され、ライトキャッシュを無効にして<option>read_ahead_kb</option>を2MBに設定するように割り当てられます。
     </para>
<screen>
  /etc/udev/rules.d/99-ssd.rules

  # Setting specific kernel parameters for a subset of block devices (Intel SSDs)
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", ATTR{queue/read_ahead_kb}="2048"
  SUBSYSTEM=="block", ATTRS{model}=="INTEL SSDS*", ACTION=="add|change", RUN+="/sbin/hdparm -W 0 /dev/%k"
</screen>
   </sect2>
   <sect2>
     <title>ネットワークスタックとデバイスのチューニング</title>
     <para>
       ネットワークスタックを適切にチューニングすると、クラスタのレイテンシとスループットが大幅に向上する可能性があります。実行したテストの完全なスクリプトについては、<xref linkend="tuning-appendix-c"/>を参照してください。
     </para>
     <para>
       最初の最も影響の大きいチューニングは、ジャンボフレームパケットを利用することです。これを実行するには、クラスタを利用するすべてのインターフェイスをMTUが9000である同じ設定に設定する必要があります。ネットワークスイッチのポートは、多くの場合、9100以上に設定されています。パケットを渡すだけで、作成しないため、これは適切です。
     </para>
     <sect3>
       <title>ネットワークデバイスのチューニング</title>
       <sect4>
         <title>ジャンボフレーム</title>
         <para>
           次のSaltコマンドは、制御下のすべてのノード(テスト負荷生成ノードを含む)の結合インターフェイスが9000のMTUを利用していたことを確認します。
          </para>
<screen>
salt '*' cmd.run 'ip link set bond0 mtu 9000'
</screen>
          <para>
            これを永続的に設定するには、YaSTを利用して結合インターフェイスのMTUを設定します。
          </para>
        </sect4>
        <sect4>
          <title>PCIeバスの調整</title>
          <para>
            PCIe最大読み取り要求サイズを調整すると、パフォーマンスがわずかに向上する場合があります。このチューニングはカードとスロットに固有であり、製造元によって提供される条件と指示に関連してのみ実行する必要があることに注意してください。最大PCIe読み取り要求サイズは、次のSaltコマンドで設定されました。
          </para>
          <warning>
            <para>
              これは、NIC製造元からのガイダンスを使用してのみ実行する必要があり、バスの場所、ドライババージョン、およびハードウェアに固有です。
            </para>
          </warning>
<screen>
  salt '*' cmd.run 'setpci -s 5b:00.0 68.w=5936'
  salt '*' cmd.run 'setpci -s 5b:00.1 68.w=5936'
</screen>
        </sect4>
        <sect4>
          <title>TCP RSS</title>
          <para>
            チューニングリストの次の項目は、単一CPUコアがすべてのパケット処理を担当しないようにすることで役立ちます。小さなスクリプトは、(NUMAの観点から)複数のローカルコアにI/Oを分散するために使用されます。
          </para>
          <note>
            <para>
              <command>ls /sys/class/net/{ifname}/queues/rx-*|wc -l</command>によって返されるキュー数が単一CPUソケットの物理コア数に等しい場合、これは不要です。
            </para>
          </note>
<screen>
salt '*' cmd.run 'for j in `cat /sys/class/net/bond0/bonding/slaves`;do \
LOCAL_CPUS=`cat /sys/class/net/$j/device/local_cpus`;echo $LOCAL_CPUS &gt; \
/sys/class/net/$j/queues/rx-0/rps_cpus;done'
</screen>
        </sect4>
        <sect4>
          <title>リングバッファ</title>
          <para>
            多くのNICドライバは、高スループットシナリオには最適ではない受信(RX)および転送(TX)バッファのデフォルト値から開始し、バッファがいっぱいになる前にバッファを空にする十分な時間をカーネルが確保できません。</para>
           <para>
            現在の設定と最大設定は、次のコマンドを適切なNICに発行することで確認できます。
          </para>
<screen>
ethtool -g eth4
</screen>
          <para>
            このコマンドの出力は次のようになります。
          </para>
<screen>
  Ring parameters for eth4:
  Pre-set maximums:
  RX:		8192
  RX Mini:	0
  RX Jumbo:	0
  TX:		8192
  Current hardware settings:
  RX:		1024
  RX Mini:	0
  RX Jumbo:	0
  TX:		1024
</screen>
          <para>
            ここでは、NICは最大8kBを割り当てることができますが、現在は1kBのバッファのみを使用していることがわかります。クラスタに対してこれを調整するには、次のコマンドを発行します。
          </para>
<screen>
  salt '*' cmd.run 'ethtool -G eth4 rx 8192 tx 8192'
  salt '*' cmd.run 'ethtool -G eth5 rx 8192 tx 8192'
</screen>
          <para>
            この値を永続的に設定するには、YaST設定モジュールを介して実現できます。
          </para>
          <figure xml:id="yast-config-module">
            <title>YaST設定モジュール</title>
            <mediaobject>
              <imageobject role="html">
                <imagedata fileref="yast_ring_buffers.png" width="70%" format="PNG"/>
              </imageobject>
            </mediaobject>
          </figure>
          <para>
            さらに、<filename>/etc/sysconfig/network</filename>にある物理インターフェイスの設定ファイルを編集することにより、設定を永続的にすることができます。付録Bに、すべてのインターフェイスを最大リングバッファ値に変更するスクリプトがあります。
          </para>
        </sect4>
      </sect3>
      <sect3>
        <title>ネットワークスタック</title>
        <para>
          次の設定は、<filename>/etc/sysctl.conf</filename>を変更することにより、すべてを永続的にすることができます。これらはsaltコマンドの引数として表され、永続化する前に使用環境でテストおよび検証できるようにします。
        </para>
        <sect4>
          <title>TCPレイテンシを少なくする</title>
          <para>
            <literal>TCP低レイテンシ</literal>オプションを設定すると、IPv4 TCP事前キュー処理が無効になり、レイテンシが改善されます。この設定を<literal>0</literal>と<literal>1</literal>の両方で試してみることをお勧めします。ラボテストでは、値を<literal>1</literal>に設定すると、わずかにパフォーマンスが改善されました。
          </para>
  <screen>
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_low_latency=1'
  </screen>
          <para>
            TCP <option>fastopen</option>オプションを使用すると、最初の<literal>syn</literal>パケットでデータを送信することができるため、レイテンシがわずかに改善されます。
          </para>
  <screen>
  salt '*' cmd.run 'sysctl -w net.ipv4.tcp_fastopen=1'
  </screen>
        </sect4>
        <sect4>
          <title>TCPスタックバッファ</title>
          <para>
            TCPスタックにインバウンドとアウトバウンドの両方のトラフィックをキューに入れるための十分なバッファスペースがあることを確認します。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_rmem="10240 87380 2147483647"'
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_wmem="10240 87380 2147483647"'
</screen>
        </sect4>
        <sect4>
          <title>TCPシーケンスとタイムスタンプ</title>
          <para>
            高速ネットワークでは、TCPシーケンス番号を非常に短いタイムフレームで再利用できます。その結果、システムはパケットが順不同で受信されたと判断し、ドロップします。パケットシーケンスを適切に追跡できるように、TCPタイムスタンプが追加されました。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_timestamps=1'
</screen>
          <para>
            <literal>TCP選択的確認応答</literal>は、主にWANまたは低速ネットワークに役立つ機能です。ただし、これを無効にすると、他の方法で悪影響が生じる可能性があります。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.ipv4.tcp_sack=1'
</screen>
        </sect4>
        <sect4>
          <title>カーネルネットワークバッファと接続</title>
          <para>
            十分なバッファスペースを提供することは、ネットワークを高パフォーマンスにチューニングする上で繰り返し発生するテーマです。<literal>netdev_max_backlog</literal>は、トラフィックがNICによって受信された後で、プロトコルスタック(IP、TCPなど)によって処理される前にキューに入れられる場所です。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.core.netdev_max_backlog=250000'
</screen>
          <para>
            システムノードおよびアプリケーションノードに対する他の予防措置には、最大接続数が<literal>syn</literal> cookieの生成を回避できるほど十分な数であることを確認することが含まれます。これは、関係するすべてのノード上で設定するのに役立ちます。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.core.somaxconn=2048'
</screen>
          <para>
            ネットワークスタックバッファを増やすと、すべてのトランザクションに対して十分なバッファが確実に存在するようになります。
          </para>
<screen>
salt '*' cmd.run 'sysctl -w net.core.rmem_max=2147483647'
salt '*' cmd.run 'sysctl -w net.core.wmem_max=2147483647'
     </screen>
   </sect4>
 </sect3>
</sect2>
 </sect1>
</chapter>
