<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>NFS Ganeshaのインストール</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編集</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganeshaは、Object GatewayまたはCephFSにNFSアクセスを提供します。SUSE Enterprise Storage 6では、NFSバージョン3と4がサポートされています。NFS Ganeshaはカーネル空間ではなくユーザ空間で動作し、Object GatewayまたはCephFSと直接対話します。
 </para>
 <warning>
  <title>クロスプロトコルアクセス</title>
  <para>
   ネイティブのCephFSおよびNFSクライアントは、Sambaを介して取得されるファイルロックによる制限を受けません。また、その逆も同様です。クロスプロトコルファイルロックに依存するアプリケーションでは、CephFSを利用するSamba共有パスに他の手段でアクセスした場合、データの破壊が発生することがあります。
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>準備</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>一般情報</title>
   <para>
    NFS Ganeshaを正しく展開するには、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>に<literal>role-ganesha</literal>を追加する必要があります。詳細については、<xref linkend="policy-configuration"/>を参照してください。さらに、<filename>policy.cfg</filename>に<literal>role-rgw</literal>または<literal>role-mds</literal>も存在する必要があります。
   </para>
   <para>
    すでに存在するCephノードにNFS Ganeshaサーバをインストールして実行することはできますが、Cephクラスタにアクセスできる専用のホストで実行することをお勧めします。通常、クライアントホストはクラスタには含まれませんが、NFS Ganeshaサーバに対するネットワークアクセスが必要です。
   </para>
   <para>
    NFS Ganeshaサーバを初期インストール後の任意の時点で有効にするには、<filename>policy.cfg</filename>に<literal>role-ganesha</literal>を追加して、少なくともDeepSeaのステージ2および4をもう一度実行します。詳細については、<xref linkend="ceph-install-stack"/>を参照してください。
   </para>
   <para>
    NFS Ganeshaの設定には、NFS Ganeshaノードに存在するファイル<filename>/etc/ganesha/ganesha.conf</filename>を使用します。ただし、このファイルはDeepSeaステージ4を実行するたびに上書きされます。したがって、Saltが使用するテンプレートを編集することをお勧めします。このテンプレートは、Salt Master上にあるファイル<filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>です。設定ファイルの詳細については、<xref linkend="ceph-nfsganesha-config"/>を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>要件の概要</title>
   <para>
    DeepSeaステージ2および4を実行してNFS Ganeshaをインストールするには、以下の要件を満たす必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      少なくとも1つのノードに<literal>role-ganesha</literal>が割り当てられている必要があります。
     </para>
    </listitem>
    <listitem>
     <para>
      1つのミニオンに定義できる<literal>role-ganesha</literal>は1つだけです。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganeshaが動作するにはObject GatewayまたはCephFSが必要です。
     </para>
    </listitem>
    <listitem>
     <para>
      <literal>role-ganesha</literal>の役割を持つミニオンでは、カーネルベースのNFSを無効にする必要があります。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>インストールの例</title>

  <para>
   この手順では、Object GatewayとNFS GaneshaのCephFS FSAL (File System Abstraction Layers)の両方を使用するインストールの例について説明します。
  </para>

  <procedure>
   <step>
    <para>
     DeepSeaステージ0と1をまだ実行していない場合は、この手順に進む前に実行します。
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     DeepSeaステージ1の実行後、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を編集して次の行を追加します。
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     <replaceable>NODENAME</replaceable>は実際のクラスタのノード名に置き換えてください。
    </para>
    <para>
     さらに、<literal>role-mds</literal>と<literal>role-rgw</literal>が割り当てられていることを確認します。
    </para>
   </step>
   <step>
    <para>
     少なくともDeepSeaステージ2と4を実行します。その間にステージ3を実行することをお勧めします。
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     NFS Ganeshaサービスがミニオンノードで実行されていることを確認することで、NFS Ganeshaが動作していることを確認します。
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>高可用性のアクティブ-パッシブ設定</title>

  <para>
   このセクションでは、NFS Ganeshaサーバの2ノードのアクティブ-パッシブ設定を行う方法について例を挙げて説明します。このセットアップでは、SUSE Linux Enterprise High Availability Extensionが必要です。これら2つのノードは、<systemitem class="domainname">earth</systemitem>および<systemitem class="domainname">mars</systemitem>という名前です。
  </para>

  <important>
   <title>サービスのコロケーション</title>
   <para>
    独自の耐障害性と独自の負荷分散機能を持つサービスは、フェールオーバーサービスのためにフェンシングされるクラスタノードでは実行しないでください。したがって、Ceph Monitor、Metadata Server、iSCSI、またはCeph OSDの各サービスは、高可用性セットアップでは実行しないでください。
   </para>
  </important>

  <para>
   SUSE Linux Enterprise High Availability Extensionの詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>を参照してください。
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>基本的なインストール</title>
   <para>
    このセットアップでは、<systemitem class="domainname">earth</systemitem>にIPアドレス<systemitem class="ipaddress">192.168.1.1</systemitem>、<systemitem class="domainname">mars</systemitem>にアドレス<systemitem class="ipaddress">192.168.1.2</systemitem>が設定されています。
   </para>
   <para>
    さらに、2つの浮動仮想IPアドレスが使用されており、実行している物理ノードがどれであれ、クライアントからの該当サービスへの接続が可能になります。Hawk2でのクラスタ管理には<systemitem class="ipaddress">192.168.1.10</systemitem>を使用し、NFSエクスポートには<systemitem class="ipaddress">192.168.2.1</systemitem>を排他的に使用します。これにより、後で簡単にセキュリティ制約を適用できます。
   </para>
   <para>
    次の手順では、インストールの例について説明します。詳細については、<link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>を参照してください。
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Salt Master上にNFS Ganeshaノードを準備します。
     </para>
     <substeps>
      <step>
       <para>
        DeepSeaステージ0および1を実行します。
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>でノード<systemitem class="domainname">earth</systemitem>および<systemitem class="domainname">mars</systemitem>に<literal>role-ganesha</literal>を割り当てます。
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        DeepSeaステージ2～4を実行します。
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      <systemitem class="domainname">earth</systemitem>および<systemitem class="domainname">mars</systemitem>でSUSE Linux Enterprise High Availability Extensionを登録します。
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      両方のノードに <package>ha-cluster-bootstrap</package> をインストールします。
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        <systemitem class="domainname">earth</systemitem>でクラスタを初期化します。
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        <systemitem class="domainname">mars</systemitem>をクラスタに参加させます。
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      クラスタの状態を確認します。クラスタに2つのノードが追加されたことがわかります。
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      両方のノードで、起動時のNFS Ganeshaサービスの自動起動を無効にします。
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      <systemitem class="domainname">earth</systemitem>で<command>crm</command>シェルを起動します。
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      以降のコマンドはcrmシェルで実行されます。
     </para>
    </step>
    <step>
     <para>
      <systemitem class="domainname">earth</systemitem>で、crmシェルを実行して次のコマンドを実行し、NFS Ganeshaデーモンのリソースをsystemdリソースタイプのクローンとして設定します。
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      crmシェルでプリミティブIPAddr2を作成します。
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      NFS Ganeshaサーバと浮動仮想IPとの間の関係を設定するため、コロケーションと順序付けを使用します。
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      クライアントから<command>mount</command>コマンドを使用して、クラスタのセットアップが完了していることを確認します。
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>リソースのクリーンアップ</title>
   <para>
    いずれかのノード(たとえば、<systemitem class="domainname">earth</systemitem>)でNFS Ganeshaに障害が発生した場合、問題を修正してリソースをクリーンアップします。<systemitem class="domainname">mars</systemitem>でNFS Ganeshaに障害が発生した場合、リソースをクリーンアップした後でのみ、リソースを<systemitem class="domainname">earth</systemitem>にフェールバックできます。
   </para>
   <para>
    リソースをクリーンアップするには、次のコマンドを実行します。
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Pingリソースの設定</title>
   <para>
    ネットワークの問題により、サーバがクライアントにアクセスできなくなることがあります。Pingリソースによってこの問題を検出および緩和できます。このリソースの設定はオプションです。
   </para>
   <procedure>
    <step>
     <para>
      Pingリソースを定義します。
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal>は、IPアドレスをスペース文字で区切ったリストです。これらのIPアドレスに定期的にpingが送信されて、ネットワークが停止していないかどうかが確認されます。クライアントが常にNFSサーバにアクセスできる必要がある場合は、そのクライアントを<literal>host_list</literal>に追加します。
     </para>
    </step>
    <step>
     <para>
      クローンを作成します。
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      次のコマンドは、NFS Ganeshaサービスに対して制約を作成します。これにより、<literal>host_list</literal>がアクセス不可能になった場合、サービスを強制的に別のノードに移動します。
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HAとDeepSea</title>
   <para>
    DeepSeaでは、NFS Ganesha HAの設定はサポートされていません。NFS Ganesha HAの設定後にDeepSeaで障害が発生するのを防止するには、DeepSeaステージ4からNFS Ganeshaサービスの起動と停止を除外します。
   </para>
   <procedure>
    <step>
     <para>
      <filename>/srv/salt/ceph/ganesha/default.sls</filename>を<filename>/srv/salt/ceph/ganesha/ha.sls</filename>にコピーします。
     </para>
    </step>
    <step>
     <para>
      <filename>/srv/salt/ceph/ganesha/ha.sls</filename>から<literal>.service</literal>エントリを削除し、次のようにします。
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      <filename>/srv/pillar/ceph/stack/global.yml</filename>に次の行を追加します。
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>アクティブ-アクティブ設定</title>

  <para>
   このセクションでは、NFS Ganeshaのシンプルなアクティブ-アクティブセットアップの例を説明します。この目的は、同じ既存のCephFSの上に階層化された2つのGaneshaサーバを展開することです。サーバは別個のアドレスを持つ2つのCephクラスタノードになります。クライアントは、それらの間で手動で分散する必要があります。この設定における<quote>フェールオーバー</quote>とは、他方のサーバをクライアントで手動でアンマウントして再マウントすることを意味します。
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>前提条件</title>
   <para>
    このサンプル設定では、以下が必要です。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      稼働中のCephクラスタ。DeepSeaを使用してCephクラスタを展開および設定する場合の詳細については、<xref linkend="ceph-install-stack"/>を参照してください。
     </para>
    </listitem>
    <listitem>
     <para>
      少なくとも1つの設定済みのCephFS。CephFSの展開と設定の詳細については、<xref linkend="cha-ceph-as-cephfs"/>を参照してください。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganeshaが展開されている2つのCephクラスタノード。NFS Ganeshaの展開の詳細については、<xref linkend="cha-as-ganesha"/>を参照してください。
     </para>
     <tip>
      <title>専用のサーバを使用する</title>
      <para>
       NFS Ganeshaノードで他のCeph関連サービスとリソースを共有することはできますが、パフォーマンスを向上させるため、専用のサーバを使用することをお勧めします。
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    NFS Ganeshaノードを展開した後、クラスタが動作可能で、デフォルトのCephFSプールが存在していることを確認します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>NFS Ganeshaの設定</title>
   <para>
    両方のNFS Ganeshaノードにファイル<filename>/etc/ganesha/ganesha.conf</filename>がインストールされていることを確認します。まだ存在しない場合は、RADOSをNFS Ganeshaの回復バックエンドとして有効にするため、設定ファイルに次のブロックを追加します。
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   次の形式の設定で既存の行を確認すると、<replaceable>rados_pool</replaceable>および<replaceable>pool_namespace</replaceable>の値を確認できます。</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   <replaceable>nodeid</replaceable>オプションの値はマシンのFQDNに対応しており、<replaceable>UserId</replaceable>オプションと<replaceable>Ceph_Conf</replaceable>オプションの値は既存の<replaceable>RADOS_URLS</replaceable>ブロックで確認できます。
   </para>
   <para>
    NFSのレガシバージョンでは、猶予期間を早期に解除できないためサーバの再起動が長引くという理由から、バージョン4.2より前のNFSのオプションは無効にしています。また、Cephライブラリはすでに積極的なキャッシュを行っているため、NFS Ganeshaのほとんどのキャッシュも無効にしています。
   </para>
   <para>
    「rados_cluster」回復バックエンドは、その情報をRADOSオブジェクトに保存します。このデータは大量ではありませんが、このデータにも高可用性が必要です。
このためにCephFSメタデータプールを使用し、CephFSオブジェクトから区別するために、そこで新しい「ganesha」ネームスペースを宣言します。
   </para>
   <note>
    <title>クラスタノードID</title>
    <para>
     大半の設定は2つのホストで同一ですが、「RADOS_KV」ブロックの<option>nodeid</option>オプションは、各ノードで固有の文字列にする必要があります。デフォルトでは、NFS Ganeshaは<option>nodeid</option>をノードのホスト名に設定します。
    </para>
    <para>
     ホスト名以外の異なる固定値を使用する必要がある場合は、たとえば一方のノードに<option>nodeid = 'a'</option>を設定し、他方のノードに<option>nodeid = 'b'</option>を設定できます。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>クラスタ猶予データベースへの入力</title>
   <para>
    クラスタ内のすべてのノードがお互いを認識していることを確認する必要があります。これは、ホスト間で共有されるRADOSオブジェクトを介して実行します。NFS Ganeshaは、このオブジェクトを使用して、猶予期間に関する現在の状態を伝達します。
   </para>
   <para>
    このデータベースのクエリおよび操作を行うためのコマンドラインツールは、 <package>nfs-ganesha-rados-grace</package> パッケージに含まれています。このパッケージが少なくとも1つのノードにインストールされていない場合は、次のコマンドを使用してインストールします。
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    このコマンドを使用してDBを作成し、両方の<option>nodeid</option>を作成します。この例では、2つのNFS Ganeshaノードに、<literal>ses6min1.example.com</literal>および<literal>ses6min2.example.com</literal>という名前が付いています。一方のNFS Ganeshaホストで、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    これにより、猶予データベースが作成され、「ses6min1.example.com」と「ses6min2.example.com」の両方がデータベースに追加されます。最後のコマンドは、現在の状態をダンプします。新しく追加したホストは常に猶予期間が適用されていると見なされるため、両方に「E」フラグが設定されています。「cur」および「rec」の値は、現在のエポックと回復エポックを示しており、これにより、どのホストがいつ回復を実行できるかを追跡できます。
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>NFS Ganeshaサービスの再起動</title>
   <para>
    両方のNFS Ganeshaノードで、関連するサービスを再起動します。
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    サービスが再起動したら、猶予データベースを確認します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>「E」フラグのクリア</title>
    <para>
     両方のノードで「E」フラグがクリアされていることに注意してください。これは、ノードに猶予期間が適用されなくなっており、現在は通常の運用モードで動作していることを示します。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>結論</title>
   <para>
    前の手順をすべて完了したら、エクスポートされたNFSを2つのNFS Ganeshaサーバのいずれかからマウントして、NFSの通常の操作を実行できます。
   </para>
   <para>
    このサンプル設定では、2つのNFS Ganeshaサーバの一方がダウンした場合、5分以内にユーザが手動で再起動することを想定しています。5分を過ぎると、NFS Ganeshaクライアントが保持していたセッションと、そのセッションに関連付けられているすべての状態がMetadata Serverによってキャンセルされる場合があります。クラスタの残りが猶予期間に入る前にセッションの機能がキャンセルされた場合、サーバのクライアントは、その状態のすべてを回復できない可能性があります。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>詳細情報</title>

  <para>
   詳細については、<xref linkend="cha-ceph-nfsganesha"/>を参照してください。
  </para>
 </sect1>
</chapter>
