<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>DeepSea/Saltを使用した展開</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SaltとDeepSeaは、サーバインフラストラクチャの展開と管理に役立つコンポーネントの「スタック」<emphasis/>です。拡張性が非常に高く高速で、比較的簡単に運用を開始できます。Saltを使用してクラスタの展開を始める前に、以下の考慮事項をお読みください。
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis/>「Salt Minion」は、Salt Masterと呼ばれる専用のノードによって制御されるノードです。Salt Minionは、Ceph OSD、Ceph Monitor、Ceph Manager、Object Gateway、iSCSI Gateway、NFS Ganeshaなどの役割を持ちます。
   </para>
  </listitem>
  <listitem>
   <para>
    Salt Masterは専用のSalt Minionを実行します。これは特権タスク(キーの作成や権限付与、ミニオンへのキーのコピーなど)を実行するために必要で、その結果、リモートミニオンは特権タスクを実行しなくても済みます。
   </para>
   <tip>
    <title>1つのサーバで複数の役割を共有</title>
    <para>
     各役割を別個のノードに展開すると、Cephクラスタで最適なパフォーマンスを実現できます。しかし、実際の展開では、1つのノードを複数の役割のために共有しなければならない場合があります。パフォーマンスやアップグレード手順で問題が起きないようにするため、Ceph OSD、Metadata Server、またはCeph Monitorの役割は管理ノードに展開しないでください。
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    Salt Minionは、ネットワークでSalt Masterのホスト名を正しく解決する必要があります。Salt Minionは、デフォルトでは<systemitem>salt</systemitem>というホスト名を検索しますが、ネットワーク経由でアクセス可能なほかのホスト名を<filename>/etc/salt/minion</filename>ファイルで指定できます。<xref linkend="ceph-install-stack"/>を参照してください。
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>リリースノートの確認</title>

  <para>
   リリースノートには、旧リリースのSUSE Enterprise Storageからの変更点に関する追加情報が記載されています。リリースノートを参照して以下を確認します。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     使用しているハードウェアに特別な配慮が必要かどうか
    </para>
   </listitem>
   <listitem>
    <para>
     使用しているソフトウェアパッケージに大幅な変更があるかどうか
    </para>
   </listitem>
   <listitem>
    <para>
     インストールのために特別な予防措置が必要かどうか
    </para>
   </listitem>
  </itemizedlist>

  <para>
   リリースノートには、マニュアルに記載できなかった情報が記載されています。また、既知の問題に関する注意も記載されています。
  </para>

  <para>
   パッケージ <package>release-notes-ses</package>をインストールすると、リリースノートは、ローカルではディレクトリ<filename>/usr/share/doc/release-notes</filename>に、オンラインでは<link xlink:href="https://www.suse.com/releasenotes/"/>にあります。
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>DeepSeaの概要</title>

  <para>
   DeepSeaの目的は、管理者の時間を節約し、Cephクラスタの複雑な操作を確実に実行できるようにすることです。
  </para>

  <para>
   Cephは、設定を細かく変更可能なソフトウェアソリューションです。Cephはシステム管理者の自由度と対応能力の両方を向上させます。
  </para>

  <para>
   デモンストレーションにはCephの最小セットアップで十分ですが、これでは大量のノードがある場合に発揮されるCephの興味深い機能はわかりません。
  </para>

  <para>
   DeepSeaは、アドレスやデバイス名など、個々のサーバのデータを収集、保存します。Cephのような分散ストレージシステムでは、収集および保存すべきこのような項目が何百個も存在する可能性があります。手動で情報を収集して設定管理ツールにデータを入力するのは、たいへんな労力が必要なだけでなく、ミスも起きがちです。
  </para>

  <para>
   サーバの準備、設定の収集、およびCephの展開に必要な手順はほぼ同じです。ただし、これは別個の機能の管理には対応していません。日々の運用では、特定の機能に簡単にハードウェアを追加したり、ハードウェアを問題なく削除したりできる必要があります。
  </para>

  <para>
   DeepSeaは、管理者の決定事項を1つのファイルに統合するというストラテジーによって、こうした現実の課題に対応します。クラスタの割り当て、役割の割り当て、プロファイルの割り当てなどが決定事項に含まれます。DeepSeaは各タスクセットを収集してシンプルな目標にまとめます。それぞれの目標は「ステージ」です。<emphasis/>
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>DeepSeaのステージの説明</title>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ 0」 - 「準備」 - このステージ中に、必要な更新がすべて適用されます。システムが再起動することがあります。
    </para>
    <important>
     <title>管理ノード再起動後のステージ0の再実行</title>
     <para>
      ステージ0の間に、管理ノードは新しいカーネルバージョンをロードするために再起動するので、もう一度ステージ0を実行する必要があります。そうしないと、ミニオンがターゲットに設定されません。
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/>「ステージ1」 - <emphasis role="bold"/>「ディスカバリ」 - ここでは、クラスタ内のすべてのハードウェアが検出され、Cephの設定に必要な情報が収集されます。設定の詳細については、<xref linkend="deepsea-pillar-salt-configuration"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ2」 - 「設定」 - 設定データを特定のフォーマットで準備する必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ3」 - 「展開」 - 必須のCephサービスが含まれる基本的なCephクラスタを作成します。リストについては、<xref linkend="storage-intro-core-nodes"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/><emphasis role="bold"/>「ステージ4」 - 「サービス」 - このステージでは、iSCSI、Object Gateway、CephFSなど、Cephの追加機能をインストールできます。各機能はオプションです。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold"/>「ステージ5」 - 削除ステージ。このステージは必須ではなく、通常、初期セットアップ時には必要ありません。このステージでは、ミニオンの役割のほかにクラスタ設定も削除されます。このステージは、クラスタからストレージノードを削除する必要がある場合に実行する必要があります。詳細については、<xref linkend="salt-node-removing"/>を参照してください。
    </para>
   </listitem>
  </itemizedlist>



  <sect2 xml:id="deepsea-organisation-locations">
   <title>組織と重要な場所</title>
   <para>
    Saltには、マスタノードで使用される標準の場所と命名規則がいくつか設定されています。
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename></term>
     <listitem>
      <para>
       このディレクトリには、クラスタミニオンの設定データが保存されます。<emphasis/>「Pillar」は、すべてのクラスタミニオンにグローバル設定値を提供するためのインタフェースです。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename></term>
     <listitem>
      <para>
       このディレクトリには、Saltの状態ファイル(<emphasis/>「sls」ファイルとも呼びます)が保存されます。状態ファイルは、クラスタのあるべき状態を特定のフォーマットで記述したものです。

      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename></term>
     <listitem>
      <para>
       このディレクトリには、ランナと呼ばれるPythonスクリプトが保存されます。ランナはマスタノードで実行されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename></term>
     <listitem>
      <para>
       このディレクトリには、モジュールと呼ばれるPythonスクリプトが保存されます。モジュールはクラスタ内のすべてのミニオンに適用されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename></term>
     <listitem>
      <para>
       このディレクトリはDeepSeaによって使用されます。収集された設定データはここに保存されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename></term>
     <listitem>
      <para>
       DeepSeaによって使用されるディレクトリ。さまざまなフォーマットのslsファイルが保存されますが、各サブディレクトリにslsファイルが含まれます。各サブディレクトリには1種類のslsファイルのみが含まれます。たとえば、<filename>/srv/salt/ceph/stage</filename>には、<command>salt-run state.orchestrate</command>によって実行されるオーケストレーションファイルが含まれます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>ミニオンのターゲット設定</title>
   <para>
    DeepSeaのコマンドはSaltインフラストラクチャ経由で実行されます。<command>salt</command>コマンドを使用する場合、コマンドの対象にするSalt Minionのセットを指定する必要があります。ここでは、そのようなミニオンのセットを<command>salt</command>コマンドの<emphasis/>「ターゲット」と呼びます。以降のセクションでは、ミニオンのターゲットを設定するために使用できる方法について説明します。
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>ミニオン名の一致</title>
    <para>
     1つのミニオンまたは複数のミニオンのグループを名前で照合してターゲットに設定できます。通常、ミニオンの名前は、ミニオンが実行されているノードの短いホスト名です。これは、Saltの一般的なターゲット設定方法で、DeepSeaには関係ありません。グロブ、正規表現、またはリストを使用して、ミニオン名の範囲を制限できます。一般的な構文は次のとおりです。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>Cephのみのクラスタ</title>
     <para>
      ユーザの環境内のすべてのSalt Minionがそのユーザ自身のCephクラスタに属する場合は、<replaceable>target</replaceable>を「<literal>*</literal>」に置き換えて、<emphasis/>「すべて」の登録済みミニオンを含めて問題ありません。
     </para>
    </tip>
    <para>
     example.netドメイン内のすべてのミニオンに一致します(ミニオン名がその「完全な」ホスト名と同じであると想定しています)。
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     「web1」～「web5」のミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     正規表現を使用して、「web1-prod」と「web1-devel」の両方のミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     ミニオンの単純なリストに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     クラスタ内のすべてのミニオンに一致します。
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>DeepSeaグレインを使用したターゲット設定</title>
    <para>
     Saltによって管理される異種環境において、SUSE Enterprise Storage 6が他のクラスタソリューションと共にノードのサブセットに展開されている場合、DeepSeaステージ0を実行する前に「deepsea」グレインを適用して、関連するミニオンにマークを付ける必要があります。これにより、ミニオン名での一致が難しい環境で、DeepSeaミニオンを簡単にターゲットに設定できます。
    </para>
    <para>
     「deepsea」グレインをミニオンのグループに適用するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     「deepsea」グレインをミニオンのグループから削除するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     「deepsea」グレインを関連するミニオンに適用した後、次のようにミニオンをターゲットに設定できます。
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     次のコマンドも同等の処理を行います。
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title><option>deepsea_minions</option>オプションの設定</title>
    <para>
     DeepSeaの展開では、<option>deepsea_minions</option>オプションのターゲットを設定する必要があります。DeepSeaは、このオプションを使用して、ステージの実行中にミニオンに命令します(詳細については、<xref linkend="deepsea-stage-description"/>を参照してください)。
    </para>
    <para>
     <option>deepsea_minions</option>オプションを設定または変更するには、Salt Masterで<filename>/srv/pillar/ceph/deepsea_minions.sls</filename>ファイルを編集し、次の行を追加または置換します。
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title><option>deepsea_minions</option>のターゲット</title>
     <para>
      <option>deepsea_minions</option>オプションの<replaceable>target</replaceable>として、「<xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/>」および「<xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>」の両方のターゲット設定方法を使用できます。
     </para>
     <para>
      クラスタ内のすべてのSalt Minionに一致します。
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      「deepsea」グレインを使用してすべてのミニオンに一致します。
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>その他の情報</title>
    <para>
     Saltインフラストラクチャを使用して、より高度な方法でミニオンをターゲットに設定できます。「deepsea-minions」のマニュアルページでは、DeepSeaのターゲット設定が詳しく説明されています(<command>man 7 deepsea_minions</command>)。
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>クラスタの展開</title>

  <para>
   クラスタの展開プロセスには複数のフェーズがあります。まず、Saltを設定してクラスタのすべてのノードを準備してから、Cephを展開および設定する必要があります。
  </para>

  <tip xml:id="dev-env">
   <title>OSDプロファイルを定義せずにモニタノードを展開</title>
   <para>
    <xref linkend="policy-role-assignment"/>で説明されている、OSDのストレージ役割の定義をスキップして、最初にCeph Monitorノードを展開する必要がある場合、<option>DEV_ENV</option>変数を設定することにより実行できます。
   </para>
   <para>
    これにより、<filename>role-storage/</filename>ディレクトリが存在しなくてもモニターを展開できるほか、ストレージの役割、モニターの役割、およびマネージャの役割が少なくとも「1つ」<emphasis/>あればCephクラスタを展開できます。
   </para>
   <para>
    この環境変数を設定するには、<filename>/srv/pillar/ceph/stack/global.yml</filename>ファイルで設定してグローバルに有効にするか、現在のシェルセッションに対してのみ設定します。
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
   <para>
    たとえば、次の内容で<filename>/srv/pillar/ceph/stack/global.yml</filename>を作成できます。
   </para>
<screen>DEV_ENV: <replaceable>True</replaceable></screen>
  </tip>

  <para>
   次の手順では、クラスタの準備について詳しく説明します。
  </para>

  <procedure>
   <step>
    <para>
     クラスタの各ノードにSUSE Linux Enterprise Server 15 SP1とSUSE Enterprise Storage 6拡張機能をインストールして登録します。
    </para>
   </step>
   <step>
    <para>
     既存のソフトウェアリポジトリを一覧にして、適切な製品がインストールおよび登録されていることを確認します。<command>zypper lr -E</command>を実行して、出力を次のリストと比較します。
    </para>
<screen>
 SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates
</screen>
   </step>
   <step>
    <para>
     ネットワークを設定します。各ノードでDNS名が適切に解決されるようにする設定も含まれます。Salt MasterとすべてのSalt Minionは、お互いをホスト名で解決する必要があります。ネットワークの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_network_yast.html"/>を参照してください。DNSサーバの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_dns.html"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     1つ以上のタイムサーバ/プールを選択し、ローカル時刻をそれらと同期します。システムを起動するたびに時刻同期サービスが有効になることを確認します。時刻同期を設定するには、<command>yast ntp-client</command>コマンドを使用できます。このコマンドは、 <package>yast2-ntp-client</package> パッケージにあります。
    </para>
    <tip>
     <para>
      仮想マシンは信頼できるNTPソースではありません。
     </para>
    </tip>
    <para>
     NTPの設定の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     Salt Masterノードに<literal>salt-master</literal>パッケージと<literal>salt-minion</literal>パッケージをインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     <systemitem>salt-master</systemitem>サービスが有効になっていて起動していることを確認します。必要であれば、サービスを有効にして起動します。
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     ファイアウォールを使用する場合は、Salt Masterノードのポート4505と4506がすべてのSalt Minionノードに対して開いていることを確認します。これらのポートが閉じている場合は、<command>yast2 firewall</command>コマンドを使用してポートを開き、<guimenu>SaltStack</guimenu>サービスを許可できます。
    </para>
    <warning>
     <title>ファイアウォールがアクティブな場合、DeepSeaのステージが失敗する</title>
     <para>
      ファイアウォールがアクティブな場合(かつ設定されている場合)、DeepSeaの展開ステージが失敗します。ステージを正しく実行するには、次のコマンドを実行してファイアウォールをオフにします。
     </para>
<screen>
    <prompt>root # </prompt>systemctl stop firewalld.service
</screen>
     <para>
      または、<filename>/srv/pillar/ceph/stack/global.yml</filename>の<option>FAIL_ON_WARNING</option>オプションを「False」に設定します。
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     パッケージ<literal>salt-minion</literal>をすべてのミニオンノードにインストールします。
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     各ノードの「完全修飾ドメイン名」<emphasis/>を他のすべてのノードがパブリックネットワークのIPアドレスに解決できることを確認します。
    </para>
   </step>
   <step>
    <para>
     すべてのミニオン(マスタミニオンを含む)を、マスタに接続するように設定します。ホスト名<literal>salt</literal>でSalt Masterに接続できない場合は、ファイル<filename>/etc/salt/minion</filename>を編集するか、次の内容で新しいファイル<filename>/etc/salt/minion.d/master.conf</filename>を作成します。
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     上で説明した設定ファイルを変更した場合は、すべてのSalt MinionのSaltサービスを再起動します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     すべてのノードで<systemitem>salt-minion</systemitem>サービスが有効になっていて起動していることを確認します。必要であれば、次のコマンドを使用して有効にして起動します。
    </para>
<screen><prompt>root # </prompt>systemctl enable salt-minion.service
<prompt>root # </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     各Salt Minionの指紋を確認して、指紋が一致する場合、Salt Master上のすべてのSaltキーを受諾します。
    </para>
    <note>
     <para>
      Salt Minionの指紋が空に戻る場合は、Salt MinionがSalt Masterの設定を持っていて、Salt Masterと通信できることを確認します。
     </para>
    </note>
    <para>
     各ミニオンの指紋を表示します。
    </para>
<screen><prompt>root@master # </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     すべてのSalt Minionの指紋を収集した後、Salt Master上の、受諾されていない全ミニオンキーの指紋を一覧にします。
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     ミニオンの指紋が一致する場合は、それらを受諾します。
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     キーが受諾されたことを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     SUSE Enterprise Storage 6を展開する前に、すべてのディスクを手動で消去します。必ず「X」を正しいディスク文字に置き換えてください。
    </para>
    <substeps>
     <step>
      <para>
       指定したディスクを使用しているすべてのプロセスを停止します。
      </para>
     </step>
     <step>
      <para>
       ディスクのパーティションがマウントされているかどうかを確認し、必要に応じてアンマウントします。
      </para>
     </step>
     <step>
      <para>
       ディスクがLVMによって管理されている場合は、LVMインフラストラクチャ全体を無効にして削除します。詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/cha_lvm.html"/>を参照してください。
      </para>
     </step>
     <step>
      <para>
       ディスクがMD RAIDの一部である場合は、RAIDを無効化します。詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/part_software_raid.html"/>を参照してください。
      </para>
     </step>
     <step>
      <tip>
       <title>サーバの再起動</title>
       <para>
        次の手順の実行中に「partition in use (パーティションが使用中です)」や「kernel can not be updated with the new partition table (カーネルを新しいパーティションテーブルで更新できません)」などのエラーメッセージが表示される場合、サーバを再起動してください。
       </para>
      </tip>
      <para>
       各パーティションの先頭を消去します(<systemitem class="username">root</systemitem>として)。
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       ドライブの先頭を消去します。
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
</screen>
     </step>
     <step>
      <para>
       ドライブの最後を消去します。
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
</screen>
     </step>
     <step>
      <para>
       次のコマンドを使用して、ドライブが空(GPT構造なし)であることを確認します。
      </para>
<screen>
<prompt>root # </prompt>parted -s /dev/sdX print free
</screen>
      <para>
       プロンプトまたは
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     オプションで、 <package>deepsea</package> パッケージがインストールされる前に、クラスタのネットワーク設定を事前設定する必要がある場合は、<filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>を手動で作成して、<option>cluster_network:</option>オプションと<option>public_network:</option>オプションを設定します。このファイルは、 <package>deepsea</package>をインストールした後も上書きされないことに注意してください。
    </para>
    <tip>
     <title>IPv6の有効化</title>
     <para>
      IPv6ネットワークのアドレス指定を有効にする必要がある場合は、<xref linkend="ds-modify-ipv6"/>を参照してください。
     </para>
    </tip>
   </step>
   <step>
    <para>
     DeepSeaをSalt Masterノードにインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     <option>master_minion</option>パラメータの値は、Salt Master上の<filename>/etc/salt/minion_id</filename>ファイルから動的に導出されます。検出された値を上書きする必要がある場合は、ファイル<filename>/srv/pillar/ceph/stack/global.yml</filename>を編集して、関連する値を設定します。
    </para>
<screen>
master_minion: <replaceable>MASTER_MINION_NAME</replaceable>
</screen>
    <para>
     ほかのホスト名でもSaltマスタにアクセスできる場合は、<command>salt-key -L</command>コマンドで返されるSalt Minion名をストレージクラスタに対して使用します。<emphasis/>「ses」ドメインでSalt Masterにデフォルトのホスト名(「salt」<emphasis/>)を使用していた場合、次のようなファイルになります。
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   続いてCephを展開して設定します。別途明記されていない限り、すべての手順が必須です。
  </para>

  <note>
   <title>Saltコマンドの規則</title>
   <para>
    <command>salt-run state.orch</command>は2つの方法で実行できます。1つは「stage.<replaceable>STAGE_NUMBER</replaceable>」を使用する方法で、もう1つはステージの名前を使用する方法です。どちらの表記でも効果は同じで、どちらのコマンドを使用するかは完全に好みの問題です。
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>展開ステージの実行</title>
   <step>
    <para>
     Cephクラスタに属するSalt Minionが<filename>/srv/pillar/ceph/deepsea_minions.sls</filename>の<option>deepsea_minions</option>オプションで正しくターゲットに設定されていることを確認します。詳細については、「<xref linkend="ds-minion-targeting-dsminions"/>」を参照してください。
    </para>
   </step>
   <step>
    <para>
     デフォルトでは、DeepSeaは、Ceph Monitor、Ceph Manager、およびCeph OSDのノード上でアクティブな調整済みプロファイルを使用してCephクラスタを展開します。場合によっては、調整済みプロファイルを使用せずに展開しなければならないことがあります。これを実行するには、DeepSeaステージを実行する前に、<filename>/srv/pillar/ceph/stack/global.yml</filename>に次の行を入力します。
    </para>
<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>
   </step>
   <step>
    <para>
     <emphasis/>オプション: <filename>/var/lib/ceph/</filename>のBtrfsサブボリュームを作成します。この手順は、DeepSeaステージ0の前に実行する必要があります。既存のディレクトリを移行する方法、または詳細については、<xref linkend="storage-tips-ceph-btrfs-subvol"/>を参照してください。
    </para>
    <para>
     Salt Minionのそれぞれに次のコマンドを適用します。
    </para>
<screen><prompt>root@master # </prompt>salt 'MONITOR_NODES' saltutil.sync_all
<prompt>root@master # </prompt>salt 'MONITOR_NODES' state.apply ceph.subvolume
</screen>
    <note>
     <para>
      Ceph.subvolumeコマンドは、<filename>/var/lib/ceph</filename>を<filename>@/var/lib/ceph</filename> Btrfsサブボリュームとして作成します。
     </para>
    </note>
    <para>
     新しいサブボリュームがマウントされ、<literal>/etc/fstab</literal>が更新されます。
    </para>
   </step>
   <step>
    <para>
     クラスタを準備します。詳細については、<xref linkend="deepsea-stage-description"/>を参照してください。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>DeepSea CLIを使用したステージの実行または監視</title>
     <para>
      DeepSea CLIを使用して、ステージの実行進行状況をリアルタイムで把握できます。そのためには、DeepSea CLIをモニタリングモードで実行するか、DeepSea CLIを通じてステージを直接実行します。詳細については、<xref linkend="deepsea-cli"/>を参照してください。
     </para>
    </note>
   </step>
   <step>
    <para>
     ディスカバリステージでは、すべてのミニオンからデータを収集して、ディレクトリ<filename>/srv/pillar/ceph/proposals</filename>に保存される設定フラグメントを作成します。データはYAMLフォーマットで*.slsファイルまたは*.ymlファイルに保存されます。
    </para>
    <para>
     次のコマンドを実行して、ディスカバリステージをトリガします。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     前のコマンドが正常に完了した後、<filename>/srv/pillar/ceph/proposals</filename>に<filename>policy.cfg</filename>ファイルを作成します。詳細については、<xref linkend="policy-configuration"/>を参照してください。
    </para>
    <tip>
     <para>
      クラスタのネットワーク設定を変更する必要がある場合は、<filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>を編集して、<literal>cluster_network:</literal>および<literal>public_network:</literal>で始まる各行を調整します。
     </para>
    </tip>
   </step>
   <step>
    <para>
     設定ステージにより、<filename>policy.cfg</filename>ファイルが解析され、含まれるファイルが最終的な形態にマージされます。クラスタと役割に関連する内容は<filename>/srv/pillar/ceph/cluster</filename>に保存され、Ceph固有の内容は<filename>/srv/pillar/ceph/stack/default</filename>に配置されます。
    </para>
    <para>
     次のコマンドを実行して、設定ステージをトリガします。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     この設定手順には数秒かかる場合があります。コマンドが完了した後、指定したミニオンのPillarデータを表示できます(たとえば、<literal>ceph_minion1</literal>、<literal>ceph_minion2</literal>という名前のミニオンなど)。このためには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <tip>
     <title>OSDのレイアウトの変更</title>
     <para>
      OSDのデフォルトのレイアウトを変更し、ドライブグループの設定を変更する場合は、<xref linkend="ds-drive-groups"/>で説明されている手順に従います。
     </para>
    </tip>
    <note>
     <title>デフォルト値の上書き</title>
     <para>
      コマンドが完了したら、すぐにデフォルト設定を参照して、ニーズに合わせて変更できます。詳細については、<xref linkend="ceph-deploy-ds-custom"/>を参照してください。
     </para>
    </note>
   </step>
   <step>
    <para>
     これで展開ステージを実行できます。このステージでは、Pillarが検証され、Ceph MonitorおよびCeph OSDデーモンが開始されます。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy</screen>
    <para>
     このコマンドには数分かかる場合があります。コマンドが失敗した場合は、問題を修正して前のステージをもう一度実行する必要があります。コマンドが成功したら、次のコマンドを実行して状態を確認します。
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     Cephクラスタ展開の最後の手順は、「サービス」<emphasis/>ステージです。ここでは、現在サポートされているサービス(iSCSI Gateway、CephFS、Object Gateway、およびNFS Ganesha)のインスタンスを生成します。このステージで、必要なプール、権限付与キーリング、および起動サービスが作成されます。ステージを開始するには、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     または
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     セットアップによっては、このコマンドの実行には数分かかる場合があります。
    </para>
   </step>
   <step>
    <para>
     続行する前に、Cephテレメトリモジュールを有効にすることを強くお勧めします。情報および手順の詳細については、<xref linkend="mgr-modules-telemetry"/>を参照してください。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSeaは、ユーザがステージを監視または実行しながら、実行進行状況をリアルタイムで可視化できるCLI (コマンドラインインタフェース)ツールも備えています。パッケージ <package>deepsea-cli</package> がインストールされていることを確認してから、<command>deepsea</command>実行可能ファイルを実行します。
  </para>

  <para>
   ステージの実行進行状況を可視化するため、次の2つのモードがサポートされています。
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>DeepSea CLIのモード</title>
   <listitem>
    <para>
     モニタリングモード<emphasis role="bold"/>: 別のターミナルセッションで発行された<command>salt-run</command>コマンドによってトリガされたDeepSeaステージの実行進行状況を可視化します。
    </para>
   </listitem>
   <listitem>
    <para>
     スタンドアロンモード<emphasis role="bold"/>: DeepSeaステージを実行すると同時に、その構成要素の実行中にリアルタイムで各手順を可視化します。
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>DeepSea CLIコマンド</title>
   <para>
    DeepSea CLIコマンドは、Salt Masterノードで、<systemitem class="username">root</systemitem>特権でのみ実行できます。
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI: モニタモード</title>
   <para>
    進行状況モニタは、他のターミナルセッションで<command>salt-run state.orch</command>コマンドを使用して実行されているステージで何が実行されているかをリアルタイムで詳しく可視化します。
   </para>
   <tip>
    <title>新しいターミナルセッションでのモニターの起動</title>
    <para>
     <emphasis/><command>salt-run state.orch</command>を実行する「前に」、ステージの実行開始を検出できるようモニターを新しいターミナルウィンドウで起動しておく必要があります。
    </para>
   </tip>
   <para>
    <command>salt-run state.orch</command>コマンドの発行後にモニタを起動した場合、実行進行状況は表示されません。
   </para>
   <para>
    次のコマンドを実行して、モニタモードを開始できます。
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    <command>deepsea monitor</command>コマンドで利用可能なコマンドラインオプションの詳細については、次のマニュアルページを参照してください。
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI: スタンドアロンモード</title>
   <para>
    スタンドアロンモードでは、DeepSea CLIを使用してDeepSeaステージを実行し、その実行をリアルタイムで表示できます。
   </para>
   <para>
    DeepSea CLIからDeepSeaステージを実行するコマンドは次の形式です。
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    ここで、<replaceable>stage-name</replaceable>は、Saltオーケストレーション状態ファイルが参照される方法に対応します。たとえば、ステージ<emphasis role="bold"/>「deploy」は、<filename>/srv/salt/ceph/stage/deploy</filename>にあるディレクトリに対応しており、「ceph.stage.deploy」<emphasis role="bold"/>として参照されます。
   </para>
   <para>
    このコマンドは、DeepSeaステージ(またはDeepSeaオーケストレーション状態ファイル)を実行するSaltベースのコマンドの代わりになります。
   </para>
   <para>
    コマンド<command>deepsea stage run ceph.stage.0</command>は、<command>salt-run state.orch ceph.stage.0</command>と同等です。
   </para>
   <para>
    <command>deepsea stage run</command>コマンドで受け付けられる利用可能なコマンドラインオプションの詳細については、次のマニュアルページを参照してください。
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    次の図に、<emphasis role="underline">ステージ2</emphasis>実行時のDeepSea CLIの出力の例を示します。
   </para>
   <figure>
    <title>DeepSea CLIのステージ実行進行状況の出力</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>DeepSea CLI <command>stage run</command>のエイリアス</title>
    <para>
     Saltの上級ユーザ向けに、DeepSeaのステージを実行するためのエイリアスもサポートされています。このエイリアスは、ステージの実行に使用するSaltコマンド(たとえば<command>salt-run state.orch <replaceable>stage-name</replaceable></command>)をDeepSea CLIのコマンドとして取ります。
    </para>
    <para>
     例:
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>設定とカスタマイズ</title>

  <sect2 xml:id="policy-configuration">
   <title><filename>policy.cfg</filename>ファイル</title>
   <para>
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>設定ファイルは、個々のクラスタノードの役割を決定するために使用されます。たとえば、Ceph OSDまたはCeph Monitorとして動作するノードなどです。目的のクラスタセットアップを反映するには、<filename>policy.cfg</filename>を編集します。セクションの順序は任意ですが、古い行の内容の一致するキーは、追加した行の内容で上書きされます。
   </para>
   <tip>
    <title><filename>policy.cfg</filename>の例</title>
    <para>
     <filename>/usr/share/doc/packages/deepsea/examples/</filename>に、完全なポリシーファイルの例がいくつかあります。
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>クラスタの割り当て</title>
    <para>
     「cluster」<emphasis role="bold"/>セクションで、クラスタのミニオンを選択します。すべてのミニオンを選択することも、ミニオンをブラックリスト/ホワイトリストに入れることもできます。次に、「ceph」<emphasis role="bold"/>という名前のクラスタの例を示します。
    </para>
    <para>
     <emphasis role="bold"/>「すべての」ミニオンを含めるには、次の行を追加します。
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     <emphasis role="bold"/>特定のミニオンを「ホワイトリスト」に入れるには、次の行を追加します。
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     ミニオンのグループをホワイトリストに入れるには、シェルグロブ展開による一致を使用できます。
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     <emphasis role="bold"/>ミニオンを「ブラックリスト」に入れるには、該当のミニオンを<literal>unassigned</literal>に設定します。
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>役割割り当て</title>
    <para>
     このセクションでは、クラスタノードへの「役割」の割り当てについて詳しく説明します。この文脈の「役割」とは、Ceph Monitor、Object Gateway、iSCSI Gatewayなど、ノードで実行する必要があるサービスを意味します。役割は自動的には割り当てられません。<command>policy.cfg</command>に追加した役割のみが展開されます。
    </para>
    <para>
     割り当ては次のパターンに従います。
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     ここで、各項目は次の意味と値を持ちます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable>は、「master」「admin」「mon」「mgr」「storage」「mds」「igw」「rgw」「ganesha」「grafana」または「prometheus」のいずれかです。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable>は、.slsファイルまたは.ymlファイルの相対ディレクトリパスです。.slsファイルの場合は通常<filename>cluster</filename>ですが、.ymlファイルは<filename>stack/default/ceph/minions</filename>にあります。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable>は、Salt状態ファイルまたはYAML設定ファイルです。通常は、Salt Minionのホスト名で構成されます。たとえば、<filename>ses5min2.yml</filename>です。シェルグロブ展開を使用して、さらに詳細に一致させることができます。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     次に各役割の例を示します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis/>「master」 - このノードは、すべてのCephクラスタに対する管理キーリングを持ちます。現在のところ、1つのCephクラスタのみがサポートされます。<emphasis/>「master」役割は必須であるため、必ず次のような行を追加してください。
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「admin」 - このミニオンは管理キーリングを持ちます。この役割は次のように定義します。
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mon」 - このミニオンは、Cephクラスタにモニターサービスを提供します。この役割には、割り当てられたミニオンのアドレスが必要です。SUSE Enterprise Storage 5から、パブリックアドレスは動的に計算されるようになり、Salt Pillarに記述する必要はなくなりました。
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       次の例は、モニターの役割をミニオンのグループに割り当てます。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mgr」 - クラスタ全体からすべての状態情報を収集するCeph Managerデーモン。Ceph Monitorの役割を展開する予定のすべてのミニオンに展開します。
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「storage」 - この役割を使用してストレージノードを指定します。
      </para>
<screen>role-storage/cluster/data*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「mds」 - このミニオンは、CephFSをサポートするためのメタデータサービスを提供します。
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「igw」 - このミニオンは、iSCSI Gatewayとして機能します。この役割には、割り当てられたミニオンのアドレスが必要です。そのため、<filename>stack</filename>ディレクトリのファイルも含める必要があります。
      </para>
<screen>role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「rgw」 - このミニオンは、Object Gatewayとして機能します。
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「ganesha」 - このミニオンは、NFS Ganeshaサーバとして機能します。「ganesha」の役割には、クラスタ内で「rgw」または「mds」の役割が必要です。そうしないと、ステージ3で検証に失敗します。
      </para>
<screen>role-ganesha/cluster/ganesha*.sls</screen>
      <para>
       NFS Ganeshaを正常にインストールするには、追加の設定が必要です。NFS Ganeshaを使用する場合は、ステージ2および4を実行する前に、<xref linkend="cha-as-ganesha"/>を読んでください。ただし、NFS Ganeshaは後からインストールできます。
      </para>
      <para>
       場合によっては、NFS Ganeshaノードに対してカスタムの役割を定義すると便利です。詳細については、<xref linkend="ceph-nfsganesha-customrole"/>を参照してください。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis/>「grafana」、<emphasis/>「prometheus」 - このノードは、Cephダッシュボードに、Prometheusのアラートに基づくGrafanaチャートを追加します。詳細な説明については、<xref linkend="ceph-dashboard"/>を参照してください。
      </para>
<screen>role-grafana/cluster/grafana*.sls</screen>
<screen>role-prometheus/cluster/prometheus*.sls</screen>
     </listitem>
    </itemizedlist>
    <note>
     <title>クラスタノードの複数の役割</title>
     <para>
      1つノードに複数の役割を割り当てることができます。たとえば、モニターノードに「mds」の役割を割り当てることができます。
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>共通設定</title>
    <para>
     共通設定のセクションには、「ディスカバリ(ステージ1)」<emphasis/>中に生成された設定ファイルが記述されています。これらの設定ファイルには、<literal>fsid</literal>や<literal>public_network</literal>などのパラメータが保存されています。必要なCeph共通設定を含めるには、次の行を追加します。
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>アイテムのフィルタリング</title>
    <para>
     場合によっては、*.slsグロブを使用して特定のディレクトリからすべてのファイルを含めるのは現実的ではありません。<filename>policy.cfg</filename>ファイルパーサは次のフィルタを理解します。
    </para>
    <warning>
     <title>高度な手法</title>
     <para>
      このセクションでは、上級ユーザ向けのフィルタリング手法について説明します。正しく使用しないと、フィルタリングによって問題が発生する可能性があります。たとえば、ノードの番号付けが変更される場合があります。
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        sliceフィルタは、「start」<emphasis/>から「end-1」<emphasis/>までのアイテムのみを含める場合に使用します。指定したディレクトリ内のアイテムは英数字順にソートされる点に注意してください。次の行は、<filename>role-mon/cluster/</filename>サブディレクトリから3～5番目のファイルを含めます。
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        正規表現フィルタは、指定した表現に一致するアイテムのみを含める場合に使用します。次に例を示します。
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title><filename>policy.cfg</filename>ファイルの例</title>
    <para>
     次に、基本的な<filename>policy.cfg</filename>ファイルの例を示します。
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# STORAGE
role-storage/cluster/ses-example-[5,6,7,8].sls <co xml:id="co-policy-storage"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>
</screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       Cephクラスタにすべてのミニオンを含めるよう指定します。Cephクラスタに含めたくないミニオンがある場合は、次の行を使用します。
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       最初の行は、すべてのミニオンを未割り当てとしてマークします。2番目の行は、「ses-example-*.sls」に一致するミニオンを上書きして、それらをCephクラスタに割り当てます。
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       「examplesesadmin」という名前のミニオンが「master」役割を持ちます。言い換えると、このミニオンはクラスタに対する管理キーを取得します。
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       「sesclient*」に一致するすべてのミニオンも管理キーを取得します。
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       「ses-example-[123]」に一致するすべてのミニオン(おそらく、ses-example-1、ses-example-2、およびses-example-3の3つのミニオン)がMONノードとして設定されます。
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       「ses-example-[123]」に一致するすべてのミニオン(この例ではすべてのMONノード)がMGRノードとして設定されます。
      </para>
     </callout>
     <callout arearefs="co-policy-storage">
      <para>
       「ses-example-[5,6,7,8]」に一致するすべてのミニオンがストレージノードとして設定されます。
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       ミニオン「ses-example-4」がMDS役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       ミニオン「ses-example-4」がIGW役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       ミニオン「ses-example-4」がRGW役割を持ちます。
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       <option>fsid</option>、<option>public_network</option>などの共通設定パラメータでデフォルト値をそのまま使用することを意味します。
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       <option>fsid</option>、<option>public_network</option>などの共通設定パラメータでデフォルト値をそのまま使用することを意味します。
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="ds-drive-groups">
   <title>DriveGroups</title>
   <para>
    <emphasis/>「DriveGroups」では、CephクラスタのOSDのレイアウトを指定します。レイアウトは、1つのファイル<filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>で定義します。
   </para>
   <para>
    管理者は、相互に関連するOSDのグループ(ソリッドステートとスピナ上に展開されるハイブリッドOSD)を手動で指定するか、同じ展開オプション(同一のオプション、たとえば、同じオブジェクトストア、同じ暗号化オプション、スタンドアロンOSDなど)を共有する必要があります。デバイスが明示的に一覧にされないようにするため、DriveGroupsでは、<command>ceph-volume</command>インベントリレポートで選択した数個のフィールドに対応するフィルタ項目のリストを使用します。最も単純な場合、ここには「rotational」フラグ(すべてのソリッドステートドライブはdb_devices、すべての回転型ドライブはデータデバイス)か、または「model」の文字列やサイズなど、より複雑な内容を指定できます。DeepSeaでは、これらのDriveGroupsを実際のデバイスリストに変換してユーザが調べられるようにするコードを提供します。
   </para>
   <para>
    DriveGroups設定時の基本的なワークフローを示す単純な手順は、次のようになります。
   </para>
   <procedure>
    <step>
     <para>
      <command>ceph-volume</command>コマンドで表示されるディスクのプロパティを調べます。DriveGropsで受け付けられるのは、これらのプロパティのみです。
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.details
</screen>
    </step>
    <step>
     <para>
      <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> YAMLファイルを開き、ニーズに合わせて調整します。<xref linkend="ds-drive-groups-specs"/>を参照してください。必ず、タブではなくスペースを使用してください。詳細な例については、<xref linkend="ds-drive-groups-examples"/>を参照してください。次の例では、使用可能なすべてのドライブをOSDとしてCephに含めます。
     </para>
<screen>
default_drive_group_name:
  target: '*'
  data_devices:
    all: true
</screen>
    </step>
    <step>
     <para>
      新しいレイアウトを確認します。
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
     <para>
      このランナは、DriveGroupsに基づいて、一致するディスクの構造を返します。結果に問題がある場合は、前の手順を繰り返します。
     </para>
     <tip>
      <title>詳細レポート</title>
      <para>
       <command>disks.list</command>ランナのほかに、<command>disks.report</command>ランナもあります。これは、次にDeepSeaステージ3が呼び出されたときの処理について詳細レポートを出力します。
      </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
     </tip>
    </step>
    <step>
     <para>
      OSDを展開します。次にDeepSeaステージ3が呼び出されたときに、ドライブグループの指定に従ってOSDディスクが展開されます。
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ds-drive-groups-specs">
    <title>仕様</title>
    <para>
     <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>では、次のオプションを指定できます。
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  db_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  wal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    <para>
     FileStoreのセットアップでは、<filename>drive_groups.yml</filename>は次のようになります。
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  journal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  format: filestore
  encryption: True
</screen>
   </sect3>
   <sect3>
    <title>一致するディスクデバイス</title>
    <para>
     次のフィルタを使用して指定を記述できます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       ディスクモデル別。
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       ディスクベンダー別。
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>小文字のベンダー文字列</title>
       <para>
        <replaceable>DISK_VENDOR_STRING</replaceable>は常に小文字で指定してください。
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       ディスクが回転型かどうか。SSDとNVMEのドライブは回転型ではありません。
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       OSDで使用可能な「すべての」ドライブを使用してノードを展開します。<emphasis/>
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       また、一致するディスクの数を制限します。
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>サイズによるデバイスのフィルタリング</title>
    <para>
     ディスクデバイスをサイズでフィルタできます(正確なサイズ、またはサイズの範囲)。<option>size:</option>パラメータには、次の形式の引数を指定できます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' -正確にこのサイズのディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - この範囲内のサイズのディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - サイズが10GB以下のディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - サイズが40GB以上のディスクを含めます。
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>ディスクサイズによる一致</title>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <note>
     <title>引用符が必要</title>
     <para>
      区切り文字「:」を使用する場合は、サイズを引用符で囲む必要があります。そうしないと、「:」記号は新しい設定のハッシュであると解釈されます。
     </para>
    </note>
    <tip>
     <title>単位のショートカット</title>
     <para>
      ギガバイト(G)の代わりに、メガバイト(M)やテラバイト(T)単位でもサイズを指定できます。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>例</title>
    <para>
     このセクションでは、さまざまなOSDセットアップの例を示します。
    </para>
    <example>
     <title>単純なセットアップ</title>
     <para>
      この例では、同じセットアップを使用する2つのノードについて説明します。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      対応する<filename>drive_groups.yml</filename>ファイルは次のようになります。
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ
   </screen>
     <para>
      このような設定は単純で有効です。問題は、管理者が将来、別のベンダーのディスクを追加することがあっても、そられのディスクが含まれない点です。この設定を向上させるには、ドライブのコアプロパティのフィルタを減らします。
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
   </screen>
     <para>
      前の例では、回転型デバイスはすべて「データデバイス」として宣言し、非回転型デバイスはすべて「共有デバイス」(wal、db)として使用します。
     </para>
     <para>
      2TBを超えるドライブが常に低速のデータデバイスであることがわかっている場合は、サイズでフィルタできます。
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <example>
     <title>詳細セットアップ</title>
     <para>
      この例では、2つの別個のセットアップについて説明します。20台のHDDで2台のSSDを共有するセットアップと、10台のSSDで2台のNVMeを共有するセットアップです。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      このようなセットアップは、次のような2つのレイアウトで定義できます。
     </para>
<screen>
drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
</screen>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB
</screen>
    </example>
    <example>
     <title>不均一なノードを使用した詳細セットアップ</title>
     <para>
      前の例では、すべてのノードに同じドライブがあることを想定しています。ただし、常にこれが当てはまるとは限りません。
     </para>
     <para>
      ノード1～5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      ノード6～10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      レイアウトに「target」キーを使用して、特定のノードをターゲットに設定できます。Saltのターゲット表記を使用すると、内容をシンプルに保つことができます。
     </para>
<screen>
drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
</screen>
     <para>
      続いて以下を設定します。
     </para>
<screen>
drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>エキスパートセットアップ</title>
     <para>
      前の事例はすべて、WALとDBが同じデバイスを使用することを想定していました。ただし、WALを専用のデバイスに展開することもできます。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>複雑な(可能性が低い)セットアップ</title>
     <para>
      次のセットアップでは、以下を定義してみます。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1つのNVMeを利用する20台のHDD
       </para>
      </listitem>
      <listitem>
       <para>
        1台のSSD (db)と1つのNVMe (wal)を利用する2台のHDD
       </para>
      </listitem>
      <listitem>
       <para>
        1つのNVMeを利用する8台のSSD
       </para>
      </listitem>
      <listitem>
       <para>
        2台SSDスタンドアロン(暗号化)
       </para>
      </listitem>
      <listitem>
       <para>
        1台のHDDはスペアで、展開しない
       </para>
      </listitem>
     </itemizedlist>
     <para>
      使用するドライブの概要は次のとおりです。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      DriveGroupsの定義は次のようになります。
     </para>
<screen>
drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True
 </screen>
     <para>
      ファイルが上から下へ解析されると、HDDが1台残ります。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2>
   <title>カスタム設定を使用した<filename>ceph.conf</filename>の調整</title>
   <para>
    <filename>ceph.conf</filename>設定ファイルにカスタム設定を記述する場合は、<xref linkend="ds-custom-cephconf"/>で詳細を参照してください。
   </para>
  </sect2>
 </sect1>
</chapter>
