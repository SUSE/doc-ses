<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_tips.xml" version="5.0" xml:id="storage.tips">
 <title>ヒント</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>○</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  この章には、Cephクラスタのパフォーマンス向上に役立つ情報と、クラスタの設定方法のヒントが記載されています。
 </para>
 <sect1 xml:id="tips.scrubbing">
  <title>スクラブの調整</title>

  <para>
   デフォルトでは、Cephは軽量スクラブ(詳細については、<xref linkend="scrubbing"/>を参照)を毎日、詳細スクラブを週1回実行します。「軽量」<emphasis/>スクラブは、オブジェクトのサイズとチェックサムを調べて、配置グループが同じオブジェクトデータを保存していることを確認します。「詳細」<emphasis/>スクラブは、オブジェクトの内容とレプリカの内容を調べて、実際の内容が同じであることを確認します。データ整合性チェックには、スクラブ手順中にクラスタのI/O負荷が増加するという代償があります。
  </para>

  <para>
   デフォルト設定では、Ceph OSDは負荷が高い時間帯など不適切な時間にもスクラブを開始できます。スクラブ操作が顧客の操作と衝突すると、顧客においてレイテンシやパフォーマンスの低下が発生する場合があります。Cephには、低負荷の期間またはピーク時以外の時間帯にスクラブを制限できるスクラブ設定が複数用意されています。
  </para>

  <para>
   クラスタの負荷が日中に高く夜間に低い場合は、スクラブを夜間の時間帯(午後11時～午前6時など)に制限することを検討します。
  </para>

<screen>
[osd]
osd_scrub_begin_hour = 23
osd_scrub_end_hour = 6
</screen>

  <para>
   時間制限ではスクラブスケジュールを効果的に決定できない場合は、<option>osd_scrub_load_threshold</option>オプションの使用を検討します。デフォルト値は0.5ですが、低負荷の条件に合わせて変更できます。
  </para>

<screen>
[osd]
osd_scrub_load_threshold = 0.25
</screen>
 </sect1>
 <sect1 xml:id="tips.stopping_osd_without_rebalancing">
  <title>リバランスなしでのOSDの停止</title>

  <para>
   保守のために定期的にOSDを停止しなければならないことがあります。大量のデータ転送を避けるため、CRUSHが自動的にクラスタをリバランスしないようにする場合、まずクラスタを<literal>noout</literal>に設定します。
  </para>

<screen>
<prompt>root@minion &gt; </prompt>ceph osd set noout
</screen>

  <para>
   クラスタが<literal>noout</literal>に設定されたら、保守作業が必要な障害ドメイン内にあるOSDの停止を開始できます。
  </para>

<screen>
<prompt>root@minion &gt; </prompt>systemctl stop ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   詳細情報については、<xref linkend="ceph.operating.services.individual"/>を参照してください。
  </para>

  <para>
   保守が完了したら、OSDをもう一度起動します。
  </para>

<screen>
<prompt>root@minion &gt; </prompt>systemctl start ceph-osd@<replaceable>OSD_NUMBER</replaceable>.service
</screen>

  <para>
   OSDサービスが起動したら、クラスタの<literal>noout</literal>を設定解除します。
  </para>

<screen>
<prompt>root@minion &gt; </prompt>ceph osd unset noout
</screen>
 </sect1>
 <sect1 xml:id="Cluster_Time_Setting">
  <title>ノードの時刻同期</title>

  <para>
   Cephでは、特定のノード間で時刻が正確に同期している必要があります。独自のNTPサーバを使用してノードを設定する必要があります。すべてのntpdインスタンスがリモートのパブリックタイムサーバを指すことはできますが、Cephではお勧めしません。このような設定にすると、クラスタ内の各ノードが専用のNTPデーモンを持ち、そのデーモンが相当なホップ数離れた3～4台の一連のタイムサーバとインターネット上で継続的に通信することになります。このソリューションでは多大なレイテンシ変動が発生し、そのためにクロックの誤差を0.05秒未満(Ceph Monitorで必要)に抑えることは困難または不可能です。
  </para>

  <para>
   したがって、クラスタ全体で1台のマシンをNTPサーバとして使用します。そうすれば、NTPサーバのntpdインスタンスはリモートの(パブリック)NTPサーバを指すことも、独自のタイムソースを使用することもできます。そのうえで、すべてのノードのntpdインスタンスがこのローカルサーバを指します。このソリューションには、不要なネットワークトラフィックやクロックスキューが解消される、パブリックNTPサーバの負荷が減少するといった複数の利点があります。NTPサーバの設定方法の詳細については、『<link xlink:href="https://www.suse.com/documentation/sled11/book_sle_admin/data/cha_netz_xntp.html">SUSE Linux Enterprise Server Administration Guide</link>』を参照してください。
  </para>

  <para>
   その後、クラスタの時刻を変更するため、次の操作を行います。
  </para>

  <important>
   <title>時刻の設定</title>
   <para>
    たとえば夏時間から標準時間に変わった場合など、時刻を戻さなければならない状況が発生することがあります。クラスタのダウン時間より長く時刻を戻すことはお勧めしません。時刻を進めても問題は発生しません。
   </para>
  </important>

  <procedure>
   <title>クラスタの時刻同期</title>
   <step>
    <para>
     Cephクラスタにアクセスするすべてのクライアントを停止します。特にiSCSIを使用しているクライアントは必ず停止します。
    </para>
   </step>
   <step>
    <para>
     Cephクラスタをシャットダウンします。各ノードで、次のコマンドを実行します。
    </para>
<screen>systemctl stop ceph.target</screen>
    <note>
     <para>
      CephとSUSE OpenStack Cloudを使用する場合は、SUSE OpenStack Cloudも停止します。
     </para>
    </note>
   </step>
   <step>
    <para>
     NTPサーバが正しく設定されていて、すべてのntpdデーモンがローカルネットワーク内の1つ以上のソースから時刻を取得していることを確認します。
    </para>
   </step>
   <step>
    <para>
     NTPサーバに正しい時刻を設定します。
    </para>
   </step>
   <step>
    <para>
     NTPが適切に稼働および動作していることを確認し、すべてのノードで次のコマンドを実行します。
    </para>
<screen>status ntpd.service</screen>
    <para>
     または
    </para>
<screen>ntpq -p</screen>
   </step>
   <step>
    <para>
     すべてのモニタリングノードを起動して、クロックスキューがないことを確認します。
    </para>
<screen>systemctl start <replaceable>target</replaceable></screen>
   </step>
   <step>
    <para>
     すべてのOSDノードを起動します。
    </para>
   </step>
   <step>
    <para>
     Cephのその他のサービスを起動します。
    </para>
   </step>
   <step>
    <para>
     SUSE OpenStack Cloudを使用している場合は起動します。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.unbalanced">
  <title>不均衡なデータ書き込みの確認</title>

  <para>
   データが各OSDに均等に書き込まれている場合、クラスタは均衡状態と見なされます。クラスタ内の各OSDには「重み」<emphasis/>が割り当てられます。重みは相対的な数字で、関連するOSDに書き込む必要があるデータの量をCephに指示します。重みが大きいほど、多くのデータが書き込まれます。OSDの重みが0の場合、そのOSDにデータは書き込まれません。あるOSDの重みが他のOSDと比べて比較的大きい場合、大部分のデータがそのOSDに書き込まれ、クラスタが不均衡になります。
  </para>

  <para>
   不均衡なクラスタはパフォーマンスが低く、大きい重みを持つOSDが突然クラッシュした場合、大量のデータを他のOSDに移動しなければならず、これによってクラスタの速度も低下します。
  </para>

  <para>
   これを避けるには、OSDでデータ書き込みの量を定期的に確認する必要があります。この量が特定のルールセットで指定されたOSDグループの容量の30～50%である場合、OSDの重みを変更する必要があります。個々のディスクを確認して、他のディスクより満杯になるのが早い(または一般的に低速である)ディスクを確認し、その重みを減らします。十分なデータが書き込まれていないOSDでも同じことが有効で、この場合は、重みを増やしてCephが書き込むデータを増やします。次の例では、ID 13のOSDの重みを確認して、重みを3から3.05に変更します。
  </para>

<screen>$ ceph osd tree | grep osd.13
 13  3                   osd.13  up  1

 $ ceph osd crush reweight osd.13 3.05
 reweighted item id 13 name 'osd.13' to 3.05 in crush map

 $ ceph osd tree | grep osd.13
 13  3.05                osd.13  up  1</screen>

  <para/>

  <tip>
   <title>使用率によるOSDの重みの変更</title>
   <para>
    <command>ceph osd reweight-by-utilization</command>
    <replaceable>threshold</replaceable>コマンドは、きわめて過剰に使用されているOSDの重みを下げるプロセスを自動化します。デフォルトでは、平均使用量が120%に達したOSDの重みが下方に調整されますが、しきい値が指定されている場合は、代わりにその割合が使用されます。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="storage.tips.ceph_btrfs_subvol">
  <title>/var/lib/cephのBtrfsサブボリューム</title>

  <para>
   SUSE Linux Enterpriseは、デフォルトでBtrfsパーティションにインストールされます。ディレクトリ<filename>/var/lib/ceph</filename>はBtrfsスナップショットおよびロールバックから除外する必要があります。これは特にノードでMONが実行されている場合に該当します。DeepSeaでは、このパスにサブボリュームを設定できる<literal>fs</literal>ランナが提供されています。
  </para>

  <sect2 xml:id="storage.tips.ceph_btrfs_subvol.req-new">
   <title>新規インストールの要件</title>
   <para>
    初めてクラスタを設定する場合、このDeepSeaランナを使用する前に、次の要件を満たす必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SaltとDeepSeaがこのマニュアルに従って適切にインストールされていて動作している。
     </para>
    </listitem>
    <listitem>
     <para>
      <command>salt-run state.orch ceph.stage.0</command>を起動して、SaltおよびDeepSeaのすべてのモジュールをMinionに同期済みである。
     </para>
    </listitem>
    <listitem>
     <para>
      Cephがまだインストールされておらず、そのためceph.stage.3がまだ実行されておらず、<filename>/var/lib/ceph</filename>がまだ存在しない。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="storage.tips.ceph_btrfs_subvol.req-existing">
   <title>既存のインストールの要件</title>
   <para>
    クラスタがすでにインストール済みの場合、このDeepSeaランナを使用する前に、次の要件を満たす必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      ノードがSUSE Enterprise Storageにアップグレードされていて、クラスタがDeepSeaによって制御されている。
     </para>
    </listitem>
    <listitem>
     <para>
      Cephクラスタが動作していて正常な状態である。
     </para>
    </listitem>
    <listitem>
     <para>
      アップグレードプロセスによってSaltおよびDeepSeaのモジュールがすべてのMinionノードに同期済みである。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="storage.tips.ceph_btrfs_subvol.automatic">
   <title>自動セットアップ</title>
   <procedure>
    <step>
     <para>
      Salt Masterで次のコマンドを実行します。
     </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.migrate.subvolume</screen>
     <para>
      このコマンドは、既存の<filename>/var/lib/ceph</filename>ディレクトリがないノードでは、次の処理を一度に1ノードずつ実行します。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        <filename>/var/lib/ceph</filename>を<literal>@/var/lib/ceph</literal> Btrfsサブボリュームとして作成する。
       </para>
      </listitem>
      <listitem>
       <para>
        新しいサブボリュームをマウントし、<filename>/etc/fstab</filename>を適切に更新する。
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/ceph</filename>のコピーオンライトを無効にする。
       </para>
      </listitem>
     </itemizedlist>
     <para>
      このコマンドは、Cephがすでにインストール済みのノードでは、次の処理を一度に1ノードずつ実行します。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        実行中のCephプロセスを終了する。
       </para>
      </listitem>
      <listitem>
       <para>
        ノードのOSDをアンマウントする。
       </para>
      </listitem>
      <listitem>
       <para>
        <literal>@/var/lib/ceph</literal> Btrfsサブボリュームを作成して、<filename>/var/lib/ceph</filename>の既存のデータを移行する。
       </para>
      </listitem>
      <listitem>
       <para>
        新しいサブボリュームをマウントし、<filename>/etc/fstab</filename>を適切に更新する。
       </para>
      </listitem>
      <listitem>
       <para>
        <filename>/var/lib/ceph/*</filename>のコピーオンライトを、<filename>/var/lib/ceph/osd/*</filename>を除いて無効にする。
       </para>
      </listitem>
      <listitem>
       <para>
        OSDを再マウントする。
       </para>
      </listitem>
      <listitem>
       <para>
        Cephデーモンを再起動する。
       </para>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.tips.ceph_btrfs_subvol.manually">
   <title>手動セットアップ</title>
   <para>
    これには新しい<literal>fs</literal>ランナを使用します。
   </para>
   <procedure>
    <step>
     <para>
      すべてのノードで<filename>/var/lib/ceph</filename>の状態を検査し、どのように操作を続行するかについての提案を出力します。
     </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> fs.inspect_var</screen>
     <para>
      次のいずれかのコマンドが返されます。
     </para>
<screen>salt-run fs.create_var
salt-run fs.migrate_var
salt-run fs.correct_var_attrs</screen>
    </step>
    <step>
     <para>
      前の手順で返されたコマンドを実行します。
     </para>
     <para>
      いずれかのノードでエラーが発生した場合、他のノードに対する実行は停止され、ランナは実行した手順を元に戻そうとします。問題があるMinionのログファイルを参照して、問題を判断します。問題が解決したら、ランナを再実行できます。
     </para>
    </step>
   </procedure>
   <para>
    コマンド<command>salt-run fs.help</command>は、<literal>fs</literal>モジュール用のランナおよびモジュールの全コマンドのリストを表示します。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.srv_maint.fds_inc">
  <title>ファイル記述子の増加</title>

  <para>
   OSDデーモンでは、Cephクラスタの均衡を維持するために読み込み/書き込み操作が重要です。ほとんどの場合、OSDデーモンは、読み込みおよび書き込み対象のファイルを同時に複数開いておく必要があります。OSレベルでは、同時に開いているファイルの最大数を「ファイル記述子の最大数」と呼びます。
  </para>

  <para>
   OSDでファイル記述子が不足するのを防ぐには、OSのデフォルト値を上書きして、<filename>/etc/ceph/ceph.conf</filename>で数値を指定できます。次に例を示します。
  </para>

<screen>max_open_files = 131072</screen>

  <para>
   <option>max_open_files</option>を変更した後、関連するCephノードでOSDサービスを再起動する必要があります。
  </para>
 </sect1>
 <sect1 xml:id="bp.osd_on_exisitng_partitions">
  <title>OSDジャーナルを含むOSDに既存のパーティションを使用する方法</title>

  <important>
   <para>
    このセクションでは、ストレージのエキスパートと開発者のみが検討する必要がある高度なトピックについて説明します。これは通常、標準のサイズではないOSDジャーナルを使用しているときに必要になります。OSDパーティションのサイズが10GB未満の場合、その初期重みは0に丸められます。これではデータが配置されないため、重みを増やす必要があります。満杯を超えたジャーナルについては一切責任を負いません。
   </para>
  </important>

  <para>
   既存のディスクパーティションをOSDノードとして使用する必要がある場合、OSDジャーナルとデータパーティションはGPTパーティションテーブルに存在する必要があります。
  </para>

  <para>
   OSDパーティションに正しいパーティションタイプを設定する必要があります。これは、<systemitem>udev</systemitem>がパーティションを正しく認識して、所有権を<literal>ceph:ceph</literal>に設定するようにするためです。
  </para>

  <para>
   たとえば、ジャーナルパーティション<filename>/dev/vdb1</filename>およびデータパーティション<filename>/dev/vdb2</filename>のパーティションタイプを設定するには、次のコマンドを実行します。
  </para>

<screen>sudo sgdisk --typecode=1:45b0969e-9b03-4f30-b4c6-b4b80ceff106 /dev/vdb
sudo sgdisk --typecode=2:4fbd7e29-9d25-41b8-afd0-062c0ceff05d /dev/vdb</screen>

  <tip>
   <para>
    Cephのパーティションテーブルのタイプは、<filename>/usr/lib/udev/rules.d/95-ceph-osd.rules</filename>に一覧にされています。
   </para>
<screen>cat /usr/lib/udev/rules.d/95-ceph-osd.rules
# OSD_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="4fbd7e29-9d25-41b8-afd0-062c0ceff05d", \
  OWNER="ceph", GROUP="ceph", MODE="660"

# JOURNAL_UUID
ACTION=="add", SUBSYSTEM=="block", \
  ENV{DEVTYPE}=="partition", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER:="ceph", GROUP:="ceph", MODE:="660", \
  RUN+="/usr/sbin/ceph-disk --log-stdout -v trigger /dev/$name"
ACTION=="change", SUBSYSTEM=="block", \
  ENV{ID_PART_ENTRY_TYPE}=="45b0969e-9b03-4f30-b4c6-b4b80ceff106", \
  OWNER="ceph", GROUP="ceph", MODE="660"
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage.admin.integration">
  <title>仮想化ソフトウェアとの統合</title>

  <sect2 xml:id="storage.bp.integration.kvm">
   <title>CephクラスタへのKVMディスクの保存</title>
   <para>
    KVMで動作する仮想マシンのディスクイメージを作成してCephプール内に保存し、オプションで既存のイメージの内容をそのディスクイメージに変換できます。その後、<command>qemu-kvm</command>を使用して、クラスタに保存したディスクイメージを利用して仮想マシンを実行できます。詳細については、<xref linkend="cha.ceph.kvm"/>を参照してください。 
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.libvirt">
   <title>Cephクラスタへの<systemitem class="library">libvirt</systemitem>ディスクの保存</title>
   <para>
    KVMの場合と同様に(<xref linkend="storage.bp.integration.kvm"/>を参照)、Cephを使用して、<systemitem class="library">libvirt</systemitem>で動作する仮想マシンを保存できます。これには、KVM、Xen、LXCなど、<systemitem class="library">libvirt</systemitem>でサポートされているあらゆる仮想化ソリューションを実行できるという利点があります。詳細については、<xref linkend="cha.ceph.libvirt"/>を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.integration.xen">
   <title>CephクラスタへのXenディスクの保存</title>
   <para>
    Cephを使用してXenディスクを保存する方法の1つは、<xref linkend="cha.ceph.libvirt"/>で説明されているように<systemitem class="library">libvirt</systemitem>を利用する方法です。
   </para>
   <para>
    Xenが<systemitem>rbd</systemitem> Block Deviceドライバと直接通信するようにするオプションもあります。
   </para>
   <procedure>
    <step>
     <para>
      Xen用に準備されたディスクイメージがない場合は、新規に作成します。
     </para>
<screen>rbd create myimage --size 8000 --pool mypool</screen>
    </step>
    <step>
     <para>
      プール<literal>mypool</literal>内のイメージを一覧にして、新しいイメージがそこに存在するかどうかを確認します。
     </para>
<screen>rbd list mypool</screen>
    </step>
    <step>
     <para>
      <literal>myimage</literal>イメージを<systemitem>rbd</systemitem>カーネルモジュールにマップすることにより、新しいBlock Deviceを作成します。
     </para>
<screen>sudo rbd map --pool mypool myimage</screen>
     <tip>
      <title>ユーザ名と認証</title>
      <para>
       ユーザ名を指定するには、<option>--id <replaceable>user-name</replaceable></option>を使用します。さらに、<systemitem>cephx</systemitem>認証を使用する場合は、秘密も指定する必要があります。秘密は、キーリング、または秘密が含まれるファイルから取得できます。
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
      <para>
       または
      </para>
<screen>sudo rbd map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
     </tip>
    </step>
    <step>
     <para>
      すべてのマップ済みデバイスを一覧にします。
     </para>
<screen><command>rbd showmapped</command>
 id pool   image   snap device
 0  mypool myimage -    /dev/rbd0</screen>
    </step>
    <step>
     <para>
      これで、このデバイスを仮想マシンとして実行するためのディスクとして使用するようXenを設定できます。たとえば、<command>xl</command>形式のドメイン設定ファイルに次の行を追加できます。
     </para>
<screen>disk = [ '/dev/rbd0,,sda', '/dev/cdrom,,sdc,cdrom' ]</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="storage.bp.net.firewall">
  <title>Cephのファイアウォール設定</title>

  <warning>
   <title>ファイアウォールがアクティブな場合、DeepSeaのステージが失敗する</title>
   <para>
    ファイアウォールがアクティブな場合(設定されていても)、DeepSeaの展開ステージが失敗します。ステージを正しく実行するには、次のコマンドを実行してファイアウォールをオフにします。
   </para>
<screen>
<prompt>root@master # </prompt>systemctl stop SuSEfirewall2.service
</screen>
   <para>
    または、<filename>/srv/pillar/ceph/stack/global.yml</filename>の<option>FAIL_ON_WARNING</option>オプションを「False」に設定します。
   </para>
<screen>
FAIL_ON_WARNING: False
</screen>
  </warning>

  <para>
   ネットワーククラスタの通信をSUSE Firewallで保護することをお勧めします。この設定ファイルを編集するには、<menuchoice><guimenu>YaST</guimenu><guimenu>Security and Users (セキュリティとユーザ)</guimenu><guimenu>Firewall (ファイアウォール)</guimenu><guimenu>Allowed Services (許可するサービス)</guimenu></menuchoice>を選択します。
  </para>

  <para>
   次に、Ceph関連サービスと通常使用されるポートの番号のリストを示します。
  </para>

  <variablelist>
   <varlistentry>
    <term>Ceph Monitor</term>
    <listitem>
     <para>
      <guimenu>Ceph MON</guimenu>サービスまたはポート6789 (TCP)を有効にします。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Ceph OSDまたはMetadata Server</term>
    <listitem>
     <para>
      <guimenu>Ceph OSD/MDS</guimenu>サービスまたはポート6800～7300 (TCP)を有効にします。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>iSCSI Gateway</term>
    <listitem>
     <para>
      ポート3260 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Object Gateway</term>
    <listitem>
     <para>
      Object Gatewayが通信するポートを開きます。これは、<filename>/etc/ceph.conf</filename>の<literal>rgw frontends =</literal>から始まる行で設定されています。デフォルトはHTTPの場合は80、HTTPSの場合は443 (TCP)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFS Ganesha</term>
    <listitem>
     <para>
      デフォルトでは、NFS Ganeshaはポート2049 (NFSサービス、TCP)と875 (rquotaサポート、TCP)を使用します。NFS Ganeshaのデフォルトポートの変更の詳細については、<xref linkend="ganesha.nfsport"/>を参照してください。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Apacheベースのサービス(openATTIC、SMT、SUSE Managerなど)</term>
    <listitem>
     <para>
      HTTPの場合はポート80、HTTPSの場合はポート443 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SSH</term>
    <listitem>
     <para>
      ポート22 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NTP</term>
    <listitem>
     <para>
      ポート123 (UDP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Salt</term>
    <listitem>
     <para>
      ポート4505および4506 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Grafana</term>
    <listitem>
     <para>
      ポート3000 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Prometheus</term>
    <listitem>
     <para>
      ポート9100 (TCP)を開きます。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage.bp.network_test">
  <title>ネットワークパフォーマンスのテスト</title>

  <para>
   ネットワークパフォーマンスをテストするために、DeepSeaの<literal>net</literal>ランナは次のコマンドを提供しています。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     すべてのノードに対する単純なping:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.ping
Succeeded: 9 addresses from 9 minions average rtt 1.35 ms</screen>
   </listitem>
   <listitem>
    <para>
     すべてのノードに対するジャンボping:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.jumbo_ping
Succeeded: 9 addresses from 9 minions average rtt 2.13 ms</screen>
   </listitem>
   <listitem>
    <para>
     帯域幅のテスト:
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> net.iperf
Fastest 2 hosts:
    |_
      - 192.168.58.106
      - 2981 Mbits/sec
    |_
      - 192.168.58.107
      - 2967 Mbits/sec
Slowest 2 hosts:
    |_
      - 192.168.58.102
      - 2857 Mbits/sec
    |_
      - 192.168.58.103
      - 2842 Mbits/sec</screen>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="storage.bd.replacing_disk">
  <title>ストレージディスクの交換</title>

  <para>
   Cephクラスタ内のストレージディスクを交換する必要がある場合、クラスタの完全な動作中に交換できます。交換を行うと、データ転送が一時的に増加します。
  </para>

  <para>
   ディスクが完全に故障した場合、Cephは少なくとも、障害が発生したディスクの容量と同じ量のデータを再書き込みする必要があります。このプロセス中に冗長性が失われるのを避けるためにディスクを適切に退避させて再追加する場合、再書き込みされるデータの量は2倍の大きさになります。新しいディスクと交換用ディスクのサイズが異なる場合、すべてのOSDの使用量を均一にするため、一部のデータが追加で再分散されます。
  </para>
 </sect1>
</chapter>
