<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>ストレージプールの管理</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Cephはデータをプール内に保存します。プールは、オブジェクトを保存するための論理グループです。プールを作成せずに初めてクラスタを展開した場合、Cephはデフォルトのプールを使用してデータを保存します。次の重要な特徴はCephプールに関連するものです。
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    「災害耐性」<emphasis/>: いくつのOSD、バケット、またはリーフに障害が発生してもデータが失われないようにするかを設定できます。複製プールの場合、これはオブジェクトに必要なコピー/レプリカの数になります。新しいプールは、デフォルトのレプリカ数が3に設定された状態で作成されます。イレージャコーディングプールの場合、これはコーディングチャンクの数になります(すなわち、イレージャコードプロファイルで<emphasis>m=2</emphasis>)。
   </para>
  </listitem>
  <listitem>
   <para>
    「配置グループ」<emphasis/>: 複数のOSDにわたるプールにデータを保存するための内部的なデータ構造です。CephがデータをPGに保存する方法はCRUSHマップで定義します。プールの配置グループの数をその作成時に設定できます。一般的な設定では、OSDあたり約100個の配置グループを使用し、大量のコンピューティングリソースを使用することなく最適なバランスを提供します。複数のプールを設定する場合は、プールとクラスタ全体の両方にとって適切な数の配置グループを設定するよう注意してください。「<link xlink:href="https://ceph.com/pgcalc/">Ceph PGs per Pool Calculator</link>」を参照してください。
   </para>
  </listitem>
  <listitem>
   <para>
    「CRUSHルール」<emphasis/>: プールにデータを保存する場合、オブジェクトとそのレプリカ(またはイレージャコーディングプールの場合はチャンク)は、プールにマップされたCRUSHルールに従って配置されます。ご使用のプールに対してカスタムCRUSHルールを作成できます。
   </para>
  </listitem>
  <listitem>
   <para>
    「スナップショット」<emphasis/>: <command>ceph osd pool mksnap</command>を使用してスナップショットを作成すると、特定のプールのスナップショットが効果的に作成されます。
   </para>
  </listitem>
 </itemizedlist>
 <para>
  データをプールに編成するために、プールを一覧、作成、および削除できます。各プールの使用量統計を表示することもできます。
 </para>
 <sect1 xml:id="ceph-pools-associate">
  <title>プールとアプリケーションの関連付け</title>

  <para>
   プールを使用する前に、プールをアプリケーションに関連付ける必要があります。CephFSで使用されるプール、またはObject Gatewayによって自動的に作成されるプールは自動的に関連付けられます。
  </para>

  <para>
   それ以外の場合は、自由な形式のアプリケーション名を手動でプールに関連付けることができます。
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>デフォルトのアプリケーション名</title>
   <para>
    アプリケーション名として、CephFSは<literal>cephfs</literal>、RADOS Block Deviceは<literal>rbd</literal>、Object Gatewayは<literal>rgw</literal>をそれぞれ使用します。
   </para>
  </tip>

  <para>
   1つのプールを複数のアプリケーションに関連付けて、各アプリケーションで専用のメタデータを使用できます。次のコマンドを使用して、指定したプールのアプリケーションのメタデータを表示できます。
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-pools-operate">
  <title>プールの操作</title>

  <para>
   このセクションでは、プールで基本的なタスクを実行するための実用的な情報を紹介します。プールの一覧、作成、削除の方法と、プールの統計の表示方法、プールのスナップショットの管理方法を理解できます。
  </para>

  <sect2>
   <title>プールの一覧</title>
   <para>
    クラスタのプールを一覧にするには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool ls</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-add-pool">
   <title>プールの作成</title>
   <para>
    オブジェクトのコピーを複数保持することによってOSDの損失から回復するには「replicated」、一種の汎用RAID5/6機能を利用するには「erasure」を指定して、プールを作成できます。必要な未加工ストレージは、複製プールでは多く、イレージャコーディングプールでは少なくなります。デフォルトは「replicated」です。
   </para>
   <para>
    複製プールを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    イレージャコーディングプールを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    OSDあたりの配置グループの制限を超える場合、<command>ceph osd pool create</command>は失敗する可能性があります。この制限はオプション<option>mon_max_pg_per_osd</option>で設定します。
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       プールの名前。固有である必要があります。このオプションは必須です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの合計数。このオプションは必須です。デフォルト値は8です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       配置目的の配置グループの合計数。配置グループ分割シナリオ以外では、配置グループの合計数と等しい必要があります。このオプションは必須です。デフォルト値は8です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       このプールのCRUSHルールセットの名前。指定したルールセットが存在しない場合、複製プールの作成は-ENOENTで失敗します。複製プールの場合、 <varname>osd pool default crush replicated ruleset</varname> 設定変数によって指定されたルールセットです。このルールセットは存在している必要があります。イレージャプールでは、デフォルトのイレージャコードプロファイルを使用する場合は「erasure-code」、それ以外の場合は<replaceable>POOL_NAME</replaceable>です。このルールセットは、まだ存在しない場合は暗黙的に作成されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       イレージャコーディングプール専用。イレージャコードプロファイルを使用します。<command>osd erasure-code-profile set</command>で定義した既存のプロファイルである必要があります。
      </para>
      <para>
       プールを作成する際、配置グループの数を適切な値に設定します。OSDあたりの配置グループの合計数も考慮してください。配置グループは計算コストが高いため、多数の配置グループが含まれるプールが大量にあると(たとえば、50個のプールと、それぞれに100個の配置グループ)、パフォーマンスが低下します。
      </para>
      <para>
       プールに適した配置グループ数の計算の詳細については、「<xref linkend="op-pgs"/>」を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       このプールの想定オブジェクト数。この値を(負の<option>filestore merge threshold</option>とともに)設定すると、プールの作成時にPGフォルダが分割されます。これにより、ランタイム時のフォルダ分割によるレイテンシの影響が避けられます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>プールクォータの設定</title>
   <para>
    最大バイト数、またはプールあたりのオブジェクトの最大数に対してプールクォータを設定できます。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    次に例を示します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    クォータを削除するには、値を0に設定します。
   </para>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-del-pool">
   <title>プールの削除</title>
   <warning>
    <title>プールの削除は元に戻せない</title>
    <para>
     プールには重要なデータが収められている場合があります。プールを削除すると、プール内のすべてのデータが消え、回復する方法はありません。
    </para>
   </warning>
   <para>
    誤ってプールを削除することはきわめて危険であるため、Cephには、プールの削除を防止するメカニズムが2つ実装されています。プールを削除するには、両方のメカニズムを無効にする必要があります。
   </para>
   <para>
    1つ目のメカニズムは<literal>NODELETE</literal>フラグです。各プールにこのフラグがあり、デフォルト値は「false」です。プールのこのフラグのデフォルト値を確認するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    <literal>nodelete: true</literal>が出力される場合、次のコマンドを使用してフラグを変更しない限り、プールを削除できません。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    2つ目のメカニズムは、クラスタ全体の設定パラメータ<option>mon allow pool delete</option>で、デフォルトは「false」です。つまり、デフォルトではプールを削除できません。表示されるエラーメッセージは次のとおりです。
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    この安全設定に関係なくプールを削除するには、<option>mon allow pool delete</option>を一時的に「true」に設定してプールを削除し、その後、パラメータを「false」に戻します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>cephadm@adm &gt; </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    <command>injectargs</command>コマンドを実行すると、次のメッセージが表示されます。
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    これは単にコマンドが正常に実行されたことを確認するものです。エラーではありません。
   </para>
   <para>
    作成したプール用に独自のルールセットとルールを作成した場合、プールが必要なくなったらルールセットとルールを削除することをお勧めします。
   </para>
  </sect2>

  <sect2>
   <title>プールの名前変更</title>
   <para>
    プールの名前を変更するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    プールの名前を変更する場合に、認証ユーザ用のプールごとのケーパビリティがあるときは、そのユーザのケーパビリティを新しいプール名で更新する必要があります。
   </para>
  </sect2>

  <sect2>
   <title>プール統計の表示</title>
   <para>
    プールの使用量統計を表示するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rados df
POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
.rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
</screen>
   <para>
    個々の列の説明は次のとおりです。
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       プールによって使用されているバイトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       プールに保存されているオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       プールに保存されているクローンの数。スナップショットが作成されてオブジェクトに書き込まれる場合、元のオブジェクトは変更されずにそのクローンが作成されるため、元のスナップショットオブジェクトの内容は変更されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>COPIES</term>
     <listitem>
      <para>
       オブジェクトレプリカの数。たとえば、レプリケーション係数3の複製プールに「x」個のオブジェクトがある場合、コピーの数は通常、3 * x個になります。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       プライマリOSDにコピーがみつからないときに劣化状態になっているオブジェクトの数(すべてのコピーが存在するわけではない)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       見つからないオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       劣化したオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       このプールに対して要求された読み込み操作の合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       このプールから読み込まれたバイトの合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       このプールに対して要求された書き込み操作の合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       プールに書き込まれたバイトの合計数。同じオブジェクトに何度も書き込むことができるため、これはプールの使用率と同じではないことに注意してください。その結果、プールの使用率は同じままであっても、プールに書き込まれたバイト数は大きくなります。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       圧縮データに割り当てられているバイトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       圧縮データが非圧縮時に使用するバイトの数。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>プールの値の取得</title>
   <para>
    プールから値を取得するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    <xref linkend="ceph-pools-values"/>に示すキーと、次のキーの値を取得できます。
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。有効な範囲は<literal>pg_num</literal>以下です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>プールのすべての値</title>
    <para>
     特定のプールに関連するすべての値を一覧にするには、次のコマンドを実行します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> all
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>プールの値の設定</title>
   <para>
    プールに値を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    次のキーの値を設定できます。
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       プール内のオブジェクトのレプリカ数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       I/Oに必要なレプリカの最小数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       確認済みであるもののコミットされていない要求の再生をクライアントに許可する秒数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの数。新しいクラスタにOSDを追加する場合は、新しいOSDの対象に指定されたすべてのプール上にある配置グループの値を確認します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       クラスタ内のオブジェクト配置のマッピングに使用するルールセット。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       指定したプールに対してHASHPSPOOLフラグを設定(1)または設定解除(0)します。このフラグを有効にすると、PGをOSDに効率的に分散するためにアルゴリズムが変更されます。HASHPSPOOLフラグがデフォルトの0に設定されたプールでこのフラグを有効にすると、クラスタは、すべてのPGをもう一度正しく配置するためにバックフィルを開始します。これはクラスタに多大なI/O負荷をかける可能性があるので、非常に負荷が高い運用クラスタでは、0～1のフラグを有効にしないでください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       プールの削除を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       プールの<option>pg_num</option>および<option>pgp_num</option>の変更を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       プールのサイズの変更を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       指定したプールに対して<literal>WRITE_FADVISE_DONTNEED</literal>フラグを設定/設定解除します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub、nodeep-scrub</term>
     <listitem>
      <para>
       I/Oの一時的な高負荷を解決するため、特定のプールに対してデータの(詳細)スクラブを無効にします。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの追跡を有効にします。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。このオプションに設定できる値は、<literal>bloom</literal>、<literal>explicit_hash</literal>、または<literal>explicit_object</literal>です。デフォルトは<literal>bloom</literal>で、他の値はテスト専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       キャッシュプールに関して保存するヒットセットの数。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの期間(秒単位)。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       bloomヒットセットタイプの誤検知確率。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。有効な範囲は0.0～1.0で、デフォルトは<literal>0.05</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       キャッシュ階層化のヒットセットを作成する際に、GMT (グリニッジ標準時)のタイムスタンプを使用するようOSDに強制します。これにより、異なるタイムゾーンにあるノードが同じ結果を返すようにします。デフォルトは<literal>1</literal>です。この値は変更できません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをバッキングストレージプールにフラッシュします。デフォルトは<literal>0.4</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをより高速なバッキングストレージプールにフラッシュします。デフォルトは<literal>0.6</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる未変更オブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは未変更(クリーン)オブジェクトをキャッシュプールから削除します。デフォルトは<literal>0.8</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       <option>max_bytes</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       <option>max_objects</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       連続する2つの<literal>hit_set</literal>間の温度減衰率。デフォルトは<literal>20</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       温度を計算するために、<literal>hit_set</literal>内で最大<literal>N</literal>個の出現をカウントします。デフォルトは<literal>1</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールからストレージプールへフラッシュするまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールから削除するまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       イレージャコーディングプールでこのフラグが有効な場合、読み込み要求は、すべてのシャードに対してサブ読み込みを発行し、クライアントの要求を実行するためにデコードする十分なシャードを受け取るまで待機します。イレージャプラグインが<emphasis>jerasure</emphasis>および<emphasis>isa</emphasis>の場合、最初の<literal>K</literal>個の応答が返された時点で、これらの応答からデコードされたデータを使用してただちにクライアントの要求が実行されます。このアプローチでは、CPUの負荷が増え、ディスク/ネットワークの負荷は減ります。現在のところ、このフラグはイレージャコーディングプールでのみサポートされます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       クラスタの負荷が低い場合にプールをスクラブする最小間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_min_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       クラスタの負荷に関係なくプールをスクラブする最大間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_max_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       プールの「詳細」<emphasis/>スクラブの間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_deep_scrub</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>オブジェクトレプリカの数の設定</title>
   <para>
    複製プール上のオブジェクトレプリカの数を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable>にはオブジェクトそのものも含まれます。たとえば、オブジェクトとそのオブジェクトの2つのコピーで合計3つのオブジェクトインスタンスが必要な場合、3を指定します。
   </para>
   <warning>
    <title>3つ未満のレプリカを設定しない</title>
    <para>
     <replaceable>num-replicas</replaceable>を2に設定した場合、データのコピーは「1つ」<emphasis/>だけになります。1つのオブジェクトインスタンスが失われた場合、たとえば回復中の前回のスクラブ以降に、他のコピーが壊れていないことを信頼する必要があります(詳細については、<xref linkend="scrubbing"/>を参照)。
    </para>
    <para>
     プールを1つのレプリカに設定することは、プール内にデータオブジェクトのインスタンスが「1つ」<emphasis/>だけ存在することを意味します。OSDに障害発生すると、データは失われます。レプリカが1つのプールの使用法としては、一時データを短時間保存することが考えられます。
    </para>
   </warning>
   <tip>
    <title>3つを超えるレプリカの設定</title>
    <para>
     1つのプールに対して4つのレプリカを設定すると、信頼性が25%向上します。
    </para>
    <para>
     2つのデータセンターの場合、各データセンターで2つのコピーを使用できるよう、1つプールに対してレプリカを4つ以上設定します。これにより、一方のデータセンターが失われてもまだ2つのコピーが存在し、さらにディスクが1つ失われてもデータが失われないようにします。
    </para>
   </tip>
   <note>
    <para>
     1つのオブジェクトが、機能低下モードにおいてレプリカが<literal>pool size</literal>未満の状態でI/Oを受け付ける場合があります。I/Oに必要なレプリカの最小数を設定するには、<literal>min_size</literal>設定を使用する必要があります。次に例を示します。
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     これにより、データプール内のオブジェクトはレプリカが<literal>min_size</literal>未満の場合、I/Oを受け取らなくなります。
    </para>
   </note>
   <tip>
    <title>オブジェクトレプリカの数の取得</title>
    <para>
     オブジェクトレプリカの数を取得するには、次のコマンドを実行します。
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump | grep 'replicated size'</screen>
    <para>
     <literal>replicated size</literal>属性が強調表示された状態でプールが一覧にされます。デフォルトでは、Cephはオブジェクトのレプリカを2つ作成します(合計で3つのコピー、またはサイズ3)。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>プールのマイグレーション</title>

  <para>
   プールを作成する際(<xref linkend="ceph-pools-operate-add-pool"/>を参照)、プールのタイプや配置グループの数など、初期パラメータを指定する必要があります。後でこれらのパラメータのいずれかを変更する場合(たとえば、複製プールをイレージャコーディングプールに変換したり、配置グループの数を減らしたりする場合)、プールのデータを、展開に適したパラメータを持つ別のプールに移行する必要があります。
  </para>

  <para>
   プールのマイグレーションには複数の方法があります。「キャッシュ層」<emphasis/>を使用することをお勧めします。この方法は透過的で、クラスタのダウンタイムを短縮し、プール全体のデータが重複するのを避けられます。
  </para>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>キャッシュ層を使用した移行</title>
   <tip>
    <title>複製プールのみの移行</title>
    <para>
     キャッシュ層による方法を使用して、複製プールからイレージャコーディングプールまたは別の複製プールに移行できます。イレージャコーディングプールからの移行はサポートされていません。
    </para>
   </tip>
   <para>
    原理は単純で、移行する必要があるプールを逆の順番でキャッシュ層に含めます。キャッシュ層の詳細については、<xref linkend="cha-ceph-tiered"/>を参照してください。次の例では、「testpool」という名前の複製プールをイレージャコーディングプールに移行します。
   </para>
   <procedure>
    <title>複製プールからイレージャコーディングプールへの移行</title>
    <step>
     <para>
      「newpool」という名前の新しいイレージャコーディングプールを作成します。プールの作成パラメータの詳細な説明については、<xref linkend="ceph-pools-operate-add-pool"/>を参照してください。
     </para>
<screen>
 <prompt>cephadm@adm &gt; </prompt>ceph osd pool create newpool <replaceable>PG_NUM</replaceable> <replaceable>PGP_NUM</replaceable> erasure default
</screen>
     <para>
      使用されているクライアントキーリングが「testpool」と少なくとも同じ機能を「newpool」に提供することを確認します。
     </para>
     <para>
      これでプールが2つできました。データが入った元の複製プール「testpool」と、新しい空のイレージャコーディングプール「newpool」です。
     </para>
     <figure>
      <title>マイグレーション前のプール</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュ層をセットアップして、複製プール「testpool」をキャッシュプールとして設定します。<option>-force-nonempty</option>オプションを使用すると、プールにすでにデータがある場合にもキャッシュ層を追加できます。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<prompt>cephadm@adm &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>キャッシュ層のセットアップ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュプールからすべてのオブジェクトを新しいプールに強制的に移動します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>データのフラッシュ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      すべてのデータが新しいイレージャコーディングプールにフラッシュされるまでは、オーバーレイを指定してオブジェクトが古いプールで検索されるようにする必要があります。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      このオーバーレイにより、すべての操作が古い複製プール「testpool」に転送されます。
     </para>
     <figure>
      <title>オーバーレイの設定</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      これで、新しいプールのオブジェクトにアクセスするようすべてのクライアントを切り替えることができます。
     </para>
    </step>
    <step>
     <para>
      すべてのデータがイレージャコーディングプール「newpool」に移行されたら、オーバーレイと古いキャッシュプール「testpool」を削除します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>cephadm@adm &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>マイグレーションの完了</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      次のコマンドを実行します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
   <warning>
    <title>RBDイメージとCephFSエクスポートはECプールに移行できない</title>
    <para>
     RBDイメージとCephFSエクスポートを複製プールからECプールに移行することはできません。ECプールはデータを保存できますが、メタデータは保存できません。RBDのヘッダオブジェクトはフラッシュできません。CephFSについても同様です。
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>RADOS Block Deviceイメージの移行</title>
   <para>
    次に、RBDイメージを1つの複製プールから別の複製プールに移行する場合に推奨する方法を示します。
   </para>
   <procedure>
    <step>
     <para>
      クライアント(仮想マシンなど)がRBDイメージにアクセスしないようにします。
     </para>
    </step>
    <step>
     <para>
      新しいイメージをターゲットプール内に作成し、親をソースイメージに設定します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>ECプールにはデータのみを移行</title>
      <para>
       イメージデータのみを新しいECプールに移行し、メタデータを元の複製プールに残す必要がある場合は場合は、代わりに次のコマンドを実行します。
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
     <note>
      <title>クライアントサポートとダウンタイム</title>
      <para>
       <command>rbd migration</command>による方法を使用すると、クライアントのダウンタイムを最小限に抑えてイメージを移行できます。必要なのは、「prepare」ステップの前にクライアントを停止して、後でクライアントを起動することだけです。「prepare」ステップの直後にイメージを開くことができるのは、この機能をサポートする<systemitem>librbd</systemitem>クライアント(Ceph Nautilus以降)のみであり、それ以前の<systemitem>librbd</systemitem>クライアントや<systemitem>krbd</systemitem>クライアントは、「commit」ステップが実行されるまでイメージを開くことができないことに注意してください。
      </para>
     </note>
    </step>
    <step>
     <para>
      クライアントがターゲットプール内のイメージにアクセスできるようにします。
     </para>
    </step>
    <step>
     <para>
      データをターゲットプールに移行します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      古いイメージを削除します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>プールのスナップショット</title>

  <para>
   プールのスナップショットは、Cephのプール全体の状態のスナップショットです。プールのスナップショットにより、プールの状態の履歴を保持できます。プールのスナップショットを作成すると、プールサイズに比例したストレージ領域が消費されます。プールのスナップショットを作成する前に、必ず関連するストレージに十分なディスク領域があることを確認してください。
  </para>

  <sect2>
   <title>プールのスナップショットの作成</title>
   <para>
    プールのスナップショットを作成するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    次に例を示します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2>
   <title>プールのスナップショットの一覧</title>
   <para>
    プールの既存のスナップショットを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    次に例を示します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2>
   <title>プールのスナップショットの削除</title>
   <para>
    プールのスナップショットを削除するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>データ圧縮</title>

  <para>
   BlueStore (詳細については、<xref linkend="about-bluestore"/>を参照)は、オンザフライでデータを圧縮してディスク容量を節約できます。圧縮率は、システムに保存されるデータによって異なります。圧縮/圧縮解除には、追加のCPUパワーが必要になることに注意してください。
  </para>

  <para>
   データ圧縮をグローバルに設定し(<xref linkend="sec-ceph-pool-bluestore-compression-options"/>を参照)、その後、個々のプールに対して固有の圧縮設定を上書きできます。
  </para>

  <para>
   プールにデータが含まれるかどうかに関係なく、プールのデータ圧縮を有効/無効にしたり、圧縮アルゴリズムやモードをいつでも変更したりできます。
  </para>

  <para>
   プールの圧縮を有効にすると、既存のデータに圧縮は適用されなくなります。
  </para>

  <para>
   プールの圧縮を無効にすると、そのプールのすべてのデータの圧縮が解除されます。
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>圧縮の有効化</title>
   <para>
    <replaceable>POOL_NAME</replaceable>という名前のプールのデータ圧縮を有効にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>プール圧縮の無効化</title>
    <para>
     プールのデータ圧縮を無効にするには、圧縮アルゴリズムとして「none」を使用します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>プール圧縮オプション</title>
   <para>
    次に、すべての圧縮設定のリストを示します。
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       使用可能な値は、<literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>です。デフォルトは<literal>snappy</literal>です。
      </para>
      <para>
       どの圧縮アルゴリズムを使用するかは、特定の使用事例によって異なります。次に、推奨事項をいくつか示します。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         変更する妥当な理由がない限り、デフォルトの<literal>snappy</literal>を使用してください。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal>は、圧縮率は優れていますが、少量のデータを圧縮する場合にはCPUオーバーヘッドが高くなります。
        </para>
       </listitem>

       <listitem>
        <para>
         クラスタのCPUとメモリの使用量に注意しながら、実際のデータのサンプルに対してこれらのアルゴリズムのベンチマークを実行します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       使用可能な値は、<literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>です。デフォルトは<literal>none</literal>です。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: 圧縮しません。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: <literal>COMPRESSIBLE</literal>と表示されている場合、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: <literal>INCOMPRESSIBLE</literal>と表示されている場合以外、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: 常に圧縮します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       値: 倍精度、比率= SIZE_COMPRESSED / SIZE_ORIGINAL。デフォルトは<literal>0.875</literal>です。これは、占有されている容量が圧縮によって12.5%以上削減されない場合は、オブジェクトは圧縮されないことを意味します。
      </para>
      <para>
       この率を上回るオブジェクトは、圧縮効果が低いため圧縮状態では保存されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>グローバル圧縮オプション</title>
   <para>
    次の設定オプションはCeph設定で指定でき、1つのプールだけでなくすべてのOSDに適用されます。<xref linkend="sec-ceph-pool-compression-options"/>に一覧にされているプール固有の設定が優先されます。
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       詳細については、「<xref linkend="compr-algorithm"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       詳細については、「<xref linkend="compr-mode"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       詳細については、「<xref linkend="compr-ratio"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。この設定はデフォルトでは無視され、<option>bluestore_compression_min_blob_size_hdd</option>と <option>bluestore_compression_min_blob_size_ssd</option>が優先されます。0以外の値に設定した場合は、この設定が優先されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。この設定はデフォルトでは無視され、<option>bluestore_compression_max_blob_size_hdd</option>と <option>bluestore_compression_max_blob_size_ssd</option>が優先されます。0以外の値に設定した場合は、この設定が優先されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>8K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>64K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>128K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>512K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
