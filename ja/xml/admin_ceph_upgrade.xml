<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>古いリリースからのアップグレード</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編集</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  この章では、SUSE Enterprise Storage 5.5をバージョン6にアップグレードする手順について説明します。バージョン5.5は、基本的にはすべての最新のパッチが適用されたバージョン5です。
 </para>
 <note>
  <title>古いリリースからのアップグレードはサポートされない</title>
  <para>
   5.5より古いバージョンのSUSE Enterprise Storageからのアップグレードはサポートされていません。まず、最新バージョンのSUSE Enterprise Storage 5.5にアップグレードしてから、この章の手順を実行する必要があります。
  </para>
 </note>
 <sect1 xml:id="upgrade-consider-points">
  <title>アップグレード前に考慮すべき点</title>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis/>「リリースノートをお読みください」 - 旧リリースのSUSE Enterprise Storageからの変更点に関する追加情報が記載されています。リリースノートを参照して以下を確認します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       使用しているハードウェアに特別な配慮が必要かどうか
      </para>
     </listitem>
     <listitem>
      <para>
       使用しているソフトウェアパッケージに大幅な変更があるかどうか
      </para>
     </listitem>
     <listitem>
      <para>
       インストールのために特別な予防措置が必要かどうか
      </para>
     </listitem>
    </itemizedlist>
    <para>
     リリースノートには、マニュアルに記載できなかった情報が記載されています。また、既知の問題に関する注意も記載されています。
    </para>
    <para>
     パッケージ <package>release-notes-ses</package>をインストールすると、リリースノートは、ローカルではディレクトリ<filename>/usr/share/doc/release-notes</filename>に、オンラインでは<link xlink:href="https://www.suse.com/releasenotes/"/>にあります。
    </para>
   </listitem>
   <listitem>
    <para>
     以前にバージョン4からアップグレードした場合は、バージョン5へのアップグレードが正常に完了していることを確認します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       ファイルが存在することを確認する
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
      <para>
       このファイルは、SES 4から5へのアップグレード中にインポートプロセスによって作成されます。また、<option>configuration_init: default-import</option>オプションがファイルに設定されます。
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <para>
       <option>configuration_init</option>がまだ<option>default-import</option>に設定されている場合、クラスタはその設定ファイルとして<filename>ceph.conf.import</filename>を使用しています。これは、次の場所にあるファイルからコンパイルされるDeepSeaのデフォルトの<filename>ceph.conf</filename>ではありません。
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       したがって、<filename>ceph.conf.import</filename>でカスタム設定を調べ、可能であれば、次の場所にあるファイルのいずれかに設定を移動する必要があります。
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       その後、次のファイルから<option>configuration_init: default-import</option>の行を削除します。
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <warning>
       <title>DeepSeaのデフォルトの設定</title>
       <para>
        <emphasis role="bold"/><filename>ceph.conf.import</filename>から設定をマージして<option>configuration_init: default-import</option>オプションを削除「しない」場合、DeepSeaの一部として出荷されているデフォルトの設定(<filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>に保存)はクラスタに適用されません。
       </para>
      </warning>
     </listitem>
     <listitem>
      <para>
       クラスタが新しいバケットタイプ「straw2」を使用しているかどうかを確認する
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep straw
</screen>
     </listitem>
     <listitem>
      <para>
       Ceph「jewel」プロファイルが使用されていることを確認する
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep profile
</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     古いRBDカーネルクライアント(SUSE Linux Enterprise Server 12 SP3より前)が使用されている場合は、<xref linkend="rbd-old-clients-map"/>を参照してください。可能であれば、古いRBDカーネルクライアントをアップグレードすることをお勧めします。
    </para>
   </listitem>
   <listitem>
    <para>
     openATTICが管理ノード上にある場合、ノードをアップグレードするとopenATTICを使用できなくなります。新しいCephダッシュボードは、DeepSeaを使用して展開するまで使用できません。
    </para>
   </listitem>
   <listitem>
    <para>
     クラスタのアップグレードには長い時間がかかることがあります。所要時間は、1台のマシンのアップグレード時間xクラスタノード数です。
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Linux Enterprise Serverの旧リリースを実行している間は単一のノードをアップグレードすることはできませんが、新しいバージョンのインストーラでノードを再起動する必要があります。したがって、ノードが提供するサービスは、しばらくの間利用できません。コアクラスタサービスは引き続き利用できます。たとえば、アップグレード中に1つのMONがダウンしても、アクティブなMONはまだ少なくとも2つ存在します。残念ながら、単一のiSCSI Gatewayなど、単一インスタンスのサービスは利用できなくなります。
    </para>
   </listitem>
   <listitem>
    <para>
     特定のタイプのデーモンは他のデーモンに依存します。たとえば、Ceph Object Gatewayは、Ceph MONデーモンとOSDデーモンに依存します。次の順序でアップグレードすることをお勧めします。
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       管理ノード
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor/Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       NFS Ganesha
      </para>
     </listitem>
     <listitem>
      <para>
       Samba Gateway
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     AppArmorを「complain」モードまたは「enforce」モードのいずれかで使用していた場合は、アップグレードする前にSalt Pillar変数を設定する必要があります。SUSE Linux Enterprise Server 15 SP1にはAppArmorがデフォルトで付属しているため、DeepSeaステージ0にAppArmorの管理が統合されました。SUSE Enterprise Storage 6では、デフォルトの動作としてAppArmorおよび関連するプロファイルを削除します。SUSE Enterprise Storage 5.5で設定していた動作を保持したい場合は、アップグレードを開始する前に、<filename>/srv/pillar/ceph/stack/global.yml</filename>ファイルに次の行のいずれかが存在することを確認してください。
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     プロンプトまたは
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     SUSE Enterprise Storage 6から、数字で始まるMDS名は使用できなくなり、MDSデーモンは起動しなくなります。<command>ceph fs status</command>コマンドを実行するか、MDSを再起動してログで次のメッセージを確認することで、デーモンにこのような名前が付いているかどうかを確認できます。
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     上のメッセージが表示される場合は、SUSE Enterprise Storage 6へのアップグレードを試行する前に、MDS名を移行する必要があります。DeepSeaでは、このような移行を自動化するためのオーケストレーションが提供されています。数字で始まるMDS名の前には「mds.」が付加されます。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <tip>
     <title>MDS名にバインドされているカスタム設定</title>
     <para>
      MDS名にバインドされている設定があり、MDSデーモンに数字で始まる名前が付いている場合は、設定が新しい名前にも適用されていることを確認します(プレフィックス「mds.」が付加されている)。例として、<filename>/etc/ceph/ceph.conf</filename>ファイルの次のセクションを考えてみます。
     </para>
<screen>
[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
mds cache memory limit = 1073741824
mds standby for name = 456-another-mds
</screen>
     <para>
      <command>ceph.mds.migrate-numerical-names</command>オーケストレータにより、MDSデーモン名「123-my-mds」は「mds.123-my-mds」に変更されます。新しい名前を反映するように設定を調整する必要があります。
     </para>
<screen>
[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds
</screen>
    </tip>
    <para>
     この処理では、新しい名前のMDSデーモンを追加してから、古いMDSデーモンを削除します。そのため、少しの間、MDSデーモンの数が2倍になります。クライアントは、フェールオーバーが完了するまで少し停止した後、CephFSにアクセスできるようになります。したがって、CephFSへの負荷がほとんどまたはまったくないと予想される時間帯に合わせてマイグレーションを計画してください。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-backup">
  <title>クラスタデータのバックアップ</title>

  <para>
   クラスタの設定とデータのバックアップを作成することは必須ではありませんが、重要な設定ファイルとクラスタデータをバックアップすることを強くお勧めします。詳細については、<xref linkend="cha-deployment-backup"/>を参照してください。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-ntp">
  <title><systemitem class="daemon">ntpd</systemitem>から<systemitem class="daemon">chronyd</systemitem>への移行</title>

  <para>
   SUSE Linux Enterprise Server 15 SP1では、ローカルホストの時刻の同期に<systemitem class="daemon">ntpd</systemitem>が使用されなくなりました。代わりに、<systemitem class="daemon">chronyd</systemitem>が使用されます。各クラスタノードで時刻同期デーモンを移行する必要があります。クラスタをアップグレードする<emphasis role="bold"/>「前に」<systemitem>chronyd</systemitem>へ移行することも、クラスタをアップグレードしてから「後で」<systemitem class="daemon">chronyd</systemitem>
   <emphasis role="bold"/>に移行することもできます。
  </para>

  <procedure>
   <title>クラスタをアップグレードする「前に」<systemitem class="daemon">chronyd</systemitem> <emphasis/>に移行する</title>
   <step>
    <para>
     次のようにして <package>chrony</package> パッケージをインストールします。
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper install chrony</screen>
   </step>
   <step>
    <para>
     <systemitem class="daemon">chronyd</systemitem>の設定ファイル<filename>/etc/chrony.conf</filename>を編集して、<filename>/etc/ntp.conf</filename>の現在の<systemitem class="daemon">ntpd</systemitem>の設定からNTPソースを追加します。
    </para>
    <tip>
     <title><systemitem class="daemon">chronyd</systemitem>の設定の詳細</title>
     <para>
      <systemitem class="daemon">chronyd</systemitem>の設定にタイムソースを含める方法の詳細については、<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>を参照してください。
     </para>
    </tip>
   </step>
   <step>
    <para>
     <systemitem class="daemon">ntpd</systemitem>サービスを無効にして停止します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     <systemitem class="daemon">chronyd</systemitem>サービスを開始して有効にします。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     chronyのステータスを確認します。
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
  </procedure>

  <procedure>
   <title>クラスタをアップグレードした「後で」<systemitem class="daemon">chronyd</systemitem> <emphasis/>に移行する</title>
   <step>
    <para>
     クラスタのアップグレード中に、次のソフトウェアリポジトリを追加します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     クラスタをバージョン 6にアップグレードします。
    </para>
   </step>
   <step>
    <para>
     <systemitem class="daemon">chronyd</systemitem>の設定ファイル<filename>/etc/chrony.conf</filename>を編集して、<filename>/etc/ntp.conf</filename>の現在の<systemitem class="daemon">ntpd</systemitem>の設定からNTPソースを追加します。
    </para>
    <tip>
     <title><systemitem class="daemon">chronyd</systemitem>の設定の詳細</title>
     <para>
      <systemitem class="daemon">chronyd</systemitem>の設定にタイムソースを含める方法の詳細については、<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>を参照してください。
     </para>
    </tip>
   </step>
   <step>
    <para>
     <systemitem class="daemon">ntpd</systemitem>サービスを無効にして停止します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     <systemitem class="daemon">chronyd</systemitem>サービスを開始して有効にします。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     <systemitem class="daemon">ntpd</systemitem>から<systemitem class="daemon">chronyd</systemitem>に移行します。
    </para>
   </step>
   <step>
    <para>
     chronyのステータスを確認します。
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
   <step>
    <para>
     アップグレードプロセス中に<systemitem class="daemon">ntpd</systemitem>をシステムに保持するために追加したレガシソフトウェアリポジトリを削除します。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-prepare">
  <title>アップグレード前にクラスタにパッチを適用</title>

  <para>
   アップグレード前にすべてのクラスタノードに最新のパッチを適用します。
  </para>

  <sect2 xml:id="upgrade-prepare-repos">
   <title>必要なソフトウェアリポジトリ</title>
   <para>
    各クラスタのノードで必要なリポジトリが設定されていることを確認します。使用可能なすべてのリポジトリを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>root@minion &gt; </prompt>zypper lr
</screen>
   <para>
    SUSE Enterprise Storage 5.5には以下が必要です。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    SUSE Linux Enterprise Server 12 SP3上のSLE HAのNFS/SMB Gatewayには以下が必要です。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-staging">
   <title>リポジトリステージングシステム</title>
   <para>
    SMT、RMT、またはSUSE Managerのいずれかのリポジトリステージングシステムを使用している場合は、現在のバージョンと新しいバージョンのSUSE Enterprise Storageに対して新しいフローズンパッチレベルを作成します。
   </para>
   <para>
    詳細については、以下を参照してください。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-manager-3/index.html"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-patch">
   <title>クラスタ全体への最新パッチの適用</title>
   <procedure>
    <step>
     <para>
      各CephクラスタノードにSUSE Enterprise Storage 5.5およびSUSE Linux Enterprise Server 12 SP3の最新パッチを適用します。正しいソフトウェアリポジトリが各クラスタノードに接続されていることを確認し(<xref linkend="upgrade-prepare-repos"/>を参照)、DeepSeaステージ0を実行します。
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      ステージ0が完了したら、各クラスタノードのステータスに「HEALTH_OK」が含まれていることを確認します。含まれていない場合は、次の手順で再起動する前に問題を解決してください。
     </para>
    </step>
    <step>
     <para>
      <command>zypper ps</command>を実行して、古いライブラリやバイナリで実行されている可能性のあるプロセスがないかどうかを確認し、ある場合は再起動します。
     </para>
    </step>
    <step>
     <para>
      実行中のカーネルが使用可能な最新のものであることを確認し、そうでない場合は再起動します。次のコマンドの出力を確認します。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>uname -a
<prompt>cephadm@adm &gt; </prompt>rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      続いて、 <package>ceph</package> パッケージがバージョン12.2.12以降であることを確認します。続いて、 <package>deepsea</package> パッケージがバージョン0.8.9以降であることを確認します。
     </para>
    </step>
    <step>
     <para>
      以前にいずれかの<option>bluestore_cache</option>設定を使用していた場合、それらの設定は、 <package>ceph</package>
      バージョン12.2.10から有効ではなくなりました。新しい設定<option>bluestore_cache_autotune</option>はデフォルトで「true」に設定されており、キャッシュサイズの手動変更が無効になります。古い動作をオンにするには、<option>bluestore_cache_autotune=false</option>を設定する必要があります。詳細については、<xref linkend="config-auto-cache-sizing"/>を参照してください。
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-verify-current">
  <title>現在の環境の検証</title>

  <itemizedlist>
   <listitem>
    <para>
     システムに明白な問題がある場合は、アップグレードを開始する前に修正します。アップグレードによってシステムの既存の問題が修正されることはありません。
    </para>
   </listitem>
   <listitem>
    <para>
     クラスタのパフォーマンスを確認します。<command>rados bench</command>、<command>ceph tell osd.* bench</command>、<command>iperf3</command>などのコマンドを使用できます。
    </para>
   </listitem>
   <listitem>
    <para>
     ゲートウェイ(iSCSI GatewayやObject Gateway)およびRADOS Block Deviceへのアクセスを検証します。
    </para>
   </listitem>
   <listitem>
    <para>
     ネットワークのセットアップ、パーティション設定、インストールの詳細など、システムセットアップの特定の部分を文書化します。
    </para>
   </listitem>
   <listitem>
    <para>
     <command>supportconfig</command>を使用して重要なシステム情報を収集し、それをクラスタノードの外部に保存します。詳細情報については、<link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_admsupport_supportconfig.html"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     各クラスタノードに十分な空きディスク容量があることを確認します。<command>df -h</command>を使用して空きディスク容量を確認します。必要に応じて、不要なファイル/ディレクトリを削除するか、古いOSスナップショットを削除して、ディスク容量を解放します。十分な空きディスク容量がない場合は、十分なディスク容量を解放するまで、アップグレードを続行しないでください。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-verify-state">
  <title>クラスタの状態の確認</title>

  <itemizedlist>
   <listitem>
    <para>
     アップグレード手順を開始する前に、<command>cluster health</command>コマンドを確認します。各クラスタノードが「HEALTH_OK」をレポートしない限り、アップグレードを開始しないでください。
    </para>
   </listitem>
   <listitem>
    <para>
     次のすべてのサービスが実行中であることを確認します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Salt MasterおよびSalt Masterデーモン
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph MonitorおよびCeph Managerデーモン
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Serverデーモン
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSDデーモン
      </para>
     </listitem>
     <listitem>
      <para>
       Object Gatewayデーモン
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI Gatewayデーモン
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   次の各コマンドは、クラスタの状態と特定の設定の詳細を提供します。
  </para>

  <variablelist>
   <varlistentry>
    <term><command>ceph -s</command></term>
    <listitem>
     <para>
      Cephクラスタのヘルス、実行中のサービス、データ使用量、およびI/O統計情報の簡単な概要を出力します。アップグレードを開始する前に「HEALTH_OK」とレポートされることを確認します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph health detail</command></term>
    <listitem>
     <para>
      Cephクラスタのヘルスに問題がある場合に詳細を出力します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph versions</command></term>
    <listitem>
     <para>
      実行中のCephデーモンのバージョンを出力します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph df</command></term>
    <listitem>
     <para>
      クラスタの合計ディスク容量と空きディスク容量を出力します。クラスタの空きディスク容量が合計ディスク容量の25%未満である場合は、アップグレードを開始しないでください。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>salt '*' cephprocesses.check results=true</command></term>
    <listitem>
     <para>
      実行中のCephプロセスとそのPIDをSalt Minion別にソートして出力します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph osd dump | grep ^flags</command></term>
    <listitem>
     <para>
      「recovery_deletes」フラグと「purged_snapdirs」フラグが存在していることを確認します。存在していない場合は、次のコマンドを実行することで、すべての配置グループに対して強制的にスクラブを実行できます。この強制スクラブは、Cephクライアントのパフォーマンスに悪影響を及ぼす可能性があることに注意してください。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>CTDBクラスタのオフラインアップグレード</title>

  <para>
   CTDBは、Samba Gatewayが使用するクラスタ化データベースを提供します。CTDBプロトコルは非常に単純であり、異なるプロトコルバージョンを使って通信するノードのクラスタをサポートしません。したがって、アップグレードを実行する前にCTDBノードをオフラインにする必要があります。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-one-node">
  <title>ノード単位のアップグレード — 基本的な手順</title>

  <para>
   アップグレード中にコアクラスタサービスを利用できるようにするため、クラスタノードを1つずつ順番にアップグレードする必要があります。ノードのアップグレードは、「<emphasis/>インストーラDVD」または<emphasis/>「Distribution Migration System」の2つの方法のいずれかで実行できます。
  </para>

  <para>
   各ノードをアップグレードした後、<command>rpmconfigcheck</command>を実行して、ローカルで編集した設定ファイルの中に、更新されたファイルがないかどうかを確認することをお勧めします。このコマンドで、サフィックス<filename>.rpmnew</filename>、<filename>.rpmorig</filename>、または<filename>.rpmsave</filename>が付くファイル名のリストが返された場合は、これらのファイルを現在の設定ファイルと比較し、ローカルでの変更が失われていないことを確認します。必要に応じて、影響を受けるファイルを更新します。<filename>.rpmnew</filename>、<filename>.rpmorig</filename>、および<filename>.rpmsave</filename>の各ファイルの使用に関する詳細については、<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-sw-cl.html#sec-rpm-packages-manage"/>を参照してください。
  </para>

  <tip>
   <title>孤立パッケージ</title>
   <para>
    ノードをアップグレードした後、多数のパッケージが親リポジトリのない「孤立」状態になります。これは、python3関連のパッケージによってpython2のパッケージが廃止されないために発生します。
   </para>
   <para>
    孤立パッケージの一覧の詳細については、<link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_zypper.html#sec_zypper_softup_orphaned"/>を参照してください。
   </para>
  </tip>

  <sect2 xml:id="upgrade-one-node-manual">
   <title>インストーラDVDを使用したノードの手動アップグレード</title>
   <procedure>
    <step>
     <para>
      SUSE Linux Enterprise Server 15 SP1のインストーラDVD/イメージからノードを再起動します。
     </para>
    </step>
    <step>
     <para>
      ブートメニューから<guimenu>Upgrade (アップグレード)</guimenu>を選択します。
     </para>
    </step>
    <step>
     <para>
      <guimenu>マイグレーションターゲットの選択</guimenu>画面で、［SUSE Linux Enterprise Server 15 SP1］が選択されていることを確認し、<guimenu>マイグレーションリポジトリの手動調整</guimenu>チェックボックスをオンにします。
     </para>
     <figure>
      <title>マイグレーションターゲットの選択</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      次のモジュールをインストールするよう選択します。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SUSE Enterprise Storage 6 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Basesystem Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Desktop Applications Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Legacy Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Server Applications Module 15 SP1 x86_64
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      <guimenu>Previously Used Repositories (以前に使用したリポジトリ)</guimenu>画面で、正しいリポジトリが選択されていることを確認します。システムがSCC/SMTに登録されていない場合は、リポジトリを手動で追加する必要があります。
     </para>
     <para>
      SUSE Enterprise Storage 6には以下が必要です。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      SESのマイグレーション後に<systemitem>ntpd</systemitem>を<systemitem class="daemon">chronyd</systemitem>に移行する場合は(<xref linkend="upgrade-ntp"/>を参照)、次のリポジトリを含めます。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      SUSE Linux Enterprise Server 15 SP1上のSLE HAのNFS/SMB Gatewayには以下が必要です。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      <guimenu>Installation Settings (インストール設定)</guimenu>を確認し、<guimenu>更新</guimenu>をクリックしてインストール手順を開始します。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-one-node-auto">
   <title>SUSE Distribution Migration Systemを使用したノードのアップグレード</title>
   <para>
    <emphasis/>DMS (「Distribution Migration System」)は、インストールされているSUSE Linux Enterpriseシステムに対し、あるメジャーバージョンから別のメジャーバージョンへのアップグレードパスを提供します。次の手順では、DMSを利用してSUSE Enterprise Storage 5.5をバージョン6にアップグレードします。これには、基礎となるSUSE Linux Enterprise Server 12 SP3からSUSE Linux Enterprise Server 15 SP1へのマイグレーションも含まれます。
   </para>
   <para>
    DMSの一般的な情報と詳細情報の両方の詳細については、<link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>を参照してください。
   </para>
   <procedure>
    <step>
     <para>
      マイグレーションRPMパッケージをインストールします。これらのパッケージは、次の再起動時に自動的にアップグレードをトリガするようGRUBブートローダを調整します。次のようにして
      <package>SLES15-SES-Migration</package> および
      <package>suse-migration-sle15-activation</package> パッケージをインストールします。
     </para>
<screen><prompt>root@minion &gt; </prompt>zypper install SLES15-SES-Migration suse-migration-sle15-activation</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        アップグレードするノードがSCC、SMT、RMT、またはSUSE Managerなどの<emphasis role="bold"/>リポジトリステージングシステムに登録「されている」場合、<filename>/etc/sle-migration-service.yml</filename>を次の内容で作成します。
       </para>
<screen>
use_zypper_migration: true
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
      </step>
      <step>
       <para>
        アップグレードするノードがSCC、SMT、RMT、またはSUSE Managerなどのリポジトリステージングシステムに登録「されていない」場合、<emphasis role="bold"/>次の変更を実行します。
       </para>
       <substeps>
        <step>
         <para>
          次の内容で<filename>/etc/sle-migration-service.yml</filename>を作成します。
         </para>
<screen>
use_zypper_migration: false
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
        </step>
        <step>
         <para>
          SLE 12 SP3およびSES 5のリポジトリを無効化または削除し、SLE 15 SP1およびSES6のリポジトリを追加します。関連するリポジトリのリストについては、<xref linkend="upgrade-prepare-repos"/>を参照してください。
         </para>
        </step>
       </substeps>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      再起動してアップグレードを開始します。アップグレードの実行中に、<link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>で説明されているように、既存のSSHキーを使用して、<command>ssh</command>経由でホストシステムから、アップグレードされたノードにマイグレーションユーザとしてログインできます。SUSE Enterprise Storageでは、マシンに物理的にアクセスできるか、マシンのコンソールに直接アクセスできる場合、パスワード<literal>sesupgrade</literal>を使用して、システムコンソールで<systemitem class="username">root</systemitem>としてログインすることもできます。アップグレード後、ノードは自動的に再起動します。
     </para>
     <tip>
      <title>アップグレードが失敗する場合</title>
      <para>
       アップグレードが失敗する場合、<filename>/var/log/distro_migration.log</filename>を調べます。問題を修正して、マイグレーションRPMパッケージを再インストールし、ノードを再起動します。
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-adm">
  <title>管理ノードのアップグレード</title>

  <itemizedlist>
   <listitem>
    <para>
     Salt Minionが古いバージョンのCephおよびSaltを実行していても、コマンドは<command>salt '*' test.ping</command>および<command>ceph status</command>は依然として機能します。
    </para>
   </listitem>
   <listitem>
    <para>
     管理ノードをアップグレードすると、openATTICはインストールされなくなります。
    </para>
   </listitem>
   <listitem>
    <para>
     管理ノードでSMTをホストしていた場合は、RMTへのマイグレーションを完了します(<link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_migrate.html"/>を参照)。
    </para>
   </listitem>
   <listitem>
    <para>
     <xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>クラスタノードのステータス</title>
   <para>
    管理ノードがアップグレードされた後で、<command>salt-run upgrade.status</command>コマンドを実行して、クラスタノードに関する役立つ情報を表示できます。このコマンドは、すべてのノードのCephとOSのバージョンを一覧にし、まだ古いバージョンを実行しているノードをどのような順序でアップグレードすべきかを推奨します。
   </para>
<screen><prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
  ceph: ceph version 14.2.1-468-g994fd9e0cc (994fd9e0ccc50c2f3a55a3b7a3d4e0ba74786d50) nautilus (stable)
  os: SUSE Linux Enterprise Server 15 SP1

Nodes running these software versions:
  admin.ceph (assigned roles: master)
  mon2.ceph (assigned roles: admin, mon, mgr)

Nodes running older software versions must be upgraded in the following order:
   1: mon1.ceph (assigned roles: admin, mon, mgr)
   2: mon3.ceph (assigned roles: admin, mon, mgr)
   3: data1.ceph (assigned roles: storage)
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="upgrade-mons">
  <title>Ceph Monitor/Ceph Managerノードのアップグレード</title>

  <itemizedlist>
   <listitem>
    <para>
     クラスタでMDSの役割を「使用しない」<emphasis role="bold"/>場合は、MON/MGRノードを1つずつアップグレードします。
    </para>
   </listitem>
   <listitem>
    <para>
     クラスタでMDSの役割を「使用する」<emphasis role="bold"/>場合に、MON/MGRとMDSの役割が同じ場所にあるときは、MDSクラスタを縮小してから、同じ場所にあるノードをアップグレードします。詳細については、<xref linkend="upgrade-mds"/>を参照してください。
    </para>
   </listitem>
   <listitem>
    <para>
     クラスタでMDSの役割を「使用する」<emphasis role="bold"/>場合に、それらを「専用」<emphasis role="bold"/>のサーバで実行するときは、すべてのMON/MGRノードを1つずつアップグレードしてから、MDSクラスタを縮小してアップグレードします。詳細については、<xref linkend="upgrade-mds"/>を参照してください。
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Ceph Monitorのアップグレード</title>
   <para>
    Ceph Monitorの設計上の制限のため、2つのMONをSUSE Enterprise Storage 6にアップグレードして定数が形成された後で、3番目のMON (まだSUSE Enterprise Storage 5.5上に存在)が何らかの理由(ノードの再起動を含む)で再起動された場合、そのMONはMONクラスタに再参加しません。したがって、2つのMONがアップグレードされている場合は、残りのMONをできるだけ早くアップグレードすることをお勧めします。
   </para>
  </note>

  <para>
   <xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
  </para>
 </sect1>
 <sect1 xml:id="upgrade-mds">
  <title>Metadata Serverのアップグレード</title>

  <para>
   MDS (Metadata Server)クラスタを縮小する必要があります。SUSE Enterprise Storage 5.5と6のバージョンの間には機能の互換性がないため、古いMDSデーモンは、SES 6レベルのMDSが1つクラスタに参加したことを確認すると、すぐにシャットダウンします。したがって、MDSノードのアップグレードの間は、MDSクラスタを1つのアクティブなMDS (およびスタンバイなし)に縮小する必要があります。2番目のノードがアップグレードされたらすぐに、再びMDSクラスタを拡張できます。
  </para>

  <tip>
   <para>
    非常に負荷の高いMDSクラスタでは、1つのアクティブMDSでワークロードを処理できるように、(たとえばクライアントを停止することにより)負荷を軽減しなければならない場合があります。
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     <option>max_mds</option>オプションの現在の値を記録します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     アクティブなMDSデーモンが複数ある場合(すなわち、<option>max_mds</option>が1より大きい場合)、MDSクラスタを縮小します。MDSクラスタを縮小するには、次のコマンドを実行します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     ここで、<replaceable>FS_NAME</replaceable>は、CephFSインスタンスの名前です(デフォルトは「cephfs」)。
    </para>
   </step>
   <step>
    <para>
     スタンバイMDSデーモンの1つをホストしているノードを見つけます。<command>ceph fs status</command>コマンドの出力を参照し、このノードのMDSクラスタのアップグレードを開始します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     この例では、ノード「mon3-6」または「mon2-6」のいずれかでアップグレード手順を開始する必要があります。
    </para>
   </step>
   <step>
    <para>
     スタンバイMDSデーモンが存在するノードをアップグレードします。アップグレードされたMDSノードが起動すると、古くなったMDSデーモンは自動的にシャットダウンされます。この時点で、クライアントにおいてCephFSサービスの短時間のダウンタイムが発生する場合があります。
    </para>
    <para>
     <xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
    </para>
   </step>
   <step>
    <para>
     残りのMDSノードをアップグレードします。
    </para>
   </step>
   <step>
    <para>
     <option>max_mds</option>を目的の設定に再設定します。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-main-osd">
  <title>Ceph OSDのアップグレード</title>

  <para>
   各ストレージノードで次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     特定のノードで実行されているOSDデーモンを特定します。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd tree
</screen>
   </step>
   <step>
    <para>
     アップグレードするノードの各OSDデーモンに「noout」フラグを設定します。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     次に例を示します。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</screen>
    <para>
     次のコマンドで確認します。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     プロンプトまたは
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</screen>
   </step>
   <step>
    <para>
     アップグレードするノードで次のコマンドを実行し、すべての既存のOSDに対して<filename>/etc/ceph/osd/*.json</filename>ファイルを作成します。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     OSDノードをアップグレードします。<xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
    </para>
   </step>
   <step>
    <para>
     システムで見つかったすべてのOSDを有効にします。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>;ceph-volume simple activate --all
</screen>
    <tip>
     <title>データパーティションの個別の有効化</title>
     <para>
      データパーティションを個別に有効にするには、各パーティション用の正しい<command>ceph-volume</command>コマンドを確認して、パーティションを有効にする必要があります。<replaceable>X1</replaceable>は、パーティションの正しい文字/数字に置き換えてください。
     </para>
<screen>
 <prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      次に例を示します。
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      出力の最後の行に、パーティションを有効にするためのコマンドが含まれます。
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--&gt; All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--&gt; Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     再起動後、OSDノードが適切に起動することを確認します。
    </para>
   </step>
   <step>
    <para>
     「Legacy BlueStore stats reporting detected on XX OSD(s) (XX OSDで古いBlueStore統計情報レポートが検出されました)」というメッセージに対処します。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
    <para>
     Cephを14.2.2にアップグレードする場合、この警告は正常です。以下を設定して、この警告を無効にできます。
    </para>
<screen>bluestore_warn_on_legacy_statfs = false</screen>
    <para>
     適切な修正方法は、すべてのOSDが停止している間に、OSDで次のコマンドを実行することです。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</screen>
    <para>
     次に、<replaceable>NODE_NAME</replaceable>ノード上のすべてのOSDに対して<command>ceph-bluestore-tool repair</command>を実行するヘルパースクリプトを示します。
    </para>
<screen>OSDNODE=<replaceable>OSD_NODE_NAME</replaceable>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE cmd.run 'systemctl stop ceph-osd@$OSD';\
 salt $OSDNODE cmd.run 'ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD';\
 salt $OSDNODE cmd.run 'systemctl start ceph-osd@$OSD';\
 done</screen>
   </step>
   <step>
    <para>
     アップグレードするノードで、各OSDデーモンの「noout」フラグを設定解除します。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     次に例を示します。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</screen>
    <para>
     次のコマンドで確認します。
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     メモ:
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
   </step>
   <step>
    <para>
     クラスタのステータスを確認します。これは次のような出力になります。
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
   </step>
   <step>
    <para>
     すべてのOSDノードを再起動したこと、および再起動後にOSDが自動的に開始したことを確認します。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="filestore2bluestore">
  <title>BlueStoreへのOSDのマイグレーション</title>

  <para>
   OSD BlueStoreは、OSDデーモン用の新しいバックエンドです。SUSE Enterprise Storage 5からデフォルトのオプションになっています。FileStoreがオブジェクトをファイルとしてXFSファイルシステムに保存するのに対し、BlueStoreは基礎となるブロックデバイスにオブジェクトを直接保存するので、パフォーマンスを向上させることができます。BlueStoreでは、組み込みの圧縮、イレージャコーディングの上書きなど、FileStoreでは利用できないほかの機能も実現されます。
  </para>

  <para>
   特にBlueStoreでは、OSDは「wal」(Write Ahead Log、先書きログ)デバイスと「db」(RocksDBデータベース)デバイスを持っています。RocksDBデータベースはBlueStore OSDのメタデータを格納します。これら2つのデバイスは、デフォルトではOSDと同じデバイス上に存在しますが、いずれかを別の(たとえば高速な)メディアに配置できます。
  </para>

  <para>
   SUSE Enterprise Storage 5では、FileStoreとBlueStoreの両方がサポートされており、FileStoreとBlueStoreのOSDが1つのクラスタに共存できます。SUSE Enterprise Storageのアップグレード手順中に、FileStore OSDは自動的にBlueStoreに変換されます。BlueStoreに移行されていないOSDではBlueStore固有の機能は利用できないので注意してください。
  </para>

  <para>
   BlueStoreに変換する前に、OSDでSUSE Enterprise Storage 5が実行されている必要があります。すべてのデータを2回再書き込みするので、変換には時間がかかります。このマイグレーションプロセスの完了には長い時間がかかる可能性がありますが、クラスタの停止はなく、この時間中、すべてのクライアントは引き続きクラスタにアクセスできます。ただし、マイグレーション中はパフォーマンスが低下することを見込んでおいてください。これは、クラスタデータのリバランスとバックフィルが原因です。
  </para>

  <para>
   FileStore OSDをBlueStoreに移行するには、次の手順を使用します。
  </para>

  <tip>
   <title>安全対策をオフにする</title>
   <para>
    マイグレーションを実行するために必要なSaltコマンドは、安全対策によってブロックされます。これらの予防措置をオフにするには、次のコマンドを実行します。
   </para>
<screen>
 <prompt>root@master # </prompt>salt-run disengage.safety
 </screen>
   <para>
    続行する前にノードを再構築します。
   </para>
<screen>
 <prompt>root@master # </prompt> salt-run rebuild.node <replaceable>TARGET</replaceable>
 </screen>
   <para>
    各ノードを個別に再構築することもできます。次に例を示します。
   </para>
<screen>
<prompt>root@master # </prompt> salt-run rebuild.node data1.ceph
 </screen>
   <para>
    <literal>rebuild.node</literal>は、常にノード上のすべてのOSDを削除して再作成します。
   </para>
   <important>
    <para>
     1つのOSDを変換できない場合、再構築を再実行すると、すでに変換済みのBlueStore OSDが破壊されます。再構築を再実行する代わりに、次のコマンドを実行できます。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.deploy <replaceable>TARGET</replaceable>
 </screen>
   </important>
  </tip>

  <para>
   BlueStoreへのマイグレーション後、オブジェクト数は同じままで、ディスク使用量もほぼ同じになります。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-appnodes-order">
  <title>アプリケーションノードのアップグレード</title>

  <para>
   次の順序でアプリケーションノードをアップグレードします。
  </para>

  <orderedlist>
   <listitem>
    <para>
     Object Gateway
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Object Gatewayの前面にロードバランサがある場合は、停止なしでObject Gatewayをローリングアップグレードできます。
      </para>
     </listitem>
     <listitem>
      <para>
       各アップグレードの後にObject Gatewayデーモンが実行されていることを確認し、S3/Swiftクライアントを使用してテストします。
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     iSCSI Gateway
    </para>
    <itemizedlist>
     <listitem>
      <para>
       iSCSIイニシエータがマルチパスで設定されている場合は、停止なしでiSCSI Gatewayをローリングアップグレードできます。
      </para>
     </listitem>
     <listitem>
      <para>
       各アップグレード後に<systemitem class="daemon">lrbd</systemitem>デーモンが実行されていることを検証し、イニシエータでテストします。
      </para>
     </listitem>
     <listitem>
      <para>
       <xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     NFS Ganesha。<xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Samba Gateway。<xref linkend="upgrade-one-node"/>で説明されている手順を使用します。<emphasis role="bold"/>
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="upgrade-main-policy">
  <title><filename>policy.cfg</filename>の更新、およびDeepSeaを使用したCephダッシュボードの展開</title>

  <para>
   管理ノードで、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>を編集して、次の変更を適用します。
  </para>

  <important>
   <title>新しいサービスを追加しない</title>
   <para>
    クラスタのアップグレード中に、新しいサービスを<filename>policy.cfg</filename>ファイルに追加しないでください。クラスタアーキテクチャの変更は、アップグレードが完了した後にのみ行ってください。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     <literal>role-openattic</literal>を削除します。
    </para>
   </step>
   <step>
    <para>
     PrometheusおよびGrafanaがインストールされているノード(通常は管理ノード)に、<literal>role-prometheus</literal>および<literal>role-grafana</literal>を追加します。
    </para>
   </step>
   <step>
    <para>
     役割<literal>profile-<replaceable>PROFILE_NAME</replaceable></literal>は無視されるようになりました。対応する新しい役割である<literal>role-storage</literal>の行を追加します。たとえば、次のような既存の行があるとします。
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     この場合、次の行を追加します。
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     すべてのSaltモジュールを同期します。
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     DeepSeaステージ1およびステージ2を実行して、Salt Pillarを更新します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     openATTICのクリーンアップ
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
<prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic</screen>
   </step>
   <step>
    <para>
     ステージ0で、まだインストールされていないiSCSI Gatewayが再起動されないようにするには、「restart_igw」グレインを設定解除します。
    </para>
<screen>Salt mastersalt '*' grains.delkey restart_igw</screen>
   </step>
   <step>
    <para>
     最後に、DeepSeaステージ0～4を実行します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <tip>
     <title>ステージ3での「サブボリュームがみつからない」というエラー</title>
     <para>
      DeepSeaステージ3が次のようなエラーで失敗する場合があります。
     </para>
<screen>subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</screen>
     <para>
      この場合、<filename role="bold">/srv/pillar/ceph/stack/global.yml</filename>を編集して、次の行を追加する必要があります。
     </para>
<screen>subvolume_init: disabled</screen>
     <para>
      その後、Salt Pillarを更新し、DeepSeaステージ3を再実行します。
     </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar
 <prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     <para>
      DeepSeaステージ3が正常に完了すると、Cephダッシュボードが実行されます。Cephダッシュボードの機能の詳細な概要については、<xref linkend="ceph-dashboard"/>を参照してください。
     </para>
     <para>
      ダッシュボードが実行されているノードを一覧にするには、次のコマンドを実行します。
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mgr services | grep dashboard</screen>
     <para>
      管理者資格情報を一覧にするには、次のコマンドを実行します。
     </para>
<screen><prompt>root@master # </prompt>salt-call grains.get dashboard_creds</screen>
    </tip>
   </step>
   <step>
    <para>
     古い「civetweb」の代わりに「beast」Webサーバを使用するため、Object Gatewayサービスを順番に再起動します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.rgw.force</screen>
   </step>
   <step>
    <para>
     続行する前に、Cephテレメトリモジュールを有効にすることを強くお勧めします。情報および手順の詳細については、<xref linkend="mgr-modules-telemetry"/>を参照してください。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-drive-groups">
  <title>プロファイルベースの展開からDriveGroupsへのマイグレーション</title>

  <para>
   SUSE Enterprise Storage 5.5では、OSDのレイアウトを記述するため、DeepSeaによっていわゆる「プロファイル」が提供されていました。SUSE Enterprise Storage 6から、「<emphasis/>DriveGroups」という別のアプローチに移行しました(詳細については、<xref linkend="ds-drive-groups"/>を参照)。
  </para>

  <note>
   <para>
    直ちに新しいアプローチに移行しなければならないわけではありません。<command>salt-run osd.remove</command>、<command>salt-run osd.replace</command>、または<command>salt-run osd.purge</command>などの破壊的操作は、引き続き使用できます。ただし、新しいOSDを追加するには、アクションが必要です。
   </para>
  </note>

  <para>
   これらの実装でアプローチが異なるため、自動マイグレーションパスは提供されていません。ただし、マイグレーションをできる限り簡単にするために、Saltランナなどのさまざまなツールが提供されています。
  </para>

  <sect2>
   <title>現在のレイアウトの分析</title>
   <para>
    現在展開されているOSDの情報を表示するには、次のコマンドを使用します。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.discover
</screen>
   <para>
    または、<filename>/srv/pillar/ceph/proposals/profile-*/</filename>ディレクトリにあるファイルの内容を調べることもできます。ディレクトリは次のような構造になっています。
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>現在のレイアウトに一致するDriveGroupsの作成</title>
   <para>
    DriveGroupsの指定の詳細については、<xref linkend="ds-drive-groups-specs"/>を参照してください。
   </para>
   <para>
    新規展開とアップグレードシナリオの違いは、移行するドライブがすでに「使用中」である点です。　
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
   <para>
    上のコマンドは、使われていないディスクのみを検索するため、次のコマンドを使用します。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list include_unavailable=True
</screen>
   <para>
    現在のセットアップに一致するまで、DriveGroupsを調整します。どのような状態になるかをより視覚的に表示するには、次のコマンドを使用します。空きディスクがない場合は、何も出力されないことに注意してください。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report bypass_pillar=True
</screen>
   <para>
    DriveGroupsが適切に設定されていることを確認し、新しいアプローチを適用する場合は、<filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename>ディレクトリからファイルを削除して、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>ファイルから対応する<literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal>の行を削除し、DeepSeaステージ2を実行してSalt Pillarを更新します。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
</screen>
   <para>
    次のコマンドを実行して、結果を確認します。
   </para>
<screen>
<prompt>root@master # </prompt>salt target_node pillar.get ceph:storage
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   <warning>
    <title>DriveGroupsの誤った設定</title>
    <para>
     DriveGroupsが適切に設定されていない状況で、セットアップにスペアディスクがある場合、ディスクは、指定したとおりに展開されます。次のコマンドを実行することをお勧めします。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2 xml:id="upgrade-osd-deployment">
   <title>OSDの展開</title>
   <para>
    スタンドアロンのOSDなどの単純なケースでは、マイグレーションは時間をかけて実行します。クラスタのOSDを削除するか置き換える場合は、常にLVMベースの新しいOSDで置き換えられます。
   </para>
   <tip>
    <title>LVM形式への移行</title>
    <para>
     ノードで1つの「レガシ」OSDを置き換える必要がある場合は常に、そのOSDとデバイスを共有しているすべてのOSDをLVMベースの形式に移行する必要があります。
    </para>
    <para>
     完全性を確保するため、ノード全体でOSDを移行することを検討してください。
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>より複雑なセットアップ</title>
   <para>
    専用のWAL/DBや暗号化されたOSDなど、単なるスタンドアロンのOSDよりも高度なセットアップを使用している場合は、そのWAL/DBデバイスに割り当てられているすべてのOSDを削除する場合にのみマイグレーションを実行できます。これは<command>ceph-volume</command>コマンドによるもので、このコマンドはディスク上に論理ボリュームを作成してから展開を行うためです。これにより、ユーザがパーティションベースの展開とLVベースの展開を混在できないようにします。このような場合は、WAL/DBデバイスに割り当てられているすべてのOSDを手動で削除し、DriveGroupsアプローチを使用してOSDを再展開することをお勧めします。
   </para>
  </sect2>
 </sect1>
</chapter>
