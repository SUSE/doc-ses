<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha.ceph.upgrade">
 <title>古いリリースからのアップグレード</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編集</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  この章では、SUSE Enterprise Storageを旧リリースから最新リリースにアップグレードする手順について説明します。
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>リリースノートの確認</title>

  <para>
   リリースノートには、旧リリースのSUSE Enterprise Storageからの変更点に関する追加情報が記載されています。リリースノートを参照して以下を確認します。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     使用しているハードウェアに特別な配慮が必要かどうか
    </para>
   </listitem>
   <listitem>
    <para>
     使用しているソフトウェアパッケージに大幅な変更があるかどうか
    </para>
   </listitem>
   <listitem>
    <para>
     インストールのために特別な予防措置が必要かどうか
    </para>
   </listitem>
  </itemizedlist>

  <para>
   リリースノートには、マニュアルに記載できなかった情報が記載されています。また、既知の問題に関する注意も記載されています。
  </para>

  <para>
   パッケージ <package>release-notes-ses</package> をインストールすると、リリースノートは、ローカルではディレクトリ<filename>/usr/share/doc/release-notes</filename>に、オンラインでは<link xlink:href="https://www.suse.com/releasenotes/"/>にあります。
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>一般的なアップグレード手順</title>

  <para>
   アップグレード手順を開始する前に、次の項目を検討してください。
  </para>

  <variablelist>
   <varlistentry>
    <term>アップグレードの順序</term>
    <listitem>
     <para>
      Cephクラスタをアップグレードする前に、基礎となっているSUSE Linux Enterprise ServerとSUSE Enterprise Storageの両方をSCCまたはSMTに正しく登録する必要があります。クラスタのオンライン中およびサービス中に、クラスタ内のデーモンをアップグレードできます。特定のタイプのデーモンは他のデーモンに依存します。たとえば、Ceph Object Gatewayは、Ceph MonitorとCeph OSDの各デーモンに依存します。次の順序でアップグレードすることをお勧めします。
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Ceph Monitor
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph Manager
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph OSD
       </para>
      </listitem>
      <listitem>
       <para>
        Metadata Server
       </para>
      </listitem>
      <listitem>
       <para>
        Object Gateway
       </para>
      </listitem>
      <listitem>
       <para>
        iSCSI Gateway
       </para>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>不要なオペレーティングシステムスナップショットの削除</term>
    <listitem>
     <para>
      ノードのオペレーティングシステムパーティション上にある不要なファイルシステムスナップショットを削除します。これにより、アップグレード中に十分な空きディスク容量を確保します。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>クラスタヘルスの確認</term>
    <listitem>
     <para>
      アップグレード手順を開始する前に、クラスタヘルスを確認することをお勧めします。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>1つずつアップグレード</term>
    <listitem>
     <para>
      特定のタイプのすべてのデーモン(たとえば、すべてのモニタデーモンまたはすべてのOSDデーモン)を1つずつアップグレードして、すべてが同じリリースになるようにすることをお勧めします。さらに、リリースの新機能を使用してみる前に、クラスタ内のすべてのデーモンをアップグレードすることもお勧めします。
     </para>
     <para>
      特定のタイプのすべてのデーモンをアップグレードした後、デーモンの状態を確認します。
     </para>
     <para>
      すべてのモニタをアップグレードした後、各モニタがクォーラムに再度参加していることを確認します。
     </para>
<screen><prompt>root # </prompt>ceph mon stat</screen>
     <para>
      すべてのCeph OSDデーモンをアップグレードした後、各OSDがクラスタに再度参加していることを確認します。
     </para>
<screen><prompt>root # </prompt>ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     <option>require-osd-release luminous</option>フラグの設定
    </term>
    <listitem>
     <para>
      最後のOSDがSUSE Enterprise Storage 5にアップグレードされると、モニタノードにより、すべてのOSDが「luminous」バージョンのCephを実行していることが検出されます。その際、osdmapのフラグ<option>require-osd-release luminous</option>が設定されていないと報告される場合があります。その場合は、このフラグを手動で設定し、クラスタはすでに「luminous」にアップグレードされているので、Ceph「jewel」にダウングレードできないことを確認する必要があります。このフラグを設定するには、次のコマンドを実行します。
     </para>
<screen><prompt>root@minion &gt; </prompt>sudo ceph osd require-osd-release luminous</screen>
     <para>
      コマンドが完了すると、警告は表示されなくなります。
     </para>
     <para>
      SUSE Enterprise Storage 5を新規インストールする場合、このフラグは、Ceph Monitorが初期osdmapを作成するときに自動的に設定されるため、エンドユーザによる操作は必要ありません。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds.migrate.osd.encrypted">
  <title>アップグレード中のOSDの暗号化</title>

  <para>
   SUSE Enterprise Storage 5から、OSDはデフォルトでFileStoreではなくBlueStoreを使用して展開されるようになりました。BlueStoreは暗号化をサポートしていますが、Ceph OSDはデフォルトでは暗号化されていない状態で展開されます。次の手順では、アップグレードプロセス中にOSDを暗号化する手順について説明します。OSDの展開に使用するデータディスクとWAL/DBディスクはどちらもパーティションのないクリーンな状態であると仮定します。以前に使用したことがあるディスクの場合は、<xref linkend="deploy.wiping.disk"/>で説明する手順に従って消去してください。
  </para>

  <important>
   <title>一度に1つのOSD</title>
   <para>
    暗号化されたOSDは、同時にではなく1つずつ展開する必要があります。OSDのデータが削除され、クラスタが何度もリバランスを繰り返すことになるためです。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     展開に使用する<option>bluestore block db size</option>および<option>bluestore block wal size</option>の値を決定して、Salt Master上の<filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename>ファイルに追加します。値はバイトで指定する必要があります。
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     <filename>ceph.conf</filename>ファイルのカスタマイズの詳細については、<xref linkend="ds.custom.cephconf"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     DeepSeaステージ3を実行して変更を配布します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     関係するOSDノードで<filename>ceph.conf</filename>ファイルが更新されていることを確認します。
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     暗号化するOSDに関係がある<filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename>ディレクトリにある*.ymlファイルを編集します。そのパスを、<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>ファイルで定義されているパスに照らして再チェックして、正しい*.ymlファイルを変更していることを確認します。
    </para>
    <important>
     <title>長いディスクID</title>
     <para>
      <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename>ファイルでOSDディスクを識別する場合、長いディスクIDを使用します。
     </para>
    </important>
    <para>
     次に、OSDの設定例を示します。暗号化が必要なので、<option>db_size</option>と<option>wal_size</option>のオプションが削除されていることに注意してください。
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     DeepSeaステージ2と3を実行し、新しいブロックストレージOSDを暗号化して展開します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    <para>
     <command>ceph -s</command>または<command>ceph osd tree</command>で進行状況を監視できます。このプロセスを次のOSDノードで繰り返す前に、クラスタにリバランスを実行させることが重要です。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>SUSE Enterprise Storage 4 (DeepSeaによる展開)から5へのアップグレード</title>

  <important xml:id="u4to5.softreq">
   <title>ソフトウェアの必要条件</title>
   <para>
    アップグレード手順を開始するには、アップグレードするすべてのCephノードに次のソフトウェアがインストールされていて、最新のパッケージバージョンに更新されている必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    さらに、アップグレードを開始する前に、<command>zypper migration</command> (または好みのアップグレード方法)を実行して、Salt MasterノードをSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5にアップグレードする必要もあります。
   </para>
  </important>

  <warning>
   <title>アップグレード前に考慮すべき点</title>
   <itemizedlist>
    <listitem>
     <para>
      AppArmorサービスが実行されているかどうかを確認し、このサービスを各クラスタノードで無効にします。YaST AppArmorモジュールを起動して、<guimenu>Settings (設定)</guimenu>を選択し、<guimenu>Enable Apparmor (Apparmorの有効化)</guimenu>チェックボックスをオフにします。<guimenu>Done (完了)</guimenu>をクリックして確認します。
     </para>
     <para>
      SUSE Enterprise Storageは、AppArmorが有効な状態では「動作しない」<emphasis/>ことに注意してください。
     </para>
    </listitem>
    <listitem>
     <para>
      アップグレード中、クラスタは完全に機能しますが、DeepSeaは「noout」フラグを設定し、ダウンタイム中にCephがデータをリバランスしないようにして、不要なデータ転送を防止します。
     </para>
    </listitem>
    <listitem>
     <para>
      アップグレードプロセスを最適化するため、DeepSeaは、Cephのアップストリームによって推奨される割り当て済みの役割に基づいて、ノードをMON、MGR、OSD、MDS、RGW、IGW、NFS Ganeshaの順序でアップグレードします。
     </para>
     <para>
      ノードで複数のサービスが実行されている場合、DeepSeaは規定の順序に違反するのを防止できません。
     </para>
    </listitem>
    <listitem>
     <para>
      アップグレード中もCephクラスタは動作しますが、新しいカーネルバージョンなどを適用するためにノードが再起動されることがあります。待機中のI/O操作を減らすため、アップグレードプロセス中は着信要求を拒否することをお勧めします。
     </para>
    </listitem>
    <listitem>
     <para>
      クラスタのアップグレードには非常に長い時間がかかることがあります。所要時間は、1台のマシンのアップグレード時間xクラスタノード数です。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph Luminousから、設定オプション<option>osd crush location</option>はサポートされなくなりました。アップグレード前に、DeepSea設定ファイルを、<command>crush location</command>を使用するように更新してください。
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   SUSE Enterprise Storage 4クラスタをバージョン5にアップグレードするには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     新しい内部オブジェクトソート順序を設定して、次のコマンドを実行します。
    </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      コマンドが成功したことを確認するため、次のコマンドを実行することをお勧めします。
     </para>
<screen><prompt>root # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     <command>rpm -q deepsea</command>を使用して、Salt MasterノードのDeepSeaパッケージのバージョンが<literal>0.7</literal>以上で始まることを確認します。次に例を示します。
    </para>
<screen><prompt>root # </prompt>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     DeepSeaパッケージのバージョン番号が0.6で始まる場合は、Salt MasterノードがSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5に正常に移行されているかどうかを再確認します(このセクションの冒頭にある<xref linkend="u4to5.softreq"/>を参照してください)。これは、アップグレード手順の開始前に完了する必要がある前提条件です。
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       システムをSUSEConnectに登録済みでSCC/SMTを使用している場合は、操作はこれ以上必要ありません。<xref linkend="step.updatepillar"/>に進みます。
      </para>
     </step>
     <step>
      <para>
       <emphasis role="bold"/>メディアISOまたは他のパッケージソース以外にSCC/SMTを使用して「いない」場合は、SLE12-SP3 Base、SLE12-SP3 Update、SES5 Base、およびSES5 Updateの各リポジトリを手動で追加します。このためには、<command>zypper</command>コマンドを使用します。まず既存のソフトウェアリポジトリをすべて削除してから、必要なソフトウェアリポジトリを新しく追加し、最後にリポジトリソースを更新します。
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       続いて、異なるストラテジーを使用するためにPillarデータを変更します。次のファイルを編集します。
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       次の行を追加します。
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        <literal>zypper-dup</literal>ストラテジーでは最新のソフトウェアリポジトリを手動で追加する必要がありますが、デフォルトの<literal>zypper-migration</literal>はSCC/SMTによって提供されるリポジトリに依存します。
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     Pillarを更新します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     Salt Minionのターゲット設定の詳細については、<xref linkend="ds.minion.targeting"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     Pillarに正常に書き込まれたことを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     コマンドの出力が追加したエントリと同じである必要があります。
    </para>
   </step>
   <step>
    <para>
     Salt Minionをアップグレードします。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     すべてのSalt Minionがアップグレードされたことを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     クラスタのSalt Minionを含めます。詳細については、<xref linkend="ds.depl.stages"/>の<xref linkend="ds.minion.targeting"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     Linux Enterprise ServerとCephのアップグレードを開始します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>再起動時に再実行</title>
     <para>
      このプロセスによってSalt Masterが再起動される場合は、コマンドを再実行して、もう一度Salt Minionのアップグレードプロセスを開始します。
     </para>
    </tip>
   </step>
   <step>
    <para>
     アップグレード後、すべてのノードでAppArmorが無効になっていて停止されていることを確認します。
    </para>
<screen><prompt>root # </prompt>systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     アップグレードが完了しても、Ceph Managerはまだインストールされていません。クラスタを正常な状態にするため、以下を実行します。
    </para>
    <substeps>
     <step>
      <para>
       ステージ0を実行して、Salt REST APIを有効にします。
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       ステージ1を実行して、<filename>role-mgr/</filename>サブディレクトリを作成します。
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       <xref linkend="policy.configuration"/>の説明に従って<guimenu>policy.cfg</guimenu>を編集し、Ceph Monitorが展開されているノードにCeph Managerの役割を追加します。さらに、クラスタノードの1つにopenATTICの役割も追加します。詳細については、<xref linkend="ceph.oa"/>を参照してください。
      </para>
     </step>
     <step>
      <para>
       ステージ2を実行してPillarを更新します。
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       DeepSeaは、<filename>ceph.conf</filename>設定ファイルを生成するのに別のアプローチを使用するようになりました。詳細については、<xref linkend="ds.custom.cephconf"/>を参照してください。
      </para>
     </step>
     <step>
      <para>
       ステージ3を実行してCeph Managerを展開します。
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       ステージ4を実行してopenATTICを適切に設定します。
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>Cephキーのケーパビリティの不一致</title>
     <para>
      <literal>ceph.stage.3</literal>が失敗して「Error EINVAL: entity client.bootstrap-osd exists but caps do not match」というエラーが表示される場合は、既存クラスタの<literal>client.bootstrap.osd</literal>のキーのケーパビリティ(caps)が、DeepSeaが設定しようとしているキーのケーパビリティと一致しないことを意味します。上のエラーメッセージの上に赤いテキストで、失敗した<command>ceph auth</command>コマンドのダンプが表示されます。このコマンドを参照して、使用しているキーIDとファイルを確認します。<literal>client.bootstrap-osd</literal>の場合、コマンドは次のようになります。
     </para>
<screen><prompt>root # </prompt>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      一致しないキーのケーパビリティを修正するには、DeepSeaが展開しようとしているキーリングファイルの内容を確認します。たとえば、次のようにします。
     </para>
<screen><prompt>cephadm &gt; </prompt>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      これを<command>ceph auth get client.bootstrap-osd</command>の出力と比較します。
     </para>
<screen><prompt>root # </prompt>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      <literal>caps mgr = "allow r"</literal>では後者のキーがどのように欠落しているかに注意してください。これを修正するため、次のコマンドを実行します。
     </para>
<screen><prompt>root # </prompt>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      これで<literal>ceph.stage.3</literal>の実行に成功します。
     </para>
     <para>
      <literal>ceph.stage.4</literal>を実行する際にも、Metadata ServerおよびObject Gatewayのキーリングで同じ問題が発生する可能性があります。上と同じ手順が適用されます。失敗したコマンド、展開するキーリングファイル、および既存のキーのケーパビリティを確認します。続いて、<command>ceph auth caps</command>を実行して、既存のキーのケーパビリティを、DeepSeaによって展開されるキーのケーパビリティに一致するように更新します。
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>アップグレードが失敗する場合</title>
   <para>
    クラスタが300秒以上「HEALTH_ERR」状態であるか、割り当てられた役割のサービスの1つが900秒以上ダウンしている場合、アップグレードは失敗します。その場合、問題を特定して解決し、アップグレード手順をもう一度実行してみてください。仮想化環境では、タイムアウトはこれより短くなることに注意してください。
   </para>
  </important>

  <important>
   <title>OSDの再起動</title>
   <para>
    SUSE Enterprise Storage 5にアップグレードした後、FileStore OSDはディスク上のファイルに対して1回限りの変換を行うので、OSDに必要な時間は約5分長くなります。
   </para>
  </important>

  <tip>
   <title>クラスタコンポーネント/ノードのバージョンの確認</title>
   <para>
    たとえば、アップグレード後にすべてのノードが実際に同じパッチレベルになっているかどうかを確認する場合など、個々のクラスタコンポーネントとノードのバージョンを確認する必要がある場合、次のコマンドを実行できます。
   </para>
<screen><prompt>root@master # </prompt>salt-run status.report</screen>
   <para>
    このコマンドは、接続されているSalt Minionを調べて、Ceph、Salt、およびSUSE Linux Enterprise Serverのバージョン番号をスキャンし、過半数のノードのバージョンと、過半数とは異なるバージョン番号を持つノードを表示するレポートを生成します。
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>BlueStoreへのOSDのマイグレーション</title>
   <para>
    OSD BlueStoreは、OSDデーモン用の新しいバックエンドです。SUSE Enterprise Storage 5からデフォルトのオプションになっています。FileStoreがオブジェクトをファイルとしてXFSファイルシステムに保存するのに対し、BlueStoreは基礎となるブロックデバイスにオブジェクトを直接保存するので、パフォーマンスを向上させることができます。BlueStoreでは、組み込みの圧縮、イレージャコーディングの上書きなど、FileStoreでは利用できないほかの機能も実現されます。
   </para>
   <para>
    特にBlueStoreでは、OSDは「wal」(Write Ahead Log、先書きログ)デバイスと「db」(RocksDBデータベース)デバイスを持っています。RocksDBデータベースはBlueStore OSDのメタデータを格納します。これら2つのデバイスは、デフォルトではOSDと同じデバイス上に存在しますが、いずれかを高速な別のメディアに配置できます。
   </para>
   <para>
    SES5では、FileStoreとBlueStoreがサポートされており、FileStoreとBlueStoreのOSDが1つのクラスタに共存できます。SUSE Enterprise Storageのアップグレード手順中に、FileStore OSDは自動的にBlueStoreに変換されます。BlueStoreに移行されていないOSDではBlueStore固有の機能は利用できないので注意してください。
   </para>
   <para>
    BlueStoreに変換する前に、OSDでSUSE Enterprise Storage 5が実行されている必要があります。すべてのデータを2回再書き込みするので、変換には時間がかかります。このマイグレーションプロセスの完了には長い時間がかかる可能性がありますが、クラスタの停止はなく、この時間中、すべてのクライアントは引き続きクラスタにアクセスできます。ただし、マイグレーション中はパフォーマンスが低下することを見込んでおいてください。これは、クラスタデータのリバランスとバックフィルが原因です。
   </para>
   <para>
    FileStore OSDをBlueStoreに移行するには、次の手順を使用します。
   </para>
   <tip>
    <title>安全対策をオフにする</title>
    <para>
     マイグレーションを実行するために必要なSaltコマンドは、安全対策によってブロックされます。これらの予防措置をオフにするには、次のコマンドを実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      ハードウェアプロファイルを移行します。
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.policy</screen>
     <para>
      このランナは、現在<filename>policy.cfg</filename>ファイルで使用されているすべてのハードウェアプロファイルを移行します。<filename>policy.cfg</filename>を処理し、元のデータ構造を使用しているハードウェアプロファイルを見つけて新しいデータ構造に変換します。その結果、「migrated-<replaceable>original_name</replaceable>」という名前の新しいハードウェアプロファイルが作成されます。<filename>policy.cfg</filename>も更新されます。
     </para>
     <para>
      元の設定に別個のジャーナルがあった場合は、BlueStore設定はそのOSDの「wal」と「db」に同じデバイスを使用します。
     </para>
    </step>
    <step>
     <para>
      DeepSeaは、OSDの重みを0に設定してOSDが空になるまでデータを「バキューム処理」することによって、OSDを移行します。OSDを1つずつ移行することも、すべてのOSDを一度に移行することもできます。どちらの場合も、OSDが空になったら、そのOSDはオーケストレーションによって削除され、新しい設定で再作成されます。
     </para>
     <tip>
      <title>推奨方法</title>
      <para>
       大量の物理ストレージノードがあるか、データがほとんどない場合は、<command>ceph.migrate.nodes</command>を使用します。1つのノードが容量の10%未満相当である場合、<command>ceph.migrate.nodes</command>は、すべてのデータをOSDから並行して移動するので、若干高速になることがあります。
      </para>
      <para>
       どの方法を使用すべきかわからない場合、またはサイトにストレージノードがほとんどない場合(たとえば、各ノードにクラスタデータの10%以上がある場合など)は、<command>ceph.migrate.osds</command>を選択します。
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        複数のOSDを一度に移行するには、次のコマンドを実行します。
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        各ノードのすべてのOSDを並行して移行するには、次のコマンドを実行します。
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       オーケストレーションではマイグレーションの進行状況についてフィードバックが表示されないため、次のコマンドを使用します。
      </para>
<screen><prompt>root # </prompt>ceph osd tree</screen>
      <para>
       これによって、どのOSDの重みが0かを定期的に確認できます。
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    BlueStoreへのマイグレーション後、オブジェクト数は同じままで、ディスク使用量もほぼ同じになります。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>SUSE Enterprise Storage 4 (<command>ceph-deploy</command>での展開)から5へのアップグレード</title>

  <important>
   <title>ソフトウェアの必要条件</title>
   <para>
    アップグレード手順を開始するには、アップグレードするすべてのCephノードに次のソフトウェアがインストールされていて、最新のパッケージバージョンに更新されている必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    クラスタのSalt Masterを選択します。クラスタにCalamariが展開されている場合、Calamariノードはすでに<emphasis/>Salt Masterになっています。または、<command>ceph-deploy</command>コマンドを実行した管理ノードがSalt Masterになります。
   </para>
   <para>
    アップグレードを開始する前に、<command>zypper migration</command> (または好みのアップグレード方法)を実行して、Salt MasterノードをSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5にアップグレードする必要があります。
   </para>
  </important>

  <para>
   <command>ceph-deploy</command>で展開されたSUSE Enterprise Storage 4クラスタをバージョン5にアップグレードするには、次の手順に従います。
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>すべてのクラスタノード(Calamariノードを含む)に適用する手順</title>
   <step>
    <para>
     SLE-12-SP2/SES4から<systemitem>salt</systemitem>パッケージをインストールします。
    </para>
<screen><prompt>root # </prompt>zypper install salt</screen>
   </step>
   <step>
    <para>
     SLE-12-SP2/SES4から<systemitem>salt-minion</systemitem>パッケージをインストールし、関連サービスを有効にして起動します。
    </para>
<screen><prompt>root # </prompt>zypper install salt-minion
<prompt>root # </prompt>systemctl enable salt-minion
<prompt>root # </prompt>systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     ホスト名「salt」がSalt MasterノードのIPアドレスに解決されることを確認します。ホスト名<literal>salt</literal>でSalt Masterに接続できない場合は、ファイル<filename>/etc/salt/minion</filename>を編集するか、次の内容で新しいファイル<filename>/etc/salt/minion.d/master.conf</filename>を作成します。
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      既存のSalt Minionには、すでに<filename>/etc/salt/minion.d/calamari.conf</filename>に<option>master:</option>オプションが設定されています。設定ファイル名は重要ではなく、<filename>/etc/salt/minion.d/</filename>ディレクトリが重要です。
     </para>
    </tip>
    <para>
     上で説明した設定ファイルを変更した場合は、すべてのSalt MinionのSaltサービスを再起動します。
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       システムをSUSEConnectに登録済みでSCC/SMTを使用している場合は、操作はこれ以上必要ありません。
      </para>
     </step>
     <step>
      <para>
       <emphasis role="bold"/>メディアISOまたは他のパッケージソース以外にSCC/SMTを使用して「いない」場合は、SLE12-SP3 Base、SLE12-SP3 Update、SES5 Base、およびSES5 Updateの各リポジトリを手動で追加します。このためには、<command>zypper</command>コマンドを使用します。まず既存のソフトウェアリポジトリをすべて削除してから、必要なソフトウェアリポジトリを新しく追加し、最後にリポジトリソースを更新します。
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Salt Masterノードに適用する手順</title>
   <step>
    <para>
     新しい内部オブジェクトソート順序を設定して、次のコマンドを実行します。
    </para>
<screen><prompt>root@master # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      コマンドが成功したことを確認するため、次のコマンドを実行することをお勧めします。
     </para>
<screen><prompt>root@master # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     Salt MasterノードをSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5にアップグレードします。SCCに登録済みのシステムの場合は、<command>zypper migration</command>を使用します。必要なソフトウェアリポジトリを手動で提供する場合は、<command>zypper dup</command>を使用します。アップグレード後、次の手順に進む前に、Salt Masterノード上でSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5用のリポジトリのみがアクティブであること(および更新されていること)を確認します。
    </para>
   </step>
   <step>
    <para>
     <systemitem>salt-master</systemitem>パッケージがまだ存在しない場合はインストールし、関連サービスを有効にして起動します。
    </para>
<screen><prompt>root@master # </prompt>zypper install salt-master
<prompt>root@master # </prompt>systemctl enable salt-master
<prompt>root@master # </prompt>systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     Salt Minionのキーを表示して、すべてのSalt Minionが存在していることを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     ミニオンマスタを含むすべてのSalt MinionのキーをSalt Masterに追加します。
    </para>
<screen><prompt>root@master # </prompt>salt-key -A -y</screen>
   </step>
   <step>
    <para>
     すべてのSalt Minionのキーが受け付けられたことを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     Salt Masterノードのソフトウェアが最新であることを確認します。
    </para>
<screen><prompt>root@master # </prompt>zypper migration</screen>
   </step>
   <step>
    <para>
     <systemitem>deepsea</systemitem>パッケージをインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     クラスタのSalt Minionを含めます。詳細については、<xref linkend="ds.depl.stages"/>の<xref linkend="ds.minion.targeting"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     <command>ceph-deploy</command>でインストールされた既存のクラスタをインポートします。
    </para>
<screen><prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster</screen>
    <para>
     このコマンドは以下を実行します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SaltおよびDeepSeaのすべての必須モジュールをすべてのSalt Minionに分散する。
      </para>
     </listitem>
     <listitem>
      <para>
       実行中のCephクラスタを検査して、<filename>/srv/pillar/ceph/proposals</filename>にクラスタのレイアウトを設定する。
      </para>
      <para>
       <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>が作成され、検出されたすべての実行中のCephサービスに一致する役割が記述されます。このファイルを参照して、既存のMON、OSD、RGW、およびMDSの各ノードが適切な役割を持っていることを確認します。OSDノードは<filename>profile-import/</filename>サブディレクトリにインポートされるため、<filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename>と<filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename>にあるファイルを調べると、OSDが正しく取得されていることを確認できます。
      </para>
      <note>
       <para>
        生成された<filename>policy.cfg</filename>は、Salt Masterノードの検出されたCephサービス「role-mon」「role-mgr」「role-mds」「role-rgw」「role-admin」および「role-master」の役割のみを適用します。ほかに必要な役割がある場合は手動で追加する必要があります(<xref linkend="policy.role.assignment"/>を参照してください)。
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       既存のクラスタの<filename>ceph.conf</filename>は<filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>に保存されます。
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>には、クラスタのfsid、クラスタ、およびパブリックネットワークが含まれ、<option>configuration_init: default-import</option>オプションも指定されています。このオプションにより、DeepSeaが、DeepSeaのデフォルトのテンプレート<filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>ではなく、前に説明した<filename>ceph.conf.import</filename>設定ファイルを使用するようにします。
      </para>
      <note>
       <title>カスタムの<filename>ceph.conf</filename></title>
       <para>
        <filename>ceph.conf</filename>ファイルにカスタムの変更を統合する必要がある場合は、インポート/アップグレードプロセスが正常に完了するまで待ってください。その後、<filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>ファイルを編集して、次の行をコメント化します。
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        ファイルを保存して、<xref linkend="ds.custom.cephconf"/>の情報に従います。
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       クラスタのさまざまなキーリングは次のディレクトリに保存されます。
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       キーリングファイルが存在すること、および次のディレクトリにキーリングファイルが「存在しない」<emphasis/>ことを確認します(Ceph ManagerはSUSE Enterprise Storage 5より前には存在していませんでした)。
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command>コマンドは、openATTIC設定のインポート処理は行いません。<filename>policy.cfg</filename>ファイルを手動で編集して、<literal>role-openattic</literal>の行を追加する必要があります。詳細については、<xref linkend="policy.configuration"/>を参照してください。
    </para>
   </step>

   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command>コマンドは、iSCSI Gateway設定のインポート処理は行いません。クラスタにiSCSI Gatewayが含まれている場合は、次の方法で設定を手動でインポートします。
    </para>
    <substeps>
     <step>
      <para>
       いずれかのiSCSI Gatewayノードで、現在の<filename>lrbd.conf</filename>をエクスポートしてSalt Masterノードにコピーします。
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt;/tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       Salt Masterノードで、デフォルトのiSCSI Gateway設定をDeepSeaのセットアップに追加します。
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       iSCSI Gatewayの役割を<filename>policy.cfg</filename>に追加して、ファイルを保存します。
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     ステージ1を実行して、想定される役割をすべて作成します。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     <filename>/srv/pillar/ceph/stack</filename>の下層に必要なサブディレクトリを生成します。
    </para>
<screen><prompt>root@master # </prompt>salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     DeepSeaによって管理されている動作中のクラスタが存在し、正しく割り当てられた役割を持っていることを確認します。
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     出力をクラスタの実際のレイアウトと比較します。
    </para>
   </step>
   <step>
    <para>
     Calamariは、クラスタの状態を確認するため、Saltのスケジュールジョブを実行中のままにします。ジョブを削除します。
    </para>
<screen>
<prompt>root@minion &gt; </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
   </step>
   <step>
    <para>
     ここからは、<xref linkend="ceph.upgrade.4to5"/>で説明する手順に従います。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5crowbar">
  <title>SUSE Enterprise Storage 4 (Crowbar展開)から5へのアップグレード</title>

  <important>
   <title>ソフトウェアの必要条件</title>
   <para>
    アップグレード手順を開始するには、アップグレードするすべてのCephノードに次のソフトウェアがインストールされていて、最新のパッケージバージョンに更新されている必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   Crowbarで展開されたSUSE Enterprise Storage 4をバージョン5にアップグレードするには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     各Cephノード(Calamariノードを含む)で、Crowbar関連サービスをすべて停止して無効にします。
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl stop chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_join
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     各Cephノード(Calamariノードを含む)で、ソフトウェアリポジトリがSUSE Enterprise Storage 5およびSUSE Linux Enterprise Server 12 SP3の各製品を指していることを確認します。古い製品バージョンを指しているリポジトリがまだ存在する場合は、それらのリポジトリを無効にします。
    </para>
   </step>
   <step>
    <para>
     各Cephノード(Calamariノードを含む)で、
     <package>salt-minion</package> がインストールされていることを確認します。インストールされていない場合は、インストールします。
    </para>
<screen><prompt>root@minion &gt; </prompt>sudo zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     パッケージ <package>salt-minion</package>
     がインストールされていない各Cephノードで、<option>master</option>オプションがCalamariノードの完全なホスト名を指すようにして、ファイル<filename>/etc/salt/minion.d/master.conf</filename>を作成します。
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      既存のSalt Minionには、すでに<filename>/etc/salt/minion.d/calamari.conf</filename>に<option>master:</option>オプションが設定されています。設定ファイル名は重要ではなく、<filename>/etc/salt/minion.d/</filename>ディレクトリが重要です。
     </para>
    </tip>
    <para>
     <systemitem class="daemon">salt-minion</systemitem>サービスを有効にして起動します。
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl enable salt-minion
<prompt>root@minion &gt; </prompt>sudo systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     Calamariノードで、残りのSalt Minionのキーを受け入れます。
    </para>
<screen>
<prompt>root@master # </prompt>salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

<prompt>root@master # </prompt>salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     Cephがパブリックネットワーク上に展開されていて、VLANインタフェースが存在しない場合は、Crowbarのパブリックネットワーク上のVLANインタフェースをCalamariノードに追加します。
    </para>
   </step>
   <step>
    <para>
     <command>zypper migration</command>または好みの方法を使用して、CalamariノードをSUSE Linux Enterprise Server 12 SP3およびSUSE Enterprise Storage 5にアップグレードします。ここから、Calamariノードが「Salt Master」<emphasis/>になります。アップグレード後、Salt Masterを再起動します。
    </para>
   </step>
   <step>
    <para>
     DeepSeaをSalt Masterにインストールします。
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     正しいSalt Minionグループが展開ステージに含まれるように<option>deepsea_minions</option>オプションを指定します。詳細については、<xref linkend="ds.minion.targeting.dsminions"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     DeepSeaでは、すべてのCephノードで<filename>/etc/ceph/ceph.conf</filename>が同一である必要があります。Crowbarが各ノードに展開する<filename>ceph.conf</filename>はノードごとに若干異なるので、統合する必要があります。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Calamariによって追加された<option>osd crush location hook</option>オプションを削除します。
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>[mon]</literal>セクションから<option>public addr</option>オプションを削除します。
      </para>
     </listitem>
     <listitem>
      <para>
       <option>mon host</option>オプションからポート番号を削除します。
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Object Gatewayが実行されていた場合、Crowbarは別個の<filename>/etc/ceph/ceph.conf.radosgw</filename>ファイルを展開して、Keystoneのシークレットを標準の<filename>ceph.conf</filename>ファイルとは別に保持します。さらにCrowbarは、カスタムの<filename>/etc/systemd/system/ceph-radosgw@.service</filename>ファイルも追加します。これはDeepSeaではサポートされないため、削除する必要があります。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       すべての<literal>[client.rgw....]</literal>セクションを、<filename>ceph.conf.radosgw</filename>ファイルから、すべてのノードの<filename>/etc/ceph/ceph.conf</filename>に追加します。
      </para>
     </listitem>
     <listitem>
      <para>
       Object Gatewayノードで、次のコマンドを実行します。
      </para>
<screen><prompt>root@minion &gt; </prompt>rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Salt Masterから実行した場合に<command>ceph status</command>が機能することを再確認します。
    </para>
<screen><prompt>root@master # </prompt>ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     既存のクラスタをインポートします。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run push.proposal
</screen>
   </step>

   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command>コマンドは、iSCSI Gateway設定のインポート処理は行いません。クラスタにiSCSI Gatewayが含まれている場合は、次の方法で設定を手動でインポートします。
    </para>
    <substeps>
     <step>
      <para>
       いずれかのiSCSI Gatewayノードで、現在の<filename>lrbd.conf</filename>をエクスポートしてSalt Masterノードにコピーします。
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt; /tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       Salt Masterノードで、デフォルトのiSCSI Gateway設定をDeepSeaのセットアップに追加します。
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       iSCSI Gatewayの役割を<filename>policy.cfg</filename>に追加して、ファイルを保存します。
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       システムをSUSEConnectに登録済みでSCC/SMTを使用している場合は、操作はこれ以上必要ありません。
      </para>
     </step>
     <step>
      <para>
       <emphasis role="bold"/>メディアISOまたは他のパッケージソース以外にSCC/SMTを使用して「いない」場合は、SLE12-SP3 Base、SLE12-SP3 Update、SES5 Base、およびSES5 Updateの各リポジトリを手動で追加します。このためには、<command>zypper</command>コマンドを使用します。まず既存のソフトウェアリポジトリをすべて削除してから、必要なソフトウェアリポジトリを新しく追加し、最後にリポジトリソースを更新します。
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       続いて、異なるストラテジーを使用するためにPillarデータを変更します。次のファイルを編集します。
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       次の行を追加します。
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        <literal>zypper-dup</literal>ストラテジーでは最新のソフトウェアリポジトリを手動で追加する必要がありますが、デフォルトの<literal>zypper-migration</literal>はSCC/SMTによって提供されるリポジトリに依存します。
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     DeepSeaがパブリックネットワーク上でCephデーモンインスタンスIDに対して短いホスト名を使用するようにホストgrainsを修正します。各ノードで、新しい(短い)ホスト名を使って<command>grains.set</command>を実行する必要があります。<command>grains.set</command>を実行する前に、<command>ceph status</command>を実行して、モニタの現在のインスタンスを確認します。実行前後の例を次に示します。
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
<prompt>root@master # </prompt>salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
<prompt>root@master # </prompt>salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
<prompt>root@master # </prompt>salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     アップグレードを実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version
<prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     すべてのノードが再起動します。クラスタが復帰し、Ceph Managerのアクティブなインスタンスがないことが報告されます。これは普通のことです。この時点では、これ以上Calamariをインストール/実行しないでください。
    </para>
   </step>
   <step>
    <para>
     必要な展開ステージをすべて実行して、クラスタを正常な状態にします。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     openATTIC (<xref linkend="ceph.oa"/>を参照)を展開するため、<literal>role-openattic</literal> (<xref linkend="policy.role.assignment"/>を参照)の適切な行を<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>に追加して、次のコマンドを実行します。
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     アップグレード中に、「Error EINVAL: entity [...] exists but caps do not match」というエラーが表示されることがあります。これを修正するには、<xref linkend="ceph.upgrade.4to5"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     残りのクリーンアップを実行します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Crowbarは、各OSDに対して<filename>/etc/fstab</filename>にエントリを作成します。これらは必要ないので削除します。
      </para>
     </listitem>
     <listitem>
      <para>
       Calamariは、クラスタの状態を確認するため、Saltのスケジュールジョブを実行中のままにします。ジョブを削除します。
      </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       不要なパッケージがまだいくつかインストールされています。その大半はRubyのGemやChef関連のものです。これらは必ずしも削除する必要はありませんが、削除する場合は<command>zypper rm <replaceable>pkg_name</replaceable></command>を実行できます。
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>SUSE Enterprise Storage 3から5へのアップグレード</title>

  <important>
   <title>ソフトウェアの必要条件</title>
   <para>
    アップグレード手順を開始するには、アップグレードするすべてのCephノードに次のソフトウェアがインストールされていて、最新のパッケージバージョンに更新されている必要があります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   SUSE Enterprise Storage 3クラスタをバージョン5にアップグレードするには、<xref linkend="upgrade4to5cephdeploy.all"/>で説明されている手順に従った後、<xref linkend="upgrade4to5cephdeploy.admin"/>の手順に従います。
  </para>
 </sect1>
</chapter>
