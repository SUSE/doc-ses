<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>クラスタ化ファイルシステム</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編集</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  この章では、通常はクラスタの設定とCephFSのエクスポート後に実行する管理タスクについて説明します。CephFSの設定の詳細については、<xref linkend="cha-ceph-as-cephfs"/>を参照してください。
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>CephFSのマウント</title>

  <para>
   ファイルシステムが作成されてMDSがアクティブになったら、クライアントホストからファイルシステムをマウントできます。
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>クライアントの準備</title>
   <para>
    クライアントホストがSUSE Linux Enterprise 12 SP2またはSP3を実行している場合、システムはCephFSを「設定なしで」すぐにマウントできるため、このセクションはスキップして構いません。
   </para>
   <para>
    クライアントホストがSUSE Linux Enterprise 12 SP1を実行している場合は、CephFSをマウントする前にすべての最新パッチを適用する必要があります。
   </para>
   <para>
    いずれの場合も、CephFSをマウントするのに必要なものはすべてSUSE Linux Enterpriseに付属しています。SUSE Enterprise Storage 6製品は必要ありません。
   </para>
   <para>
    完全な<command>mount</command>構文をサポートするには、CephFSのマウントを試みる前に、
    <package>ceph-common</package> パッケージ(SUSE Linux Enterpriseに付属)をインストールする必要があります。
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>シークレットファイル</title>
   <para>
    Cephクラスタは、デフォルトで認証がオンの状態で動作します。秘密鍵(キーリングそのものではない)を保存するファイルを作成する必要があります。特定のユーザの秘密鍵を入手してファイルを作成するには、次の操作を行います。
   </para>
   <procedure>
    <title>秘密鍵の作成</title>
    <step>
     <para>
      キーリングファイル内の特定のユーザの鍵を表示します。
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      マウントしたCephFS (Ceph File System)を使用するユーザの鍵をコピーします。通常、鍵は次のような形式です。
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      ファイル名の部分にユーザ名を使用してファイルを作成します。たとえば、ユーザ<emphasis>admin</emphasis>の場合は、<filename>/etc/ceph/admin.secret</filename>のようになります。
     </para>
    </step>
    <step>
     <para>
      前の手順で作成したファイルに鍵の値を貼り付けます。
     </para>
    </step>
    <step>
     <para>
      ファイルに適切なアクセス権を設定します。このユーザは、ファイルを読み込める唯一のユーザである必要があります。ほかのユーザは一切アクセス権を持つことはできません。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>CephFSのマウント</title>
   <para>
    <command>mount</command>コマンドでCephFSをマウントできます。Monitorのホスト名またはIPアドレスを指定する必要があります。SUSE Enterprise Storageでは<systemitem>cephx</systemitem>認証がデフォルトで有効になっているため、ユーザ名とその関連シークレットも指定する必要があります。
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    以前のコマンドはシェルの履歴に残るため、ファイルからシークレットを読み込むアプローチの方が安全です。
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    シークレットファイルには実際のキーリングシークレットだけが含まれる必要があることに注意してください。この例では、ファイルに含まれるのは次の行だけです。
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>複数のMonitorの指定</title>
    <para>
     マウント時に特定のMonitorがダウンしている事態に備え、<command>mount</command>コマンドラインで複数のMonitorをコンマで区切って指定することをお勧めします。各Monitorのアドレスは<literal>host[:port]</literal>という形式です。ポートを指定しない場合は、デフォルトで6789が使用されます。
    </para>
   </tip>
   <para>
    ローカルホストでマウントポイントを作成します。
   </para>
<screen><prompt>root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    CephFSをマウントします。
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    ファイルシステムのサブセットをマウントする場合は、サブディレクトリ<filename>subdir</filename>を指定できます。
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    <command>mount</command>コマンドで複数のMonitorホストを指定できます。
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>ルートディレクトリに対する読み込みアクセス</title>
    <para>
     パス制約付きのクライアントを使用する場合は、MDSのケーパビリティにルートディレクトリに対する読み込みアクセスを含める必要があります。たとえば、キーリングは次のようになります。
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     <literal>allow r path=/</literal>の部分は、パス制約付きのクライアントは、ルートボリュームを表示できても書き込みはできないことを意味します。これは、完全な分離が要件である使用事例で問題になることがあります。
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>CephFSのアンマウント</title>

  <para>
   CephFSをアンマウントするには、<command>umount</command>コマンドを使用します。
  </para>

<screen><prompt>root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title><filename>/etc/fstab</filename>でのCephFSの指定</title>

  <para>
   クライアントの起動時にCephFSを自動的にマウントするには、対応する行をファイルシステムテーブル<filename>/etc/fstab</filename>に挿入します。
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>複数のアクティブMDSデーモン(アクティブ-アクティブMDS)</title>

  <para>
   CephFSは、デフォルトでは単一のアクティブMDSデーモン用に設定されています。大規模システム用にメタデータのパフォーマンスを拡張する場合、複数のアクティブMDSデーモンを有効にできます。これにより、各デーモンがお互いにメタデータワークロードを共有します。
  </para>

  <sect2>
   <title>アクティブ-アクティブMDSを使用する状況</title>
   <para>
    デフォルトの単一のMDSではメタデータのパフォーマンスがボトルネックになる場合、複数のアクティブMDSデーモンの使用を検討します。
   </para>
   <para>
    デーモンを追加しても、すべてのワークロードタイプのパフォーマンスが向上するわけではありません。たとえば、単一のクライアント上で動作している単一のアプリケーションの場合、そのアプリケーションが大量のメタデータ操作を並列で実行していない限り、MDSデーモンの数を増やしてもメリットはありません。
   </para>
   <para>
    一般的に大量のアクティブMDSデーモンのメリットを受けられるワークロードは、クライアントが複数あり、多数の別個のディレクトリを操作する可能性が高いワークロードです。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>MDSのアクティブクラスタサイズの増加</title>
   <para>
    各CephFSファイルシステムには、作成するランクの数を制御する<option>max_mds</option>設定があります。ファイルシステム内の実際のランク数は、新しいランクを引き受けるスペアデーモンが利用可能な場合にのみ増やされます。たとえば、実行中のMDSデーモンが1つだけで、<option>max_mds</option>が2に設定されている場合、2番目のランクは作成されません。
   </para>
   <para>
    次の例では、<option>max_mds</option>オプションを2に設定して、デフォルトのランクとは別の新しいランクを作成します。変更を確認するには、<option>max_mds</option>の設定前と設定後に<command>ceph status</command>を実行し、<literal>fsmap</literal>が含まれる行を確認します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 2
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    新しく作成されたランク(1)は、「creating (作成中)」状態を経由して「active (アクティブ)」状態になります。
   </para>
   <important>
    <title>スタンバイデーモン</title>
    <para>
     複数のアクティブMDSデーモンを使用していても、高可用性システムには、アクティブデーモンを実行するサーバに障害が発生した場合に処理を引き継ぐスタンバイデーモンも必要です。
    </para>
    <para>
     そのため、高可用性システムの<option>max_mds</option>の実用的な最大数は、システムのMDSサーバの合計数から1を引いた数になります。複数のサーバ障害時に可用性を維持するには、切り抜ける必要があるサーバ障害の数に一致するようにシステムのスタンバイデーモンの数を増やします。
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>ランク数の減少</title>
   <para>
    最初に、すべてのランク(削除するランクを含む)がアクティブになっている必要があります。つまり、少なくとも<option>max_mds</option> MDSデーモンが利用可能である必要があります。
   </para>
   <para>
    最初に、<option>max_mds</option>をより低い数字に設定します。たとえば、単一のアクティブMDSに戻します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 1
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    これでもまだアクティブMDSが2つあることに注意してください。<option>max_mds</option>が制限するのは新しいランクの作成だけなので、<option>max_mds</option>を減らしても、これらのランクはまだ存在しています。
   </para>
   <para>
    次に、<command>ceph mds deactivate <replaceable>rank</replaceable></command>コマンドを使用して、不要なランクを削除します。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    無効にしたランクは、共有しているメタデータを残りのアクティブデーモンに引き渡す間、まず一定時間「stopping (停止中)」状態になります。このフェーズには数秒から数分かかる可能性があります。MDSが「stopping (停止中)」状態で止まっているように見える場合は、バグの可能性があるので調査が必要です。
   </para>
   <para>
    「stopping (停止中)」状態の間にMDSデーモンがクラッシュまたは終了した場合、スタンバイが処理を引き継ぎ、ランクが「active (アクティブ)」に戻ります。デーモンが復帰したら、もう一度無効にしてみることができます。
   </para>
   <para>
    停止を完了すると、デーモンはもう一度起動してスタンバイに戻ります。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>ランクへのディレクトリツリーの手動固定</title>
   <para>
    複数のアクティブMetadata Server設定では、バランサが動作し、メタデータの負荷をクラスタに均等に分散します。これは通常、ほとんどのユーザにとって十分有効に機能しますが、メタデータを特定のランクに明示的にマッピングして動的バランサを無効にした方が良い場合もあります。これにより、管理者やユーザは、アプリケーションの負荷を均等に分散したり、ユーザのメタデータ要求によるクラスタ全体への影響を抑えたりできます。
   </para>
   <para>
    このために提供されているメカニズムを「エクスポートピン」と呼びます。これはディレクトリの拡張属性です。この拡張属性の名前は<literal>ceph.dir.pin</literal>です。標準のコマンドを使用して、この属性を設定できます。
   </para>
<screen><prompt>root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    拡張属性の値(<option>-v</option>)は、ディレクトリサブツリーの割り当て先となるランクです。デフォルト値-1は、ディレクトリが固定されないことを示します。
   </para>
   <para>
    ディレクトリのエクスポートピンは、設定されているエクスポートピンを持つ最も近い親から継承されます。したがって、ディレクトリにエクスポートピンを設定すると、そのすべての子に影響します。ただし、子ディレクトリのエクスポートピンを設定して親のピンを上書きできます。次に例を示します。
   </para>
<screen><prompt>root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>フェールオーバーの管理</title>

  <para>
   MDSデーモンがMonitorとの通信を停止した場合、そのMonitorは<option>mds_beacon_grace</option>の秒数(デフォルトは15秒)待機してから、デーモンを「遅延」<emphasis/>としてマークします。MDSデーモンのフェールオーバー中に処理を引き継ぐ「スタンバイ」デーモンを1つ以上設定できます。
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>スタンバイデーモンの設定</title>
   <para>
    スタンバイ中のデーモンの動作を制御する設定は複数あります。これらの設定は、MDSデーモンが実行されるホストの<filename>ceph.conf</filename>で指定できます。デーモンは起動時にこれらの設定をロードしてMonitorに送信します。
   </para>
   <para>
    どの設定も使用されていない場合、デフォルトで、ランクを保有していないすべてのMDSデーモンがすべてのランクに対して「スタンバイ」として使用されます。
   </para>
   <para>
    スタンバイデーモンを特定の名前またはランクに関連付ける設定は、デーモンがそのランクにのみ使用されることを保証するものでありません。複数のスタンバイが利用可能な場合、関連付けられたスタンバイデーモンが使用されることを意味します。ランクが失敗した場合、利用可能なスタンバイがあれば、別のランクや特定のデーモンに関連付けられていても、そのスタンバイが使用されます。
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       trueに設定すると、スタンバイデーモンは継続的に上のランクのメタデータジャーナルを読み込みます。これによってウォームメタデータキャッシュが作成され、そのランクにサービスを提供するデーモンに障害が発生した場合のフェールオーバー処理が高速化されます。
      </para>
      <para>
       上のランクには、スタンバイ再生デーモンを1つだけ割り当てることができます。2つのデーモンが両方ともスタンバイ再生に割り当てられている場合、その一方が任意に勝利し、もう一方は通常の非再生スタンバイになります。
      </para>
      <para>
       スタンバイ再生状態になったデーモンは、そのデーモンが追跡しているランクのスタンバイとしてのみ使用されます。別のランクが失敗し、利用可能な他のスタンバイがない場合であっても、このスタンバイ再生デーモンは代わりとして使用されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       障害が発生したランクを保有する最後のデーモンがこの名前に一致した場合、スタンバイデーモンがこのランクのみを引き継ぐようにする場合に設定します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       スタンバイデーモンが指定ランクのみを引き継ぐようにする場合に設定します。別のランクに障害が発生した場合、このデーモンはランクを置き換えるために使用されません。
      </para>
      <para>
       複数のファイルシステムが存在する環境で、対象にするファイルシステムのランクを具体的に指定する場合は、<option>mds_standby_for_fscid</option>と組み合わせて使用します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       <option>mds_standby_for_rank</option>が設定されている場合、これは単に、参照されるファイルシステムのランクを示す修飾子です。
      </para>
      <para>
       <option>mds_standby_for_rank</option>が設定されていない場合、FSCIDを設定すると、このデーモンは指定されたFSCIDのすべてのランクを対象にします。特定のファイルシステム内だけですべてのランクに対して使用したいデーモンがある場合に使用します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       Monitorホストで使用します。デフォルトはtrueです。
      </para>
      <para>
       falseの場合、<option>standby_replay=true</option>が設定されたデーモンは、追跡対象として設定されているランク/名前に障害が発生した場合にのみアクティブになります。一方、この設定がtrueの場合、<option>standby_replay=true</option>が設定されたデーモンは他のランクに割り当てられることがあります。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>例</title>
   <para>
    次に、<filename>ceph.conf</filename>の設定例をいくつか示します。すべてのデーモンの設定が含まれる<filename>ceph.conf</filename>をすべてのサーバにコピーすることも、特定のサーバのデーモン設定が含まれるサーバ別のファイルを使用することもできます。
   </para>
   <sect3>
    <title>単純なペア</title>
    <para>
     2つのMDSデーモン「a」と「b」がペアとして機能しています。現在ランクが割り当てられていない方が、もう一方のスタンバイ再生のフォロワになります。
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>CephFSのクォータの設定</title>

  <para>
   Cephファイルシステムの任意のサブディレクトリにクォータを設定できます。クォータは、ディレクトリ階層の指定したポイントの下層に保存される「バイト」または「ファイル」の数を制限します。<emphasis role="bold"/><emphasis role="bold"/>
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>制限</title>
   <para>
    CephFSでのクォータの使用には、次の制限があります。
   </para>
   <variablelist>
    <varlistentry>
     <term>クォータは協調的で非競合</term>
     <listitem>
      <para>
       Cephクォータは、ファイルシステムをマウントしているクライアントに依存し、制限に達すると書き込みを停止します。サーバ側では、悪意のあるクライアントが必要なだけデータを書き込むのを防止することはできません。クライアントが完全に信頼されていない環境では、ファイルシステムがいっぱいになるのを防ぐため、クォータを使用しないでください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>クォータは正確ではない</term>
     <listitem>
      <para>
       ファイルシステムへの書き込み中のプロセスは、クォータ制限に達した直後に停止されます。そのため必然的に、設定された制限を超える量のデータを書き込むことができます。クライアントのライタは、設定された制限を超えてから1/10秒以内に停止されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>バージョン4.17からクォータはカーネルクライアントに実装される</term>
     <listitem>
      <para>
       クォータは、ユーザスペースクライアント(libcephfs、ceph-fuse)によってサポートされます。Linuxカーネルクライアント4.17以降は、SUSE Enterprise Storage 6クラスタ上のCephFSクォータをサポートします。カーネルクライアントが最新バージョンであっても、クォータ拡張属性を設定できても、古いクラスタ上のクォータは処理できません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>パスベースのマウント制限とともに使用する場合はクォータを慎重に設定する</term>
     <listitem>
      <para>
       クライアントは、クォータを適用するには、クォータが設定されているディレクトリiノードにアクセスできる必要があります。クライアントがMDSの機能に基づいて特定のパス(たとえば、<filename>/home/user</filename>)へのアクセスを制限されている場合に、そのクライアントがアクセスできない祖先ディレクトリ(<filename>/home</filename>)にクォータが設定されているときは、クライアントはクォータを適用しません。パスベースのアクセス制限を使用する場合は、クライアントがアクセスできるディレクトリ(たとえば、<filename>/home/user</filename>や/home/user/quota_dir)にクォータを設定してください。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>設定</title>
   <para>
    仮想拡張属性を使用して、CephFSクォータを設定できます。
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       「ファイル」制限を設定します。<emphasis/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       「バイト」制限を設定します。<emphasis/>
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    これらの属性がディレクトリiノード上に存在する場合、そこにクォータが設定されています。存在しない場合は、そのディレクトリにクォータは設定されていません(ただし、親ディレクトリに設定されている場合があります)。
   </para>
   <para>
    100MBのクォータを設定するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    10,000ファイルのクォータを設定するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    クォータ設定を表示するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>クォータが設定されない</title>
    <para>
     拡張属性の値が「0」の場合、クォータは設定されません。
    </para>
   </note>
   <para>
    クォータを削除するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>CephFSスナップショットの管理</title>

  <para>
   CephFSスナップショットは、スナップショットを作成した時点でのファイルシステムの読み込み専用ビューを作成します。スナップショットは任意のディレクトリに作成できます。スナップショットでは、ファイルシステムの指定ディレクトリの下層にあるすべてのデータが対象になります。スナップショットの作成後、バッファされたデータはさまざまなクライアントから非同期にフラッシュされます。その結果、スナップショットの作成は非常に高速です。
  </para>

  <important>
   <title>複数のファイルシステム</title>
   <para>
    複数のCephFSファイルシステムが(ネームスペースを介して) 1つのプールを共有している場合、それらのスナップショットは競合し、1つのスナップショットを削除すると同じプールを共有している他のスナップショットのファイルデータがなくなります。
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>スナップショットの作成</title>
   <para>
    CephFSスナップショット機能は、新しいファイルシステムではデフォルトで有効になっています。既存のファイルシステムでこの機能を有効にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    スナップショットを有効にすると、CephFSのすべてのディレクトリに特別な<filename>.snap</filename>サブディレクトリが作成されます。
   </para>
   <para>
    CephFSカーネルクライアントには、1つのディレクトリツリー内で400を超えるスナップショットを処理できないという制限があります。スナップショットの数は、使用しているクライアントに関係なく、常にこの制限を下回るようにする必要があります。SLE12-SP3などの古いCephFSクライアントを使用する場合は、スナップショットが400を超えるとクライアントがクラッシュするため、操作に害を及ぼすことに注意してください。
   </para>
   <tip>
    <title>カスタムスナップショットサブディレクトリ名</title>
    <para>
     <option>client snapdir</option>設定により、スナップショットサブディレクトリに異なる名前を設定できます。
    </para>
   </tip>
   <para>
    スナップショットを作成するには、<filename>.snap</filename>ディレクトリに、カスタム名を持つサブディレクトリを作成します。たとえば、<filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>ディレクトリのスナップショットを作成するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>スナップショットの削除</title>
   <para>
    スナップショットを削除するには、<filename>.snap</filename>ディレクトリ内にあるそのサブディレクトリを削除します。
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
