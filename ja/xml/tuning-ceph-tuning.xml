<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="tuning-ceph-tuning.xml" version="5.0" xml:id="tuning-ceph">
 <title>Cephのチューニング</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   Cephには、匿名化された情報をCeph開発者コミュニティに提供するテレメトリモジュールが含まれています。テレメトリレポートに含まれている情報は、開発者が作業の優先順位を付け、さらに作業が必要になる可能性のある領域を特定するのに役立つ情報を提供します。テレメトリモジュールをオンにする前に有効にする必要がある場合があります。モジュールを有効にするには、次のコマンドを実行します。
 </para>
<screen>
ceph mgr module enable telemetry
</screen>
 <para>
   テレメトリレポートをオンにするには、次のコマンドを使用します。
 </para>
<screen>
ceph telemetry on
</screen>
 <para>
   Cephテレメトリモジュールに関するその他の情報については、<xref linkend="book-storage-admin"/>を参照してください。
 </para>
 <sect1 xml:id="tuning-obtaining-metrics">
   <title>Cephメトリックの取得</title>
   <para>
     Ceph調整可能パラメータを調整する前に、監視する重要なメトリックとそれらが示す内容を理解しておくと役立ちます。これらのパラメータの多くは、デーモンから生データをダンプすることで検出されます。これは、<command>ceph daemon dump</command>コマンドで実行されます。次の例は、<literal>osd.104</literal>に対して利用されているダンプコマンドを示しています。
   </para>
<screen>
ceph --admin-daemon /var/run/ceph/ceph-osd.104.asok perf dump
</screen>
   <para>
     Ceph Nautilusリリース以降、次のコマンドも使用できます。
   </para>
<screen>
ceph daemon osd.104 perf dump
</screen>
   <para>
     コマンドの出力は非常に長く、ファイルにリダイレクトされることでメリットが得られる場合があります。
   </para>
 </sect1>
 <sect1 xml:id="tuning-tuning-persistent">
   <title>チューニングの永続化</title>
   <para>
     パラメータ調整を永続化するには、<filename>/etc/ceph/ceph.conf</filename>ファイルを変更する必要があります。これは、DeepSeaがクラスタを管理するために使用するソースコンポーネントファイルを変更することによって実行するのが最適です。各セクションは、次のようなヘッダーで識別されます。
   </para>
<screen>
  [global]
  [osd]
  [mds]
  [mon]
  [mgr]
  [client]
</screen>
   <para>
     設定のセクションは、<filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>ディレクトリの正しい<filename>[sectionname].conf</filename>を変更することによって調整されます。設定ファイルを変更した後で、 <literal>master</literal>をマスタminion-name (通常は管理ノード)に置き換えます。その結果、変更がすべてのクラスタノードにプッシュされます。
   </para>
<screen>
  salt 'master' state.apply ceph.configuration.create
  salt '*' state.apply ceph.configuration
</screen>
   <para>
     この方法で行われる変更では、影響を受けるサービスを再起動してから有効にする必要があります。また、SUSE Enterprise Storage展開プロセスのステージ2を実行する前に、これらのファイルを展開することもできます。ノードやデバイスの再展開を必要とする設定を変更する場合は、これを実行することが特に望ましいです。
   </para>
 </sect1>
 <sect1 xml:id="tuning-core">
   <title>コア</title>
   <sect2>
     <title>ログ記録</title>
     <para>
       さまざまなコードパスのレイテンシを減らすために、すべてのログ記録を無効にすることができます。
     </para>
     <warning>
       <para>
         このチューニングは、サポートが必要な場合にログ記録を再度有効にする「必要がある」ことを理解し、注意して使用する必要があります。<emphasis/>これは、ログ記録を再度有効化した「後で」、インシデントを再現する必要があることを意味します。<emphasis/>
       </para>
     </warning>
<screen>
  debug ms=0
  debug mds=0
  debug osd=0
  debug optracker=0
  debug auth=0
  debug asok=0
  debug bluestore=0
  debug bluefs=0
  debug bdev=0
  debug kstore=0
  debug rocksdb=0
  debug eventtrace=0
  debug default=0
  debug rados=0
  debug client=0
  debug perfcounter=0
  debug finisher=0
</screen>
   </sect2>
   <sect2>
     <title>認証パラメータ</title>
     <para>
       クラスタが物理的に安全で、外部に公開されていないセキュアネットワーク内で分離されている特定の条件下では<systemitem class="service">cephx</systemitem>を無効にすることができます。<systemitem class="service">cephx</systemitem>を無効にすることが可能な2つのレベルがあります。第1レベルは、認証トラフィックの署名を無効にすることです。これは次の設定で実行できます。
     </para>
<screen>
cephx_require_signatures = <replaceable>false</replaceable>
cephx_cluster_require_signatures = <replaceable>false</replaceable>
cephx_sign_messages = <replaceable>false</replaceable>
</screen>
     <para>
       第2レベルのチューニングでは、<systemitem class="service">cephx</systemitem>認証を完全に無効にします。これは、パブリックネットワークインフラストラクチャから分離されたネットワーク上でのみ実行する必要があります。この変更は、グローバルセクションに次の3行を追加することにより実現されます。
     </para>
<screen>
auth cluster required = none
auth service required = none
auth client required = none
</screen>
   </sect2>
   <sect2>
     <title>RADOS操作</title>
     <para>
       RADOS操作を実行するためのバックエンドプロセスは、さまざまなデーモンをダンプするときに、<literal>throttle-*objector_ops</literal>に表示されます。<literal>待機</literal>に費やされる時間が多すぎる場合は、処理中の操作のメモリを増やすか、処理中の操作の合計数を全体的に増やすことによって、パフォーマンスを向上させることができます。
     </para>
<screen>
objecter inflight op bytes = 1073741824 # default 100_M
objecter inflight ops = 24576
</screen>
   </sect2>
   <sect2>
     <title>OSDパラメータ</title>
     <para>
       <literal>opスレッド</literal>の数を増やすと、操作用のワークキューが増えるため、SSDおよびNVMeデバイスで役立つ場合があります。
     </para>
<screen>
osd_op_num_threads_per_shard = 4
</screen>
   </sect2>
   <sect2>
     <title>RocksDBまたはWALデバイス</title>
     <para>
       BlueStoreのパフォーマンスをチェックする際には、メタデータが高速デバイス(定義されている場合)から大容量データストレージデバイスに流出しているかどうかを理解することが重要です。この場合に役立つパラメータは、bluefs <literal>slow_used_bytes</literal>の下にあります。<literal>slow_used_bytes</literal>がゼロより大きい場合、クラスタはRocksDB/WALデバイスではなく、ストレージデバイスを使用しています。これは、より多くのスペースをRocksDB/WALに割り当てる必要があることを示しています。
     </para>
     <para>
       Ceph Nautilusリリース以降、スピルオーバが<command>ceph health</command>コマンドの出力に表示されます。
     </para>
     <para>
       より多くのスペースを割り当てるプロセスは、OSDが導入された方法によって異なります。SUSE Enterprise Storage 6より前のバージョンで構築された場合、OSDを再導入する必要があります。バージョン6以降で構築された場合、使用可能なスペースに応じて、RocksDBとWALが常駐するLVMを拡張できる場合があります。
     </para>
   </sect2>
   <sect2 xml:id="bluestore-parameters">
     <title>BlueStoreパラメータ</title>
     <para>
       Cephは、Write-Ahead Log (WAL)ファイルを含め、シンプロビジョニングされています。WAL用のファイルを事前に拡張することにより、アロケータを使用する必要がなくなり、時間が節約されます。また、WALファイルのフラグメンテーションの可能性を低減できる場合があります。これは、クラスタの初期の使用期間中にのみメリットが得られる可能性があります。
     </para>
<screen>
bluefs_preextend_wal_files=1
</screen>
     <para>
       BlueStoreにはバッファリングされた書き込みを実行する機能があります。バッファリングされた書き込みにより、書き込みプロセス中に読み取りキャッシュにデータを読み込むことができます。この設定は、実質的に、BlueStoreキャッシュをライトスルーキャッシュに変更します。
     </para>
<screen>
bluestore_default_buffered_write = true
</screen>
     <para>
       SSDやNVMeなど、高速デバイスの使用時にWALへの書き込みを回避するためには、次のように設定します。
     </para>
<screen>
prefer_deferred_size_ssd=0 (pre-deployment)
</screen>
   </sect2>
   <sect2 xml:id="bluestore-alloc-size">
     <title>BlueStoreの割り当てサイズ</title>
     <important>
       <para>
         次の設定は、新規の構築には必要ありません。メリットが得られる可能性があるアップグレードまたは初期のSUSE Enterprise Storage 6の導入にのみ適用してください。
       </para>
     </important>
     <para>
       以下の設定では、混合ワークロード条件下での小さなオブジェクトの書き込みパフォーマンスをわずかに改善させることが示されています。<literal>alloc_size</literal>を4kBに減らすと、小さいオブジェクトの書き込み増幅を減らし、より小さいオブジェクトのイレージャコード化済みプールを使用できます。この変更は、OSDの構築前に実行する必要があります。事後に実行される場合、これを有効にするためにOSDを再構築する必要があります。
     </para>
     <para>
       SSD/NVMeは4kBに設定することでメリットが得られる可能性があるのに対して、HDDでは、引き続き64kBを使用することが推奨されます。
     </para>
<screen>
min_alloc_size_ssd=4096
min_alloc_size_hdd=65536
</screen>
      <warning>
        <para>
          <literal>alloc_size_ssd</literal>を64kBに設定すると、OSDの最大スループット機能が低下する可能性があります。
        </para>
      </warning>
   </sect2>
   <sect2 xml:id="bluestore-cache">
     <title>BlueStoreキャッシュ</title>
     <para>
       BlueStoreキャッシュサイズを増やすと、多くのワークロードでパフォーマンスが改善できます。FileStore OSDはカーネルのページキャッシュにデータをキャッシュしますが、BlueStore OSDは、OSDデーモン自体によって割り当てられたメモリ内にデータをキャッシュします。OSDデーモンは、(<literal>osd_memory_target</literal>パラメータによって制御される)メモリターゲットまでメモリを割り当てます。これにより、BlueStoreキャッシュの潜在的なサイズが決まります。BlueStoreキャッシュはオブジェクトが読み取られるときにデフォルトで読み込まれる読み取りキャッシュです。キャッシュの最小サイズをデフォルトより大きく設定することにより、指定された値がOSDごとに利用可能な最小キャッシュとなることが保証されます。より低い確率のキャッシュヒットが発生する可能性があるという考えです。
     </para>
     <para>
       デフォルトの<literal>osd_memory_target</literal>値は4GBです。たとえば、特定のノードで実行されている各OSDデーモンはその量のメモリを消費することが予想されます。ノードの合計RAMが<literal>OSD数× 4GB</literal>よりも大幅に大きい場合、およびノード上で実行されている他のデーモンがない場合、<literal>osd_memory_target</literal>の値を増やすことで、パフォーマンスを向上させることができます。これは、安全マージンを残しながら、オペレーティングシステムがそのニーズに対して十分なメモリを確保できるように注意して行う必要があります。
     </para>
     <para>
       BlueStoreキャッシュが特定の最小値を下回らないようにする場合は、<literal>osd_memory_cache_min</literal>パラメータを使用します。次に例を示します(値はバイト単位で表されます)。
     </para>
<screen>
osd_memory_target = 6442450944
osd_memory_cache_min = 4294967296
</screen>
     <tip>
       <para>
         ベストプラクティスとして、ノードのフルメモリから開始します。OSの16GBまたは32GBを差し引き、ノードで実行されている他のワークロードの適切な量を差し引きます。たとえば、MDSが共存している場合はMDSキャッシュ。そのホスト上のOSD数で残りを割ります。改善の余地があることを確認します。例:
       </para>
<screen>
(256 GB - 32 GB ) / 20 OSDs = 11,2 GB/OSD (max)
</screen>
       <para>
         この例を使用して、OSDごとに約8または10GBを設定します。
       </para>
     </tip>
     <para>
       デフォルトで、BlueStoreはデータとキー値データ間のキャッシュ比率を自動的に調整します。場合によっては、手動で比率を調整したり、キャッシュサイズを増やしたりすることも役立つことがあります。キャッシュには次のようないくつかの関連するカウンタがあります。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_onode_hits</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_misses</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_shard_hits</literal>
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_onode_shard_misses</literal>
         </para>
       </listitem>
     </itemizedlist>
     <para>
       ミスが多い場合は、キャッシュ設定を増やしたり、比率を調整したりしてみる価値があります。
     </para>
     <para>
       BlueStoreキャッシュサイズをデフォルトを上回るように調整すると、小さなブロックのワークロードのパフォーマンスが改善する可能性があります。これは、<option>_cache_size</option>値を調整することでグローバルに実行できます。デフォルトで、クラスタはHDDおよびSSD/NVMeデバイスに対して異なる値を使用します。ベストプラクティスは、調整する特定のメディアキャッシュを増やすことです。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_cache_size_hdd</literal> (デフォルト1073741824 - 1GB)
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_cache_size_ssd</literal> (デフォルト3221225472 - 3GB)
         </para>
       </listitem>
     </itemizedlist>
     <note>
       <para>
         キャッシュサイズパラメータが調整され、自動モードが使用される場合、<option>osd_memory_target</option>を調整して、OSDベースRAMおよびキャッシュ割り当てに対応する必要があります。
       </para>
     </note>
     <para>
       場合によっては、キャッシュ割り当ての割合を手動で調整すると、パフォーマンスが向上します。これは、次の設定行でキャッシュの自動調整の無効化を変更することで実現します。
     </para>
<screen>
bluestore_cache_autotune=0
</screen>
     <para>
       この値を変更すると、<option>osd_memory_cache_min</option>値の調整が無効になります。
     </para>
     <para>
       キャッシュ割り当ては以下を調整することで変更されます。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           <literal>bluestore_cache_kv_ratio</literal> (デフォルト.4)
         </para>
       </listitem>
       <listitem>
         <para>
           <literal>bluestore_cache_meta_ratio values</literal> (デフォルト.4)
         </para>
       </listitem>
     </itemizedlist>
     <para>
       指定されていない部分は、オブジェクト自体のキャッシュに使用されます。
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-rbd">
   <title>RBD</title>
   <sect2>
     <title>RBDクラスタ</title>
     <para>
       RBDはネイティブなプロトコルであるため、チューニングは以前のセクションで説明したOSDまたは一般的なCephコアオプションに直接関連しています。
     </para>
   </sect2>
   <sect2>
     <title>RBDクライアント</title>
     <para>
       先読みキャッシュのデフォルトは512kBです。クライアントノードで上下に調整してテストします。
     </para>
<screen>
echo {bytes} &gt; /sys/block/rbd0/queue/read_ahead_kb
</screen>
     <para>
       ワークロードでバックアップや復元などの大規模な順次読み取りを実行する場合、これにより復元パフォーマンスに大きな違いが生じる可能性があります。
     </para>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-cephfs">
   <title>CephFS</title>
   <para>
     このセクションで説明するパフォーマンスチューニングのほとんどは、CephFS Metadata Serverに関連しています。CephFSはネイティブプロトコルであるため、パフォーマンスチューニングの多くはオペレーティングシステム、OSD、およびBlueStore階層で処理されます。クライアントによってマウントされるファイルシステムであるため、クライアントセクションで説明されるいくつかのクライアントオプションがあります。
   </para>
   <sect2>
     <title>MDSチューニング</title>
     <para>
       数百万のファイルがあるファイルシステムでは、CephFSメタデータプールにNVMeなどの非常に低レイテンシのメディアを使用することにはいくつかの利点があります。
     </para>
     <para>
       <command>ceph-daemon perf dump</command>コマンドを使用すると、Ceph Metadata Serverに対して大量のデータを調査できます。MDSパフォーマンスカウンタはメタデータ操作にのみ適用されることに注意してください。実際のIOパスはクライアントからOSDへの直接パスです。
     </para>
     <para>
       CephFSでは、複数のメタデータサーバをサポートしています。これらのサーバはマルチアクティブモードで動作し、メタデータ操作要求の負荷分散を提供できます。MDSインフラストラクチャのパフォーマンスが低下しているかどうかを確認するには、MDSデータの要求数と応答レイテンシを調べます。これは、クラスタのアイドル期間中に実行してベースラインを形成し、負荷がかかっているときに比較する必要があります。応答レイテンシの平均時間が長くなりすぎた場合は、MDSサーバをさらに調べて、アクティブなメタデータサーバ数を増やす必要があるかどうか、または単にメタデータサーバキャッシュを増やすだけで十分かどうかを確認する必要があります。数および応答レイテンシの一般的なMDSデータからの出力サンプルは次のとおりです。
     </para>
<screen>
  "mds": {
    # request count, interesting to get a sense of MDS load
           "request": 0,
           "reply": 0,
    # reply and the latencies of replies can point to load issues
           "reply_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           }
          }
</screen>
     <para>
       出力の<literal>mds_mem</literal>セクションを調べると、キャッシュの使用方法を理解するのに役立ちます。高いiノードカウンタは、多数のファイルが同時に開いていることを示している可能性があります。これは、一般的にメモリをMDSに追加する必要があることを示しています。MDSメモリを増やすことができない場合は、追加のアクティブなMDSデーモンを導入する必要があります。
     </para>
<screen>
  "mds_mem": {

           "ino": 13,
           "ino+": 13,
           "ino-": 0,
           "dir": 12,
           "dir+": 12,
           "dir-": 0,
           "dn": 10,
           "dn+": 10,
           "dn-": 0,
</screen>
     <para>
       高い<literal>cap</literal>数はクライアント動作が誤っていることを示している可能性があります。たとえば、キャップを返さないクライアント。これは、一部のクライアントをより新しいバージョンにアップグレードする必要があるか、またはクライアントが潜在的な問題について調査する必要があることを示している可能性があります。
     </para>
<screen>
  "cap": 0,
  "cap+": 0,
  "cap-": 0,
</screen>
     <para>
       この最後のセクションでは、メモリ利用率を示しています。RSS値は現在使用されているメモリサイズです。これがほぼ<literal>mds_cache_memory_limit</literal>に等しい場合、MDSはおそらくさらに多くのメモリを使用することができます。
     </para>
<screen>
  "rss": 41524,
  "heap": 314072
},
</screen>
     <para>
       分散ファイルシステムのチューニングの別の重要な側面は、問題のあるワークロードを認識することです。次の出力値はMDSデーモンが時間を費やしているものに対する洞察を提供します。各見出しには<literal>req_create_latency</literal>と同じ3つの属性があります。この情報を使用して、ワークロードをより適切に調整できる場合があります。
     </para>
<screen>
  "mds_server": {
           "dispatch_client_request": 0,
           "dispatch_server_request": 0,
           "handle_client_request": 0,
           "handle_client_session": 0,
           "handle_slave_request": 0,
           "req_create_latency": {
               "avgcount": 0,
               "sum": 0.000000000,
               "avgtime": 0.000000000
           },
           "req_getattr_latency": {},
           "req_getfilelock_latency": {},
           "req_link_latency": {},
           "req_lookup_latency": {},
           "req_lookuphash_latency": {},
           "req_lookupino_latency": {},
           "req_lookupname_latency": {},
           "req_lookupparent_latency": {},
           "req_lookupsnap_latency": {},
           "req_lssnap_latency": {},
           "req_mkdir_latency": {},
           "req_mknod_latency": {},
           "req_mksnap_latency": {},
           "req_open_latency": {},
           "req_readdir_latency": {},
           "req_rename_latency": {},
           "req_renamesnap_latency": {},
           "req_rmdir_latency": {},
           "req_rmsnap_latency": {},
           "req_rmxattr_latency": {},
           "req_setattr_latency": {},
           "req_setdirlayout_latency": {},
           "req_setfilelock_latency": {},
           "req_setlayout_latency": {},
           "req_setxattr_latency": {},
           "req_symlink_latency": {},
           "req_unlink_latency": {},
       }
</screen>
     <para>
       メタデータサーバキャッシュを調整すると、RAMからより多くのメタデータ操作が実行できるようになるため、パフォーマンスが向上します。次の例では、キャッシュを16GBに設定します。
     </para>
<screen>
mds_cache_memory_limit=17179869184
</screen>
   </sect2>
   <sect2>
     <title>CephFS - クライアント</title>
     <para>
       クライアント側からは、使用可能なマウントオプションに影響を及ぼす多くのパフォーマンスがあります。これらのオプションを使用する前に、利用されているアプリケーションに対する潜在的な影響を理解することが重要です。</para>
      <para>
       次のマウントオプションは、パフォーマンスを向上させるために調整できますが、運用環境で実装する前にその影響を明確に理解しておくことをお勧めします。
     </para>
     <variablelist>
       <varlistentry>
         <term>noacl</term>
         <listitem>
           <para>
             このマウントオプションを設定すると、CephFSマウントのPOSIXアクセス制御リストが無効になり、潜在的なメタデータオーバーヘッドが低くなります。
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>noatime</term>
         <listitem>
           <para>
             このオプションを使用すると、ファイルのアクセス時間メタデータが更新されなくなります。
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>nodiratime</term>
         <listitem>
           <para>
             このオプションを設定すると、ディレクトリのアクセス時間のメタデータが更新されなくなります。
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>nocrc</term>
         <listitem>
           <para>
             これによりCephFS CRCが無効になり、検証されるデータの正確性をTCPチェックサムに依存します。
           </para>
         </listitem>
       </varlistentry>
       <varlistentry>
         <term>rasize</term>
         <listitem>
           <para>
             マウントに対してより大きな先読みを設定すると、大規模な順次操作のパフォーマンスが向上する場合があります。デフォルトは8MiBです。
           </para>
         </listitem>
       </varlistentry>
     </variablelist>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-rgw">
   <title>RGW</title>
   <para>
     Rados GateWay (RGW)には多数の調整可能パラメータがあります。これらは、ゲートウェイによって処理されるワークロードのタイプに固有のものであり、異なるゲートウェイが異なるワークロードを個別に処理することが意味を持つ場合があります。
   </para>
   <sect2>
     <title>シャーディング</title>
     <para>
       理想的な状況は、バケットがホストするオブジェクトの合計数を把握することです。これにより、最初に適切な数のシャードでバケットを作成できるようになります。バケットのシャーディングに関する情報を収集するには、次のコマンドを発行します。
     </para>
<screen>
radosgw-admin bucket limit check
</screen>
     <para>
       このコマンドの出力は、次のような形式になります。
     </para>
<screen>
  "user_id": "myusername",
          "buckets": [
              {
                  "bucket": "mybucketname",
                  "tenant": "",
                  "num_objects": 611493,
                  "num_shards": 50,
                  "objects_per_shard": 12229,
                  "fill_status": "OK"
              }
          ]
</screen>
     <para>
       デフォルトでは、Cephはバケットを再度シャーディングして妥当なパフォーマンスを試して維持します。100,000オブジェクトあたり1シャードの比率に基づいて、バケットに必要なシャードの数が事前にわかっている場合は、事前にシャーディングされることがあります。これにより、再シャーディングが発生する場合の競合と潜在的なレイテンシの問題が軽減されます。バケットを事前にシャーディングするには、バケットを作成してから、<command>rgw-admin</command>コマンドを使用してシャーディング用に送信する必要があります。例:
     </para>
<screen>
radosgw-admin bucket reshard --bucket={bucket name} --num-shards={prime number}
</screen>
     <para>
       ここで、<literal>num-shards</literal>が素数です。各シャードは約100,000オブジェクトを表します。
     </para>
   </sect2>
   <sect2>
     <title>バケットリスト結果の制限</title>
     <para>
       プロセスが頻繁にバケットを一覧表示して結果を反復処理することに依存しているが、反復ごとに少数の結果しか使用しない場合は、<literal>rgw_max_listing_results</literal>パラメータを設定すると便利です。
     </para>
   </sect2>
   <sect2>
     <title>パラレルI/O要求</title>
     <para>
       デフォルトで、Object Gatewayプロセスはインデックスの8つの同時I/O操作に制限されています。これは、<literal>rgw_bucket_index_max_aio</literal>パラメータで調整できます。
     </para>
     </sect2>
     <sect2>
       <title>ウィンドウサイズ</title>
     <para>
       大きなオブジェクトで作業する場合、<literal>put</literal>および<literal>get</literal>のObject Gatewayウィンドウのサイズを大きくすると、パフォーマンスが向上します。設定のObject Gatewayセクションの次の値を変更します。
     </para>
<screen>
rgw put obj min window size = [size in bytes, 16MiB default]
rgw get obj min window size = [size in bytes, 16MiB default]
</screen>
     </sect2>
     <sect2>
       <title>Nagleのアルゴリズム</title>
     <para>
       Nagleのアルゴリズムは、バッファの使用を最大化し、ネットワークを介して送信される小さなパケット数を削減するために導入されました。これは低帯域幅環境では役立ちますが、高帯域幅環境ではパフォーマンスが低下することを表します。RGWノードからこれを無効にすると、パフォーマンスが向上する可能性があります。Ceph設定RGWセクションに以下を含めます。
     </para>
<screen>
tcp_nodelay=1
</screen>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-admin-usage">
   <title>管理および使用方法の選択</title>
   <sect2>
     <title>データ保護スキーム</title>
     <para>
       デフォルトのレプリケーション設定では、書き込まれたすべてのオブジェクトの合計3つのコピーが保持されます。データを保護しながら最大2つのデバイスまたはノードに障害が発生することを許可することで、高レベルのデータ保護を提供します。</para>
      <para>
       データの保護は重要ではないが、パフォーマンスが重要な利用環境があります。HPCスクラッチストレージなどの場合は、レプリケーション数を削減することが重要な場合があります。これは、次のようなコマンドを発行して実現できます。
     </para>
<screen>
ceph osd pool set rbd size 2
</screen>
   </sect2>
   <sect2>
     <title>イレージャコーディング</title>
     <para>
       イレージャコーディングを使用する場合、最適化されたコーディングプールサイズを利用するのが最適です。実験データは、最適なプールサイズには4つまたは8つのデータチャンクがあることを示しています。障害ドメインモデルとの関連でこれをマッピングすることも重要です。クラスタ障害ドメインがノードレベルである場合、少なくとも<literal>k+m</literal>のノード数が必要です。同様に、障害ドメインがラックレベルである場合、クラスタは<literal>k+m</literal>ラックに分散させる必要があります。重要な考慮事項は、障害ドメインに関連するデータの分散を考慮することです。
     </para>
     <para>
       単一ノードより大きい障害ドメインでイレージャコーディングスキームを使用する場合、ローカル再構築コード(LRC)を使用すると、ネットワークバックボーンの利用率が低下するため、特に障害と回復のシナリオで役立つ場合があります。
     </para>
     <para>
       イレージャコーディングによってパフォーマンスが向上する可能性がある特定のユースケースがあります。これらは主に、大きなブロック(1MB+)の順次読み取り/書き込みワークロードに制限されています。これは、オブジェクトをチャンクに分割して複数のOSDに書き込むときに発生するI/O要求の並列化によるものです。
     </para>
   </sect2>
 </sect1>
</chapter>
