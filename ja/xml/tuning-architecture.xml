<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="tuning-architecture.xml" version="5.0" xml:id="tuning-architecture">
 <title>アーキテクチャとハードウェアのチューニング</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/master/xml/</dm:editurl>
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
   アーキテクチャチューニングには、展開されるシステムの低レベル設計から、ネットワークトポロジと冷却に関するマクロレベルの決定まで、さまざまな側面が含まれます。これらのすべてをこの作業で扱うことができるわけではありませんが、可能な場合はガイダンスを提供します。
 </para>
 <para>
   トップダウンビューから、ノードの物理的な場所、ノード間で利用可能な接続、および電源ルーティングや防火区画などの項目の影響について考えることが重要です。ただし、パフォーマンスの観点からは、接続性、および書き込みがクラスタの観点から実際にどのように見えるかについて考えることが最も重要です。
 </para>
 <para>
   アーキテクチャチューニングの目的は、パフォーマンスボトルネックがどこにあるかを制御することです。ほとんどの場合、ボトルネックがストレージデバイスにあることが望ましいです。これは、パフォーマンスの低下と関連する制限の予測可能なパターンが作成されるためです。CPUやネットワークなど他の領域にボトルネックを配置すると、リソースに負荷がかかったときに、一貫性のない動作が発生する可能性があります。これは、リカバリ、リバランシング、ガーベッジコレクションなど、他のプロセスが実行されている可能性があり、またネットワークまたはCPUにバインドされていることにより一貫性のない動作が発生します。ストレージがボトルネックである場合、パフォーマンスの低下により、キューに入れられた要求への応答時間が長くなるため、ボトルネックを配置する最も望ましいポイントになります。
 </para>
 <sect1 xml:id="tuning-network">
   <title>ネットワーク</title>
   <para>
     ネットワークはクラスタのバックボーンであり、堅牢な方法でネットワークを実装しないと、他の手順を実行しても、クラスタのパフォーマンスが低下する可能性があります。ベストプラクティスの観点から、これには、クラスタの拡張に合わせて総コア帯域幅を拡張できるフォールトトレラントスイッチングインフラストラクチャの実装が伴います。非常に大規模なクラスタの場合、これはリーフとスパインのアーキテクチャを伴う場合がありますが、小規模なクラスタの場合は、より一般的なハブアンドスポークまたはメッシュネットワークのように見える場合があります。
   </para>
   <para>
     どのネットワークアーキテクチャを選択しても、パスに沿った各ホップを注意深く調べ、悪条件の間に発生する可能性のある最大のネットワークトラフィックを考慮することが重要です。
   </para>
   <para>
     ここでの不適切な判断の例としては、各ラックには24x 7200rpmドライブの16個のストレージノードがあり、各ノードが2x25Gb接続を介してスタックされたトップオブラック(TOR)スイッチのペアに接続されているマルチラッククラスタを使用することです。この接続は、接続あたり3GB/秒、またはノードの合計6GB/秒で十分であり、これは7200rpmドライブが維持できる最大値に近い値です。不適切な判断であるのは、アップリンクに25GBのインターフェイスのうち、4つを使用していること「だけ」です。<emphasis/>100Gbはかなりの帯域幅のように見えますが、約10GB/秒にしか変換されず、一方でラックは約48GB/秒の集約が可能です。
   </para>
   <para>
     不利な状況では、ネットワークの輻輳により、レイテンシ、パケット遅延、ドロップなどが劇的に増加する可能性があります。簡単な改善策は、4×100Gbのアップリンクポートを備えたスイッチを選択することでした。その結果、各スイッチは完全な総帯域幅負荷をネットワークコアに送信することができます。
   </para>
   <para>
     実際には、ほとんどのネットワークである程度のオーバーサブスクリプションが発生することになりますが、比率が小さいほど、結果として生じるクラスタが悪条件に、より適切に対処できるようになります。
   </para>
   <sect2>
     <title>ネットワークチューニング</title>
     <para>
       Cephクラスタネットワークを計画する際に従うべきいくつかの一般的な原則を次に示します。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           25/50/100GbE接続を利用 - 信号レートは10/40GbEの2.5倍高速で、その結果、回線のレイテンシが低くなります。高速の信号レートによる影響は、HDDでは最小限に抑えられ、NVMeなどの高速のテクノロジではより影響が大きくなります。
         </para>
       </listitem>
       <listitem>
         <para>
           ネットワーク帯域幅は、少なくともストレージノードに存在するすべてのストレージデバイスの合計帯域幅である必要があります
         </para>
       </listitem>
       <listitem>
         <para>
           LACPボンディングイーサネットでVLANを使用することにより、帯域幅集約+フォールトトレランスの最適なバランスを実現します
         </para>
       </listitem>
       <listitem>
         <para>
           ストレージに接続するすべてのノードがジャンボフレームイーサネットを使用できる場合、ネットワークはジャンボフレームイーサネットを使用する必要があります。使用できない場合は、標準のMTUを使用します。
         </para>
       </listitem>
     </itemizedlist>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-node-hw-recommendations">
   <title>ノードのハードウェア推奨事項</title>
   <sect2>
     <title>CPU</title>
     <para>
       今日のマーケットでは、プロセッサの選択に関して多くの選択肢があります。x86スペースでは主要ベンダから利用可能な複数の種類があり、64ビットArmスペースでもさまざまな選択肢があります。コアアーキテクチャに関係なく、常に当てはまる1つのルールがあります。つまり、クロック速度が高いほど、同じ時間内により多くの作業を行えます。この考慮事項は、より高速のストレージおよびネットワークデバイスで作業する場合に最も重要です。
     </para>
     <para>
       CPUの選択も、特定のサービスにとって重要な考慮事項です。メタデータサーバ、NFS、Samba、 ISCSIゲートウェイなどの一部のサービスは、少数の非常に高速なコアからメリットを得られますが、OSDノードはよりコア密度の高いソリューションを必要とします。
     </para>
     <para>
       2番目の考慮事項は、単一のソケットを使用するか、複数のソケットを使用するかです。これに対する答えは、デバイスの密度、使用されているネットワークハードウェアのタイプなどによって異なります。多くのノードでは、プロセッサの相互リンクがボトルネックになるため、単一のソケットの方がパフォーマンスが向上しますが、これはすべてのNVMeベースのノードタイプで発生する可能性が高いです。一般的な推奨事項は、可能な限り、単一ソケットを使用することです。
     </para>
     <para>
       どのプロセッサを選択するかを検討する際には、クロック速度以外に、気をつけるべきいくつかの考慮事項があり、次の要素が考えられます。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           「メモリ帯域幅」: CephはRAMのヘビーユーザであるため、使用可能なメモリ帯域幅が大きいほど、ノードのパフォーマンスが向上します。<emphasis/>
         </para>
       </listitem>
       <listitem>
         <para>
           「メモリレイアウト」: 選択されたメモリが高速であっても、すべてのメモリチャネルが活用されていない場合、パフォーマンスはそのままです。<emphasis/>RAMがすべてのチャネルに均等に分散されるようにすると便利です。
         </para>
       </listitem>
       <listitem>
         <para>
           「オフロード機能」: たとえば、Intel CPUは<literal>zlib</literal>とReed-Solomonオフロードを提供します。後者は、ISAプラグインが指定されている場合にイレージャコーディングで使用されます。<emphasis/>
         </para>
       </listitem>
       <listitem>
         <para>
           「PCIeバスの速度とレーン」: これは、NVMeなど、多数のPCIeデバイスを搭載したデバイスを調べる場合に特に重要です。<emphasis/>バス速度もネットワークデバイスのパフォーマンスに影響します。
         </para>
       </listitem>
     </itemizedlist>
   </sect2>
   <sect2>
     <title>ストレージデバイス</title>
     <para>
       ストレージデバイスの選択は、Cephクラスタのパフォーマンスと信頼性に大きく影響する可能性があります。パフォーマンスのために構築する場合、読み取り/書き込みと適用されるワークロードに関してデバイスの性質を理解することが重要です。これは特にフラッシュメディアに当てはまります。
     </para>
     <sect3>
       <title>デバイスタイプ</title>
       <para>
         最初の推奨事項は、システムがエンタープライズクラスのストレージメディアを利用するようにすることです。NVMeおよびSSDデバイスでは、これはいくつかの重要なアイテムを意味します。
       </para>
       <itemizedlist>
         <listitem>
           <para>
             メディアの消耗に対処するための追加のスペアセル
           </para>
         </listitem>
         <listitem>
           <para>
             予期しない電源イベント時にバッファダンプを完了できるようにするバッテリ/コンデンサ
           </para>
         </listitem>
       </itemizedlist>
       <para>
         また、利用するメディアがクラスタのワークロードをサポートするようにすることも重要です。たとえば、クラスタを使用するアプリケーションの読み取り/書き込みの組み合わせが90:10である場合、読み取り集中型NVMeデバイスを使用することはおそらく許容されます。ただし、この比率が反転している場合、または50:50の場合は、少なくとも混合使用を検討するか、書き込み集中型メディアを検討することをお勧めします。この選択には、メディアの耐久性だけでなく、設計に関する考慮事項も含まれます。書き込み集中型メディアは通常、メディアへの書き込み要求の処理により多くのPCIeレーンを割り当てるため、読み取り集中型デバイスが負荷のかかった状態で提供するよりも高速なコミットが保証されます。また、書き込み集中型のデバイスは、ほとんどの場合、不揮発性メモリテクノロジのより高速なクラスを使用するか、大容量のスーパーキャップバックキャッシュを備えています。
       </para>
     </sect3>
   </sect2>
   <sect2>
     <title>デバイスバス</title>
     <para>
       途中でストレージバスとハードウェアの構成要素の影響を理解することも重要です。明らかに、6Gb/秒は12Gb/秒より低速であり、12Gb/秒はPCIe Gen3 (レーンあたり8Gb/秒)よりも低速ですが、SATA 3Gb/秒とSATA 6Gb/秒の混在、または6Gb/秒と12Gb/秒のSASの混在についてはどうでしょうか?
     </para>
     <para>
       一般的なルールは、混在させないことです。6Gb/秒のデバイスが12Gb/秒のバスに導入されると、バス全体が6Gb/秒まで減速し、全体的なスループット機能が大幅に低下します。これが実際に害を及ぼすのは、高密度SAS SSDシステムです。2チャネル、12Gb/秒のバスに24個のSAS SSDがあり、デバイスの1つが6Gb/秒しかない場合、データレートが低下するため、850MB/秒をプッシュできる12Gb/秒SASドライブは、バスをオーバーサブスクライブするようになりました。
     </para>
     <para>
       別の考慮事項は、バスエクスパンダの存在です。バスエクスパンダを使用すると、システムは単一のチャネルで複数のデバイスを多重化できます。その結果、より低いパフォーマンス最大値で、密度が高くなります。HDDの場合など、エクスパンダは問題なく動作する場合がありますが、SSDの場合、エクスパンダはすぐにボトルネックになる可能性があります。
     </para>
   </sect2>
   <sect2>
     <title>一般的な推奨事項</title>
     <para>
       以下に、サーバプラットフォームのパフォーマンスチューニングに適用できるいくつかの一般的なチューニングオプションを示します。
     </para>
     <itemizedlist>
       <listitem>
         <para>
           ファームウェアの電源/パフォーマンスコントロールをパフォーマンスプロファイルに設定します。これにより、周波数スケーリングがなくなり、周波数スケーリングによって引き起こされる追加のレイテンシがなくなります。
         </para>
       </listitem>
       <listitem>
         <para>
           SMT対応のCPUでマルチスレッドを有効にします。この追加の処理能力は、Cephによって効果的に利用されます。
         </para>
       </listitem>
       <listitem>
         <para>
           すべてのアドインカードがパフォーマンスに最適なスロットにあることを確認します。
         </para>
       </listitem>
     </itemizedlist>
   </sect2>
 </sect1>
 <sect1 xml:id="tuning-cpeh-rocksdb-wal">
   <title>Ceph</title>
   <sect2>
     <title>RocksDBとWAL</title>
     <para>
       Cephでは、Write-Ahead-Log (WAL)と<literal>RocksDB</literal>の両方を使用します。WALは、バックエンドストレージにコミットする前に小さな書き込みがキューに格納される内部ジャーナルです。RocksDBは、BlueStoreに書き込まれたオブジェクトに関連付けられているメタデータをCephが格納する場所です。HDD(SSDの場合でも)を使用する際に、NVMeなどのより高速なデバイス上でRocksDBとWALを見つけることは一般的に理にかなっています。この場合、これらの適切なサイジングは、長期間にわたってクラスタの安定したパフォーマンスプロファイルを確保するために重要です。
     </para>
     <para>
       パフォーマンスの観点から、経験則では、WAL/RocksDBデバイスの書き込みパフォーマンスをデータデバイスの書き込みパフォーマンスで割り引きます。これにより、WAL/RocksDBデバイスごとにデータデバイスの最大比率と見なされるものが得られます。
     </para>
     <sect3>
       <title>WAL</title>
       <para>
         WALは、2ギガバイトを少し下回ります。保守作業の余地を残すため、約4ギガバイトのスペースを割り当てる/許可することが最適です。
       </para>
     </sect3>
     <sect3>
       <title>RocksDB</title>
       <para>
         RocksDBは一連の階層レベルで動作し、それぞれが最後のレベルよりもかなり大きくなっています。レベル1～4は、それぞれ256MB、2.56GB、25.6GB、256GBです。これらに適切なスペースを割り当てることは、集約の行為です。インストールで4番目の階層を必要とするのに十分なメタデータを使用するものがほとんどないと仮定すると、最初の3つの階層と関連する保守に割り当てるだけで十分です。25.6+2.56+.256 GB = 28.416 GB。最大30GBに切り上げ、100％のオーバーヘッドを提供して保守を可能にすると、最初の3層に推奨されるスペース割り当てが60GBになります。
       </para>
       <note>
         <para>
           WALデバイスには、4GBを予約することをお勧めします。DBの推奨されるサイズは、ほとんどのワークロードで合計64GBです。詳細については、<xref linkend="rec-waldb-size"/>を参照してください。
         </para>
       </note>
       <para>
         RocksDBの第4層に高速スペースをプロビジョニングする決定は、想定されるメタデータの負荷に完全に関連しています。RBDのようなプロトコルはメタデータをほとんど使用しませんが、CephFSは、軽度から中程度の量を使用します。S3とネイティブRADOSは、メタデータを最大限に利用でき、一般的に、第4層をより高速なメディアに移行することが理にかなっているかどうかを評価することが意味を持ち始めています。
       </para>
     </sect3>
   </sect2>
 </sect1>
</chapter>
