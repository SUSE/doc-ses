<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage.bp.hwreq">
 <title>ハードウェア要件と推奨事項</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>○</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Cephのハードウェア要件は、IOワークロードに大きく依存します。次のハードウェア要件と推奨事項は、詳細な計画を立てる際の起点と考えてください。
 </para>
 <para>
  一般的に、このセクションで説明する推奨事項はプロセスごとの推奨事項です。同じマシンに複数のプロセスがある場合は、CPU、RAM、ディスク、およびネットワークの各要件を追加する必要があります。
 </para>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>オブジェクトストレージノード</title>

  <sect2 xml:id="sysreq.osd">
   <title>最小要件</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      少なくとも4つのOSDノードと、そのそれぞれに8つのOSDディスクが必要です。
     </para>
    </listitem>
    <listitem>
     <para>
      BlueStoreを「使用しない」<emphasis/>OSDの場合、各OSDストレージノードに対して、最低でもOSDの未加工容量1テラバイトあたり1GBのRAMが必要です。OSDの未加工容量1テラバイトあたり1.5GBのRAMを推奨します。回復時には、OSDの未加工容量1テラバイトあたり2GBのRAMが最適な場合があります。
     </para>
     <para>
      BlueStoreを「使用する」<emphasis/>OSDの場合は、まずBlueStoreを使用しないOSDの推奨RAMサイズを計算します。次に、各OSDプロセスに推奨されるBlueStore RAMキャッシュサイズに2GBを足した値を計算し、2つの結果のうち大きい方のRAMの値を選択します。デフォルトのBlueStoreキャッシュは、HDDでは1GB、SSDでは3GBです。つまり、次のうちの大きい方を選択します。
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      または
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      各OSDデーモンプロセスに対して、最低でもOSDあたり1つの1.5GHzの論理CPUコアが必要です。OSDデーモンプロセスあたり2GHzを推奨します。Cephはストレージディスクあたり1つのOSDデーモンプロセスを実行します。OSDジャーナル、WALジャーナル、omapメタデータ、またはこれら3つのケースの任意の組み合わせ専用として予約されたディスクはカウントしないでください。
     </para>
    </listitem>
    <listitem>
     <para>
      10Gb Ethernet (複数のスイッチにボンディングされた2つのネットワークインタフェース)。
     </para>
    </listitem>
    <listitem>
     <para>
      JBOD設定のOSDディスク。
     </para>
    </listitem>
    <listitem>
     <para>
      OSDディスクはSUSE Enterprise Storage専用である必要があります。
     </para>
    </listitem>
    <listitem>
     <para>
      オペレーティングシステム専用のディスク/SSD (できればRAID 1設定)。
     </para>
    </listitem>
    <listitem>
     <para>
      このOSDホストが、キャッシュ階層化に使用するキャッシュプールの一部をホストする場合、追加で4GB以上のRAMを割り当てます。
     </para>
    </listitem>
    <listitem>
     <para>
      OSDノードは、ディスクパフォーマンス上の理由から、ベアメタルで、かつ仮想化されていない必要があります。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>最小ディスクサイズ</title>
   <para>
    OSD上で実行する必要があるディスク領域には2つのタイプがあります。ディスクジャーナル用の領域(FileStoreの場合)またはWAL/DBデバイス用の領域(BlueStoreの場合)と、保存データ用のプライマリ領域です。ジャーナル/WAL/DBの最小(デフォルト)の値は6GBです。データ用の最小領域は5GBです。これは、5GB未満のパーティションには自動的に重み0が割り当てられるためです。
   </para>
   <para>
    したがって、OSDの最小ディスク領域は11GBになりますが、テスト目的であっても20GB未満のディスクはお勧めしません。
   </para>
  </sect2>

  <sect2 xml:id="rec.waldb.size">
   <title>BlueStoreのWALおよびDBデバイスの推奨サイズ</title>
   <tip>
    <title>詳細情報</title>
    <para>
     BlueStoreの詳細については、<xref linkend="about.bluestore"/>を参照してください。
    </para>
   </tip>
   <para>
    以下に、WAL/DBデバイスのサイジングのルールをいくつか示します。DeepSeaを使用してOSDをBlueStoreと共に展開する場合、推奨ルールが自動的に適用され、管理者にそのことが通知されます。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      OSD容量の1テラバイトごとに10GBのDBデバイス(OSDの1/100)。
     </para>
    </listitem>
    <listitem>
     <para>
      WALデバイス用に500MB～2GB。WALのサイズは、OSDのサイズではなく、データトラフィックとワークロードによって変わります。OSDが少量の書き込みと上書きを非常に高いスループットで物理的に処理できることがわかっている場合、WALの容量を減らすのではなく増やすことをお勧めします。1GBのWALデバイスは、ほとんどの展開を満足するちょうど良い妥協点です。
     </para>
    </listitem>
    <listitem>
     <para>
      WALとDBデバイスを同じディスクに配置する予定の場合は、それぞれに別個のパーティションを設けるのではなく、両方のデバイスに対して単一のパーティションを使用することをお勧めします。これにより、CephはDBデバイスをWAL操作にも使用できます。Cephは必要時にのみDBパーティションをWALに使用するので、ディスク領域の管理が効率化します。もう1つの利点は、WALパーティションがいっぱいになる可能性は極めて低く、完全に使い切っていなければ、その領域が無駄になることはなく、DB操作に使用されることです。
     </para>
     <para>
      DBデバイスをWALと共有するには、WALデバイスを「指定しない」<emphasis/>で、DBデバイスのみを指定します。
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      または、WALを別個の専用のデバイスに配置できます。このような場合、WALの操作には最も高速なデバイスをお勧めします。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>OSDジャーナルでのSSDの使用</title>
   <para>
    SSD (ソリッドステートドライブ)には可動部品がありません。これは、ランダムアクセス時間と読み込みレイテンシを短縮すると同時に、データスループットを加速します。SSDの1MBあたりの価格は回転型ハードディスクの価格より大幅に高いため、SSDは小容量のストレージにのみ適しています。
   </para>
   <para>
    ジャーナルをSSDに保存し、オブジェクトデータは別個のハードディスクに保存することで、OSDのパフォーマンスを大幅に向上することができます。
   </para>
   <tip>
    <title>複数のジャーナルでのSSDの共有</title>
    <para>
     ジャーナルデータは比較的少ない使用領域しか占有しないため、1つのSSDディスクに複数のジャーナルディレクトリをマウントできます。共有ジャーナルを増やすと、そのたびにSSDディスクのパフォーマンスが低下するので注意してください。同じSSDディスクでジャーナルを7つ以上共有することはお勧めしません。NVMeディスクの場合は、13個以上の共有はお勧めしません。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>推奨されるディスクの最大数</title>
   <para>
    1台のサーバで、そのサーバで使用できる数だけのディスクを使用できます。サーバあたりのディスク数を計画する際には、考慮すべき点がいくつかあります。
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      「ネットワーク帯域幅」<emphasis/>サーバのディスクが増えるほど、ディスク書き込み操作のためにネットワークカード経由で転送しなければならないデータが増えます。
     </para>
    </listitem>
    <listitem>
     <para>
      「メモリ」<emphasis/>最適なパフォーマンスを得るには、インストールされているディスク領域1テラバイトあたり2GB以上のRAMを予約します。
     </para>
    </listitem>
    <listitem>
     <para>
      「耐障害性」<emphasis/>サーバ全体に障害が発生した場合、搭載ディスクの数が多いほど、クラスタが一時的に失うOSDが増えます。さらに、レプリケーションルールを実行し続けるために、障害が発生したサーバからクラスタ内のほかのノードにすべてのデータをコピーする必要があります。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.mon">
  <title>モニタノード</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     少なくとも3つのCeph Monitorノードが必要です。モニタの数は常に基数(1+2n)である必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     4GBのRAM。
    </para>
   </listitem>
   <listitem>
    <para>
     4つの論理コアを持つプロセッサ。
    </para>
   </listitem>
   <listitem>
    <para>
     特に各モニタノードの<filename>/var/lib/ceph</filename>パスには、SSDか、その他の十分高速なストレージタイプを強くお勧めします。ディスクのレイテンシが大きいと、クォーラムが不安定になる可能性があるためです。冗長性を確保するには、RAID 1設定で2台のディスクを使用することをお勧めします。モニタが利用可能なディスク領域をログファイルの増大などから保護するため、モニタプロセスには、別個のディスク、または少なくとも別個のディスクパーティションを使用することをお勧めします。
    </para>
   </listitem>
   <listitem>
    <para>
     各ノードのモニタプロセスは1つだけにする必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     OSDノード、モニタノード、またはObject Gatewayノードを混在させることは、十分なハードウェアリソースが利用可能な場合にのみサポートされます。つまり、すべてのサービスの要件を合計する必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     複数のスイッチにボンディングされた2つのネットワークインタフェース。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>Object Gatewayノード</title>

  <para>
   Object Gatewayノードには、6～8個のCPUコアと32GBのRAM (64GBを推奨)が必要です。同じマシンに他のプロセスも配置されている場合、それらの要件を合計する必要があります。
  </para>
 </sect1>
 <sect1 xml:id="sysreq.mds">
  <title>Metadata Serverノード</title>

  <para>
   Metadata Serverノードの適切なサイズは、具体的な使用事例によって異なります。一般的には、Metadata Serverが処理する開いているファイルが多いほど、より多くのCPUとRAMが必要になります。最小要件は次のとおりです。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Metadata Serverデーモンあたり3GのRAM。
    </para>
   </listitem>
   <listitem>
    <para>
     ボンディングされた2つのネットワークインタフェース。
    </para>
   </listitem>
   <listitem>
    <para>
     2個以上のコアを持つ2.5GHzのCPU。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.smaster">
  <title>Salt Master</title>

  <para>
   少なくとも4GBのRAMとクアッドコアCPUが必要です。これには、Salt MasterでopenATTICを実行することが含まれます。数百のノードで構成される大規模クラスタでは、6GBのRAMをお勧めします。
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>iSCSIノード</title>

  <para>
   iSCSIノードには、6～8個のCPUコアと16GBのRAMが必要です。
  </para>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>ネットワーク要件</title>

  <para>
   Cephを実行する予定のネットワーク環境は、2つ以上のネットワークインタフェースボンディングしたセットにし、このセットを、VLANを使用してパブリック部分と信頼する内部部分に論理的に分割するのが理想です。最大の帯域幅と災害耐性を提供するため、可能であればボンディングモードは802.3adにすることをお勧めします。
  </para>

  <para>
   パブリックVLANは顧客にサービスを提供する役目を果たすのに対し、内部部分は認証されたCephネットワーク通信を提供します。この主な理由は、Cephはいったん秘密鍵が用意されれば認証と攻撃に対する保護を提供しますが、その鍵の設定に使用されるメッセージはオープンに転送される場合があり、脆弱であるためです。
  </para>

  <tip>
   <title>DHCP経由で設定されたノード</title>
   <para>
    ストレージノードがDHCP経由で設定されている場合、さまざまCephデーモンが起動する前にネットワークを正しく設定するのにデフォルトのタイムアウトでは十分でないことがあります。この場合、Ceph MONとOSDは正しく起動しません(<command>systemctl status ceph\*</command>を実行すると、「unable to bind (バインドできません)」というエラーが発生します)。この問題を避けるため、Storage Cluster内の各ノードでDHCPクライアントのタイムアウトを30秒以上に増やすことをお勧めします。このためには、各ノードで以下の設定を変更します。
   </para>
   <para>
    <filename>/etc/sysconfig/network/dhcp</filename>で、以下を設定します。
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    <filename>/etc/sysconfig/network/config</filename>で、以下を設定します。
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage.bp.net.private">
   <title>実行中のクラスタへのプライベートネットワークの追加</title>
   <para>
    Cephの展開中にクラスタネットワークを指定しない場合、単一のパブリックネットワーク環境と想定されます。Cephはパブリックネットワークで問題なく動作しますが、2つ目のプライベートクラスタネットワークを設定すると、パフォーマンスとセキュリティが向上します。2つのネットワークをサポートするには、各Cephノードに少なくとも2つのネットワークカードが必要です。
   </para>
   <para>
    各Cephノードに以下の変更を適用する必要があります。これらの変更は、小規模なクラスタの場合は比較的短時間で済みますが、数百または数千のノードで構成されるクラスタの場合は非常に時間がかかる可能性があります。
   </para>
   <procedure>
    <step>
     <para>
      各クラスタノードでCeph関連サービスを停止します。
     </para>
     <para>
      <filename>/etc/ceph/ceph.conf</filename>に行を追加してクラスタネットワークを定義します。次に例を示します。
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      特に静的IPアドレスを割り当てたり、<option>cluster network</option>設定を上書きしたりする必要がある場合は、オプションの<option>cluster addr</option>で設定できます。
     </para>
    </step>
    <step>
     <para>
      プライベートクラスタネットワークがOSレベルで想定どおりに動作していることを確認します。
     </para>
    </step>
    <step>
     <para>
      各クラスタノードでCeph関連サービスを起動します。
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.net.subnets">
   <title>異なるサブネット上のモニタノード</title>
   <para>
    モニタノードが複数のサブネット上に存在する場合(たとえば、別の部屋に配置されていたり、別のスイッチによってサービスを提供されていたりする場合)、適切に<filename>ceph.conf</filename>ファイルを調整する必要があります。たとえば、ノードにIPアドレス192.168.123.12、1.2.3.4、および242.12.33.12が設定されている場合、[global]セクションに次の行を追加します。
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    さらに、モニタごとにパブリックアドレスまたはネットワークを指定する必要がある場合は、各モニタに対して<literal>[mon.<replaceable>X</replaceable>]</literal>セクションを追加する必要があります。
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>命名の制限</title>

  <para>
   一般的に、Cephは、設定ファイル、プール名、ユーザ名などでASCII以外の文字をサポートしません。Cephクラスタを設定する場合、すべてのCephオブジェクト名/設定名で単純な英数字の文字(A～Z、a～z、0～9)と、最小限の句読点(「.」「-」「_」)のみを使用することをお勧めします。
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>OSDとモニタでの1台のサーバの共有</title>

  <para>
   テスト環境ではCeph OSDとモニタを同じサーバで実行することは技術的に可能ですが、運用ではモニタノードごとに別個のサーバを用意することを強くお勧めします。その主な理由はパフォーマンスです。クラスタのOSDが増えるほど、モニタノードが実行しなければならないI/O操作が増えます。さらに、1台のサーバをモニタノードとOSDで共有する場合、OSDのI/O操作がモニタノードにとって制限要因になります。
  </para>

  <para>
   考慮すべきもう1つの点は、サーバ上のOSD、モニタノード、およびオペレーティングシステムでディスクを共有するかどうかです。答えは単純です。可能であれば、OSDには別個の専用ディスクを使用し、モニタノードには別個の専用サーバを使用します。
  </para>

  <para>
   CephはディレクトリベースのOSDをサポートしますが、OSDには、常にオペレーティングシステムのディスクではなく専用ディスクを使用する必要があります。
  </para>

  <tip>
   <para>
    OSDとモニタノードを「本当に」<emphasis/>同じサーバで実行する必要がある場合は、モニタを別個のディスクで実行し、そのディスクを<filename>/var/lib/ceph/mon</filename>ディレクトリにマウントすることで、少しでもパフォーマンスを向上させます。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses.bp.minimum_cluster">
  <title>最小クラスタ設定</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     4つのオブジェクトストレージノード
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10Gb Ethernet (複数のスイッチにボンディングされた2つのネットワーク)
      </para>
     </listitem>
     <listitem>
      <para>
       Storage Clusterあたり32のOSD
      </para>
     </listitem>
     <listitem>
      <para>
       OSDジャーナルはOSDディスクに配置可能
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対して専用のOSディスク
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対して、OSDの未加工容量1TBあたり1GBのRAM
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対してOSDあたり1.5GHz
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor、ゲートウェイ、およびMetadata Serverはオブジェクトストレージノードに配置可能
      </para>
      <itemizedlist>
       <listitem>
        <para>
         3つのCeph Monitorノード(OS専用ドライブ用にSSDが必要)
        </para>
       </listitem>
       <listitem>
        <para>
         Ceph Monitorノード、Object Gatewayノード、およびMetadata Serverノードは冗長展開が必要
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI Gateway、Object Gateway、およびMetadata Serverには、4GBのRAMと4コアの追加が必要
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     4GBのRAM、4コア、1TBの容量を備えた別個の管理ノード
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses.bp.production_cluster">
  <title>運用クラスタの推奨設定</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     7つのオブジェクトストレージノード
    </para>
    <itemizedlist>
     <listitem>
      <para>
       1つのノードが最大で合計ストレージの15%を超えないこと
      </para>
     </listitem>
     <listitem>
      <para>
       10Gb Ethernet (複数のスイッチにボンディングされた4つの物理ネットワーク)
      </para>
     </listitem>
     <listitem>
      <para>
       Storage Clusterあたり56以上のOSD
      </para>
     </listitem>
     <listitem>
      <para>
       各OSDストレージノードに対してRAID 1のOSディスク
      </para>
     </listitem>
     <listitem>
      <para>
       ジャーナル用のSSD (SSDジャーナルとOSDの比率は6:1)
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対して、OSDの未加工容量1TBあたり1.5GBのRAM
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対してOSDあたり2GHz
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     専用の物理インフラストラクチャノード
    </para>
    <itemizedlist>
     <listitem>
      <para>
       3つのCeph Monitorノード: 4GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       1つのSES管理ノード: 4GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイまたはMetadata Serverノードの冗長な物理展開:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Object Gatewayノード: 32GBのRAM、8コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI Gatewayノード: 16GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
       <listitem>
        <para>
         Metadata Serverノード(アクティブ x 1/ホットスタンバイ x 1): 32GBのRAM、8コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req.ses.other">
  <title>SUSE Enterprise Storageとその他のSUSE製品</title>

  <para>
   このセクションには、SUSE Enterprise Storageと他のSUSE製品との統合に関する重要な情報が記載されています。
  </para>

  <sect2 xml:id="req.ses.suma">
   <title>SUSE Manager</title>
   <para>
    SUSE ManagerとSUSE Enterprise Storageは統合されていません。そのため、現在のところSUSE ManagerでSUSE Enterprise Storage Clusterを管理することはできません。
   </para>
  </sect2>
 </sect1>
</chapter>
