<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>ハードウェア要件と推奨事項</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Cephのハードウェア要件は、IOワークロードに大きく依存します。次のハードウェア要件と推奨事項は、詳細な計画を立てる際の起点と考えてください。
 </para>
 <para>
  一般的に、このセクションで説明する推奨事項はプロセスごとの推奨事項です。同じマシンに複数のプロセスがある場合は、CPU、RAM、ディスク、およびネットワークの各要件を追加する必要があります。
 </para>
 <sect1 xml:id="multi-architecture">
  <title>複数のアーキテクチャの設定</title>

  <para>
   SUSE Enterprise Storageでは、x86とArmの両方のアーキテクチャをサポートしています。各アーキテクチャを検討する際は、OSDあたりのコア数、周波数、およびRAMの観点から見ると、サイジングに関して、CPUアーキテクチャ間に実質的な差異はないことに注意することが重要です。
  </para>

  <para>
   小型のx86プロセッサ(サーバ以外)と同様に、パフォーマンスの低いArmベースのコアは、特にイレージャコーディングプールに使用する場合は最適なエクスペリエンスを提供できない可能性があります。
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>最小クラスタ設定</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     少なくとも4つのOSDノードと、そのそれぞれに8つのOSDディスクが必要。
    </para>
   </listitem>
   <listitem>
    <para>
     3つのCeph Monitorノード(OS専用ドライブ用にSSDが必要)。
    </para>
   </listitem>
   <listitem>
    <para>
     iSCSI Gateway、Object Gateway、およびMetadata Serverには、4GBのRAMと4コアの追加が必要。
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph Monitorノード、Object Gatewayノード、およびMetadata Serverノードは冗長展開が必要。
    </para>
   </listitem>
   <listitem>
    <para>
     4GBのRAM、4コア、1TBの容量を備えた別個の管理ノード。これは通常、Salt Masterノードです。Ceph Monitor、Ceph Manager、Metadata Server、Ceph OSD、Object Gateway、NFS GaneshaなどのCephサービスおよびゲートウェイは、管理ノードではサポートされません。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>オブジェクトストレージノード</title>

  <sect2 xml:id="sysreq-osd">
   <title>最小要件</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      CPUの推奨事項
     </para>
     <itemizedlist>
      <listitem>
       <para>
        スピナあたり1つの2GHz CPUスレッド
       </para>
      </listitem>
      <listitem>
       <para>
        SSDあたり2つの2GHz CPUスレッド
       </para>
      </listitem>
      <listitem>
       <para>
        MVMeあたり4つの2GHz CPUスレッド
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      独立した10GbEネットワーク(パブリック/クライアントおよびバックエンド)、4つの10GbEが必須、2つの25GbEを推奨。
     </para>
    </listitem>
    <listitem>
     <para>
      必要なRAMの合計 = OSDの数 x (1GB + <option>osd_memory_target</option>) + 16GB
     </para>
     <para>
      <option>osd_memory_target</option>の詳細については、<xref linkend="config-auto-cache-sizing"/>を参照してください。
     </para>
    </listitem>
    <listitem>
     <para>
      JBOD設定または個々のRAID-0設定のOSDディスク。
     </para>
    </listitem>
    <listitem>
     <para>
      OSDジャーナルはOSDディスクに配置可能.
     </para>
    </listitem>
    <listitem>
     <para>
      OSDディスクはSUSE Enterprise Storage専用である必要があります。
     </para>
    </listitem>
    <listitem>
     <para>
      オペレーティングシステム専用のディスク/SSD (できればRAID 1設定)。
     </para>
    </listitem>
    <listitem>
     <para>
      このOSDホストが、キャッシュ階層化に使用するキャッシュプールの一部をホストする場合、追加で4GB以上のRAMを割り当てます。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph Monitor、ゲートウェイ、およびMetadata Serverはオブジェクトストレージノードに配置可能.
     </para>
    </listitem>
    <listitem>
     <para>
      ディスクパフォーマンス上の理由から、OSDノードには、仮想マシンではなくベアメタルを使用することをお勧めします。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>最小ディスクサイズ</title>
   <para>
    OSD上で実行する必要があるディスク領域には2つのタイプがあります。ディスクジャーナル用の領域(FileStoreの場合)またはWAL/DBデバイス用の領域(BlueStoreの場合)と、保存データ用のプライマリ領域です。ジャーナル/WAL/DBの最小(デフォルト)の値は6GBです。データ用の最小領域は5GBです。これは、5GB未満のパーティションには自動的に重み0が割り当てられるためです。
   </para>
   <para>
    したがって、OSDの最小ディスク領域は11GBになりますが、テスト目的であっても20GB未満のディスクはお勧めしません。
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>BlueStoreのWALおよびDBデバイスの推奨サイズ</title>
   <tip>
    <title>詳細情報</title>
    <para>
     BlueStoreの詳細については、<xref linkend="about-bluestore"/>を参照してください。
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      WALデバイス用に4GBを予約することをお勧めします。DBの推奨サイズは、ほとんどのワークロードにおいて64MBです。
     </para>
    </listitem>
    <listitem>
     <para>
      WALとDBデバイスを同じディスクに配置する予定の場合は、それぞれに別個のパーティションを設けるのではなく、両方のデバイスに対して単一のパーティションを使用することをお勧めします。これにより、CephはDBデバイスをWAL操作にも使用できます。Cephは必要時にのみDBパーティションをWALに使用するので、ディスク領域の管理が効率化します。もう1つの利点は、WALパーティションがいっぱいになる可能性は極めて低く、完全に使い切っていなければ、その領域が無駄になることはなく、DB操作に使用されることです。
     </para>
     <para>
      DBデバイスをWALと共有するには、WALデバイスを「指定しない」<emphasis/>で、DBデバイスのみを指定します。
     </para>
     <para>
      OSDレイアウトを指定する方法の詳細については、<xref linkend="ds-drive-groups"/>を参照してください。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>OSDジャーナルでのSSDの使用</title>
   <para>
    SSD (ソリッドステートドライブ)には可動部品がありません。これは、ランダムアクセス時間と読み込みレイテンシを短縮すると同時に、データスループットを加速します。SSDの1MBあたりの価格は回転型ハードディスクの価格より大幅に高いため、SSDは小容量のストレージにのみ適しています。
   </para>
   <para>
    ジャーナルをSSDに保存し、オブジェクトデータは別個のハードディスクに保存することで、OSDのパフォーマンスを大幅に向上することができます。
   </para>
   <tip>
    <title>複数のジャーナルでのSSDの共有</title>
    <para>
     ジャーナルデータは比較的少ない使用領域しか占有しないため、1つのSSDディスクに複数のジャーナルディレクトリをマウントできます。共有ジャーナルを増やすと、そのたびにSSDディスクのパフォーマンスが低下するので注意してください。同じSSDディスクでジャーナルを7つ以上共有することはお勧めしません。NVMeディスクの場合は、13個以上の共有はお勧めしません。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>推奨されるディスクの最大数</title>
   <para>
    1台のサーバで、そのサーバで使用できる数だけのディスクを使用できます。サーバあたりのディスク数を計画する際には、考慮すべき点がいくつかあります。
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      「ネットワーク帯域幅」<emphasis/>サーバのディスクが増えるほど、ディスク書き込み操作のためにネットワークカード経由で転送しなければならないデータが増えます。
     </para>
    </listitem>
    <listitem>
     <para>
      「メモリ」<emphasis/>2GBを超えるRAMは、BlueStoreキャッシュに使用されます。<option>osd_memory_target</option>のデフォルトである4GBを使用すると、回転型メディアに適したキャッシュ開始サイズがシステムに設定されます。SSDまたはNVMEを使用する場合は、OSDあたりのキャッシュサイズとRAM割り当てを増やすことを検討します。
     </para>
    </listitem>
    <listitem>
     <para>
      「耐障害性」<emphasis/>サーバ全体に障害が発生した場合、搭載ディスクの数が多いほど、クラスタが一時的に失うOSDが増えます。さらに、レプリケーションルールを実行し続けるために、障害が発生したサーバからクラスタ内のほかのノードにすべてのデータをコピーする必要があります。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>モニタノード</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     少なくとも3つのCeph Monitorノードが必要です。モニタの数は常に基数(1+2n)である必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     4GBのRAM。
    </para>
   </listitem>
   <listitem>
    <para>
     4つの論理コアを持つプロセッサ。
    </para>
   </listitem>
   <listitem>
    <para>
     特に各モニタノードの<filename>/var/lib/ceph</filename>パスには、SSDか、その他の十分高速なストレージタイプを強くお勧めします。ディスクのレイテンシが大きいと、クォーラムが不安定になる可能性があるためです。冗長性を確保するには、RAID 1設定で2台のディスクを使用することをお勧めします。モニタが利用可能なディスク領域をログファイルの増大などから保護するため、モニタプロセスには、別個のディスク、または少なくとも別個のディスクパーティションを使用することをお勧めします。
    </para>
   </listitem>
   <listitem>
    <para>
     各ノードのモニタプロセスは1つだけにする必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     OSDノード、モニタノード、またはObject Gatewayノードを混在させることは、十分なハードウェアリソースが利用可能な場合にのみサポートされます。つまり、すべてのサービスの要件を合計する必要があります。
    </para>
   </listitem>
   <listitem>
    <para>
     複数のスイッチにボンディングされた2つのネットワークインタフェース。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Object Gatewayノード</title>

  <para>
   Object Gatewayノードには、6～8個のCPUコアと32GBのRAM (64GBを推奨)が必要です。同じマシンに他のプロセスも配置されている場合、それらの要件を合計する必要があります。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Metadata Serverノード</title>

  <para>
   Metadata Serverノードの適切なサイズは、具体的な使用事例によって異なります。一般的には、Metadata Serverが処理する開いているファイルが多いほど、より多くのCPUとRAMが必要になります。最小要件は次のとおりです。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     各Metadata Serverドメインに対して3GのRAM。
    </para>
   </listitem>
   <listitem>
    <para>
     ボンディングされた2つのネットワークインタフェース。
    </para>
   </listitem>
   <listitem>
    <para>
     2個以上のコアを持つ2.5GHzのCPU。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   少なくとも4GBのRAMとクアッドコアCPUが必要です。これには、管理ノードでのCephダッシュボードの実行が含まれます。数百のノードで構成される大規模クラスタでは、6GBのRAMをお勧めします。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>iSCSIノード</title>

  <para>
   iSCSIノードには、6～8個のCPUコアと16GBのRAMが必要です。
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>ネットワーク要件</title>

  <para>
   Cephを実行する予定のネットワーク環境は、2つ以上のネットワークインタフェースボンディングしたセットにし、このセットを、VLANを使用してパブリック部分と信頼する内部部分に論理的に分割するのが理想です。最大の帯域幅と災害耐性を提供するため、可能であればボンディングモードは802.3adにすることをお勧めします。
  </para>

  <para>
   パブリックVLANは顧客にサービスを提供する役目を果たすのに対し、内部部分は認証されたCephネットワーク通信を提供します。この主な理由は、Cephはいったん秘密鍵が用意されれば認証と攻撃に対する保護を提供しますが、その鍵の設定に使用されるメッセージはオープンに転送される場合があり、脆弱であるためです。
  </para>

  <tip>
   <title>DHCP経由で設定されたノード</title>
   <para>
    ストレージノードがDHCP経由で設定されている場合、さまざまCephデーモンが起動する前にネットワークを正しく設定するのにデフォルトのタイムアウトでは十分でないことがあります。この場合、Ceph MONとOSDは正しく起動しません(<command>systemctl status ceph\*</command>を実行すると、「unable to bind (バインドできません)」というエラーが発生します)。この問題を避けるため、Storage Cluster内の各ノードでDHCPクライアントのタイムアウトを30秒以上に増やすことをお勧めします。このためには、各ノードで以下の設定を変更します。
   </para>
   <para>
    <filename>/etc/sysconfig/network/dhcp</filename>で、以下を設定します。
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    <filename>/etc/sysconfig/network/config</filename>で、以下を設定します。
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>実行中のクラスタへのプライベートネットワークの追加</title>
   <para>
    Cephの展開中にクラスタネットワークを指定しない場合、単一のパブリックネットワーク環境と想定されます。Cephはパブリックネットワークで問題なく動作しますが、2つ目のプライベートクラスタネットワークを設定すると、パフォーマンスとセキュリティが向上します。2つのネットワークをサポートするには、各Cephノードに少なくとも2つのネットワークカードが必要です。
   </para>
   <para>
    各Cephノードに以下の変更を適用する必要があります。これらの変更は、小規模なクラスタの場合は比較的短時間で済みますが、数百または数千のノードで構成されるクラスタの場合は非常に時間がかかる可能性があります。
   </para>
   <procedure>
    <step>
     <para>
      各クラスタノードでCeph関連サービスを停止します。
     </para>
     <para>
      <filename>/etc/ceph/ceph.conf</filename>に行を追加してクラスタネットワークを定義します。次に例を示します。
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      特に静的IPアドレスを割り当てたり、<option>cluster network</option>設定を上書きしたりする必要がある場合は、オプションの<option>cluster addr</option>で設定できます。
     </para>
    </step>
    <step>
     <para>
      プライベートクラスタネットワークがOSレベルで想定どおりに動作していることを確認します。
     </para>
    </step>
    <step>
     <para>
      各クラスタノードでCeph関連サービスを起動します。
     </para>
<screen><prompt>root # </prompt>systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>異なるサブネット上のモニタノード</title>
   <para>
    モニタノードが複数のサブネット上に存在する場合(たとえば、別の部屋に配置されていたり、別のスイッチによってサービスを提供されていたりする場合)、適切に<filename>ceph.conf</filename>ファイルを調整する必要があります。たとえば、ノードにIPアドレス192.168.123.12、1.2.3.4、および242.12.33.12が設定されている場合、<literal>global</literal>セクションに次の行を追加します。
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    さらに、モニタごとにパブリックアドレスまたはネットワークを指定する必要がある場合は、各モニタに対して<literal>[mon.<replaceable>X</replaceable>]</literal>セクションを追加する必要があります。
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>命名の制限</title>

  <para>
   一般的に、Cephは、設定ファイル、プール名、ユーザ名などでASCII以外の文字をサポートしません。Cephクラスタを設定する場合、すべてのCephオブジェクト名/設定名で単純な英数字の文字(A～Z、a～z、0～9)と、最小限の句読点(「.」「-」「_」)のみを使用することをお勧めします。
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>OSDとモニタでの1台のサーバの共有</title>

  <para>
   テスト環境ではCeph OSDとモニタを同じサーバで実行することは技術的に可能ですが、運用ではモニタノードごとに別個のサーバを用意することを強くお勧めします。その主な理由はパフォーマンスです。クラスタのOSDが増えるほど、モニタノードが実行しなければならないI/O操作が増えます。さらに、1台のサーバをモニタノードとOSDで共有する場合、OSDのI/O操作がモニタノードにとって制限要因になります。
  </para>

  <para>
   考慮すべきもう1つの点は、サーバ上のOSD、モニタノード、およびオペレーティングシステムでディスクを共有するかどうかです。答えは単純です。可能であれば、OSDには別個の専用ディスクを使用し、モニタノードには別個の専用サーバを使用します。
  </para>

  <para>
   CephはディレクトリベースのOSDをサポートしますが、OSDには、常にオペレーティングシステムのディスクではなく専用ディスクを使用する必要があります。
  </para>

  <tip>
   <para>
    OSDとモニタノードを「本当に」<emphasis/>同じサーバで実行する必要がある場合は、モニタを別個のディスクで実行し、そのディスクを<filename>/var/lib/ceph/mon</filename>ディレクトリにマウントすることで、少しでもパフォーマンスを向上させます。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>運用クラスタの推奨設定</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     7つのオブジェクトストレージノード
    </para>
    <itemizedlist>
     <listitem>
      <para>
       1つのノードが最大で合計ストレージの15%を超えないこと
      </para>
     </listitem>
     <listitem>
      <para>
       10Gb Ethernet (複数のスイッチにボンディングされた4つの物理ネットワーク)
      </para>
     </listitem>
     <listitem>
      <para>
       Storage Clusterあたり56以上のOSD
      </para>
     </listitem>
     <listitem>
      <para>
       各OSDストレージノードに対してRAID 1のOSディスク
      </para>
     </listitem>
     <listitem>
      <para>
       ジャーナル用のSSD (SSDジャーナルとOSDの比率は6:1)
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対して、OSDの未加工容量1TBあたり1.5GBのRAM
      </para>
     </listitem>
     <listitem>
      <para>
       各オブジェクトストレージノードに対してOSDあたり2GHz
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     専用の物理インフラストラクチャノード
    </para>
    <itemizedlist>
     <listitem>
      <para>
       3つのCeph Monitorノード: 4GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       1つのSES管理ノード: 4GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイまたはMetadata Serverノードの冗長な物理展開:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Object Gatewayノード: 32GBのRAM、8コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI Gatewayノード: 16GBのRAM、4コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
       <listitem>
        <para>
         Metadata Serverノード(アクティブ x 1/ホットスタンバイ x 1): 32GBのRAM、8コアプロセッサ、ディスク用のRAID 1 SSD
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage 6とその他のSUSE製品</title>

  <para>
   このセクションには、SUSE Enterprise Storage 6と他のSUSE製品との統合に関する重要な情報が記載されています。
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE ManagerとSUSE Enterprise Storageは統合されていません。そのため、現在のところSUSE ManagerでSUSE Enterprise Storage Clusterを管理することはできません。
   </para>
  </sect2>
 </sect1>
</chapter>
