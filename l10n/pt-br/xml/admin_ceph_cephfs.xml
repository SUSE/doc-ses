<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Sistema de arquivos em cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Este capítulo descreve as tarefas de administração que normalmente são executadas depois que o cluster é configurado e o CephFS é exportado. Se você precisar de mais informações sobre como configurar o CephFS, consulte o <xref linkend="deploy-cephadm-day2-service-mds"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montando o CephFS</title>

  <para>
   Quando o sistema de arquivos é criado e o MDS está ativo, você está pronto para montar o sistema de arquivos de um host de cliente.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Preparando o cliente</title>
   <para>
    Se o host de cliente executa o SUSE Linux Enterprise 12 SP2 ou versão mais recente, o sistema está pronto para montar o CephFS “out-of-the-box”.
   </para>
   <para>
    Se o host de cliente executa o SUSE Linux Enterprise 12 SP1, você precisa aplicar todos os patches mais recentes antes de montar o CephFS.
   </para>
   <para>
    Em qualquer caso, tudo o que é preciso para montar o CephFS está incluído no SUSE Linux Enterprise. O produto SUSE Enterprise Storage 7 não é necessário.
   </para>
   <para>
    Para suportar a sintaxe completa de <command>mount</command>, o pacote
    <package>ceph-common</package> (que é fornecido com o SUSE Linux Enterprise) deve ser instalado antes de tentar montar o CephFS.
   </para>
   <important>
    <para>
     Sem o pacote <package>ceph-common</package> (e, portanto, sem o ajudante <command>mount.ceph</command>), os IPs dos monitores precisarão ser usados em vez dos nomes. O motivo é que o cliente do kernel não poderá executar a resolução de nome.
    </para>
    <para>
     A sintaxe de montagem básica é:
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Criando um arquivo secreto</title>
   <para>
    Por padrão, o cluster do Ceph é executado com a autenticação ativada. Você deve criar um arquivo que armazena sua chave secreta (não o chaveiro propriamente dito). Para obter a chave secreta para determinado usuário e, em seguida, criar o arquivo, faça o seguinte:
   </para>
   <procedure>
    <title>Criando uma chave secreta</title>
    <step>
     <para>
      Veja a chave do usuário específico em um arquivo de chaveiro:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copie a chave do usuário que utilizará o sistema de arquivos Ceph FS montado. Normalmente, a chave tem a seguinte aparência:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Crie um arquivo com o nome de usuário como parte do nome de arquivo. Por exemplo, <filename>/etc/ceph/admin.secret</filename> para o usuário <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Cole o valor da chave no arquivo criado na etapa anterior.
     </para>
    </step>
    <step>
     <para>
      Defina os direitos de acesso apropriados para o arquivo. O usuário deve ser a única pessoa que pode ler o arquivo. Outras pessoas não devem ter nenhum direito de acesso.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montando o CephFS</title>
   <para>
    Você pode montar o CephFS com o comando <command>mount</command>. Você precisa especificar o nome de host ou endereço IP do monitor. Como a autenticação <systemitem>cephx</systemitem> está habilitada por padrão no SUSE Enterprise Storage, você precisa especificar um nome de usuário e também o segredo relacionado:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Como o comando anterior permanece no histórico do shell, uma abordagem mais segura é ler o segredo de um arquivo:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Observe que o arquivo de segredo deve conter apenas o segredo do chaveiro real. Em nosso exemplo, o arquivo incluirá apenas a seguinte linha:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>Especificar vários monitores</title>
    <para>
     Convém especificar vários monitores separados por vírgulas na linha de comando <command>mount</command> para o caso de um monitor ficar inativo no momento da montagem. Cada endereço de monitor adota o formato <literal>host[:porta]</literal>. Se a porta não for especificada, será usado o padrão 6789.
    </para>
   </tip>
   <para>
    Crie o ponto de montagem no host local:
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Monte o CephFS:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Um subdiretório <filename>subdir</filename> poderá ser especificado se um subconjunto do sistema de arquivos tiver que ser montado:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Você pode especificar mais de um host de monitor no comando <command>mount</command>:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>acesso de leitura ao diretório raiz</title>
    <para>
     Se forem usados clientes com restrição de caminho, os recursos do MDS precisarão incluir o acesso de leitura ao diretório raiz. Por exemplo, um chaveiro pode ter a seguinte aparência:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     A parte <literal>allow r path=/</literal> indica que os clientes com restrição de caminho podem ver o volume raiz, mas não podem gravar nele. Isso pode ser um problema para casos de uso em que o isolamento completo é um requisito.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Desmontando o CephFS</title>

  <para>
   Para desmontar o CephFS, use o comando <command>umount</command>:
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>Montando o CephFS em <filename>/etc/fstab</filename></title>

  <para>
   Para montar o CephFS automaticamente na inicialização do cliente, insira a linha correspondente na respectiva tabela de sistemas de arquivos <filename>/etc/fstab</filename>:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Vários daemons MDS ativos (MDS ativo-ativo)</title>

  <para>
   Por padrão, o CephFS é configurado para um único daemon MDS ativo. Para aumentar o desempenho dos metadados em sistemas de grande escala, é possível habilitar vários daemons MDS ativos, que compartilharão a carga de trabalho dos metadados entre eles.
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>Usando o MDS ativo-ativo</title>
   <para>
    Considere o uso de vários daemons MDS ativos em caso de gargalo no desempenho dos metadados no MDS único padrão.
   </para>
   <para>
    A adição de mais daemons não aumenta o desempenho em todos os tipos de carga de trabalho. Por exemplo, um único aplicativo em execução em um só cliente não se beneficiará de um número maior de daemons MDS, a menos que o aplicativo esteja efetuando muitas operações de metadados em paralelo.
   </para>
   <para>
    As cargas de trabalho que costumam se beneficiar de um número maior de daemons MDS ativos são aquelas com vários clientes, que podem atuar em muitos diretórios separados.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Aumentando o tamanho do cluster MDS ativo</title>
   <para>
    Cada sistema de arquivos CephFS tem uma configuração <option>max_mds</option> que controla quantas classificações serão criadas. O número real de classificações no sistema de arquivos apenas será aumentado se um daemon sobressalente estiver disponível para assumir a nova classificação. Por exemplo, se houver apenas um daemon MDS em execução, e <option>max_mds</option> estiver definido como dois, não será criada uma segunda classificação.
   </para>
   <para>
    No exemplo a seguir, definimos a opção <option>max_mds</option> como 2 para criar uma nova classificação separadamente do padrão. Para ver as mudanças, execute <command>ceph status</command> antes e depois que você definir <option>max_mds</option> e observe a linha que contém <literal>fsmap</literal>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    A classificação recém-criada (1) passa pelo estado de “criação” e depois entra no estado “ativo”.
   </para>
   <important>
    <title>Daemons de standby</title>
    <para>
     Mesmo com vários daemons MDS ativos, um sistema altamente disponível ainda requer daemons de standby para assumir o controle em caso de falha em qualquer um dos servidores que executam um daemon ativo.
    </para>
    <para>
     Consequentemente, o limite máximo ideal de <option>max_mds</option> para sistemas de alta disponibilidade é menor do que o número total de servidores MDS no sistema. Para se manter disponível em caso de várias falhas do servidor, aumente o número de daemons de standby no sistema para corresponder ao número de falhas do servidor que você precisa superar.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Diminuindo o número de classificações</title>
   <para>
    Todas as classificações, incluindo as que devem ser removidas, devem primeiro estar ativas. Isso significa que é necessário ter pelo menos <option>max_mds</option> daemons MDS disponíveis.
   </para>
   <para>
    Primeiramente, defina <option>max_mds</option> como um número mais baixo. Por exemplo, volte a ter um único MDS ativo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Fixando manualmente árvores de diretório em uma classificação</title>
   <para>
    Em várias configurações de servidor de metadados ativas, um balanceador é executado, que funciona para distribuir a carga de metadados igualmente no cluster. Em geral, isso funciona bem o suficiente para a maioria dos usuários; mas, às vezes, convém anular o balanceador dinâmico com mapeamentos explícitos de metadados para classificações específicas. Isso pode permitir que o administrador ou os usuários distribuam a carga do aplicativo igualmente ou limitem o impacto das solicitações de metadados dos usuários sobre o cluster inteiro.
   </para>
   <para>
    O mecanismo fornecido para essa finalidade é chamado “export pin”. Ele é um atributo estendido de diretórios. O nome desse atributo estendido é <literal>ceph.dir.pin</literal>. Os usuários podem definir esse atributo usando os comandos padrão:
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    O valor (<option>-v</option>) do atributo estendido é a classificação à qual atribuir a subárvore do diretório. O valor padrão -1 indica que o diretório não foi fixado.
   </para>
   <para>
    O export pin de um diretório é herdado do seu pai mais próximo com um export pin definido. Portanto, a definição do export pin em um diretório afeta todos os seus filhos. No entanto, a fixação do pai pode ser anulada pela definição do export pin do diretório filho. Por exemplo:
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Gerenciando o failover</title>

  <para>
   Se um daemon MDS parar de se comunicar com o monitor, o monitor aguardará <option>mds_beacon_grace</option> segundos (o padrão é 15 segundos) antes de marcar o daemon como <emphasis>lento</emphasis>. Você pode configurar um ou mais daemons de “standby” para assumir o controle durante o failover do daemon MDS.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configurando a reprodução de standby</title>
   <para>
    Cada sistema de arquivos CephFS pode ser configurado para adicionar daemons de reprodução de standby. Esses daemons de standby seguem o diário de metadados do MDS ativo para reduzir o tempo de failover caso o MDS ativo se torne indisponível. Cada MDS ativo pode ter apenas um daemon de reprodução de standby o seguindo.
   </para>
   <para>
    Configure a reprodução de standby em um sistema de arquivos com o seguinte comando:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    Quando definidos, os monitores atribuirão os daemons de standby disponíveis para seguir os MDSs ativos nesse sistema de arquivos.
   </para>
   <para>
    Depois que um MDS entrar no estado de reprodução de standby, ele apenas será usado como standby para a classificação que está seguindo. Se houver falha em outra classificação, esse daemon de reprodução de standby não será usado como substituição, mesmo que não haja outros standbys disponíveis. Por esse motivo, se a reprodução de standby for usada, será aconselhável que cada MDS ativo tenha um daemon de reprodução de standby.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Definindo cotas do CephFS</title>

  <para>
   Você pode definir cotas em qualquer subdiretório do sistema de arquivos Ceph. A cota restringe o número de <emphasis role="bold">bytes</emphasis> ou de <emphasis role="bold">arquivos</emphasis> armazenados abaixo do ponto especificado na hierarquia de diretórios.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limitações de cota do CephFS</title>
   <para>
    O uso de cotas com o CephFS tem as seguintes limitações:
   </para>
   <variablelist>
    <varlistentry>
     <term>As cotas são cooperativas e não concorrentes.</term>
     <listitem>
      <para>
       As cotas do Ceph confiam que o cliente que está montando o sistema de arquivos pare de gravar nele quando um limite é atingido. A parte do servidor não pode evitar que um cliente mal intencionado grave a quantidade de dados que ele precisar. Não use cotas para evitar o preenchimento do sistema de arquivos em ambientes em que os clientes não são totalmente confiáveis.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>As cotas não são precisas.</term>
     <listitem>
      <para>
       Os processos que estão gravando no sistema de arquivos serão interrompidos logo após o limite da cota ser atingido. Inevitavelmente, eles poderão gravar alguma quantidade de dados acima do limite configurado. Os gravadores do cliente serão interrompidos dentro de décimos de segundos após ultrapassar o limite configurado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>As cotas são implementadas no cliente do kernel a partir da versão 4.17.</term>
     <listitem>
      <para>
       As cotas são suportadas pelo cliente do espaço de usuário (libcephfs, ceph-fuse). Os clientes do kernel do Linux 4.17 e superiores suportam cotas do CephFS nos clusters do SUSE Enterprise Storage 7. Haverá falha nos clientes do kernel (até nas versões recentes) ao processar cotas em clusters mais antigos, mesmo que eles possam definir os atributos estendidos das cotas. Os kernels do SLE12-SP3 (e versões mais recentes) já incluem os backports necessários para administrar as cotas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configure as cotas com cuidado quando forem usadas com restrições de montagem com base no caminho.</term>
     <listitem>
      <para>
       O cliente precisa ter acesso ao inode do diretório em que as cotas são configuradas para aplicá-las. Se o cliente tiver acesso restrito a um determinado caminho (por exemplo, <filename>/home/user</filename>) com base no recurso do MDS, e se a cota for configurada em um diretório de origem ao qual ele não tem acesso a (<filename>/home</filename>), o cliente não a aplicará. Ao usar restrições de acesso com base no caminho, certifique-se de configurar a cota no diretório que o cliente pode acessar (por exemplo, <filename>/home/user</filename> ou <filename>/home/user/quota_dir</filename>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configurando cotas do CephFS</title>
   <para>
    Você pode configurar cotas do CephFS usando atributos estendidos virtuais:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configura um limite de <emphasis>arquivos</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configura um limite de <emphasis>bytes</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Se os atributos aparecerem no inode de um diretório, uma cota será configurada nele. Se eles não estiverem presentes, nenhuma cota será definida nesse diretório (embora uma ainda possa ser configurada em um diretório pai).
   </para>
   <para>
    Para definir uma cota de 100 MB, execute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Para definir uma cota de 10.000 arquivos, execute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Para ver a configuração de cota, execute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>Cota não definida</title>
    <para>
     Se o valor do atributo estendido for “0”, a cota não será definida.
    </para>
   </note>
   <para>
    Para remover uma cota, execute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Gerenciando instantâneos do CephFS</title>

  <para>
   Os instantâneos do CephFS criam uma visão apenas leitura do sistema de arquivos no momento em que são capturados. Você pode criar um instantâneo em qualquer diretório. O instantâneo incluirá todos os dados no sistema de arquivos abaixo do diretório especificado. Após a criação de um instantâneo, os dados incluídos no buffer serão descarregados dos vários clientes de maneira assíncrona. Dessa forma, a criação de um instantâneo é muito rápida.
  </para>

  <important>
   <title>vários sistemas de arquivos</title>
   <para>
    Se você tiver vários sistemas de arquivos CephFS compartilhando um único pool (por meio de namespaces), seus instantâneos colidirão, e a exclusão de um instantâneo resultará em dados de arquivos ausentes em outros instantâneos que compartilham o mesmo pool.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Criando instantâneos</title>
   <para>
    Por padrão, o recurso de instantâneo do CephFS está habilitado nos sistemas de arquivos novos. Para habilitá-lo nos sistemas de arquivos existentes, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Após a habilitação dos instantâneos, todos os diretórios no CephFS terão um subdiretório <filename>.snap</filename> especial.
   </para>
   <note>
    <para>
     Este é um subdiretório <emphasis>virtual</emphasis>. Ele não aparece na listagem de diretórios do diretório pai, mas o nome <filename>.snap</filename> não pode ser usado como nome de arquivo ou diretório. O diretório <filename>.snap</filename> precisa ser acessado explicitamente, por exemplo:
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>limitação de clientes do kernel</title>
    <para>
     Os clientes do kernel do CephFS têm uma limitação: eles não podem processar mais do que 400 instantâneos em um sistema de arquivos. O número de instantâneos deve ser mantido sempre abaixo desse limite, seja qual for o cliente que você usa. Se você usa clientes CephFS mais antigos, como SLE12-SP3, lembre-se de que ultrapassar 400 instantâneos é prejudicial para as operações, porque haverá falha no cliente.
    </para>
   </important>
   <tip>
    <title>Nome personalizado do subdiretório de instantâneos</title>
    <para>
     Você pode configurar um nome diferente para o subdiretório de instantâneos definindo a configuração <option>client snapdir</option>.
    </para>
   </tip>
   <para>
    Para criar um instantâneo, crie um subdiretório abaixo do diretório <filename>.snap</filename> com um nome personalizado. Por exemplo, para criar um instantâneo do diretório <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, execute:
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Apagando instantâneos</title>
   <para>
    Para apagar um instantâneo, remova seu subdiretório dentro do diretório <filename>.snap</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
