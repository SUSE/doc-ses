<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Tarefas operacionais</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modificando a configuração do cluster</title>

  <para>
   Para modificar a configuração de um cluster do Ceph existente, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Exporte a configuração atual do cluster para um arquivo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Edite o arquivo com a configuração e atualize as linhas relevantes. Encontre exemplos de especificações no <xref linkend="deploy-cephadm-day2"/> e <xref linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Aplique a nova configuração:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Adicionando nós</title>

  <para>
   Para adicionar um novo nó a um cluster do Ceph, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Instale o SUSE Linux Enterprise Server e o SUSE Enterprise Storage no novo host. Consulte o <xref linkend="deploy-os"/> para obter mais informações.
    </para>
   </step>
   <step>
    <para>
     Configure o host como um Minion Salt de um Master Salt existente. Consulte o <xref linkend="deploy-salt"/> para obter mais informações.
    </para>
   </step>
   <step>
    <para>
     Adicione o novo host ao <systemitem class="resource">ceph-salt</systemitem> e informe ao cephadm, por exemplo:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Consulte o <xref linkend="deploy-cephadm-configure-minions"/> para obter mais informações.
    </para>
   </step>
   <step>
    <para>
     Verifique se o nó foi adicionado ao <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Aplique a configuração ao novo host de cluster:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verifique se o host recém-adicionado agora pertence ao ambiente do cephadm:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Removendo nós</title>

  <tip>
   <title>Remover OSDs</title>
   <para>
    Se o nó que você vai remover executa OSDs, remova-os primeiro e verifique se nenhum OSD está sendo executado nesse nó. Consulte <xref linkend="removing-node-osds"/> para obter mais detalhes sobre como remover OSDs.
   </para>
  </tip>

  <para>
   Para remover um nó de um cluster, faça o seguinte:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Para todos os tipos de serviço do Ceph, exceto <literal>node-exporter</literal> e <literal>crash</literal>, remova o nome de host do nó do arquivo de especificação de posicionamento do cluster (por exemplo, <filename>cluster.yml</filename>). Consulte o <xref linkend="cephadm-service-and-placement-specs"/> para obter mais detalhes. Por exemplo, se você remover o host chamado <literal>ses-min2</literal>, remova todas as ocorrências de <literal>- ses-min2</literal> de todas as seções <literal>placement:</literal>
    </para>
    <para>
     Atualização
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     para
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Aplique suas mudanças ao arquivo de configuração:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Remova o nó do ambiente do cephadm:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Se o nó estiver executando os serviços <literal>crash.osd.1</literal> e <literal>crash.osd.2</literal>, remova-os executando o seguinte comando no host:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Por exemplo:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Remova todas as funções do minion que deseja apagar:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Se o minion que você deseja remover for de boot, você também precisará remover a função de boot:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     Após remover todos os OSDs de um único host, remova o host do mapa CRUSH:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      O nome do compartimento de memória deve ser igual ao nome de host.
     </para>
    </note>
   </step>
   <step>
    <para>
     Agora você pode remover o minion do cluster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    Em caso de falha e se o minion que você está tentando remover estiver em um estado permanentemente desligado, você precisará remover o nó do Master Salt:
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    Em seguida, remova o nó de <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>. Normalmente, ele está localizado em <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>Gerenciamento de OSD</title>

  <para>
   Esta seção descreve como adicionar, apagar ou remover OSDs de um cluster do Ceph.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Listando dispositivos de disco</title>
   <para>
    Para identificar os dispositivos de disco usados e não usados em todos os nós do cluster, liste-os executando o seguinte comando:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Apagando dispositivos de disco</title>
   <para>
    Para reutilizar um dispositivo de disco, você precisa apagá-lo (ou <emphasis>zap</emphasis>) primeiro:
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Se você já implantou os OSDs por meio de DriveGroups ou da opção <option>--all-available-devices</option> sem o flag <literal>unmanaged</literal> definido, o cephadm implantará esses OSDs automaticamente depois que você os apagar.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Adicionando OSDs por meio da especificação DriveGroups</title>
   <para>
    Os <emphasis>DriveGroups</emphasis> especificam os layouts dos OSDs no cluster do Ceph. Eles são definidos em um único arquivo YAML. Nesta seção, usaremos <filename>drive_groups.yml</filename> como exemplo.
   </para>
   <para>
    Um administrador deve especificar manualmente um grupo de OSDs que estão interligados (OSDs híbridos implantados em uma combinação de HDDs e SDDs) ou compartilhar opções idênticas de implantação (por exemplo, mesmo armazenamento de objetos, mesma opção de criptografia, OSDs independentes). Para evitar a listagem explícita de dispositivos, os DriveGroups usam uma lista de itens de filtro que correspondem a poucos campos selecionados dos relatórios de inventário do <command>ceph-volume</command>. O cephadm fornecerá o código que converte esses DriveGroups em listas reais de dispositivos para inspeção pelo usuário.
   </para>
   <para>
    O comando para aplicar a especificação de OSD ao cluster é:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Para obter uma visualização das ações e testar seu aplicativo, você pode usar a opção <option>--dry-run</option> junto com o comando <command>ceph orch apply osd</command>. Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Se a saída <option>--dry-run</option> atender às suas expectativas, basta executar novamente o comando sem a opção <option>--dry-run</option>.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>OSDs não gerenciados</title>
    <para>
     Todos os dispositivos de disco limpos disponíveis que correspondem à especificação DriveGroups serão usados como OSDs automaticamente depois que você os adicionar ao cluster. Esse comportamento é chamado de modo <emphasis>gerenciado</emphasis>.
    </para>
    <para>
     Para desabilitar o modo <emphasis>gerenciado</emphasis>, adicione a linha <literal>unmanaged: true</literal> às especificações relevantes, por exemplo:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Para mudar os OSDs já implantados do modo <emphasis>gerenciado</emphasis> para <emphasis>não gerenciado</emphasis>, adicione as linhas <literal>unmanaged: true</literal> aos locais aplicáveis durante o procedimento descrito na <xref linkend="modifying-cluster-configuration"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Especificação DriveGroups</title>
    <para>
     Veja a seguir um exemplo do arquivo de especificação DriveGroups:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         A opção antes chamada de "criptografia" no DeepSea foi renomeada para "criptografada". Ao usar o DriveGroups no SUSE Enterprise Storage 7, adote essa nova terminologia na especificação do serviço; do contrário, haverá falha na operação <command>ceph orch apply</command>.
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Correspondendo dispositivos de disco</title>
    <para>
     Você pode descrever a especificação usando os seguintes filtros:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Por um modelo de disco:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Por um fornecedor de disco:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Insira <replaceable>DISK_VENDOR_STRING</replaceable> sempre em letras minúsculas.
       </para>
      </tip>
      <para>
       Para obter detalhes sobre o modelo e o fornecedor do disco, observe a saída do seguinte comando:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Se um disco é ou não rotacional. Os SSDs e as unidades NVMe não são rotacionais.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Implante um nó usando <emphasis>todas</emphasis> as unidades disponíveis para OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Limite também o número de discos correspondentes:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Filtrando dispositivos por tamanho</title>
    <para>
     Você pode filtrar dispositivos de disco por tamanho, seja por um tamanho exato ou por uma faixa de tamanhos. O parâmetro <option>size:</option> aceita argumentos no seguinte formato:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       “10G”: Inclui discos de um tamanho exato.
      </para>
     </listitem>
     <listitem>
      <para>
       “10G:40G”: Inclui discos cujo tamanho está dentro da faixa.
      </para>
     </listitem>
     <listitem>
      <para>
       “:10G”: Inclui discos menores do que ou iguais a 10 GB.
      </para>
     </listitem>
     <listitem>
      <para>
       “40G:”: Inclui discos iguais ou maiores do que 40 GB.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Correspondendo por tamanho do disco</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>Aspas obrigatórias</title>
     <para>
      Ao usar o delimitador “:”, você precisa colocar o tamanho entre aspas; do contrário, o sinal “:” será interpretado como um novo hash de configuração.
     </para>
    </note>
    <tip>
     <title>Atalhos de unidade</title>
     <para>
      Em vez de Gigabytes (G), você pode especificar os tamanhos em Megabytes (M) ou Terabytes (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Exemplos de DriveGroups</title>
    <para>
     Esta seção inclui exemplos de configurações de OSD diferentes.
    </para>
    <example>
     <title>Configuração simples</title>
     <para>
      Este exemplo descreve dois nós com a mesma configuração:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      O arquivo <filename>drive_groups.yml</filename> correspondente será da seguinte maneira:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Essa configuração é simples e válida. O problema é que um administrador pode adicionar discos de diferentes fornecedores no futuro, e eles não serão incluídos. Você pode melhorá-la reduzindo os filtros nas propriedades de núcleo das unidades:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      No exemplo anterior, impomos todos os dispositivos rotacionais que serão declarados como "dispositivos de dados", e todos os dispositivos não rotacionais serão usados como "dispositivos compartilhados" (wal, db).
     </para>
     <para>
      Se você sabe que as unidades com mais de 2 TB sempre serão os dispositivos de dados mais lentos, pode filtrar por tamanho:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Configuração avançada</title>
     <para>
      Este exemplo descreve duas configurações distintas: 20 HDDs devem compartilhar 2 SSDs, enquanto 10 SSDs devem compartilhar 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Essa configuração pode ser definida com dois layouts da seguinte maneira:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Configuração avançada com nós não uniformes</title>
     <para>
      Os exemplos anteriores consideraram que todos os nós tinham as mesmas unidades. No entanto, esse nem sempre é o caso:
     </para>
     <para>
      Nós 1-5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nós 6-10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Você pode usar a chave de “destino” no layout para direcionar nós específicos. A notação de destino do Salt ajuda a simplificar as coisas:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      seguido de
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Configuração técnica</title>
     <para>
      Todos os casos anteriores consideraram que os WALs e BDs usavam o mesmo dispositivo. No entanto, também é possível implantar o WAL em um dispositivo dedicado:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Configuração complexa (e improvável)</title>
     <para>
      Na configuração a seguir, tentamos definir:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs com 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs com 1 SSD(db) e 1 NVMe(wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs com 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSDs independentes (criptografados)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD é sobressalente e não deve ser implantado
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Veja a seguir o resumo das unidades usadas:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fornecedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamanho: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      A definição dos DriveGroups será a seguinte:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Um HDD permanecerá enquanto o arquivo está sendo analisado de cima para baixo.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Removendo OSDs</title>
   <para>
    Antes de remover um nó OSD do cluster, verifique se o cluster tem mais espaço livre em disco do que o disco OSD que será removido. Saiba que a remoção de um OSD provoca a redistribuição do cluster inteiro.
   </para>
   <procedure>
    <step>
     <para>
      Identifique o OSD que será removido obtendo seu ID:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Remova um ou mais OSDs do cluster:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Por exemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      Você pode consultar o estado da operação de remoção:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Interrompendo a remoção do OSD</title>
    <para>
     Após programar uma remoção do OSD, você poderá interrompê-la, se necessário. O comando a seguir redefinirá o estado inicial do OSD e o removerá da fila:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Substituindo OSDs</title>
   <para>
    Para substituir um OSD mantendo seu ID, execute:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    A substituição de um OSD é idêntica à remoção de um OSD (consulte a <xref linkend="removing-node-osds"/> para obter mais detalhes), exceto que o OSD não é permanentemente removido da hierarquia CRUSH e recebe um flag <literal>destroyed</literal>.
   </para>
   <para>
    O flag <literal>destroyed</literal> é usado para determinados IDs de OSD que serão reutilizados durante a próxima implantação de OSD. Os discos recém-adicionados que corresponderem à especificação DriveGroups (consulte a <xref linkend="drive-groups"/> para obter mais detalhes) receberão os IDs de OSD da contraparte substituída.
   </para>
   <tip>
    <para>
     Anexar a opção <option>--dry-run</option> não executará a substituição real, mas apresentará uma visualização das etapas que normalmente ocorrem.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Movendo o Master Salt para um novo nó</title>

  <para>
   Se você precisar substituir o host Master Salt por um novo, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Exporte a configuração do cluster e faça backup do arquivo JSON exportado. Encontre mais detalhes no <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     Se o Master Salt antigo também for o único nó de administração no cluster, mova manualmente <filename>/etc/ceph/ceph.client.admin.keyring</filename> e <filename>/etc/ceph/ceph.conf</filename> para o novo Master Salt.
    </para>
   </step>
   <step>
    <para>
     Pare e desabilite o serviço <systemitem class="daemon">systemd</systemitem> do Master Salt no nó do Master Salt antigo:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Se o nó do Master Salt antigo não estiver mais no cluster, também pare e desabilite o serviço <systemitem class="daemon">systemd</systemitem> do Minion Salt:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      Não pare nem desabilite o <literal>salt-minion.service</literal> se o nó do Master Salt antigo tiver daemons do Ceph (MON, MGR, OSD, MDS, gateway, monitoramento) em execução.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Instale o SUSE Linux Enterprise Server 15 SP2 no novo Master Salt seguindo o procedimento descrito no <xref linkend="deploy-os"/>.
    </para>
    <tip>
     <title>Transição de Minion Salt</title>
     <para>
      Para simplificar a transição dos Minions Salt para o novo Master Salt, remova a chave pública do Master Salt original de cada um deles:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Instale o pacote <package>salt-master</package> e, se aplicável, o pacote
     <package>salt-minion</package> no novo Master Salt.
    </para>
   </step>
   <step>
    <para>
     Instale o <systemitem class="resource">ceph-salt</systemitem> no novo nó do Master Salt:
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Execute todos os três comandos antes de continuar. Os comandos são idempotentes; não faz diferença se eles são repetidos.
     </para>
    </important>
   </step>
   <step>
    <para>
     Inclua o novo Master Salt no cluster, conforme descrito no <xref linkend="deploy-cephadm-cephsalt"/>, <xref linkend="deploy-cephadm-configure-minions"/> e <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Importe a configuração do cluster de backup e aplique-a:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Renomeie o <literal>minion id</literal> do Master Salt no arquivo <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> exportado antes de importá-lo.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Atualizando os nós do cluster</title>

  <para>
   Mantenha os nós do cluster do Ceph atualizados aplicando as atualizações sequenciais regularmente.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Repositórios do software</title>
   <para>
    Antes de corrigir o cluster com os pacotes de software mais recentes, verifique se todos os nós do cluster têm acesso aos repositórios relevantes. Consulte o <xref linkend="verify-previous-upgrade-patch-repos-repos"/> para obter uma lista completa dos repositórios necessários.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Propagação em fases do repositório</title>
   <para>
    Se você usa uma ferramenta de propagação em fases; por exemplo, SUSE Manager ou Repository Management Tool (RMT), que processa os repositórios do software nos nós do cluster, verifique se as fases dos dois repositórios “Updates” para o SUSE Linux Enterprise Server e o SUSE Enterprise Storage foram criadas no mesmo momento.
   </para>
   <para>
    É altamente recomendável usar uma ferramenta de propagação em fases para aplicar os patches que têm níveis <literal>congelados</literal> ou <literal>em fases</literal>. Isso garante que os novos nós que ingressarem no cluster tenham o mesmo nível de patch que os nós que já estão em execução no cluster. Dessa forma, você não precisa aplicar os patches mais recentes a todos os nós do cluster antes que os novos nós possam ingressar no cluster.
   </para>
  </sect2>

  <sect2>
   <title>Tempo de espera dos serviços do Ceph</title>
   <para>
    Dependendo da configuração, os nós do cluster podem ser reinicializados durante a atualização. Se houver um ponto único de falha para os serviços, como Gateway de Objetos, Samba Gateway, NFS Ganesha ou iSCSI, as máquinas cliente poderão ser temporariamente desconectadas dos serviços cujos nós estão sendo reinicializados.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Executando a atualização</title>
   <para>
    Para atualizar os pacotes de software em todos os nós do cluster para a versão mais recente, execute o seguinte comando:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Atualizando o Ceph</title>

  <para>
   Você pode instruir o cephadm a atualizar o Ceph de uma versão de correção de bug para outra. A atualização automatizada dos serviços do Ceph respeita a ordem recomendada: ela começa com os Ceph Managers, os Ceph Monitors e continua com os outros serviços, como Ceph OSDs, Servidores de Metadados e Gateways de Objetos. Cada daemon será reiniciado apenas depois que o Ceph indicar que o cluster permanecerá disponível.
  </para>

  <note>
   <para>
    O procedimento de atualização a seguir usa o comando <command>ceph orch upgrade</command>. Lembre-se de que as instruções a seguir detalham como atualizar o cluster do Ceph com uma versão do produto (por exemplo, uma atualização de manutenção) e <emphasis>não</emphasis> fornecem instruções sobre como fazer upgrade do cluster de uma versão do produto para outra.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Iniciando a atualização</title>
   <para>
    Antes de iniciar a atualização, verifique se todos os nós estão online e se o cluster está saudável:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    Para atualizar para uma versão específica do Ceph:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    Fazer upgrade de pacotes nos hosts:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Monitorando a atualização</title>
   <para>
    Execute o seguinte comando para determinar se uma atualização está em andamento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Enquanto a atualização estiver em andamento, você verá uma barra de andamento na saída de status do Ceph:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    Você também pode observar o registro do cephadm:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Cancelando uma atualização</title>
   <para>
    Você pode interromper o processo de atualização a qualquer momento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Parando ou reinicializando o cluster</title>

  <para>
   Em alguns casos, talvez seja necessário parar ou reinicializar o cluster inteiro. Recomendamos verificar com cuidado as dependências dos serviços em execução. As seguintes etapas apresentam uma descrição de como parar e iniciar o cluster:
  </para>

  <procedure>
   <step>
    <para>
     Especifique para o cluster do Ceph não marcar os OSDs com o flag “out”:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Pare os daemons e os nós na seguinte ordem:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clientes de armazenamento
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways. Por exemplo, NFS Ganesha ou Gateway de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de Metadados
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Se necessário, execute as tarefas de manutenção.
    </para>
   </step>
   <step>
    <para>
     Inicie os nós e os servidores na ordem inversa do processo de encerramento:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de Metadados
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways. Por exemplo, NFS Ganesha ou Gateway de Objetos
      </para>
     </listitem>
     <listitem>
      <para>
       Clientes de armazenamento
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Remova o flag “noout”:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Removendo um cluster inteiro do Ceph</title>

  <para>
   O comando <command>ceph-salt purge</command> remove todo o cluster do Ceph. Se houver mais clusters do Ceph implantados, será purgado aquele que for relatado pelo <command>ceph -s</command>. Desta forma, você pode limpar o ambiente do cluster ao testar configurações diferentes.
  </para>

  <para>
   Para evitar a exclusão acidental, a orquestração verifica se a segurança está desligada. Você pode desligar as medidas de segurança e remover o cluster do Ceph executando:
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
