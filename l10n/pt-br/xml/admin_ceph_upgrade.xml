<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade de uma versão anterior</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Este capítulo apresenta as etapas para fazer upgrade do SUSE Enterprise Storage 6 para a versão 7.
 </para>
 <para>
  O upgrade inclui as seguintes tarefas:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Fazer upgrade do Ceph Nautilus para o Octopus.
   </para>
  </listitem>
  <listitem>
   <para>
    Alternar da instalação e execução do Ceph por meio de pacotes RPM para a execução em containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Remoção completa do DeepSea e substituição pelo <systemitem class="resource">ceph-salt</systemitem> e cephadm.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   As informações de upgrade neste capítulo são válidas <emphasis>apenas</emphasis> para upgrades do DeepSea para o cephadm. Não tente seguir estas instruções para implantar o SUSE Enterprise Storage na Plataforma SUSE CaaS.
  </para>
 </warning>
 <important>
  <para>
   O upgrade de versões do SUSE Enterprise Storage mais antigas do que a 6 não é suportado. Você precisa primeiro fazer upgrade para a versão mais recente do SUSE Enterprise Storage 6 e, em seguida, seguir as etapas neste capítulo.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Antes de fazer upgrade</title>

  <para>
   As tarefas a seguir <emphasis>devem</emphasis> ser concluídas antes de você iniciar o upgrade. Isso pode ser feito a qualquer momento durante a vida útil do SUSE Enterprise Storage 6.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A migração do OSD do FileStore para o BlueStore <emphasis>deve</emphasis> ser feita antes do upgrade, pois o FileStore não é suportado no SUSE Enterprise Storage 7. Encontre mais detalhes sobre o BlueStore e como migrar do FileStore em <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Se você executa um cluster mais antigo que ainda usa OSDs do <literal>ceph-disk</literal>, <emphasis>precisa</emphasis> alternar para o <literal>ceph-volume</literal> antes do upgrade. Encontre mais detalhes na <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Pontos a serem considerados</title>
   <para>
    Antes de fazer upgrade, leia as seções a seguir para garantir que você entenda todas as tarefas que precisam ser executadas.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Leia os detalhes da versão</emphasis>: onde você encontra mais informações sobre o que mudou desde a versão anterior do SUSE Enterprise Storage. Consulte os detalhes da versão para verificar se:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Seu hardware precisa de considerações especiais.
       </para>
      </listitem>
      <listitem>
       <para>
        Qualquer pacote de software usado foi significativamente modificado.
       </para>
      </listitem>
      <listitem>
       <para>
        São necessárias precauções especiais para a instalação.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Os detalhes da versão também apresentam informações que não puderam ser incluídas a tempo no manual. Eles também incluem notas sobre problemas conhecidos.
     </para>
     <para>
      Você encontra os detalhes da versão do SES 7 online em <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Além disso, após instalar o pacote
      <package>release-notes-ses</package> do repositório SES 7, encontre os detalhes da versão no diretório local <filename>/usr/share/doc/release-notes</filename> ou online em <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Leia o <xref linkend="deploy-cephadm"/> para se familiarizar com o <systemitem class="resource">ceph-salt</systemitem> e o orquestrador do Ceph, especialmente as informações sobre as especificações do serviço.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade do cluster pode levar algum tempo, aproximadamente o tempo necessário para fazer upgrade de uma máquina multiplicado pelo número de nós do cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Você precisa primeiro fazer upgrade do Master Salt e depois substituir o DeepSea pelo <systemitem class="resource">ceph-salt</systemitem> e cephadm. Você <emphasis>não</emphasis> poderá começar a usar o módulo de orquestrador cephadm até que o upgrade de todos os Ceph Managers seja feito.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade do uso de RPMs do Nautilus para containers do Octopus precisa ser feito tudo em uma etapa. Isso significa fazer upgrade de um nó inteiro de uma vez, e não um daemon de cada vez.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade dos serviços básicos (MON, MGR, OSD) é feito de forma ordenada. Cada serviço fica disponível durante o upgrade. Os serviços de gateway (Servidor de Metadados, Gateway de Objetos, NFS Ganesha, iSCSI Gateway) precisarão ser reimplantados após o upgrade dos serviços básicos. Há um determinado tempo de espera para cada um dos seguintes serviços:
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         Os Servidores de Metadados e os Gateways de Objetos ficam inativos a partir do momento em que o upgrade dos nós é feito do SUSE Linux Enterprise Server 15 SP1 para o SUSE Linux Enterprise Server 15 SP2 até a reimplantação dos serviços no final do procedimento de upgrade. É importante ter isso em mente principalmente quando esses serviços estão colocados com MONs, MGRs ou OSDs, porque, neste caso, eles podem ficar inativos durante todo o período de upgrade do cluster. Se isso for um problema, considere a implantação desses serviços separadamente em nós adicionais antes do upgrade, para que eles fiquem inativos pelo menor tempo possível. Essa é a duração do upgrade dos nós de gateway, não a duração do upgrade de todo o cluster.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        O NFS Ganesha e os iSCSI Gateways ficam inativos apenas no período de reinicialização dos nós durante o upgrade do SUSE Linux Enterprise Server 15 SP1 para o SUSE Linux Enterprise Server 15 SP2, e rapidamente depois quando cada serviço é reimplantado no modo de container.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Fazendo backup da configuração e dos dados do cluster</title>
   <para>
    É altamente recomendável fazer backup de todas as configurações e dados do cluster antes de iniciar o upgrade para o SUSE Enterprise Storage 7. Para obter instruções sobre como fazer backup de todos os seus dados, consulte <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verificando etapas do upgrade anterior</title>
   <para>
    Se você fez upgrade da versão 5, verifique se o upgrade para a versão 6 foi concluído com êxito:
   </para>
   <para>
    Verifique se existe o arquivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
   </para>
   <para>
    Esse arquivo é criado pelo processo de integração durante o upgrade do SUSE Enterprise Storage 5 para 6. A opção <option>configuration_init: default-import</option> é definida em <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    Se <option>configuration_init</option> ainda está definido como <option>default-import</option>, o cluster usa <filename>ceph.conf.import</filename> como arquivo de configuração, e não o <filename>ceph.conf</filename> padrão do DeepSea, que é compilado dos arquivos em <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Portanto, você precisa inspecionar se há qualquer configuração personalizada em <filename>ceph.conf.import</filename> e, possivelmente, mover a configuração para um dos arquivos em <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Em seguida, remova a linha <option>configuration_init: default-import</option> de <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Atualizando os nós e verificando a saúde do cluster</title>
   <para>
    Verifique se todas as atualizações mais recentes do SUSE Linux Enterprise Server 15 SP1 e do SUSE Enterprise Storage 6 foram aplicadas a todos os nós do cluster:
   </para>
<screen><prompt role="root">root # </prompt>zypper refresh &amp;&amp; zypper patch</screen>
   <para>
    Após a aplicação das atualizações, verifique a saúde do cluster:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verificando o acesso a repositórios do software e imagens de container</title>
   <para>
    Verifique se cada nó do cluster tem acesso aos repositórios do software SUSE Linux Enterprise Server 15 SP2 e SUSE Enterprise Storage 7 e também ao registro de imagens de container.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Repositórios do software</title>
    <para>
     Se todos os nós foram registrados no SCC, você poderá usar o comando <command>zypper migration</command> para fazer upgrade. Consulte <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/> para obter mais detalhes.
    </para>
    <para>
     Se os nós <emphasis role="bold">não</emphasis> foram registrados no SCC, desabilite todos os repositórios existentes do software e adicione os dois repositórios <literal>Pool</literal> e <literal>Updates</literal> para cada uma das seguintes extensões:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Imagens de container</title>
    <para>
     Todos os nós do cluster precisam acessar o registro de imagens de container. Na maioria dos casos, você usará o registro público do SUSE em <literal>registry.suse.com</literal>. Você precisa das seguintes imagens:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Por exemplo, para implantações isoladas (air-gapped), uma alternativa é configurar um registro local e verificar se você tem o conjunto correto de imagens de container disponível. Consulte a <xref linkend="deploy-cephadm-configure-registry"/> para obter mais detalhes sobre como configurar um registro de imagens de container local.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Fazendo upgrade do Master Salt</title>

  <para>
   O procedimento a seguir descreve o processo de upgrade do Master Salt:
  </para>

  <procedure>
   <step>
    <para>
     Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP2:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Para um cluster em que todos os nós foram registrados no SCC, execute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Para um cluster em que os nós têm repositórios de software atribuídos manualmente, execute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Desabilite as fases do DeepSea para evitar o uso acidental. Adicione o seguinte conteúdo a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Grave o arquivo e aplique as mudanças:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     Se você <emphasis role="bold">não</emphasis> usa imagens de container do <literal>registry.suse.com</literal>, mas sim o registro configurado localmente, edite o <filename>/srv/pillar/ceph/stack/global.yml</filename> para especificar a imagem de container e o registro do Ceph que o DeepSea usará. Por exemplo, para usar <literal>192.168.121.1:5000/my/ceph/image</literal>, adicione as seguintes linhas:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Grave o arquivo e aplique as mudanças:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Observe a configuração existente:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verifique o status do upgrade. A saída pode ser diferente dependendo da configuração do seu cluster:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Fazendo upgrade dos nós MON, MGR e OSD</title>

  <para>
   Faça upgrade do Ceph Monitor, do Ceph Manager e dos nós OSD, um de cada vez. Para cada serviço, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Se o nó do qual você está fazendo upgrade é um OSD, execute o comando a seguir para evitar marcá-lo como <literal>out</literal>:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Substitua <replaceable>SHORT_NODE_NAME</replaceable> pelo nome abreviado do nó da forma como ele aparece na saída do comando <command>ceph osd tree</command>. Na entrada a seguir, os nomes abreviados de host são <literal>ses-min1</literal> e <literal>ses-min2</literal>
    </para>
<screen>
<prompt>root@master # </prompt>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP2:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Se todos os nós do cluster foram registrados no SCC, execute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Se os nós do cluster tiveram repositórios de software atribuídos manualmente, execute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Após a reinicialização do nó, coloque em contêiner todos os daemons MON, MGR e OSD existentes nesse nó executando o seguinte comando no Master Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Substitua <replaceable>MINION_ID</replaceable> pelo ID do minion do qual você está fazendo upgrade. Você pode gerar uma lista de IDs de minions ao executar o comando <command>salt-key -L</command> no Master Salt.
    </para>
    <tip>
     <para>
      Para ver o status e o andamento da <emphasis>adoção</emphasis>, verifique o Ceph Dashboard ou execute um dos seguintes comandos no Master Salt:
     </para>
<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     Após a conclusão bem-sucedida da adoção, cancele a definição do flag <literal>noout</literal> se o nó do qual você estiver fazendo upgrade for OSD:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Fazendo upgrade de nós de gateway</title>
  <para>
   Na sequência, faça upgrade dos nós de gateway separados (Servidor de Metadados, Gateway de Objetos, NFS Ganesha ou iSCSI Gateway). Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP2 para cada nó:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Se todos os nós do cluster foram registrados no SUSE Customer Center, execute o comando <command>zypper migration</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Se os nós do cluster tiveram repositórios de software atribuídos manualmente, execute o comando <command>zypper dup</command> seguido do comando <command>reboot</command>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Esta etapa também vale para qualquer nó que faça parte do cluster, mas que ainda não recebeu nenhuma função (em caso de dúvida, verifique a lista de hosts no Master Salt gerada pelo comando <command>salt-key -L</command> e compare-a com a saída do comando <command>salt-run upgrade.status</command>).
  </para>
  <para>
   Após o upgrade do OS em todos os nós do cluster, a próxima etapa é instalar o pacote <package>ceph-salt</package> e aplicar a configuração do cluster. Os serviços de gateway reais são reimplantados no modo em container no final do procedimento de upgrade.
  </para>
  <note>
   <para>
    Os serviços Servidor de Metadados e Gateway de Objetos ficam indisponíveis a partir do momento do upgrade para o SUSE Linux Enterprise Server 15 SP2 até serem reimplantados no final do procedimento de upgrade.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Instalando o <systemitem class="resource">ceph-salt</systemitem> e aplicando a configuração do cluster</title>

  <para>
   Antes de iniciar o procedimento de instalação do <systemitem class="resource">ceph-salt</systemitem> e aplicar a configuração do cluster, verifique o status do cluster e do upgrade executando os seguintes comandos:
  </para>

<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Remova os Crons <literal>rbd_exporter</literal> e <literal>rgw_exporter</literal> criados pelo DeepSea. No Master Salt, execute o comando <command>crontab -e</command> como <systemitem class="username">root</systemitem> para editar o crontab. Apague os seguintes itens, se houver:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null
</screen>
   </step>
   <step>
    <para>
     Exporte a configuração do cluster do DeepSea executando os seguintes comandos:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<prompt>root@master # </prompt>salt-run upgrade.generate_service_specs &gt; specs.yaml
</screen>
   </step>
   <step>
    <para>
     Desinstale o DeepSea e instale o <systemitem class="resource">ceph-salt</systemitem> no Master Salt:
    </para>
<screen>
<prompt>root@master # </prompt>zypper remove 'deepsea*'
<prompt>root@master # </prompt>zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Reinicie o Master Salt e sincronize os módulos do Salt:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Importe a configuração do cluster do DeepSea para <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Gere as chaves SSH para comunicação do nó do cluster:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verifique se a configuração do cluster foi importada do DeepSea e especifique as opções que possam ter sido perdidas:
     </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
     <para>
      Para ver uma descrição completa da configuração do cluster, consulte a <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Aplique a configuração e habilite o cephadm:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </step>
   <step>
    <para>
     Se for necessário inserir o URL de registro do container local e as credenciais de acesso, siga as etapas descritas na <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     Se você <emphasis role="bold">não</emphasis> usa imagens de container do <literal>registry.suse.com</literal>, mas usa o registro configurado localmente, informe a imagem de container que o Ceph usará executando o comando:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Pare e desabilite os daemons <systemitem class="daemon">ceph-crash</systemitem> do SUSE Enterprise Storage 6. Novos formulários em container desses daemons são iniciados automaticamente mais tarde.
    </para>
<screen>
<prompt>root@master # </prompt>salt '*' service.stop ceph-crash
<prompt>root@master # </prompt>salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Fazendo upgrade e adotando a pilha de monitoramento</title>

  <para>
   O procedimento a seguir adota todos os componentes da pilha de monitoramento (consulte o <xref linkend="monitoring-alerting"/> para obter mais detalhes).
  </para>

  <procedure>
   <step>
    <para>
     Pause o orquestrador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch pause</screen>
   </step>
   <step>
    <para>
     Em qualquer nó que esteja executando o Prometheus, o Grafana e o Alertmanager (por padrão, o Master Salt), execute os seguintes comandos:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      Se você <emphasis role="bold">não</emphasis> executa o registro de imagem de container padrão <literal>registry.suse.com</literal>, precisa especificar a imagem que será usada, por exemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-server:2.18.0 \
  adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-alertmanager:0.16.2 \
  adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.0.3 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      Para obter mais detalhes sobre como usar imagens de container personalizadas ou locais, consulte o <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Remova o Node-Exporter. Ele não precisa ser migrado e será reinstalado como um container quando o arquivo <filename>specs.yaml</filename> for aplicado.
    </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> zypper rm golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Aplique as especificações de serviço que você já exportou do DeepSea:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i specs.yaml</screen>
   </step>
   <step>
    <para>
     Continue o orquestrador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Reimplantação do serviço de gateway</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Fazendo upgrade do Gateway de Objetos</title>
   <para>
    No SUSE Enterprise Storage 7, os Gateways de Objetos são sempre configurados com um domínio, o que permite multissite no futuro (consulte o <xref linkend="ceph-rgw-fed"/> para obter mais detalhes). Se você usou uma configuração de site único do Gateway de Objetos no SUSE Enterprise Storage 6, siga estas etapas para adicionar um domínio. Se você não planeja de fato usar a funcionalidade multissite, pode usar o <literal>padrão</literal> para os nomes de domínio, grupo de zonas e zona.
   </para>
   <procedure>
    <step>
     <para>
      Crie um novo domínio:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Opcionalmente, renomeie a zona padrão e o grupo de zonas.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure o grupo de zonas master:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure a zona master. Para isso, você precisará da ACCESS_KEY e da SECRET_KEY de um usuário do Gateway de Objetos com o flag <option>system</option> habilitado. O usuário costuma ser o <literal>admin</literal>. Para obter a ACCESS_KEY e a SECRET_KEY, execute <command>radosgw-admin user info --uid admin</command>.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Confirme a configuração atualizada:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    Para colocar o serviço Gateway de Objetos em container, crie o respectivo arquivo de especificação conforme descrito na <xref linkend="deploy-cephadm-day2-service-ogw"/> e aplique-o.
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Fazendo upgrade do NFS Ganesha</title>
   <para>
    Veja a seguir como migrar um serviço NFS Ganesha existente que executa o Ceph Nautilus para um container do NFS Ganesha que executa o Ceph Octopus.
   </para>
   <warning>
    <para>
     A documentação a seguir requer que você já tenha feito upgrade dos serviços básicos do Ceph.
    </para>
   </warning>
   <para>
    O NFS Ganesha armazena a configuração adicional por daemon e a exporta para um pool RADOS. O pool RADOS configurado pode ser encontrado na linha <literal>watch_url</literal> do bloco <literal>RADOS_URLS</literal> no arquivo <filename>ganesha.conf</filename>. Por padrão, esse pool será denominado <literal>ganesha_config</literal>.
   </para>
   <para>
    Antes de tentar qualquer migração, é altamente recomendável fazer uma cópia dos objetos de configuração de exportação e daemon localizados no pool RADOS. Para localizar o pool RADOS configurado, execute o seguinte comando:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    Para listar o conteúdo do pool RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    Para copiar os objetos RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>OBJS=$(rados $RADOS_ARGS ls)
<prompt>cephuser@adm &gt; </prompt>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    Para cada nó, um serviço NFS Ganesha existente precisa ser interrompido e, em seguida, substituído por um container gerenciado pelo cephadm.
   </para>
   <procedure>
    <step>
     <para>
      Pare e desabilite o serviço NFS Ganesha existente:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>systemctl stop nfs-ganesha
<prompt>cephuser@adm &gt; </prompt>systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      Depois que o serviço NFS Ganesha existente for interrompido, um novo serviço poderá ser implantado em um container usando o cephadm. Para fazer isso, você precisa criar uma especificação de serviço que contenha um <literal>service_id</literal>, que será usado para identificar esse novo cluster do NFS, o nome de host do nó que estamos migrando listado como host na especificação de posicionamento e o pool RADOS e o namespace que contêm os objetos de exportação do NFS configurados. Por exemplo:
     </para>
     <screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      Para obter mais informações sobre como criar uma especificação de posicionamento, consulte a <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Aplique a especificação de posicionamento:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirme se o daemon do NFS Ganesha está em execução no host:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Repita essas etapas para cada nó do NFS Ganesha. Você não precisa criar uma especificação de serviço separada para cada nó. É suficiente adicionar o nome de host de cada nó à especificação do serviço NFS existente e reaplicá-la.
     </para>
    </step>
   </procedure>
   <para>
    É possível migrar as exportações existentes de duas maneiras diferentes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Recriação ou reatribuição manual usando o Ceph Dashboard.
     </para>
    </listitem>
    <listitem>
     <para>
      Cópia manual do conteúdo de cada objeto RADOS por daemon para a configuração comum recém-criada do NFS Ganesha.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Copiando manualmente as exportações para o arquivo de configuração comum do NFS Ganesha</title>
    <step>
     <para>
      Determine a lista de objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Faça uma cópia dos objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Classifique e faça a fusão em uma única lista de exportações:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat conf-* | sort -u &gt; conf-nfs.<replaceable>SERVICE_ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Grave o novo arquivo de configuração comum do NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Notifique o daemon do NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       Essa ação fará com que o daemon recarregue a configuração.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    Após a migração bem-sucedida do serviço, o serviço NFS Ganesha baseado no Nautilus poderá ser removido.
   </para>
   <procedure>
    <step>
     <para>
      Remova o NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remova as configurações do cluster legado do Ceph Dashboard:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Fazendo upgrade do servidor de metadados</title>
   <para>
    Ao contrário dos MONs, MGRs e OSDs, o Servidor de Metadados não pode ser adotado no local. Em vez disso, você precisa reimplantá-lo em containers usando o orquestrador do Ceph.
   </para>
   <procedure>
    <step>
     <para>
      Execute o comando <command>ceph fs ls</command> para obter o nome do seu sistema de arquivos, por exemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Crie um novo arquivo de especificação de serviço <filename>mds.yml</filename>, conforme descrito na <xref linkend="deploy-cephadm-day2-service-mds"/>, usando o nome do sistema de arquivos como o <option>service_id</option> e especificando os hosts que executarão os daemons MDS. Por exemplo:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Execute o comando <command>ceph orch apply -i mds.yml</command> para aplicar a especificação de serviço e iniciar os daemons MDS.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Fazendo upgrade do iSCSI Gateway</title>
   <para>
    Para fazer upgrade do iSCSI Gateway, é necessário reimplantá-lo em containers usando o orquestrador do Ceph. Se você tiver vários iSCSI Gateways, precisará reimplantá-los um por um para reduzir o tempo de espera do serviço.
   </para>
   <procedure>
    <step>
     <para>
      Pare e desabilite os daemons iSCSI existentes em cada nó do iSCSI Gateway:
     </para>
<screen>
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-api
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Crie uma especificação de serviço para o iSCSI Gateway conforme descrito na <xref linkend="deploy-cephadm-day2-service-igw"/>. Para isso, você precisa das configurações <option>pool</option>, <option>trusted_ip_list</option> e <option>api_*</option> do arquivo <filename>/etc/ceph/iscsi-gateway.cfg</filename> existente. Se você tem o suporte a SSL habilitado (<literal>api_secure = true</literal>), precisa também do certificado SSL (<filename>/etc/ceph/iscsi-gateway.crt</filename>) e da chave (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      Por exemplo, se <filename>/etc/ceph/iscsi-gateway.cfg</filename> contém o seguinte:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Você precisa criar o seguinte arquivo de especificação de serviço <filename>iscsi.yml</filename>:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       As configurações <option>pool</option>, <option>trusted_ip_list</option>, <option>api_port</option>, <option>api_user</option>, <option>api_password</option> e <option>api_secure</option> são idênticas às do arquivo <filename>/etc/ceph/iscsi-gateway.cfg</filename>. É possível copiar os valores <option>ssl_cert</option> e <option>ssl_key</option> dos arquivos de chave e de certificado SSL existentes. Verifique se eles estão indentados corretamente e se o caractere <emphasis>barra vertical</emphasis> <literal>|</literal> aparece no final das linhas <literal>ssl_cert:</literal> e <literal>ssl_key:</literal> (veja o conteúdo do arquivo <filename>iscsi.yml</filename> acima).
      </para>
     </note>
    </step>
    <step>
     <para>
      Execute o comando <command>ceph orch apply -i iscsi.yml</command> para aplicar a especificação de serviço e iniciar os daemons do iSCSI Gateway.
     </para>
    </step>
    <step>
     <para>
      Remova o pacote <package>ceph-iscsi</package> antigo de cada um dos nós existentes do iSCSI Gateway:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Limpeza após o upgrade</title>

  <para>
   Após o upgrade, execute as seguintes etapas de limpeza:
  </para>

  <procedure>
   <step>
    <para>
     Verifique a versão atual do Ceph para conferir se o upgrade do cluster foi bem-sucedido:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph versions</screen>
   </step>
   <step>
    <para>
     Garanta que nenhum OSD antigo ingresse no cluster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd require-osd-release octopus</screen>
   </step>
   <step>
    <para>
     Habilite o módulo de dimensionador automático:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable pg_autoscaler</screen>
    <important>
     <para>
      Por padrão, os pools no SUSE Enterprise Storage 6 tinham o <option>pg_autoscale_mode</option> definido como <option>warn</option>. Isso resultava em uma mensagem de aviso em caso de número de PGs abaixo do ideal, mas o dimensionamento automático não era feito. O padrão no SUSE Enterprise Storage 7 é a opção <option>pg_autoscale_mode</option> definida como <option>on</option> para que os novos pools e PGs sejam dimensionados automaticamente. O processo de upgrade não muda automaticamente o <option>pg_autoscale_mode</option> dos pools existentes. Para mudá-lo para <option>on</option> e aproveitar todos os benefícios do dimensionador automático, consulte as instruções no <xref linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Encontre mais detalhes no <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Impedir clientes pré-Luminous:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Habilite o módulo de balanceador:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph balancer mode upmap
<prompt>cephuser@adm &gt; </prompt>ceph balancer on
</screen>
    <para>
     Encontre mais detalhes no <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Se preferir, habilite o módulo de telemetria:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph mgr module enable telemetry
<prompt>cephuser@adm &gt; </prompt>ceph telemetry on
 </screen>
    <para>
     Encontre mais detalhes no <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
