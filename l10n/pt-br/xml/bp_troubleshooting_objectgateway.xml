<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="bp_troubleshooting_objectgateway.xml" version="5.0" xml:id="bp-troubleshooting-objectgateway">
 <title>Troubleshooting Object Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="troubleshooting-gateway-healthcheck">
  <title>Running a basic health check</title>

  <para>
   The most basic health check to test a running Object Gateway process is to simply
   point your browser or client at the Object Gateway endpoint. This should return an
   empty bucket results for the anonymous user:
  </para>

<screen>
<prompt>root@master # </prompt>curl -I
</screen>

  <para>
   If the result returns a 405 status, this indicates
   <literal>MethodNotAllowed</literal>. The output will result in a
   <literal>NoSuchBucket</literal> XML file. If the result returns a 404
   status, this means the Object Gateway DNS name is misconfigured or the requests were
   not made at an endpoint resolving to Object Gateway DNS name.
  </para>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-notrunning">
  <title>Identifying gateway issues</title>

  <para>
   If the gateway is not running, usually restartinng the gateway (automatic
   under <systemitem class="daemon">systemd</systemitem>) should restore the service. This can be achieved via:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch daemon restart <replaceable>rgw-daemon-name</replaceable>
</screen>

  <para>
   For increasing the log level under the
   <literal>client.<replaceable>RGW_NAME</replaceable></literal> section, the
   Object Gateway configurable can be increased from 1 (default) up to 20 (very
   verbose). For the messages sent to the cluster itself the messenger debug
   levels can be raised (off by default). This is controlled via <option>debug
   ms</option>. Usually a level of 1 is sufficient to gather enough
   information. For injecting arguments in a running process:
  </para>

  <note>
   <para>
    This is a temporary change.
   </para>
  </note>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph-daemon <replaceable>CLIENT_RGW_NAME</replaceable> config set debug_rgw 20
</screen>

  <para>
   For persistent changes, or a non-running process, it is best to set these
   lines in the <filename>ceph.conf</filename> file under the
   <literal>client.<replaceable>rgw-name</replaceable></literal> section or,
   alternatively, using the Ceph CLI. In this case, the debug levels are
   extremely verbose, so it is best to do this only to capture error logs for a
   short time. To set the logs using the Ceph CLI, run:
  </para>

<screen>
   <prompt>cephuser@adm &gt; </prompt>ceph config set <replaceable>CLIENT_RGW_NAME</replaceable> debug_rgw 20
   </screen>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-crashed-rrgw">
  <title>Diagnosing crashed Object Gateway process</title>

  <para>
   If the Object Gateway process dies, you will normally see a connection refused at the
   client. In that situation, restarting Object Gateway will restore the service.
  </para>

  <para>
   To diagnose the cause of the crash, check the log in
   <filename>/var/log/ceph</filename> or the core file (if one was generated).
  </para>
 </sect1>
 <sect1 xml:id="troubleshooting-gateway-blocked">
  <title>Identifying blocked Object Gateway requests</title>

  <para>
   If some (or all) Object Gateway requests appear to be blocked, you can get some
   insight into the internal state of the Object Gateway daemon via its admin socket. By
   default, there will be a socket configured to reside in
   <filename>/var/run/ceph</filename>, and the daemon can be queried with:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph daemon /var/run/ceph/client.<replaceable>rgw-name</replaceable> help
  help                list available commands
  objecter_requests   show in-progress osd requests
  perfcounters_dump   dump perfcounters value
  perfcounters_schema dump perfcounters schema
  version             get protocol version
</screen>

  <para>
   Of particular interest:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph daemon /var/run/ceph/client.rgw objecter_requests
...
</screen>

  <para>
   This will dump information about current in-progress requests with the
   RADOS cluster. This allows you to identify if any requests are blocked by
   a non-responsive OSD. For example, one might see:
  </para>

<screen>
{ "ops": [
      { "tid": 1858,
        "pg": "2.d2041a48",
        "osd": 1,
        "last_sent": "2012-03-08 14:56:37.949872",
        "attempts": 1,
        "object_id": "fatty_25647_object1857",
        "object_locator": "@2",
        "snapid": "head",
        "snap_context": "0=[]",
        "mtime": "2012-03-08 14:56:37.949813",
        "osd_ops": [
              "write 0~4096"]},
      { "tid": 1873,
        "pg": "2.695e9f8e",
        "osd": 1,
        "last_sent": "2012-03-08 14:56:37.970615",
        "attempts": 1,
        "object_id": "fatty_25647_object1872",
        "object_locator": "@2",
        "snapid": "head",
        "snap_context": "0=[]",
        "mtime": "2012-03-08 14:56:37.970555",
        "osd_ops": [
              "write 0~4096"]}],
"linger_ops": [],
"pool_ops": [],
"pool_stat_ops": [],
"statfs_ops": []}
</screen>

  <para>
   In this dump, two requests are in progress. The <literal>last_sent</literal>
   field is the time the RADOS request was sent. If this is a while ago, it
   suggests that the OSD is not responding. For example, for request 1858, you
   could check the OSD status with:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg map 2.d2041a48
osdmap e9 pg 2.d2041a48 (2.0) -&gt; up [1,0] acting [1,0]
</screen>

  <para>
   This tells you to look at <literal>osd.1</literal>, the primary copy for
   this PG:
  </para>

<screen>
  ceph daemon osd.1 ops
  { "num_ops": 651,
   "ops": [
         { "description": "osd_op(client.4124.0:1858 fatty_25647_object1857 [write 0~4096] 2.d2041a48)",
           "received_at": "1331247573.344650",
           "age": "25.606449",
           "flag_point": "waiting for sub ops",
           "client_info": { "client": "client.4124",
               "tid": 1858}},
  ...
</screen>

  <para>
   The <literal>flag_point</literal> field indicates that the OSD is currently
   waiting for replicas to respond, in this case <literal>osd.0</literal>.
  </para>
 </sect1>
 <sect1 xml:id="large-omap-issues">
  <title>Large OMAP issues</title>

  <para>
   If you receive a cluster warning for a large OMAP issue, it means that an
   object has exceeded one of the OSD deep scrub large OMAP object thresholds,
   usually set to 200,000 keys and 1 GiB of storage. The relevant pool or PG
   involved can be found by grepping for <emphasis>large OMAP
   object</emphasis>, as mentioned in the warning message. Depending on the
   pool and the object key involved it might indicate one of the following:
  </para>

  <sect2 xml:id="resharding-issues">
   <title>Resharding issues</title>
   <para>
    If the large OMAP object name happens to be in the
    <literal><replaceable>zone</replaceable>.rgw.buckets.index</literal> pool,
    this means that a bucket has more than 200,000 keys. Dynamic resharding is
    set on by default in single site clusters which would automatically have
    resharded the bucket. For multi-site clusters, however, this is not
    supported and would have to be manually done
    <xref linkend="ogw-bucket-reshard"/>. Looking at the bucket statistics
    reveals the total object count and the number of shards. This is helpful to
    understand if the bucket has already been resharded, in which case the
    warning is just because the deep scrub happened to run before the reshard
    process and would be cleared in the next deep scrub.
   </para>
  </sect2>

  <sect2 xml:id="usage-statistics">
   <title>Reading usage statistics</title>
   <para>
    Usage statistics are also stored as OMAP keys in the
    <literal><replaceable>zone</replaceable>.rgw.log</literal> pool. However,
    these are not automatically trimmed, therefore a manual trimming of usage
    statistics would have to be done. For example:
   </para>
<screen>
<prompt role="root">root # </prompt>radosgw-admin usage trim [--uid=<replaceable>user-id</replaceable>] --start-date=2019-01-01 --end-date=2019-03-03
</screen>
  </sect2>
 </sect1>
</chapter>
