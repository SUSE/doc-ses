<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>RADOS Block Device</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  ブロックとは連続するバイトのことで、たとえば4MBブロックのデータなどです。ブロックベースのストレージインタフェースは、ハードディスク、CD、フロッピーディスクなどの回転型媒体にデータを保存する最も一般的な方法です。Block Deviceインタフェースはあらゆるところで利用されているため、仮想ブロックデバイスは、Cephのような大容量データストレージシステムを操作するための理想的な候補です。
 </para>
 <para>
  Ceph Block Deviceは物理リソースを共有でき、サイズの変更が可能です。データはCephクラスタ内の複数のOSD上にストライプされて保存されます。Ceph Block Deviceは、スナップショットの作成、レプリケーション、整合性などのRADOSの機能を利用します。CephのRBD (RADOS Block Device)は、カーネルモジュールまたは<systemitem>librbd</systemitem>ライブラリを使用してOSDと対話します。
 </para>
 <figure>
  <title>RADOSプロトコル</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Cephのブロックデバイスは、高いパフォーマンスと無限のスケーラビリティをカーネルモジュールに提供します。これらは、QEMUなどの仮想化ソリューションや、OpenStackなど、<systemitem class="library">libvirt</systemitem>に依存するクラウドベースのコンピューティングシステムをサポートします。同じクラスタを使用して、Object Gateway、CephFS、およびRADOS Block Deviceを同時に運用できます。
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Block Deviceのコマンド</title>

  <para>
   <command>rbd</command>コマンドを使用して、Block Deviceイメージを作成、一覧、イントロスペクト、および削除できます。さらに、イメージのクローン作成、スナップショットの作成、スナップショットへのイメージのロールバック、スナップショットの表示などの操作にも使用できます。
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>複製プールでのBlock Deviceイメージの作成</title>
   <para>
    Block Deviceをクライアントに追加する前に、既存のプール内に、関連するイメージを作成する必要があります(<xref linkend="ceph-pools"/>を参照)。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    たとえば、「mypool」という名前のプールに情報を保存する「myimage」という名前の1GBのイメージを作成するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>イメージサイズの単位</title>
    <para>
     サイズの単位のショートカット(「G」または「T」)を省略した場合、イメージのサイズはメガバイト単位になります。ギガバイトまたはテラバイトを指定するには、サイズの数字の後に「G」または「T」を使用します。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>イレージャコーディングプールでのBlock Deviceイメージの作成</title>
   <para>
    Block DeviceイメージのデータをEC (イレージャコーディング)プールに直接保存できます。RADOS Block Deviceイメージは、<emphasis/>「データ」部分と<emphasis/>「メタデータ」部分で構成されます。ECプールには、RADOS Block Deviceイメージの「データ」部分のみを保存できます。プールは<option>overwrite</option>フラグが<emphasis>true</emphasis>に設定されている必要があります。このように設定できるのは、プールが保存されているすべてのOSDがBlueStoreを使用している場合のみです。
   </para>
   <para>
    ECプールにイメージの「メタデータ」の部分を保存することはできません。<command>rbd create</command>コマンドの<option>--pool=</option>オプションを使用してイメージのメタデータを保存する複製プールを指定するか、イメージ名のプレフィックスとして<option>pool/</option>を指定できます。
   </para>
   <para>
    ECプールを作成します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    メタデータを保存する複製プールを指定します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    または:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Block Deviceイメージの一覧</title>
   <para>
    「mypool」という名前のプール内のBlock Deviceを一覧にするには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>イメージ情報の取得</title>
   <para>
    「mypool」という名前のプール内のイメージ「myimage」から情報を取得するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Block Deviceイメージのサイズの変更</title>
   <para>
    RADOS Block Deviceイメージはシンプロビジョニングされます。つまり、そこにデータを保存し始めるまでは、実際に物理ストレージを使用しません。ただし、<option>--size</option>オプションで設定する最大容量があります。イメージの最大サイズを増やす(または減らす)場合、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Block Deviceイメージの削除</title>
   <para>
    「mypool」という名前のプール内にあるイメージ「myimage」に対応するBlock Deviceを削除するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>マウントとアンマウント</title>

  <para>
   RADOS Block Deviceを作成した後は、他のディスクデバイスと同じように使用できます。デバイスをフォーマットし、マウントしてファイルを交換できるようにし、完了したらアンマウントできます。
  </para>

  <para>
   デフォルトでは、<command>rbd</command>コマンドはCephの<literal>admin</literal>ユーザアカウントを使用してクラスタにアクセスします。このアカウントはクラスタに対する完全な管理アクセス権限を持ちます。そのため、Linuxワークステーションに<systemitem class="username">root</systemitem>でログインした場合と同様に、誤って損害を発生させてしまうリスクが発生します。したがって、特権を制限したユーザアカウントを作成して、RADOS Block Deviceの通常の読み込み/書き込みアクセスに使用することが望ましいです。
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Cephユーザアカウントの作成</title>
   <para>
    Ceph Manager、Ceph Monitor、Ceph OSDのケーパビリティを使用して新しいユーザアカウントを作成するには、<command>ceph</command>コマンドと<command>auth get-or-create</command>サブコマンドを使用します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    たとえば、<replaceable>vms</replaceable>プールへの読み込み/書き込みアクセスと、<replaceable>images</replaceable>プールへの読み込み専用アクセスができる、<replaceable>qemu</replaceable>という名前のユーザを作成するには、次のコマンドを実行します。
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    <command>ceph auth get-or-create</command>コマンドは特定のユーザ用のキーリングを出力します。また、キーリングを<filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>に書き込むことができます。
   </para>
   <note>
    <para>
     <command>rbd</command>コマンドを使用する場合、オプションの<command>--id</command>
     <replaceable>ID</replaceable>引数を与えることでユーザIDを指定できます。
    </para>
   </note>
   <para>
    Cephユーザアカウントの管理の詳細については、<xref linkend="cha-storage-cephx"/>を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>ユーザ認証</title>
   <para>
    ユーザ名を指定するには、<option>--id <replaceable>user-name</replaceable></option>を使用します。<systemitem>cephx</systemitem>認証を使用する場合は、秘密を指定する必要もあります。秘密は、キーリング、または秘密が含まれるファイルから取得できます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    あるいは、
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>RADOS Block Deviceを使用するための準備</title>
   <procedure>
    <step>
     <para>
      Cephクラスタに、マップするディスクイメージが存在するプールが含まれることを確認します。プールは<literal>mypool</literal>、イメージは<literal>myimage</literal>という名前であると想定します。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      イメージを新しいBlock Deviceにマップします。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      すべてのマップ済みデバイスを一覧にします。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      作業対象のデバイスは<filename>/dev/rbd0</filename>です。
     </para>
     <tip>
      <title>RBDデバイスのパス</title>
      <para>
       <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>の代わりに、永続的なデバイスパスとして<filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename>を使用できます。例:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      <filename>/dev/rbd0</filename>デバイス上にXFSファイルシステムを作成します。
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      使用するマウントポイントに<filename>/mnt</filename>を置き換えてから、デバイスをマウントして正しくマウントされたかを確認します。
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      これで、ローカルディレクトリと同じように、このデバイスとの間でデータを移動できます。
     </para>
     <tip>
      <title>RBDデバイスのサイズを増やす</title>
      <para>
       RBDデバイスのサイズが十分ではなくなった場合、簡単にサイズを増やすことができます。
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         RBDイメージのサイズを、たとえば10GBに増やします。
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         デバイスの新しいサイズ全体を使用するようファイルシステムを拡張します。
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      デバイスへのアクセスが終わったら、デバイスをマップ解除してアンマウントできます。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>手動によるマウントとアンマウント</title>
    <para>
     ブート後にRBDのマッピングとマウントを行い、シャットダウン前にRBDをアンマウントするプロセスをスムーズにするため、<command>rbdmap</command>スクリプトと<systemitem class="daemon">systemd</systemitem>ユニットが提供されています。<xref linkend="ceph-rbd-rbdmap"/>を参照してください。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command>: ブート時のRBDデバイスのマッピング</title>
   <para>
    <command>rbdmap</command>は、1つ以上のRBDイメージに対する<command>rbd map</command>および<command>rbd device unmap</command>の操作を自動化するシェルスクリプトです。このスクリプトはいつでも手動で実行できますが、ブート時にRBDイメージを自動的にマップしてマウント(シャットダウン時にはアンマウントしてマップ解除)するのが主な利点です。これはInitシステムによってトリガされます。このために、<systemitem>ceph-common</systemitem>パッケージに<systemitem class="daemon">systemd</systemitem>のユニットファイルである<filename>rbdmap.service</filename>が含まれています。
   </para>
   <para>
    このスクリプトは引数を1つ取り、<option>map</option>または<option>unmap</option>のどちらかを指定できます。どちらの場合も、スクリプトは設定ファイルを解析します。デフォルトは<filename>/etc/ceph/rbdmap</filename>ですが、環境変数<literal>RBDMAPFILE</literal>で上書きできます。設定ファイルの各行が、マップまたはマップ解除する1つのRBDイメージに対応します。
   </para>
   <para>
    構成ファイルは次のような形式になっています。
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       プール内のイメージのパス。<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>として指定します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       基礎となる<command>rbd device map</command>コマンドに渡されるパラメータのオプションのリスト。これらのパラメータとその値をコンマ区切り文字列として指定する必要があります。次に例を示します。
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       次の例では、<command>rbdmap</command>スクリプトで次のコマンドを実行します。
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       次の例では、ユーザ名とキーリングを対応する秘密とともに指定する方法を確認できます。
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <command>rbdmap map</command>として実行すると、設定ファイルを解析し、指定されているRBDイメージそれぞれに対して、最初にイメージをマップし(<command>rbd device map</command>を使用)、次にイメージをマウントしようと試みます。
   </para>
   <para>
    <command>rbdmap unmap</command>として実行すると、設定ファイルに一覧にされているイメージがアンマウントされてマップ解除されます。
   </para>
   <para>
    <command>rbdmap unmap-all</command>は、設定ファイルに一覧にされているかどうかに関係なく、現在マップされているRBDイメージをすべてアンマウントし、その後マップ解除しようと試みます。
   </para>
   <para>
    成功した場合、イメージは<command>rbd device map</command>操作によって<filename>/dev/rbdX</filename>デバイスにマップされます。この時点でudevルールがトリガされ、実際にマップされたデバイスを指すフレンドリデバイス名のシンボリックリンク<filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename>が作成されます。
   </para>
   <para>
    正常にマウントおよびアンマウントするには、<filename>/etc/fstab</filename>に「フレンドリ」デバイス名に対応するエントリが必要です。RBDイメージの<filename>/etc/fstab</filename>エントリを記述する場合、「noauto」(または「nofail」)マウントオプションを指定します。<filename>rbdmap.service</filename>は一般的にブートシーケンスのかなり遅い段階でトリガされるため、このオプションを指定することによって、Initシステムが、対象デバイスがまだ存在しない早すぎるタイミングでデバイスをマウントしないようにします。
   </para>
   <para>
    <command>rbd</command>オプションの完全なリストについては、<command>rbd</command>のマニュアルページ(<command>man 8 rbd</command>)を参照してください。
   </para>
   <para>
    <command>rbdmap</command>の使用法の例については、<command>rbdmap</command>のマニュアルページ(<command>man 8 rbdmap</command>)を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>RBDデバイスのサイズを増やす</title>
   <para>
    RBDデバイスのサイズが十分ではなくなった場合、簡単にサイズを増やすことができます。
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      RBDイメージのサイズを、たとえば10GBに増やします。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      デバイスの新しいサイズ全体を使用するようファイルシステムを拡張します。
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>スナップショット</title>

  <para>
   RBDのスナップショットは、RADOS Block Deviceイメージのスナップショットです。スナップショットにより、イメージの状態の履歴を保持します。Cephはスナップショットの階層化もサポートしており、VMイメージのクローンを素早く簡単に作成できます。<command>rbd</command>コマンド、およびさまざまな高レベルのインタフェース(QEMU、<systemitem>libvirt</systemitem>、OpenStack、CloudStackなど)を使用したBlock Deviceのスナップショットをサポートしています。
  </para>

  <note>
   <para>
    イメージのスナップショットを作成する前に、入出力操作を停止し、保留中の書き込みをすべてフラッシュする必要があります。イメージにファイルシステムが含まれる場合、スナップショットの作成時に、そのファイルシステムが整合性のある状態である必要があります。
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title><systemitem>cephx</systemitem>の有効化と設定</title>
   <para>
    <systemitem>cephx</systemitem>が有効な場合、ユーザ名またはIDと、そのユーザに対応する鍵が含まれるキーリングのパスを指定する必要があります。詳しくは「<xref linkend="cha-storage-cephx"/>」を参照してください。以降のパラメータを再入力せずに済むよう、<systemitem>CEPH_ARGS</systemitem>環境変数を追加することもできます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    例:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     <systemitem>CEPH_ARGS</systemitem>環境変数にユーザと秘密を追加して、毎回入力しなくて済むようにします。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>スナップショットの基本</title>
   <para>
    次の手順では、コマンドラインで<command>rbd</command>を使用して、スナップショットを作成、一覧、および削除する方法を説明します。
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>スナップショットの作成</title>
    <para>
     <command>rbd</command>を使用してスナップショットを作成するには、<option>snap create</option>オプション、プール名、およびイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>スナップショットの一覧</title>
    <para>
     イメージのスナップショットを一覧にするには、プール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>スナップショットのロールバック</title>
    <para>
     <command>rbd</command>を使用して特定のスナップショットにロールバックするには、<option>snap rollback</option>オプション、プール名、イメージ名、およびスナップショット名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      イメージをスナップショットにロールバックすることは、イメージの現在のバージョンをスナップショットのデータで上書きすることを意味します。ロールバックの実行にかかる時間は、イメージのサイズに応じて長くなります。イメージをスナップショットに「ロールバック」<emphasis/>するよりもスナップショットから「クローンを作成する方が高速」<emphasis/>であり、以前の状態に戻す場合はこの方法をお勧めします。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>スナップショットの削除</title>
    <para>
     <command>rbd</command>を使用してスナップショットを削除するには、<option>snap rm</option>オプション、プール名、イメージ名、およびユーザ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Ceph OSDはデータを非同期で削除するので、スナップショットを削除してもディスク領域はすぐには解放されません。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>スナップショットの消去</title>
    <para>
     <command>rbd</command>を使用してイメージのすべてのスナップショットを削除するには、<option>snap purge</option>オプションとイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>スナップショットの階層化</title>
   <para>
    Cephでは、Block DeviceスナップショットのCOW (コピーオンライト)クローンを複数作成できます。スナップショットの階層化により、Ceph Block Deviceのクライアントはイメージを非常に素早く作成できます。たとえば、Linux VMが書き込まれたBlock Deviceイメージを作成してから、そのイメージのスナップショットを作成し、スナップショットを保護して、コピーオンライトクローンを必要な数だけ作成できます。スナップショットは読み込み専用なので、スナップショットのクローンを作成することでセマンティクスが簡素化され、クローンを素早く作成できます。
   </para>
   <note>
    <para>
     次のコマンドラインの例で使われている「親」および「子」という用語は、Ceph Block Deviceのスナップショット(親)と、そのスナップショットから作成された対応するクローンイメージ(子)を意味します。
    </para>
   </note>
   <para>
    クローンイメージ(子)にはその親イメージへの参照が保存されており、これによってクローンイメージから親のスナップショットを開いて読み込むことができます。
   </para>
   <para>
    スナップショットのCOWクローンは、他のCeph Block Deviceイメージとまったく同じように動作します。クローンイメージに対して読み書きを行ったり、クローンを作成したり、サイズを変更したりできます。クローンイメージに特別な制約はありません。ただし、スナップショットのコピーオンライトクローンはスナップショットを参照するので、クローンを作成する前に「必ず」<emphasis/>スナップショットを保護する必要があります。
   </para>
   <note>
    <title><option>--image-format 1</option>はサポートされない</title>
    <para>
     非推奨の<command>rbd create --image-format 1</command>オプションを使用して作成されたイメージのスナップショットを作成することはできません。Cephでサポートされているのは、<emphasis/>デフォルトの「format 2」のイメージのクローン作成のみです。
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>階層化の基本事項</title>
    <para>
     Ceph Block Deviceの階層化は簡単なプロセスです。まずイメージを用意する必要があります。続いて、イメージのスナップショットを作成し、スナップショットを保護する必要があります。これらの手順を実行した後、スナップショットのクローンの作成を開始できます。
    </para>
    <para>
     クローンイメージは親スナップショットへの参照を持ち、プールID、イメージID、およびスナップショットIDを含みます。プールIDが含まれることは、あるプールから別のプール内のイメージへスナップショットのクローンを作成できることを意味します。
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       「イメージテンプレート」<emphasis/>: Block Deviceの階層化の一般的な使用事例は、マスタイメージと、クローンのテンプレートとして機能するスナップショットを作成することです。たとえば、Linux配布パッケージ(たとえば、SUSE Linux Enterprise Server)のイメージを作成して、そのスナップショットを作成できます。定期的にイメージを更新して新しいスナップショットを作成できます(たとえば、<command>zypper ref &amp;&amp; zypper patch</command>の後に<command>rbd snap create</command>を実行します)。イメージが完成したら、いずれかのスナップショットのクローンを作成できます。
      </para>
     </listitem>
     <listitem>
      <para>
       「拡張テンプレート」<emphasis/>: より高度な使用事例として、ベースイメージより多くの情報を提供するテンプレートイメージを拡張することがあります。たとえば、イメージ(VMテンプレート)のクローンを作成して、他のソフトウェア(たとえば、データベース、コンテンツ管理システム、分析システム)をインストールしてから、拡張イメージのスナップショットを作成でき、このスナップショットそのものをベースイメージと同じ方法で更新できます。
      </para>
     </listitem>
     <listitem>
      <para>
       「テンプレートプール」<emphasis/>: Block Deviceの階層化を使用する方法の1つが、テンプレートとして機能するマスタイメージと、それらのテンプレートの各スナップショットが含まれるプールを作成することです。その後、読み込み専用特権をユーザに拡張し、プール内での書き込みまたは実行の能力を持たなくても、スナップショットのクローンを作成できるようにします。
      </para>
     </listitem>
     <listitem>
      <para>
       「イメージのマイグレーション/回復」<emphasis/>: Block Deviceの階層化を使用する方法の1つが、あるプールから別のプールへデータを移行または回復することです。
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>スナップショットの保護</title>
    <para>
     クローンは親スナップショットにアクセスします。ユーザが誤って親スナップショットを削除すると、すべてのクローンが壊れます。データの損失を防ぐため、クローンを作成する前に、スナップショットを保護する必要があります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      保護されたスナップショットは削除できません。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>スナップショットのクローンの作成</title>
    <para>
     スナップショットのクローンを作成するには、親プール、イメージ、スナップショット、子プール、およびイメージ名を指定する必要があります。クローンを作成する前に、スナップショットを保護する必要があります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      あるプールから別のプール内のイメージへスナップショットのクローンを作成できます。たとえば、一方のプール内に読み込み専用のイメージとスナップショットをテンプレートとして維持しておき、別のプール内に書き込み可能クローンを維持できます。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>スナップショットの保護の解除</title>
    <para>
     スナップショットを削除するには、まず保護を解除する必要があります。また、クローンから参照されているスナップショットは削除「できません」<emphasis/>。スナップショットを削除する前に、スナップショットの各クローンをフラット化する必要があります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>スナップショットの子の一覧</title>
    <para>
     スナップショットの子を一覧にするには、次のコマンドを実行します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>クローンイメージのフラット化</title>
    <para>
     クローンイメージは親スナップショットへの参照を保持しています。子クローンから親スナップショットへの参照を削除する場合、スナップショットからクローンへ情報をコピーすることによって効果的にイメージを「フラット化」します。クローンのフラット化にかかる時間は、スナップショットのサイズに応じて長くなります。スナップショットを削除するには、まず子イメージをフラット化する必要があります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      フラット化されたイメージにはスナップショットからの情報がすべて含まれるため、階層化されたクローンよりも多くのストレージ領域を使用します。
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>RBDイメージのミラーリング</title>

  <para>
   RBDイメージを2つのCephクラスタ間で非同期でミラーリングできます。この機能には2つのモードがあります。
  </para>

  <variablelist>
   <varlistentry>
    <term>ジャーナルベース</term>
    <listitem>
     <para>
      このモードは、RBDイメージのジャーナリング機能を使用して、クラスタ間でクラッシュコンシステントなレプリケーションを保証します。RBDイメージに対する書き込みが発生すると、まず関連するジャーナルに記録されてから、イメージが実際に変更されます。<literal>remote</literal>クラスタはジャーナルを読み込み、イメージのローカルコピーに更新内容を再現します。RBDイメージのジャーナリング機能を使用すると、RBDイメージに1回書き込むたびに2回の書き込みが発生するため、書き込みに伴う遅延が約2倍となることが見込まれます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>スナップショットベース</term>
    <listitem>
     <para>
      このモードは、RBDイメージのミラースナップショットを使用して、クラッシュコンシステントなRBDイメージをクラスタ間で複製します。このミラースナップショットはスケジュールに沿って定期的に作成するか、手動で作成します。<literal>remote</literal>クラスタは2つのミラースナップショットの間でなんらかのデータかメタデータが更新されているかを判定し、イメージのローカルコピーに差分をコピーします。RBDのfast-diffイメージ機能のおかげで、更新されたデータブロックは素早く計算できます。RBDイメージ全体をスキャンする必要はありません。このモードには時間的な整合性がないため、フェールオーバーシナリオの際に利用するには事前にスナップショットの差分全体を同期する必要があります。部分的に適用されたスナップショット差分については、使用する前に最新の完全に同期されたスナップショットまでロールバックします。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   ミラーリングは、ピアクラスタ内のプールごとに設定します。ジャーナルベースのミラーリングだけを使用している場合、プール内の特定のイメージサブセットに設定することも、プール内のすべてのイメージを自動的にミラーリングするように設定することもできます。ミラーリングは<command>rbd</command>コマンドを使用して設定します。<systemitem class="daemon">rbd-mirror</systemitem>デーモンは、<literal>remote</literal>のピアクラスタからイメージの更新を取得して、<literal>local</literal>クラスタ内のイメージに適用する処理を受け持ちます。
  </para>

  <para>
   レプリケーションに対する要望に応じて、RBDミラーリングは単方向レプリケーション用または双方向レプリケーション用に設定できます。
  </para>

  <variablelist>
   <varlistentry>
    <term>単方向レプリケーション</term>
    <listitem>
     <para>
      データがプライマリクラスタからセカンダリクラスタにミラーリングされるだけであれば、<systemitem class="daemon">rbd-mirror</systemitem>デーモンはセカンダリクラスタ上でのみ実行されます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>双方向レプリケーション</term>
    <listitem>
     <para>
      データが、あるクラスタのプライマリイメージから別のクラスタの非プライマリイメージにミラーリングされる場合(逆も同様)、<systemitem class="daemon">rbd-mirror</systemitem>デーモンは両方のクラスタで実行されます。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    <systemitem class="daemon">rbd-mirror</systemitem>デーモンの各インスタンスは、<literal>local</literal> Cephクラスタと<literal>remote</literal> Cephクラスタへ同時に接続できる必要があります。たとえば、すべてのMonitorホストとOSDホストに接続できる必要があります。さらに、ミラーリングのワークロードを扱うため、ネットワークの2つのデータセンター間には十分な帯域幅が必要です。
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>プールの設定</title>
   <para>
    次の手順では、<command>rbd</command>コマンドを使用してミラーリングを設定するための基本的な管理タスクを実行する方法を説明します。ミラーリングは、Cephクラスタ内のプールごとに設定します。
   </para>
   <para>
    これらのプール設定手順は、両方のピアクラスタで実行する必要があります。これらの手順では、わかりやすくするため、<literal>local</literal>および<literal>remote</literal>という名前の2つのクラスタが1つのホストからアクセス可能であることを想定しています。
   </para>
   <para>
    異なるCephクラスタに接続する方法の詳細については、<command>rbd</command>のマニュアルページ(<command>man 8 rbd</command>)を参照してください。
   </para>
   <tip>
    <title>複数のクラスタ</title>
    <para>
     以下の例におけるクラスタ名は、同名のCeph設定ファイルである<filename>/etc/ceph/remote.conf</filename>と、同名のCephキーリングファイルである<filename>/etc/ceph/remote.client.admin.keyring</filename>に対応しています。
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>プールのミラーリングの有効化</title>
    <para>
     プールのミラーリングを有効にするには、<command>mirror pool enable</command>サブコマンド、プール名、およびミラーリングモードを指定します。ミラーリングモードはpoolまたはimageにすることができます。
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        ジャーナリング機能が有効な、プール内のすべてのイメージをミラーリングします。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        各イメージに対して明示的にミラーリングを有効にする必要があります。詳細については、<xref linkend="rbd-mirror-enable-image-mirroring"/>を参照してください。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>ミラーリングの無効化</title>
    <para>
     プールのミラーリングを無効にするには、<command>mirror pool disable</command>サブコマンドとプール名を指定します。この方法でプールのミラーリングを無効にした場合、ミラーリングを明示的に有効にしたイメージ(プール内)のミラーリングも無効になります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>ピアのブートストラップ処理</title>
    <para>
     <systemitem class="daemon">rbd-mirror</systemitem>デーモンがピアクラスタを検出するためには、ピアをプールに登録し、ユーザアカウントを作成する必要があります。このプロセスは<command>rbd</command>とともに<command>mirror pool peer bootstrap create</command>と<command>mirror pool peer bootstrap import</command>のコマンドを使用することで自動化できます。
    </para>
    <para>
     <command>rbd</command>を使用して新しいブートストラップトークンを手動で作成するには、<command>mirror pool peer bootstrap create</command>コマンドとプール名に加えて、<literal>local</literal>クラスタを記述するためにオプションのフレンドリサイト名を指定します。
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     <command>mirror pool peer bootstrap create</command>コマンドの出力はトークンです。このトークンを<command>mirror pool peer bootstrap import</command>コマンドに提供する必要があります。たとえば、<literal>local</literal>クラスタで次のコマンドを実行します。
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     <command>rbd</command>コマンドにより他のクラスタが作成したブートストラップトークンを手動でインポートするには、次の構文を使用します。
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     ここで、
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        <literal>local</literal>クラスタを記述するための、オプションのフレンドリサイト名。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        ミラーリング方向。デフォルトは双方向ミラーリングを表す<literal>rx-tx</literal>に設定されていますが、単方向ミラーリングを表す<literal>rx-only</literal>に設定することもできます。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        プールの名前。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        作成されたトークンへのファイルパス(標準入力から読み込む場合は、<literal>-</literal>)。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     たとえば、<literal>remote</literal>クラスタで次のコマンドを実行します。
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>クラスタピアの手動追加</title>
    <para>
     <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>に記載したピアのブートストラップ方法の代わりに、手動でピアを指定することもできます。ミラーリングを実行するには、リモートの<systemitem class="daemon">rbd-mirror</systemitem>デーモンがローカルクラスタにアクセスする必要があります。リモートの<systemitem class="daemon">rbd-mirror</systemitem>デーモンが使用する新しいローカルCephユーザを作成します。次の例では<literal>rbd-mirror-peer</literal>です。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     <command>rbd</command>コマンドにより、ミラーリングピアのCephクラスタを追加するには、次の構文を使用します。
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     例:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     デフォルトでは、<systemitem class="daemon">rbd-mirror</systemitem>デーモンが、<filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename>に置かれたCeph設定ファイルにアクセスできる必要があります。この設定ファイルにはピアクラスタのMONのIPアドレスと、デフォルトまたはカスタムのキーリング検索パスに配置された<replaceable>CLIENT_NAME</replaceable>という名前のクライアント用キーリングが含まれます。キーリング検索パスの例は、<filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>などです。
    </para>
    <para>
     もしくは、ピアクラスタのMONとクライアントキーの両方または一方を、ローカルのCeph設定キーストアに安全に保存することもできます。ミラーリングピアを追加する際にピアクラスタの接続属性を指定するには、<option>--remote-mon-host</option>オプションと<option>--remote-key-file</option>オプションを使用します。例:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>クラスタピアの削除</title>
    <para>
     ミラーリングピアクラスタを削除するには、<command>mirror pool peer remove</command>サブコマンド、プール名、およびピアのUUID (<command>rbd mirror pool info</command>コマンドで参照可能)を指定します。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>データプール</title>
    <para>
     宛先クラスタにイメージを作成する場合、<systemitem class="daemon">rbd-mirror</systemitem>は次のようにデータプールを選択します。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       宛先クラスタにデフォルトのデータプールが設定されている場合は、そのプールを使用します(設定には、<option>rbd_default_data_pool</option>設定オプションを使用します)。
      </para>
     </listitem>
     <listitem>
      <para>
       デフォルトのデータプールが設定されていない場合、ソースイメージが別のデータプールを使用しており、宛先クラスタに同じ名前のプールが存在するなら、そのプールを使用します。
      </para>
     </listitem>
     <listitem>
      <para>
       これら2つの条件が満たされない場合、データプールは設定されません。
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>RBDイメージの設定</title>
   <para>
    プール設定と異なり、イメージ設定はミラーリングピアの1つのCephクラスタのみで実行する必要があります。
   </para>
   <para>
    ミラーリングされたRBDイメージは、「プライマリ」<emphasis/>または「非プライマリ」<emphasis/>のいずれかとして指定されます。これはイメージのプロパティであり、プールのプロパティではありません。非プライマリとして指定されたイメージは変更できません。
   </para>
   <para>
    イメージに対して初めてミラーリングを有効にすると、イメージは自動的にプライマリに昇格します(プールのミラーモードが「pool」で、イメージのジャーナリング機能が有効な場合、ミラーリングは暗黙的に有効になります。または、<command>rbd</command>コマンドによって明示的に有効にします(<xref linkend="rbd-mirror-enable-image-mirroring"/>を参照してください)。
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>イメージミラーリングの有効化</title>
    <para>
     ミラーリングが<literal>image</literal>モードで設定されている場合、プール内の各イメージに対して明示的にミラーリングを有効にする必要があります。<command>rbd</command>コマンドを使用して特定のイメージのミラーリングを有効にするには、<command>mirror image enable</command>サブコマンドと共にプール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     イメージのミラーリングモードは、<literal>journal</literal>または<literal>snapshot</literal>のどちらかを使用できます。
    </para>
    <variablelist>
     <varlistentry>
      <term>journal(デフォルト)</term>
      <listitem>
       <para>
        <literal>journal</literal>モードに設定した場合、ミラーリング処理にRBDイメージのジャーナリング機能を使用して、イメージ内容の複製を行います。イメージに対するRBDイメージのジャーナリング機能が有効化されていなかった場合、自動的に有効化されます。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>スナップショット</term>
      <listitem>
       <para>
        <literal>snapshot</literal>モードに設定した場合、ミラーリング処理にRBDイメージのミラースナップショット機能を使用して、イメージ内容の複製を行います。有効化した場合、最初のミラースナップショットが自動的に作成されます。RBDイメージのミラースナップショットを<command>rbd</command>コマンドにより追加で作成することもできます。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>イメージジャーナリング機能の有効化</title>
    <para>
     RBDのミラーリングは、RBDのジャーナリング機能を使用して、複製イメージが常にクラッシュコンシステントな状態を保つようにします。<literal>image</literal>ミラーリングモードを使用する場合、イメージのミラーリングを有効にするとジャーナリング機能が自動的に有効になります。<literal>pool</literal>ミラーリングモードを使用する場合、イメージをピアクラスタにミラーリングするには、RBDイメージジャーナリング機能を有効にする必要があります。この機能は、イメージの作成時に<command>rbd</command>コマンドで<option>--image-feature exclusive-lock,journaling</option>オプションを指定することによって有効にできます。
    </para>
    <para>
     または、既存のRBDイメージに対して動的にジャーナリング機能を有効にすることもできます。ジャーナリングを有効にするには、<command>feature enable</command>サブコマンド、プール名、イメージ名、および機能名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>オプションの依存関係</title>
     <para>
      <option>journaling</option>機能は<option>exclusive-lock</option>機能に依存します。<option>exclusive-lock</option>機能がまだ有効になっていない場合は、有効にしてから<option>journaling</option>機能を有効にする必要があります。
     </para>
    </note>
    <tip>
     <para>
      デフォルトですべての新規イメージのジャーナリングを有効にできます。Ceph設定ファイルに<option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option>を追加してください。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>イメージのミラースナップショットの作成</title>
    <para>
     スナップショットベースのミラーリングを使用する場合、RBDイメージの変更内容のミラーリングが必要になるたびに、ミラースナップショットを作成する必要があります。<command>rbd</command>コマンドを使用してミラースナップショットを手動で作成するには、<command>mirror image snapshot</command>コマンドと共に、プール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     例:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     デフォルトでイメージごとに作成されるミラースナップショットは3つだけです。上限に達した場合は、最新のミラースナップショットが自動的に削除されます。<option>rbd_mirroring_max_mirroring_snapshots</option>設定オプションにより、必要に応じて上限を上書きできます。また、イメージが削除された場合やミラーリングが無効化された場合には、ミラースナップショットが自動的に削除されます。
    </para>
    <para>
     ミラースナップショットのスケジュールを定義することで、定期的にミラースナップショットを自動作成することも可能です。ミラースナップショットのスケジュールは、グローバル、プールごと、イメージごとのレベルで設定できます。どのレベルにも複数のミラースナップショットのスケジュールを設定できます。ただし、個別のミラーイメージに適用される最も詳細なスナップショットスケジュールだけが実行されます。
    </para>
    <para>
     <command>rbd</command>コマンドを使用してミラースナップショットスケジュールを作成するには、<command>mirror snapshot schedule add</command>コマンドと、プール名またはイメージ名(オプション)、スナップショット間隔、開始時間(オプション)を指定します。
    </para>
    <para>
     スナップショット間隔はサフィックス<option>d</option>、<option>h</option>、<option>m</option>を使用することでそれぞれ日、時、分の単位で指定できます。必要に応じて、開始時間をISO 8601日時フォーマットにより指定できます。次に例を示します。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     <command>rbd</command>コマンドを使用してミラースナップショットスケジュールを削除するには、<command>mirror snapshot schedule remove</command>コマンドと、対応するスケジュールの追加コマンドに一致するオプションを指定します。
    </para>
    <para>
     <command>rbd</command>コマンドを使用して特定のレベル(グローバル、プール、イメージ)のすべてのスナップショットスケジュールを一覧にするには、<command>mirror snapshot schedule ls</command>コマンドと、必要に応じてプール名またはイメージ名を指定します。また、<option>--recursive</option>オプションを指定すると、指定したレベル以下のすべてのスケジュールを一覧にできます。次に例を示します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     <command>rbd</command>コマンドを使用して、スナップショットベースのミラーリングRBDイメージが次はいつ作成されるかを確認するには、<command>mirror snapshot schedule status</command>コマンドとプール名またはイメージ名(オプション)を指定します。次に例を示します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>イメージミラーリングの無効化</title>
    <para>
     特定のイメージのミラーリングを無効にするには、<command>mirror image disable</command>サブコマンドと共にプール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>イメージの昇格と降格</title>
    <para>
     プライマリ指定をピアクラスタ内のイメージに移動する必要があるフェールオーバーシナリオの場合、プライマリイメージへのアクセスを停止し、現在のプライマリイメージを降格してから、新しいプライマリイメージを昇格し、代替クラスタ上のイメージへのアクセスを再開する必要があります。
    </para>
    <note>
     <title>強制昇格</title>
     <para>
      <option>--force</option>オプションを使用して昇格を強制できます。強制昇格は、降格をピアクラスタに伝搬できない場合(たとえば、クラスタ障害や通信停止が発生した場合)に必要です。この結果、2つのピア間でスプリットブレインシナリオが発生し、<command>resync</command>サブコマンドを発行するまでイメージは同期されなくなります。
     </para>
    </note>
    <para>
     特定のイメージを非プライマリに降格するには、<command>mirror image demote</command>サブコマンドと共にプール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     プール内のすべてのプライマリイメージを非プライマリに降格するには、<command>mirror pool demote</command>サブコマンドと共にプール名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     特定のイメージをプライマリに昇格するには、<command>mirror image promote</command>サブコマンドと共にプール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     プール内のすべての非プライマリイメージをプライマリに昇格するには、<command>mirror pool promote</command>サブコマンドと共にプール名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>I/O負荷の分割</title>
     <para>
      プライマリまたは非プライマリの状態はイメージごとなので、2つのクラスタでI/O負荷を分割したり、フェールオーバーまたはフェールバックを実行したりできます。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>イメージの再同期の強制</title>
    <para>
     <systemitem class="daemon">rbd-mirror</systemitem>デーモンがスプリットブレインイベントを検出した場合、このデーモンは、イベントが修正されるまで、影響を受けるイメージのミラーリングを試行しません。イメージのミラーリングを再開するには、まず、古いと判定されたイメージを降格してから、プライマリイメージへの再同期を要求します。イメージの再同期を要求するには、<command>mirror image resync</command>サブコマンドと共にプール名とイメージ名を指定します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>ミラーリング状態の確認</title>
   <para>
    ピアクラスタのレプリケーションの状態は、ミラーリングされたすべてのプライマリイメージについて保存されます。この状態は、<command>mirror image status</command>および<command>mirror pool status</command>の各サブコマンドを使用して取得できます。
   </para>
   <para>
    ミラーイメージの状態を要求するには、<command>mirror image status</command>サブコマンドと共にプール名とイメージ名を指定します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    ミラープールのサマリ状態を要求するには、<command>mirror pool status</command>サブコマンドと共にプール名を指定します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     <command>mirror pool status</command>サブコマンドに<option>--verbose</option>オプションを追加すると、プール内にあるすべてのミラーリングイメージについて状態の詳細も出力されます。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>キャッシュの設定</title>

  <para>
   Ceph Block Device (<systemitem>librbd</systemitem>)のユーザスペースの実装では、Linuxページキャッシュを利用できません。したがって、Ceph Block Deviceには独自のインメモリキャッシングが含まれます。RBDのキャッシュはハードディスクキャッシュと同様に動作します。OSがバリア要求またはフラッシュ要求を送信すると、すべての「ダーティ」データがOSDに書き込まれます。つまり、ライトバックキャッシュを使用することは、フラッシュを適切に送信するVMで正常に動作する物理ハードディスクを使用することと同様に安全です。キャッシュは<emphasis/>LRU (「Least Rec ntly Used」)アルゴリズムを使用しており、ライトバックモードでは、隣接する要求をマージしてスループットを向上させることができます。
  </para>

  <para>
   Cephは、RBDのライトバックキャッシュをサポートしています。RBDのライトバックキャッシュを有効にするには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   デフォルトでは、<systemitem>librbd</systemitem>はキャッシュを実行しません。書き込みと読み込みはストレージクラスタに直接送信され、書き込みはデータがすべてのレプリカのディスク上にある場合にのみ返されます。キャッシュを有効にすると、<option>rbd cache max dirty</option>オプションで設定されている値より多くの未フラッシュバイトがある場合を除いて、書き込みはすぐに返されます。このような場合、書き込みはライトバックをトリガし、十分なバイトがフラッシュされるまでブロックされます。
  </para>

  <para>
   CephはRBDのライトスルーキャッシュをサポートします。キャッシュのサイズを設定したり、ターゲットと制限を設定して、ライトバックキャッシュからライトスルーキャッシュに切り替えたりすることができます。ライトスルーモードを有効にするには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   つまり、書き込みはデータがすべてのレプリカのディスク上にある場合にのみ返されますが、読み込みはキャッシュから行われる場合があります。キャッシュはクライアントのメモリ内にあり、各RBDイメージは専用のキャッシュを持ちます。キャッシュはクライアントに対してローカルであるため、イメージにアクセスする他のユーザがいる場合、整合性はありません。キャッシュが有効な場合、RBDに加えてGFSまたはOCFSを実行することはできません。
  </para>

  <para>
   以下のパラメータがRADOS Block Deviceの動作に影響します。これらのパラメータを設定するには、<literal>client</literal>カテゴリを使用します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      RBD (RADOS Block Device)のキャッシュを有効にします。デフォルトは「true」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      RBDキャッシュのサイズ(バイト単位)。デフォルトは32MBです。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      キャッシュがライトバックをトリガする「ダーティ」の制限(バイト単位)。<option>rbd cache max dirty</option>は、<option>rbd cache size</option>より小さくする必要があります。0に設定すると、ライトスルーキャッシュを使用します。デフォルトは24MBです。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      キャッシュがデータをデータストレージに書き込み始めるまでの「ダーティターゲット」。キャッシュへの書き込みはブロックしません。デフォルトは16MBです。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      ライトバックの開始前にダーティデータがキャッシュ内に存在する秒数。デフォルトは1です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      ライトスルーモードで開始し、最初のフラッシュ要求を受信したらライトバックに切り替えます。<systemitem>rbd</systemitem>で実行されている仮想マシンが古すぎてフラッシュを送信できない場合(たとえば、カーネル2.6.32より前のLinuxのvirtioドライバ)は、この設定を有効にするのは消極的ですが安全です。デフォルトは「true」です。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS設定</title>

  <para>
   一般的に、QoS (サービスの品質)とは、トラフィックの優先順位付けとリソース予約の方法のことを指します。これは特に、特別な要件を持つトラフィックを転送するために重要です。
  </para>

  <important>
   <title>iSCSIではサポートされない</title>
   <para>
    次のQoS設定は、ユーザスペースのRBD実装である<systemitem class="daemon">librbd</systemitem>によってのみ使用され、<systemitem>kRBD</systemitem>実装では使用されません。<emphasis/>iSCSIは<systemitem>kRBD</systemitem>を使用するため、QoS設定を使用しません。ただし、iSCSIでは、標準のカーネル機能を使用して、カーネルブロックデバイス層でQoSを設定できます。
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      希望する秒あたりI/O操作数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      希望する秒あたりI/Oバイト数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      希望する秒あたり読み取り操作数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      希望する秒あたり書き込み操作数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      希望する秒あたり読み取りバイト数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      希望する秒あたり書き込みバイト数の上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      希望するI/O操作数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      希望するI/Oバイト数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      希望する読み取り操作数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      希望する書き込み操作数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      希望する読み取りバイト数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      希望する書き込みバイト数のバースト上限。デフォルトは0 (制限なし)です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      QoSの最小スケジュールチック(ミリ秒)。デフォルトは50です。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>先読み設定</title>

  <para>
   RADOS Block Deviceは、先読み/プリフェッチをサポートしており、小容量の順次読み込みが最適化されます。これは、仮想マシンの場合は通常はゲストOSによって処理されますが、ブートローダは効率的な読み込みを発行できません。キャッシュが無効な場合、先読みは自動的に無効になります。
  </para>

  <important>
   <title>iSCSIではサポートされない</title>
   <para>
    次の先読み設定は、ユーザスペースのRBD実装である<systemitem class="daemon">librbd</systemitem>によってのみ使用され、<systemitem>kRBD</systemitem>実装では使用されません。<emphasis/>iSCSIは<systemitem>kRBD</systemitem>を使用するため、先読み設定を使用しません。ただし、iSCSIでは、標準のカーネル機能を使用して、カーネルブロックデバイス層で先読みを設定できます。
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      先読みをトリガするために必要な順次読み込み要求の数。デフォルトは10です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      先読み要求の最大サイズ。0に設定すると、先読みは無効になります。デフォルトは512KBです。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      この量のバイトRBDイメージから読み込みを行った後は、そのイメージが閉じられるまで先読みは無効になります。これにより、ゲストOSは起動時に先読みを引き継ぐことができます。0に設定すると、先読みは有効なままです。デフォルトは50MBです。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>拡張機能</title>

  <para>
   RADOS Block Deviceは、RBDイメージの機能を拡張する拡張機能をサポートしています。RBDイメージの作成時にコマンドラインで機能を指定することも、<option>rbd_default_features</option>オプションを使用してCeph設定ファイルで機能を指定することもできます。
  </para>

  <para>
   <option>rbd_default_features</option>オプションの値は、次の2つの方法で指定できます。
  </para>

  <itemizedlist>
   <listitem>
    <para>
     機能の内部値の合計として指定する。各機能には独自の内部値があります。たとえば、「layering」は1で、「fast-diff」は16です。したがって、これらの2つの機能をデフォルトで有効にするには、以下を含めます。
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     機能のカンマ区切りリストとして指定する。この場合、前の例は次のようになります。
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>iSCSIではサポートされない機能</title>
   <para>
    <option>deep-flatten</option>、<option>object-map</option>、<option>journaling</option>、<option>fast-diff</option>、および<option>striping</option>の機能を使用するRBDイメージは、iSCSIではサポートされません。
   </para>
  </note>

  <para>
   次に、RBDの拡張機能のリストを示します。
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      階層化により、クローン作成を使用できます。
     </para>
     <para>
      内部値は1で、デフォルトは「yes」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      ストライピングは、複数のオブジェクトにデータを分散し、順次読み込み/書き込みワークロードの並列処理に役立ちます。これにより、大容量またはビジー状態のRADOS Block Deviceにおいて単一ノードのボトルネックを防ぎます。
     </para>
     <para>
      内部値は2で、デフォルトは「yes」す。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      有効にすると、クライアントは書き込みを行う前にオブジェクトのロックを取得する必要があります。単一のクライアントが同時に1つのイメージにアクセスしている場合にのみ、排他ロックを有効にします。内部値は4です。デフォルトは「yes」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      オブジェクトマップのサポートは、排他ロックのサポートに依存します。ブロックデバイスはシンプロビジョニングされます。つまり、実際に存在するデータのみを保存します。オブジェクトマップのサポートは、どのオブジェクトが実際に存在するか(ドライブに保存されたデータを持つか)を追跡するのに役立ちます。オブジェクトマップのサポートを有効にすると、クローン作成、保存密度の低いイメージのインポートとエクスポート、および削除のI/O操作が高速化されます。
     </para>
     <para>
      内部値は8で、デフォルトは「yes」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Fast-diffのサポートは、オブジェクトマップのサポートと排他ロックのサポートに依存します。これは、オブジェクトマップに別のプロパティを追加することで、イメージのスナップショットと、スナップショットの実際のデータ使用と間の差分を生成する速度が大幅に向上します。
     </para>
     <para>
      内部値は16で、デフォルトは「yes」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      ディープフラット化は、<command>rbd flatten</command> (<xref linkend="rbd-flatten-cloned-image"/>を参照)を、イメージそのもの以外にイメージのすべてのスナップショットでも機能するようにします。ディープフラット化がなければ、イメージのスナップショットは引き続き親に依存するため、スナップショットが削除されるまで親イメージを削除することはできません。ディープフラット化は、スナップショットがある場合でも、親をそのクローンから独立させます。
     </para>
     <para>
      内部値は32で、デフォルトは「yes」です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option> ジャーナリング</option></term>
    <listitem>
     <para>
      ジャーナリングのサポートは排他ロックに依存します。ジャーナリングは、イメージに対するすべての変更を発生順に記録します。RBDミラーリング(<xref linkend="ceph-rbd-mirror"/>を参照)では、ジャーナルを使用してクラッシュ整合イメージを<literal>remote</literal>クラスタに複製します。
     </para>
     <para>
      内部値は64で、デフォルトは「no」です。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>古いカーネルクライアントを使用したRBDのマッピング</title>

  <para>
   SUSE Enterprise Storage 7を使用して展開したクラスタでは、古いクライアント(SLE11 SP4 など)でサポートされない複数の機能(RBDイメージレベルの機能とRADOSレベルの機能の両方)が強制的に適用されるため、これらの古いクライアントはRBDイメージをマップできない場合があります。これが発生した場合、OSDログに次のようなメッセージが表示されます。
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>CRUSHマップのバケットタイプを変更すると大規模なリバランスが発生する</title>
   <para>
    CRUSHマップのバケットタイプを「straw」と「straw2」の間で切り替える場合は、計画的に行ってください。バケットタイプを変更するとクラスタの大規模なリバランスが発生するため、クラスタノードに重大な影響があることを想定しておいてください。
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     サポートされていないRBDイメージ機能を無効にします。例:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     CRUSHマップのバケットタイプを「straw2」から「straw」に変更します。
    </para>
    <substeps>
     <step>
      <para>
       CRUSHマップを保存します。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       CRUSHマップを逆コンパイルします。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       CRUSHマップを編集して、「straw2」を「straw」に置き換えます。
      </para>
     </step>
     <step>
      <para>
       CRUSHマップを再コンパイルします。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       新しいCRUSHマップを設定します。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Block DeviceとKubernetesの有効化</title>

  <para>
   Ceph RBDとKubernetes v1.13以上を<literal>ceph-csi</literal>ドライバにより併用できます。このドライバはRBDイメージを動的にプロビジョニングしてKubernetesボリュームを支援します。また、RBDに支援されたボリュームを参照するポッドを実行中のワーカーノードで、RBDイメージをBlock Deviceとしてマッピングします。
  </para>

  <para>
   Ceph Block DeviceとKubernetesを併用するには、Kubernetes環境に<literal>ceph-csi</literal>をインストールして設定する必要があります。
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal>はデフォルトではRBDカーネルモジュールを使用します。しかし、このモジュールがCeph CRUSHの調整可能パラメータや、RBDイメージ機能をすべてサポートしているとは限りません。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     デフォルトでは、Ceph Block DeviceはRBDプールを使用します。Kubernetesボリュームストレージ用のプールを作成します。Cephクラスタが実行中であることを確認してから、プールを作成します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     RBDツールを使用してプールを初期化します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Kubernetesと<literal>ceph-csi</literal>用の新しいユーザを作成します。次のコマンドを実行し、生成されたキーを記録します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal>は、Cephクラスタ用のCeph Monitorアドレスを定義するために、Kubernetesに保存されたConfigMapオブジェクトを必要とします。Cephクラスタの固有fsidとMonitorアドレスの両方を収集します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     次の例に示すような<filename>csi-config-map.yaml</filename>ファイルを生成します。なお、<literal>clusterID</literal>はFSIDで、<literal>monitors</literal>はMonitorアドレスで置き換えてください。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     作成されたら、Kubernetesに新しいConfigMapオブジェクトを保存します。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal>はCephクラスタとの通信にcephx資格情報を必要とします。新しく作成したKubernetesユーザIDとcephxキーを使用して、次の例に示すような<filename>csi-rbd-secret.yaml</filename>ファイルを生成します。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     生成されたら、Kubernetesに新しいシークレットオブジェクトを保存します。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     必要となるServiceAccountとRBAC ClusterRole/ClusterRoleBindingのKubernetesオブジェクトを作成します。これらのオブジェクトを、お客様のKubernetes環境に合わせてカスタマイズする必要はありません。そのため、<literal>ceph-csi</literal>展開用のYAMLファイルから、オブジェクトを直接利用できます。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal>プロビジョナとノードプラグインを作成します。
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      デフォルトでは、プロビジョナとノードプラグインのYAMLファイルは、<literal>ceph-csi</literal>コンテナの開発版リリースを取得します。リリース版を使用するように、YAMLファイルをアップデートする必要があります。
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Kubernetes環境におけるCeph Block Deviceの利用</title>
   <para>
    Kubernetes StorageClassはストレージクラスを定義します。複数のStorageClassオブジェクトを作成して、多様なサービス品質レベルと機能をマッピングできます。例としては、NVMeをベースのプールとHDDベースのプールの併用などです。
   </para>
   <para>
    作成済みのKubernetesプールをマッピングする<literal>ceph-csi</literal> StorageClassを作成するには、次のYAMLファイルを使用できます。ただし、作成前に<literal>clusterID</literal>プロパティと使用するCephクラスタのFSIDが一致することを確認してください。
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    <literal>PersistentVolumeClaim</literal>はユーザからの抽象化ストレージリソースに対する要求です。要求後、<literal>PersistentVolumeClaim</literal>はポッドのリソースに関連付けられ、<literal>PersistentVolume</literal>をプロビジョニングします。これは、Cephブロックイメージに支援されます。必要に応じて<option>volumeMode</option>を付加することで、マウント済みのファイルシステム(デフォルト)と、Block DeviceベースのRAWボリュームを選択できます。
   </para>
   <para>
    <literal>ceph-csi</literal>を使用する場合、<option>volumeMode</option>に<option>Filesystem</option>を指定すると、<literal>ReadWriteOnce accessMode</literal>要求と<literal>ReadOnlyMany accessMode</literal>要求の両方をサポートできます。また、<option>volumeMode</option>に<option>Block</option>を指定すると、<literal>ReadWriteOnce accessMode</literal>要求、<literal>ReadWriteMany accessMode</literal>要求、<literal>ReadOnlyMany accessMode</literal>要求をサポートできます。
   </para>
   <para>
    たとえば、前の手順で作成した<literal>ceph-csi-based StorageClass</literal>を使用する、ブロックベースの<literal>PersistentVolumeClaim</literal>を作成するには、次のYAMLファイルを使用できます。これにより、<literal>csi-rbd-sc StorageClass</literal>からRAWブロックストレージを要求します。
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    次に示すのは、前に述べた<literal>PersistentVolumeClaim</literal>をRAW Block Deviceとしてポッドのリソースにバインドする例です。
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    前の手順で作成した <literal>ceph-csi-based StorageClass</literal>を使用する、ファイルシステムベースの<literal>PersistentVolumeClaim</literal>を作成するには、次のYAMLファイルを使用できします。これにより、<literal>csi-rbd-sc StorageClass</literal>からマウント済みのファイルシステム(RBDイメージにより支援)を要求します。
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    次に示すのは、前に述べた<literal>PersistentVolumeClaim</literal>をマウント済みのファイルシステムとしてポッドのリソースにバインドする例です。
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
