<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>ストレージプールの管理</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Cephはデータをプール内に保存します。プールは、オブジェクトを保存するための論理グループです。プールを作成せずに初めてクラスタを展開した場合、Cephはデフォルトのプールを使用してデータを保存します。次の重要な特徴はCephプールに関連するものです。
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    「災害耐性」<emphasis/>: Cephプールは、プール内のデータを複製またはエンコードすることで、災害耐性をもたらします。各プールは複製プール(<literal>replicated</literal>)、またはイレージャコーディングプール(<literal>erasure coding</literal>)に設定できます。複製プールの場合、プール内の各データオブジェクトが持つレプリカ(コピー)の数をさらに設定します。失っても問題ないコピー(OSD、CRUSHバケット/リーフ)の数は、レプリカの数 - 1個までです。イレージャコーディングを使用する場合、値<option>k</option>と値<option>m</option>を設定します。値<option>k</option>はデータチャンクの数で、値<option>m</option>はコーディングチャンクの数です。イレージャコーディングプールの場合、失ってもデータに問題が生じないOSD(CRUSHバケット/リーフ)の数は、コーディングチャンクの数により決まります。
   </para>
  </listitem>
  <listitem>
   <para>
    「配置グループ」<emphasis/>: プールの配置グループの数を設定できます。一般的な設定では、OSDあたり約100個の配置グループを使用し、大量のコンピューティングリソースを使用することなく最適なバランスを提供します。複数のプールを設定する場合は、プールとクラスタ全体の両方にとって適切な数の配置グループを設定するよう注意してください。
   </para>
  </listitem>
  <listitem>
   <para>
    「CRUSHルール」<emphasis/>: プールにデータを保存する場合、オブジェクトとそのレプリカ(またはイレージャコーディングプールの場合はチャンク)は、プールにマップされたCRUSHルールに従って配置されます。ご使用のプールに対してカスタムCRUSHルールを作成できます。
   </para>
  </listitem>
  <listitem>
   <para>
    「スナップショット」<emphasis/>: <command>ceph osd pool mksnap</command>を使用してスナップショットを作成すると、特定のプールのスナップショットが効果的に作成されます。
   </para>
  </listitem>
 </itemizedlist>
 <para>
  データをプールに編成するために、プールを一覧、作成、および削除できます。各プールの使用量統計を表示することもできます。
 </para>
 <sect1 xml:id="ceph-pools-operate-add-pool">
  <title>プールの作成</title>

  <para>
   オブジェクトのコピーを複数保持することによってOSDの損失から回復するには<literal>replicated</literal>、汎用RAID5またはRAID6機能を利用するには<literal>erasure</literal>を指定して、プールを作成できます。必要な未加工ストレージは、複製プールでは多く、イレージャコーディングプールでは少なくなります。デフォルトの設定値は<literal>replicated</literal>です。イレージャコーディングプールの詳細については、<xref linkend="cha-ceph-erasure"/>を参照してください。
  </para>

  <para>
   複製プールを作成するには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable></screen>

  <note>
   <para>
    これ以外のオプション引数については、自動拡張機能で処理されます。詳細については、<xref linkend="op-pgs-autoscaler"/>を参照してください。
   </para>
  </note>

  <para>
   イレージャコーディングプールを作成するには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable> erasure <replaceable>CRUSH_RULESET_NAME</replaceable> \
<replaceable>EXPECTED_NUM_OBJECTS</replaceable></screen>

  <para>
   OSDあたりの配置グループの制限を超える場合、<command>ceph osd pool create</command>コマンドは失敗する可能性があります。この制限はオプション<option>mon_max_pg_per_osd</option>で設定します。
  </para>

  <variablelist>
   <varlistentry>
    <term>POOL_NAME</term>
    <listitem>
     <para>
      プールの名前。固有である必要があります。このオプションは必須です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_TYPE</term>
    <listitem>
     <para>
      プールタイプ。オブジェクトのコピーを複数保持することによってOSDの損失から回復するには「replicated」<literal/>、一種の汎用RAID5機能を利用するには「erasure」<literal/>を指定できます。複製プールの場合、必要な未加工ストレージが増えますが、Cephのすべての操作が実装されます。イレージャプールの場合、必要な未加工ストレージは減りますが、利用可能な操作のサブセットのみが実装されます。デフォルトの<literal>POOL_TYPE</literal>は<literal>replicated</literal>です。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CRUSH_RULESET_NAME</term>
    <listitem>
     <para>
      このプールのCRUSHルールセットの名前。指定したルールセットが存在しない場合、複製プールの作成は-ENOENTで失敗します。複製プールでは<varname>osd pool default CRUSH replicated ruleset</varname>設定変数で指定されるルールセットです。このルールセットは存在している必要があります。イレージャプールでは、デフォルトのイレージャコードプロファイルを使用する場合は「erasure-code」、それ以外の場合は<replaceable>POOL_NAME</replaceable>です。このルールセットは、まだ存在しない場合は暗黙的に作成されます。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>erasure_code_profile=profile</term>
    <listitem>
     <para>
      イレージャコーディングプール専用。イレージャコードプロファイルを使用します。<command>osd erasure-code-profile set</command>で定義した既存のプロファイルである必要があります。
     </para>
     <note>
      <para>
       何らかの理由でプールの自動拡張が無効化(<literal>pg_autoscale_mode</literal>がoffに設定)されている場合は、手動でPG数を計算して設定できます。プールに適した配置グループ数の計算の詳細については、「<xref linkend="op-pgs"/>」を参照してください。
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>EXPECTED_NUM_OBJECTS</term>
    <listitem>
     <para>
      このプールの想定オブジェクト数。この値を(負の<option>filestore merge threshold</option>とともに)設定すると、プールの作成時にPGフォルダが分割されます。これにより、ランタイム時のフォルダ分割によるレイテンシの影響が避けられます。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-listing-pools">
  <title>プールの一覧</title>

  <para>
   クラスタのプールを一覧にするには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool ls</screen>
 </sect1>
 <sect1 xml:id="ceph-renaming-pool">
  <title>プールの名前変更</title>

  <para>
   プールの名前を変更するには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rename <replaceable>CURRENT_POOL_NAME</replaceable> <replaceable>NEW_POOL_NAME</replaceable></screen>

  <para>
   プールの名前を変更する場合に、認証ユーザ用のプールごとのケーパビリティがあるときは、そのユーザのケーパビリティを新しいプール名で更新する必要があります。
  </para>
 </sect1>
 <sect1 xml:id="ceph-pools-operate-del-pool">
  <title>プールの削除</title>

  <warning>
   <title>プールの削除は元に戻せない</title>
   <para>
    プールには重要なデータが収められている場合があります。プールを削除すると、プール内のすべてのデータが消え、回復する方法はありません。
   </para>
  </warning>

  <para>
   誤ってプールを削除することはきわめて危険であるため、Cephには、プールの削除を防止するメカニズムが2つ実装されています。プールを削除するには、両方のメカニズムを無効にする必要があります。
  </para>

  <para>
   1つ目のメカニズムは<literal>NODELETE</literal>フラグです。各プールにこのフラグがあり、デフォルト値は「false」です。プールのこのフラグのデフォルト値を確認するには、次のコマンドを実行します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>

  <para>
   <literal>nodelete: true</literal>が出力される場合、次のコマンドを使用してフラグを変更しない限り、プールを削除できません。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>

  <para>
   2つ目のメカニズムは、クラスタ全体の設定パラメータ<option>mon allow pool delete</option>で、デフォルトは「false」です。つまり、デフォルトではプールを削除できません。表示されるエラーメッセージは次のとおりです。
  </para>

<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>

  <para>
   この安全設定に関係なくプールを削除するには、<option>mon allow pool delete</option>を一時的に「true」に設定してプールを削除し、その後、パラメータを「false」に戻します。
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>cephuser@adm &gt; </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>

  <para>
   <command>injectargs</command>コマンドを実行すると、次のメッセージが表示されます。
  </para>

<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>

  <para>
   これは単にコマンドが正常に実行されたことを確認するものです。エラーではありません。
  </para>

  <para>
   作成したプール用に独自のルールセットとルールを作成した場合、プールが必要なくなったらルールセットとルールを削除することをお勧めします。
  </para>
 </sect1>
 <sect1 xml:id="ceph-pool-other-operations">
  <title>その他の操作</title>

  <sect2 xml:id="ceph-pools-associate">
   <title>プールとアプリケーションの関連付け</title>
   <para>
    プールを使用する前に、プールをアプリケーションに関連付ける必要があります。CephFSで使用されるプール、またはObject Gatewayによって自動的に作成されるプールは自動的に関連付けられます。
   </para>
   <para>
    それ以外の場合は、自由な形式のアプリケーション名を手動でプールに関連付けることができます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application enable <replaceable>POOL_NAME</replaceable> <replaceable>APPLICATION_NAME</replaceable></screen>
   <tip>
    <title>デフォルトのアプリケーション名</title>
    <para>
     アプリケーション名として、CephFSは<literal>cephfs</literal>、RADOS Block Deviceは<literal>rbd</literal>、Object Gatewayは<literal>rgw</literal>をそれぞれ使用します。
    </para>
   </tip>
   <para>
    1つのプールを複数のアプリケーションに関連付けて、各アプリケーションで専用のメタデータを使用できます。プールに関連付けられたアプリケーションを一覧にするには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-set-pool-quotas">
   <title>プールのクォータの設定</title>
   <para>
    最大バイト数、またはプールあたりのオブジェクトの最大数に対してプールクォータを設定できます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>POOL_NAME</replaceable> <replaceable>MAX_OBJECTS</replaceable> <replaceable>OBJ_COUNT</replaceable> <replaceable>MAX_BYTES</replaceable> <replaceable>BYTES</replaceable></screen>
   <para>
    以下に例を示します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    クォータを削除するには、値を0に設定します。
   </para>
  </sect2>

  <sect2 xml:id="ceph-showing-pool-statistics">
   <title>プールの統計情報の表示</title>
   <para>
    プールの使用量統計を表示するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados df
 POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
 .rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
 cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
 cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
 default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
 default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
 default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
 default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
 example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
 iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
 mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
 pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
 pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
 pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
 </screen>
   <para>
    個々の列の説明は次のとおりです。
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       プールによって使用されているバイトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       プールに保存されているオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       プールに保存されているクローンの数。スナップショットが作成されてオブジェクトに書き込まれる場合、元のオブジェクトは変更されずにそのクローンが作成されるため、元のスナップショットオブジェクトの内容は変更されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>COPIES</term>
     <listitem>
      <para>
       オブジェクトレプリカの数。たとえば、レプリケーション係数3の複製プールに「x」個のオブジェクトがある場合、コピーの数は通常、3 * x個になります。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       プライマリOSDにコピーがみつからないときに劣化状態になっているオブジェクトの数(すべてのコピーが存在するわけではない)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       見つからないオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       劣化したオブジェクトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       このプールに対して要求された読み込み操作の合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       このプールから読み込まれたバイトの合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       このプールに対して要求された書き込み操作の合計数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       プールに書き込まれたバイトの合計数。同じオブジェクトに何度も書き込むことができるため、これはプールの使用率と同じではないことに注意してください。その結果、プールの使用率は同じままであっても、プールに書き込まれたバイト数は大きくなります。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       圧縮データに割り当てられているバイトの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       圧縮データが非圧縮時に使用するバイトの数。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-getting-pool-values">
   <title>プールから値を取得</title>
   <para>
    プールから値を取得するには、次のように<command>get</command>コマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable></screen>
   <para>
    <xref linkend="ceph-pools-values"/>に示すキーと、次のキーの値を取得できます。
   </para>
   <variablelist>
    <varlistentry>
     <term>PG_NUM</term>
     <listitem>
      <para>
       プールの配置グループの数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PGP_NUM</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。有効な範囲は<option>PG_NUM</option>以下です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>プールのすべての値</title>
    <para>
     特定のプールに関連するすべての値を一覧にするには、次のコマンドを実行します。
    </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> all
 </screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>プールに値を設定</title>
   <para>
    プールに値を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable> <replaceable>VALUE</replaceable></screen>
   <para>
    プールタイプごとにソートしたプールの値のリストを以下に示します。
   </para>
   <variablelist>
    <title>共通するプールの値</title>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       確認済みであるもののコミットされていない要求の再生をクライアントに許可する秒数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       プールの配置グループの数。新しいクラスタにOSDを追加する場合は、新しいOSDの対象に指定されたすべてのプール上にある配置グループの値を確認します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       データ配置を計算する際に使用する配置グループの有効数。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       クラスタ内のオブジェクト配置のマッピングに使用するルールセット。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       指定したプールに対してHASHPSPOOLフラグを設定(1)または設定解除(0)します。このフラグを有効にすると、PGをOSDに効率的に分散するためにアルゴリズムが変更されます。HASHPSPOOLフラグがデフォルトの0に設定されたプールでこのフラグを有効にすると、クラスタは、すべてのPGをもう一度正しく配置するためにバックフィルを開始します。これはクラスタに多大なI/O負荷をかける可能性があるので、非常に負荷が高い運用クラスタでは、0～1のフラグを有効にしないでください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       プールの削除を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       プールの<option>pg_num</option>および<option>pgp_num</option>の変更を防止します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub、nodeep-scrub</term>
     <listitem>
      <para>
       I/Oの一時的な高負荷を解決するため、特定のプールに対してデータの(ディープ)スクラブを無効にします。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       指定したプールの読み込み/書き込み要求の<literal>WRITE_FADVISE_DONTNEED</literal>フラグを設定または設定解除して、キャッシュへのデータ格納をバイパスします。デフォルトは<literal>false</literal>です。複製プールとECプールの両方に適用されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       クラスタの負荷が低い場合にプールをスクラブする最小間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_min_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       クラスタの負荷に関係なくプールをスクラブする最大間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_scrub_max_interval</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       プールの「詳細」<emphasis/>スクラブの間隔(秒単位)。デフォルトの<literal>0</literal>は、Ceph設定ファイルの<option>osd_deep_scrub</option>の値が使用されることを意味します。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>複製プールの値</title>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       プール内のオブジェクトのレプリカ数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       I/Oに必要なレプリカの最小数を設定します。詳細については、<xref linkend="ceph-pools-options-num-of-replicas"/>を参照してください。複製プール専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       プールのサイズの変更を防止します。プールが作成された際にデフォルト値として<option>osd_pool_default_flag_nosizechange</option>パラメータの値を取得します。このパラメータはデフォルトでは<literal>false</literal>です。複製プールにのみ適用できます。ECプールはサイズを変更できないためです。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの追跡を有効にします。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。このオプションに設定できる値は、<literal>bloom</literal>、<literal>explicit_hash</literal>、または<literal>explicit_object</literal>です。デフォルトは<literal>bloom</literal>で、他の値はテスト専用です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       キャッシュプールに関して保存するヒットセットの数。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       キャッシュプールのヒットセットの期間(秒単位)。値を増やすほど、<systemitem>ceph-osd</systemitem>デーモンのRAM消費量が増えます。プールが作成された際にデフォルト値として<option>osd_tier_default_cache_hit_set_period</option>パラメータの値を取得します。このパラメータのデフォルト値は<literal>1200</literal>です。複製プールにのみ適用できます。ECプールはキャッシュ層として使用できないためです。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       bloomヒットセットタイプの誤検知確率。詳細については、「<link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link>」を参照してください。有効な範囲は0.0～1.0で、デフォルトは<literal>0.05</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       キャッシュ階層化のヒットセットを作成する際に、GMT (グリニッジ標準時)のタイムスタンプを使用するようOSDに強制します。これにより、異なるタイムゾーンにあるノードが同じ結果を返すようにします。デフォルトは<literal>1</literal>です。この値は変更できません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをバッキングストレージプールにフラッシュします。デフォルトは<literal>0.4</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる変更済みオブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは変更済み(ダーティ)オブジェクトをより高速なバッキングストレージプールにフラッシュします。デフォルトは<literal>0.6</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       キャッシュプールに含まれる未変更オブジェクトの割合で、この割合を超えると、キャッシュ階層化エージェントは未変更(クリーン)オブジェクトをキャッシュプールから削除します。デフォルトは<literal>0.8</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       <option>max_bytes</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       <option>max_objects</option>のしきい値がトリガされた場合、Cephはオブジェクトのフラッシュまたは削除を開始します。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       連続する2つの<literal>hit_set</literal>間の温度減衰率。デフォルトは<literal>20</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       温度を計算するために、<literal>hit_set</literal>内で最大<literal>N</literal>個の出現をカウントします。デフォルトは<literal>1</literal>です。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールからストレージプールへフラッシュするまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       キャッシュ階層化エージェントがオブジェクトをキャッシュプールから削除するまでの時間(秒単位)。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist xml:id="pool-values-ec">
    <title>イレージャコーディングプールの値</title>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       イレージャコーディングプールでこのフラグが有効な場合、読み込み要求は、すべてのシャードに対してサブ読み込みを発行し、クライアントの要求を実行するためにデコードする十分なシャードを受け取るまで待機します。イレージャプラグインが<emphasis>jerasure</emphasis>および<emphasis>isa</emphasis>の場合、最初の<literal>K</literal>個の応答が返された時点で、これらの応答からデコードされたデータを使用してただちにクライアントの要求が実行されます。このアプローチでは、CPUの負荷が増え、ディスク/ネットワークの負荷は減ります。現在のところ、このフラグはイレージャコーディングプールでのみサポートされます。デフォルトは<literal>0</literal>です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>オブジェクトレプリカの数の設定</title>
   <para>
    複製プール上のオブジェクトレプリカの数を設定するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable>にはオブジェクトそのものも含まれます。たとえば、オブジェクトとそのオブジェクトの2つのコピーで合計3つのオブジェクトインスタンスが必要な場合、3を指定します。
   </para>
   <warning>
    <title>3つ未満のレプリカを設定しない</title>
    <para>
     <replaceable>num-replicas</replaceable>を2に設定した場合、データのコピーは「1つ」<emphasis/>だけになります。1つのオブジェクトインスタンスが失われた場合、たとえば回復中の前回のスクラブ以降に、他のコピーが壊れていないことを信頼する必要があります(詳細については、<xref linkend="scrubbing-pgs"/>を参照)。
    </para>
    <para>
     プールを1つのレプリカに設定することは、プール内にデータオブジェクトのインスタンスが「1つ」<emphasis/>だけ存在することを意味します。OSDに障害発生すると、データは失われます。レプリカが1つのプールの使用法としては、一時データを短時間保存することが考えられます。
    </para>
   </warning>
   <tip>
    <title>3つを超えるレプリカの設定</title>
    <para>
     1つのプールに対して4つのレプリカを設定すると、信頼性が25%向上します。
    </para>
    <para>
     2つのデータセンターの場合、各データセンターで2つのコピーを使用できるよう、1つプールに対してレプリカを4つ以上設定します。これにより、一方のデータセンターが失われてもまだ2つのコピーが存在し、さらにディスクが1つ失われてもデータが失われないようにします。
    </para>
   </tip>
   <note>
    <para>
     1つのオブジェクトが、機能低下モードにおいてレプリカが<literal>pool size</literal>未満の状態でI/Oを受け付ける場合があります。I/Oに必要なレプリカの最小数を設定するには、<literal>min_size</literal>設定を使用する必要があります。次に例を示します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     これにより、データプール内のオブジェクトはレプリカが<literal>min_size</literal>未満の場合、I/Oを受け取らなくなります。
    </para>
   </note>
   <tip>
    <title>オブジェクトレプリカの数の取得</title>
    <para>
     オブジェクトレプリカの数を取得するには、次のコマンドを実行します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump | grep 'replicated size'</screen>
    <para>
     <literal>replicated size</literal>属性が強調表示された状態でプールが一覧にされます。デフォルトでは、Cephはオブジェクトのレプリカを2つ作成します(合計で3つのコピー、またはサイズ3)。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>プールのマイグレーション</title>

  <para>
   プールを作成する際(<xref linkend="ceph-pools-operate-add-pool"/>を参照)、プールのタイプや配置グループの数など、初期パラメータを指定する必要があります。後でこれらのパラメータのいずれかを変更する場合(たとえば、複製プールをイレージャコーディングプールに変換したり、配置グループの数を減らしたりする場合)、プールのデータを、展開に適したパラメータを持つ別のプールに移行する必要があります。
  </para>

  <para>
   このセクションでは、2つのマイグレーション方法を説明します。1つは「キャッシュ層」<emphasis/>を使う方法で一般的なプールのデータマイグレーションに使用します。もう1つは<command>rbd migrate</command>サブコマンドを使用する方法で、RBDイメージを新しいプールに移行します。どちらの方法にもその詳細と制限があります。
  </para>

  <sect2 xml:id="pool-migrate-limits">
   <title>制限</title>
   <itemizedlist>
    <listitem>
     <para>
      「キャッシュ層」<emphasis/>の方法を使用して、複製プールからECプールまたは別の複製プールに移行できます。ECプールからの移行はサポートされていません。
     </para>
    </listitem>
    <listitem>
     <para>
      RBDイメージとCephFSエクスポートを複製プールからECプールに移行することはできません。その理由として、RBDとCephFSは<literal>omap</literal>を使用してメタデータを保存していますが、ECプールは<literal>omap</literal>をサポートしていないためです。たとえば、RBDのヘッダオブジェクトはフラッシュできません。それでも、メタデータを複製プールに残したまま、データをECプールに移行することは可能です。
     </para>
    </listitem>
    <listitem>
     <para>
      <command>rbd migration</command>による方法を使用すると、クライアントのダウンタイムを最小限に抑えてイメージを移行できます。必要なのは、<option>prepare</option>ステップの前にクライアントを停止して、後でクライアントを起動することだけです。<option>prepare</option>ステップの直後にイメージを開くことができるのは、この機能をサポートする<systemitem>librbd</systemitem>クライアント(Ceph Nautilus以降)のみであり、それ以前の<systemitem>librbd</systemitem>クライアントや<systemitem>krbd</systemitem>クライアントは<option>commit</option>ステップが実行されるまでイメージを開くことができないことに注意してください。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>キャッシュ層を使用した移行</title>
   <para>
    原理は単純で、移行する必要があるプールを逆の順番でキャッシュ層に含めます。次の例では、「testpool」という名前の複製プールをイレージャコーディングプールに移行します。
   </para>
   <procedure>
    <title>複製プールからイレージャコーディングプールへの移行</title>
    <step>
     <para>
      「newpool」という名前の新しいイレージャコーディングプールを作成します。プールの作成パラメータの詳細な説明については、<xref linkend="ceph-pools-operate-add-pool"/>を参照してください。
     </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool create newpool erasure default
</screen>
     <para>
      使用されているクライアントキーリングが「testpool」と少なくとも同じ機能を「newpool」に提供することを確認します。
     </para>
     <para>
      これでプールが2つできました。データが入った元の複製プール「testpool」と、新しい空のイレージャコーディングプール「newpool」です。
     </para>
     <figure>
      <title>マイグレーション前のプール</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュ層をセットアップして、複製プール「testpool」をキャッシュプールとして設定します。<option>-force-nonempty</option>オプションを使用すると、プールにすでにデータがある場合にもキャッシュ層を追加できます。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<prompt>cephuser@adm &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>cephuser@adm &gt; </prompt>ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>キャッシュ層のセットアップ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      キャッシュプールからすべてのオブジェクトを新しいプールに強制的に移動します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>データのフラッシュ</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      すべてのデータが新しいイレージャコーディングプールにフラッシュされるまでは、オーバーレイを指定してオブジェクトが古いプールで検索されるようにする必要があります。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      このオーバーレイにより、すべての操作が古い複製プール「testpool」に転送されます。
     </para>
     <figure>
      <title>オーバーレイの設定</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      これで、新しいプールのオブジェクトにアクセスするようすべてのクライアントを切り替えることができます。
     </para>
    </step>
    <step>
     <para>
      すべてのデータがイレージャコーディングプール「newpool」に移行されたら、オーバーレイと古いキャッシュプール「testpool」を削除します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>マイグレーションの完了</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      実行
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>RBDイメージの移行</title>
   <para>
    次に、RBDイメージを1つの複製プールから別の複製プールに移行する場合に推奨する方法を示します。
   </para>
   <procedure>
    <step>
     <para>
      クライアント(仮想マシンなど)がRBDイメージにアクセスしないようにします。
     </para>
    </step>
    <step>
     <para>
      新しいイメージをターゲットプール内に作成し、親をソースイメージに設定します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>イレージャコーディングプールにデータだけを移行する</title>
      <para>
       イメージデータのみを新しいECプールに移行し、メタデータを元の複製プールに残す必要がある場合は、代わりに次のコマンドを実行します。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
    </step>
    <step>
     <para>
      クライアントがターゲットプール内のイメージにアクセスできるようにします。
     </para>
    </step>
    <step>
     <para>
      データをターゲットプールに移行します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      古いイメージを削除します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>プールのスナップショット</title>

  <para>
   プールのスナップショットは、Cephのプール全体の状態のスナップショットです。プールのスナップショットにより、プールの状態の履歴を保持できます。プールのスナップショットを作成すると、プールサイズに比例したストレージ領域が消費されます。プールのスナップショットを作成する前に、必ず関連するストレージに十分なディスク領域があることを確認してください。
  </para>

  <sect2 xml:id="ceph-make-snapshot-pool">
   <title>プールのスナップショットの作成</title>
   <para>
    プールのスナップショットを作成するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    以下に例を示します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2 xml:id="ceph-listing-snapshots-pool">
   <title>プールのスナップショットの一覧</title>
   <para>
    プールの既存のスナップショットを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    以下に例を示します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2 xml:id="ceph-removing-snapshot-pool">
   <title>プールのスナップショットの削除</title>
   <para>
    プールのスナップショットを削除するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>データ圧縮</title>

  <para>
   BlueStore (詳細については、<xref linkend="about-bluestore"/>を参照)は、オンザフライでデータを圧縮してディスク容量を節約できます。圧縮率は、システムに保存されるデータによって異なります。圧縮/圧縮解除には、追加のCPUパワーが必要になることに注意してください。
  </para>

  <para>
   データ圧縮をグローバルに設定し(<xref linkend="sec-ceph-pool-bluestore-compression-options"/>を参照)、その後、個々のプールに対して固有の圧縮設定を上書きできます。
  </para>

  <para>
   プールにデータが含まれるかどうかに関係なく、プールのデータ圧縮を有効/無効にしたり、圧縮アルゴリズムやモードをいつでも変更したりできます。
  </para>

  <para>
   プールの圧縮を有効にすると、既存のデータに圧縮は適用されなくなります。
  </para>

  <para>
   プールの圧縮を無効にすると、そのプールのすべてのデータの圧縮が解除されます。
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>圧縮の有効化</title>
   <para>
    <replaceable>POOL_NAME</replaceable>という名前のプールのデータ圧縮を有効にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>プール圧縮の無効化</title>
    <para>
     プールのデータ圧縮を無効にするには、圧縮アルゴリズムとして「none」を使用します。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>プール圧縮オプション</title>
   <para>
    次に、すべての圧縮設定のリストを示します。
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       使用可能な値は、<literal>none</literal>、<literal>zstd</literal>、<literal>snappy</literal>です。デフォルトは<literal>snappy</literal>です。
      </para>
      <para>
       どの圧縮アルゴリズムを使用するかは、特定の使用事例によって異なります。次に、推奨事項をいくつか示します。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         変更する妥当な理由がない限り、デフォルトの<literal>snappy</literal>を使用してください。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal>は、圧縮率は優れていますが、少量のデータを圧縮する場合にはCPUオーバーヘッドが高くなります。
        </para>
       </listitem>
       <listitem>
        <para>
         クラスタのCPUとメモリの使用量に注意しながら、実際のデータのサンプルに対してこれらのアルゴリズムのベンチマークを実行します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       使用可能な値は、<literal>none</literal>、<literal>aggressive</literal>、<literal>passive</literal>、<literal>force</literal>です。デフォルトは<literal>none</literal>です。
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: 圧縮しません。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: <literal>COMPRESSIBLE</literal>と表示されている場合、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: <literal>INCOMPRESSIBLE</literal>と表示されている場合以外、圧縮します。
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: 常に圧縮します。
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       値: 倍精度、比率= SIZE_COMPRESSED / SIZE_ORIGINAL。デフォルトは<literal>0.875</literal>です。これは、占有されている容量が圧縮によって12.5%以上削減されない場合は、オブジェクトは圧縮されないことを意味します。
      </para>
      <para>
       この率を上回るオブジェクトは、圧縮効果が低いため圧縮状態では保存されません。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>グローバル圧縮オプション</title>
   <para>
    次の設定オプションはCeph設定で指定でき、1つのプールだけでなくすべてのOSDに適用されます。<xref linkend="sec-ceph-pool-compression-options"/>に一覧にされているプール固有の設定が優先されます。
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       <xref linkend="compr-algorithm"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       <xref linkend="compr-mode"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       <xref linkend="compr-ratio"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。この設定はデフォルトでは無視され、<option>bluestore_compression_min_blob_size_hdd</option>と​<option>bluestore_compression_min_blob_size_ssd</option>が優先されます。0以外の値に設定した場合は、この設定が優先されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>0</literal>。
      </para>
      <para>
       圧縮されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。この設定はデフォルトでは無視され、<option>bluestore_compression_max_blob_size_hdd</option>と​<option>bluestore_compression_max_blob_size_ssd</option>が優先されます。0以外の値に設定した場合は、この設定が優先されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>8K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>64K</literal>。
      </para>
      <para>
       圧縮してソリッドステートドライブに保存されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>128K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最小サイズ。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       値: 符号なし整数、バイト単位のサイズ。デフォルト: <literal>512K</literal>。
      </para>
      <para>
       圧縮してハードディスクに保存されるオブジェクトの最大サイズ。このサイズを超えると、オブジェクトはより小さいチャンクに分割されます。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
