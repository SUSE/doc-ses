<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_bootstrap.xml" version="5.0" xml:id="deploy-bootstrap">
 <info>
  <title>Distribuzione del cluster di bootstrap mediante <systemitem class="resource">ceph-salt</systemitem></title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Questa sezione illustra il processo di distribuzione di un cluster Ceph di base. Leggere con attenzione le sottosezioni seguenti ed eseguire i comandi inclusi nell&apos;ordine dato.
 </para>
 <sect1 xml:id="deploy-cephadm-cephsalt">
  <title>Installazione <systemitem class="resource">ceph-salt</systemitem></title>

  <para>
   <systemitem class="resource">ceph-salt</systemitem> fornisce strumenti per la distribuzione dei cluster Ceph gestiti da cephadm. <systemitem class="resource">ceph-salt</systemitem> utilizza l&apos;infrastruttura Salt per la gestione del sistema operativo, ad esempio gli aggiornamenti del software o la sincronizzazione dell&apos;orario, e per la definizione dei ruoli dei Salt Minion.
  </para>

  <para>
   Sul Salt Master, installare il pacchetto <package>ceph-salt</package>:
  </para>

<screen><prompt>root@master # </prompt>zypper install ceph-salt</screen>

  <para>
   Il comando precedente installa <package>ceph-salt-formula</package> come dipendenza che modifica la configurazione del Salt Master inserendo file aggiuntivi nella directory <filename>/etc/salt/master.d</filename>. Per applicare le modifiche, riavviare <systemitem class="daemon">salt-master.service</systemitem> e sincronizzare i moduli Salt:
  </para>

<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
 </sect1>
 <sect1 xml:id="deploy-cephadm-configure">
  <title>Configurazione delle proprietà del cluster</title>

  <para>
   Utilizzare il comando <command>ceph-salt config</command> per configurare le proprietà di base del cluster.
  </para>

  <important>
   <para>
    Il file <filename>/etc/ceph/ceph.conf</filename> è gestito da cephadm e gli utenti <emphasis>non devono</emphasis> modificarlo. Impostare i parametri di configurazione Ceph con il nuovo comando <command>ceph config</command>. Consultare <xref linkend="cha-ceph-configuration-db"/> per maggiori informazioni.
   </para>
  </important>

  <sect2 xml:id="deploy-cephadm-configure-shell">
   <title>Utilizzo della shell <systemitem class="resource">ceph-salt</systemitem></title>
   <para>
    Se si esegue <command> config</command> senza alcun percorso o sottocomando, viene creata una shell <systemitem class="resource">ceph-salt</systemitem>ceph-salt interattiva. La shell è utile se è necessario configurare contemporaneamente più proprietà o se non si desidera digitare l&apos;intera sintassi del comando.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config
<prompt>/&gt;</prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
   <para>
    Come è possibile vedere dall&apos;output del comando <systemitem class="resource">ceph-salt</systemitem>ls di <command></command>, la configurazione del cluster presenta una struttura ad albero. Sono disponibili due opzioni per configurare una proprietà specifica del cluster nella shell <systemitem class="resource">ceph-salt</systemitem>:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Eseguire il comando dalla posizione corrente e immettere il percorso assoluto alla proprietà come primo argomento:
     </para>
<screen>
<prompt>/&gt;</prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/&gt; /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
    </listitem>
    <listitem>
     <para>
      Modificare inserendo il percorso di cui occorre configurare la proprietà ed eseguire il comando:
     </para>
<screen>
<prompt>/&gt;</prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions&gt;</prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
    </listitem>
   </itemizedlist>
   <tip>
    <title>completamento automatico degli snippet di configurazione</title>
    <para>
     Dalla shell <systemitem class="resource">ceph-salt</systemitem>, è possibile utilizzare la funzione di completamento automatico in modo simile a come la si utilizza in una normale shell Linux (Bash). Tale funzione consente di completare i percorsi di configurazione, i sottocomandi o i nomi dei Salt Minion. Durante il completamento automatico di un percorso di configurazione, sono disponibili due opzioni:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Per fare in modo che la shell termini un percorso relativo sulla posizione corrente, premere due volte il tasto TAB <keycap function="tab"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       Per fare in modo che la shell termini un percorso assoluto, immettere <keycap>/</keycap> e premere due volte il tasto TAB <keycap function="tab"/>.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <tip>
    <title>navigazione con i tasti del cursore</title>
    <para>
     Se si immette <command>cd</command> dalla shell <systemitem class="resource">ceph-salt</systemitem> senza specificare alcun percorso, il comando stamperà una struttura ad albero della configurazione del cluster con la riga dell&apos;attuale percorso attivo. È possibile utilizzare i tasti su è giù del cursore per spostarsi tra le singole righe. Dopo aver confermato con <keycap function="enter"/>, il percorso di configurazione verrà modificato sull&apos;ultimo percorso attivo.
    </para>
   </tip>
   <important>
    <title>convenzione</title>
    <para>
     Per assicurare la coerenza della documentazione, viene utilizzata la sintassi di un singolo comando senza immettere la shell <systemitem class="resource">ceph-salt</systemitem>. Ad esempio, è possibile elencare l&apos;albero di configurazione del cluster con il comando seguente:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
   </important>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-minions">
   <title>Aggiunta di Salt Minion</title>
   <para>
    Includere nella configurazione del cluster Ceph tutti o un sottoinsieme di Salt Minion distribuiti e accettati nella <xref linkend="deploy-salt"/>. È possibile specificare i Salt Minion utilizzando il loro nome intero oppure le espressioni glob &quot;*&quot; e &quot;?&quot; per includere contemporaneamente più Salt Minion. Utilizzare il sottocomando <command>add</command> nel percorso <literal>/ceph_cluster/minions</literal>. Il comando seguente include tutti i Salt Minion accettati:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add '*'</screen>
   <para>
    Verificare che i Salt Minion specificati siano stati aggiunti:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-cephadm">
   <title>Specifica dei Salt Minion gestiti da cephadm</title>
   <para>
    Specificare i nodi che apparterranno al cluster Ceph e che saranno gestiti da cephadm. Includere tutti i nodi che eseguiranno i servizi Ceph, oltre al nodo admin:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-admin">
   <title>Specifica del nodo admin</title>
   <para>
    Il nodo admin è il nodo in cui sono installati il file di configurazione <filename>ceph.conf</filename> e il portachiavi di amministrazione Ceph. In genere, i comandi correlati a Ceph vengono eseguiti sul nodo admin.
   </para>
   <tip>
    <title>Salt Master e nodo admin sullo stesso nodo</title>
    <para>
     Negli ambienti eterogenei, dove tutti o quasi tutti gli host appartengono a SUSE Enterprise Storage, si consiglia di posizionare il nodo admin sullo stesso host del Salt Master.
    </para>
    <para>
     Negli ambienti eterogenei dove un&apos;infrastruttura Salt ospita più di un cluster, ad esempio SUSE Enterprise Storage insieme a SUSE Manager, <emphasis>non</emphasis> posizionare il nodo admin sullo stesso host del Salt Master.
    </para>
   </tip>
   <para>
    Per specificare il nodo admin, eseguire il comando seguente:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
   <tip>
    <title>installazione di <filename>ceph.conf</filename> e del portachiavi di amministrazione su più nodi</title>
    <para>
     È possibile installare il file di configurazione e il portachiavi di amministrazione Ceph su più nodi, se richiesto dalla distribuzione. Per motivi di sicurezza, evitare di installarli su tutti i nodi del cluster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-mon">
   <title>Specifica del primo nodo MON/MGR</title>
   <para>
    È necessario specificare quale Salt Minion del cluster eseguirà il bootstrap del cluster. Questo minion diventerà il primo a eseguire i servizi Ceph Monitor e Ceph Manager.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
   <para>
    Inoltre, è necessario specificare l&apos;indirizzo IP del MON di bootstrap sulla rete pubblica per assicurarsi che il parametro <option>public_network</option> sia impostato correttamente, ad esempio:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-tuned-profiles">
   <title>Specifica dei profili ottimizzati</title>
   <para>
    È necessario specificare quali minion del cluster dispongono di profili ottimizzati attivamente. A questo scopo, aggiungere questi ruoli esplicitamente con i comandi seguenti:
   </para>
   <note>
    <para>
     Un minion non può ricoprire sia il ruolo <literal>latency</literal> che il ruolo <literal>throughput</literal>.
    </para>
   </note>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ssh">
   <title>Generazione di una coppia di chiavi SSH</title>
   <para>
    cephadm utilizza il protocollo SSH per comunicare con i nodi del cluster. L&apos;account utente denominato <literal>cephadm</literal> viene creato automaticamente e utilizzato per la comunicazione SSH.
   </para>
   <para>
    È necessario generare la parte privata e pubblica della coppia di chiavi SSH:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ssh generate
Key pair generated.
<prompt>root@master # </prompt>ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ntp">
   <title>Configurazione del server dell&apos;orario</title>
   <para>
    L&apos;orario di tutti i nodi del cluster deve essere sincronizzato con un&apos;origine dell&apos;orario affidabile. Sono previsti diversi scenari di approccio alla sincronizzazione dell&apos;orario:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Se tutti i nodi del cluster sono già configurati sulla sincronizzazione dell&apos;orario tramite un servizio NTP preferito, disabilitare del tutto la gestione del server dell&apos;orario:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server disable
</screen>
    </listitem>
    <listitem>
     <para>
      Se il sito dispone già di un&apos;origine dell&apos;orario singola, specificare il nome host di tale origine:
     </para>
<screen>
 <prompt>root@master # </prompt>ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
    </listitem>
    <listitem>
     <para>
      In alternativa, <systemitem class="resource">ceph-salt</systemitem> è in grado di configurare uno dei Salt Minion nel ruolo di server dell&apos;orario per il resto del cluster. In questi casi, si parla di &quot;server dell&apos;orario interno&quot;. In questo scenario, <systemitem class="resource">ceph-salt</systemitem> configurerà il server dell&apos;orario interno (che deve essere uno dei Salt Minion) sulla sincronizzazione del suo orario con un server dell&apos;orario esterno, come <literal>pool.ntp.org</literal>, e configurerà tutti gli altri minion per fare in modo che ricavino il proprio orario dal server dell&apos;orario interno. È possibile eseguire questa operazione come segue:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/servers add ses-master.example.com
<prompt>root@master # </prompt>ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
     <para>
      L&apos;opzione <option>/time_server/subnet</option> specifica la sottorete da cui i client NTP possono accedere al server NTP. Questa opzione è impostata automaticamente se si specifica <option>/time_server/servers</option>. Se è necessario modificarla o specificarla manualmente, eseguire:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
    </listitem>
   </itemizedlist>
   <para>
    Verificare le impostazioni del server dell&apos;orario:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
   <para>
    All&apos;indirizzo <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-ntp.html#sec-ntp-yast"/> sono disponibili ulteriori informazioni sulla configurazione della sincronizzazione dell&apos;orario.
   </para>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-dashboardlogin">
   <title>Configurazione delle credenziali di login del Ceph Dashboard</title>
   <para>
    Il Ceph Dashboard sarà disponibile in seguito alla distribuzione del cluster di base. Per accedervi, è necessario impostare un nome utente e una password validi, ad esempio:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/username set admin
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
   <tip>
    <title>forzatura dell&apos;aggiornamento della password</title>
    <para>
     Per default, il primo utente del dashboard sarà forzato a modificare la password al primo login al dashboard. Per disabilitare questa funzione, eseguire il comando seguente:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-registry">
   <title>Utilizzo del registro del container</title>
   <para>
    Il cluster Ceph deve avere accesso a un registro del container per poter effettuare il download e la distribuzione dei servizi Ceph in container. Sono previsti due modi per accedere al registro:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Se il cluster può accedere al registro di default all&apos;indirizzo <literal>registry.suse.com</literal> (direttamente o tramite proxy), è possibile puntare <systemitem class="resource">ceph-salt</systemitem> direttamente a tale URL senza creare un registro locale. Continuare seguendo i passaggi descritti nella <xref linkend="deploy-cephadm-configure-imagepath"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Se il cluster non può accedere al registro di default, ad esempio per una distribuzione con air gap, è necessario configurare un registro container locale. Dopo che il registro locale è stato creato e configurato, è necessario puntare <systemitem class="resource">ceph-salt</systemitem> a tale registro.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="updating-ceph-local-registry">
    <title>Creazione e configurazione del registro locale (facoltativo)</title>
    <important>
     <para>
      Esistono diversi metodi per creare un registro locale. Le istruzioni fornite in questa sezione sono esempi di creazione di registri sicuri e non sicuri. Per informazioni generali sull&apos;esecuzione di un registro delle immagini del container, fare riferimento a <link xlink:href="https://documentation.suse.com/sles/15-SP3/single-html/SLES-container/#sec-docker-registry-installation"/>.
     </para>
    </important>
    <tip>
     <title>posizionamento e utilizzo della porta</title>
     <para>
      Distribuire il registro su un computer accessibile da tutti i nodi del cluster. Si consiglia di utilizzare il nodo admin. Per default, il registro resta in ascolto sulla porta 5000.
     </para>
     <para>
      Sul nodo del registro, utilizzare il seguente comando per assicurarsi che la porta sia libera:
     </para>
<screen>ss -tulpn | grep :5000</screen>
     <para>
      Se altri processi (come <literal>iscsi-tcmu</literal>) sono già in ascolto sulla porta 5000, individuare un&apos;altra porta libera utilizzabile per mappare la porta 5000 nel container del registro.
     </para>
    </tip>
    <procedure>
     <title>creazione del registro locale</title>
     <step>
      <para>
       Verificare che l&apos;estensione <package>Containers Module</package> sia abilitata:
      </para>
<screen>
<prompt>&gt; </prompt>SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP3 x86_64 (Activated)
</screen>
     </step>
     <step>
      <para>
       Verificare che i seguenti pacchetti siano installati: <package>apache2-utils</package> (se si abilita un registro sicuro), <package>cni</package>, <package>cni-plugins</package>, <package>podman</package>, <package>podman-cni-config</package> e <package>skopeo</package>.
      </para>
     </step>
     <step>
      <para>
       Recuperare le seguenti informazioni:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Il nome di dominio completo dell&apos;host del registro (<option>REG_HOST_FQDN</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Un numero di porta disponibile utilizzabile per mappare la porta 5000 del container del registro (<option>REG_HOST_PORT</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Se il registro sarà sicuro o non sicuro (<option>insecure=[true|false]</option>).
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Per avviare un registro non sicuro (senza cifratura SSL), seguire la procedura indicata di seguito:
      </para>
      <substeps>
       <step>
        <para>
         Configurare <systemitem class="resource">ceph-salt</systemitem> per il registro non sicuro:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf enable
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf/registries \
 add prefix=<option>REG_HOST_FQDN</option> insecure=true \
 location=<option>REG_HOST_PORT</option>:5000
</screen>
       </step>
       <step>
        <para>
         Avviare il registro non sicuro creando la directory necessaria (ad esempio, <filename>/var/lib/registry</filename>) e avviando il registro con il comando <command>podman</command>:
        </para>
<screen>
<prompt role="root"># </prompt>mkdir -p /var/lib/registry
<prompt role="root"># </prompt>podman run --privileged -d --name registry \
 -p <option>REG_HOST_PORT</option>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
       </step>
       <step>
        <para>
         Per avviare il registro dopo un riavvio, creare un file di unità <systemitem class="daemon">systemd</systemitem> per il registro e abilitarlo:
        </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> podman generate systemd --files --name registry
<prompt>&gt; </prompt><command>sudo</command> mv container-registry.service /etc/systemd/system/
<prompt>&gt; </prompt><command>sudo</command> systemctl enable container-registry.service
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Per avviare un registro sicuro, seguire la procedura indicata di seguito:
      </para>
      <substeps>
       <step>
        <para>
         Creare le directory necessarie:
        </para>
<screen><prompt role="root"># </prompt>mkdir -p /var/lib/registry/{auth,certs}</screen>
       </step>
       <step>
        <para>
         Generare un certificato SSL:
        </para>
<screen>
<prompt role="root"># </prompt>openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt
</screen>
        <note>
         <para>
          Impostare il valore di <literal>CN=[value]</literal> sul nome di dominio completo dell&apos;host ([<option>REG_HOST_FQDN</option>]).
         </para>
        </note>
       </step>
       <step>
        <para>
         Copiare il certificato su tutti i nodi del cluster e aggiornare la cache del certificato:
        </para>
<screen>
<prompt role="root"># </prompt>salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
<prompt role="root"># </prompt>salt '*' cmd.shell "update-ca-certificates"
</screen>
       </step>
       <step>
        <para>
         Generare una combinazione di nome utente e password per l&apos;autenticazione al registro:
        </para>
<screen>
<prompt role="root"># </prompt>htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <option>REG_USERNAME</option> <option>REG_PASSWORD</option>
</screen>
       </step>
       <step>
        <para>
         Avviare il registro sicuro. Utilizzare il flag <option>REGISTRY_STORAGE_DELETE_ENABLED=true</option> per poter eliminare in seguito le immagini con il comando <command>skopeo delete</command>.
        </para>
<screen>
podman run --name myregistry -p <option>REG_HOST_PORT</option>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_STORAGE_DELETE_ENABLED=true \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
       </step>
       <step>
        <para>
         Effettuare un test dell&apos;accesso sicuro al registro:
        </para>
<screen>
<prompt>&gt; </prompt>curl https://<option>REG_HOST_FQDN</option>:<option>REG_HOST_PORT</option>/v2/_catalog \
 -u <option>REG_USERNAME</option>:<option>REG_PASSWORD</option>
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Dopo aver creato il registro locale, è necessario sincronizzare le immagini del container dal registro ufficiale SUSE, disponibile all&apos;indirizzo <literal>registry.suse.com</literal> a quello locale. A tale scopo, è possibile utilizzare il comando <command>skopeo sync</command> disponibile nel pacchetto <package>skopeo</package>. Per ulteriori dettagli, fare riferimento alla documentazione (<command>man 1 skopeo-sync</command>). Si considerino i seguenti esempi:
      </para>
      <example>
       <title>visualizzazione dei file manifest</title>
<screen>
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/ceph | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/grafana | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 | jq .RepoTags
</screen>
      </example>
      <example>
       <title>sincronizzazione con una directory</title>
       <para>
        Sincronizzare tutte le immagini Ceph:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph /root/images/</screen>
       <para>
        Sincronizzare le sole immagini più recenti:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph:latest /root/images/</screen>
      </example>
      <example>
       <title>sincronizzazione delle immagini Grafana:</title>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana /root/images/</screen>
       <para>
        Sincronizzare le sole immagini Grafana più recenti:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana:latest /root/images/</screen>
      </example>
      <example>
       <title>sincronizzazione delle immagini Prometheus più recenti</title>
<screen>
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 /root/images/
</screen>
      </example>
     </step>
    </procedure>
    <procedure>
     <title>configurazione del registro locale e delle credenziali di accesso</title>
     <step>
      <para>
       Configurare l&apos;URL del registro locale:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/registry set <replaceable>REG_HOST_URL</replaceable></screen>
     </step>
     <step>
      <para>
       Configurare il nome utente e la password per accedere al registro locale:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable></screen>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable></screen>
     </step>
    </procedure>
    <tip>
     <title>cache del registro</title>
     <para>
      Per evitare di ripetere la sincronizzazione del registro locale quando sono presenti nuovi container aggiornati, è possibile configurare una <emphasis>cache del registro</emphasis>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configurazione del percorso alle immagini del container</title>
    <important>
     <para>
      Questa sezione consente di configurare il percorso delle immagini del container del cluster di bootstrap (distribuzione della prima coppia Ceph Monitor e Ceph Manager). Il percorso non si applica alle immagini del container di servizi aggiuntivi, ad esempio dello stack di monitoraggio.
     </para>
    </important>
    <tip>
     <title>configurazione del proxy HTTPS</title>
     <para>
      Se è necessario utilizzare un proxy per comunicare con il server del registro del container, eseguire la seguente procedura di configurazione su tutti i nodi del cluster:
     </para>
     <procedure>
      <step>
       <para>
        Copiare il file di configurazione per i container:
       </para>
<screen><prompt>&gt; </prompt><command>sudo</command> cp /usr/share/containers/containers.conf /etc/containers/containers.conf</screen>
      </step>
      <step>
       <para>
        Modificare il file appena copiato e aggiungi l&apos;impostazione <option>http_proxy</option> alla rispettiva sezione <literal>[engine]</literal>; ad esempio:
       </para>
<screen><prompt>&gt; </prompt>cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]
 </screen>
      </step>
     </procedure>
    </tip>
    <para>
     È necessario che cephadm conosca un percorso URI valido per le immagini del container. Verificare l&apos;impostazione di default eseguendo il comando:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
    <para>
     Se non è necessario un registro locale o alternativo, specificare il registro del container SUSE di default:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7.1/ceph/ceph</screen>
    <para>
     Se la distribuzione richiede un percorso specifico, ad esempio un percorso a un registro locale, configurarlo nel modo seguente:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-inflight-encryption">
   <title>Abilitazione della cifratura in esecuzione dei dati (msgr2)</title>
   <para>
    Il protocollo Messenger v2 (MSGR2) è il protocollo cablato di Ceph. Fornisce una modalità di sicurezza che cifra tutti i dati in transito sulla rete, l&apos;incapsulamento dei payload di autenticazione e l&apos;abilitazione dell&apos;integrazione futura delle nuove modalità di autenticazione (come Kerberos).
   </para>
   <important>
    <para>
     msgr2 non è attualmente supportato dai client Ceph del kernel Linux, come CephFS e il dispositivo di blocco RADOS (RADOS Block Device, RBD).
    </para>
   </important>
   <para>
    I daemon Ceph possono eseguire l&apos;associazione a più porte, consentendo ai client Ceph esistenti e ai nuovi client abilitati per v2 di connettersi allo stesso cluster. Per default, i MON eseguono adesso l&apos;associazione alla nuova porta 3300 con assegnazione IANA (CE4h o 0xCE4) per il nuovo protocollo v2, oltre all&apos;associazione alla precedente porta 6789 di default per il protocollo v1 legacy.
   </para>
   <para>
    Il protocollo v2 (MSGR2) supporta due modalità di connessione:
   </para>
   <variablelist>
    <varlistentry>
     <term>crc mode</term>
     <listitem>
      <para>
       Un&apos;autenticazione iniziale sicura quando viene stabilita la connessione e un controllo dell&apos;integrità CRC32.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>secure mode</term>
     <listitem>
      <para>
       Un&apos;autenticazione iniziale sicura quando viene stabilita la connessione e una cifratura completa di tutto il traffico post-autenticazione, incluso un controllo dell&apos;integrità crittografico.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Per la maggior parte delle connessioni, sono disponibili opzioni per controllare le modalità da utilizzare:
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_cluster_mode</term>
     <listitem>
      <para>
       La modalità di connessione (o le modalità consentite) utilizzata per la comunicazione interna al cluster tra i daemon Ceph. Se sono elencate più modalità, sono preferite quelle nelle prime posizioni dell&apos;elenco.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_service_mode</term>
     <listitem>
      <para>
       Un elenco delle modalità consentite che i client possono utilizzare durante la connessione al cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_client_mode</term>
     <listitem>
      <para>
       Un elenco di modalità di connessione, in ordine di preferenza, che i client possono utilizzare (o consentire) durante la comunicazione con un cluster Ceph.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    È presente un insieme di opzioni parallele che si applica specificamente ai monitor e che consente agli amministratori di impostare requisiti diversi (in genere più sicuri) per la comunicazione con i monitor.
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_mon_cluster_mode</term>
     <listitem>
      <para>
       La modalità di connessione (o le modalità consentite) da utilizzare tra i monitor.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_service_mode</term>
     <listitem>
      <para>
       Un elenco delle modalità consentite che i client o altri daemon Ceph possono utilizzare durante la connessione ai monitor.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_client_mode</term>
     <listitem>
      <para>
       Un elenco delle modalità di connessione, in ordine di preferenza, che i client o i daemon diversi dai monitor possono utilizzare durante la connessione ai monitor.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Per abilitare la modalità di cifratura MSGR2 durante la distribuzione, è necessario aggiungere delle opzioni di configurazione alla configurazione <systemitem class="resource">ceph-salt</systemitem> prima di eseguire <command>ceph-salt apply</command>.
   </para>
   <para>
    Per utilizzare la modalità <literal>secure</literal>, eseguire i comandi seguenti.
   </para>
   <para>
    Aggiungere la sezione globale a <filename>ceph_conf</filename> nello strumento di configurazione <systemitem class="resource">ceph-salt</systemitem>:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
   <para>
    Impostare le opzioni seguenti:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
   <note>
    <para>
     Assicurarsi che <literal>secure</literal> preceda <literal>crc</literal>.
    </para>
   </note>
   <para>
    Per <emphasis>forzare la modalità</emphasis> <literal> secure</literal>, eseguire i comandi seguenti:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
   <tip xml:id="update-inflight-encryption-settings">
    <title>aggiornamento delle impostazioni</title>
    <para>
     Se si desidera modificare le impostazioni riportate sopra, impostare le modifiche alla configurazione nell&apos;archivio di configurazione del monitor. A questo scopo, utilizzare il comando <command>ceph config set</command>.
    </para>
<screen><prompt>root@master # </prompt>ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
    <para>
     Esempio:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global ms_cluster_mode "secure crc"</screen>
    <para>
     Se si desidera selezionare il valore attuale, incluso quello di default, eseguire il comando seguente:
    </para>
<screen><prompt>root@master # </prompt>ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
    <para>
     Ad esempio, per attivare la <literal>ms_cluster_mode</literal> per gli OSD, eseguire:
    </para>
<screen><prompt>root@master # </prompt>ceph config get osd ms_cluster_mode</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-enable-network">
   <title>Configurazione della rete di cluster</title>
   <para>
    Se si esegue una rete di cluster separata, potrebbe essere necessario impostare l&apos;indirizzo IP della rete di cluster seguito dalla porzione della maschera di sottorete dopo il simbolo della barra, ad esempio <literal>192.168.10.22/24</literal>.
   </para>
   <para>
    Eseguire i comandi seguenti per abilitare <literal>cluster_network</literal>:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-verify">
   <title>Verifica della configurazione del cluster</title>
   <para>
    La configurazione minima del cluster è stata completata. Analizzarla per individuare errori evidenti:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .............. [registry.suse.com/ses/7.1/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
   <tip>
    <title>stato della configurazione del cluster</title>
    <para>
     È possibile verificare la validità della configurazione del cluster eseguendo il comando seguente:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-export">
   <title>Esportazione delle configurazioni del cluster</title>
   <para>
    Dopo aver configurato il cluster di base e averne verificato la validità della configurazione, è consigliabile esportare tale configurazione in un file:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt export &gt; cluster.json</screen>
   <warning>
    <para>
     L&apos;output dell&apos;esportazione <command>ceph-salt export</command> include la chiave privata SSH. In caso di dubbi sulle implicazioni di sicurezza, non eseguire questo comando senza le appropriate precauzioni.
    </para>
   </warning>
   <para>
    Se si dovesse interrompere la configurazione del cluster e fosse necessario ripristinarla a uno stato di backup precedente, eseguire:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt import cluster.json</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-deploy">
  <title>Aggiornamento dei nodi e bootstrap del cluster minimo</title>

  <para>
   Prima di distribuire il cluster, aggiornare tutti i pacchetti software su tutti i nodi:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update</screen>

  <para>
   Se durante l&apos;aggiornamento un nodo restituisce il messaggio che informa che <literal>è necessario eseguire il riavvio</literal>, vuol dire che i pacchetti importanti del sistema operativo (come il kernel) sono stati aggiornati a una versione più recente ed è necessario riavviare il nodo per applicare le modifiche.
  </para>

  <para>
   Per riavviare tutti i nodi pertinenti, aggiungere l&apos;opzione <option>--reboot</option>
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update --reboot</screen>

  <para>
   Oppure, riavviarli in un passaggio separato:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt reboot</screen>

  <important>
   <para>
    Il Salt Master non viene mai riavviato dai comandi <command>ceph-salt update --reboot</command> o <command>ceph-salt reboot</command>. Se è necessario riavviare il Salt Master, occorre procedere manualmente.
   </para>
  </important>

  <para>
   In seguito all&apos;aggiornamento dei nodi, eseguire il bootstrap del cluster minimo:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt apply</screen>

  <note>
   <para>
    Al termine del bootstrap, sul cluster saranno presenti un Ceph Monitor e un Ceph Manager.
   </para>
  </note>

  <para>
   Il comando riportato sopra consentirà di aprire un&apos;interfaccia utente interattiva in cui è mostrato l&apos;avanzamento di ogni minion.
  </para>

  <figure>
   <title>Distribuzione di un cluster minimo</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>modalità non interattiva</title>
   <para>
    Se è necessario applicare la configurazione da uno script, è disponibile anche una modalità di distribuzione non interattiva, utile anche quando il cluster viene distribuito da un computer remoto, per evitare le distrazioni causate dall&apos;aggiornamento continuo delle informazioni di avanzamento sulla schermata online:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt apply --non-interactive</screen>
  </tip>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-final-steps">
  <title>Revisione dei passaggi finali</title>

  <para>
   Al termine dell&apos;esecuzione del comando <command>ceph-salt apply</command>, dovrebbe essere presente un Ceph Monitor e un Ceph Manager. Dovrebbe essere possibile eseguire correttamente il comando <command>ceph status</command> su uno dei minion a cui è stato assegnato il ruolo <literal>admin</literal> come <literal>root</literal> o utente <literal>cephadm</literal> tramite <literal>sudo</literal>.
  </para>

  <para>
   I passaggi successivi riguardano l&apos;uso di cephadm per la distribuzione di Ceph Monitor, Ceph Manager, OSD, stack di monitoraggio e gateway aggiuntivi.
  </para>

  <para>
   Prima di continuare, rivedere le nuove impostazioni di rete del cluster. A questo punto, l&apos;impostazione <literal>public_network</literal> è stata popolata in base ai valori immessi per <literal>/cephadm_bootstrap/mon_ip</literal> nella configurazione <literal>ceph-salt</literal>. Tuttavia, questa impostazione è stata applicata soltanto a Ceph Monitor. È possibile rivederla con il comando seguente:
  </para>

<screen><prompt>root@master # </prompt>ceph config get mon public_network</screen>

  <para>
   Si tratta della configurazione minima richiesta per il funzionamento di Ceph, ma si consiglia di configurare l&apos;impostazione <literal>public_network</literal> come <literal>global</literal>, ovvero di applicarla a tutti i tipi di daemon Ceph e non soltanto ai MON:
  </para>

<screen><prompt>root@master # </prompt>ceph config set global public_network "$(ceph config get mon public_network)"</screen>

  <note>
   <para>
    Questo passaggio non è obbligatorio. Tuttavia, se si sceglie di non utilizzare questa impostazione, i Ceph OSD e altri daemon (ad eccezione di Ceph Monitor) resteranno in ascolto su <emphasis>tutti gli indirizzi</emphasis>.
   </para>
   <para>
    Se si desidera che gli OSD comunichino tra di loro su una rete completamente separata, eseguire il comando seguente:
   </para>
<screen><prompt>root@master # </prompt>ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
   <para>
    Eseguendo questo comando, gli OSD creati nella distribuzione utilizzeranno fin dall&apos;inizio la rete di cluster designata.
   </para>
  </note>

  <para>
   Se sono stati impostati nodi dense per il cluster (più di 62 OSD per host), assicurarsi di assegnare un numero sufficiente di porte ai Ceph OSD. L&apos;intervallo di default (6800-7300) attualmente non consente più di 62 OSD per host. Per un cluster con nodi dense, regolare l&apos;impostazione <literal>ms_bind_port_max</literal> su un valore adeguato. Ogni OSD consumerà otto porte aggiuntive. Ad esempio, per un host impostato sull&apos;esecuzione di 96 OSD, saranno necessarie 768 porte. L&apos;impostazione <literal>ms_bind_port_max</literal> deve essere configurata su almeno 7568 tramite l&apos;esecuzione del comando seguente:
  </para>

<screen><prompt>root@master # </prompt>ceph config set osd.* ms_bind_port_max 7568</screen>

  <para>
   A questo scopo, è necessario regolare le impostazioni del firewall di conseguenza. Consultare <xref linkend="storage-bp-net-firewall"/> per maggiori informazioni.
  </para>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-disable-insecure">
  <title>Disabilitazione dei client non sicuri</title>

  <para>
   Dalla versione v15.2.11 di Pacific, è stato introdotto un nuovo avviso sullo stato di integrità che informa che i client non sicuri possono unirsi al cluster. Per default, questo avviso è <emphasis>attivo</emphasis>. Il Ceph Dashboard mostrerà il cluster nello stato <literal>HEALTH_WARN</literal> e una verifica dello stato del cluster dalla riga di comando fornisce le seguenti informazioni:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>

  <para>
   L&apos;avviso indica che i Ceph Monitor stanno ancora consentendo ai client meno recenti, e privi di patch, di connettersi al cluster. Questo assicura la possibilità di connessione dei client esistenti durante l&apos;upgrade del cluster, ma segnala la presenza di un problema che deve essere risolto. Dopo aver completato l&apos;upgrade del cluster e di tutti i client all&apos;ultima versione di Ceph, disattivare i client privi di patch mediante il seguente comando:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
 </sect1>
</chapter>
