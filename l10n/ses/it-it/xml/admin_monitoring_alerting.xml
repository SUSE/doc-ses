<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_monitoring_alerting.xml" version="5.0" xml:id="monitoring-alerting">
 <title>Monitoraggio e creazione di avvisi</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In SUSE Enterprise Storage 7.1, cephadm esegue la distribuzione di uno stack di monitoraggio e creazione di avvisi. Gli utenti devono definire i servizi (ad esempio Prometheus, Alertmanager e Grafana) da distribuire con cephadm in un file di configurazione YAML oppure distribuirli tramite l&apos;interfaccia riga di comando. Se sono distribuiti più servizi dello stesso tipo, viene eseguita una configurazione ad elevata disponibilità. L&apos;utilità di esportazione dei nodi rappresenta un&apos;eccezione a questa regola.
 </para>
 <para>
  Tramite cephadm è possibile distribuire i servizi di monitoraggio seguenti:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis role="bold">Prometheus</emphasis> è il kit di strumenti per la creazione di avvisi e il monitoraggio. Raccoglie i dati forniti dalle utilità di esportazione di Prometheus e genera avvisi preconfigurati se vengono raggiunte delle soglie predefinite.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Alertmanager</emphasis> gestisce gli avvisi inviati dal server Prometheus. Deduplica, raggruppa e instrada gli avvisi al ricevitore corretto. Per default, il Ceph Dashboard sarà configurato automaticamente come ricevitore.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Grafana</emphasis> è il software di visualizzazione e creazione di avvisi. La funzionalità di creazione di avvisi di Grafana non è utilizzata da questo stack di monitoraggio. Per la creazione di avvisi, è utilizzato Alertmanager.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis role="bold">Node exporter</emphasis> è un&apos;utilità di esportazione per Prometheus che fornisce i dati relativi al nodo su cui è installato. Si consiglia di installare questa utilità di esportazione dei nodi su tutti i nodi.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Il modulo Manager di Prometheus fornisce un&apos;utilità di esportazione Prometheus da passare ai contatori delle prestazioni di Ceph dal punto di raccolta in <literal>ceph-mgr</literal>.
 </para>
 <para>
  La configurazione di Prometheus, incluse le destinazioni di <emphasis>scrape</emphasis> (daemon che forniscono metriche), viene impostata automaticamente da cephadm. cephadm inoltre distribuisce un elenco di avvisi di default, ad esempio <literal>health error</literal>, <literal>10% OSDs down</literal> o <literal>pgs inactive</literal>.
 </para>
 <para>
  Per default, il traffico verso Grafana è cifrato tramite TLS. È possibile specificare un proprio certificato TLS o utilizzarne uno autofirmato. Se prima della distribuzione di Grafana non è stato configurato nessun certificato personalizzato, ne verrà creato e configurato automaticamente uno autofirmato per Grafana.
 </para>
 <para>
  È possibile configurare i certificati personalizzati per Grafana seguendo la procedura indicata di seguito:
 </para>
 <procedure>
  <step>
   <para>
    Configurare i file del certificato:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt> ceph config-key set mgr/cephadm/grafana_key -i $PWD/key.pem
<prompt>cephuser@adm &gt; </prompt> ceph config-key set mgr/cephadm/grafana_crt -i $PWD/certificate.pem
</screen>
  </step>
  <step>
   <para>
    Riavviare il servizio Ceph Manager:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart mgr</screen>
  </step>
  <step>
   <para>
    Riconfigurare il servizio Grafana in modo che rifletta i nuovi percorsi del certificato e impostare l&apos;URL corretto per Ceph Dashboard:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch reconfig grafana</screen>
  </step>
 </procedure>
 <para>
  Alertmanager gestisce gli avvisi inviati dal server Prometheus. Si occupa di deduplicarli, raggrupparli e instradarli al ricevitore corretto. È possibile silenziare gli eventi tramite Alertmanager, ma è possibile gestire i silenziamenti anche dal Ceph Dashboard.
 </para>
 <para>
  Si consiglia di distribuire l&apos;utilità <systemitem class="daemon">Node exporter</systemitem> su tutti i nodi. A questo scopo, utilizzare il file <filename>monitoring.yaml</filename> con il tipo di servizio <literal>node-exporter</literal>. Per ulteriori informazioni sulla distribuzione dei servizi, vedere <xref linkend="deploy-cephadm-day2-service-monitoring"/>.
 </para>
 <sect1 xml:id="monitoring-custom-images">
  <title>Configurazione di immagini personalizzate o locali</title>

  <tip>
   <para>
    Questa sezione descrive come modificare la configurazione delle immagini del container utilizzate durante la distribuzione o l&apos;aggiornamento dei servizi. Non include i comandi da eseguire per la distribuzione o la ridistribuzione dei servizi.
   </para>
   <para>
    Il metodo consigliato per la distribuzione dello stack di monitoraggio consiste nell&apos;applicarne la specifica come descritto in <xref linkend="deploy-cephadm-day2-service-monitoring"/>.
   </para>
  </tip>

  <para>
   Per distribuire le immagini del container personalizzate o locali, è necessario impostare tali immagini in cephadm. A questo scopo, è necessario eseguire il comando seguente:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable> <replaceable>VALUE</replaceable></screen>

  <para>
   Dove <replaceable>OPTION_NAME</replaceable> è uno dei nomi seguenti:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     container_image_prometheus
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_node_exporter
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_alertmanager
    </para>
   </listitem>
   <listitem>
    <para>
     container_image_grafana
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Se non viene impostata alcuna opzione o se l&apos;impostazione è stata rimossa, come <replaceable>VALUE</replaceable> vengono utilizzate le immagini seguenti:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0
    </para>
   </listitem>
   <listitem>
    <para>
     registry.suse.com/ses/7.1/ceph/grafana:7.5.12
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Esempio:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/cephadm/container_image_prometheus prom/prometheus:v1.4.1</screen>

  <note>
   <para>
    Se viene impostata un&apos;immagine personalizzata, il valore di default verrà sostituito (ma non sovrascritto). Il valore di default cambia quando sono disponibili degli aggiornamenti. Se si imposta un&apos;immagine personalizzata, non sarà possibile aggiornare automaticamente il componente per cui è stata impostata tale immagine. Sarà necessario aggiornare manualmente la configurazione (tag e nome immagine) per poter installare gli aggiornamenti.
   </para>
   <para>
    Se si sceglie di seguire invece le raccomandazioni, sarà possibile reimpostare l&apos;immagine personalizzata impostata in precedenza. Dopodiché, il valore di default verrà utilizzato nuovamente. Utilizzare <command>ceph config rm</command> per reimpostare l&apos;opzione di configurazione:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm mgr mgr/cephadm/<replaceable>OPTION_NAME</replaceable></screen>
   <para>
    Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm mgr mgr/cephadm/container_image_prometheus</screen>
  </note>
 </sect1>
 <sect1 xml:id="monitoring-applying-updates">
  <title>Aggiornamento dei servizi di monitoraggio</title>

  <para>
   Come accennato nella <xref linkend="monitoring-custom-images"/>, cephadm è fornito con gli URL delle immagini del container consigliate e testate, che sono utilizzate per default.
  </para>

  <para>
   Con l&apos;aggiornamento dei pacchetti Ceph, potrebbero essere disponibili nuove versioni di tali URL. Questo aggiornamento si applica solo alla posizione da cui viene eseguito il pull delle immagini del container e non vale per i servizi.
  </para>

  <para>
   In seguito all&apos;aggiornamento degli URL delle nuove immagini del container (manualmente come descritto nella <xref linkend="monitoring-custom-images"/> o automaticamente tramite l&apos;aggiornamento del pacchetto Ceph), è possibile aggiornare i servizi di monitoraggio.
  </para>

  <para>
   A questo scopo, utilizzare <command>ceph orch reconfig</command> nel modo seguente:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch reconfig node-exporter
<prompt>cephuser@adm &gt; </prompt>ceph orch reconfig prometheus
<prompt>cephuser@adm &gt; </prompt>ceph orch reconfig alertmanager
<prompt>cephuser@adm &gt; </prompt>ceph orch reconfig grafana
</screen>

  <para>
   Al momento, non esiste nessun comando singolo che consenta di aggiornare tutti i servizi di monitoraggio. L&apos;ordine in cui i servizi vengono aggiornati non ha importanza.
  </para>

  <note>
   <para>
    Se si utilizzano immagini del container personalizzate, gli URL specificati per i servizi di monitoraggio non cambieranno automaticamente se i pacchetti Ceph vengono aggiornati. Se sono state specificate immagini del container personalizzate, sarà necessario immettere manualmente gli URL delle nuove immagini del container. Ad esempio, ciò si applica nel caso in cui si utilizzi un registro del container locale.
   </para>
   <para>
    Alla <xref linkend="monitoring-custom-images"/>, è possibile trovare gli URL delle immagini del container consigliate per l&apos;utilizzo.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitoring-stack-disable">
  <title>Disabilitazione del monitoraggio</title>

  <para>
   Per disabilitare lo stack di monitoraggio, eseguire i comandi seguenti:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch rm grafana
<prompt>cephuser@adm &gt; </prompt>ceph orch rm prometheus --force   # this will delete metrics data collected so far
<prompt>cephuser@adm &gt; </prompt>ceph orch rm node-exporter
<prompt>cephuser@adm &gt; </prompt>ceph orch rm alertmanager
<prompt>cephuser@adm &gt; </prompt>ceph mgr module disable prometheus
      </screen>
 </sect1>
 <sect1 xml:id="monitoring-grafana-config">
  <title>Configurazione di Grafana</title>

  <para>
   Per poter verificare la presenza dei dashboard di Grafana prima che questi vengano caricati dal front-end, il back-end del Ceph Dashboard richiede l&apos;URL Grafana. A causa del metodo applicato per l&apos;implementazione di Grafana nel Ceph Dashboard, ciò vuol dire che sono necessarie due connessioni attive per poter visualizzare i grafici di Grafana nel Ceph Dashboard:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Il back-end (modulo Ceph MGR) deve verificare l&apos;esistenza del grafico richiesto. Se la richiesta va a buon fine, comunica al front-end che può accedere a Grafana in sicurezza.
    </para>
   </listitem>
   <listitem>
    <para>
     Il front-end richiede quindi i grafici di Grafana direttamente dal browser dell&apos;utente tramite un <literal>iframe</literal>. L&apos;accesso all&apos;istanza di Grafana viene eseguito direttamente dal Ceph Dashboard senza deviazioni.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A questo punto, l&apos;ambiente potrebbe rendere difficile al browser dell&apos;utente l&apos;accesso diretto all&apos;URL configurato nel Ceph Dashboard. Per risolvere questo problema, è possibile configurare un URL separato che verrà utilizzato esclusivamente per comunicare al front-end (il browser dell&apos;utente) quale URL utilizzare per accedere a Grafana.
  </para>

  <para>
   Per modificare l&apos;URL restituito al front-end, immettere il comando seguente:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard set-grafana-frontend-api-url <replaceable>GRAFANA-SERVER-URL</replaceable></screen>

  <para>
   Se per questa opzione non è impostato alcun valore, verrà semplicemente eseguito il fallback al valore dell&apos;opzione <replaceable>GRAFANA_API_URL</replaceable>, impostato automaticamente e aggiornato su base periodica da cephadm. Se è presente un valore impostato, il browser riceverà l&apos;istruzione di utilizzare questo URL per accedere a Grafana.
  </para>
 </sect1>
 <sect1 xml:id="monitoring-cephadm-config">
  <title>Configurazione del modulo Manager di Prometheus</title>

  <para>
   Il modulo Manager di Prometheus è un modulo interno di Ceph che ne estende le funzionalità. Il modulo legge i (meta)dati di Ceph relativi allo stato e all&apos;integrità e fornisce a Prometheus tali dati (sottoposti a scrape) in un formato utilizzabile.
  </para>

  <note>
   <para>
    Per applicare le modifiche alla configurazione, è necessario riavviare il modulo Manager di Prometheus.
   </para>
  </note>

  <sect2 xml:id="monitoring-http-requests">
   <title>Configurazione dell&apos;interfaccia di rete</title>
   <para>
    Per default, il modulo Manager di Prometheus accetta le richieste HTTP sulla porta 9283 su tutti gli indirizzi IPv4 e IPv6 sull&apos;host. È possibile configurare la porta e l&apos;indirizzo di ascolto con <option>ceph config-key set</option>, con le chiavi <option>mgr/prometheus/server_addr</option> e <option>mgr/prometheus/server_port</option>. Questa porta è registrata nel registro di Prometheus.
   </para>
   <para>
    Per aggiornare il valore di <literal>server_addr</literal>, eseguire il comando seguente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/server_addr <replaceable>0.0.0.0</replaceable>
      </screen>
   <para>
    Per aggiornare il valore di <literal>server_port</literal>, eseguire il comando seguente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/server_port <replaceable>9283</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-scrape-intervals">
   <title>Configurazione del valore di <literal>scrape_interval</literal></title>
   <para>
    Per default, il modulo Manager di Prometheus è configurato su un intervallo di scraping di 15 secondi. Si sconsiglia di utilizzare un intervallo di scraping inferiore a 10 secondi. Per impostare un diverso intervallo di scraping nel modulo di Prometheus, impostare <literal>scrape_interval</literal> sul valore desiderato:
   </para>
   <important>
    <para>
     Per garantire il corretto funzionamento del modulo, il relativo valore di <literal>scrape_interval</literal> impostato deve corrispondere sempre all&apos;intervallo di scraping di Prometheus.
    </para>
   </important>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/scrape_interval <replaceable>15</replaceable>
      </screen>
  </sect2>

  <sect2 xml:id="monitoring-stale-cache">
   <title>Configurazione della cache</title>
   <para>
    Sui cluster di grandi dimensioni (più di 1000 OSD), può essere necessario molto tempo per completare il recupero delle metriche. Senza la cache, il modulo Manager di Prometheus potrebbe sovraccaricare il manager e causare l&apos;arresto anomalo o il blocco delle istanze di Ceph Manager. Di conseguenza, la cache è abilitata per default e non è possibile disabilitarla, ma ciò implica che può diventare obsoleta. La cache è considerata obsoleta quando l&apos;intervallo di tempo impiegato per il recupero delle metriche da Ceph supera il valore impostato per <literal>scrape_interval</literal>.
   </para>
   <para>
    In questo caso, viene registrato un avviso e il modulo reagirà in uno dei modi seguenti:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Risponderà con un codice di stato 503 HTTP (servizio non disponibile).
     </para>
    </listitem>
    <listitem>
     <para>
      Restituirà i contenuti della cache, anche se potrebbero essere obsoleti.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Questo comportamento può essere configurato con i comandi <command>ceph config set</command>.
   </para>
   <para>
    Per inviare al modulo l&apos;istruzione di rispondere con i dati verosimilmente obsoleti, impostarlo su <literal>return</literal>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/stale_cache_strategy return</screen>
   <para>
    Per inviare al modulo l&apos;istruzione di rispondere con l&apos;errore <literal>service unavailable</literal>, impostarlo su <literal>fail</literal>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/stale_cache_strategy fail</screen>
  </sect2>

  <sect2 xml:id="monitoring-rbd-image">
   <title>Abilitazione del monitoraggio dell&apos;immagine RBD</title>
   <para>
    Tramite l&apos;abilitazione dei contatori delle prestazioni dinamiche dell&apos;OSD, il modulo Manager di Prometheus può facoltativamente raccogliere le statistiche I/O per ogni singola immagine RBD. Tali statistiche vengono raccolte per tutte le immagini all&apos;interno dei pool specificati nel parametro di configurazione <literal>mgr/prometheus/rbd_stats_pools</literal>.
   </para>
   <para>
    Il parametro è un elenco di voci <literal>pool[/namespace]</literal> separate da virgole o spazi. Se lo spazio dei nomi non è specificato, vengono raccolte le statistiche relative a tutti gli spazi dei nomi nel pool.
   </para>
   <para>
    Esempio:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/rbd_stats_pools "<replaceable>pool1,pool2,poolN</replaceable>"
      </screen>
   <para>
    Il modulo effettua la scansione dei pool e degli spazi dei nomi specificati e crea un elenco di tutte le immagini disponibili e lo aggiorna periodicamente. È possibile configurare l&apos;intervallo tramite il parametro <literal>mgr/prometheus/rbd_stats_pools_refresh_interval</literal> (secondi), impostato per default su 300 secondi (cinque minuti).
   </para>
   <para>
    Ad esempio, se l&apos;intervallo di sincronizzazione è stato modificato in 10 minuti:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/prometheus/rbd_stats_pools_refresh_interval <replaceable>600</replaceable>
      </screen>
  </sect2>
 </sect1>
 <sect1 xml:id="prometheus-security-model">
  <title>Modello di sicurezza di Prometheus</title>

  <para>
   Il modello di sicurezza di Prometheus presume che utenti non attendibili dispongano dell&apos;accesso ai log e all&apos;endpoint HTTP di Prometheus. Gli utenti non attendibili dispongono dell&apos;accesso a tutti i (meta)dati raccolti da Prometheus e contenuti nel database, oltre a una serie di informazioni operative e di debug.
  </para>

  <para>
   Tuttavia l&apos;API HTTP di Prometheus è limitata alle operazioni di sola lettura. Non è possibile modificare le configurazioni utilizzando l&apos;API e i segreti non vengono esposti. Inoltre, Prometheus dispone di misure integrate per attenuare l&apos;impatto degli attacchi Denial of Service.
  </para>
 </sect1>
 <sect1 xml:id="prometheus-webhook-snmp">
  <title>Gateway SNMP di Prometheus Alertmanager</title>

  <para>
   Se si desidera ricevere notifiche sugli avvisi di Prometheus tramite trap SNMP, è possibile installare il gateway SNMP di Prometheus Alertmanager tramite cephadm o il Ceph Dashboard. A tale scopo, ad esempio per SNMPv2c, è necessario creare un file della specifica del posizionamento e del servizio con i contenuti seguenti:
  </para>

  <note>
   <para>
    Per ulteriori informazioni sui file del servizio e del posizionamento, vedere <xref linkend="cephadm-service-and-placement-specs"/>.
   </para>
  </note>

<screen>
service_type: snmp-gateway
service_name: snmp-gateway
placement:
    <replaceable>ADD_PLACEMENT_HERE</replaceable>
spec:
  credentials:
    snmp_community: <replaceable>ADD_COMMUNITY_STRING_HERE</replaceable>
  snmp_destination: <replaceable>ADD_FQDN_HERE</replaceable>:<replaceable>ADD_PORT_HERE</replaceable>
  snmp_version: V2c
</screen>

  <para>
   In alternativa, è possibile utilizzare il Ceph Dashboard per distribuire il servizio del gateway SNMP per SNMPv2c e SNMPv3. Per ulteriori informazioni, fare riferimento al <xref linkend="dashboard-cluster-services"/>.
  </para>
 </sect1>
</chapter>
