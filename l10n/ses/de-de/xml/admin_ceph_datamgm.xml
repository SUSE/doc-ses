<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_datamgm.xml" version="5.0" xml:id="cha-storage-datamgm">
 <title>Verwaltung gespeicherter Daten</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Der CRUSH-Algorithmus bestimmt, wie Daten gespeichert und abgerufen werden, indem er die Datenspeicherorte berechnet. CRUSH ist die Grundlage für die direkte Kommunikation der Ceph-Clients mit OSDs, ansonsten müssten sie über einen zentralen Server oder Broker kommunizieren. Mit einer algorithmisch festgelegten Methode zum Speichern und Abrufen von Daten vermeidet Ceph einen Single-Point-of-Failure, einen Leistungsengpass und eine physische Beschränkung der Skalierbarkeit.
 </para>
 <para>
  CRUSH benötigt eine Zuordnung Ihres Clusters und verwendet die CRUSH-Zuordnung zum pseudozufälligen Speichern und Abrufen von Daten in OSDs gleichmäßiger Datenverteilung im Cluster.
 </para>
 <para>
  CRUSH-Zuordnungen enthalten eine Liste von OSDs, eine Liste der „Buckets“ zum Aggregieren der Geräte an physischen Standorten sowie eine Liste der Regeln, die CRUSH anweisen, wie es Daten in den Pools eines Ceph-Clusters reproduzieren soll. Durch Widerspiegeln der zugrundeliegenden physischen Struktur der Installation kann CRUSH potenzielle Ursachen von korrelierten Gerätefehlern nachbilden und somit eine Lösung suchen. Zu den typischen Ursachen zählen die physische Umgebung, eine gemeinsame Energiequelle und ein gemeinsames Netzwerk. Durch Verschlüsseln dieser Informationen in die Cluster-Zuordnung können CRUSH-Platzierungsrichtlinien Objektreproduktionen auf verschiedene Fehlerdomänen auslagern und gleichzeitig die gewünschte Verteilung beibehalten. Um beispielsweise für den Fall gleichzeitig auftretender Fehler vorzusorgen, sollte am besten sichergestellt werden, dass sich Datenreproduktionen auf Geräten mit unterschiedlichen Ablagefächern, Racks, Netzanschlüssen, Controllern und/oder physischen Speicherorten befinden.
 </para>
 <para>
  Nach Bereitstellung eines Ceph-Clusters wird eine standardmäßige CRUSH-Zuordnung generiert. Sie eignet sich sehr gut für Ihre Ceph Sandbox-Umgebung. Wenn Sie jedoch einen sehr großen Daten-Cluster bereitstellen, sollten Sie die Entwicklung einer benutzerdefinierten CRUSH-Zuordnung ernsthaft in Erwägung ziehen, weil sie Ihnen die Verwaltung Ihres Ceph-Clusters erleichtert, die Leistung verbessert und die Datensicherheit gewährleistet.
 </para>
 <para>
  Wenn beispielsweise ein OSD ausfällt, können Sie anhand der CRUSH-Zuordnung das physische Rechenzentrum, den Raum, die Reihe und das Rack des Hosts mit dem fehlerhaften OSD finden, für den Fall, dass Sie Support vor Ort benötigen oder die Hardware austauschen müssen.
 </para>
 <para>
  Entsprechend kann CRUSH Ihnen auch dabei helfen, Fehler schneller zu finden. Wenn beispielsweise alle OSDs in einem bestimmten Rack gleichzeitig ausfallen, liegt der Fehler möglicherweise bei einem Netzwerkschalter oder der Energiezufuhr zum Rack bzw. am Netzwerkschalter statt an den OSDs selbst.
 </para>
 <para>
  Eine benutzerdefinierte CRUSH-Zuordnung kann Ihnen auch dabei helfen, die physischen Standorte zu finden, an denen Ceph redundante Kopien der Daten speichert, wenn sich die Platzierungsgruppen (siehe <xref linkend="op-pgs"/>), die mit einem fehlerhaften Host verknüpft sind, in einem eingeschränkt leistungsfähigen Zustand befinden.
 </para>
 <para>
  Eine CRUSH-Zuordnung setzt sich aus drei Hauptabschnitten zusammen.
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <xref linkend="datamgm-devices" xrefstyle="select: title"/> umfassen alle Objektspeichergeräte, die einem <systemitem>ceph-osd</systemitem>-Daemon entsprechen.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-buckets" xrefstyle="select: title"/> sind eine hierarchische Ansammlung von Speicherorten (beispielsweise Reihen, Racks, Hosts usw.) und ihr zugewiesenes Gewicht.
   </para>
  </listitem>
  <listitem>
   <para>
    <xref linkend="datamgm-rules" xrefstyle="select: title"/> bezeichnen die Art und Weise, wie Buckets ausgewählt werden.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="datamgm-devices">
  <title>OSD-Geräte</title>

  <para>
   Zum Zuordnen von Platzierungsgruppen zu OSDs benötigt eine CRUSH-Zuordnung eine Liste von OSD-Geräten (den Namen des OSD Daemons). Die Liste der Geräte erscheint in der CRUSH-Zuordnung an erster Stelle.
  </para>

<screen>#devices
device <replaceable>NUM</replaceable> osd.<replaceable>OSD_NAME</replaceable> class <replaceable>CLASS_NAME</replaceable></screen>

  <para>
   Beispiel:
  </para>

<screen>
#devices
device 0 osd.0 class hdd
device 1 osd.1 class ssd
device 2 osd.2 class nvme
device 3 osd.3 class ssd
</screen>

  <para>
   In der Regel wird ein OSD-Daemon einer einzelnen Festplatte zugeordnet.
  </para>

  <sect2 xml:id="crush-devclasses">
   <title>Geräteklassen</title>
   <para>
    Die flexible Steuerung der Datenplatzierung mithilfe der CRUSH-Zuordnung zählt zu Cephs Stärken. Gleichzeitig ist dies mit der schwierigste Teil bei der Clusterverwaltung. <emphasis>Geräteklassen</emphasis> automatisieren die häufigsten Änderungen an CRUSH-Zuordnungen, die der Administrator bisher manuell vornehmen musste.
   </para>
   <sect3 xml:id="crush-management-problem">
    <title>Das CRUSH-Verwaltungsproblem</title>
    <para>
     Ceph-Cluster setzen sich häufig aus verschiedenen Typen von Speichergeräten zusammen, also HDD, SSD, NVMe oder sogar gemischten Klassen dieser Geräte. Diese unterschiedlichen Speichergerättypen werden hier als <emphasis>Geräteklassen</emphasis> bezeichnet, sodass eine Verwechslung mit der Eigenschaft <emphasis>type</emphasis> der CRUSH-Buckets verhindert wird (z. B. „host“, „rack“, „row“; siehe <xref linkend="datamgm-buckets"/>). SSD-gestützte Ceph-OSDs sind deutlich schneller als OSDs mit drehbaren Scheiben und damit für bestimmte Workloads besser geeignet. Mit Ceph können Sie ganz einfach RADOS-Pools für unterschiedliche Datenmengen oder Workloads erstellen und unterschiedliche CRUSH-Regeln für die Datenplatzierung in diesen Pools zuweisen.
    </para>
    <figure>
     <title>OSDs mit gemischten Geräteklassen</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="device_classes.svg" width="70%" format="SVG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="device_classes.png" width="70%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
    <para>
     Es ist jedoch mühsam, die CRUSH-Regeln so einzurichten, dass die Daten nur auf eine bestimmte Geräteklasse platziert werden. Die Regeln folgen der CRUSH-Hierarchie, doch wenn die Geräte in den Hosts oder Racks gemischt angeordnet sind (wie in der Beispielhierarchie oben), werden sie (standardmäßig) miteinander vermischt, sodass sie in den gleichen Unterbäumen der Hierarchie auftreten. In früheren Versionen von SUSE Enterprise Storage mussten sie manuell in separate Bäume sortiert werden, wobei für jede Geräteklasse mehrere Versionen der einzelnen Zwischen-Knoten erstellt werden mussten.
    </para>
   </sect3>
   <sect3 xml:id="osd-crush-device-classes">
    <title>Geräteklassen</title>
    <para>
     Für diese Situation bietet Ceph eine elegante Lösung: Jedem OSD wird eine <emphasis>Geräteklasse</emphasis> als Eigenschaft zugewiesen. Standardmäßig stellen die OSDs ihre jeweilige Geräteklasse automatisch auf „hdd“, „ssd“ oder „nvme“ ein, abhängig von den Hardware-Eigenschaften, die der Linux-Kernel bereitstellt. Diese Geräte werden in der Ausgabe des Kommandos <command>ceph osd tree</command> in einer neuen Spalte aufgeführt:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd tree
 ID CLASS WEIGHT   TYPE NAME      STATUS REWEIGHT PRI-AFF
 -1       83.17899 root default
 -4       23.86200     host cpach
 2   hdd  1.81898         osd.2      up  1.00000 1.00000
 3   hdd  1.81898         osd.3      up  1.00000 1.00000
 4   hdd  1.81898         osd.4      up  1.00000 1.00000
 5   hdd  1.81898         osd.5      up  1.00000 1.00000
 6   hdd  1.81898         osd.6      up  1.00000 1.00000
 7   hdd  1.81898         osd.7      up  1.00000 1.00000
 8   hdd  1.81898         osd.8      up  1.00000 1.00000
 15  hdd  1.81898         osd.15     up  1.00000 1.00000
 10  nvme 0.93100         osd.10     up  1.00000 1.00000
 0   ssd  0.93100         osd.0      up  1.00000 1.00000
 9   ssd  0.93100         osd.9      up  1.00000 1.00000</screen>
    <para>
     Wenn die automatische Erkennung der Geräteklasse fehlschlägt, da beispielsweise der Gerätetreiber die Daten zum Gerät nicht ordnungsgemäß über <filename>/sys/block</filename> bereitstellt, können Sie die Geräteklassen über die Kommandozeile anpassen:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush rm-device-class osd.2 osd.3
done removing class of osd(s): 2,3
<prompt>cephuser@adm &gt; </prompt>ceph osd crush set-device-class ssd osd.2 osd.3
set osd(s) 2,3 to class 'ssd'
</screen>
   </sect3>
   <sect3 xml:id="crush-placement-rules">
    <title>Festlegen von CRUSH-Platzierungsregeln</title>
    <para>
     CRUSH-Regeln beschränken die Platzierung auf eine bestimmte Geräteklasse. Mit folgendem Kommando können Sie beispielsweise einen „schnell“ <emphasis role="bold">reproduzierten</emphasis> Pool erstellen, der die Daten ausschließlich auf SSD-Datenträger verteilt:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush rule create-replicated <replaceable>RULE_NAME</replaceable> <replaceable>ROOT</replaceable> <replaceable>FAILURE_DOMAIN_TYPE</replaceable> <replaceable>DEVICE_CLASS</replaceable>
</screen>
    <para>
     Beispiel:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush rule create-replicated fast default host ssd
</screen>
    <para>
     Erstellen Sie einen Pool mit der Bezeichnung „fast_pool“ und weisen Sie ihn der Regel „fast“ zu:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create fast_pool 128 128 replicated fast</screen>
    <para>
     Die Erstellung von <emphasis role="bold">Löschcode</emphasis>-Regeln läuft geringfügig anders ab. Zunächst erstellen Sie ein Löschcode-Profil mit einer Eigenschaft für die gewünschte Geräteklasse. Anschließend legen Sie den Pool mit Löschcodierung an:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd erasure-code-profile set myprofile \
 k=4 m=2 crush-device-class=ssd crush-failure-domain=host
<prompt>cephuser@adm &gt; </prompt>ceph osd pool create mypool 64 erasure myprofile
</screen>
    <para>
     Die Syntax wurde erweitert, sodass Sie die Geräteklasse auch manuell eingeben können, falls die CRUSH-Zuordnung manuell bearbeitet werden muss. Die obigen Kommandos erzeugen beispielsweise die folgende CRUSH-Regel:
    </para>
<screen>
rule ecpool {
  id 2
  type erasure
  min_size 3
  max_size 6
  step set_chooseleaf_tries 5
  step set_choose_tries 100
  step take default <emphasis role="bold">class ssd</emphasis>
  step chooseleaf indep 0 type host
  step emit
}
</screen>
    <para>
     Der wichtige Unterschied: Das Kommando „take“ umfasst hier das zusätzliche Suffix „class <replaceable>CLASS_NAME</replaceable>“.
    </para>
   </sect3>
   <sect3 xml:id="crush-additional-commands">
    <title>Zusätzliche Kommandos</title>
    <para>
     Mit folgendem Kommando rufen Sie eine Liste der Geräte in einer CRUSH-Zuordnung ab:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush class ls
[
  "hdd",
  "ssd"
]
</screen>
    <para>
     Mit folgendem Kommando rufen Sie eine Liste der vorhandenen CRUSH-Regeln ab:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush rule ls
replicated_rule
fast</screen>
    <para>
     Mit folgendem Kommando rufen Sie Details zur CRUSH-Regel „+++fast“ ab:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush rule dump fast
{
		"rule_id": 1,
		"rule_name": "fast",
		"ruleset": 1,
		"type": 1,
		"min_size": 1,
		"max_size": 10,
		"steps": [
						{
										"op": "take",
										"item": -21,
										"item_name": "default~ssd"
						},
						{
										"op": "chooseleaf_firstn",
										"num": 0,
										"type": "host"
						},
						{
										"op": "emit"
						}
		]
}</screen>
    <para>
     Mit folgendem Kommando rufen Sie eine Liste der OSDs ab, die zur Klasse „ssd“ gehören:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush class ls-osd ssd
0
1
</screen>
   </sect3>
   <sect3 xml:id="device-classes-reclassify">
    <title>Migration von einer Legacy-SSD-Regel zu Geräteklassen</title>
    <para>
     In SUSE Enterprise Storage vor Version 5 mussten Sie die CRUSH-Zuordnung manuell bearbeiten und parallele Hierarchien für jeden einzelnen Gerätetyp (z. B. SSD) pflegen, damit Sie überhaupt Regeln für diese Regeln schreiben konnten. Seit SUSE Enterprise Storage 5 ist dies mit der Geräteklassenfunktion transparent möglich.
    </para>
    <para>
     Mit dem Kommando <command>crushtool</command> wandeln Sie eine Legacy-Regel und -Hierarchie in die neuen klassenbasierten Regeln um. Für die Umwandlung stehen mehrere Möglichkeiten zur Auswahl:
    </para>
    <variablelist>
     <varlistentry>
      <term><command>crushtool --reclassify-root <replaceable>ROOT_NAME</replaceable> <replaceable>DEVICE_CLASS</replaceable></command></term>
      <listitem>
       <para>
        Dieses Kommando erfasst alle Elemente in der Hierarchie unterhalb von <replaceable>ROOT_NAME</replaceable> und ersetzt alle Regeln, die mit
       </para>
<screen>
take <replaceable>ROOT_NAME</replaceable>
</screen>
       <para>
        auf diesen Root verweisen, durch
       </para>
<screen>
take <replaceable>ROOT_NAME</replaceable> class <replaceable>DEVICE_CLASS</replaceable>
</screen>
       <para>
        Die Buckets werden neu nummeriert, so dass die bisherigen IDs in den „Schattenbaum“ der jeweiligen Klasse übernommen werden. Somit werden keine Daten verschoben.
       </para>
       <example>
        <title><command>crushtool --reclassify-root</command></title>
        <para>
         Betrachten Sie die folgende vorhandene Regel:
        </para>
<screen>
rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default
   step chooseleaf firstn 0 type rack
   step emit
}
</screen>
        <para>
         Wenn Sie den Root „default“ als Klasse „hdd“ neu klassifizieren, lautet die Regel nunmehr
        </para>
<screen>
rule replicated_ruleset {
   id 0
   type replicated
   min_size 1
   max_size 10
   step take default class hdd
   step chooseleaf firstn 0 type rack
   step emit
}
</screen>
       </example>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>crushtool --set-subtree-class <replaceable>BUCKET_NAME</replaceable> <replaceable>DEVICE_CLASS</replaceable></command></term>
      <listitem>
       <para>
        Mit dieser Methode werden alle Geräte im Teilbaum mit dem Root <replaceable>BUCKET_NAME</replaceable> und der angegebenen Geräteklasse gekennzeichnet.
       </para>
       <para>
        <option>--set-subtree-class</option> wird in der Regel gemeinsam mit der Option <option>--reclassify-root</option> herangezogen, damit alle Geräte im betreffenden Root mit der richtigen Klasse gekennzeichnet werden. Einige Geräte, denen bewusst eine andere Klasse zugewiesen wurde, sollen jedoch nicht neu gekennzeichnet werden. In diesen Fällen lassen Sie die Option <option>--set-subtree-class</option> weg. Beachten Sie, dass diese Neuzuordnung nicht völlig einwandfrei ist, da die bisherige Regel auf Geräte mehrerer Klassen verteilt ist, während die angepassten Regeln lediglich Geräten der angegebenen Geräteklasse zugeordnet werden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>crushtool --reclassify-bucket <replaceable>MATCH_PATTERN</replaceable> <replaceable>DEVICE_CLASS</replaceable> <replaceable>DEFAULT_PATTERN</replaceable></command></term>
      <listitem>
       <para>
        Mit dieser Methode lässt sich eine parallele typenspezifische Hierarchie mit der normalen Hierarchie zusammenführen. Bei vielen Benutzern sehen die CRUSH-Zuordnungen beispielsweise so oder ähnlich aus:
       </para>
       <example>
        <title><command>crushtool --reclassify-bucket</command></title>
<screen>
host node1 {
   id -2           # do not change unnecessarily
   # weight 109.152
   alg straw
   hash 0  # rjenkins1
   item osd.0 weight 9.096
   item osd.1 weight 9.096
   item osd.2 weight 9.096
   item osd.3 weight 9.096
   item osd.4 weight 9.096
   item osd.5 weight 9.096
   [...]
}

host node1-ssd {
   id -10          # do not change unnecessarily
   # weight 2.000
   alg straw
   hash 0  # rjenkins1
   item osd.80 weight 2.000
   [...]
}

root default {
   id -1           # do not change unnecessarily
   alg straw
   hash 0  # rjenkins1
   item node1 weight 110.967
   [...]
}

root ssd {
   id -18          # do not change unnecessarily
   # weight 16.000
   alg straw
   hash 0  # rjenkins1
   item node1-ssd weight 2.000
   [...]
}
</screen>
       </example>
       <para>
        Mit dieser Funktion werden alle Buckets neu klassifiziert, die mit einem bestimmten Muster übereinstimmen. Dieses Muster lautet beispielsweise <literal>%suffix</literal> oder <literal>prefix%</literal>. Im obigen Beispiel würden Sie das Muster <literal>%-ssd</literal> heranziehen. Bei jedem passenden Bucket bezeichnet der verbleibende Teil des Namens, also der Teil im Platzhalter „%“, den Basis-Bucket. Alle Geräte im passenden Bucket werden mit der angegebenen Geräteklasse gekennzeichnet und dann in den Basis-Bucket verschoben. Wenn der Basis-Bucket noch nicht vorhanden ist („node12-ssd“ liegt beispielsweise vor, „node12“ dagegen nicht), wird er erstellt und unter dem angegebenen standardmäßigen übergeordneten Bucket verknüpft. Die bisherigen Bucket-IDs werden für die neuen Shadow-Buckets beibehalten, wodurch keine Daten verschoben werden müssen. Regeln mit <literal>take</literal>-Schritten, die auf die bisherigen Buckets verweisen, werden angepasst.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><command>crushtool --reclassify-bucket <replaceable>BUCKET_NAME</replaceable> <replaceable>DEVICE_CLASS</replaceable> <replaceable>BASE_BUCKET</replaceable></command></term>
      <listitem>
       <para>
        Mit der Option <option>--reclassify-bucket</option>ohne Platzhalter können Sie einen einzelnen Bucket zuordnen. Im obigen Beispiel soll beispielsweise der Bucket „ssd“ dem Standard-Bucket zugeordnet werden.
       </para>
       <para>
        Das Kommando, mit dem die Zuordnung mit den obigen Fragmenten endgültig konvertiert wird, lautet dann:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o original
<prompt>cephuser@adm &gt; </prompt>crushtool -i original --reclassify \
  --set-subtree-class default hdd \
  --reclassify-root default hdd \
  --reclassify-bucket %-ssd ssd default \
  --reclassify-bucket ssd ssd default \
  -o adjusted
</screen>
       <para>
        Mit der Option <option>--compare</option> können Sie prüfen, ob die Konvertierung fehlerfrei durchgeführt wurde. Hiermit wird eine große Anzahl von Eingaben in die CRUSH-Zuordnung getestet und es wird verglichen, ob wieder das gleiche Ergebnis erzielt wird. Diese Eingaben werden mit denselben Optionen gesteuert wie <option>--test</option>. Im obigen Beispiel lautet das Kommando dann:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -i original --compare adjusted
rule 0 had 0/10240 mismatched mappings (0)
rule 1 had 0/10240 mismatched mappings (0)
maps appear equivalent
</screen>
       <tip>
        <para>
         Bei Abweichungen würde das Verhältnis der neu zugeordneten Eingaben in Klammern angegeben.
        </para>
       </tip>
       <para>
        Wenn Sie mit der angepassten CRUSH-Zuordnung zufrieden sind, können Sie sie auf den Cluster anwenden:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i adjusted
</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Weitere Informationen</title>
    <para>
     Weitere Informationen zu CRUSH-Zuordnungen finden Sie in <xref linkend="op-crush"/>.
    </para>
    <para>
     Weitere Informationen zu Ceph Pools im Allgemeinen finden Sie in <xref linkend="ceph-pools"/>.
    </para>
    <para>
     Weitere Informationen zu Pools mit Löschcodierung finden Sie in <xref linkend="cha-ceph-erasure"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="datamgm-buckets">
  <title>Buckets</title>

  <para>
   CRUSH-Zuordnungen enthalten eine Liste von OSDs, die in einer Baumstruktur von Buckets angeordnet werden können, um die Geräte an physischen Standorten zu aggregieren. Die einzelnen OSDs stellen die Blätter am Baum dar. 
  </para>

  <informaltable frame="none">
   <tgroup cols="3">
    <colspec colwidth="10*"/>
    <colspec colwidth="30*"/>
    <colspec colwidth="70*"/>
    <tbody>
     <row>
      <entry>
       <para>
        0
       </para>
      </entry>
      <entry>
       <para>
        osd
       </para>
      </entry>
      <entry>
       <para>
        Ein spezifisches Gerät oder OSD (<literal>osd.1</literal>, <literal>osd.2</literal> usw).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        1
       </para>
      </entry>
      <entry>
       <para>
        Host
       </para>
      </entry>
      <entry>
       <para>
        Der Name eines Hosts mit einem oder mehreren OSDs.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        2
       </para>
      </entry>
      <entry>
       <para>
        chassis
       </para>
      </entry>
      <entry>
       <para>
        Kennung dafür, welches Chassis im Rack den <literal>Host</literal> enthält.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        3
       </para>
      </entry>
      <entry>
       <para>
        rack
       </para>
      </entry>
      <entry>
       <para>
        Ein Computer-Rack. Der Standardwert ist <literal>unknownrack</literal>.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        4
       </para>
      </entry>
      <entry>
       <para>
        row
       </para>
      </entry>
      <entry>
       <para>
        Eine Reihe in einer Serie von Racks.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        5
       </para>
      </entry>
      <entry>
       <para>
        pdu
       </para>
      </entry>
      <entry>
       <para>
        Abkürzung für „Power Distribution Unit“ (Stromversorgungseinheit).
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        6
       </para>
      </entry>
      <entry>
       <para>
        pod
       </para>
      </entry>
      <entry>
       <para>
        Abkürzung für „Point of Delivery“ (Zustellort): in diesem Zusammenhang eine Gruppe von PDUs oder eine Gruppe von Rackreihen.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        7
       </para>
      </entry>
      <entry>
       <para>
        room
       </para>
      </entry>
      <entry>
       <para>
        Ein Raum mit Rackreihen.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        8
       </para>
      </entry>
      <entry>
       <para>
        Rechenzentrum
       </para>
      </entry>
      <entry>
       <para>
        Ein physisches Rechenzentrum mit einem oder mehreren Räumen.
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        9
       </para>
      </entry>
      <entry>
       <para>
        region
       </para>
      </entry>
      <entry>
       <para>
        Geografische Region der Welt (z. B. NAM, LAM, EMEA, APAC usw)
       </para>
      </entry>
     </row>
     <row>
      <entry>
       <para>
        10
       </para>
      </entry>
      <entry>
       <para>
        root
       </para>
      </entry>
      <entry>
       <para>
        Der Stammknoten des Baums der OSD-Buckets (normalerweise auf <literal>default</literal> festgelegt).
       </para>
      </entry>
     </row>
    </tbody>
   </tgroup>
  </informaltable>

  <tip>
   <para>
    Sie können die vorhandenen Typen bearbeiten und eigene Bucket-Typen erstellen.
   </para>
  </tip>

  <para>
   Die Bereitstellungswerkzeuge von Ceph generieren eine CRUSH-Zuordnung, die einen Bucket für jeden Host sowie den Pool „default“ enthält, was für den standardmäßigen <literal>rbd</literal>-Pool nützlich ist. Die restlichen Bucket-Typen dienen zum Speichern von Informationen zum physischen Standort von Knoten/Buckets. Dadurch wird die Cluster-Verwaltung erheblich erleichtert, wenn bei OSDs, Hosts oder Netzwerkhardware Störungen auftreten und der Administrator Zugriff auf die physische Hardware benötigt.
  </para>

  <para>
   Ein Bucket umfasst einen Typ, einen eindeutigen Namen (Zeichenkette), eine eindeutige als negative Ganzzahl ausgedrückte ID, ein Gewicht relativ zur Gesamtkapazität/Capability seiner Elemente, den Bucket-Algorithmus (standardmäßig <literal>straw</literal>) sowie den Hash (standardmäßig <literal>0</literal>, was dem CRUSH Hash <literal>rjenkins1</literal> entspricht). Ein Bucket kann ein oder mehrere Elemente enthalten. Die Elemente bestehen möglicherweise aus anderen Buckets oder OSDs. Elemente können ein Gewicht aufweisen, das dem relativen Gewicht des Elements entspricht.
  </para>

<screen>[bucket-type] [bucket-name] {
  id [a unique negative numeric ID]
  weight [the relative capacity/capability of the item(s)]
  alg [the bucket type: uniform | list | tree | straw2 | straw ]
  hash [the hash type: 0 by default]
  item [item-name] weight [weight]
}</screen>

  <para>
   Das folgende Beispiel veranschaulicht, wie Sie Buckets zum Aggregieren eines Pools und der physischen Standorte wie Rechenzentrum, Raum, Rack und Reihe verwenden.
  </para>

<screen>host ceph-osd-server-1 {
        id -17
        alg straw2
        hash 0
        item osd.0 weight 0.546
        item osd.1 weight 0.546
}

row rack-1-row-1 {
        id -16
        alg straw2
        hash 0
        item ceph-osd-server-1 weight 2.00
}

rack rack-3 {
        id -15
        alg straw2
        hash 0
        item rack-3-row-1 weight 2.00
        item rack-3-row-2 weight 2.00
        item rack-3-row-3 weight 2.00
        item rack-3-row-4 weight 2.00
        item rack-3-row-5 weight 2.00
}

rack rack-2 {
        id -14
        alg straw2
        hash 0
        item rack-2-row-1 weight 2.00
        item rack-2-row-2 weight 2.00
        item rack-2-row-3 weight 2.00
        item rack-2-row-4 weight 2.00
        item rack-2-row-5 weight 2.00
}

rack rack-1 {
        id -13
        alg straw2
        hash 0
        item rack-1-row-1 weight 2.00
        item rack-1-row-2 weight 2.00
        item rack-1-row-3 weight 2.00
        item rack-1-row-4 weight 2.00
        item rack-1-row-5 weight 2.00
}

room server-room-1 {
        id -12
        alg straw2
        hash 0
        item rack-1 weight 10.00
        item rack-2 weight 10.00
        item rack-3 weight 10.00
}

datacenter dc-1 {
        id -11
        alg straw2
        hash 0
        item server-room-1 weight 30.00
        item server-room-2 weight 30.00
}

root data {
        id -10
        alg straw2
        hash 0
        item dc-1 weight 60.00
        item dc-2 weight 60.00
}</screen>
 </sect1>
 <sect1 xml:id="datamgm-rules">
  <title>Regelsatz</title>

  <para>
   CRUSH-Zuordnungen unterstützen das Konzept der „CRUSH-Regeln“. Dabei handelt es sich um die Regeln, die die Datenplatzierung für einen Pool bestimmen. Bei großen Clustern erstellen Sie wahrscheinlich viele Pools und jeder Pool verfügt über einen eigenen CRUSH-Regelsatz und Regeln. Die standardmäßige CRUSH-Zuordnung umfasst eine Regel für den Standard-Root. Wenn Sie weitere Roots und Regeln benötigen, erstellen Sie sie später. Ansonsten werden sie automatisch angelegt, sobald neue Pools eingerichtet werden.
  </para>

  <note>
   <para>
    In den meisten Fällen müssen Sie die Standardregeln nicht ändern. Beim Erstellen eines neuen Pools lautet der Standardregelsatz 0.
   </para>
  </note>

  <para>
   Eine Regel sieht folgendermaßen aus:
  </para>

<screen>rule <replaceable>rulename</replaceable> {

        ruleset <replaceable>ruleset</replaceable>
        type <replaceable>type</replaceable>
        min_size <replaceable>min-size</replaceable>
        max_size <replaceable>max-size</replaceable>
        step <replaceable>step</replaceable>

}</screen>

  <variablelist>
   <varlistentry>
    <term>ruleset</term>
    <listitem>
     <para>
      Eine Ganzzahl. Klassifiziert eine Regel als Teil einer Gruppe von Regeln. Wird dadurch aktiviert, dass der Regelsatz in einem Pool festgelegt wird. Diese Option muss aktiviert sein. Der Standardwert ist <literal>0</literal>. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>type</term>
    <listitem>
     <para>
      Eine Zeichenkette. Beschreibt eine Regel für einen „reproduzierten“ oder einen „Erasure“ Coded Pool. Diese Option muss aktiviert sein. Die Standardeinstellung ist <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>min_size</term>
    <listitem>
     <para>
      Eine Ganzzahl. CRUSH wählt diese Regel NICHT aus, wenn eine Poolgruppe weniger Reproduktionen erstellt als diese Zahl. Diese Option muss aktiviert sein. Die Standardeinstellung ist <literal>2</literal>. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>max_size</term>
    <listitem>
     <para>
      Eine Ganzzahl. CRUSH wählt diese Regel NICHT aus, wenn eine Poolgruppe mehr Reproduktionen erstellt als diese Zahl. Diese Option muss aktiviert sein. Die Standardeinstellung ist <literal>10</literal>. 
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step take <replaceable>bucket</replaceable></term>
    <listitem>
     <para>
      Nimmt einen Bucket, der durch einen Namen angegeben wird, und beginnt, den Baum nach unten zu durchlaufen. Diese Option muss aktiviert sein. Eine Erläuterung zum Durchlaufen des Baums finden Sie in <xref linkend="datamgm-rules-step-iterate"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step <replaceable>target</replaceable><replaceable>mode</replaceable><replaceable>num</replaceable> type <replaceable>bucket-type</replaceable></term>
    <listitem>
     <para>
      <replaceable>target</replaceable> ist entweder <literal>choose</literal> oder <literal>chooseleaf</literal>. Wenn der Wert auf <literal>choose</literal> festgelegt ist, wird eine Reihe von Buckets ausgewählt. <literal>chooseleaf</literal> wählt direkt die OSDs (Blatt-Knoten) aus dem Teilbaum der einzelnen Buckets in der Gruppe der Buckets aus.
     </para>
     <para>
      <replaceable>mode</replaceable> ist entweder <literal>firstn</literal> oder <literal>indep</literal>. Siehe <xref linkend="datamgm-rules-step-mode"/>.
     </para>
     <para>
      Wählt die Anzahl der Buckets des angegebenen Typs aus. Wenn N die Anzahl der verfügbaren Optionen ist und <replaceable>num</replaceable> &gt; 0 &amp;&amp; &lt; N, wählen Sie genauso viele Buckets. <replaceable>num</replaceable> &lt; 0 bedeutet N - <replaceable>num</replaceable>. Bei <replaceable>num</replaceable> == 0 wählen Sie N Buckets (alle verfügbar). Folgt auf <literal>step take</literal> oder <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>step emit</term>
    <listitem>
     <para>
      Gibt den aktuellen Wert aus und leert den Stack. Wird normalerweise am Ende einer Regel verwendet, kann jedoch auch zum Erstellen unterschiedlicher Bäume in derselben Regel verwendet werden. Folgt auf <literal>step choose</literal>.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="datamgm-rules-step-iterate">
   <title>Durchlaufen des Knoten-Baums</title>
   <para>
    Die mit den Buckets definierte Struktur kann als Knoten-Baum angezeigt werden. Buckets sind Knoten und OSDs sind Blätter an diesem Baum.
   </para>
   <para>
    Die Regeln in der CRUSH-Zuordnung definieren, wie OSDs aus diesem Baum ausgewählt werden. Eine Regel beginnt mit einem Knoten und durchläuft dann den Baum nach unten, um eine Reihe von OSDs zurückzugeben. Es ist nicht möglich, zu definieren, welcher Zweig ausgewählt werden muss. Stattdessen wird durch den CRUSH-Algorithmus sichergestellt, dass die Gruppe der OSDs die Reproduktionsanforderungen erfüllt und die Daten gleichmäßig verteilt.
   </para>
   <para>
    Bei <literal>step take</literal> <replaceable>bucket</replaceable> beginnt der Durchlauf des Knoten-Baums am angegebenen Bucket (nicht am Bucket-Typ). Wenn OSDs aus allen Zweigen am Baum zurückgegeben werden müssen, dann muss der Bucket der Root Bucket sein. Andernfalls durchlaufen die folgenden Schritte nur einen Teilbaum.
   </para>
   <para>
    In der Regeldefinition folgen nach <literal>step take</literal> ein oder zwei <literal>step choose</literal>-Einträge. Mit jedem <literal>step choose</literal> wird eine definierte Anzahl von Knoten (oder Zweigen) aus dem vorher ausgewählten oberen Knoten gewählt.
   </para>
   <para>
    Am Ende werden die ausgewählten OSDs mit <literal>step emit</literal> zurückgegeben.
   </para>
   <para>
    <literal>step chooseleaf</literal> ist eine praktische Funktion, mit der OSDs direkt aus Zweigen des angegebenen Buckets ausgewählt werden.
   </para>
   <para>
    <xref linkend="datamgm-rules-step-iterate-figure"/> zeigt ein Beispiel, wie <literal>step</literal> zum Durchlaufen eines Baums verwendet wird. In den folgenden Regeldefinitionen entsprechen die orangen Pfeile und Zahlen <literal>example1a</literal> und <literal>example1b</literal> und die blauen Pfeile <literal>example2</literal>.
   </para>
   <figure xml:id="datamgm-rules-step-iterate-figure">
    <title>Beispielbaum</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-step.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-step.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
<screen># orange arrows
rule example1a {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2)
        step choose firstn 0 host
        # orange (3)
        step choose firstn 1 osd
        step emit
}

rule example1b {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # orange (1)
        step take rack1
        # orange (2) + (3)
        step chooseleaf firstn 0 host
        step emit
}

# blue arrows
rule example2 {
        ruleset 0
        type replicated
        min_size 2
        max_size 10
        # blue (1)
        step take room1
        # blue (2)
        step chooseleaf firstn 0 rack
        step emit
}</screen>
  </sect2>

  <sect2 xml:id="datamgm-rules-step-mode">
   <title><literal></literal>firstn und indep<literal></literal></title>
   <para>
    Eine CRUSH-Regel definiert den Ersatz für fehlerhafte Knoten oder OSDs (weitere Informationen finden Sie in <xref linkend="datamgm-rules"/>). Für das Schlüsselwort <literal>step</literal> ist entweder <literal>firstn</literal> oder <literal>indep</literal> als Parameter erforderlich. <xref linkend="datamgm-rules-step-mode-indep-figure"/> zeigt ein Beispiel.
   </para>
   <para>
    <literal>firstn</literal> fügt Ersatz-Knoten am Ende der Liste der aktiven Knoten hinzu. Im Fall eines fehlerhaften Knotens werden die folgenden fehlerfreien Knoten nach links verschoben, um die Lücke der fehlerhaften Knoten zu schließen. Dies ist die standardmäßige und erwünschte Methode für <emphasis>reproduzierte Pools</emphasis>, weil ein sekundärer Knoten bereits alle Daten enthält und daher die Aufgaben des primären Knoten sofort übernehmen kann.
   </para>
   <para>
    <literal>indep</literal> wählt feste Ersatz-Knoten für jeden aktiven Knoten aus. Der Ersatz für einen fehlerhaften Knoten ändert nicht die Reihenfolge der anderen Knoten. Dies ist die erwünschte Methode für <emphasis>Erasure Coded Pools</emphasis>. In Erasure Coded Pools hängen die in einem Knoten gespeicherten Daten von ihrer Position in der Knoten-Auswahl ab. Wenn sich die Reihenfolge der Knoten ändert, müssen alle Daten in den betroffenen Knoten neu platziert werden.
   </para>
   <figure xml:id="datamgm-rules-step-mode-indep-figure">
    <title>Methoden für den Austausch von Knoten</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="crush-firstn-indep.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="crush-firstn-indep.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>
 </sect1>
 <sect1 xml:id="op-pgs">
  <title>Platzierungsgruppen</title>

  <para>
   Ceph ordnet die Objekte bestimmten Platzierungsgruppen (PGs) zu. Die Platzierungsgruppen sind Shards oder Fragmente einer logischen Gruppe, mit der Objekte als Gruppe in OSDs platziert werden. Platzierungsgruppen verringern die Menge der Metadaten pro Objekt, wenn Ceph die Daten in OSDs speichert. Eine größere Anzahl von Platzierungsgruppen – beispielsweise 100 pro OSD – bewirkt einen besseren Ausgleich.
  </para>

  <sect2 xml:id="op-pgs-usage">
   <title>Verwenden von Platzierungsgruppen</title>
   <para>
    Eine Platzierungsgruppe (PG) aggregiert Objekte in einem Pool. Der wichtigste Grund: Die Nachverfolgung der Objektplatzierung und der Metadaten für einzelne Objekte ist rechnerisch aufwendig. In einem System mit Millionen von Objekten ist es beispielsweise nicht möglich, die Platzierung einzelner Objekte direkt zu verfolgen.
   </para>
   <figure>
    <title>Platzierungsgruppen in einem Pool</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_pgs_schema.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_pgs_schema.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Der Ceph-Client berechnet die Platzierungsgruppe, zu der ein Objekt gehört. Hierzu erhält die Objekt-ID ein Hash und es wird eine Aktion aufgeführt, die auf der Anzahl der PGs im definierten Pool und auf der ID des Pools beruht.
   </para>
   <para>
    Der Inhalt des Objekts in einer Platzierungsgruppe wird in einer Gruppe von OSDs gespeichert. In einem reproduzierten Pool der Größe 2 werden Objekte durch die Platzierungsgruppen beispielsweise auf zwei OSDs gespeichert:
   </para>
   <figure>
    <title>Platzierungsgruppen und OSDs</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_pgs_osds.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_pgs_osds.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Wenn OSD 2 ausfällt, wird der Platzierungsgruppe 1 ein anderes OSD zugewiesen, auf dem dann Kopien aller Objekte auf OSD 1 abgelegt werden. Wird die Poolgröße von 2 auf 3 erhöht, wird der Platzierungsgruppe ein zusätzliches OSD zugewiesen, das dann Kopien aller Objekte in der Platzierungsgruppe erhält.
   </para>
   <para>
    Platzierungsgruppen fungieren nicht als Eigentümer des OSD, sondern nutzen es gemeinsam mit anderen Platzierungsgruppen aus demselben Pool oder sogar aus anderen Pools. Wenn OSD 2 ausfällt, muss die Platzierungsgruppe 2 ebenfalls Kopien der Objekte wiederherstellen, wobei OSD 3 herangezogen wird.
   </para>
   <para>
    Wenn die Anzahl der Platzierungsgruppen wächst, werden den neuen Platzierungsgruppen entsprechend OSDs zugewiesen. Auch das Ergebnis der CRUSH-Funktion ändert sich und einige Objekte aus den bisherigen Platzierungsgruppen werden in die neuen Platzierungsgruppen kopiert und aus den bisherigen Gruppen entfernt.
   </para>
  </sect2>

  <sect2 xml:id="op-pgs-pg-num">
   <title>Ermitteln des Werts für <replaceable>PG_NUM</replaceable></title>
   <note>
    <para>
     Seit Ceph Nautilus (v14.x) können Sie das Ceph-Manager-Modul <literal>pg_autoscaler</literal> verwenden, um die PGs bei Bedarf automatisch zu skalieren. Wenn Sie diese Funktion aktivieren möchten, lesen Sie im <xref linkend="default-pg-and-pgp-counts"/> nach.
    </para>
   </note>
   <para>
    Beim Erstellen eines neuen Pools können Sie den Wert von <replaceable>PG_NUM</replaceable> noch manuell wählen:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable> <replaceable>PG_NUM</replaceable>
</screen>
   <para>
    <replaceable>PG_NUM</replaceable> kann nicht automatisch berechnet werden. Die folgenden Werte kommen häufig zum Einsatz, je nach Anzahl der OSDs im Cluster:
   </para>
   <variablelist>
    <varlistentry>
     <term>Weniger als 5 OSDs:</term>
     <listitem>
      <para>
       Legen Sie <replaceable>PG_NUM</replaceable> auf 128 fest.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zwischen 5 und 10 OSDs:</term>
     <listitem>
      <para>
       Legen Sie <replaceable>PG_NUM</replaceable> auf 512 fest.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Zwischen 10 und 50 OSDs:</term>
     <listitem>
      <para>
       Legen Sie <replaceable>PG_NUM</replaceable> auf 1.024 fest.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Mit wachsender Anzahl der OSDs wird die Auswahl des richtigen Werts für <replaceable>PG_NUM</replaceable> immer wichtiger. <replaceable>PG_NUM</replaceable> wirkt sich stark auf das Verhalten des Clusters und auf die Haltbarkeit der Daten bei einem OSD-Fehler aus.
   </para>
   <sect3 xml:id="op-pgs-choosing">
    <title>Berechnen der Platzierungsgruppen für mehr als 50 OSDs</title>
    <para>
     Wenn Sie weniger als 50 OSDs nutzen, beachten Sie die Vorauswahl unter <xref linkend="op-pgs-pg-num"/>. Wenn Sie mehr als 50 OSDs nutzen, werden etwa 50–100 Platzierungsgruppen pro OSD empfohlen, sodass die Ressourcenauslastung, die Datenhaltbarkeit und die Verteilung ausgeglichen sind. Bei einem einzelnen Pool mit Objekten können Sie mit der folgenden Formel einen Referenzwert berechnen:
    </para>
<screen>total PGs = (OSDs * 100) / <replaceable>POOL_SIZE</replaceable></screen>
    <para>
     <replaceable>POOL_SIZE</replaceable> bezeichnet hierbei entweder die Anzahl der Reproduktionen bei reproduzierten Pools oder die Summe aus „k“+„m“ bei Pools mit Löschcodierung, die durch das Kommando <command>ceph osd erasure-code-profile get</command> zurückgegeben wird. Runden Sie das Ergebnis auf die nächste Zweierpotenz auf. Das Aufrunden wird empfohlen, damit der CRUSH-Algorithmus die Anzahl der Objekte gleichmäßig auf die Platzierungsgruppen verteilen kann.
    </para>
    <para>
     Für einen Cluster mit 200 OSDs und einer Poolgröße von 3 Reproduktionen würden Sie die Anzahl der PGs beispielsweise wie folgt näherungsweise ermitteln:
    </para>
<screen>
          (200 * 100) / 3 = 6667
</screen>
    <para>
     Die nächste Zweierpotenz ist <emphasis role="bold">8.192</emphasis>.
    </para>
    <para>
     Wenn Objekte in mehreren Daten-Pools gespeichert werden, müssen Sie die Anzahl der Platzierungsgruppen pro Pool in jedem Fall mit der Anzahl der Platzierungsgruppen pro OSD abgleichen. Dabei müssen Sie eine sinnvolle Gesamtanzahl der Platzierungsgruppen erreichen, die eine angemessen niedrige Varianz pro OSD gewährleistet, ohne die Systemressourcen überzustrapazieren oder den Peering-Prozess zu stark zu verlangsamen.
    </para>
    <para>
     Ein Cluster mit 10 Pools mit je 512 Platzierungsgruppen auf 10 OSDs umfasst beispielsweise insgesamt 5.120 Platzierungsgruppen auf 10 OSDs, also 512 Platzierungsgruppen pro OSD. Eine solche Einrichtung verbraucht nicht allzu viele Ressourcen. Würden jedoch 1.000 Pools mit je 512 Platzierungsgruppen erstellt, müssten die OSDs je etwa 50.000 Platzierungsgruppen verarbeiten, was die Ressourcenauslastung und den Zeitaufwand für das Peering erheblich erhöhen würde.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="op-pg-set">
   <title>Festlegen der Anzahl der Platzierungsgruppen</title>
   <note>
    <para>
     Seit Ceph Nautilus (v14.x) können Sie das Ceph-Manager-Modul <literal>pg_autoscaler</literal> verwenden, um die PGs bei Bedarf automatisch zu skalieren. Wenn Sie diese Funktion aktivieren möchten, lesen Sie im <xref linkend="default-pg-and-pgp-counts"/> nach.
    </para>
   </note>
   <para>
    Wenn Sie die Anzahl der Platzierungsgruppen in einem Pool noch manuell festlegen müssen, müssen Sie sie zum Zeitpunkt der Poolerstellung angeben (weitere Informationen finden Sie in <xref linkend="ceph-pools-operate-add-pool"/>). Die festgelegte Anzahl der Platzierungsgruppen für einen Pool kann mit folgendem Kommando erhöht werden:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_num <replaceable>PG_NUM</replaceable>
</screen>
   <para>
    Wenn Sie die Anzahl der Platzierungsgruppen erhöhen, müssen Sie auch die Anzahl der zu platzierenden Platzierungsgruppen (<option>PGP_NUM</option>) erhöhen, damit der Cluster neu ausgeglichen werden kann. <option>PGP_NUM</option> ist die Anzahl der Platzierungsgruppen, die der CRUSH-Algorithmus zur Platzierung berücksichtigt. Wird <option>PG_NUM</option> erhöht, werden die Platzierungsgruppen geteilt; die Daten werden allerdings erst dann zu den neueren Platzierungsgruppen migriert, wenn <option>PGP_NUM</option> erhöht wird. <option>PGP_NUM</option> muss gleich <option>PG_NUM</option> sein. Mit folgendem Kommando erhöhen Sie die Anzahl der Platzierungsgruppen zur Platzierung:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> pgp_num <replaceable>PGP_NUM</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="op-pg-get">
   <title>Feststellen der Anzahl der Platzierungsgruppen</title>
   <para>
    Führen Sie folgendes <command>get</command>-Kommando aus, um die Anzahl der Platzierungsgruppen in einem Pool festzustellen:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> pg_num
</screen>
  </sect2>

  <sect2 xml:id="op-pg-getpgstat">
   <title>Feststellen der PG-Statistiken für einen Cluster</title>
   <para>
    Mit folgendem Kommando stellen Sie die Statistik für die Platzierungsgruppen in Ihrem Cluster fest:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg dump [--format <replaceable>FORMAT</replaceable>]
</screen>
   <para>
    Zulässige Formate sind „plain“ (Standard) und „json“.
   </para>
  </sect2>

  <sect2 xml:id="op-pg-getstuckstat">
   <title>Feststellen von Statistiken für hängen gebliebene PGs</title>
   <para>
    Mit folgendem Kommando stellen Sie die Statistiken für alle Platzierungsgruppen fest, die in einem bestimmten Status hängen geblieben sind:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg dump_stuck <replaceable>STATE</replaceable> \
 [--format <replaceable>FORMAT</replaceable>] [--threshold <replaceable>THRESHOLD</replaceable>]
</screen>
   <para>
    <replaceable>STATE</replaceable> lautet „inactive“ (PGs können keine Lese- oder Schreibvorgänge verarbeiten, da sie darauf warten, dass ein OSD die jeweils neuesten Daten bereitstellt), „unclean“ (PGs enthalten Objekte, die nicht so oft wie gefordert reproduziert wurden), „stale“ (PGs besitzen einen unbekannten Status – die OSDs, auf denen sie gehostet werden, haben den Status nicht im Zeitraum zurückgegeben, der in der Option <option>mon_osd_report_timeout</option> festgelegt ist), „undersized“ oder „degraded“.
   </para>
   <para>
    Zulässige Formate sind „plain“ (Standard) und „json“.
   </para>
   <para>
    Der Schwellwert definiert den Mindestzeitraum (in Sekunden), über den die Platzierungsgruppe hängen geblieben sein muss, bevor sie in die zurückgegebenen Statistiken aufgenommen wird (standardmäßig 300 Sekunden).
   </para>
  </sect2>

  <sect2 xml:id="op-pgs-pgmap">
   <title>Suchen einer Platzierungsgruppenzuordnung</title>
   <para>
    Mit folgendem Kommando suchen Sie die Platzierungsgruppenzuordnung für eine bestimmte Platzierungsgruppe:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg map <replaceable>PG_ID</replaceable>
</screen>
   <para>
    Ceph gibt die Platzierungsgruppenzuordnung, die Platzierungsgruppe und den OSD-Status zurück:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg map 1.6c
osdmap e13 pg 1.6c (1.6c) -&gt; up [1,0] acting [1,0]
</screen>
  </sect2>

  <sect2 xml:id="op-pg-pgstats">
   <title>Abrufen einer Platzierungsgruppenstatistik</title>
   <para>
    Mit folgendem Kommando rufen Sie die Statistiken für eine bestimmte Platzierungsgruppe ab:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg <replaceable>PG_ID</replaceable> query
</screen>
  </sect2>

  <sect2 xml:id="op-pg-scrubpg">
   <title>Scrubbing einer Platzierungsgruppe</title>
   <para>
    Mit folgendem Kommando führen Sie das Scrubbing (<xref linkend="scrubbing-pgs"/>) einer Platzierungsgruppe aus:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg scrub <replaceable>PG_ID</replaceable>
</screen>
   <para>
    Ceph prüft die primären Knoten und die Reproduktionsknoten, erzeugt einen Katalog aller Objekte der Platzierungsgruppe und führt einen Vergleich aus, mit dem gewährleistet wird, dass keine Objekte fehlen oder fehlerhaft zugeordnet wurden und dass die Inhalte der Objekte konsistent sind. Unter der Voraussetzung, dass alle Reproduktionen übereinstimmen, wird mit einem abschließenden semantischen Durchlauf geprüft, ob alle snapshotspezifischen Objekt-Metadaten konsistent sind. Fehler werden in Protokollen erfasst.
   </para>
  </sect2>

  <sect2 xml:id="op-pg-backfill">
   <title>Priorisierung des Abgleichs und der Wiederherstellung von Platzierungsgruppen</title>
   <para>
    Unter Umständen müssen mehrere Platzierungsgruppen wiederhergestellt und/oder abgeglichen werden, deren Daten unterschiedlich wichtig sind. Die PGs enthalten beispielsweise Daten für Images, die auf derzeit laufenden Computern verwendet werden, andere PGs dagegen werden von inaktiven Computern herangezogen oder enthalten weniger relevante Daten. In diesem Fall muss die Wiederherstellung der betreffenden Gruppen priorisiert werden, sodass die Leistung und Verfügbarkeit der in diesen Gruppen gespeicherten Daten rascher wieder bereitstehen. Mit folgendem Kommando markieren Sie bestimmte Platzierungsgruppen bei einem Abgleich oder einer Wiederherstellung als priorisiert:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg force-recovery <replaceable>PG_ID1</replaceable> [<replaceable>PG_ID2</replaceable> ... ]
<prompt role="root"># </prompt>ceph pg force-backfill <replaceable>PG_ID1</replaceable> [<replaceable>PG_ID2</replaceable> ... ]
</screen>
   <para>
    Hiermit führt Ceph zuerst die Wiederherstellung oder den Abgleich der angegebenen Platzierungsgruppen durch und verarbeitet dann erst andere Platzierungsgruppen. Laufende Abgleich- oder Wiederherstellungsvorgänge werden nicht unterbrochen, sondern die angegebenen PGs werden so rasch wie möglich verarbeitet. Wenn Sie Ihre Meinung ändern oder nicht die richtigen Gruppen priorisiert haben, können Sie die Priorisierung abbrechen:
   </para>
<screen>
<prompt role="root"># </prompt>ceph pg cancel-force-recovery <replaceable>PG_ID1</replaceable> [<replaceable>PG_ID2</replaceable> ... ]
<prompt role="root"># </prompt>ceph pg cancel-force-backfill <replaceable>PG_ID1</replaceable> [<replaceable>PG_ID2</replaceable> ... ]
</screen>
   <para>
    Mit den Kommandos <command>cancel-*</command> wird das Flag „force“ aus den PGs entfernt, sodass sie in der standardmäßigen Reihenfolge verarbeitet werden. Auch dieser Vorgang wirkt sich nicht auf die derzeit verarbeiten Platzierungsgruppen aus, sondern lediglich auf Gruppen, die sich noch in der Warteschlange befinden. Nach der Wiederherstellung oder dem Abgleich der Gruppe wird das Flag „force“ automatisch gelöscht.
   </para>
  </sect2>

  <sect2 xml:id="op-pgs-revert">
   <title>Wiederherstellen verlorener Objekte</title>
   <para>
    Wenn mindestens ein Objekt im Cluster verloren gegangen ist und Sie nicht mehr weiter nach den verlorenen Daten suchen möchten, müssen Sie die nicht gefundenen Objekte als „verloren“ markieren.
   </para>
   <para>
    Können die Objekte auch nach Abfrage aller denkbaren Speicherorte nicht abrufen werden, müssen Sie die verlorenen Objekte ggf. aufgeben. Dieser Fall kann bei außergewöhnlichen Fehlerkombinationen auftreten, bei denen Informationen über Schreibvorgänge an den Cluster weitergegeben werden, bevor die Schreibvorgänge selbst wiederhergestellt werden.
   </para>
   <para>
    Derzeit wird ausschließlich die Option „revert“ unterstützt, mit der entweder ein Abgleich mit einer früheren Version des Objekts erfolgt oder das Objekt „vergessen“ wird (sofern ein neues Objekt vorliegt). Mit folgendem Kommando markieren Sie die nicht gefundenen („unfound“) Objekte als verloren („lost“):
   </para>
<screen>
  <prompt>cephuser@adm &gt; </prompt>ceph pg <replaceable>PG_ID</replaceable> mark_unfound_lost revert|delete
  </screen>
  </sect2>

  <sect2 xml:id="op-pgs-autoscaler">
   <title>Aktivieren der PG-Autoskalierung</title>
   <para>
    Platzierungsgruppen (PGs) sind ein internes Implementierungsdetail zur Verteilung der Daten durch Ceph. Wenn Sie die PG-Autoskalierung aktivieren, können Sie zulassen, dass der Cluster PGs entweder selbst erstellt oder automatisch einstellt, je nachdem, wie der Cluster genutzt wird.
   </para>
   <para>
    Jeder Pool im System verfügt über eine Eigenschaft <option>pg_autoscale_mode</option>, die auf <literal>off</literal>, <literal>on</literal> oder <literal>warn</literal> festgelegt werden kann:
   </para>
   <para>
    Die Autoskalierung wird auf einer Pro-Pool-Basis konfiguriert und kann in drei Modi ausgeführt werden:
   </para>
   <variablelist>
    <varlistentry>
     <term>off</term>
     <listitem>
      <para>
       Deaktiviert die Autoskalierung für diesen Pool. Es liegt im Ermessen des Administrators, eine geeignete PG-Nummer für jeden Pool zu wählen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>on</term>
     <listitem>
      <para>
       Aktiviert die automatische Anpassung der PG-Anzahl für den angegebenen Pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>warn</term>
     <listitem>
      <para>
       Löst Zustandswarnungen aus, wenn die PG-Anzahl angepasst werden sollte.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    So legen Sie den Autoskalierungsmodus für bestehende Pools fest:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> pg_autoscale_mode <replaceable>mode</replaceable></screen>
   <para>
    Mit folgendem Kommando können Sie auch den Standardmodus <option>pg_autoscale_mode</option> konfigurieren, der auf alle zukünftig erstellten Pools angewendet wird:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set global osd_pool_default_pg_autoscale_mode <replaceable>MODE</replaceable></screen>
   <para>
    Mit diesem Kommando können Sie jeden Pool, seine relative Auslastung und alle vorgeschlagenen Änderungen an der PG-Anzahl anzeigen:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool autoscale-status</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="op-crush">
  <title>Umgang mit der CRUSH-Zuordnung</title>

  <para>
   In diesem Abschnitt werden grundlegende Methoden zum Umgang mit der CRUSH-Zuordnung vorgestellt, wie Bearbeiten einer CRUSH-Zuordnung, Ändern der CRUSH-Zuordnungs-Parameter und Hinzufügen/Verschieben/Entfernen eines OSD.
  </para>

  <sect2>
   <title>Bearbeiten einer CRUSH-Zuordnung</title>
   <para>
    Gehen Sie zum Bearbeiten einer bestehenden CRUSH-Zuordnung folgendermaßen vor:
   </para>
   <procedure>
    <step>
     <para>
      Rufen Sie eine CRUSH-Zuordnung ab. Führen Sie folgendes Kommando aus, um die CRUSH-Zuordnung für Ihren Cluster abzurufen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph gibt (<option>-o</option>) eine kompilierte CRUSH-Zuordnung an den angegebenen Dateinamen aus. Da die CRUSH-Zuordnung in kompilierter Form vorliegt, muss sie vor der Bearbeitung zunächst dekompiliert werden.
     </para>
    </step>
    <step>
     <para>
      Dekompilieren Sie eine CRUSH-Zuordnung. Führen Sie zum Dekompilieren einer CRUSH-Zuordnung folgendes Kommando aus:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>crushtool -d <replaceable>compiled-crushmap-filename</replaceable> \
 -o <replaceable>decompiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph dekompiliert (<option>-d</option>) die kompilierte CRUSH-Zuordnung und gibt Sie (<option>-o</option>) an den angegebenen Dateinamen aus.
     </para>
    </step>
    <step>
     <para>
      Bearbeiten Sie mindestens einen der Geräte-, Buckets- und Regel-Parameter.
     </para>
    </step>
    <step>
     <para>
      Kompilieren Sie eine CRUSH-Zuordnung. Führen Sie zum Kompilieren einer CRUSH-Zuordnung folgendes Kommando aus:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>crushtool -c <replaceable>decompiled-crush-map-filename</replaceable> \
 -o <replaceable>compiled-crush-map-filename</replaceable></screen>
     <para>
      Ceph speichert eine kompilierte CRUSH-Zuordnung an den angegebenen Dateinamen.
     </para>
    </step>
    <step>
     <para>
      Legen Sie eine CRUSH-Zuordnung fest. Führen Sie folgendes Kommando aus, um die CRUSH-Zuordnung für Ihren Cluster festzulegen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i <replaceable>compiled-crushmap-filename</replaceable></screen>
     <para>
      Ceph gibt die CRUSH-Zuordnung des angegebenen Dateinamens als CRUSH-Zuordnung für den Cluster ein.
     </para>
    </step>
   </procedure>
   <tip>
    <title>Verwenden Sie ein Versionierungssystem</title>
    <para>
     Verwenden Sie ein Versionierungssystem (z. B. git oder svn) für die exportierten und bearbeiteten CRUSH-Zuordnung-Dateien. Damit wird ein etwaiger Abgleich vereinfacht.
    </para>
   </tip>
   <tip>
    <title>Testen Sie die neue CRUSH-Zuordnung</title>
    <para>
     Testen Sie die neue, angepasste CRUSH-Zuordnung mit dem Kommando <command>crushtool --test</command> und vergleichen Sie sie mit dem Status vor Anwendung der neuen CRUSH-Zuordnung. Nützliche Kommandoschalter: <option>--show-statistics</option>, <option>--show-mappings</option>, <option>--show-bad-mappings</option>, <option>--show-utilization</option>, <option>--show-utilization-all</option>, <option>--show-choose-tries</option>
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-crush-addosd">
   <title>Hinzufügen oder Verschieben eines OSD</title>
   <para>
    Führen Sie zum Hinzufügen eines OSD in der CRUSH-Zuordnung des aktiven Clusters folgendes Kommando aus:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush set <replaceable>id_or_name</replaceable> <replaceable>weight</replaceable> root=<replaceable>pool-name</replaceable>
<replaceable>bucket-type</replaceable>=<replaceable>bucket-name</replaceable> ...</screen>
   <variablelist>
    <varlistentry>
     <term>id</term>
     <listitem>
      <para>
       Eine Ganzzahl. Die numerische ID des OSD. Diese Option muss aktiviert sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Name</term>
     <listitem>
      <para>
       Eine Zeichenkette. Der vollständige Name des OSD. Diese Option muss aktiviert sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>weight</term>
     <listitem>
      <para>
       Der Gleitkomma-Datentyp „double“. Das CRUSH-Gewicht für den OSD. Diese Option muss aktiviert sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>root</term>
     <listitem>
      <para>
       Ein Schlüssel-Wert-Paar. Standardmäßig enthält die CRUSH-Hierarchie den Pool-Standardwert als Root. Diese Option muss aktiviert sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bucket-type</term>
     <listitem>
      <para>
       Schlüssel-Wert-Paare. Sie haben die Möglichkeit, den Standort des OSD in der CRUSH-Hierarchie anzugeben.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Im folgenden Beispiel wird <literal>osd.0</literal> zur Hierarchie hinzugefügt oder der OSD wird von einem vorigen Standort verschoben.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush set osd.0 1.0 root=data datacenter=dc1 room=room1 \
row=foo rack=bar host=foo-bar-1</screen>
  </sect2>

  <sect2 xml:id="op-crush-osdweight">
   <title>Unterschied zwischen <command>ceph osd reweight</command> und <command>ceph osd crush reweight</command></title>
   <para>
    Das „Gewicht“ eines Ceph OSD kann mit zwei ähnlichen Kommandos geändert werden. Die Kommandos unterscheiden sich durch ihren Nutzungskontext, was zu Verwirrung führen kann.
   </para>
   <sect3 xml:id="ceph-osd-reweight">
    <title><command>ceph osd reweight</command></title>
    <para>
     Wert:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd reweight <replaceable>OSD_NAME</replaceable> <replaceable>NEW_WEIGHT</replaceable></screen>
    <para>
     Mit <command>ceph osd reweight</command> wird ein überschreibendes Gewicht für das Ceph OSD festgelegt. Dieser Wert im Bereich 0 bis 1 zwingt CRUSH, die Daten zu verlagern, die sich ansonsten auf diesem Laufwerk befinden würden. Das Gewicht, das den Buckets oberhalb des OSD zugewiesen ist, wird hiermit <emphasis role="bold">nicht</emphasis> geändert. Dieser Vorgang fungiert als Korrekturmaßnahme für den Fall, dass die normale CRUSH-Distribution nicht ordnungsgemäß funktioniert. Wenn beispielsweise ein OSD bei 90 % steht und die anderen bei 40 %, können Sie dieses Gewicht senken und das Ungleichgewicht damit beheben.
    </para>
    <note>
     <title>OSD-Gewicht ist temporär</title>
     <para>
      Beachten Sie, dass <command>ceph osd reweight</command> keine dauerhafte Einstellung ist. Wird ein OSD als „out“ gekennzeichnet, wird sein Gewicht auf 0 festgelegt; sobald es wieder als „in“ gekennzeichnet wird, erhält das Gewicht den Wert 1.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="ceph-osd-crush-reweight">
    <title><command>ceph osd crush reweight</command></title>
    <para>
     Wert:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush reweight <replaceable>OSD_NAME</replaceable> <replaceable>NEW_WEIGHT</replaceable></screen>
    <para>
     Mit <command>ceph osd crush reweight</command> wird das <emphasis role="bold">CRUSH</emphasis>-Gewicht des OSD festgelegt. Dieses Gewicht ist ein willkürlicher Wert (im Allgemeinen die Größe der Festplatte in TB) und steuert die Datenmenge, die das System dem OSD zuordnet.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="op-crush-osdremove">
   <title>Entfernen eines OSD</title>
   <para>
    Führen Sie zum Entfernen eines OSD in der CRUSH-Zuordnung eines aktiven Clusters folgendes Kommando aus:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>OSD_NAME</replaceable></screen>
  </sect2>

  <sect2 xml:id="op-crush-addbaucket">
   <title>Hinzufügen eines Buckets</title>
   <para>
    Mit dem Kommando <command>ceph osd crush add-bucket</command> fügen Sie der CRUSH-Zuordnung eines aktiven Clients einen Bucket hinzu:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush add-bucket <replaceable>BUCKET_NAME</replaceable> <replaceable>BUCKET_TYPE</replaceable></screen>
  </sect2>

  <sect2 xml:id="op-crush-movebucket">
   <title>Verschieben eines Buckets</title>
   <para>
    Mit folgendem Kommando verschieben Sie einen Bucket an einen anderen Standort oder eine andere Position in der CRUSH-Zuordnung-Hierarchie:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush move <replaceable>BUCKET_NAME</replaceable> <replaceable>BUCKET_TYPE</replaceable>=<replaceable>BUCKET_NAME</replaceable> [...]</screen>
   <para>
    Beispiel:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush move bucket1 datacenter=dc1 room=room1 row=foo rack=bar host=foo-bar-1
</screen>
  </sect2>

  <sect2 xml:id="op-crush-rmbucket">
   <title>Entfernen eines Buckets</title>
   <para>
    Mit folgendem Kommando entfernen Sie einen Bucket:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>BUCKET_NAME</replaceable>
</screen>
   <note>
    <title>Nur leere Buckets</title>
    <para>
     Ein Bucket kann nur dann aus der CRUSH-Hierarchie entfernt werden, wenn er leer ist.
    </para>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="scrubbing-pgs">
  <title>Scrubbing von Platzierungsgruppen</title>

  <para>
   Ceph legt nicht nur mehrere Kopien der Objekte an, sondern schützt die Datenintegrität durch <emphasis>Scrubbing</emphasis> der Platzierungsgruppen (für weitere Informationen zu Platzierungsgruppen, siehe das <xref linkend="storage-intro-structure-pg"/>). Ceph Scrubbing ist analog zur Ausführung von <command>fsck</command> auf Objektspeicherebene zu verstehen. Ceph generiert für jede Placement Group einen Katalog aller Objekte und vergleicht jedes primäre Objekt und dessen Reproduktionen, um sicherzustellen, dass keine Objekte fehlen oder falsch abgeglichen wurden. Beim täglichen leichten Scrubbing werden die Objektgröße und Attribute geprüft. Beim wöchentlichen umfassenden Scrubbing werden die Daten gelesen und die Datenintegrität wird anhand von Prüfsummen sichergestellt.
  </para>

  <para>
   Scrubbing ist wichtig zur Sicherung der Datenintegrität, kann jedoch die Leistung beeinträchtigen. Passen Sie die folgenden Einstellungen an, um mehr oder weniger Scrubbing-Vorgänge festzulegen:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>osd max scrubs</option></term>
    <listitem>
     <para>
      Die maximale Anzahl der gleichzeitig ausgeführten Scrubbing-Operationen für ein Ceph OSD. Der Standardwert ist 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub begin hour</option>, <option>osd scrub end hour</option></term>
    <listitem>
     <para>
      Die Stunden am Tag (0 bis 24), die ein Zeitfenster für das Scrubbing definieren. Beginnt standardmäßig bei 0 und endet bei 24.
     </para>
     <important>
      <para>
       Wenn das Scrubbing-Intervall der Platzierungsgruppe die Einstellung <option>osd scrub max interval</option> überschreitet, wird das Scrubbing ungeachtet des für ein Scrubbing definierten Zeitfensters durchgeführt.
      </para>
     </important>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub during recovery</option></term>
    <listitem>
     <para>
      Lässt Scrubbing-Vorgänge bei der Wiederherstellung zu. Wird diese Option auf „false“ festgelegt, wird die Planung neuer Scrubbing-Vorgänge während einer aktiven Wiederherstellung deaktiviert. Bereits laufende Scrubbing-Vorgänge werden fortgesetzt. Diese Option ist nützlich, um die Last ausgelasteter Cluster zu reduzieren. Die Standardeinstellung ist „true“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub thread timeout</option></term>
    <listitem>
     <para>
      Der maximale Zeitraum in Sekunden vor der Zeitüberschreitung eines Scrubbing Threads. Der Standardwert ist 60.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub finalize thread timeout</option></term>
    <listitem>
     <para>
      Der maximale Zeitraum in Sekunden vor der Zeitüberschreitung eines Scrubbing Finalize Threads. Der Standardwert ist 60*10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub load threshold</option></term>
    <listitem>
     <para>
      Die normalisierte Maximallast. Ceph führt kein Scrubbing durch, wenn die Systemlast (definiert durch das Verhältnis von <literal>getloadavg()</literal> / Anzahl von <literal>online cpus</literal>) höher ist als dieser Wert. Der Standardwert ist 0.5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub min interval</option></term>
    <listitem>
     <para>
      Das Mindestintervall in Sekunden für Scrubbing-Vorgänge am Ceph OSD, wenn die Ceph-Cluster-Last gering ist. Der Standardwert ist 60*60*24 (einmal täglich).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub max interval</option></term>
    <listitem>
     <para>
      Das maximale Intervall in Sekunden für Scrubbing-Vorgänge am Ceph OSD ungeachtet der Cluster-Last. Der Standardwert ist 7*60*60*24 (einmal pro Woche).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk min</option></term>
    <listitem>
     <para>
      Die maximale Anzahl der Objektspeicher-Datenblöcke für einen einzelnen Scrubbing-Vorgang. Ceph blockiert Schreibvorgänge an einen einzelnen Datenblock während eines Scrubbing-Vorgangs. Der Standardwert ist 5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub chunk max</option></term>
    <listitem>
     <para>
      Die Mindestanzahl der Objektspeicher-Datenblöcke für einen einzelnen Scrubbing-Vorgang. Der Standardwert ist 25.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub sleep</option></term>
    <listitem>
     <para>
      Ruhezeit vor dem Scrubbing der nächsten Gruppe von Datenblöcken. Durch Erhöhen dieses Werts wird der gesamte Scrubbing-Vorgang verlangsamt. Die Client-Vorgänge insgesamt werden dadurch weniger beeinträchtigt. Der Standardwert ist 0.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub interval</option></term>
    <listitem>
     <para>
      Das Intervall für ein umfassendes Scrubbing (alle Daten werden vollständig gelesen). Die Option <option>osd scrub load threshold</option> hat keinen Einfluss auf diese Einstellung. Der Standardwert ist 60*60*24*7 (einmal pro Woche).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd scrub interval randomize ratio</option></term>
    <listitem>
     <para>
      Fügen Sie beim Planen des nächsten Scrubbing-Auftrags für eine Placement Group eine zufällige Verzögerung zum Wert <option>osd scrub min interval</option> hinzu. Die Verzögerung ist ein Zufallswert kleiner als das Ergebnis aus <option>osd scrub min interval</option> * <option>osd scrub interval randomized ratio</option>. Daher werden mit dem Standardwert die Scrubbing-Vorgänge praktisch zufällig auf das zulässige Zeitfenster von [1, 1,5] * <option>osd scrub min interval</option> verteilt. Der Standardwert ist 0.5.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>osd deep scrub stride</option></term>
    <listitem>
     <para>
      Lesegröße für einen umfassenden Scrub. Der Standardwert ist 524288 (512 kB).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
</chapter>
