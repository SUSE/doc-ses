<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>RADOS Block Device</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ein Block ist eine Folge von Byte, beispielsweise ein 4-MB-Datenblock. Blockbasierte Speicherschnittstellen werden am häufigsten zum Speichern von Daten auf rotierenden Medien wie Festplatten, CDs, Disketten verwendet. Angesichts der Omnipräsenz von Blockgeräteschnittstellen ist ein virtuelles Blockgerät für ein Massenspeichersystem wie Ceph hervorragend zur Interaktion geeignet.
 </para>
 <para>
  Ceph-Blockgeräte lassen die gemeinsame Nutzung physischer Ressourcen zu und ihre Größe kann geändert werden. Sie speichern Daten auf mehreren OSDs in einem Ceph-Cluster verteilt. Ceph-Blockgeräte nutzen die RADOS-Funktionen wie Snapshotting, Reproduktion und Konsistenz. Cephs RADOS Block Devices (RBD) interagieren mit OSDs über Kernel-Module oder die <systemitem>librbd</systemitem>-Bibliothek.
 </para>
 <figure>
  <title>RADOS-Protokoll</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Die Blockgeräte von Ceph sind sehr leistungsfähig und unbegrenzt auf Kernel-Module skalierbar. Sie unterstützen Virtualisierungslösungen wie QEMU oder Cloud-basierte Rechnersysteme wie OpenStack, die auf <systemitem class="library">libvirt</systemitem> basieren. Sie können Object Gateway, CephFS und RADOS Block Devices gleichzeitig am selben Cluster ausführen.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Kommandos für Blockgeräte</title>

  <para>
   Mit dem Kommando <command>rbd</command> werden Blockgeräte-Images erstellt, aufgelistet, intern geprüft und entfernt. Sie können es beispielsweise auch zum Klonen von Images, zum Erstellen von Snapshots, für ein Rollback eines Image zu einem Snapshot oder zum Anzeigen eines Snapshots verwenden.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Erstellen eines Blockgeräte-Image in einem reproduzierten Pool</title>
   <para>
    Bevor Sie ein Blockgerät in einen Client aufnehmen können, müssen Sie ein zugehöriges Image in einem vorhandenen Pool erstellen (siehe <xref linkend="ceph-pools"/>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Mit folgendem Kommando erstellen Sie beispielsweise das 1-GB-Image „myimage“, in dem Informationen aus dem Pool „mypool“ gespeichert werden:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Image-Größeneinheiten</title>
    <para>
     Wenn Sie die Abkürzung einer Größeneinheit („G“ oder „T“) auslassen, wird die Größe des Images in Megabyte angegeben. Mit „G“ oder „T“ nach der Zahl geben Sie Gigabyte oder Terabyte an.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Erstellen eines Blockgeräte-Image in einem Pool mit Löschcodierung</title>
   <para>
    Daten eines Blockgeräte-Image können direkt in Pools mit Löschcodierung (EC-Pools) gespeichert werden. Ein RADOS-Block-Device-Image besteht aus einem <emphasis>Data</emphasis>- und einem <emphasis>metadata</emphasis>-Teil. Sie können lediglich den Datenteil eines RADOS Blockgeräte-Image in einem EC-Pool speichern. Das Flag <option>overwrite</option> des Pools muss auf <emphasis>true</emphasis> festgelegt sein. Dies ist nur möglich, wenn alle OSDs, auf denen der Pool gespeichert ist, mit BlueStore arbeiten.
   </para>
   <para>
    Es ist nicht möglich, den Metadaten-Teil des Image in einem EC-Pool zu speichern. Sie können den reproduzierten Pool zum Speichern der Metadaten des Image mit der Option <option>--pool=</option> des Kommandos <command>rbd create</command> angeben oder <option>pool/</option> als Präfix vor dem Imagenamen angeben.
   </para>
   <para>
    Erstellen Sie einen EC-Pool:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Geben Sie den reproduzierten Pool zum Speichern von Metadaten an:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    oder:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Auflisten von Blockgeräte-Images</title>
   <para>
    Mit folgendem Kommando rufen Sie die Blockgeräte im Pool „mypool“ ab:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Abrufen von Image-Informationen</title>
   <para>
    Mit folgendem Kommando rufen Sie Informationen aus dem Image „myimage“ im Pool „mypool“ ab:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Ändern der Größe eines Blockgeräte-Image</title>
   <para>
    RADOS-Block-Device-Images werden schlank bereitgestellt, was bedeutet, dass Sie erst physischen Speicherplatz belegen, wenn Sie damit beginnen, Daten darin zu speichern. Ihre Kapazität ist jedoch auf den Wert beschränkt, den Sie mit der Option <option>--size</option> festlegen. Führen Sie folgendes Kommando aus, wenn Sie die maximale Größe des Image erhöhen (oder verringern) möchten:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Entfernen eines Blockgeräte-Image</title>
   <para>
    Mit folgendem Kommando entfernen Sie ein Blockgerät, das dem Image „myimage“ im Pool „mypool“ entspricht:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Ein- und Aushängen</title>

  <para>
   Nach Erstellung eines RADOS Block Device können Sie es wie jedes andere Datenträgergerät nutzen: Sie können es formatieren, für den Dateiaustausch einhängen und danach wieder aushängen.
  </para>

  <para>
   Das Kommando <command>rbd</command> greift standardmäßig mit dem Ceph-<literal>Admin</literal>-Benutzerkonto auf den Cluster zu. Dieses Konto hat vollen Verwaltungszugriff auf den Cluster. Dadurch besteht die Gefahr, dass versehentlich Schaden angerichtet wird, ähnlich wie bei der Anmeldung an einem Linux-Arbeitsplatzrechner als <systemitem class="username">root</systemitem>. Daher ist es vorzuziehen, Benutzerkonten mit weniger Rechten zu erstellen und diese Konten für den normalen Lese-/Schreibzugriff auf RADOS-Blockgeräte zu verwenden.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Erstellen eines Ceph-Benutzerkontos</title>
   <para>
    Erstellen Sie ein neues Benutzerkonto mit Ceph-Manager-, Ceph-Monitor- und Ceph-OSD-Funktionen über das Kommando <command>ceph</command> mit dem Unterkommando <command>auth get-or-create</command>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Erstellen Sie beispielsweise einen Benutzer namens <replaceable>qemu</replaceable> mit Lese- und Schreibzugriff auf die Pool-<replaceable>vms</replaceable> und Nur-Lese-Zugriff auf die Pool-<replaceable>Images</replaceable> mit folgendem Kommando:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    Die Ausgabe des Kommandos <command>ceph auth get-or-create</command> ist der Schlüsselring für den angegebenen Benutzer, der in <filename>/etc/ceph/ceph.client.<replaceable>Id.keyring</replaceable>.keyring</filename> geschrieben werden kann.
   </para>
   <note>
    <para>
     Wenn Sie das Kommando <command>rbd</command> verwenden, können Sie die Benutzer-ID mit dem optionalen Argument <command>--id</command>
     <replaceable>ID</replaceable> angeben.
    </para>
   </note>
   <para>
    Weitere Details zur Verwaltung von Ceph-Benutzerkonten finden Sie in <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Benutzerauthentifizierung</title>
   <para>
    Geben Sie einen Benutzernamen mit <option>--id <replaceable>user-name</replaceable></option> an. Bei der <systemitem>cephx</systemitem>-Authentifizierung müssen Sie außerdem ein Geheimnis angeben. Es kann von einem Schlüsselbund stammen oder aus einer Datei, die das Geheimnis enthält:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    oder
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Vorbereiten eines RADOS-Blockgeräts für den Einsatz</title>
   <procedure>
    <step>
     <para>
      Stellen Sie sicher, dass Ihr Ceph-Cluster einen Pool mit dem Festplatten-Image enthält, das zugeordnet werden soll. Nehmen wir an, der Name des Pools lautet <literal>mypool</literal> und das Image ist <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Ordnen Sie das Image einem neuen Blockgerät zu:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Listen Sie alle zugeordneten Geräte auf:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      Das Gerät, mit dem wir arbeiten möchten, heißt <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>RBD-Gerätepfad</title>
      <para>
       Statt <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename> können Sie <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename> als dauerhaften Gerätepfad verwenden. Beispiel:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Erstellen Sie am Gerät namens <filename>/dev/rbd0</filename> ein XFS-Dateisystem:
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Ersetzen Sie <filename>/mnt</filename> durch Ihren Einhängepunkt, hängen Sie das Gerät ein und prüfen Sie, ob es korrekt eingehängt ist:
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Nun können Sie Daten auf das und vom Gerät verschieben, als wäre es ein lokales Verzeichnis.
     </para>
     <tip>
      <title>Vergrößern des RBD-Geräts</title>
      <para>
       Wenn sich herausstellt, dass die Größe des RBD-Geräts nicht mehr ausreicht, lässt es sich leicht vergrößern.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Vergrößern Sie das RBD-Image, beispielsweise auf 10 GB.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Erweitern Sie das Dateisystem, bis es die neue Größe des Geräts ausfüllt:
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Wenn Sie Ihre Arbeit an dem Gerät beenden, können Sie dessen Zuordnung aufheben und das Gerät aushängen.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>Manuelles Einhängen und Aushängen</title>
    <para>
     Ein <command>rbdmap</command>-Skript und eine <systemitem class="daemon">systemd</systemitem>-Einheit werden bereitgestellt, um den Prozess des Zuordnens und Einhängens von RBDs nach dem Booten und des Aushängens vor dem Herunterfahren reibungsloser zu gestalten. Weitere Informationen finden Sie unter <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>Rbdmap</command> – Zuordnen von RBD-Geräten beim Booten</title>
   <para>
    Das Shell-Skript <command>rbdmap</command> automatisiert die Operationen <command>rbd map</command> und <command>rbd device unmap</command> an einem oder mehreren RBD-Images. Obwohl Sie das Skript jederzeit manuell ausführen können, ist sein wichtigster Vorteil, es automatisch zuordnen und die RBD-Images beim Booten einhängen (und sie beim Herunterfahren aushängen und die Zuordnung aufheben) zu können. Dieser Vorgang wird vom Init-System ausgelöst. Zu diesem Zweck ist eine Datei für die <systemitem class="daemon">systemd</systemitem>-Einheit (<filename>rbdmap.service</filename>) im Paket <systemitem>ceph-common</systemitem> enthalten.
   </para>
   <para>
    Das Skript nimmt ein einzelnes Argument, entweder <option>map</option> oder <option>unmap</option>. In beiden Fällen analysiert das Skript eine Konfigurationsdatei. Die Standardeinstellung <filename>/etc/ceph/rbdmap</filename> kann mit der Umgebungsvariablen <literal>RBDMAPFILE</literal> überschrieben werden. Jede Zeile der Konfigurationsdatei entspricht einem RBD-Image, das zugeordnet oder dessen Zuordnung aufgehoben werden soll.
   </para>
   <para>
    Die Konfigurationsdatei hat das folgende Format:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Pfad zu einem Image in einem Pool. Geben Sie diesen als <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable> an.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Eine optionale Liste der Parameter, die an das zugrunde liegende Kommando <command>rbd device map</command> weitergegeben werden sollen. Diese Parameter und ihre Werte sollten als durch Komma getrennte Zeichenkette angegeben werden, wie zum Beispiel:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       Im Beispiel führt das Skript <command>rbdmap</command> folgendes Kommando aus:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       Im folgenden Beispiel wird erläutert, wie Sie einen Benutzernamen und einen Schlüsselbund mit zugehörigem Geheimnis angeben:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Wenn das Skript als <command>rbdmap map</command> ausgeführt wird, analysiert es die Konfigurationsdatei und versucht, für jedes angegebene RBD-Image zunächst das Image zuzuordnen (mit dem Kommando <command>rbd device map</command>) und dann das Image einzuhängen.
   </para>
   <para>
    Wenn es als <command>rbdmap unmap</command> ausgeführt wird, werden die in der Konfigurationsdatei aufgelisteten Images ausgehängt und ihre Zuordnungen werden aufgehoben.
   </para>
   <para>
    <command>rbdmap unmap-all</command> versucht, alle aktuell zugeordneten RBD-Images auszuhängen und danach deren Zuordnungen aufzuheben, unabhängig davon, ob sie in der Konfigurationsdatei aufgelistet sind.
   </para>
   <para>
    Bei erfolgreicher Ausführung wird mit dem Vorgang <command>rbd device map</command> das Image einem <filename>/dev/rbdX</filename>-Gerät zugeordnet. Zu diesem Zeitpunkt wird eine udev-Regel ausgelöst, um einen symbolischen Link zu einem Geräteanzeigenamen <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename> zu erstellen, der auf das reale zugeordnete Gerät zeigt.
   </para>
   <para>
    Damit das Einhängen und Aushängen erfolgreich ausgeführt wird, muss der Anzeigename des Geräts einen entsprechenden Eintrag in <filename>/etc/fstab</filename> haben. Geben Sie beim Schreiben von <filename>/etc/fstab</filename>-Einträgen für RBD-Images die Einhängeoption „noauto“ (oder „nofail“) an. Dadurch wird verhindert, dass das Init-System das Gerät zu früh einhängt, also noch bevor das betreffende Gerät überhaupt vorhanden ist, weil <filename>rbdmap.service</filename> normalerweise ziemlich spät in der Boot-Sequenz ausgelöst wird.
   </para>
   <para>
    Eine vollständige Liste der <command>rbd</command>-Optionen finden Sie  auf der <command>rbd</command>-Handbuchseite (<command>man 8 rbd</command>).
   </para>
   <para>
    Beispiele zur Anwendung von <command>rbdmap</command> finden Sie auf der <command>rbdmap</command>-Handbuchseite (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Vergrößern von RBD-Geräten</title>
   <para>
    Wenn sich herausstellt, dass die Größe des RBD-Geräts nicht mehr ausreicht, lässt es sich leicht vergrößern.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Vergrößern Sie das RBD-Image, beispielsweise auf 10 GB
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Erweitern Sie das Dateisystem, bis es die neue Größe des Geräts ausfüllt.
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Aufnahmen</title>

  <para>
   Ein RBD-Image ist eine Snapshot eines RADOS-Block-Device-Image. Mit Snapshots behalten Sie den Verlauf des Zustands eines Image bei: Ceph unterstützt auch ein Snapshot Layering zum schnellen und einfachen Klonen von VM-Images. Ceph unterstützt Blockgeräte-Snapshots mit dem Kommando <command>rbd</command> sowie viele übergeordnete Schnittstellen wie QEMU, <systemitem>libvirt</systemitem>, OpenStack und CloudStack.
  </para>

  <note>
   <para>
    Halten Sie die Eingabe- und Ausgabeoperationen an und entfernen Sie alle ausstehenden Schreibvorgänge, bevor Sie einen Snapshot eines Images anfertigen. Wenn das Image ein Dateisystem enthält, muss sich das Dateisystem zum Zeitpunkt der Snapshot-Erstellung in einem konsistenten Zustand befinden.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Aktivieren und Konfigurieren von <systemitem>cephx</systemitem></title>
   <para>
    Wenn <systemitem>cephx</systemitem> aktiviert ist, dann müssen Sie einen Benutzernamen oder eine ID und einen Pfad zum Schlüsselbund mit dem entsprechenden Schlüssel für den Benutzer angeben. Weitere Einzelheiten finden Sie unter <xref linkend="cha-storage-cephx"/>. Es ist auch möglich, die Umgebungsvariable <systemitem>CEPH_ARGS</systemitem> hinzuzufügen, um die erneute Eingabe der folgenden Parameter zu verhindern.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Fügen Sie den Benutzer und das Geheimnis zur Umgebungsvariable <systemitem>CEPH_ARGS</systemitem> hinzu, damit Sie diese Informationen nicht jedes Mal neu eingeben müssen.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Allgemeine Informationen zu Snapshots</title>
   <para>
    Das folgende Verfahren zeigt, wie Snapshots mit dem Kommando <command>rbd</command> in der Kommandozeile erstellt, aufgelistet und entfernt werden.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Erstellen von Snapshots</title>
    <para>
     Geben Sie zum Erstellen eines Snapshots mit <command>rbd</command> die Option <option>snap create</option>, den Pool-Namen und den Image-Namen an.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Auflisten von Snapshots</title>
    <para>
     Geben Sie zum Auflisten von Snapshots eines Image den Pool-Namen und den Image-Namen an.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Rollback von Snapshots</title>
    <para>
     Geben Sie zur Durchführung eines Rollbacks zu einem Snapshot mit <command>rbd</command> die Option <option>snap rollback</option>, den Pool-Namen, den Image-Namen und den Namen des Snapshots an.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Bei einem Rollback eines Image zu einem Snapshot wird die aktuelle Version des Image mit den Daten aus einem Snapshot überschrieben. Ein Rollback dauert umso länger je größer das Image ist. Einen Snapshot zu <emphasis>klonen ist schneller</emphasis> als ein <emphasis>Rollback</emphasis> eines Image zu einem Snapshot durchzuführen und ist die bevorzugte Methode, wenn zu einem früheren Zustand zurückgekehrt werden soll.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Löschen eines Snapshots</title>
    <para>
     Geben Sie zum Löschen eines Snapshots mit <command>rbd</command> die Option <option>snap rm</option>, den Pool-Namen, den Image-Namen und den Benutzernamen an.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Ceph OSDs löschen Daten asynchron. Daher wird beim Löschen eines Snapshots nicht sofort Festplattenspeicherplatz frei.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Bereinigen von Snapshots</title>
    <para>
     Geben Sie zum Entfernen aller Snapshots für ein Image mit <command>rbd</command> die Option <option>snap purge</option> und den Image-Namen an.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Snapshot-Layering</title>
   <para>
    Ceph unterstützt die Möglichkeit zur Erstellung mehrerer COW(copy-on-write)-Klone eines Blockgeräte-Snapshots. Durch ein Snapshot Layering können Ceph-Blockgeräte-Clients Images sehr schnell erstellen. Beispielsweise könnten Sie ein Blockgeräte-Image mit einem darauf geschriebenen Linux VM und danach einen Snapshot von diesem Image erstellen, dann den Snapshot schützen und beliebig viele COW-Klone erstellen. Ein Snapshot ist schreibgeschützt. Daher vereinfacht ein Snapshot die Semantik und ermöglicht es, Klone schnell zu erstellen.
   </para>
   <note>
    <para>
     Die im folgenden Kommandozeilenbeispiel genannten Elemente „parent“ (übergeordnet) und „child“ (untergeordnet) beziehen sich auf einen Ceph-Blockgeräte-Snapshot (parent) und das entsprechende Image, das vom Snapshot geklont wurde (child).
    </para>
   </note>
   <para>
    Jedes geklonte Image (child) speichert einen Verweis auf das übergeordnete Image (parent), wodurch das geklonte Image den übergeordneten Snapshot (parent) öffnen und lesen kann.
   </para>
   <para>
    Ein COW-Klon eines Snapshots verhält sich exakt genauso wie jedes andere Ceph-Blockgeräte-Image. Geklonte Images können gelesen, geschrieben und geklont werden und ihre Größe lässt sich ändern. Geklonte Images haben keine besonderen Einschränkungen. Der COW-Klon eines Snapshots verweist jedoch auf den Snapshot. Daher <emphasis>müssen</emphasis> Sie den Snapshot schützen, bevor Sie ihn klonen.
   </para>
   <note>
    <title><option>--image-format 1</option> wird nicht unterstützt</title>
    <para>
     Sie können keine Snapshots von Images anfertigen, die mit der veralteten Option <command>rbd create --image-format 1</command> erstellt wurden. Ceph unterstützt lediglich das Klonen der standardmäßigen <emphasis>format 2</emphasis>-Images.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Erste Schritte mit Layering</title>
    <para>
     Ceph-Blockgeräte-Layering ist ein einfacher Prozess. Sie benötigen ein Image. Sie müssen einen Snapshot vom Image erstellen. Sie müssen den Snapshot schützen. Nach Ausführung dieser Schritte beginnen Sie mit dem Klonen des Snapshots.
    </para>
    <para>
     Das geklonte Image verweist auf den übergeordneten Snapshot und enthält Pool-ID, Image-ID und Snapshot-ID. Durch die enthaltene Pool-ID können Snapshots von einem Pool zu Images in einem anderen Pool geklont werden.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Image-Schablone</emphasis>: Bei einem üblichen Anwendungsfall für Blockgeräte-Layering wird ein Master Image erstellt und ein Snapshot, der als Schablone für Klone dient. Beispielsweise erstellt ein Benutzer ein Image für eine Linux-Distribution (zum Beispiel SUSE Linux Enterprise Server) und dann einen Snapshot dafür. Der Benutzer aktualisiert möglicherweise das Image regelmäßig und erstellt einen neuen Snapshot (zum Beispiel <command>zypper ref &amp;&amp; zypper patch</command> gefolgt von <command>rbd snap create</command>). So wie sich das Image weiterentwickelt, kann der Benutzer beliebige einzelne Snapshots klonen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Erweiterte Schablone</emphasis>: Bei einem anspruchsvolleren Anwendungsfall wird ein Schablonen-Image erweitert, das mehr Informationen als eine Basis-Image enthält. Beispielsweise könnte ein Benutzer ein Image (eine VM-Schablone) klonen und weitere Software installieren (beispielsweise eine Datenbank, ein Content-Management-System oder ein Analysesystem) und dann einen Snapshot des erweiterten Image erstellen, das genauso wie das Basis-Image aktualisiert wird.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Schablonen-Pool</emphasis>: Eine Methode des Blockgeräte-Layerings ist die Erstellung eines Pools, der Master-Images enthält, die als Schablonen fungieren, sowie Snapshots dieser Schablonen. Sie könnten dann Nur-Lesen-Berechtigungen an Benutzer verteilen. Die Benutzer haben dadurch die Möglichkeit, die Snapshots zu klonen, dürfen jedoch nicht im Pool schreiben oder Vorgänge ausführen.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Image-Migration/Wiederherstellung</emphasis>: Eine Methode des Blockgeräte-Layerings ist die Migration oder Wiederherstellung von Daten von einem Pool in einen anderen Pool.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Schützen eines Snapshots</title>
    <para>
     Klone greifen auf die übergeordneten Snapshots zu. Alle Klone würden zerstört werden, wenn ein Benutzer versehentlich den übergeordneten Snapshot löscht. Sie müssen den Snapshot schützen, bevor Sie ihn klonen, um Datenverlust zu verhindern.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Geschützte Snapshots können nicht gelöscht werden.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Klonen eines Snapshots</title>
    <para>
     Zum Klonen eines Snapshots müssen Sie den übergeordneten Pool, das Image, den Snapshot, den untergeordneten Pool und den Image-Namen angeben. Der Snapshot muss vor dem Klonen geschützt werden.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Ein Snapshot kann von einem Pool zu einem Image in einem anderen Pool geklont werden. Sie könnten beispielsweise schreibgeschützte Images und Snapshots als Schablonen in einem Pool beibehalten und beschreibbare Klone in einem anderen Pool.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Aufheben des Schutzes eines Snapshots</title>
    <para>
     Vor dem Löschen eines Snapshots muss zunächst dessen Schutz aufgehoben werden. Außerdem dürfen Sie <emphasis>keine</emphasis> Snapshots löschen, die Verweise von Klonen enthalten. Sie müssen jeden Klon eines Snapshots vereinfachen, bevor Sie den Snapshot löschen.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Auflisten der untergeordneten Klone eines Snapshots</title>
    <para>
     Führen Sie zum Auflisten der untergeordneten Klone eines Snapshots folgendes Kommando aus:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Vereinfachen eines geklonten Images</title>
    <para>
     Geklonte Images behalten einen Verweis auf den übergeordneten Snapshot bei. Wenn Sie den Verweis vom untergeordneten Klon zum übergeordneten Snapshot entfernen, wird das Image tatsächlich „vereinfacht“, indem die Informationen vom Snapshot zum Klon kopiert werden. Die Vereinfachung eines Klons dauert umso länger je größer der Snapshot ist. Zum Löschen eines Snapshots müssen Sie zunächst die untergeordneten Images vereinfachen.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Da ein vereinfachtes Image alle Informationen des Snapshots enthält, belegt ein vereinfachtes Image mehr Speicherplatz als ein Layering-Klon.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>RBD-Image-Spiegel</title>

  <para>
   RBD-Images können asynchron zwischen zwei Ceph-Clustern gespiegelt werden. Diese Funktion ist in zwei Modi verfügbar:
  </para>

  <variablelist>
   <varlistentry>
    <term>Journal-basiert</term>
    <listitem>
     <para>
      Bei diesem Modus wird die Funktion des RBD-Journaling-Image verwendet, um die absturzkonsistente Reproduktion zwischen Clustern sicherzustellen. Jeder Schreibvorgang in das RBD-Image wird zunächst im zugehörigen Journal aufgezeichnet, bevor das eigentliche Image geändert wird. Der <literal>Remote</literal>-Cluster liest aus dem Journal und gibt die Aktualisierungen in seiner lokalen Kopie des Image wieder. Da jeder Schreibvorgang auf das RBD-Image zu zwei Schreibvorgängen auf dem Ceph-Cluster führt, müssen Sie bei Verwendung der RBD-Journaling-Image-Funktion mit nahezu doppelt so langen Schreiblatenzen rechnen.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Snapshot-basiert</term>
    <listitem>
     <para>
      Dieser Modus verwendet periodisch geplante oder manuell erstellte RBD-Image-Spiegel-Snapshots, um absturzsichere RBD-Images zwischen Clustern zu reproduzieren. Der <literal>Remote</literal>-Cluster ermittelt alle Daten- oder Metadaten-Aktualisierungen zwischen zwei Spiegel-Snapshots und kopiert die Deltas in seine lokale Kopie des Image. Mit Hilfe der RBD-Image-Funktion „fast-diff“ können aktualisierte Datenblöcke schnell berechnet werden, ohne dass das gesamte RBD-Image abgesucht werden muss. Da dieser Modus nicht zeitpunktkonsistent ist, muss das vollständige Snapshot-Delta vor der Verwendung in einem Failover-Szenario synchronisiert werden. Alle teilweise angewendeten Snapshot-Deltas werden vor der Verwendung auf den letzten vollständig synchronisierten Snapshot zurückgesetzt.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Die Spiegelung wird pro Pool in den Peer-Clustern konfiguriert. Dies kann für eine bestimmte Untergruppe von Images innerhalb des Pools konfiguriert werden oder so, dass alle Images innerhalb eines Pools automatisch gespiegelt werden, wenn nur die Journal-basierte Spiegelung verwendet wird. Die Spiegelung wird mit dem <command>rbd</command>-Kommando ausgeführt. Der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon ist dafür zuständig, Image-Aktualisierungen aus dem <literal>Remote</literal>-Peer-Cluster zu entnehmen und sie auf das Image im <literal>lokalen</literal> Cluster anzuwenden.
  </para>

  <para>
   Abhängig von den gewünschten Anforderungen an die Reproduktion kann die RBD-Spiegelung entweder für eine ein- oder zweiseitige Reproduktion konfiguriert werden:
  </para>

  <variablelist>
   <varlistentry>
    <term>Einseitige Reproduktion</term>
    <listitem>
     <para>
      Wenn Daten nur von einem primären Cluster auf einen sekundären Cluster gespiegelt werden, läuft der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon nur auf dem sekundären Cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Zweiseitige Reproduktion</term>
    <listitem>
     <para>
      Wenn Daten von primären Images auf einem Cluster auf nicht primäre Images auf einem anderen Cluster gespiegelt werden (und umgekehrt), wird der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon auf beiden Clustern ausgeführt.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Jede Instanz des <systemitem class="daemon">rbd-mirror</systemitem>-Daemons muss in der Lage sein, sich gleichzeitig mit dem <literal>lokalen</literal> und dem <literal>Remote</literal>-Ceph-Cluster zu verbinden, beispielsweise mit allen Überwachungs- und OSD-Hosts. Außerdem muss das Netzwerk über eine ausreichende Bandbreite zwischen den beiden Rechenzentren verfügen, um die Spiegelung des Workloads zu bewältigen.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Pool-Konfiguration</title>
   <para>
    Die folgenden Verfahren zeigen, wie einfache Verwaltungsaufgaben zum Konfigurieren der Spiegelung mit dem <command>rbd</command>-Kommando ausgeführt werden. Die Spiegelung wird pro Pool in den Ceph-Clustern konfiguriert.
   </para>
   <para>
    Sie müssen die Schritte zur Pool-Konfiguration an beiden Peer-Clustern ausführen. Bei diesen Verfahren wird der Einfachheit halber angenommen, dass von einem einzelnen Host aus auf zwei Cluster (<literal>lokal</literal> und <literal>Remote</literal>) zugegriffen werden kann.
   </para>
   <para>
    Weitere detaillierte Informationen zum Herstellen einer Verbindung zu verschiedenen Ceph-Clustern finden Sie auf der <command>rbd</command>-Handbuchseite (<command>man 8 rbd</command>).
   </para>
   <tip>
    <title>Mehrere Cluster</title>
    <para>
     Der Clustername in den folgenden Beispielen entspricht einer Ceph-Konfigurationsdatei mit dem gleichen Namen <filename>/etc/ceph/remote.conf</filename> und einer Ceph-Schlüsselbunddatei mit dem gleichen Namen <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Aktivieren der Spiegelung für einen Pool</title>
    <para>
     Geben Sie zum Aktivieren der Spiegelung in einem Pool das Unterkommando <command>mirror pool enable</command>, den Pool-Namen und den Spiegelungsmodus an. Der Spiegelungsmodus kann entweder „pool“ oder „image“ lauten:
    </para>
    <variablelist>
     <varlistentry>
      <term>Speicherplatz auf dem Pool</term>
      <listitem>
       <para>
        Alle Images im Pool mit aktivierter Journaling-Funktion werden gespiegelt.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Image</term>
      <listitem>
       <para>
        Die Spiegelung muss auf jedem Image explizit aktiviert werden. Weitere Informationen zu diesem Thema finden Sie unter dem Stichwort <xref linkend="rbd-mirror-enable-image-mirroring"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Deaktivieren der Spiegelung</title>
    <para>
     Geben Sie zum Deaktivieren der Spiegelung in einem Pool das Unterkommando <command>mirror pool disable</command> und den Pool-Namen an. Wenn die Spiegelung auf diese Weise in einem Pool deaktiviert wird, dann wird sie auch in anderen Images (im Pool) deaktiviert, für die eine Spiegelung explizit aktiviert wurde.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Bootstrapping von Peers</title>
    <para>
     Damit der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon seinen Peer-Cluster erkennen kann, muss der Peer im Pool registriert und ein Benutzerkonto angelegt werden. Dieser Vorgang kann mit <command>rbd</command> und den Kommandos <command>mirror pool peer bootstrap create</command> und <command>mirror pool peer bootstrap import</command> automatisiert werden.
    </para>
    <para>
     Um ein neues Bootstrap-Token mit <command>rbd</command> manuell zu erstellen, geben Sie das Kommando <command>mirror pool peer bootstrap create</command>, einen Pool-Namen sowie einen optionalen Anzeigenamen für den Standort zur Beschreibung des <literal>lokalen</literal> Clusters an:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Die Ausgabe von <command>mirror pool peer bootstrap create</command> ist ein Token, das dem Kommando <command>mirror pool peer bootstrap import</command> übergeben werden sollte, beispielsweise am <literal>lokalen</literal> Cluster:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Importieren Sie über folgende Syntax mit dem Kommando <command>rbd</command> das von einem anderen Cluster erstellte Bootstrap-Token manuell:
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Ort:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        Ein optionaler Anzeigename für den Standort zum Beschreiben des <literal>lokalen</literal> Clusters.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Eine Spiegelrichtung. Die Standardeinstellung ist <literal>rx-tx</literal> für die zweiseitige Spiegelung, kann aber auch auf <literal>rx-only</literal> für die einseitige Spiegelung festgelegt werden.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        Name des Pools.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        Ein Dateipfad zum erstellten Token (oder <literal>-</literal>, um es von der Standardeingabe zu lesen),
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     beispielsweise am <literal>Remote</literal>-Cluster:
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Manuelles Hinzufügen eines Cluster-Peers</title>
    <para>
     Alternativ zum Bootstrapping von Peers, wie in <xref linkend="ceph-rbd-mirror-bootstrap-peer"/> beschrieben, können Sie Peers auch manuell angeben. Der entfernte <systemitem class="daemon">rbd-mirror</systemitem>-Daemon benötigt zur Durchführung der Spiegelung Zugriff auf den lokalen Cluster. Erstellen Sie einen neuen lokalen Ceph-Benutzer, den der entfernte <systemitem class="daemon">rbd-mirror</systemitem>-Daemon verwendet, z. B. <literal>rbd-mirror-peer</literal>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Verwenden Sie die folgende Syntax, um einen Spiegelungs-Peer-Ceph-Cluster mit dem Kommando <command>rbd</command> hinzuzufügen:
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Standardmäßig muss der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon Zugriff auf die Ceph-Konfigurationsdatei haben, die sich unter <filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename> befindet. Es stellt IP-Adressen der MONs des Peer-Clusters und einen Schlüsselring für einen Client namens <replaceable>CLIENT_NAME</replaceable> bereit, der sich in den Standard- oder benutzerdefinierten Schlüsselring-Suchpfaden befindet, z. B. <filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>.
    </para>
    <para>
     Alternativ kann der MON- und/oder Client-Schlüssel des Peer-Clusters sicher im lokalen config-Schlüsselspeicher von Ceph gespeichert werden. Geben Sie die Verbindungsattribute des Peer-Clusters beim Hinzufügen eines Spiegelungs-Peers mit den Optionen <option>--remote-mon-host</option> und <option>--remote-key-file</option> an. Beispiel:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Entfernen des Cluster-Peers</title>
    <para>
     Geben Sie zum Entfernen eines Peer Clusters für die Spiegelung das Unterkommando <command>mirror pool peer remove</command>, den Pool-Namen und die Peer-UUID (verfügbar über das Kommando <command>rbd mirror pool info</command>) an:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Datenpools</title>
    <para>
     Beim Erstellen von Images im Zielcluster wählt <systemitem class="daemon">rbd-mirror</systemitem> einen Datenpool wie folgt aus:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Wenn der Ziel-Cluster einen Standard-Datenpool konfiguriert hat (mit der Konfigurationsoption <option>rbd_default_data_pool</option>), wird dieser verwendet.
      </para>
     </listitem>
     <listitem>
      <para>
       Andernfalls, wenn das Quell-Image einen separaten Datenpool verwendet und ein Pool mit demselben Namen auf dem Ziel-Cluster vorhanden ist, wird dieser Pool verwendet.
      </para>
     </listitem>
     <listitem>
      <para>
       Wenn keiner der beiden oben genannten Punkte zutrifft, wird kein Datenpool festgelegt.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>RBD-Image-Konfiguration</title>
   <para>
    Im Gegensatz zur Pool-Konfiguration muss die Image-Konfiguration nur für einen einzigen für die Spiegelung vorgesehenen Peer Ceph-Cluster durchgeführt werden.
   </para>
   <para>
    Gespiegelte RBD-Images werden entweder als <emphasis>primär</emphasis> oder <emphasis>nicht primär</emphasis> ausgewiesen. Dies ist eine Eigenschaft des Image und nicht des Pools. Als nicht primär ausgewiesene Images können nicht bearbeitet werden.
   </para>
   <para>
    Images werden automatisch zu primären Images hochgestuft, wenn die Spiegelung zuvor für ein Image aktiviert wird (entweder implizit mit dem Pool-Spiegelungsmodus „pool“ und durch Aktivieren der Journaling-Funktion für das Image oder explizit (Informationen hierzu finden Sie in <xref linkend="rbd-mirror-enable-image-mirroring"/>) mit dem <command>rbd</command>-Kommando).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Aktivieren der Image-Spiegelung</title>
    <para>
     Wenn eine Spiegelung im Modus <literal>Image</literal> konfiguriert ist, muss die Spiegelung für jedes Image im Pool explizit aktiviert werden. Geben Sie zum Aktivieren der Spiegelung für ein bestimmtes Image mit <command>rbd</command> das Unterkommando <command>mirror image enable</command> zusammen mit dem Pool- und dem Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Der Spiegel-Image-Modus kann entweder <literal>Journal</literal> oder <literal>Snapshot</literal> lauten:
    </para>
    <variablelist>
     <varlistentry>
      <term>Journal (Standardeinstellung)</term>
      <listitem>
       <para>
        Wenn die Spiegelung im <literal>Journal</literal>-Modus konfiguriert ist, wird mit der RBD-Journaling-Image-Funktion der Image-Inhalt reproduziert. Wenn die RBD-Journaling-Image-Funktion noch nicht für das Image aktiviert ist, wird sie automatisch aktiviert.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Snapshot</term>
      <listitem>
       <para>
        Wenn die Spiegelung im <literal> Snapshot</literal>-Modus konfiguriert ist, verwendet sie RBD-Image-Spiegel-Snapshots zum Reproduzieren des Image-Inhalts. Sobald diese Option aktiviert ist, wird automatisch ein erster Spiegel-Snapshot erstellt. Zusätzliche RBD-Image-Spiegel-Snapshots können mit dem Kommando <command>rbd</command> erstellt werden.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Aktivieren der Image-Journaling-Funktion</title>
    <para>
     Bei der RBD-Spiegelung wird immer die RBD Journaling-Funktion verwendet, um sicherzustellen, dass das reproduzierte Image immer absturzkonsistent bleibt. Bei Verwendung des <literal>Image</literal>-Spiegelungsmodus wird die Journaling-Funktion automatisch aktiviert, wenn die Spiegelung auf dem Image aktiviert ist. Wenn Sie den <literal>Pool</literal>-Spiegelungsmodus verwenden, muss die RBD-Image-Journaling-Funktion aktiviert sein, bevor ein Image auf einen Peer-Cluster gespiegelt werden kann. Die Funktion kann bei der Image-Erstellung durch Angabe der Option <option>--image-feature exclusive-lock,journaling</option> für das <command>rbd</command>-Kommando aktiviert werden.
    </para>
    <para>
     Alternativ kann die Journaling-Funktion für bereits vorhandene RBD-Images dynamisch aktiviert werden. Geben Sie zum Aktivieren des Journaling den Unterbefehl <command>feature enable</command>, den Pool-Namen, den Image-Namen und den Namen der Funktion an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Abhängigkeit der Option</title>
     <para>
      Die Funktion <option>journaling</option> hängt von der Funktion <option>exclusive-lock</option> ab. Wenn die Funktion <option>exclusive-lock</option> nicht bereits aktiviert ist, müssen Sie diese vor Aktivierung der Funktion <option>journaling</option> aktivieren.
     </para>
    </note>
    <tip>
     <para>
      Sie können das Journaling für alle neuen Images standardmäßig aktivieren, indem Sie <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option> zu Ihrer Ceph-Konfigurationsdatei hinzufügen.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Erstellen von Image-Spiegel-Snapshots</title>
    <para>
     Bei der Snapshot-basierten Spiegelung müssen immer dann Spiegel-Snapshots erstellt werden, wenn der geänderte Inhalt des RBD-Image gespiegelt werden soll. Geben Sie zum manuellen Erstellen eines Spiegel-Snapshots mit <command>rbd</command> das Kommando <command>mirror image snapshot</command> zusammen mit dem Pool- und dem Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Standardmäßig werden nur drei Spiegel-Snapshots pro Image erstellt. Der letzte Spiegel-Snapshot wird automatisch entfernt, wenn das Limit erreicht ist. Das Limit kann bei Bedarf über die Konfigurationsoption <option>rbd_mirroring_max_mirroring_snapshots</option> außer Kraft gesetzt werden. Außerdem werden Spiegel-Snapshots automatisch gelöscht, wenn das Image entfernt oder die Spiegelung deaktiviert wird.
    </para>
    <para>
     Spiegel-Snapshots können auch automatisch auf periodischer Basis erstellt werden, wenn Spiegel-Snapshot-Zeitpläne definiert sind. Der Spiegel-Snapshot kann auf globaler Ebene, pro Pool oder pro Image geplant werden. Es können zwar mehrere Zeitpläne für Spiegel-Snapshots auf jeder Ebene definiert werden, doch nur die spezifischsten Snapshot-Zeitpläne, die zu einem einzelnen gespiegelten Image passen, werden ausgeführt.
    </para>
    <para>
     Geben Sie zum Erstellen eines Spiegel-Snapshot-Zeitplans mit <command>rbd</command> das Kommando <command>mirror snapshot schedule add</command> zusammen mit einem optionalen Pool- oder Image-Namen, einem Intervall und einer optionalen Startzeit an.
    </para>
    <para>
     Das Intervall kann in Tagen, Stunden oder Minuten mit den entsprechenden Suffixen <option>d</option>, <option>h</option> oder <option>m</option> angegeben werden. Die optionale Startzeit wird im ISO 8601-Zeitformat angegeben. Beispiel:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Geben Sie zum Entfernen eines Spiegel-Snapshot-Zeitplans mit <command>rbd</command> das Kommando <command>mirror snapshot schedule remove</command> mit Optionen an, die dem entsprechenden Kommando „add schedule“ entsprechen.
    </para>
    <para>
     Geben Sie zum Auflisten aller Snapshot-Zeitpläne für eine bestimmte Ebene (global, Pool oder Image) mit <command>rbd</command> das Kommando <command>mirror snapshot schedule ls</command> zusammen mit einem optionalen Pool- oder Image-Namen an. Zusätzlich kann die Option <option>--recursive </option>angegeben werden, um alle Zeitpläne auf der angegebenen Ebene und darunter aufzulisten. Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Um herauszufinden, wann die nächsten Snapshots für die Snapshot-basierte Spiegelung von RBD-Images mit <command>rbd</command> erstellt werden, geben Sie das Kommando <command>mirror snapshot schedule status</command> zusammen mit einem optionalen Pool- oder Image-Namen an. Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Aktivieren der Image-Spiegelung</title>
    <para>
     Geben Sie zum Deaktivieren der Spiegelung für ein bestimmtes Image das Unterkommando <command>mirror image disable</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Hoch- und Herabstufen von Images</title>
    <para>
     In einem Failover-Szenario, in dem die primäre Bezeichnung zum Image im Peer Cluster verschoben werden muss, müssen Sie den Zugriff auf das primäre Image stoppen, das aktuelle primäre Image herabstufen, das neue primäre Image hochstufen und den Zugriff auf das Image am alternativen Cluster wieder aufnehmen.
    </para>
    <note>
     <title>Erzwungene Hochstufung</title>
     <para>
      Die Hochstufung wird mit der Option <option>--force</option> erzwungen. Die erzwungene Hochstufung ist erforderlich, wenn die Herabstufung nicht auf den Peer Cluster übertragen werden kann (beispielsweise im Fall eines Cluster-Fehlers oder Kommunikationsausfalls). Dies führt zu einem Split-Brain-Szenario zwischen den beiden Peers und das Image wird nicht mehr synchronisiert, bis ein Unterkommando <command>resync</command> ausgestellt wird.
     </para>
    </note>
    <para>
     Geben Sie zum Herabstufen eines bestimmten Image zu „nicht primär“ das Unterkommando <command>mirror image demote</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Geben Sie zum Herabstufen aller primären Images in einem Pool zu „nicht primär“ das Unterkommando <command>mirror pool demote</command> zusammen mit dem Pool-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Geben Sie zum Hochstufen eines bestimmten Image zu „primär“ das Unterkommando <command>mirror image promote</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Geben Sie zum Hochstufen aller primären Images in einem Pool zu „primär“ das Unterkommando <command>mirror pool promote</command> zusammen mit dem Pool-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Aufteilen der E/A-Last</title>
     <para>
      Da der Status „primär“ oder „nicht primär“ pro Image gilt, ist es möglich, die E/A-Last auf zwei Cluster aufzuteilen und dort ein Failover oder Failback durchzuführen.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Erzwingen der erneuten Image-Synchronisierung</title>
    <para>
     Wenn der <systemitem class="daemon">rbd-mirror</systemitem>-Daemon ein Split Brain-Szenario erkennt, versucht er erst wieder, das betreffende Image zu spiegeln, wenn das Problem behoben ist. Um die Spiegelung für ein Image wieder aufzunehmen, müssen Sie zunächst das Image, das als veraltet ermittelt wurde, herabstufen und dann eine erneute Synchronisierung mit dem primären Image anfordern. Geben Sie zum Anfordern einer erneuten Synchronisierung des Image das Unterkommando <command>mirror image resync</command> zusammen mit dem Pool- und Image-Namen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Prüfen des Spiegelungsstatus</title>
   <para>
    Der Reproduktionsstatus des Peer Clusters wird für jedes primäre gespiegelte Image gespeichert. Dieser Status wird mit den Unterkommandos <command>mirror image status</command> und <command>mirror pool status</command> abgerufen:
   </para>
   <para>
    Geben Sie zum Anfordern des Spiegel-Image-Status das Unterkommando <command>mirror image status</command> zusammen mit dem Pool- und Image-Namen an:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Geben Sie zum Anfordern einer Übersicht zum Spiegel-Pool-Status das Unterkommando <command>mirror pool status</command> zusammen mit dem Pool-Namen an:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Durch Hinzufügen der Option <option>--verbose</option> zum Unterkommando <command>mirror pool status</command> werden zusätzlich Statusdetails für jedes Spiegel-Image im Pool ausgegeben.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Cache-Einstellungen</title>

  <para>
   Die Implementierung des Benutzerbereichs auf dem Ceph-Blockgerät (<systemitem>librbd</systemitem>) kann den Linux-Seiten-Cache nicht nutzen. Sie umfasst daher ein eigenes Caching im Speicher. Das RBD-Caching läuft ähnlich ab wie das Festplatten-Caching. Wenn das Betriebssystem eine Sperre oder eine Verschiebungsanforderung sendet, werden alle kürzlich bearbeiteten („dirty“) Daten auf die OSDs geschrieben. Dies bedeutet, dass das Caching mit Zurückschreiben ebenso sicher ist wie eine wohldefinierte physische Festplatte mit einer VM, die ordnungsgemäß Verschiebungen sendet. Der Cache beruht auf einem <emphasis>Least-Recently-Used</emphasis>(LRU)-Algorithmus; im Zurückschreiben-Modus kann er nebeneinanderliegende Anforderungen zusammenführen und damit den Durchsatz erhöhen.
  </para>

  <para>
   Ceph unterstützt das Caching mit Zurückschreiben für RBD. Aktivieren Sie es mit
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Standardmäßig nimmt <systemitem>librbd</systemitem> kein Caching vor. Schreib- und Lesevorgänge gehen direkt in den Speicher-Cluster und Schreibvorgänge werden nur dann zurückgegeben, wenn sich die Daten in allen Reproduktionen auf dem Datenträger befinden. Bei aktiviertem Caching werden Schreibvorgänge sofort zurückgegeben, außer die Menge der nicht verschobenen Daten (in Byte) ist höher als der Wert der Option <option>rbd cache max dirty</option>. Der Schreibvorgang löst in diesem Fall das Zurückschreiben aus und bleibt so lange gesperrt, bis eine ausreichende Datenmenge verschoben wurde.
  </para>

  <para>
   Ceph unterstützt das Caching mit Durchschreiben für RBD. Sie können die Größe des Caches festlegen und auch die Ziele und Einschränkungen vom Caching mit Zurückschreiben auf das Caching mit Durchschreiben umstellen. Aktivieren Sie den Durchschreiben-Modus mit
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Dies bedeutet, dass Schreibvorgänge nur dann zurückgegeben werden, wenn sich die Daten in allen Reproduktionen auf dem Datenträger befinden; Lesevorgänge können dagegen aus dem Cache stammen. Der Cache befindet sich im Speicher auf dem Client und jedes RBD-Image umfasst einen eigenen Cache. Da sich der Cache lokal auf dem Client befindet, ist keine Kohärenz gegeben, wenn andere auf das Image zugreifen. Bei aktiviertem Caching können GFS oder OCFS nicht zusätzlich zu RBD ausgeführt werden.
  </para>

  <para>
   Die folgenden Parameter beeinflussen das Verhalten von RADOS-Blockgeräten. Legen Sie sie mit der <literal>Client</literal>-Kategorie fest:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Caching für RADOS Block Device (RBD) aktivieren. Die Standardeinstellung ist „true“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      Größe des RBD-Caches (in Byte). Standardwert ist 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      Grenzwert für die Menge der kürzlich bearbeiteten („dirty“) Daten, bei dem der Cache das Zurückschreiben auslöst. <option>rbd cache max dirty</option> muss kleiner als <option>rbd cache size</option> sein. Beim Wert 0 wird das Caching mit Durchschreiben herangezogen. Standardwert ist 24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      Das kürzlich bearbeitete Ziel („dirty target“), bevor der Cache die ersten Daten in den Datenspeicher schreibt. Blockiert keine Schreibvorgänge in den Cache. Standardwert ist 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Zeitraum (in Sekunden), über den soeben bearbeitete Daten im Cache verbleiben, bevor das Zurückschreiben beginnt. Der Standardwert ist 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Der Vorgang beginnt im Durchschreiben-Modus und wechselt zum Zurückschreiben, sobald die erste Verschiebungsanforderung eingeht. Dies ist eine konservative, jedoch sichere Einstellung für den Fall, dass virtuelle Maschinen mit <systemitem>rbd</systemitem> zu alt sind, um Verschiebungen zu senden (z. B. der Virtio-Treiber in Linux vor Kernel 2.6.32). Die Standardeinstellung ist „true“.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS-Einstellungen</title>

  <para>
   „Quality of Service“ (QoS) bezeichnet im Allgemeinen verschiedene Methoden, mit denen der Datenverkehr priorisiert und Ressourcen reserviert werden. Dies ist insbesondere für Datenverkehr mit besonderen Anforderungen von Bedeutung.
  </para>

  <important>
   <title>Keine Unterstützung durch iSCSI</title>
   <para>
    Die folgenden QoS-Einstellungen werden ausschließlich durch die Benutzerbereichs-RBD-Implementierung <systemitem class="daemon">librbd</systemitem> verwendet, <emphasis>nicht</emphasis> jedoch von der <systemitem>kRBD</systemitem>-Implementierung. iSCSI greift auf <systemitem>kRBD</systemitem> zurück und nutzt daher nicht die QoS-Einstellungen. Für iSCSI können Sie jedoch QoS mit den standardmäßigen Kernel-Funktionen in der Kernel-Blockgeräteschicht konfigurieren.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die E/A-Operationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die E/A-Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die Leseoperationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die Schreiboperationen pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die gelesene Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      Gewünschter Grenzwert für die geschriebene Datenmenge (in Byte) pro Sekunde. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für E/A-Operationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die E/A-Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für Leseoperationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für Schreiboperationen. Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die gelesene Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      Gewünschter Blockgrenzwert für die geschriebene Datenmenge (in Byte). Der Standardwert ist 0 (keine Einschränkung).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      Minimaler Zeitplanimpuls (in Millisekunden) für QoS. Der Standardwert ist 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Read-Ahead-Einstellungen</title>

  <para>
   RADOS Block Device unterstützt das Read-Ahead/Prefetching zur Optimierung kleiner, sequenzieller Lesevorgänge. Bei einer virtuellen Maschine wird dies in der Regel durch das Gast-Betriebssystem abgewickelt, doch Bootloader geben unter Umständen keine effizienten Lesevorgänge aus. Das Read-Ahead wird automatisch deaktiviert, wenn das Caching aktiviert ist.
  </para>

  <important>
   <title>Keine Unterstützung durch iSCSI</title>
   <para>
    Die folgenden Read-Ahead-Einstellungen werden ausschließlich durch die Benutzerbereichs-RBD-Implementierung <systemitem class="daemon">librbd</systemitem> verwendet, <emphasis>nicht</emphasis> jedoch von der <systemitem>kRBD</systemitem>-Implementierung. iSCSI greift auf <systemitem>kRBD</systemitem> zurück und nutzt daher nicht die Read-Ahead-Einstellungen. Für iSCSI können Sie jedoch Read-Ahead mit den standardmäßigen Kernel-Funktionen in der Kernel-Blockgeräteschicht konfigurieren.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Anzahl der sequenziellen Leseanforderungen, die zum Auslösen des Read-Ahead erforderlich sind. Der Standardwert ist 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Maximale Größe einer Read-Ahead-Anforderung. Beim Wert 0 ist das Read-Ahead deaktiviert. Der Standardwert ist 512 KB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Sobald die angegebene Datenmenge (in Byte) aus einem RBD-Image gelesen wurde, wird das Read-Ahead für dieses Image deaktiviert, bis es geschlossen wird. Damit kann das Gast-Betriebssystem das Read-Ahead beim Starten übernehmen. Beim Wert 0 bleibt das Read-Ahead aktiviert. Der Standardwert ist 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Erweiterte Funktionen</title>

  <para>
   RADOS Block Device unterstützt erweiterte Funktionen, die den Funktionsumfang von RBD-Images vergrößern. Sie können die Funktionen entweder in der Kommandozeile beim Erstellen eines RBD-Images angeben oder in der Ceph-Konfigurationsdatei mit der Option <option>rbd_default_features</option>.
  </para>

  <para>
   Die Werte für die Option <option>rbd_default_features</option> können auf zwei Arten angegeben werden:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     als Summe der internen Werte der Funktion. Jede Funktion besitzt einen eigenen internen Wert – „layering“ beispielsweise den Wert 1 und „fast-diff“ den Wert 16. Sollen diese beiden Funktionen standardmäßig aktiviert werden, geben Sie daher Folgendes an:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     als durch Komma getrennte Liste mit Funktionen. Das obige Beispiel sieht wie folgt aus:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Funktionen ohne Unterstützung durch iSCSI</title>
   <para>
    RBD-Images mit den folgenden Funktionen werden nicht durch iSCSI unterstützt: <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option>, <option>striping</option>
   </para>
  </note>

  <para>
   Liste der erweiterten RBD-Funktionen:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Mit dem Layering können Sie Elemente klonen.
     </para>
     <para>
      Der interne Wert ist gleich 1, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Das Striping verteilt Daten auf mehrere Objekte und bewirkt Parallelität für sequenzielle Lese-/Schreib-Workloads. Damit werden Engpässe durch einzelne Knoten bei großen oder stark ausgelasteten RADOS Block Devices verhindert.
     </para>
     <para>
      Der interne Wert ist gleich 2, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Wenn diese Option aktiviert ist, muss ein Client eine Sperre für ein Objekt erwirken, bevor ein Schreibvorgang ausgeführt werden kann. Aktivieren Sie die exklusive Sperre nur dann, wenn nur ein einzelner Client auf ein bestimmtes Image zugreift, nicht mehrere Clients gleichzeitig. Der interne Wert ist gleich 4. Die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Die Objektzuordnungsunterstützung ist abhängig von der Unterstützung der exklusiven Sperre. Blockgeräte werden per Thin Provisioning bereitgestellt; dies bedeutet, dass lediglich Daten auf diesen Geräten gespeichert werden, die tatsächlich vorhanden sind. Die Objektzuordnungsunterstützung hilft bei der Ermittlung, welche Objekte tatsächlich vorhanden sind (für welche Objekte also Daten auf einem Server gespeichert sind). Wird die Objektzuordnungsunterstützung aktiviert, beschleunigt dies die E/A-Operationen beim Klonen, Importieren und Exportieren eines kaum gefüllten Images sowie den Löschvorgang.
     </para>
     <para>
      Der interne Wert ist gleich 8, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Die Fast-diff-Unterstützung ist abhängig von der Objektzuordnungsunterstützung und der Unterstützung der exklusiven Sperre. Hiermit wird eine zusätzliche Eigenschaft in die Objektzuordnung aufgenommen, die die Erzeugung von Vergleichen zwischen Snapshots eines Images und der tatsächlichen Datennutzung eines Snapshots erheblich beschleunigt.
     </para>
     <para>
      Der interne Wert ist gleich 16, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      „deep-flatten“ sorgt dafür, dass <command>rbd flatten</command> (siehe <xref linkend="rbd-flatten-cloned-image"/>) für alle Snapshots eines Images und zusätzlich für das Image selbst ausgeführt werden kann. Ohne diese Option beruhen Snapshots eines Images weiterhin auf dem übergeordneten Image, sodass Sie das übergeordnete Image erst dann löschen können, wenn die Snapshots gelöscht wurden. Mit „deep-flatten“ wird die Abhängigkeit eines übergeordneten Elements von seinen Klonen aufgehoben, selbst wenn sie Snapshots umfassen.
     </para>
     <para>
      Der interne Wert ist gleich 32, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>Journal</option></term>
    <listitem>
     <para>
      Die Journaling-Unterstützung ist abhängig von der Unterstützung der exklusiven Sperre. Mit dem Journaling werden alle Änderungen an einem Image in der Reihenfolge festgehalten, in der sie vorgenommen wurden. Die RBD-Spiegelung (siehe <xref linkend="ceph-rbd-mirror"/>) reproduziert anhand des Journals ein absturzkonsistentes Image auf einem Remote-Cluster.<literal/>
     </para>
     <para>
      Der interne Wert ist gleich 64, die Standardeinstellung lautet „yes“.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>RBD-Zuordnung mit älteren Kernel-Clients</title>

  <para>
   Ältere Clients (z. B. SLE11 SP4) können RBD-Images unter Umständen nicht zuordnen, da ein mit SUSE Enterprise Storage 7 bereitgestellter Cluster bestimmte Funktionen erzwingt (sowohl auf Ebene der RBD-Images als auch auf RADOS-Ebene), die diese älteren Clients nicht unterstützen. In diesem Fall enthalten die OSD-Protokolle die folgenden oder ähnliche Meldungen:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Eine Änderung der Bucket-Typen für die CRUSH-Zuordnung löst einen massiven Ausgleich aus</title>
   <para>
    Sollen die CRUSH-Zuordnung-Bucket-Typen zwischen „straw“ und „straw2“ gewechselt werden, gehen Sie dabei wohlgeplant vor. Erwarten Sie erhebliche Auswirkungen auf die Cluster-Last, da eine Änderung des Bucket-Typs einen massiven Cluster-Ausgleich auslöst.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Deaktivieren Sie alle nicht unterstützten RBD-Image-Funktionen. Beispiel:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Stellen Sie die CRUSH-Zuordnung-Bucket-Typen von „straw2“ auf „straw“ um:
    </para>
    <substeps>
     <step>
      <para>
       Speichern Sie die CRUSH-Zuordnung:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Dekompilieren Sie die CRUSH-Zuordnung:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Bearbeiten Sie die CRUSH-Zuordnung und ersetzen Sie „straw2“ durch „straw“.
      </para>
     </step>
     <step>
      <para>
       Kompilieren Sie die CRUSH-Zuordnung neu:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Legen Sie die neue CRUSH-Zuordnung fest:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Aktiveren von Blockgeräten und Kubernetes</title>

  <para>
   Sie können Ceph RBD mit Kubernetes v1.13 und höher über den <literal>ceph-csi</literal>-Treiber verwenden. Dieser Treiber stellt RBD-Images dynamisch für Kubernetes-Volumes bereit und ordnet diese RBD-Images als Blockgeräte (und optionales Einhängen eines im Image enthaltenen Dateisystems) zu Arbeitsknoten zu, auf denen Pods ausgeführt werden, die auf ein RBD-gestütztes Volume verweisen.
  </para>

  <para>
   Um Ceph-Blockgeräte mit Kubernetes zu verwenden, müssen Sie <literal>ceph-csi</literal> in Ihrer Kubernetes-Umgebung installieren und konfigurieren.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> verwendet standardmäßig die RBD-Kernelmodule, die möglicherweise nicht alle Ceph CRUSH-Tunables oder RBD-Image-Funktionen unterstützen.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Ceph-Blockgeräte verwenden standardmäßig den RBD-Pool. Erstellen Sie einen Pool für Kubernetes-Volume-Speicher. Stellen Sie sicher, dass Ihr Ceph-Cluster ausgeführt wird, und erstellen Sie dann den Pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Initialisieren Sie den Pool mit dem RBD-Werkzeug:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Erstellen Sie einen neuen Benutzer für Kubernetes und <literal>ceph-csi</literal>. Führen Sie folgendes Kommando aus und zeichnen Sie den generierten Schlüssel auf:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     Für <literal>ceph-csi</literal> ist ein in Kubernetes gespeichertes ConfigMap-Objekt erforderlich, um die Ceph-Monitoradressen für den Ceph-Cluster zu definieren. Erfassen Sie sowohl die eindeutige FSID des Ceph-Clusters als auch die Monitoradressen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Generieren Sie eine <filename>csi-config-map.yaml</filename>-Datei ähnlich dem folgenden Beispiel und ersetzen Sie die FSID durch <literal>clusterID</literal> und die Monitoradressen durch <literal>monitors</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Speichern Sie nach der Generierung das neue ConfigMap-Objekt in Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     Für <literal>ceph-csi</literal> ist der cephx-Berechtigungsnachweis für die Kommunikation mit dem Ceph-Cluster erforderlich. Generieren Sie eine <filename>csi-rbd-secret.yaml</filename>-Datei ähnlich dem folgenden Beispiel mit der neu erstellten Kubernetes-Benutzer-ID und dem cephx-Schlüssel:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Speichern Sie das generierte neue geheime Objekt in Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Erstellen Sie die erforderlichen Kubernetes-Objekte ServiceAccount und RBAC ClusterRole/ClusterRoleBinding. Diese Objekte müssen nicht unbedingt für Ihre Kubernetes-Umgebung angepasst werden und können daher direkt aus den <literal>ceph-csi</literal>-YAML-Bereitstellungsdateien verwendet werden:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Erstellen Sie das <literal>ceph-csi</literal>-Bereitstellungsprogramm und die Knoten-Plugins:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Standardmäßig wird mit den YAML-Dateien des Bereitstellungsprogramms und des Knoten-Plugins die Entwicklungsversion des <literal>ceph-csi</literal>-Containers abgerufen. Die YAML-Dateien sollten zur Verwendung einer Freigabeversion aktualisiert werden.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Verwenden von Ceph-Blockgeräten in Kubernetes</title>
   <para>
    Die Kubernetes StorageClass definiert eine Speicherklasse. Es können mehrere StorageClass-Objekte erstellt werden, um verschiedene Quality-of-Service-Stufen und Funktionen zuzuordnen. Zum Beispiel NVMe- versus HDD-basierte Pools.
   </para>
   <para>
    Um eine <literal>ceph-csi</literal>-StorageClass zu erstellen, die auf den oben erstellten Kubernetes-Pool zugeordnet wird, kann die folgende YAML-Datei verwendet werden, nachdem sichergestellt wurde, dass die Eigenschaft <literal>clusterID</literal> mit der FSID Ihres Ceph-Clusters übereinstimmt:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    Ein <literal>PersistentVolumeClaim</literal> ist eine Anforderung von abstrakten Speicherressourcen durch einen Benutzer. Der <literal>PersistentVolumeClaim</literal> würde dann einer Pod-Ressource zugeordnet werden, um ein <literal>PersistentVolume</literal> bereitzustellen, das durch ein Ceph-Block-Image gesichert wird. Ein optionaler <option>volumeMode</option> kann eingeschlossen werden, um zwischen einem eingehängten Dateisystem (Standard) oder einem Volume auf Basis eines Basisblockgeräts zu wählen.
   </para>
   <para>
    Bei <literal>ceph-csi</literal> kann die Angabe von <option>Filesystem</option> für <option>volumeMode</option> die Forderungen <literal>ReadWriteOnce</literal> und <literal>ReadOnlyMany accessMode</literal> unterstützen. Die Angabe von <option>Block</option> für <option>volumeMode</option> kann die Forderungen <literal>ReadWriteOnce</literal>, <literal>ReadWriteMany</literal> und <literal>ReadOnlyMany accessMode</literal> unterstützen.
   </para>
   <para>
    Um beispielsweise einen blockbasierten <literal>PersistentVolumeClaim</literal> zu erstellen, der die oben erstellte <literal>ceph-csi-basierte StorageClass</literal> verwendet, kann die folgende YAML-Datei verwendet werden, um Basisblockspeicher von der <literal>csi-rbd-sc StorageClass</literal> anzufordern:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    Im Folgenden wird ein Beispiel für die Bindung des obigen <literal>PersistentVolumeClaim</literal> an eine Pod-Ressource als Basisblockgerät gezeigt:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Um einen dateisystembasierten <literal>PersistentVolumeClaim</literal> zu erstellen, der die oben erstellte <literal>ceph-csi-basierte StorageClass</literal> verwendet, kann die folgende YAML-Datei verwendet werden, um ein eingehängtes (von einem RBD-Image gesichertes) Dateisystem von der <literal>csi-rbd-sc StorageClass</literal> anzufordern:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    Im Folgenden wird ein Beispiel für die Bindung des obigen <literal>PersistentVolumeClaim</literal> an eine Pod-Ressource als eingehängtes Dateisystem gezeigt:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
