<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_core.xml" version="5.0" xml:id="deploy-core">
 <info>
  <title>Bereitstellen der verbleibenden wichtigsten Services mit cephadm</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Nachdem Sie den Ceph-Basiscluster bereitgestellt haben, sollten Sie die wichtigsten Services auf weitere Cluster-Knoten verteilen. Stellen Sie zusätzliche Services bereit, um die Daten des Clusters für Clients zugänglich zu machen.
 </para>
 <para>
  Derzeit unterstützen wir die Bereitstellung von Ceph-Services auf der Kommandozeile mit dem Ceph-Orchestrator (<command>ceph orch</command>-Unterkommandos).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>Das Kommando <command>ceph orch</command></title>

  <para>
   Mit dem Ceph-Orchestrator-Kommando <command>ceph orch</command> (einer Schnittstelle zum cephadm-Modul) werden die Clusterkomponenten aufgelistet und Ceph-Services auf neuen Cluster-Knoten bereitgestellt.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Anzeigen des Orchestrator-Status</title>
   <para>
    Mit folgendem Kommando werden der aktuelle Modus und Status des Ceph-Orchestrators angezeigt.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Auflisten von Geräten, Services und Daemons</title>
   <para>
    Führen Sie zum Auflisten aller Datenträgergeräte folgendes Kommando aus:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>Services und Daemons</title>
    <para>
     <emphasis>Service</emphasis> ist ein allgemeiner Begriff für einen Ceph-Service eines bestimmten Typs, zum Beispiel Ceph Manager.
    </para>
    <para>
     <emphasis>Daemon</emphasis> ist eine bestimmte Instanz eines Service, z. B. ein Prozess <literal>mgr.ses-min1.gdlcik</literal>, der auf einem Knoten namens <literal>ses-min1</literal> ausgeführt wird.
    </para>
   </tip>
   <para>
    Führen Sie zum Auflisten aller Services, die cephadm kennt, folgendes Kommando aus:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     Sie können die Liste mit dem optionalen Parameter <option>--host</option> auf Services auf einem bestimmten Knoten und mit dem optionalen Parameter <option>--service-type</option> auf Services eines bestimmten Typs beschränken. Akzeptiert werden die Typen <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>, <literal>mds</literal> und <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    Führen Sie folgendes Kommando aus, um alle von cephadm bereitgestellten aktiven Daemons aufzulisten:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     Fragen Sie den Status eines bestimmten Daemons mit <option>--daemon_type</option> und <option>--daemon_id</option> ab. Bei OSDs ist die ID die numerische OSD-ID. Bei MDS ist die ID der Name des Dateisystems:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd --daemon_id 0
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Service- und Platzierungsspezifikation</title>

  <para>
   Die Spezifikation der Bereitstellung von Ceph-Services erfolgt am besten durch Erstellen einer YAML-formatierten Datei mit der Spezifikation der Services, die Sie bereitstellen möchten.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Erstellen von Servicespezifikationen</title>
   <para>
    Sie können für jede Art von Service eine eigene Spezifikationsdatei erstellen, wie zum Beispiel:
   </para>
<screen>
<prompt>root@master # </prompt>cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Alternativ können Sie mehrere (oder alle) Servicetypen in einer Datei angeben, z. B. <filename>cluster.yml</filename>, die beschreibt, auf welchen Knoten bestimmte Services ausgeführt werden sollen. Denken Sie daran, einzelne Servicetypen mit drei Bindestrichen (<literal>---</literal>) zu trennen:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    Die oben genannten Eigenschaften haben folgende Bedeutung:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       Der Typ des Service. Er kann entweder ein Ceph-Service (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>, <literal>crash</literal>, <literal>osd</literal> oder <literal>rbd-mirror</literal>), ein Gateway (<literal>nfs</literal> oder <literal>rgw</literal>) oder Teil des Überwachungs-Stacks (<literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> oder <literal>prometheus</literal>) sein.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       Der Name des Service. Für Spezifikationen vom Typ <literal>mon</literal>, <literal>mgr</literal>, <literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> und <literal>prometheus</literal> ist die Eigenschaft <literal>service_id</literal> nicht erforderlich.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Gibt an, auf welchen Knoten der Service ausgeführt wird. Weitere Informationen finden Sie in <xref linkend="cephadm-placement-specs"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Zusätzliche Spezifikation, die für den Servicetyp relevant ist.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Anwenden spezifischer Services</title>
    <para>
     Ceph-Cluster-Services haben in der Regel eine Reihe von für sie spezifischen Eigenschaften. Beispiele und Details zu den Spezifikationen der einzelnen Services finden Sie in <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Erstellen von Platzierungsspezifikationen</title>
   <para>
    Zum Bereitstellen von Ceph-Services muss cephadm wissen, auf welchen Knoten sie bereitgestellt werden sollen. Verwenden Sie die Eigenschaft <literal>placement</literal> und führen Sie die Host-Kurznamen der Knoten auf, für die der Service gilt:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Anwenden von Clusterspezifikationen</title>
   <para>
    Nachdem Sie eine vollständige <filename>cluster.yml</filename>-Datei mit Spezifikationen zu allen Services und deren Platzierung erstellt haben, können Sie den Cluster mit folgendem Kommando anwenden:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yml</screen>
   <para>
    Führen Sie zum Anzeigen des Clusterstatus das Kommando <command>ceph orch</command> aus. Weitere Informationen finden Sie unter <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exportieren der Spezifikation eines aktiven Clusters</title>
   <para>
    Obwohl Sie dem Ceph-Cluster Services mithilfe der Spezifikationsdateien bereitgestellt haben (wie in <xref linkend="cephadm-service-and-placement-specs"/> beschrieben), kann die Konfiguration des Clusters während seines Betriebs von der ursprünglichen Spezifikation abweichen. Möglicherweise haben Sie auch die Spezifikationsdateien versehentlich entfernt.
   </para>
   <para>
    Führen Sie zum Abrufen der vollständigen Spezifikation eines aktiven Clusters folgendes Kommando aus:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     Sie können die Option <option>--format</option> anhängen, um das Standard-Ausgabeformat <literal>yaml</literal> zu ändern. Sie können zwischen <literal>json</literal>, <literal>json-pretty</literal> oder <literal>yaml</literal> wählen. Beispiel:
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Bereitstellen von Ceph-Services</title>

  <para>
   Sobald der Basiscluster ausgeführt wird, können Sie Ceph-Services auf weiteren Knoten bereitstellen.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Bereitstellen von Ceph Monitors und Ceph Managers</title>
   <para>
    Für den Ceph-Cluster werden drei oder fünf MONs bereitgestellt, die auf verschiedenen Knoten verteilt sind. Wenn sich fünf oder mehr Knoten im Cluster befinden, empfehlen wir die Bereitstellung von fünf MONs. Es hat sich bewährt, MGRs auf denselben Knoten wie MONs zu installieren.
   </para>
   <important>
    <title>Beziehen Sie den Bootstrap-MON ein</title>
    <para>
     Wenn Sie MONs und MGRs bereitstellen, denken Sie daran, den ersten MON einzubeziehen, den Sie bei der Konfiguration des Basisclusters in <xref linkend="deploy-cephadm-configure-mon"/> hinzugefügt haben.
    </para>
   </important>
   <para>
    Wenden Sie zum Bereitstellen von MONs folgende Spezifikation an:
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     Wenn Sie einen weiteren Knoten hinzufügen müssen, hängen Sie den Hostnamen an dieselbe YAML-Liste an. Beispiel:
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    Wenden Sie auf ähnliche Weise zum Bereitstellen von MONs folgende Spezifikationen an:
   </para>
   <important>
    <para>
     Stellen Sie sicher, dass mindestens drei Ceph Manager in jeder Bereitstellung vorhanden sind.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     Wenn sich MONs oder MGRs <emphasis>nicht</emphasis> im gleichen Teilnetz befinden, müssen Sie die Teilnetzadressen anhängen. Beispiel:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Bereitstellen von Ceph OSDs</title>
   <important>
    <title>Wenn ein Speichergerät verfügbar ist</title>
    <para>
     Ein Speichergerät gilt als <emphasis>verfügbar</emphasis>, wenn alle folgenden Bedingungen erfüllt sind:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Das Gerät hat keine Partitionen.
      </para>
     </listitem>
     <listitem>
      <para>
       Das Gerät hat keinen LVM-Status.
      </para>
     </listitem>
     <listitem>
      <para>
       Das Gerät ist nicht eingehängt.
      </para>
     </listitem>
     <listitem>
      <para>
       Das Gerät enthält kein Dateisystem.
      </para>
     </listitem>
     <listitem>
      <para>
       Das Gerät enthält kein BlueStore-OSD.
      </para>
     </listitem>
     <listitem>
      <para>
       Das Gerät ist größer als 5 GB.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Wenn die oben genannten Bedingungen nicht erfüllt sind, verweigert Ceph die Bereitstellung derartiger OSDs.
    </para>
   </important>
   <para>
    Zum Bereitstellen von OSDs haben Sie zwei Möglichkeiten:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Weisen Sie Ceph an, alle verfügbaren und nicht verwendeten Speichergeräte zu verbrauchen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Verwenden Sie DriveGroups (weitere Informationen hierzu finden Sie im <xref linkend="drive-groups"/>), um OSD-Spezifikationen zu erstellen, die Geräte beschreiben, die basierend auf ihren Eigenschaften bereitgestellt werden. Beispiele hierfür sind der Gerätetyp (SSD oder HDD), die Gerätemodellnamen, die Größe oder die Knoten, auf denen die Geräte vorhanden sind. Wenden Sie dann die Spezifikation mit folgendem Kommando an:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Bereitstellen von Metadatenservern</title>
   <para>
    Für CephFS sind ein oder mehrere Metadata Server (MDS)-Services erforderlich. Wenn Sie ein CephFS erstellen möchten, erstellen Sie zunächst MDS-Server durch Anwenden der folgenden Spezifikation:
   </para>
   <note>
    <para>
     Stellen Sie sicher, dass Sie mindestens zwei Pools, einen für CephFS-Daten und einen für CephFS-Metadaten, erstellt haben, bevor Sie die folgende Spezifikation anwenden.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    Erstellen Sie das CephFS, sobald die MDS funktionsfähig sind:
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Bereitstellen von Object Gateways</title>
   <para>
    cephadm stellt ein Object Gateway als eine Sammlung von Daemons bereit, die einen bestimmten <emphasis>Bereich</emphasis> und eine <emphasis>Zone</emphasis> verwalten.
   </para>
   <para>
    Sie können entweder einen Object-Gateway-Service mit einem bereits vorhandenen Bereich und einer bereits vorhandenen Zone verknüpfen (weitere Informationen finden Sie im <xref linkend="ceph-rgw-fed"/>), oder Sie können einen nicht vorhandenen <replaceable>REALM_NAME</replaceable> (Bereichsnamen) und <replaceable>ZONE_NAME</replaceable> (Zonennamen) angeben, die dann automatisch erstellt werden, nachdem Sie die folgende Konfiguration angewendet haben:
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Verwenden des sicheren SSL-Zugangs</title>
    <para>
     Um eine sichere SSL-Verbindung zum Object Gateway zu verwenden, benötigen Sie ein Paar gültiger SSL-Zertifikats- und Schlüsseldateien (weitere Details finden Sie im <xref linkend="ceph-rgw-https"/>). Sie müssen SSL aktivieren, eine Portnummer für SSL-Verbindungen sowie die SSL-Zertifikats- und Schlüsseldateien angeben.
    </para>
    <para>
     Nehmen Sie Folgendes in Ihre Spezifikation auf, um SSL zu aktivieren und die Portnummer anzugeben:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     Um das SSL-Zertifikat und den Schlüssel festzulegen, können Sie deren Inhalte direkt in die YAML-Spezifikationsdatei einfügen. Das Pipe-Zeichen (<literal>|</literal>) am Zeilenende weist den Parser darauf hin, dass er eine mehrzeilige Zeichenkette als Wert erwarten soll. Beispiel:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      Statt den Inhalt der SSL-Zertifikats- und Schlüsseldateien einzufügen, können Sie die Schlüsselwörter <literal>rgw_frontend_ssl_certificate:</literal> und <literal>rgw_frontend_ssl_key:</literal> weglassen und sie in die Konfigurationsdatenbank hochladen:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>Konfigurieren des Object Gateway zur Überwachung von Port 443 und 80</title>
     <para>
      Führen Sie die folgenden Schritte aus, um das Object Gateway zur Überwachung beider Ports, Port 443 (HTTPS) und Port 80 (HTTP), zu konfigurieren:
     </para>
     <note>
      <para>
       Die Kommandos in diesem Verfahren verwenden den Standardwert <literal>default</literal> für den Bereich und die Zone.
      </para>
     </note>
     <procedure>
      <step>
       <para>
        Stellen Sie das Object Gateway bereit, indem Sie eine Spezifikationsdatei angeben. Weitere Informationen zur Object-Gateway-Spezifikation finden Sie in <xref linkend="deploy-cephadm-day2-service-ogw"/>. Verwenden Sie den folgenden Befehl:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        Wenn SSL-Zertifikate nicht in der Spezifikationsdatei enthalten sind, fügen Sie sie mit dem folgenden Kommando hinzu:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        Ändern Sie den Standardwert der Option <option>rgw_frontends</option>:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      
      <step>
       <para>
        Entfernen Sie die von cephadm erstellte spezifische Konfiguration. Stellen Sie fest, für welches Ziel die Option <option>rgw_frontends</option> konfiguriert wurde, indem Sie dieses Kommando ausführen:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config dump | grep rgw</screen>
       <para>
        Wenn das Ziel beispielsweise <literal>client.rgw.default.default.node4.yiewdu</literal> lautet, entfernen Sie den aktuellen spezifischen Wert für <option>rgw_frontends</option>:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         Statt einen Wert für <option>rgw_frontends</option> zu entfernen, können Sie ihn auch angeben. Beispiel:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      
      <step>
       <para>
        Starten Sie Object Gateways neu:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Bereitstellung mit einem Untercluster</title>
    <para>
     Mit <emphasis>Unterclustern</emphasis> können Sie die Knoten in Ihren Clustern organisieren, um Workloads zu isolieren und die elastische Skalierung zu erleichtern. Wenden Sie die folgende Konfiguration an, wenn Sie die Bereitstellung mit einem Untercluster durchführen:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Bereitstellen von iSCSI-Gateways</title>
   <para>
    iSCSI ist ein Storage Area Network (SAN)-Protokoll, das Clients (genannt Initiators) das Senden von SCSI-Kommandos an SCSI-Speichergeräte (Ziele) auf Remote-Servern ermöglicht.
   </para>
   <para>
    Wenden Sie die folgende Konfiguration für die Bereitstellung an. Stellen Sie sicher, dass <literal>trusted_ip_list</literal> die IP-Adressen aller iSCSI-Gateway- und Ceph-Manager-Knoten enthält (wie im folgenden Ausgabebeispiel gezeigt).
   </para>
   <note>
    <para>
     Stellen Sie sicher, dass der Pool erstellt wurde, bevor Sie die folgende Spezifikation anwenden.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Stellen Sie sicher, dass die für <literal>trusted_ip_list</literal> aufgelisteten IPs <emphasis>kein</emphasis> Leerzeichen nach der Kommatrennung aufweisen.
    </para>
   </note>
   <sect3>
    <title>Sichere SSL-Konfiguration</title>
    <para>
     Zur Verwendung einer sicheren SSL-Verbindung zwischen dem Ceph Dashboard und der iSCSI-Ziel-API benötigen Sie ein Paar gültiger SSL-Zertifikate und Schlüsseldateien. Sie können entweder von einer Zertifizierungsstelle ausgestellt oder eigensigniert sein (weitere Informationen finden Sie im <xref linkend="self-sign-certificates"/>). Nehmen Sie zum Aktivieren von SSL die Einstellung <literal>api_secure: true</literal> in Ihre Spezifikationsdatei auf:
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     Um das SSL-Zertifikat und den Schlüssel festzulegen, können Sie den Inhalt direkt in die YAML-Spezifikationsdatei einfügen. Das Pipe-Zeichen (<literal>|</literal>) am Zeilenende weist den Parser darauf hin, dass er eine mehrzeilige Zeichenkette als Wert erwarten soll. Beispiel:
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Bereitstellen von NFS Ganesha</title>
    
<important>
 <para>
  NFS Ganesha unterstützt NFS-Version 4.1 und neuer. NFS Ganesha unterstützt nicht NFS-Version 3.
 </para>
</important>

    <para>
    cephadm stellt NFS Ganesha unter Verwendung eines vordefinierten RADOS-Pools und eines optionalen Namespace bereit. Wenden Sie zum Bereitstellen von NFS Ganesha folgende Spezifikation an:
   </para>
   <note>
    <para>
     Sie müssen über einen vordefinierten RADOS-Pool verfügen, da sonst der <command>ceph orch apply</command>-Vorgang fehlschlägt. Weitere Informationen zum Erstellen eines Pools finden Sie im <xref linkend="ceph-pools-operate-add-pool"/>.
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NFS</replaceable> mit einer beliebigen Zeichenkette, die den NFS-Export identifiziert.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_POOL</replaceable> mit dem Namen des Pools, in dem das NFS-Ganesha-RADOS-Konfigurationsobjekt gespeichert werden soll.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NAMESPACE</replaceable> (optional) mit dem gewünschten Object-Gateway-NFS-Namespace (zum Beispiel <literal>ganesha</literal>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Bereitstellung <systemitem class="daemon">rbd-mirror</systemitem></title>
   <para>
    Der Service <systemitem class="daemon">rbd-mirror</systemitem> synchronisiert RADOS-Blockgeräte-Images zwischen zwei Ceph-Clustern (weitere Details finden Sie im <xref linkend="ceph-rbd-mirror"/>). Stellen Sie <systemitem class="daemon">rbd-mirror</systemitem> mit folgender Spezifikation bereit:
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Bereitstellen des Überwachungs-Stacks</title>
   <para>
    Der Überwachungs-Stack besteht aus Prometheus, Prometheus-Exportern, Prometheus Alertmanager und Grafana. Ceph Dashboard nutzt diese Komponenten, um detaillierte Kennzahlen zur Clusternutzung und -leistung zu speichern und zu visualisieren.
   </para>
   <tip>
    <para>
     Wenn für Ihre Bereitstellung benutzerdefinierte oder lokal bereitgestellte Container-Images der Überwachungs-Stack-Services erforderlich sind, finden Sie Informationen hierzu im <xref linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    Führen Sie zum Bereitstellen des Überwachungs-Stacks folgende Schritte aus:
   </para>
   <procedure>
    <step>
     <para>
      Aktivieren Sie das Modul <literal>prometheus</literal> im Ceph-Manager-Daemon. Dadurch werden die internen Ceph-Kennzahlen offengelegt, so dass Prometheus sie lesen kann:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Stellen Sie sicher, dass dieses Kommando vor dem Bereitstellen von Prometheus ausgeführt wird. Wenn das Kommando vor der Bereitstellung nicht ausgeführt wurde, müssen Sie Prometheus erneut bereitstellen, um die Konfiguration von Prometheus zu aktualisieren:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Erstellen Sie eine Spezifikationsdatei (z. B. <filename>monitoring.yaml</filename>) mit einem Inhalt ähnlich dem folgenden:
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Wenden Sie die Überwachungsservices mit folgendem Kommando an:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i monitoring.yaml</screen>
     <para>
      Es kann ein oder zwei Minuten dauern, bis die Überwachungsservices bereitgestellt sind.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     Prometheus, Grafana und das Ceph Dashboard sind automatisch so konfiguriert, dass sie miteinander kommunizieren. Dies führt zu einer voll funktionsfähigen Grafana-Integration im Ceph Dashboard, wenn es wie oben beschrieben bereitgestellt wird.
    </para>
    <para>
     Die einzige Ausnahme von dieser Regel ist die Überwachung mit RBD-Images. Weitere Informationen zu diesem Thema finden Sie unter dem Stichwort <xref linkend="monitoring-rbd-image"/>.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
