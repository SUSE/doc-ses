<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Betriebsaufgaben</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Ändern der Cluster-Konfiguration</title>

  <para>
   Gehen Sie zum Ändern der Konfiguration eines vorhandenen Ceph-Clusters folgendermaßen vor:
  </para>

  <procedure>
   <step>
    <para>
     Exportieren Sie die aktuelle Konfiguration des Clusters in eine Datei:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Bearbeiten Sie die Datei mit der Konfiguration und aktualisieren Sie die entsprechenden Zeilen. Beispiele der Spezifikation finden Sie im <xref linkend="deploy-core"/> sowie in <xref linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Wenden Sie die neue Konfiguration an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Hinzufügen von Knoten</title>

  <para>
   Führen Sie zum Hinzufügen eines neuen Knotens zu einem Ceph-Cluster folgende Schritte aus:
  </para>

  <procedure>
   <step>
    <para>
     Installieren Sie SUSE Linux Enterprise Server und SUSE Enterprise Storage auf dem neuen Host. Weitere Informationen finden Sie im <xref linkend="deploy-sles"/>.
    </para>
   </step>
   <step>
    <para>
     Konfigurieren Sie den Host als Salt Minion eines bereits vorhandenen Salt Masters. Weitere Informationen finden Sie im <xref linkend="deploy-salt"/>.
    </para>
   </step>
   <step>
    <para>
     Fügen Sie den neuen Host zu <systemitem class="resource">ceph-salt</systemitem> hinzu und machen Sie cephadm darauf aufmerksam:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Weitere Informationen finden Sie im <xref linkend="deploy-cephadm-configure-minions"/>.
    </para>
   </step>
   <step>
    <para>
     Überprüfen Sie, ob der Knoten zu <systemitem class="resource">ceph-salt</systemitem> hinzugefügt wurde:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Wenden Sie die Konfiguration auf den neuen Clusterhost an:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Stellen Sie sicher, dass der neu hinzugefügte Host nun zur cephadm-Umgebung gehört:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Entfernen von Knoten</title>

  <tip>
   <title>Entfernen Sie die OSDs</title>
   <para>
    Wenn der Knoten, den Sie entfernen möchten, OSDs ausführt, entfernen Sie zuerst die OSDs und überprüfen Sie, dass keine OSDs auf diesem Knoten ausgeführt werden. Weitere detaillierte Informationen zum Entfernen von OSDs finden Sie in <xref linkend="removing-node-osds"/>.
   </para>
  </tip>

  <para>
   Gehen Sie zum Entfernen eines Knotens von einem Cluster folgendermaßen vor:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Entfernen Sie bei allen Ceph-Servicetypen mit Ausnahme von <literal>node-exporter</literal> und <literal>crash</literal> den Hostnamen des Knotens aus der Spezifikationsdatei für die Clusterplatzierung (z. B. <filename>cluster.yml</filename>). Weitere Informationen finden Sie im <xref linkend="cephadm-service-and-placement-specs"/>. Wenn Sie beispielsweise den Host namens <literal>ses-min2</literal> entfernen, müssen Sie alle Vorkommen von <literal>- ses-min2</literal> aus allen <literal>placement:</literal>-Abschnitten entfernen:
    </para>
    <para>
     Aktualisieren Sie
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     zu
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Wenden Sie die Änderungen an der Konfigurationsdatei an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Entfernen Sie den Knoten aus der Umgebung von cephadm:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Wenn auf dem Knoten die Services <literal>crash.osd.1</literal> und <literal>crash.osd.2</literal> ausgeführt werden, entfernen Sie sie, indem Sie folgendes Kommando auf dem Host ausführen:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Beispiel:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Entfernen Sie alle Rollen von dem Minion, den Sie löschen möchten:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Wenn der zu entfernende Minion der Bootstrap-Minion ist, müssen Sie auch die Bootstrap-Rolle entfernen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     Nachdem Sie alle OSDs auf einem einzelnen Host entfernt haben, entfernen Sie den Host aus der CRUSH-Zuordnung:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      Der Bucket-Name sollte mit dem Hostnamen identisch sein.
     </para>
    </note>
   </step>
   <step>
    <para>
     Sie können den Minion nun aus dem Cluster entfernen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    Wenn ein Fehler auftritt und sich der zu entfernende Minion in einem dauerhaft ausgeschalteten Zustand befindet, müssen Sie den Knoten vom Salt Master entfernen:
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    Entfernen Sie dann den Knoten manuell aus <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>. Er befindet sich normalerweise in <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSD-Verwaltung</title>

  <para>
   In diesem Abschnitt wird beschrieben, wie Sie OSDs in einem Ceph-Cluster hinzufügen, löschen oder entfernen.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Auflisten von Datenträgergeräten</title>
   <para>
    Um verwendete und nicht verwendete Datenträgergeräte auf allen Cluster-Knoten zu identifizieren, listen Sie sie mit folgendem Kommando auf:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Löschen von Datenträgergeräten</title>
   <para>
    Um ein Datenträgergerät wieder zu verwenden, müssen Sie es zuerst löschen (oder <emphasis>zappen</emphasis>):
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Wenn Sie zuvor OSDs mithilfe von DriveGroups oder der Option <option>--all-available-devices</option> bereitgestellt haben und das Flag <literal>unmanaged</literal> dabei nicht gesetzt war, stellt cephadm diese OSDs automatisch bereit, nachdem Sie sie gelöscht haben.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Hinzufügen von OSDs mit der DriveGroups-Spezifikation</title>
   <para>
    <emphasis>DriveGroups</emphasis> bestimmen das Layout der OSDs im Ceph-Cluster. Sie werden in einer einzigen YAML-Datei definiert. In diesem Abschnitt wird die Datei <filename>drive_groups.yml</filename> als Beispiel herangezogen.
   </para>
   <para>
    Ein Administrator muss manuell eine Gruppe von OSDs erstellen, die zusammenhängen (Hybrid-OSDs, die auf einer Mischung von HDDs und SDDs implementiert sind) oder dieselben Bereitstellungsoptionen aufweisen (z. B. gleicher Objektspeicher, gleiche Verschlüsselungsoption, Stand-Alone-OSDs). Damit die Geräte nicht explizit aufgelistet werden müssen, werden sie in den DriveGroups anhand einer Liste von Filterelementen gefiltert, die einigen wenigen ausgewählten Feldern in den <command>ceph-volume</command>-Bestandsberichten entsprechen. Der Code in cephadm wandelt diese DriveGroups in Gerätelisten um, die dann vom Benutzer geprüft werden können.
   </para>
   <para>
    Das Kommando zum Anwenden der OSD-Spezifikation auf den Cluster lautet:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Für eine Vorschau der Aktionen und einen Test der Anwendung können Sie die Option <option>--dry-run</option> zusammen mit dem Kommando <command>ceph orch apply osd</command> verwenden. Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Wenn die <option>--dry-run</option>-Ausgabe Ihren Erwartungen entspricht, führen Sie das Kommando einfach erneut ohne die Option <option>--dry-run</option> aus.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>Nicht verwaltete OSDs</title>
    <para>
     Alle verfügbaren fehlerfreien Datenträgergeräte, die der DriveGroups-Spezifikation entsprechen, werden automatisch als OSDs verwendet, nachdem Sie sie dem Cluster hinzugefügt haben. Dieses Verhalten wird als <emphasis>verwalteter</emphasis> Modus bezeichnet.
    </para>
    <para>
     Fügen Sie zum Deaktivieren des <emphasis>verwalteten</emphasis> Modus beispielsweise die Zeile <literal>unmanaged: true</literal> zu den entsprechenden Spezifikationen hinzu:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Um bereits bereitgestellte OSDs vom <emphasis>verwalteten</emphasis> zum <emphasis>nicht verwalteten</emphasis> Modus zu ändern, fügen Sie während des in <xref linkend="modifying-cluster-configuration"/> beschriebenen Vorgangs die Zeilen <literal>unmanaged: true</literal> an den entsprechenden Stellen hinzu.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>DriveGroups-Spezifikation</title>
    <para>
     Es folgt ein Beispiel für eine DriveGroups-Spezifikationsdatei:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
    <note>
     <para>
      Die bisher als „Verschlüsselung“ bezeichnete Option in DeepSea wurde in „verschlüsselt“ umbenannt. Stellen Sie beim Anwenden von DriveGroups in SUSE Enterprise Storage 7 sicher, dass Sie diese neue Terminologie in Ihrer Servicespezifikation verwenden, da sonst der <command>ceph orch apply</command>-Vorgang fehlschlägt.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Abstimmung von Datenträgergeräten</title>
    <para>
     Sie können die Spezifikation mit folgenden Filtern beschreiben:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Nach Datenträgermodell:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Nach Datenträgerhersteller:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Geben Sie den <replaceable>DISK_VENDOR_STRING</replaceable> immer in Kleinbuchstaben ein.
       </para>
      </tip>
      <para>
       Details zu Datenträgermodell und Hersteller erhalten Sie in der Ausgabe des folgenden Kommandos:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Angabe, ob ein rotierender oder ein nicht rotierender Datenträger vorliegt. SSDs und NVMe-Laufwerke sind keine rotierenden Datenträger.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Implementieren Sie einen Knoten mit <emphasis>allen</emphasis> verfügbaren Laufwerken für OSDs:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Zusätzlich mit Einschränkung der Anzahl passender Datenträger
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Filtern von Geräten nach Größe</title>
    <para>
     Sie können die Datenträgergeräte nach ihrer Größe filtern – wahlweise nach einer genauen Größenangabe oder einem Größenbereich. Der Parameter <option>size:</option> akzeptiert Argumente wie folgt:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       „10G“ – Datenträger mit einer bestimmten Größe.
      </para>
     </listitem>
     <listitem>
      <para>
       „10G:40G“ – Datenträger mit einer Größe im angegebenen Bereich.
      </para>
     </listitem>
     <listitem>
      <para>
       „:10G“ – Datenträger mit einer Größe von maximal 10 GB.
      </para>
     </listitem>
     <listitem>
      <para>
       „40G:“ – Datenträger mit einer Größe von mindestens 40 GB.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Abstimmung nach Datenträgergröße</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>Anführungszeichen erforderlich</title>
     <para>
      Beim Begrenzer „:“ müssen Sie die Größe in Anführungszeichen setzen, da das Zeichen „:“ ansonsten als neuer Konfiguration-Hash interpretiert wird.
     </para>
    </note>
    <tip>
     <title>Abkürzungen für Einheiten</title>
     <para>
      Anstelle von Gigabyte (G) können Sie die Größen auch in Megabyte (M) oder Terabyte (T) angeben.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Beispiele für DriveGroups</title>
    <para>
     Dieser Abschnitt enthält Beispiele verschiedener OSD-Einrichtungen.
    </para>
    <example>
     <title>Einfache Einrichtung</title>
     <para>
      Dieses Beispiel zeigt zwei Knoten mit derselben Einrichtung:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Die entsprechende Datei <filename>drive_groups.yml</filename> sieht wie folgt aus:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Eine solche Konfiguration ist unkompliziert und zulässig. Es stellt sich allerdings das Problem, dass ein Administrator in Zukunft eventuell Datenträger von anderen Herstellern einfügt, die dann nicht berücksichtigt werden. Zur Verbesserung geben Sie weniger Filter für die wesentlichen Eigenschaften der Laufwerke an:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      Im vorherigen Beispiel wird die Deklaration aller rotierenden Geräte als „Datengeräte“ erzwungen und alle nicht rotierenden Geräte werden als „freigegebene Geräte“ (wal, db) genutzt.
     </para>
     <para>
      Wenn Laufwerke mit mehr als 2 TB stets als langsamere Datengeräte eingesetzt werden sollen, können Sie nach Größe filtern:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Erweiterte Einrichtung</title>
     <para>
      Dieses Beispiel umfasst zwei getrennte Einrichtungen: 20 HDDs sollen gemeinsam 2 SSDs nutzen und 10 SSDs nutzen gemeinsam 2 NVMes.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Eine solche Einrichtung kann wie folgt mit zwei Layouts definiert werden:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Erweiterte Einrichtung mit nicht einheitlichen Knoten</title>
     <para>
      In den vorherigen Beispielen wurde angenommen, dass alle Knoten dieselben Laufwerke umfassen. Dies ist jedoch nicht immer der Fall:
     </para>
     <para>
      Knoten 1–5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Knoten 6–10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Mit dem „target“-Schlüssel im Layout können Sie bestimmte Knoten adressieren. Die Salt-Zielnotation trägt zur Vereinfachung bei:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      gefolgt von:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Experteneinrichtung</title>
     <para>
      In allen vorherigen Fällen wird angenommen, dass die WALs und DBs dasselbe Gerät nutzen. Es ist jedoch auch möglich, WAL auf einem dedizierten Gerät zu implementieren:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMes
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Komplexe (und unwahrscheinliche) Einrichtung</title>
     <para>
      In der folgenden Einrichtung soll Folgendes definiert werden:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDDs mit 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDDs mit 1 SSD(db) und 1 NVMe(wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSDs mit 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 Stand-Alone-SSDs (verschlüsselt)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD fungiert als Ersatz und soll nicht bereitgestellt werden
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Zusammenfassung der verwendeten Datenträger:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSDs
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Hersteller: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modell: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Größe: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Die DriveGroups-Definition lautet:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Eine HDD verbleibt, da die Datei von oben nach unten analysiert wird.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Entfernen von OSDs</title>
   <para>
    Vergewissern Sie sich vor dem Entfernen eines OSD-Knotens aus dem Cluster, dass der Cluster über mehr freien Festplattenspeicher verfügt als der OSD-Datenträger, den Sie entfernen möchten. Bedenken Sie, dass durch Entfernen eines OSDs ein Ausgleich des gesamten Clusters durchgeführt wird.
   </para>
   <procedure>
    <step>
     <para>
      Identifizieren Sie das zu entfernende OSD, indem Sie dessen ID ermitteln:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Entfernen Sie ein oder mehrere OSDs aus dem Cluster:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Beispiel:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      Sie können den Status des Entfernungsvorgangs abfragen:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Anhalten der OSD-Entfernung</title>
    <para>
     Nach dem Planen einer OSD-Entfernung können Sie die Entfernung bei Bedarf stoppen. Mit folgendem Kommando wird der Ausgangszustand des OSDs zurückgesetzt und es wird aus der Warteschlange entfernt:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Ersetzen von OSDs</title>
   <para>
    Es gibt verschiedene Gründe für den Austausch eines OSD-Datenträgers. Beispiel:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Der OSD-Datenträger ist laut SMART-Informationen ausgefallen (oder wird bald ausfallen), weshalb keine Daten mehr sicher darauf gespeichert werden können.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie müssen den OSD-Datenträger aufrüsten, beispielsweise vergrößern.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie müssen das Layout des OSD-Datenträgers ändern.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie möchten von einem nicht-LVM- zu einem LVM-basierten Layout wechseln.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Ersetzen Sie ein OSD unter Beibehaltung seiner ID mit folgendem Kommando:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    Der Vorgang zum Ersetzen eines OSDs ist identisch mit dem Vorgang zum Entfernen eines OSDs (weitere Details finden Sie in <xref linkend="removing-node-osds"/>). Allerdings wird beim Ersetzen das OSD nicht dauerhaft aus der CRUSH-Hierarchie entfernt und stattdessen ein Flag <literal>destroyed</literal> gesetzt.
   </para>
   <para>
    Durch das Flag <literal>destroyed</literal> werden OSD-IDs ermittelt, die beim nächsten OSD-Einsatz wiederverwendet werden. Neu hinzugefügte Datenträger, die der DriveGroups-Spezifikation entsprechen (weitere Details finden Sie in <xref linkend="drive-groups"/>), erhalten die OSD-IDs des ersetzten Gegenstücks.
   </para>
   <tip>
    <para>
     Durch Anhängen der Option <option>--dry-run</option> wird die eigentliche Ersetzung nicht ausgeführt, sondern es wird eine Vorschau der Schritte angezeigt, die normalerweise ablaufen würden.
    </para>
   </tip>
   <note>
    <para>
     Falls Sie ein OSD nach einem Ausfall austauschen, wird dringend empfohlen, einen umfassenden Scrub der Platzierungsgruppen auszulösen. Weitere Einzelheiten finden Sie unter <xref linkend="scrubbing-pgs"/>.
    </para>
    <para>
     Führen Sie folgendes Kommando aus, um einen umfassenden Scrub zu starten:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd deep-scrub osd.<replaceable>OSD_NUMBER</replaceable></screen>
   </note>
   <important>
    <title>Ausfall eines freigegebenen Geräts</title>
    <para>
     Wenn ein freigegebenes Gerät für DB/WAL ausfällt, müssen Sie die Ersetzung für alle OSDs durchführen, die das ausgefallene freigegebene Gerät nutzen.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Verschieben des Salt Masters auf einen neuen Knoten</title>

  <para>
   Wenn Sie den Salt-Master-Host durch einen neuen ersetzen müssen, führen Sie folgende Schritte aus:
  </para>

  <procedure>
   <step>
    <para>
     Exportieren Sie die Cluster-Konfiguration und sichern Sie die exportierte JSON-Datei. Weitere Informationen finden Sie im <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     Wenn der alte Salt Master auch der einzige Verwaltungsknoten im Cluster ist, dann verschieben Sie <filename>/etc/ceph/ceph.client.admin.keyring</filename> und <filename>/etc/ceph/ceph.conf</filename> manuell auf den neuen Salt Master.
    </para>
   </step>
   <step>
    <para>
     Stoppen und deaktivieren Sie den Salt Master <systemitem class="daemon">systemd</systemitem>-Service auf dem alten Salt-Master-Knoten:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Wenn der alte Salt-Master-Knoten nicht mehr im Cluster ist, stoppen und deaktivieren Sie auch den <systemitem class="daemon">systemd</systemitem>-Service des Salt Minion:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      Stoppen oder deaktivieren Sie den <literal>salt-minion.service</literal> nicht, wenn auf dem alten Salt-Master-Knoten irgendwelche Ceph-Daemons (MON, MGR, OSD, MDS, Gateway, Monitoring) ausgeführt werden.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Installieren Sie SUSE Linux Enterprise Server 15 SP3 auf dem neuen Salt Master entsprechend dem im <xref linkend="deploy-sles"/> beschriebenen Verfahren.
    </para>
    <tip>
     <title>Übergang der Salt Minions</title>
     <para>
      Damit der Übergang der Salt Minions auf den neuen Salt Master erleichtert wird, entfernen Sie den öffentlichen Schlüssel des ursprünglichen Salt Masters aus allen Minions:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Installieren Sie das Paket <package>salt-master</package> und gegebenenfalls das Paket <package>salt-minion</package> auf dem neuen Salt Master.
    </para>
   </step>
   <step>
    <para>
     Installieren Sie <systemitem class="resource">ceph-salt</systemitem> auf dem neuen Salt-Master-Knoten:
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Stellen Sie sicher, dass Sie alle drei Kommandos ausführen, bevor Sie fortfahren. Die Befehle sind idempotent. Es spielt keine Rolle, ob sie wiederholt werden.
     </para>
    </important>
   </step>
   <step>
    <para>
     Fügen Sie den neuen Salt Master im Cluster hinzu, wie im <xref linkend="deploy-cephadm-cephsalt"/>, im <xref linkend="deploy-cephadm-configure-minions"/> und im <xref linkend="deploy-cephadm-configure-admin"/> beschrieben.
    </para>
   </step>
   <step>
    <para>
     Importieren Sie die gesicherte Cluster-Konfiguration und wenden Sie sie an:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Benennen Sie die <literal>minion id</literal> des Salt Masters in der exportierten Datei <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> um, bevor Sie sie importieren.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Aktualisieren der Cluster-Knoten</title>

  <para>
   Wenden Sie regelmäßig Updates im laufenden Betrieb an, damit die Ceph-Cluster-Knoten auf dem neuesten Stand bleiben.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Software-Repositorys</title>
   <para>
    Bevor Sie den Cluster mit den aktuellen Software-Paketen patchen, prüfen Sie, ob alle Cluster-Knoten auf die relevanten Repositorys zugreifen können. Eine Liste aller erforderlichen Repositorys finden Sie in <xref linkend="verify-previous-upgrade-patch-repos-repos"/>.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Repository-Staging</title>
   <para>
    Wenn Sie mit einem Staging-Tool arbeiten (z. B. SUSE Manager, Subscription Management Tool oder RMT), mit dem die Software-Repositorys für die Cluster-Knoten bereitgestellt werden, müssen die Phasen für beide „Updates“-Repositorys für SUSE Linux Enterprise Server und SUSE Enterprise Storage zum gleichen Zeitpunkt erstellt werden.
   </para>
   <para>
    Wir empfehlen dringend, ein Staging-Tool zu verwenden, um Patches mit <literal>eingefrorenen</literal> oder <literal>gestaffelten</literal> Patch-Stufen anzuwenden. Damit wird gewährleistet, dass die neu in den Cluster eintretenden Knoten dieselbe Patchstufe aufweisen wie die Knoten, die sich bereits im Cluster befinden. So müssen Sie nicht mehr die aktuellen Patches auf alle Cluster-Knoten anwenden, bevor neue Knoten in den Cluster aufgenommen werden können.
   </para>
  </sect2>

  <sect2>
   <title>Ausfallzeit von Ceph-Services</title>
   <para>
    Je nach Konfiguration werden die Cluster-Knoten während der Aktualisierung ggf. neu gestartet. Wenn ein Single-Point-of-Failure für Services wie Object Gateway, Samba Gateway, NFS Ganesha oder iSCSI vorliegt, werden die Client-Computer unter Umständen vorübergehend von Services getrennt, dessen Knoten neu gestartet werden.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Ausführen der Aktualisierung</title>
   <para>
    Aktualisieren Sie mit folgendem Kommando die Softwarepakete auf allen Cluster-Knoten auf die aktuelle Version:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Aktualisieren von Ceph</title>

  <para>
   Sie können cephadm anweisen, Ceph von einer Bugfix-Version auf eine andere zu aktualisieren. Die automatische Aktualisierung der Ceph-Services hält sich an die empfohlene Reihenfolge. Sie beginnt mit Ceph Managers, Ceph Monitors und geht dann weiter zu anderen Services wie Ceph OSDs, Metadatenservern und Object Gateways. Jeder Daemon wird erst dann neu gestartet, wenn Ceph anzeigt, dass der Cluster verfügbar bleibt.
  </para>

  <note>
   <para>
    Die folgende Aktualisierung wird mit dem Kommando <command>ceph orch upgrade</command> ausgeführt. Beachten Sie, dass in den folgenden Anweisungen detailliert beschrieben ist, wie Sie Ihren Ceph-Cluster mit einer Produktversion aktualisieren (z. B. einem Wartungsupdate). Sie erhalten darin <emphasis>keine</emphasis> Informationen, wie Sie Ihren Cluster von einer Produktversion auf eine andere aktualisieren.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Starten der Aktualisierung</title>
   <para>
    Vergewissern Sie sich vor dem Starten der Aktualisierung, dass alle Knoten zurzeit online sind und der Cluster integer ist:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    So aktualisieren Sie auf eine bestimmte Ceph-Version:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7.1/ceph/ceph:latest</screen>
   <para>
    Rüsten Sie die Pakete auf den Hosts auf:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Überwachen der Aktualisierung</title>
   <para>
    Ermitteln Sie mit folgendem Kommando, ob eine Aktualisierung durchgeführt wird:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Während des Aktualisierungsvorgangs sehen Sie in der Ceph-Statusausgabe einen Fortschrittsbalken:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7.1/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    Sie können auch das cephadm-Protokoll beobachten:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Abbrechen einer Aktualisierung</title>
   <para>
    Sie können den Aktualisierungsvorgang jederzeit stoppen:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Anhalten oder Neustarten des Clusters</title>

  <para>
   In einigen Fällen muss möglicherweise der gesamte Cluster angehalten oder neugestartet werden. Wir empfehlen, sorgfältig nach Abhängigkeiten von aktiven Services zu suchen. Die folgenden Schritte beschreiben den Vorgang zum Stoppen und Starten des Clusters:
  </para>

  <procedure>
   <step>
    <para>
     Weisen Sie den Ceph-Cluster an, für OSDs das Flag „noout“ zu setzen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Stoppen Sie die Daemons und Knoten in der folgenden Reihenfolge:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Speicher-Clients
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways wie NFS Ganesha oder Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Führen Sie Wartungsaufgaben aus, falls erforderlich.
    </para>
   </step>
   <step>
    <para>
     Starten Sie die Knoten und Server in umgekehrter Reihenfolge des Herunterfahrens:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways wie NFS Ganesha oder Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Speicher-Clients
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Entfernen Sie das Flag „noout“:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Vollständiges Entfernen eines Ceph-Clusters</title>

  <para>
   Mit dem Kommando <command>ceph-salt purge</command> wird der gesamte Cluster entfernt. Wenn weitere Ceph-Cluster bereitgestellt sind, wird der von <command>ceph -s</command> gemeldete Cluster bereinigt. Auf diese Weise können Sie die kleinste menschliche Umgebung bereinigen, wenn Sie verschiedene Einrichtungen testen.
  </para>

  <para>
   Damit nicht versehentlich ein Löschvorgang durchgeführt wird, prüft die Orchestrierung, ob die Sicherheitsmaßnahmen deaktiviert sind. Mit folgendem Kommando können Sie die Sicherheitsmaßnahmen deaktivieren und den Ceph-Cluster entfernen:
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
