<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph-monitor">
 <title>Ermitteln des Clusterzustands</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Die Überwachung eines aktiven Clusters ist mit dem Werkzeug <command>ceph</command> möglich. Zur Ermittlung des Cluster-Zustands wird in der Regel der Status der Ceph OSDs, Ceph Monitors, Platzierungsgruppen und Metadatenserver geprüft.
 </para>
 <tip>
  <title>Interaktiver Modus</title>
  <para>
   Geben Sie zur Ausführung des Werkzeugs <command>ceph</command> im interaktiven Modus <command>ceph</command> in der Kommandozeile ohne Argumente ein. Der interaktive Modus ist praktischer, wenn Sie vorhaben, mehrere <command>ceph</command>-Kommandos in einer Zeile einzugeben. Beispiel:
  </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon stat</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>Überprüfen des Status eines Clusters</title>

  <para>
   Mit <command>ceph -s</command> oder <command>ceph -s</command> rufen Sie den aktuellen Zustand des Clusters ab:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph -s
cluster:
    id:     b4b30c6e-9681-11ea-ac39-525400d7702d
    health: HEALTH_OK

  services:
    mon: 5 daemons, quorum ses-min1,ses-master,ses-min2,ses-min4,ses-min3 (age 2m)
    mgr: ses-min1.gpijpm(active, since 3d), standbys: ses-min2.oopvyh
    mds: my_cephfs:1 {0=my_cephfs.ses-min1.oterul=up:active}
    osd: 3 osds: 3 up (since 3d), 3 in (since 11d)
    rgw: 2 daemons active (myrealm.myzone.ses-min1.kwwazo, myrealm.myzone.ses-min2.jngabw)

  task status:
    scrub status:
        mds.my_cephfs.ses-min1.oterul: idle

  data:
    pools:   7 pools, 169 pgs
    objects: 250 objects, 10 KiB
    usage:   3.1 GiB used, 27 GiB / 30 GiB avail
    pgs:     169 active+clean
</screen>

  <para>
   Die Ausgabe bietet die folgenden Informationen:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Cluster-ID
    </para>
   </listitem>
   <listitem>
    <para>
     Cluster-Zustand
    </para>
   </listitem>
   <listitem>
    <para>
     Die Monitor-Zuordnungsepoche und den Status des Monitor-Quorums
    </para>
   </listitem>
   <listitem>
    <para>
     Die OSD-Zuordnungsepoche und den Status der OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     Status der Ceph Managers
    </para>
   </listitem>
   <listitem>
    <para>
     Status der Object Gateways
    </para>
   </listitem>
   <listitem>
    <para>
     Die Version der Placement-Group-Zuordnung
    </para>
   </listitem>
   <listitem>
    <para>
     Die Anzahl der Placement Groups und Pools
    </para>
   </listitem>
   <listitem>
    <para>
     Die <emphasis>nominelle</emphasis> Menge der gespeicherten Daten und die Anzahl der gespeicherten Objekte
    </para>
   </listitem>
   <listitem>
    <para>
     Die Gesamtmenge der gespeicherten Daten
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Wie Ceph die Datennutzung berechnet</title>
   <para>
    Der Wert <literal>used</literal> bezeichnet den tatsächlich belegten Basisspeicherplatz. Der Wert <literal>xxx GB / xxx GB</literal> bezeichnet den verfügbaren Speicherplatz (der kleinere Wert) der gesamten Speicherkapazität des Clusters. Die nominelle Menge spiegelt die Menge der gespeicherten Daten wider, bevor diese reproduziert oder geklont werden oder ein Snapshot davon erstellt wird. Daher übersteigt die Menge der tatsächlich gespeicherten Daten normalerweise die nominelle gespeicherte Menge, weil Ceph Reproduktionen der Daten erstellt und die Speicherkapazität möglicherweise auch zum Klonen und für Snapshots verwendet.
   </para>
  </tip>

  <para>
   Mit den folgenden Kommandos werden die Statusinformationen ebenfalls sofort angezeigt:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Sollen die Angaben in Echtzeit aktualisiert werden, geben Sie diese Kommandos (auch <command>ceph -s</command>) als Argument für das Kommando <command>watch</command> an:
  </para>

<screen><prompt role="root"># </prompt>watch -n 10 'ceph -s'</screen>

  <para>
   Drücken Sie <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>, wenn Sie den Vorgang nicht länger beobachten möchten.
  </para>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>Überprüfen der Clusterintegrität</title>

  <para>
   Überprüfen Sie den Zustand Ihres Clusters nach dessen Start und bevor Sie mit dem Lesen und/oder Schreiben von Daten beginnen:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    Wenn Sie nicht standardmäßige Standorte für Ihre Konfiguration oder Ihren Schlüsselbund angegeben haben, können Sie diese angeben mit:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   Der Ceph-Cluster gibt einen der folgenden Zustandscodes zurück:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs sind als inaktiv gekennzeichnet. Der OSD Daemon wurde möglicherweise gestoppt oder Peer-OSDs erreichen den OSD möglicherweise nicht über das Netzwerk. Üblicherweise handelt es sich dabei um einen gestoppten oder abgestürzten Daemon, einen inaktiven Host oder einen Netzwerkausfall.
     </para>
     <para>
      Verifizieren Sie, dass sich der Host in einem ordnungsgemäßen Zustand befindet, der Daemon gestartet ist und das Netzwerk funktioniert. Falls der Daemon abgestürzt ist, enthält die Protokolldatei des Daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) möglicherweise Informationen zur Fehlersuche.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>CRUSH-Typ</replaceable>_DOWN, beispielsweise OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Alle OSDs in einem bestimmten CRUSH-Teilbaum sind als inaktiv gekennzeichnet, wie zum Beispiel alle OSDs auf einem Host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      In der CRUSH Map-Hierarchie wird auf einen OSD verwiesen, er ist jedoch nicht vorhanden. Der OSD kann aus der CRUSH-Hierarchie entfernt werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Die Auslastungsgrenzwerte für <emphasis>backfillfull</emphasis> (Standardwert 0,90), <emphasis>nearfull</emphasis> (Standardwert 0,85), <emphasis>full</emphasis> (Standardwert 0,95) und/oder <emphasis>failsafe_full</emphasis> sind nicht aufsteigend. Insbesondere erwarten wir <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> und <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      Mit folgendem Kommando lesen Sie die aktuellen Werte aus:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      Die Grenzwerte können mit folgenden Kommandos angepasst werden:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>full</emphasis> überschritten, was verhindert, dass der Cluster Schreibvorgänge ausführt. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
     <para>
      Der zurzeit definierte Grad <emphasis>full</emphasis> wird angezeigt mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Eine kurzfristige Behelfslösung zum Wiederherstellen der Schreibverfügbarkeit ist die geringfügige Erhöhung des Grenzwerts:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Fügen Sie neuen Speicherplatz zum Cluster hinzu, indem Sie weitere OSDs bereitstellen, oder löschen Sie bestehende Daten, um Speicherplatz freizugeben.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>backfillfull</emphasis> überschritten, wodurch verhindert wird, dass auf diesem Gerät ein Datenausgleich stattfindet. Hierbei handelt es sich um eine frühzeitige Warnung, dass der Ausgleich möglicherweise nicht durchgeführt werden kann und der Cluster fast voll ist. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Einer oder mehrere OSDs haben den Grenzwert <emphasis>nearfull</emphasis> überschritten. Hierbei handelt es sich um eine frühzeitige Warnung, dass der Cluster fast voll ist. Die Auslastung pro Pool wird überprüft mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Eine oder mehrere interessante Cluster-Flags wurden gesetzt. Mit Ausnahme von <emphasis>full</emphasis> können diese Flags festgelegt oder gelöscht werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Die folgenden Flags sind verfügbar:
     </para>
     <variablelist>
      <varlistentry>
       <term>Vollständig</term>
       <listitem>
        <para>
         Der Cluster wird als voll gekennzeichnet und kann keine Schreibvorgänge mehr bedienen.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Lese- und Schreibvorgänge werden ausgesetzt.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         OSDs dürfen nicht gestartet werden.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         OSD-Fehlerberichte werden ignoriert, sodass die Monitors keine OSDs mit <emphasis>down</emphasis> kennzeichnen.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         OSDs, die vorher mit <emphasis>out</emphasis> gekennzeichnet wurden, werden beim Starten nicht wieder mit <emphasis>in</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>Down</emphasis> OSDs werden nach dem konfigurierten Intervall nicht automatisch mit <emphasis>out</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Wiederherstellung oder Datenausgleich ist angehalten.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Scrubbing (Informationen hierzu finden Sie in <xref linkend="scrubbing-pgs"/>) ist deaktiviert.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         Die Cache-Tiering-Aktivität ist angehalten.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Für eine oder mehrere OSDs wurde eine Interessensflagge pro OSD gesetzt. Die folgenden Flags sind verfügbar:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         Der OSD darf nicht starten.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Fehlerberichte für diesen OSD werden ignoriert.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Wenn dieser OSD vorher automatisch nach einem Fehler mit <emphasis>out</emphasis> markiert wurde, wird er beim Start nicht mit <emphasis>in</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Wenn dieser OSD inaktiv ist, wird er nach dem konfigurierten Intervall nicht automatisch mit <emphasis>out</emphasis> gekennzeichnet.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Pro-OSD-Flags werden gesetzt oder gelöscht mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      Die CRUSH Map verwendet sehr alte Einstellungen und sollte aktualisiert werden. Die ältesten Tunables, die verwendet werden können (d. h. die älteste Client-Version, die eine Verbindung zum Cluster herstellen kann), ohne diese Zustandswarnung auszulösen, werden durch die Konfigurationsoption <option>mon_crush_min_required_version</option> festgelegt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      Die CRUSH Map verwendet eine ältere, nicht optimale Methode zur Berechnung von Gewichtungszwischenwerten für Straw Buckets. Die CRUSH Map sollte aktualisiert werden, um die neuere Methode zu verwenden (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Einer oder mehrere Cache Pools wurden nicht mit einem Treffersatz zum Verfolgen der Auslastung konfiguriert. Dadurch wird verhindert, dass der Tiering-Agent unbenutzte Objekte erkennt, die aus dem Cache entfernt werden müssen. Treffersätze werden im Cache Pool konfiguriert mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      Es werden keine OSDs vor Luminous Version 12 ausgeführt, doch das Flag <option>sortbitwise</option> wurde nicht gesetzt. Das Flag <option>sortbitwise</option> muss gesetzt werden, damit OSDs ab Luminous Version 12 starten:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben das Kontingent erreicht und lassen keine weiteren Schreibvorgänge zu. Pool-Kontingente und Auslastungswerte werden mit folgendem Kommando festgelegt:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df detail</screen>
     <para>
      Legen Sie entweder das Pool-Kontingent mit 
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      fest oder löschen Sie einige vorhandene Daten, um die Auslastung zu verringern.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      Die Datenverfügbarkeit ist reduziert. Dies bedeutet, dass der Cluster potenzielle Lese- oder Schreibanforderungen für einige Daten im Cluster nicht bedienen kann. Dies ist insbesondere dann der Fall, wenn sich mindestens eine PG in einem Zustand befindet, der die Ausführung von E/A-Anforderungen nicht zulässt. Problematische PG-Zustände sind <emphasis>peering</emphasis>, <emphasis>stale</emphasis>, <emphasis>incomplete</emphasis> und das Fehlen von <emphasis>active</emphasis> (wenn diese Zustände nicht schnell gelöscht werden). Detaillierte Informationen dazu, welche PGs betroffen sind, erhalten Sie durch Ausführen von:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph health detail</screen>
     <para>
      In den meisten Fällen besteht die Ursache darin, dass einer oder mehrere OSDs aktuell inaktiv sind. Der Zustand bestimmter problematischer PGs kann abgefragt werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      Die Datenredundanz ist für einige Daten reduziert. Dies bedeutet, dass der Cluster nicht über die gewünschte Anzahl der Reproduktionen für alle Daten (für reproduzierte Pools) oder Erasure Code-Fragmente (für Erasure Coded Pools) verfügt. Dies ist insbesondere dann der Fall, wenn bei einer oder mehreren PGs das Flag <emphasis>degraded</emphasis> oder <emphasis>undersized</emphasis> gesetzt wurde (es sind nicht genügend Instanzen dieser Placement Group im Cluster vorhanden) oder wenn einige Zeit das Flag <emphasis>clean</emphasis> nicht gesetzt war. Detaillierte Informationen dazu, welche PGs betroffen sind, erhalten Sie durch Ausführen von:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph health detail</screen>
     <para>
      In den meisten Fällen besteht die Ursache darin, dass einer oder mehrere OSDs aktuell inaktiv sind. Der Zustand bestimmter problematischer PGs kann abgefragt werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      Die Datenredundanz ist möglicherweise für einige Daten reduziert oder in Gefahr, weil freier Speicherplatz im Cluster fehlt. Dies ist insbesondere dann der Fall, wenn für eine oder mehrere PGs das Flag <emphasis>backfill_toofull</emphasis> oder <emphasis>recovery_toofull</emphasis> gesetzt wurde. Dies bedeutet, dass der Cluster keine Daten migrieren oder wiederherstellen kann, weil einer oder mehrere OSDs den Grenzwert <emphasis>backfillfull</emphasis> überschritten haben.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Beim Daten-Scrubbing (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing-pgs"/>) wurden einige Probleme bezüglich der Datenkonsistenz im Cluster erkannt. Dies ist insbesondere dann der Fall, wenn für eine oder mehrere PGs das Flag <emphasis>inconsistent</emphasis> oder <emphasis>snaptrim_error</emphasis> gesetzt wurde. Dadurch wird angegeben, dass bei einem früheren Scrubbing-Vorgang ein Problem gefunden wurde. Möglicherweise wurde auch das Flag <emphasis>repair</emphasis> gesetzt, was bedeutet, dass eine derartige Inkonsistenz gerade repariert wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Bei kürzlich durchgeführten OSD-Scrubbing-Vorgängen wurden Inkonsistenzen erkannt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Ein Cache-Schicht-Pool ist fast voll. In diesem Kontext wird „full“ durch die Eigenschaften <emphasis>target_max_bytes</emphasis> und <emphasis>target_max_objects</emphasis> des Caches Pools bestimmt. Wenn der Pool den Grenzwert erreicht, werden Schreibanforderungen an den Pool möglicherweise blockiert und Daten aus dem Cache entfernt und gelöscht. Dieser Zustand führt normalerweise zu sehr hohen Latenzen und schlechter Leistung. Der Grenzwert für die Größe des Pools kann angepasst werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      Die normale Aktivität zum Leeren des Caches wird möglicherweise auch gedrosselt durch die reduzierte Verfügbarkeit oder Leistung der Basisschicht oder durch die Cluster-Last insgesamt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      Die Anzahl der verwendeten PGs liegt unterhalb des konfigurierbaren Grenzwerts der <option>mon_pg_warn_min_per_osd</option>-PGs pro OSD. Dies führt eventuell dazu, dass die Daten nicht optimal auf die OSDs im Cluster verteilt und ausgeglichen werden und die Leistung insgesamt verschlechtert wird.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      Die Anzahl der verwendeten PGs liegt oberhalb des konfigurierbaren Grenzwerts der <option>mon_pg_warn_min_per_osd</option>-PGs pro OSD. Dies führt möglicherweise zu höherer Arbeitsspeicherauslastung für OSD-Daemons, langsamerem Peering nach Änderungen des Cluster-Zustands (beispielsweise OSD-Neustarts, Hinzufügen oder Entfernen) sowie höherer Last für Ceph Managers und Ceph Monitors.
     </para>
     <para>
      Der Wert <option>pg_num</option> für bestehende Pools kann nicht reduziert werden, der Wert <option>pgp_num</option> jedoch schon. Dadurch werden einige PGs effizient auf denselben Gruppen von OSDs angeordnet, wodurch einige der oben beschriebenen negativen Auswirkungen abgeschwächt werden. Der <option>pgp_num</option>-Wert kann angepasst werden mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      Bei einem oder mehreren Pools ist der <option>pgp_num</option>-Wert kleiner als der <option>pg_num</option>-Wert. Dies ist normalerweise ein Anzeichen dafür, dass die PG-Anzahl erhöht wurde, ohne auch das Platzierungsverhalten zu erhöhen. Dieses Problem wird normalerweise dadurch behoben, dass <option>pgp_num</option> mit <option>pg_num</option> abgeglichen wird, was die Datenmigration auslöst. Verwenden Sie hierzu:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Bei mindestens einem Pool ist die durchschnittliche Anzahl von Objekten pro PG erheblich höher als der Durchschnitt im Cluster insgesamt. Der spezifische Grenzwert wird gesteuert durch den Konfigurationswert<option>mon_pg_warn_max_object_skew</option>. Dies ist normalerweise ein Anzeichen dafür, dass die Pools mit den meisten Daten im Cluster zu wenige PGs enthalten und/oder dass andere Pools mit weniger Daten zu viele PGs enthalten. Um die Zustandswarnung abzustellen, kann der Grenzwert durch Anpassen der Konfigurationsoption <option>mon_pg_warn_max_object_skew</option> an den Monitors erhöht werden.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Es ist zwar ein Pool mit einem oder mehreren Objekten vorhanden, wurde jedoch nicht für die Verwendung durch eine bestimmte Anwendung gekennzeichnet. Heben Sie diese Warnmeldung auf, indem Sie den Pool für die Verwendung durch eine Anwendung kennzeichnen. Beispiel, wenn der Pool von RBD verwendet wird:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Wenn der Pool durch eine benutzerdefinierte Anwendung „foo“ verwendet wird, können Sie ihn auch mit dem Kommando der untersten Ebene kennzeichnen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben das Kontingent erreicht (oder erreichen es bald). Der Grenzwert zum Auslösen dieser Fehlerbedingung wird durch die Konfigurationsoption <option>mon_pool_quota_crit_threshold</option> gesteuert. Pool-Kontingente werden nach oben oder unten angepasst (oder entfernt) mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Durch Festlegen des Kontingentwerts auf 0 wird das Kontingent deaktiviert.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Einer oder mehrere Pools haben bald das Kontingent erreicht. Der Grenzwert zum Auslösen dieser Fehlerbedingung wird durch die Konfigurationsoption <option>mon_pool_quota_warn_threshold</option> gesteuert. Pool-Kontingente werden nach oben oder unten angepasst (oder entfernt) mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Durch Festlegen des Kontingentwerts auf 0 wird das Kontingent deaktiviert.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Mindestens ein Objekt im Cluster ist nicht auf dem Knoten gespeichert, auf dem es laut Cluster gespeichert werden soll. Dies ist ein Anzeichen dafür, dass die Datenmigration aufgrund einer kürzlich vorgenommenen Änderung am Cluster noch nicht abgeschlossen ist. Falsch platzierte Daten stellen an sich keine Gefahr dar. Die Datenkonsistenz ist niemals in Gefahr und alte Kopien von Objekten werden niemals entfernt, bevor die gewünschte Anzahl an neuen Kopien (an den gewünschten Speicherorten) vorhanden ist.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      Ein oder mehrere Objekte im Cluster werden nicht gefunden. Dies ist insbesondere dann der Fall, wenn die OSDs wissen, dass eine neue oder aktualisierte Kopie eines Objekts vorhanden sein sollte, jedoch keine Kopie dieser Version des Objekts auf OSDs gefunden wurden, die zurzeit aktiv sind. Lese- oder Schreibanforderungen an die „unfound“-Objekte werden blockiert. Im Idealfall kann das inaktive OSD, auf dem sich das aktuelle Exemplar des nicht gefundenen Objekts befindet, wieder aktiviert werden. Dafür in Frage kommende OSDs können anhand des Peering-Zustands für die PGs identifiziert werden, die für das nicht gefundene Objekt zuständig sind:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      Die Verarbeitung einer oder mehrerer OSD-Anforderungen dauert sehr lange. Dies ist möglicherweise ein Anzeichen für eine extreme Last, ein langsames Speichergerät oder einen Softwarefehler. Mit folgendem Kommando, das auf dem OSD-Host ausgeführt wird, fragen Sie die Anforderungswarteschlange auf den in Frage kommenden OSDs ab:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm enter --name osd.<replaceable>ID</replaceable> -- ceph daemon osd.<replaceable>ID</replaceable> ops</screen>
     <para>
      Sie sehen eine Zusammenfassung der langsamsten kürzlich vorgenommenen Anforderungen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm enter --name osd.<replaceable>ID</replaceable> -- ceph daemon osd.<replaceable>ID</replaceable> dump_historic_ops</screen>
     <para>
      Sie finden den Standort eines OSDs mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Mindestens eine OSD-Anforderung wurde relativ lange Zeit blockiert, z. B. 4.096 Sekunden. Dies ist ein Anzeichen dafür, dass sich entweder der Cluster für einen längeren Zeitraum in einem fehlerhaften Zustand befindet (beispielsweise wenn nicht genügend OSDs ausgeführt werden oder inaktive PGs vorliegen) oder wenn interne Probleme am OSD aufgetreten sind.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      Es wurde länger kein Scrubbing bei mindestens einer PG durchgeführt (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing-pgs"/>). Bei PGs wird normalerweise alle <option>mon_scrub_interval</option> Sekunden ein Scrubbing durchgeführt und diese Warnmeldung wird ausgelöst, wenn diese <option>mon_warn_not_scrubbed</option>-Intervalle ohne Scrubbing abgelaufen sind. Bei PGs wird kein Scrubbing durchgeführt, wenn sie nicht als intakt („clean“) gekennzeichnet sind. Dies kann vorkommen, wenn sie falsch platziert wurden oder eingeschränkt leistungsfähig sind (weitere Informationen hierzu finden Sie oben unter PG_AVAILABILITY und PG_DEGRADED). Ein Scrubbing einer intakten PG wird manuell initiiert mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Bei einer oder mehreren PGs wurden länger kein umfassender Scrub durchgeführt (weitere Informationen hierzu finden Sie in <xref linkend="scrubbing-pgs"/>). Bei PGs wird normalerweise alle <option>osd_deep_mon_scrub_interval</option> Sekunden ein Scrubbing durchgeführt und diese Warnmeldung wird ausgelöst, wenn dieser <option>mon_warn_not_deep_scrubbed</option>-Zeitraum ohne Scrubbing abgelaufen ist. Bei PGs wird kein (Deep) Scrubbing durchgeführt, wenn sie nicht als intakt („clean“) gekennzeichnet sind. Dies kann vorkommen, wenn sie falsch platziert wurden oder eingeschränkt leistungsfähig sind (weitere Informationen hierzu finden Sie oben unter PG_AVAILABILITY und PG_DEGRADED). Ein Scrubbing einer intakten PG wird manuell initiiert mit:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Wenn Sie nicht standardmäßige Standorte für Ihre Konfiguration oder Ihren Schlüsselbund angegeben haben, können Sie diese angeben mit:
   </para>
<screen><prompt role="root"># </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Überprüfen der Nutzungsstatistik eines Clusters</title>

  <para>
   Mit dem Kommando <command>ceph df</command> prüfen Sie die Datenauslastung und Datenverteilung auf die Pools in einem Cluster. Mit <command>ceph df detail</command> erhalten Sie weitere Details.
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph df
--- RAW STORAGE ---
CLASS  SIZE    AVAIL   USED     RAW USED  %RAW USED
hdd    30 GiB  27 GiB  121 MiB   3.1 GiB      10.40
TOTAL  30 GiB  27 GiB  121 MiB   3.1 GiB      10.40

--- POOLS ---
POOL                   ID  STORED   OBJECTS  USED     %USED  MAX AVAIL
device_health_metrics   1      0 B        0      0 B      0    8.5 GiB
cephfs.my_cephfs.meta   2  1.0 MiB       22  4.5 MiB   0.02    8.5 GiB
cephfs.my_cephfs.data   3      0 B        0      0 B      0    8.5 GiB
.rgw.root               4  1.9 KiB       13  2.2 MiB      0    8.5 GiB
myzone.rgw.log          5  3.4 KiB      207    6 MiB   0.02    8.5 GiB
myzone.rgw.control      6      0 B        8      0 B      0    8.5 GiB
myzone.rgw.meta         7      0 B        0      0 B      0    8.5 GiB
</screen>

  <para>
   Der Abschnitt <literal>RAW STORAGE</literal> der Ausgabe gibt einen Überblick über den Speicherplatz, den Ihr Cluster für die Daten nutzt.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>CLASS</literal>: Die Speicherklasse des Geräts. Weitere detaillierte Informationen zu Geräteklassen finden Sie in <xref linkend="crush-devclasses"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>SIZE</literal>: Die gesamte Speicherkapazität des Clusters.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: Der verfügbare freie Speicherplatz im Cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: Der Speicherplatz (über alle OSDs akkumuliert), der ausschließlich für Datenobjekte auf dem Blockgerät zugewiesen ist.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: Summe des belegten Speicherplatzes („USED“) und des Speicherplatzes auf dem Blockgerät, der für Ceph-Zwecke zugewiesen/reserviert ist, z. B. der BlueFS-Bereich für BlueStore.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: Der Prozentsatz des genutzten Basisspeicherplatzes. Verwenden Sie diese Zahl in Verbindung mit <literal>full ratio</literal> und <literal>near full ratio</literal>, um sicherzustellen, dass die Kapazität des Clusters nicht voll ausgelastet wird. Weitere Informationen finden Sie in <xref linkend="storage-capacity"/>.
    </para>
    <note>
     <title>Füllstand des Clusters</title>
     <para>
      Wenn sich der Rohspeicherfüllstand dem Wert 100 % nähert, müssen Sie neuen Speicher in den Cluster aufnehmen. Eine höhere Auslastung führt möglicherweise zu einzelnen vollen OSDs und Problemen mit dem Cluster-Zustand.
     </para>
     <para>
      Listen Sie mit dem Kommando <command>ceph osd df tree</command> den Füllstand aller OSDs auf.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   Im Abschnitt <literal>POOLS</literal> der Ausgabe finden Sie eine Liste der Pools und die nominelle Auslastung der einzelnen Pools. Die Ausgabe in diesem Abschnitt umfasst <emphasis>keine</emphasis> Reproduktionen, Klone oder Snapshots. Beispiel: Wenn Sie ein Objekt mit 1 MB Daten speichern, beträgt die nominelle Auslastung 1 MB. Die tatsächliche Auslastung beträgt jedoch möglicherweise 2 MB oder mehr, je nach Anzahl der Reproduktionen, Klone und Snapshots.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>POOL</literal>: Der Name des Pools.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: Die Pool-ID.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>STORED</literal>: Die vom Benutzer gespeicherte Datenmenge.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: Die nominelle Anzahl der pro Pool gespeicherten Objekte.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: Der Speicherplatz (in kB), der von allen OSD-Knoten ausschließlich für Daten zugewiesen ist.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: Der nominelle Prozentsatz des genutzten Speichers pro Pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: Der maximal verfügbare Speicherplatz im jeweiligen Pool.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Die Zahlen im Abschnitt POOLS sind nominell. Sie sind nicht inklusive der Anzahl der Reproduktionen, Snapshots oder Klone zu verstehen. Folglich entspricht die Summe der Zahlen unter <literal>USED</literal> und <literal>%USED</literal> nicht den Zahlen unter <literal>RAW USED</literal> und <literal>%RAW USED</literal> im Abschnitt <literal>RAW STORAGE</literal> der Ausgabe.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Überprüfen des OSD-Status</title>

  <para>
   Mit folgenden Kommandos prüfen Sie die OSDs, um sicherzustellen, dass sie aktiv sind:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd stat</screen>

  <para>
   oder
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump</screen>

  <para>
   OSDs lassen sich auch entsprechend ihrer Position in der CRUSH-Map anzeigen.
  </para>

  <para>
   Mit <command>ceph osd tree</command> werden ein CRUSH-Baum mit einem Host und dessen OSDs, der Status der Aktivität und deren Gewicht ausgegeben:
  </para>

<screen>
   <prompt>cephuser@adm &gt; </prompt>ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME              STATUS  REWEIGHT  PRI-AFF
-1      3  0.02939  root default
-3      3  0.00980    rack mainrack
-2      3  0.00980            host osd-host
0       1  0.00980                    osd.0   up   1.00000   1.00000
1       1  0.00980                    osd.1   up   1.00000   1.00000
2       1  0.00980                    osd.2   up   1.00000   1.00000
</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Suchen nach vollen OSDs</title>

  <para>
   Ceph verhindert, dass Sie an einen vollen OSD schreiben, damit Sie keine Daten verlieren. In einem betriebsbereiten Cluster sollten Sie eine Warnmeldung erhalten, wenn der Cluster annähernd voll ist. Der Wert für <command>mon osd full ratio</command> beträgt standardmäßig 0,95 bzw. 95 % der Kapazität. Wenn dieser Wert erreicht ist, werden die Clients davon abgehalten, Daten zu schreiben. Der Wert <command>mon osd nearfull ratio</command> beträgt standardmäßig 0,85 bzw. 85 % der Kapazität. Wenn dieser Wert erreicht ist, wird eine Zustandswarnung generiert.
  </para>

  <para>
   Volle OSD-Knoten werden durch <command>ceph health</command> gemeldet:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   oder
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   Am besten fügen Sie bei einem vollen Cluster neue OSD-Hosts/-Datenträger hinzu. Dadurch kann der Cluster Daten auf dem neu verfügbaren Speicherplatz verteilen.
  </para>

  <tip>
   <title>Verhindern voller OSDs</title>
   <para>
    Wenn ein OSD voll wird, also 100 % der Festplattenkapazität nutzt, stürzt er normalerweise schnell und ohne Warnmeldung ab. Nachfolgend sehen Sie einige Tipps, die Sie beim Verwalten von OSD-Knoten beachten sollten.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Der Festplattenspeicherplatz der einzelnen OSDs (normalerweise eingehängt unter<filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) muss jeweils auf eine dedizierte zugrundeliegende Festplatte oder Partition gestellt werden.
     </para>
    </listitem>
    <listitem>
     <para>
      Überprüfen Sie die Ceph-Konfigurationsdateien und stellen Sie sicher, dass Ceph die Protokolldatei nicht auf den Festplatten/Partitionen speichert, die explizit für die OSDs vorgesehen sind.
     </para>
    </listitem>
    <listitem>
     <para>
      Vergewissern Sie sich, dass kein anderer Prozess auf die Festplatten/Partitionen schreibt, die explizit für die OSDs vorgesehen sind.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Prüfen des Monitorstatus</title>

  <para>
   Nachdem Sie den Cluster gestartet haben und bevor die ersten Daten gelesen und/oder geschrieben werden, prüfen Sie die Kontingentstatus der Ceph Monitors. Wenn der Cluster bereits Anforderungen verarbeitet, prüfen Sie den Status der Ceph Monitors regelmäßig, ob sie tatsächlich ausgeführt werden.
  </para>

  <para>
   Führen Sie zum Anzeigen der Monitor-Zuordnung Folgendes aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph mon stat</screen>

  <para>
   oder
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump</screen>

  <para>
   Führen Sie zum Prüfen des Quorum-Status für den Monitor-Cluster Folgendes aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph quorum_status</screen>

  <para>
   Ceph gibt den Quorum-Status zurück. Beispielsweise gibt ein Ceph-Cluster, der aus drei Monitors besteht, möglicherweise Folgendes zurück:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Überprüfen des Zustands von Platzierungsgruppen</title>

  <para>
   Placement Groups ordnen Objekte zu OSDs zu. Bei der Überwachung Ihrer Placement Groups sollten diese <literal>active</literal> (aktiv) und <literal>clean</literal> (intakt) sein. Ausführliche Informationen finden Sie in <xref linkend="op-mon-osd-pg"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-capacity">
  <title>Speicherkapazität</title>

  <para>
   Wenn sich ein Ceph-Speicher-Cluster seiner maximalen Kapazität annähert, unterbindet Ceph aus Sicherheitsgründen weitere Schreib- oder Lesevorgänge auf Ceph OSDs, damit kein Datenverlust eintritt. Es wird daher nicht empfohlen, die Quote eines Produktions-Clusters voll auszuschöpfen, da damit die Hochverfügbarkeit nicht mehr gewährleistet ist. Die standardmäßige „full“-Quote ist auf 0,95 eingestellt, also auf 95 % der Kapazität. Für einen Test-Cluster mit nur wenigen OSDs ist dies eine äußerst aggressive Einstellung.
  </para>

  <tip>
   <title>Erhöhen der Speicherkapazität</title>
   <para>
    Achten Sie bei der Überwachung des Clusters auf Warnmeldungen zur <literal>nearfull</literal>-Quote. Dies bedeutet, dass ein Ausfall eines (oder mehrerer) OSDs zu einer Serviceunterbrechung führen würde. Ziehen Sie in Erwägung, weitere OSDs einzufügen und damit die Speicherkapazität zu erhöhen.
   </para>
  </tip>

  <para>
   In einem gängigen Szenario für Test-Cluster entfernt ein Systemadministrator ein Ceph OSD aus dem Ceph-Speicher-Cluster und beobachtet dann den Neuausgleich des Clusters. Anschließend wird ein weiterer Ceph OSD entfernt usw., bis der Cluster schließlich die „full“-Quote erreicht und zum Stillstand kommt. Selbst bei einem Test-Cluster wird eine gewisse Kapazitätsplanung empfohlen. Durch die Planung können Sie einschätzen, wie viel Ersatzkapazität Sie benötigen, damit die Hochverfügbarkeit aufrechterhalten werden kann. Im Idealfall sollten Sie eine ganze Reihe von Ceph OSD-Ausfällen einplanen, bei der Cluster wieder den Status <literal>active + clean</literal> erreichen kann, ohne die betreffenden Ceph OSDs austauschen zu müssen. Sie können einen Cluster im Status <literal>active + degraded</literal> ausführen, doch unter normalen Betriebsbedingungen ist dies nicht ideal.
  </para>

  <para>
   Das nachfolgende Diagramm zeigt einen vereinfachten Ceph-Speicher-Cluster mit 33 Ceph Knoten und je einem Ceph OSD pro Host, die jeweils Lese- und Schreibvorgänge auf einem 3-TB-Laufwerk ausführen. Dieser Beispiel-Cluster besitzt eine maximale Istkapazität von 99 TB. Die Option <option>mon osd full ratio</option> ist auf 0,95 eingestellt. Sobald der Speicherplatz im Cluster unter 5 TB der Restkapazität fällt, werden Lese- und Schreibvorgänge der Clients unterbunden. Die Betriebskapazität des Speicher-Clusters liegt also bei 95 TB, nicht bei 99 TB.
  </para>

  <figure>
   <title>Ceph-Cluster</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   In einem solchen Cluster ist ein Ausfall von einem oder zwei OSDs durchaus normal. In einem weniger häufigen, jedoch denkbaren Szenario fällt der Router oder die Netzversorgung eines Racks aus, sodass mehrere OSDs gleichzeitig ausfallen (z. B. OSDs 7–12). In einem solchen Szenario muss der Cluster dennoch weiterhin betriebsbereit bleiben und den Status <literal>active + clean</literal> erreichen – selbst wenn dafür einige Hosts mit zusätzlichen OSDs in kürzester Zeit eingefügt werden müssen. Wenn die Kapazitätsauslastung zu hoch ist, verlieren Sie unter Umständen keine Daten. Doch beim Beheben eines Ausfalls in einer Fehlerdomäne ist die Datenverfügbarkeit dennoch beeinträchtigt, wenn die Kapazitätsauslastung des Clusters die für Code übersteigt. Aus diesem Grund wird zumindest eine grobe Kapazitätsplanung empfohlen.
  </para>

  <para>
   Ermitteln Sie zwei Zahlen für Ihren Cluster:
  </para>

  <orderedlist>
   <listitem>
    <para>
     die Anzahl der OSDs,
    </para>
   </listitem>
   <listitem>
    <para>
     die Gesamtkapazität des Clusters.
    </para>
   </listitem>
  </orderedlist>

  <para>
   Wenn Sie die Gesamtkapazität des Clusters durch die Anzahl der OSDs im Cluster dividieren, erhalten Sie die Durchschnittskapazität eines OSDs im Cluster. Multiplizieren Sie diese Zahl ggf. mit der Anzahl der OSDs, die laut Ihren Erwartungen im Normalbetrieb gleichzeitig ausfallen könnten (relativ kleine Zahl). Multiplizieren Sie schließlich die Kapazität des Clusters mit der „full“-Quote. Damit erhalten Sie die maximale Betriebskapazität. Subtrahieren Sie dann die Datenmenge der OSDs, die vermutlich ausfallen werden, und Sie erhalten eine angemessene „full“-Quote. Wenn Sie den obigen Vorgang mit einer größeren Anzahl an OSD-Ausfällen (einem OSD-Rack) wiederholen, erhalten Sie eine angemessene Zahl für die „near full“-Quote.
  </para>

  <para>
   Die folgenden Einstellungen gelten nur bei der Cluster-Erstellung und werden dann in der OSD-Karte gespeichert:
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    Diese Einstellungen gelten nur bei der Cluster-Erstellung. Später müssen sie mit den Kommandos <command>ceph osd set-nearfull-ratio</command> und <command>ceph osd set-full-ratio</command>in der OSD-Karte geändert werden.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      Prozentsatz des belegten Datenträgerspeichers, bevor ein OSD als <literal>full</literal> gilt. Der Standardwert lautet 0,95.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      Prozentsatz des belegten Datenträgerspeichers, bevor ein OSD als zu <literal>full</literal> für einen Abgleich gilt. Der Standardwert lautet 0,90.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      Prozentsatz des belegten Datenträgerspeichers, bevor ein OSD als <literal>nearfull</literal> gilt. Der Standardwert lautet 0,85.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Prüfen des OSD-Gewichts</title>
   <para>
    Wenn einige OSDs <literal>nearfull</literal> sind, andere aber noch reichlich Kapazität besitzen, liegt eventuell ein Problem mit dem CRUSH-Gewicht für die <literal>nearfull</literal>-OSDs vor.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op-mon-osd-pg">
  <title>Überwachen der OSDs und Platzierungsgruppen</title>

  <para>
   Voraussetzung für Hochverfügbarkeit und hohe Zuverlässigkeit ist eine fehlertolerante Bearbeitung von Hardware- und Softwareproblemen. Ceph enthält keinen Single-Point-of-Failure und kann Anforderungen nach Daten in einem eingeschränkt leistungsfähigen Modus („degraded“) verarbeiten. Die Datenplatzierung bei Ceph umfasst eine Dereferenzierungsschicht, mit der gewährleistet wird, dass die Daten nicht direkt an bestimmte OSD-Adressen binden. Bei der Suche nach Systemfehlern bedeutet dies, dass die richtige Platzierungsgruppe und die zugrunde liegenden OSDs als Problemursache ermittelt werden müssen.
  </para>

  <tip>
   <title>Zugriff im Fehlerfall</title>
   <para>
    Bei einem Fehler in einem Teil des Clusters können Sie unter Umständen nicht auf ein bestimmtes Objekt zugreifen. Dies bedeutet nicht, dass Sie nicht auf andere Objekte zugreifen können. Wenn ein Fehler auftritt, befolgen Sie die Anweisungen für die Überwachung Ihrer OSDs und Platzierungsgruppen. Nehmen Sie dann die Fehlerbehebung vor.
   </para>
  </tip>

  <para>
   Ceph repariert sich im Allgemeinen selbst. Bleiben Probleme jedoch längere Zeit bestehen, trägt die Überwachung der OSDs und Platzierungsgruppen dazu bei, das Problem zu erkennen.
  </para>

  <sect2 xml:id="op-mon-osds">
   <title>Überwachen der OSDs</title>
   <para>
    Ein OSD befindet sich entweder <emphasis>im Cluster</emphasis> (Status „in“) oder <emphasis>außerhalb des Clusters</emphasis> („out“). Gleichzeitig ist er entweder <emphasis>betriebsbereit und aktiv</emphasis> („up“) oder <emphasis>nicht betriebsbereit und inaktiv</emphasis> („down“). Wenn ein OSD „up“ ist, befindet er sich entweder im Cluster (Sie können Daten lesen und schreiben) oder außerhalb des Clusters. Wenn er sich bis vor Kurzem im Cluster befand und dann aus dem Cluster heraus verschoben wurde, migriert Ceph die Platzierungsgruppen zu anderen OSDs. Befindet sich ein OSD außerhalb des Clusters, weist CRUSH diesem OSD keine Platzierungsgruppen zu. Wenn ein OSD „down“ ist, sollte er auch „out“ sein.
   </para>
   <note>
    <title>Fehlerhafter Zustand</title>
    <para>
     Ist ein OSD „down“ und „in“, liegt ein Problem vor und der Cluster ist nicht in einem einwandfreien Zustand.
    </para>
   </note>
   <para>
    Wenn Sie ein Kommando wie <command>ceph health</command>, <command>ceph -s</command> oder <command>ceph -w</command> ausführen, meldet der Cluster nicht immer <literal>HEALTH OK</literal>. Im Hinblick auf OSDs ist unter den folgenden Umständen zu erwarten, dass der Cluster <emphasis>nicht</emphasis> die Meldung <literal>HEALTH OK</literal> zurückgibt:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Sie haben den Cluster noch nicht gestartet (er kann nicht antworten).
     </para>
    </listitem>
    <listitem>
     <para>
      Sie haben den Cluster gestartet oder neu gestartet. Er ist jedoch noch nicht betriebsbereit, da die Platzierungsgruppen noch erstellt werden und die OSDs gerade den Peering-Prozess durchlaufen.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie haben ein OSD hinzugefügt oder entfernt.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie haben die Clusterzuordnung geändert.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Bei der Überwachung der OSDs ist in jedem Fall darauf zu achten, dass alle OSDs im Cluster betriebsbereit und aktiv sind, wenn der Cluster selbst betriebsbereit und aktiv ist. Mit folgendem Kommando prüfen Sie, ob alle OSDs aktiv sind:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    Das Ergebnis umfasst die Gesamtanzahl der OSDs (x), die Anzahl der OSDs mit dem Status „up“ (y), die Anzahl der OSDs mit dem Status „in“ (z) sowie die Zuordnungsepoche (eNNNN). Wenn die Anzahl der „in“-OSDs höher ist als die Anzahl der „up“-OSDs, ermitteln Sie die nicht aktiven <literal>ceph-osd</literal>-Daemons mit folgendem Kommando:
   </para>
<screen>
<prompt role="root"># </prompt>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    Wenn beispielsweise ein OSD mit ID 1 inaktiv ist, starten Sie es mit:
   </para>
<screen>
<prompt>cephuser@osd &gt; </prompt>sudo systemctl start ceph-<replaceable>CLUSTER_ID</replaceable>@osd.0.service
</screen>
   <para>
    Weitere Informationen zu Problemen im Zusammenhang mit OSDs, die angehalten wurden oder sich nicht neu starten lassen, finden Sie im <xref linkend="bp-troubleshooting-not-running"/>.
   </para>
  </sect2>

  <sect2 xml:id="op-pgsets">
   <title>Zuweisen von Platzierungsgruppen</title>
   <para>
    Beim Zuweisen von Platzierungsgruppen zu den OSDs prüft CRUSH die Anzahl der Reproduktionen für den Pool und weist die Platzierungsgruppe den OSDs so zu, dass jede Reproduktion dieser Platzierungsgruppe einem anderen OSD zugeordnet wird. Wenn der Pool beispielsweise drei Reproduktionen einer Platzierungsgruppe benötigt, weist CRUSH die Reproduktionen beispielsweise <literal>osd.1</literal>, <literal>osd.2</literal> und <literal>osd.3</literal> zu. CRUSH strebt dabei eine pseudozufällige Platzierung an, bei der die in der CRUSH-Zuordnung festgelegten Fehlerdomänen berücksichtigt werden, sodass Platzierungsgruppen nur in seltenen Fällen den jeweils nächstgelegenen OSDs in einem großen Cluster zugewiesen werden. Der OSD-Satz, in dem sich die Reproduktion einer bestimmten Platzierungsgruppe befinden soll, wird als <emphasis>„acting“-Satz</emphasis> bezeichnet. In einigen Fällen ist ein OSD im „acting“-Satz nicht betriebsbereit oder kann anderweitig keine Anforderungen nach Objekten in der Platzierungsgruppe verarbeiten. In diesen Situationen liegt eines der folgenden Szenarien vor:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Sie haben ein OSD hinzugefügt oder entfernt. CRUSH hat die Platzierungsgruppe dann anderen OSDs zugewiesen und damit die Zusammensetzung des <emphasis>agierenden Satzes</emphasis> verändert, sodass die Migration der Daten im Rahmen eines „backfill“-Prozesses ausgelöst wurde.
     </para>
    </listitem>
    <listitem>
     <para>
      Ein OSD war „down“, wurde neu gestartet und wird nunmehr wiederhergestellt.
     </para>
    </listitem>
    <listitem>
     <para>
      Ein OSD im <emphasis>„acting“-Satz</emphasis> ist „down“ oder kann keine Anforderungen verarbeiten und ein anderer OSD hat vorübergehend dessen Aufgaben übernommen.
     </para>
     <para>
      Ceph verarbeitet eine Client-Anforderung mit dem <emphasis>„up“-Satz</emphasis>, also dem Satz der OSDs, die die Anforderungen tatsächlich verarbeiten. In den meisten Fällen sind der <emphasis>„up“-Satz</emphasis> und der <emphasis>„acting“-Satz</emphasis> praktisch identisch. Ist dies nicht der Fall, kann dies darauf hinweisen, dass Ceph gerade Daten migriert, ein OSD wiederhergestellt wird oder ein Problem vorliegt (in diesen Szenarien gibt Ceph in der Regel den Status <literal>HEALTH WARN</literal> mit der Meldung „stuck stale“ zurück).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Mit folgendem Kommando rufen Sie eine Liste der Platzierungsgruppen ab:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    Mit folgendem Kommando stellen Sie fest, welche OSDs sich im <emphasis>„acting“-Satz</emphasis> oder im <emphasis>„up“-Satz</emphasis> einer bestimmten Platzierungsgruppe befinden:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg map <replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -&gt; up [0,1,2] acting [0,1,2]
</screen>
   <para>
    Das Ergebnis umfasst die osdmap-Epoche (eNNN), die Nummer der Platzierungsgruppe (<replaceable>PG_NUM</replaceable>), die OSDs im <emphasis>„up“-Satz</emphasis> („up“) sowie die OSDs im <emphasis>„acting“-Satz</emphasis> („acting“):
   </para>
   <tip>
    <title>Hinweis auf Clusterprobleme</title>
    <para>
     Wenn der <emphasis>„up“-Satz</emphasis> und der <emphasis>„acting“-Satz</emphasis> nicht übereinstimmen, kann dies darauf hinweisen, dass der Cluster sich gerade ausgleicht oder dass ein potenzielles Problem mit dem Cluster vorliegt.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-peering">
   <title>Peering</title>
   <para>
    Bevor Sie Daten in eine Platzierungsgruppe schreiben können, muss sie den Status <literal>active</literal> und auch den Status <literal>clean</literal> aufweisen. Damit Ceph den aktuellen Status einer Fertigungsgruppe feststellen kann, führt der primäre OSD der Platzierungsgruppe (der erste OSD im <emphasis>„acting“-Satz</emphasis>) ein Peering mit dem sekundären und dem tertiären OSD durch, sodass sie sich über den aktuellen Status der Platzierungsgruppe abstimmen können (unter der Annahme eines Pools mit drei Reproduktionen der PG).
   </para>
   <figure>
    <title>Peering-Schema</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op-mon-pg-states">
   <title>Überwachen des Zustands von Platzierungsgruppen</title>
   <para>
    Wenn Sie ein Kommando wie <command>ceph health</command>, <command>ceph -s</command> oder <command>ceph -w</command> ausführen, gibt der Cluster nicht immer die Meldung <literal>HEALTH OK</literal> zurück. Sobald Sie geprüft haben, ob die OSDs aktiv sind, prüfen Sie auch den Status der Platzierungsgruppen.
   </para>
   <para>
    Unter verschiedenen, mit dem Platzierungsgruppen-Peering zusammenhängenden Umständen ist zu erwarten, dass der Cluster <emphasis role="bold">nicht</emphasis> die Meldung <literal>HEALTH OK</literal> zurückgibt:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Sie haben einen Pool erstellt, und die Platzierungsgruppen haben noch nicht das Peering durchgeführt.
     </para>
    </listitem>
    <listitem>
     <para>
      Die Platzierungsgruppen werden wiederhergestellt.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie haben einen OSD in einen Cluster eingefügt oder daraus entfernt.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie haben die CRUSH-Zuordnung geändert, und Ihre Platzierungsgruppen werden migriert.
     </para>
    </listitem>
    <listitem>
     <para>
      Die verschiedenen Reproduktionen einer Platzierungsgruppe enthalten inkonsistente Daten.
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph führt das Scrubbing der Reproduktionen einer Platzierungsgruppe durch.
     </para>
    </listitem>
    <listitem>
     <para>
      Die Speicherkapazität reicht nicht für die Ausgleichsoperationen von Ceph aus.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Wenn Ceph aus den obigen Gründen die Meldung <literal>HEALTH WARN</literal> ausgibt, ist dies kein Grund zur Sorge. In zahlreichen Fällen kann sich der Cluster selbst wiederherstellen. In bestimmten Fällen müssen Sie jedoch selbst gewisse Maßnahmen ergreifen. Bei der Überwachung der OSDs ist in jedem Fall darauf zu achten, dass alle Platzierungsgruppen den Status „active“ und nach Möglichkeit auch den Status „clean“ besitzen, wenn der Cluster selbst betriebsbereit und aktiv ist. Mit folgendem Kommando rufen Sie den Status aller Platzierungsgruppen ab:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    Das Ergebnis umfasst die Gesamtanzahl der Platzierungsgruppen (x), die Anzahl der Platzierungsgruppen in einem bestimmten Status, z. B. „active+clean“ (y) sowie die gespeicherte Datenmenge (z).
   </para>
   <para>
    Neben dem Status der Platzierungsgruppen gibt Ceph außerdem die belegte Speicherkapazität (aa), die verbleibende Speicherkapazität (bb) und die Gesamtspeicherkapazität für die Platzierungsgruppe zurück. Diese Zahlen sind in einigen Fällen von großer Bedeutung:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Sie nähern sich dem Wert für <option>near full ratio</option> oder <option>full ratio</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Die Daten werden aufgrund eines Fehlers in der CRUSH-Konfiguration nicht im Cluster verteilt.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Platzierungsgruppen-IDs</title>
    <para>
     Die Platzierungsgruppen-IDs bestehen aus der Pool-Nummer (nicht dem Pool-Namen), gefolgt von einem Punkt (.) und der eigentlichen Platzierungsgruppen-ID (eine hexadezimale Zahl). Die Pool-Nummern und zugehörige Namen sind in der Ausgabe des Kommandos <command>ceph osd lspools</command> ersichtlich. Der Standard-Pool <literal>rbd</literal> entspricht beispielsweise der Pool-Nummer 0. Eine vollständige Platzierungsgruppen-ID weist das folgende Format auf:
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     und sieht in der Regel wie folgt aus:
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    Mit folgendem Kommando rufen Sie eine Liste der Platzierungsgruppen ab:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    Sie können auch die Ausgabe im JSON-Format formatieren und in einer Datei speichern:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    Mit folgendem Kommando fragen Sie eine bestimmte Platzierungsgruppe ab:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    In der nachfolgenden Liste werden die gängigen Statusangaben für Platzierungsgruppen näher beschrieben.
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING</term>
     <listitem>
      <para>
       Beim Erstellen eines Pools wird die angegebene Anzahl der Platzierungsgruppen erstellt. Ceph gibt „creating“ zurück, während eine Platzierungsgruppe erstellt wird. Beim Erstellen führen die OSDs im <emphasis>„acting“-Satz</emphasis> der Platzierungsgruppe das Peering durch. Nach dem Peering sollte die Platzierungsgruppe den Status „active+clean“ aufweisen, sodass ein Ceph-Client in die Platzierungsgruppe schreiben kann.
      </para>
      <figure>
       <title>Statusangaben für Platzierungsgruppen</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING</term>
     <listitem>
      <para>
       Beim Peering einer Platzierungsgruppe stimmt Ceph die OSDs, auf denen die Reproduktionen der Platzierung gespeichert sind, hinsichtlich des Status der Objekte und Metadaten in der Platzierungsgruppe ab. Nach dem Peering zeigen also alle OSDs, auf denen die Platzierungsgruppe gespeichert ist, den aktuellen Status dieser Gruppe. Der Abschluss des Peering-Prozesses bedeutet jedoch <emphasis role="bold">nicht</emphasis>, dass jede Reproduktion den aktuellen Inhalt enthält.
      </para>
      <note>
       <title>Maßgeblicher Verlauf</title>
       <para>
        Ceph bestätigt <emphasis role="bold">nur dann</emphasis> einen Schreibvorgang auf einen Client, wenn alle OSDs im <emphasis>„acting“-Satz</emphasis> den Schreibvorgang dauerhaft speichern. Auf diese Weise ist sichergestellt, dass mindestens ein Mitglied des <emphasis>„acting“-Satzes</emphasis> einen Datensatz pro bestätigtem Schreibvorgang seit dem letzten erfolgreichen Peering-Vorgang besitzt.
       </para>
       <para>
        Anhand der sorgfältigen Aufzeichnungen der einzelnen bestätigten Schreiboperationen kann Ceph einen neuen maßgeblichen Verlauf der Platzierungsgruppe aufstellen und fortführen – also einen vollständigen, geordneten Satz von Operationen, mit dem das Exemplar einer Platzierungsgruppe, die sich auf einem OSD befindet, auf den neuesten Stand gebracht werden kann.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>AKTIV</term>
     <listitem>
      <para>
       Nach Abschluss des Peering-Prozesses kann eine Platzierungsgruppe den Status <literal>active</literal> erhalten. Im Status <literal>active</literal> stehen die Daten in der Platzierungsgruppe im Allgemeinen in der primären Platzierungsgruppe zur Verfügung, die Reproduktionen dagegen für Lese- und Schreiboperationen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN</term>
     <listitem>
      <para>
       Wenn eine Platzierungsgruppe den Status <literal>clean</literal> aufweist, haben der primäre OSD und die Reproduktions-OSDs das Peering erfolgreich beendet, und es gibt keine nicht zugeordneten Reproduktionen für die Platzierungsgruppe. Ceph hat alle Objekte in der Platzierungsgruppe so oft wie gefordert reproduziert.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Wenn ein Client ein Objekt in den primären OSD schreibt, ist der primäre OSD dafür zuständig, die Reproduktionen in die Reproduktions-OSDs zu schreiben. Sobald der primäre OSD das Objekt in den Speicher geschrieben hat, verbleibt die Platzierungsgruppe so lange im Status „degraded“, bis der primäre OSD eine Bestätigung von den Reproduktions-OSDs erhält, dass Ceph die Reproduktionsobjekte erfolgreich erstellt hat.
      </para>
      <para>
       Eine Platzierungsgruppe kann dann den Status „active+degraded“ aufweisen, wenn ein OSD den Status „active“ besitzt, obwohl er noch nicht alle 
Objekte enthält. Wenn ein OSD ausfällt, kennzeichnet Ceph die einzelnen Platzierungsgruppen, die diesem OSD zugewiesen sind, als „degraded“. Sobald der OSD wieder funktionsfähig ist, müssen die OSDs das Peering wiederholen. Wenn eine eingeschränkt leistungsfähige Platzierungsgruppe den Status „active“ aufweist, kann ein Client jedoch weiterhin in ein neues Objekt in dieser Gruppe schreiben.
      </para>
      <para>
       Wenn ein OSD den Status „down“ aufweist und die Bedingung „degraded“ weiterhin vorliegt, kennzeichnet Ceph den ausgefallenen OSD als außerhalb des Clusters („out“) und ordnet die Daten des „down“-OSDs einem anderen OSD zu. Der Zeitraum zwischen der Kennzeichnung als „down“ und der Kennzeichnung als „out“ wird durch die Option <option>mon osd down out interval</option> gesteuert (Standardwert 600 Sekunden).
      </para>
      <para>
       Eine Platzierungsgruppe kann auch dann den Status „degraded“ erhalten, wenn Ceph mindestens ein Objekt, das sich in der Platzierungsgruppe befinden sollte, nicht auffinden kann. Es ist zwar nicht möglich, in nicht gefundenen Objekten zu lesen oder zu schreiben, doch Sie können weiterhin auf alle anderen Objekte in der „degraded“-Platzierungsgruppe zugreifen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING</term>
     <listitem>
      <para>
       Ceph ist für die Fehlertoleranz in Situationen mit anhaltenden Hardware- und Softwareproblemen ausgelegt. Wenn ein OSD ausfällt („down“), bleibt sein Inhalt hinter dem Status anderer Reproduktionen in den Platzierungsgruppen zurück. Sobald der OSD wieder funktionsfähig ist („up“), muss der Inhalt der Platzierungsgruppen gemäß dem aktuellen Stand aktualisiert werden. In diesem Zeitraum befindet sich der OSD im Status „recovering“.
      </para>
      <para>
       Die Wiederherstellung ist nicht unbedingt trivial, da ein Hardwarefehler durchaus eine Kettenreaktion mit Ausfall mehrerer OSDs auslösen kann. Wenn beispielsweise ein Netzwerkschalter für ein Rack oder einen Schrank ausfällt, können die OSDs zahlreicher Hostmaschinen unter Umständen hinter den aktuellen Status des Clusters zurückfallen. Sobald der Fehler behoben wurde, muss jeder einzelne OSD wiederhergestellt werden.
      </para>
      <para>
       Ceph bietet verschiedene Einstellungen für den Ausgleich der Ressourcenkonflikte zwischen neuen Serviceanforderungen und der Notwendigkeit, Datenobjekte wiederherzustellen und die Platzierungsgruppen wieder auf den neuesten Stand zu bringen. Mit der Einstellung <option>osd recovery delay start</option>kann ein OSD vor Beginn des Wiederherstellungsprozesses neu starten, das Peering erneut durchführen und sogar einige Wiederholungsanforderungen verarbeiten. Die Einstellung <option>osd recovery thread timeout</option> legt eine Thread-Zeitüberschreitung fest, da mehrere OSDs gestaffelt ausfallen, neu starten und das Peering erneut durchführen können. Die Einstellung <option>osd recovery max active</option> beschränkt die Anzahl der Wiederherstellungsanforderungen, die ein OSD gleichzeitig verarbeitet, damit der OSD nicht ausfällt. Die Einstellung <option>osd recovery max chunk</option> beschränkt die Größe der wiederhergestellten Datenblöcke, sodass eine Überlastung des Netzwerks verhindert wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING</term>
     <listitem>
      <para>
       Wenn ein neuer OSD in den Cluster eintritt, weist CRUSH die Platzierungsgruppen von den OSDs im Cluster zum soeben hinzugefügten OSD zu. Wird die sofortige Annahme der neu zugewiesenen Platzierungsgruppen durch den neuen OSD erzwungen, kann dies den OSD immens belasten. Durch den Ausgleich des OSD mit den Platzierungsgruppen kann dieser Prozess im Hintergrund beginnen. Nach erfolgtem Ausgleich verarbeitet der neue OSD die ersten Anforderungen, sobald er dazu bereit ist.
      </para>
      <para>
       Während des Ausgleichs werden ggf. verschiedene Statusangaben angezeigt: „backfill_wait“, bedeutet, dass ein Ausgleichsvorgang aussteht, jedoch noch nicht gestartet wurde; „backfill“ bedeutet, dass ein Ausgleichsvorgang läuft; „backfill_too_full“ bedeutet, dass ein Ausgleichsvorgang angefordert wurde, jedoch aufgrund unzureichender Speicherkapazität nicht ausgeführt werden konnte. Wenn der Ausdruck einer Platzierungsgruppe nicht durchgeführt werden kann, gilt sie als „unvollständig“.
      </para>
      <para>
       Ceph bietet verschiedene Einstellungen für die Bewältigung der Belastung im Zusammenhang mit der Neuzuweisung von Placement Groups zu einem OSD (insbesondere zu einem neuen OSD). Standardmäßig beschränkt <option>osd max backfills</option> die Anzahl gleichzeitiger Ausgleichsvorgänge mit einem OSD auf maximal 10 Vorgänge. Mit <option>backfill full ratio</option> kann ein OSD eine Ausgleichsanforderung verweigern, wenn der OSD sich seiner „full“-Quote nähert (standardmäßig 90 %). Dieser Wert kann mit dem Kommando <command>ceph osd set-backfillfull-ratio</command> geändert werden. Wenn ein OSD eine Ausgleichsanforderung verweigert, kann der OSD die Anforderung mit <option>osd backfill retry interval</option> wiederholen (standardmäßig nach 10 Sekunden). Die OSDs können außerdem die Scanintervalle (standardmäßig 64 und 512) mit <option>osd backfill scan min</option> und <option>osd backfill scan max</option> verwalten.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED</term>
     <listitem>
      <para>
       Wenn sich der <emphasis>„acting“-Satz</emphasis> ändert, der für eine Platzierungsgruppe zuständig ist, werden die Daten vom bisherigen <emphasis>„acting“-Satz</emphasis> zum neuen <emphasis>„acting“-Satz</emphasis> migriert. Ein neuer primärer OSD kann unter Umständen erst nach einiger Zeit die ersten Anforderungen verarbeiten. In diesem Fall weist er den bisherigen primären OSD an, weiterhin die Anforderungen zu verarbeiten, bis die Migration der Platzierungsgruppe abgeschlossen ist. Nach erfolgter Datenmigration greift die Zuordnung auf den primären OSD des neuen <emphasis>„acting“-Satzes</emphasis> zurück.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>INAKTIV</term>
     <listitem>
      <para>
       Obwohl Ceph mithilfe von Heartbeats prüft, ob die Hosts und Daemons ausgeführt werden, können die <literal>ceph-osd</literal>-Daemons in den Status „stuck“ gelangen, in dem sie die Statistiken nicht zeitnah melden (z. B. bei einem vorübergehenden Netzwerkfehler). Standardmäßig melden die OSD-Daemons in Abständen von einer halben Sekunde (0,5) ihre Statistiken zu Platzierungsgruppen, Startvorgängen und Fehlern, also in deutlich kürzeren Abständen als die Heartbeat-Grenzwerte. Wenn der primäre OSD im <emphasis>„acting“-Satz</emphasis> keine Meldung an den Monitor übergibt oder wenn andere OSDs den primären OSD als „down“ gemeldet haben, kennzeichnen die Monitors die Platzierungsgruppe als „stale“.
      </para>
      <para>
       Beim Starten des Clusters wird häufig der Status „stale“ angezeigt, bis der Peering-Prozess abgeschlossen ist. Wenn der Cluster eine Weile aktiv war, weisen Platzierungsgruppen mit dem Status „stale“ darauf hin, dass der primäre OSD dieser Platzierungsgruppen ausgefallen ist oder keine Platzierungsgruppenstatistiken an den Monitor meldet.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-pg-objectfinding">
   <title>Auffinden des Speicherorts eines Objekts</title>
   <para>
    Zum Speichern von Objektdaten im Ceph Object Store muss ein Ceph-Client einen Objektnamen festlegen und einen zugehörigen Pool angeben. Der Ceph-Client ruft die aktuelle Cluster-Zuordnung ab. Der CRUSH-Algorithmus berechnet, wie das Objekt einer Platzierungsgruppe zugeordnet werden soll, und dann, wie die Platzierungsgruppe dynamisch einem OSD zugewiesen werden soll. Zum Auffinden des Objektspeicherorts benötigen Sie lediglich den Objektnamen und den Poolnamen. Beispiel:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>Auffinden eines Objekts</title>
    <para>
     Erstellen Sie für dieses Beispiel zunächst ein Objekt. Geben Sie den Objektnamen „test-object-1“, den Pfad zur Beispieldatei „testfile.txt“ mit einigen Objektdaten sowie den Poolnamen „data“ mit dem Kommando <command>rados put</command> in der Kommandozeile an:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     Prüfen Sie mit folgendem Kommando, ob das Objekt im Ceph Object Store gespeichert wurde:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados -p data ls
</screen>
    <para>
     Ermitteln Sie nun den Objektspeicherort. Ceph gibt den Speicherort des Objekts zurück:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     Löschen Sie das Beispielobjekt mit dem Kommando <command>rados rm</command>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
</chapter>
