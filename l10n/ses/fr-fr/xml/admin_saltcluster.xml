<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Tâches opérationnelles</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modification de la configuration d&apos;une grappe</title>

  <para>
   Pour modifier la configuration d&apos;une grappe Ceph existante, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Exportez la configuration actuelle de la grappe dans un fichier :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Modifiez le fichier de configuration et mettez à jour les lignes appropriées. Vous trouverez des exemples de spécification dans le <xref linkend="deploy-core"/> et à la <xref linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Appliquez la nouvelle configuration :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Ajout de noeuds</title>

  <para>
   Pour ajouter un noeud à une grappe Ceph, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Installez SUSE Linux Enterprise Server et SUSE Enterprise Storage sur le nouvel hôte. Reportez-vous au <xref linkend="deploy-sles"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d&apos;informations.
    </para>
   </step>
   <step>
    <para>
     Configurez l&apos;hôte en tant que minion Salt d&apos;un Salt Master préexistant. Reportez-vous au <xref linkend="deploy-salt"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d&apos;informations.
    </para>
   </step>
   <step>
    <para>
     Ajoutez le nouvel hôte à <systemitem class="resource">ceph-salt</systemitem> et informez-en cephadm, par exemple :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Reportez-vous au <xref linkend="deploy-cephadm-configure-minions"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d&apos;informations.
    </para>
   </step>
   <step>
    <para>
     Vérifiez que le noeud a bien été ajouté à <systemitem class="resource">ceph-salt</systemitem> :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Appliquez la configuration au nouvel hôte de la grappe :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Vérifiez que l&apos;hôte qui vient d&apos;être ajouté appartient désormais à l&apos;environnement cephadm :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Suppression de noeuds</title>

  <tip>
   <title>suppression des OSD</title>
   <para>
    Si le noeud que vous souhaitez supprimer exécute des OSD, commencez par supprimer ces derniers et vérifiez qu&apos;aucun OSD ne s&apos;exécute sur ce noeud. Pour plus d&apos;informations sur la suppression des OSD, reportez-vous à la <xref linkend="removing-node-osds"/>.
   </para>
  </tip>

  <para>
   Pour supprimer un noeud d&apos;une grappe, procédez comme suit :
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Pour tous les types de service Ceph, à l&apos;exception de <literal>node-exporter</literal> et <literal>crash</literal>, supprimez le nom d&apos;hôte du noeud du fichier de spécification de placement de la grappe (par exemple, <filename>cluster.yml</filename>). Pour plus d&apos;informations, reportez-vous au <xref linkend="cephadm-service-and-placement-specs"/>. Par exemple, si vous souhaitez supprimer l&apos;hôte nommé <literal>ses-min2</literal>, supprimez toutes les occurrences de <literal>- ses-min2</literal> de toutes les sections <literal>placement:</literal> :
    </para>
    <para>
     Remplacez
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     par
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Appliquez vos modifications au fichier de configuration :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Supprimez le noeud de l&apos;environnement de cephadm :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Si le noeud exécute les services <literal>crash.osd.1</literal> et <literal>crash.osd.2</literal>, supprimez-les en exécutant la commande suivante sur l&apos;hôte :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Par exemple :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Supprimez tous les rôles du minion à supprimer :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Si le minion que vous souhaitez supprimer est le minion Bootstrap, vous devez également supprimer le rôle Bootstrap :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     Après avoir supprimé tous les OSD sur un hôte unique, supprimez ce dernier de la carte CRUSH :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      Le nom du compartiment doit être identique au nom d&apos;hôte.
     </para>
    </note>
   </step>
   <step>
    <para>
     Vous pouvez à présent supprimer le minion de la grappe :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    En cas d&apos;échec et si le minion que vous essayez de supprimer se trouve dans un état de désactivation permanente, vous devrez supprimer le noeud du Salt Master :
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    Supprimez ensuite manuellement le noeud du fichier <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>. Celui-ci se trouve généralement dans <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>Gestion des OSD</title>

  <para>
   Cette section explique comment ajouter, effacer ou supprimer des OSD dans une grappe Ceph.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Liste des périphériques de disque</title>
   <para>
    Pour identifier les périphériques de disque utilisés et inutilisés sur tous les noeuds de la grappe, listez-les en exécutant la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Effacement de périphériques de disque</title>
   <para>
    Pour réutiliser un périphérique de disque, vous devez d&apos;abord l&apos;effacer <emphasis></emphasis>:
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Si vous avez déjà déployé des OSD à l&apos;aide de DriveGroups ou de l&apos;option <option>--all-available-devices</option> alors que l&apos;indicateur <literal>unmanaged</literal> n&apos;était pas défini, cephadm déploiera automatiquement ces OSD une fois que vous les aurez effacés.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Ajout d&apos;OSD à l&apos;aide de la spécification DriveGroups</title>
   <para>
    Les <emphasis>groupes d&apos;unités</emphasis> (« DriveGroups ») spécifient les dispositions des OSD dans la grappe Ceph. Ils sont définis dans un fichier YAML unique. Dans cette section, nous utiliserons le fichier <filename>drive_groups.yml</filename> comme exemple.
   </para>
   <para>
    Un administrateur doit spécifier manuellement un groupe d&apos;OSD interdépendants (OSD hybrides déployés sur un mélange de disques durs et SSD) ou qui partagent des options de déploiement identiques (par exemple, même magasin d&apos;objets, même option de chiffrement, OSD autonomes). Pour éviter de lister explicitement les périphériques, les groupes d&apos;unités utilisent une liste d&apos;éléments de filtre qui correspondent à quelques champs sélectionnés de rapports d&apos;inventaire de <command>ceph-volume</command>. cephadm fournit le code qui traduit ces DriveGroups en listes de périphériques réelles pour inspection par l&apos;utilisateur.
   </para>
   <para>
    La commande permettant d&apos;appliquer la spécification d&apos;OSD à la grappe est la suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Pour afficher un aperçu des opérations et tester votre application, vous pouvez utiliser l&apos;option <option>--dry-run</option> en combinaison avec la commande <command>ceph orch apply osd</command>. Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Si la sortie de l&apos;option <option>--dry-run</option> correspond à vos attentes, réexécutez simplement la commande sans l&apos;option <option>--dry-run</option>.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>OSD non gérés</title>
    <para>
     Tous les périphériques de disque propres disponibles correspondant à la spécification DriveGroups sont automatiquement utilisés comme OSD une fois que vous les avez ajoutés à la grappe. Ce comportement est appelé mode <emphasis>managed</emphasis> (géré).
    </para>
    <para>
     Pour désactiver le mode <emphasis>managed</emphasis>, ajoutez la ligne <literal>unmanaged: true</literal> aux spécifications appropriées, par exemple :
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Pour faire passer des OSD déjà déployés du mode <emphasis>managed</emphasis> au mode <emphasis>unmanaged</emphasis>, ajoutez les lignes <literal>unmanaged: true</literal>, le cas échéant, au cours de la procédure décrite à la <xref linkend="modifying-cluster-configuration"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Spécification DriveGroups</title>
    <para>
     Voici un exemple de fichier de spécification DriveGroups :
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
    <note>
     <para>
      L&apos;option précédemment appelée « encryption » (chiffrement) dans DeepSea a été renommée « encrypted » (chiffré). Lorsque vous appliquez des DriveGroups dans SUSE Enterprise Storage 7, veillez à utiliser cette nouvelle terminologie dans votre spécification de services, sinon l&apos;opération <command>ceph orch apply</command> échouera.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Périphériques de disque correspondants</title>
    <para>
     Vous pouvez décrire les spécifications à l&apos;aide des filtres suivants :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Par modèle de disque :
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Par fournisseur de disque :
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Saisissez toujours <replaceable>DISK_VENDOR_STRING</replaceable> en minuscules.
       </para>
      </tip>
      <para>
       Pour obtenir des informations sur le modèle et le fournisseur du disque, examinez la sortie de la commande suivante :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Selon qu&apos;un disque est rotatif ou non. Les disques SSD et NVMe ne sont pas rotatifs.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Déployez un noeud à l&apos;aide de <emphasis>tous</emphasis> les disques disponibles pour les OSD :
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       En outre, vous pouvez limiter le nombre de disques correspondants :
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Filtrage des périphériques par taille</title>
    <para>
     Vous pouvez filtrer les périphériques de disque par leur taille, soit en fonction d&apos;une taille précise, soit selon une plage de tailles. Le paramètre <option>size:</option> (taille :) accepte les arguments sous la forme suivante :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       « 10G » : inclut les disques d&apos;une taille exacte.
      </para>
     </listitem>
     <listitem>
      <para>
       « 10G:40G » : inclut les disques dont la taille est dans la plage.
      </para>
     </listitem>
     <listitem>
      <para>
       « :10G » : inclut les disques dont la taille est inférieure ou égale à 10 Go.
      </para>
     </listitem>
     <listitem>
      <para>
       « 40G: » : inclut les disques dont la taille est égale ou supérieure à 40 Go.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Correspondance par taille de disque</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>guillemets requis</title>
     <para>
      Lorsque vous utilisez le délimiteur « : », vous devez entourer la taille par des guillemets simples, faute de quoi le signe deux-points est interprété comme un nouveau hachage de configuration.
     </para>
    </note>
    <tip>
     <title>abréviations des unités</title>
     <para>
      Au lieu d&apos;indiquer les tailles en gigaoctets (G), vous pouvez les spécifier en mégaoctets (M) ou téraoctets (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Exemples de DriveGroups</title>
    <para>
     Cette section comprend des exemples de différentes configurations OSD.
    </para>
    <example>
     <title>Configuration simple</title>
     <para>
      Cet exemple décrit deux noeuds avec la même configuration :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Le fichier <filename>drive_groups.yml</filename> correspondant se présentera comme suit :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Une telle configuration est simple et valide. Le problème est qu&apos;un administrateur peut ajouter des disques de fournisseurs différents par la suite et ceux-ci ne seront pas inclus. Vous pouvez améliorer cela en limitant les filtres aux propriétés de base des unités :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      Dans l&apos;exemple précédent, nous imposons de déclarer tous les périphériques rotatifs comme « périphériques de données » et tous les périphériques non rotatifs seront utilisés comme « périphériques partagés » (wal, db).
     </para>
     <para>
      Si vous savez que les unités de plus de 2 To seront toujours les périphériques de données plus lents, vous pouvez filtrer par taille :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Configuration avancée</title>
     <para>
      Cet exemple décrit deux configurations distinctes : 20 HDD devraient partager 2 SSD, tandis que 10 SSD devraient partager 2 NVMe.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Une telle configuration peut être définie avec deux dispositions comme suit :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Configuration avancée avec des noeuds non uniformes</title>
     <para>
      Les exemples précédents supposaient que tous les noeuds avaient les mêmes unités. Cependant, ce n&apos;est pas toujours le cas :
     </para>
     <para>
      Noeuds 1 à 5 :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Noeuds 6 à 10 :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Vous pouvez utiliser la clé « target » dans la disposition pour cibler des noeuds spécifiques. La notation de cible Salt aide à garder les choses simples :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      suivi de
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Configuration experte</title>
     <para>
      Tous les cas précédents supposaient que les WAL et les DB utilisaient le même périphérique. Il est cependant possible également de déployer le WAL sur un périphérique dédié :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Configuration complexe (et peu probable)</title>
     <para>
      Dans la configuration suivante, nous essayons de définir :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD soutenus par 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDD soutenus par 1 SSD (db) et 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSD soutenus par 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSD autonomes (chiffrés)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD est de rechange et ne doit pas être déployé
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Le résumé des unités utilisées est le suivant :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      La définition des groupes d&apos;unités sera la suivante :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Il reste un HDD dans la mesure où le fichier est en cours d&apos;analyse du haut vers le bas.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Suppression des OSD</title>
   <para>
    Avant de supprimer un noeud OSD de la grappe, vérifiez que cette dernière dispose de plus d&apos;espace disque disponible que le disque OSD que vous allez supprimer. Gardez à l&apos;esprit que la suppression d&apos;un OSD entraîne un rééquilibrage de l&apos;ensemble de la grappe.
   </para>
   <procedure>
    <step>
     <para>
      Identifiez l&apos;OSD à supprimer en obtenant son ID :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Supprimez un ou plusieurs OSD de la grappe :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Par exemple :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      Vous pouvez exécuter une requête pour connaître l&apos;état de l&apos;opération de suppression :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Arrêt de la suppression d&apos;un OSD</title>
    <para>
     Après avoir planifié la suppression d&apos;un OSD, vous pouvez interrompre l&apos;opération si nécessaire. La commande suivante permet de rétablir l&apos;état initial de l&apos;OSD et de le supprimer de la file d&apos;attente :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Remplacement d&apos;OSD</title>
   <para>
    Plusieurs raisons peuvent vous pousser à remplacer un disque OSD. Par exemple :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Le disque OSD est défaillant ou, d&apos;après les informations SMART, sera bientôt défaillant et ne peut plus être utilisé pour stocker des données en toute sécurité.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous devez mettre à niveau le disque OSD, par exemple pour augmenter sa taille.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous devez modifier la disposition du disque OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous envisagez de passer d&apos;une disposition non-LVM à une disposition LVM.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Pour remplacer un OSD tout en conservant son ID, exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    Le remplacement d&apos;un OSD est identique à la suppression d&apos;un OSD (pour plus de détails, reportez-vous à la <xref linkend="removing-node-osds"/>) à la différence près que l&apos;OSD n&apos;est pas supprimé de façon permanente de la hiérarchie CRUSH et se voit assigner un indicateur <literal>destroyed</literal> (détruit).
   </para>
   <para>
    L&apos;indicateur <literal>destroyed</literal> (détruit) sert à déterminer les ID d&apos;OSD qui seront réutilisés lors du prochain déploiement d&apos;OSD. Les nouveaux disques ajoutés qui correspondent à la spécification DriveGroups (pour plus de détails, reportez-vous à la <xref linkend="drive-groups"/>) se verront assigner les ID d&apos;OSD de leur homologue remplacé.
   </para>
   <tip>
    <para>
     L&apos;ajout de l&apos;option <option>--dry-run</option> ne permet pas d&apos;exécuter le remplacement réel, mais d&apos;afficher un aperçu des étapes qui se produiraient normalement.
    </para>
   </tip>
   <note>
    <para>
     En cas de remplacement d&apos;un OSD à la suite d&apos;un échec, nous vous recommandons vivement de déclencher un nettoyage en profondeur des groupes de placement. Pour plus d&apos;informations, reportez-vous au <xref linkend="scrubbing-pgs"/>. 
    </para>
    <para>
     Exécutez la commande suivante pour lancer un nettoyage en profondeur :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd deep-scrub osd.<replaceable>OSD_NUMBER</replaceable></screen>
   </note>
   <important>
    <title>échec du périphérique partagé</title>
    <para>
     En cas d&apos;échec d&apos;un périphérique partagé pour DB/WAL, vous devez effectuer la procédure de remplacement pour tous les OSD qui partagent le périphérique ayant échoué.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Déplacement du Salt Master vers un nouveau noeud</title>

  <para>
   Si vous devez remplacer l&apos;hôte Salt Master par un autre, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Exportez la configuration de la grappe et sauvegardez le fichier JSON exporté. Pour plus de détails, reportez-vous au <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     Si l&apos;ancien Salt Master est également le seul noeud d&apos;administration de la grappe, déplacez manuellement les fichiers <filename>/etc/ceph/ceph.client.admin.keyring</filename> et <filename>/etc/ceph/ceph.conf</filename> vers le nouveau Salt Master.
    </para>
   </step>
   <step>
    <para>
     Arrêtez et désactivez le service <systemitem class="daemon">systemd</systemitem> Salt Master sur l&apos;ancien noeud Salt Master :
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Si l&apos;ancien noeud Salt Master ne se trouve plus dans la grappe, arrêtez et désactivez également le service <systemitem class="daemon">systemd</systemitem> du minion Salt :
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      N&apos;arrêtez ou ne désactivez pas <literal>salt-minion.service</literal> si des daemons Ceph (MON, MGR, OSD, MDS, passerelle, surveillance) s&apos;exécutent sur l&apos;ancien noeud Salt Master.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Installez SUSE Linux Enterprise Server 15 SP3 sur le nouveau Salt Master en suivant la procédure décrite dans le <xref linkend="deploy-sles"/>.
    </para>
    <tip>
     <title>transition des minions Salt</title>
     <para>
      Pour simplifier la transition des minions Salt vers le nouveau Salt Master, retirez la clé publique Salt Master d&apos;origine de chacun d&apos;eux :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Installez le paquetage <package>salt-master</package> et, le cas échéant, le paquetage <package>salt-minion</package> sur le nouveau Salt Master.
    </para>
   </step>
   <step>
    <para>
     Installez <systemitem class="resource">ceph-salt</systemitem> sur le nouveau noeud Salt Master :
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Veillez à exécuter les trois commandes avant de continuer. Les commandes sont idempotentes ; peu importe si elles sont exécutées à plusieurs reprises.
     </para>
    </important>
   </step>
   <step>
    <para>
     Incluez le nouveau Salt Master dans la grappe comme décrit dans les <xref linkend="deploy-cephadm-cephsalt"/>, <xref linkend="deploy-cephadm-configure-minions"/> et <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Importez la configuration de grappe sauvegardée et appliquez-la :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Renommez le <literal>minion id</literal> du Salt Master dans le fichier <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> exporté avant de l&apos;importer.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Mise à jour des noeuds de grappe</title>

  <para>
   Gardez les noeuds de grappe Ceph à jour en appliquant régulièrement des mises à jour progressives.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Dépôts logiciels</title>
   <para>
    Avant d&apos;appliquer des correctifs à la grappe avec les paquetages les plus récents, vérifiez que tous les noeuds de la grappe ont accès aux dépôts pertinents. Pour obtenir la liste complète des dépôts requis, reportez-vous au <xref linkend="verify-previous-upgrade-patch-repos-repos"/>.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Préparation du dépôt</title>
   <para>
    Si vous utilisez un outil de préparation (SUSE Manager, Subscription Management Tool ou RMT, par exemple) qui met à disposition des dépôts logiciels pour les noeuds de la grappe, vérifiez que les phases pour les dépôts de mise à jour de SUSE Linux Enterprise Server et de SUSE Enterprise Storage sont créées au même moment.
   </para>
   <para>
    Il est vivement recommandé d&apos;utiliser un outil de préparation pour appliquer des correctifs de niveau <literal>frozen</literal> ou <literal>staged</literal>. Cela garantit le même niveau de correctif aux noeuds qui rejoignent la grappe et à ceux qui y sont déjà en cours d&apos;exécution. Vous évitez ainsi de devoir appliquer les correctifs les plus récents à tous les noeuds de la grappe avant que de nouveaux noeuds puissent la rejoindre.
   </para>
  </sect2>

  <sect2>
   <title>Temps d&apos;indisponibilité des services Ceph</title>
   <para>
    Selon la configuration, les noeuds de grappe peuvent être redémarrés pendant la mise à jour. S&apos;il existe un point d&apos;échec unique pour des services tels qu&apos;Object Gateway, Samba Gateway, NFS Ganesha ou iSCSI, les machines clientes peuvent être temporairement déconnectées des services dont les noeuds sont redémarrés.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Exécution de la mise à jour</title>
   <para>
    Pour mettre à jour les paquetages logiciels sur tous les noeuds de grappe vers la dernière version, exécutez la commande suivante :
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Mise à jour de Ceph</title>

  <para>
   Vous pouvez demander à cephadm de mettre à jour Ceph d&apos;une version de correctifs vers une autre. La mise à jour automatique des services Ceph respecte l&apos;ordre recommandé : elle commence par les instances Ceph Manager, Ceph Monitor, puis continue avec d&apos;autres services tels que les OSD Ceph et les instances Metadata Server et Object Gateway. Chaque daemon est redémarré uniquement après que Ceph indique que la grappe restera disponible.
  </para>

  <note>
   <para>
    La procédure de mise à jour ci-dessous utilise la commande <command>ceph orch upgrade</command>. Gardez à l&apos;esprit que les instructions suivantes expliquent comment mettre à jour votre grappe Ceph avec une version de produit (par exemple, une mise à jour de maintenance), et <emphasis>non</emphasis> comment mettre à niveau votre grappe d&apos;une version de produit à une autre.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Démarrage de la mise à jour</title>
   <para>
    Avant de démarrer la mise à jour, vérifiez que tous les noeuds sont en ligne et que votre grappe est saine :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    Pour effectuer une mise à jour vers une version spécifique de Ceph :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7.1/ceph/ceph:latest</screen>
   <para>
    Mettez à niveau les paquetages sur les hôtes :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Surveillance de la mise à jour</title>
   <para>
    Exécutez la commande suivante pour déterminer si une mise à jour est en cours :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Pendant l&apos;exécution de la mise à jour, vous verrez une barre de progression dans la sortie d&apos;état de Ceph :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7.1/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    Vous pouvez également consulter le journal cephadm :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Annulation d&apos;une mise à jour</title>
   <para>
    Vous pouvez arrêter le processus de mise à jour à tout moment :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Arrêt ou redémarrage de la grappe</title>

  <para>
   Dans certains cas, il faudra peut-être arrêter ou redémarrer l&apos;ensemble de la grappe. Nous vous recommandons de contrôler soigneusement les dépendances des services en cours d&apos;exécution. Les étapes suivantes fournissent un aperçu de l&apos;arrêt et du démarrage de la grappe :
  </para>

  <procedure>
   <step>
    <para>
     Ordonnez à la grappe Ceph de ne pas marquer les OSD comme étant hors service :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Arrêtez les daemons et les noeuds dans l&apos;ordre suivant :
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clients de stockage
      </para>
     </listitem>
     <listitem>
      <para>
       Passerelles, par exemple NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Serveur de métadonnées
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Si nécessaire, effectuez des tâches de maintenance.
    </para>
   </step>
   <step>
    <para>
     Démarrez les noeuds et les serveurs dans l&apos;ordre inverse du processus d&apos;arrêt :
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Serveur de métadonnées
      </para>
     </listitem>
     <listitem>
      <para>
       Passerelles, par exemple NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Clients de stockage
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Supprimez l&apos;indicateur noout :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Suppression d&apos;une grappe Ceph entière</title>

  <para>
   La commande <command>ceph-salt purge</command> permet de supprimer l&apos;intégralité de la grappe Ceph. Si d&apos;autres grappes Ceph sont déployées, celle spécifiée par <command>ceph -s</command> est purgée. De cette façon, vous pouvez nettoyer l&apos;environnement de grappe lors du test de différentes configurations.
  </para>

  <para>
   Pour éviter toute suppression accidentelle, l&apos;orchestration vérifie si la sécurité est désengagée. Vous pouvez désengager les mesures de sécurité et supprimer la grappe Ceph en exécutant les commandes suivantes :
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
