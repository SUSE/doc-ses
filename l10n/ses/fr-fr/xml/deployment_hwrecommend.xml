<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>Configuration matérielle requise et recommandations</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  La configuration matérielle requise de Ceph dépend fortement du workload des E/S. La configuration matérielle requise et les recommandations suivantes doivent être considérées comme un point de départ de la planification détaillée.
 </para>
 <para>
  En général, les recommandations données dans cette section dépendent du processus. Si plusieurs processus sont situés sur la même machine, les besoins en UC, RAM, disque et réseau doivent être additionnés.
 </para>
 <sect1 xml:id="network-overview">
  <title>Aperçu du réseau</title>

  <para>
   Ceph compte plusieurs réseaux logiques :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Un réseau d&apos;interface client appelé <literal>réseau public</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Un réseau interne approuvé, le réseau back-end, appelé <literal>réseau de grappes</literal>. Cette option est facultative.
    </para>
   </listitem>
   <listitem>
    <para>
     Un ou plusieurs réseaux client pour les passerelles. Ceci est facultatif et sort du cadre de ce chapitre.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Le réseau public est le réseau sur lequel les daemons Ceph communiquent entre eux et avec leurs clients. Cela signifie que tout le trafic de la grappe Ceph passe par ce réseau, sauf dans le cas où un réseau de grappe est configuré.
  </para>

  <para>
   Le réseau de grappe est le réseau back-end entre les noeuds OSD, pour la réplication, le rééquilibrage et la récupération. Lorsqu&apos;il est configuré, ce réseau facultatif devrait idéalement fournir deux fois la bande passante du réseau public avec la réplication tripartite par défaut, étant donné que l&apos;OSD primaire envoie deux copies aux autres OSD par le biais de ce réseau. Le réseau public est situé entre les clients et les passerelles d&apos;un côté pour communiquer avec les moniteurs, les gestionnaires ainsi qu&apos;avec les noeuds MDS et OSD. Il est également utilisé par les moniteurs, les gestionnaires et les noeuds MDS pour communiquer avec les noeuds OSD.
  </para>

  <figure xml:id="network-overview-figure">
   <title>Aperçu du réseau</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="network-overview-diagram.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="ceph-install-ceph-deploy-network">
   <title>Recommandations concernant le réseau</title>
   <para>
    Nous vous recommandons d&apos;utiliser un seul réseau tolérant aux pannes avec une bande passante suffisante pour répondre à vos besoins. Pour l&apos;environnement de réseau public Ceph, nous recommandons deux interfaces réseau 25 GbE (ou plus rapides) liées à l&apos;aide de 802.3ad (LACP). Cette recommandation est considérée comme la configuration minimale pour Ceph. Si vous utilisez également un réseau en grappe, nous recommandons quatre interfaces réseau 25 GbE liées. La liaison de deux ou plusieurs interfaces réseau offre un meilleur débit grâce à l&apos;agrégation de liens et, compte tenu des liens et commutateurs redondants, une meilleure tolérance aux pannes et une meilleure maintenabilité.
   </para>
   <para>
    Vous pouvez également créer des VLAN pour isoler différents types de trafic sur une liaison. Par exemple, vous pouvez créer une liaison pour fournir deux interfaces VLAN, l&apos;une pour le réseau public et l&apos;autre pour le réseau en grappe. Ce n&apos;est toutefois <emphasis>pas</emphasis> nécessaire lors de la configuration du réseau Ceph. Vous trouverez des détails sur la liaison des interfaces sur le site <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-network.html#sec-network-iface-bonding"/>.
   </para>
   <para>
    La tolérance aux pannes peut être améliorée en isolant les composants dans des domaines de défaillance. Pour améliorer la tolérance aux pannes du réseau, la liaison d&apos;une interface à partir de deux cartes d&apos;interface réseau (NIC) distinctes offre une protection contre la défaillance d&apos;une seule carte réseau. De même, la création d&apos;une liaison entre deux commutateurs protège contre la défaillance d&apos;un commutateur. Nous vous recommandons de vous adresser au fournisseur de l&apos;équipement réseau afin de définir le niveau de tolérance aux pannes requis.
   </para>
   <important>
    <title>réseau d&apos;administration non pris en charge</title>
    <para>
     La configuration réseau d&apos;administration supplémentaire (qui permet, par exemple, de séparer les réseaux SSH, Salt ou DNS) n&apos;est ni testée ni prise en charge.
    </para>
   </important>
   <tip>
    <title>noeuds configurés via DHCP</title>
    <para>
     Si les noeuds de stockage sont configurés via DHCP, les timeouts par défaut peuvent ne pas être suffisants pour que le réseau soit configuré correctement avant le démarrage des différents daemons Ceph. Si cela se produit, les instances MON et OSD de Ceph ne démarreront pas correctement (l&apos;exécution de <command>systemctl status ceph\*</command> entraînera des erreurs de type « unable to bind » [liaison impossible]). Pour éviter ce problème, nous vous recommandons d&apos;augmenter le timeout du client DHCP sur au moins 30 secondes pour chaque noeud de votre grappe de stockage. Pour ce faire, vous pouvez modifier les paramètres suivants sur chaque noeud :
    </para>
    <para>
     Dans <filename>/etc/sysconfig/network/dhcp</filename>, définissez
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     Dans <filename>/etc/sysconfig/network/config</filename>, définissez
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
   <sect3 xml:id="storage-bp-net-private">
    <title>Ajout d&apos;un réseau privé à une grappe en cours d&apos;exécution</title>
    <para>
     Si vous ne spécifiez pas de réseau pour la grappe lors du déploiement de Ceph, il suppose un environnement de réseau public unique. Bien que Ceph fonctionne parfaitement avec un réseau public, ses performances et sa sécurité s&apos;améliorent lorsque vous définissez un second réseau de grappe privé. Pour prendre en charge deux réseaux, chaque noeud Ceph doit disposer d&apos;au moins deux cartes réseau.
    </para>
    <para>
     Vous devez apporter les modifications suivantes à chaque noeud Ceph. Ces modifications sont relativement rapides à apporter à une petite grappe, mais peuvent prendre beaucoup de temps si votre grappe est composée de centaines ou de milliers de noeuds.
    </para>
    <procedure>
     <step>
      <para>
       Définissez le réseau de la grappe à l&apos;aide de la commande suivante :
      </para>
<screen><prompt role="root"># </prompt>ceph config set global cluster_network <replaceable>MY_NETWORK</replaceable></screen>
      <para>
       Redémarrez les OSD pour qu&apos;ils se lient au réseau de grappe spécifié :
      </para>
<screen><prompt role="root"># </prompt>systemctl restart ceph-*@osd&#x002E;*&#x002E;service</screen>
     </step>
     <step>
      <para>
       Vérifiez que le réseau privé de la grappe fonctionne comme prévu au niveau du système d&apos;exploitation.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="storage-bp-net-subnets">
    <title>Noeuds de moniteur sur des sous-réseaux distincts</title>
    <para>
     Si les noeuds de moniteur se trouvent sur plusieurs sous-réseaux, par exemple s&apos;ils se trouvent dans des pièces différentes et sont desservis par différents commutateurs, vous devez spécifier leur adresse de réseau public en notation CIDR :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon public_network "<replaceable>MON_NETWORK_1</replaceable>, <replaceable>MON_NETWORK_2</replaceable>, <replaceable>MON_NETWORK_N</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon public_network "192.168.1.0/24, 10.10.0.0/16"</screen>
    <warning>
     <para>
      Si vous spécifiez plusieurs segments de réseau pour le réseau public (ou en grappe) comme décrit dans cette section, chacun de ces sous-réseaux doit pouvoir être routé vers tous les autres. Dans le cas contraire, les daemons MON et les autres daemons Ceph sur différents segments de réseau ne peuvent pas communiquer et il en résulte une grappe divisée. De plus, si vous utilisez un pare-feu, veillez à inclure chaque sous-réseau ou adresse IP dans vos iptables et ouvrez leur les ports nécessaires sur tous les noeuds.
     </para>
    </warning>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="multi-architecture">
  <title>Configurations d&apos;architecture multiples</title>

  <para>
   SUSE Enterprise Storage prend en charge les architectures x86 et Arm. Si nous considérons chaque architecture, il est important de noter que d&apos;un point de vue du nombre de coeurs par OSD, de la fréquence et de la mémoire RAM, il n&apos;y a pas de différence réelle entre les architectures de processeurs en termes de dimensionnement.
  </para>

  <para>
   Comme pour les processeurs x86 plus petits (non-serveurs), les coeurs basés sur Arm moins performants peuvent ne pas fournir une expérience optimale, en particulier lorsqu&apos;ils sont utilisés pour des réserves codées à effacement.
  </para>

  <note>
   <para>
    Dans toute la documentation, <replaceable>SYSTEM-ARCH</replaceable> est utilisé à la place de x86 ou Arm.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="ses-hardware-config">
  <title>Configuration du matériel</title>

  <para>
   Pour bénéficier d&apos;une expérience optimale avec le produit, nous vous recommandons de commencer par configurer la grappe recommandée. Pour une grappe de test ou une grappe ne nécessitant pas de performances très importantes, nous documentons une configuration minimale prise en charge pour la grappe.
  </para>

  <sect2 xml:id="ses-bp-minimum-cluster">
   <title>Configuration minimale de la grappe</title>
   <para>
    Une configuration de grappe minimale pour le produit se compose des éléments suivants :
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Au moins quatre noeuds physiques (noeuds OSD) avec cohabitation des services
     </para>
    </listitem>
    <listitem>
     <para>
      Ethernet Dual-10 Go en tant que réseau lié
     </para>
    </listitem>
    <listitem>
     <para>
      Un noeud Admin distinct (peut être virtualisé sur un noeud externe)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Une configuration détaillée est la suivante :
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Un noeud Admin distinct avec 4 Go de RAM, quatre coeurs et 1 To de capacité de stockage. Il s&apos;agit généralement du noeud Salt Master. Les passerelles et services Ceph et de passerelles, tels que Ceph Monitor, Metadata Server, Ceph OSD, Object Gateway ou NFS Ganesha ne sont pas pris en charge sur le noeud Admin, car il doit orchestrer indépendamment les processus de mise à jour et de mise à niveau de la grappe.
     </para>
    </listitem>
    <listitem>
     <para>
      Au moins quatre noeuds OSD physiques, avec huit disques OSD chacun, reportez-vous à la <xref linkend="sysreq-osd"/> pour consulter la configuration requise.
     </para>
     <para>
      La capacité totale de la grappe doit être dimensionnée de sorte que, même si un noeud est indisponible, la capacité totale utilisée (y compris la redondance) ne dépasse pas 80 %.
     </para>
    </listitem>
    <listitem>
     <para>
      Trois instances de Ceph Monitor. Pour des raisons de latence, les moniteurs doivent être exécutés à partir d&apos;un espace de stockage SSD/NVMe et non à partir de disques durs.
     </para>
    </listitem>
    <listitem>
     <para>
      Les moniteurs, le serveur de métadonnées et les passerelles peuvent cohabiter sur les noeuds OSD. Reportez-vous à la <xref linkend="ses-bp-diskshare"/> concernant la cohabitation des moniteurs. Si vous faites cohabiter des services, les exigences de mémoire et d&apos;UC doivent être additionnées.
     </para>
    </listitem>
    <listitem>
     <para>
      La passerelle iSCSI, Object Gateway et le serveur de métadonnées ont au minimum besoin de 4 Go de RAM incrémentielle et de quatre coeurs.
     </para>
    </listitem>
    <listitem>
     <para>
      Si vous utilisez CephFS, S3/Swift, iSCSI, au moins deux instances des rôles respectifs (serveur de métadonnées, Object Gateway, iSCSI) sont requises à des fins de redondance et de disponibilité.
     </para>
    </listitem>
    <listitem>
     <para>
      Les noeuds doivent être dédiés à SUSE Enterprise Storage et ne doivent pas être utilisés pour d&apos;autres workloads physiques, conteneurisés ou virtualisés.
     </para>
    </listitem>
    <listitem>
     <para>
      Si l&apos;une des passerelles (iSCSI, Object Gateway, NFS Ganesha, serveur de métadonnées, etc.) est déployée au sein de machines virtuelles, ces dernières ne doivent pas être hébergées sur les machines physiques servant d&apos;autres rôles de grappe. (Ce n&apos;est pas nécessaire puisqu&apos;elles sont prises en charge en tant que services cohabitants.)
     </para>
    </listitem>
    <listitem>
     <para>
      Lors du déploiement de services en tant que machines virtuelles sur des hyperviseurs en dehors de la grappe physique principale, les domaines de défaillance doivent être respectés afin de garantir la redondance.
     </para>
     <para>
      Par exemple, ne déployez pas plusieurs rôles du même type sur le même hyperviseur, tels que plusieurs instances MON ou MDS.
     </para>
    </listitem>
    <listitem>
     <para>
      Lors d&apos;un déploiement sur des machines virtuelles, il est particulièrement important de s&apos;assurer que les noeuds disposent d&apos;une solide connectivité réseau et d&apos;une synchronisation horaire fonctionnant convenablement.
     </para>
    </listitem>
    <listitem>
     <para>
      Les noeuds d&apos;hyperviseur doivent avoir la bonne taille pour éviter les interférences occasionnées par d&apos;autres workloads consommant des ressources de processeur, de RAM, de réseau et de stockage.
     </para>
    </listitem>
   </itemizedlist>
   <figure>
    <title>Configuration minimale de la grappe</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="ses-bp-production-cluster">
   <title>Configuration recommandée pour une grappe en production</title>
   <para>
    Une fois votre grappe développée, nous vous recommandons de déplacer les instances Ceph Monitor, les serveurs de métadonnées et les passerelles vers des noeuds distincts afin d&apos;améliorer la tolérance aux pannes.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Sept noeuds de stockage des objets
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Aucun noeud n&apos;excède environ 15 % du stockage total.
       </para>
      </listitem>
      <listitem>
       <para>
        La capacité totale de la grappe doit être dimensionnée de sorte que, même si un noeud est indisponible, la capacité totale utilisée (y compris la redondance) ne dépasse pas 80 %.
       </para>
      </listitem>
      <listitem>
       <para>
        Ethernet de 25 Go ou plus chacun lié à la grappe interne et au réseau public externe.
       </para>
      </listitem>
      <listitem>
       <para>
        Plus de 56 OSD par grappe de stockage.
       </para>
      </listitem>
      <listitem>
       <para>
        Reportez-vous à la <xref linkend="sysreq-osd"/> pour d&apos;autres recommandations.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Noeuds d&apos;infrastructure physique dédiés.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Trois noeuds de moniteur Ceph : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1.
       </para>
       <para>
        Reportez-vous à la <xref linkend="sysreq-mon"/> pour d&apos;autres recommandations.
       </para>
      </listitem>
      <listitem>
       <para>
        Noeuds Object Gateway : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1.
       </para>
       <para>
        Reportez-vous à la <xref linkend="sysreq-rgw"/> pour d&apos;autres recommandations.
       </para>
      </listitem>
      <listitem>
       <para>
        Noeuds Passerelle iSCSI : 16 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1.
       </para>
       <para>
        Reportez-vous à la <xref linkend="sysreq-iscsi"/> pour d&apos;autres recommandations.
       </para>
      </listitem>
      <listitem>
       <para>
        Noeuds MDS (un actif/un de secours) : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1.
       </para>
       <para>
        Reportez-vous à la <xref linkend="sysreq-mds"/> pour d&apos;autres recommandations.
       </para>
      </listitem>
      <listitem>
       <para>
        Un noeud Admin SES : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deployment-hw-multipath">
   <title>Configuration multipath</title>
   <para>
    Si vous souhaitez utiliser du matériel multipath, assurez-vous que LVM voit <literal>multipath_component_detection = 1</literal> dans le fichier de configuration sous la section <literal>devices</literal>. Vous pouvez vérifier à l&apos;aide de la commande <command>lvm config</command>.
   </para>
   <para>
    Vous pouvez également vous assurer que LVM filtre les composants mpath d&apos;un périphérique via la configuration du filtre LVM. Cela dépendra de l&apos;hôte.
   </para>
   <note>
    <para>
     Ce n&apos;est pas recommandé et ne doit être envisagé que si <literal>multipath_component_detection = 1</literal> ne peut pas être défini.
    </para>
   </note>
   <para>
    Pour plus d&apos;informations sur la configuration multipath, consultez l&apos;adresse <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-multipath.html#sec-multipath-lvm"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>Noeuds de stockage des objets</title>

  <sect2 xml:id="sysreq-osd">
   <title>Configuration minimale requise</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Les recommandations d&apos;UC suivantes tiennent compte des périphériques indépendamment de l&apos;utilisation par Ceph :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1x thread d&apos;UC 2 GHz par disque rotatif.
       </para>
      </listitem>
      <listitem>
       <para>
        2x threads d&apos;UC 2 GHz par disque SSD.
       </para>
      </listitem>
      <listitem>
       <para>
        4x threads d&apos;UC 2 GHz par disque NVMe.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Réseaux 10 GbE séparés (public/client et interne), 4x 10 GbE requis, 2x 25 GbE recommandés.
     </para>
    </listitem>
    <listitem>
     <para>
      Mémoire RAM totale requise = nombre d&apos;OSD x (1 Go + <option>osd_memory_target</option>) + 16 Go
     </para>
     <para>
      Reportez-vous au <xref linkend="config-auto-cache-sizing"/> pour plus de détails sur <option>osd_memory_target</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Disques OSD dans les configurations JBOD ou les configurations RAID-0 individuelles.
     </para>
    </listitem>
    <listitem>
     <para>
      Le journal OSD peut résider sur le disque OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Les disques OSD doivent être exclusivement utilisés par SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Disque et SSD dédiés au système d&apos;exploitation, de préférence dans une configuration RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Allouez au moins 4 Go de RAM supplémentaires si cet hôte OSD doit héberger une partie d&apos;une réserve en cache utilisée pour la hiérarchisation du cache.
     </para>
    </listitem>
    <listitem>
     <para>
      Les moniteurs Ceph, la passerelle et les serveurs de métadonnées peuvent résider sur les noeuds de stockage des objets.
     </para>
    </listitem>
    <listitem>
     <para>
      Pour des raisons de performances de disque, les noeuds OSD sont des noeuds sans système d&apos;exploitation. Aucun autre workload ne doit s&apos;exécuter sur un noeud OSD, sauf s&apos;il s&apos;agit d&apos;une configuration minimale d&apos;instances Ceph Monitor et Ceph Manager.
     </para>
    </listitem>
    <listitem>
     <para>
      Disques SSD pour journal avec un ratio journal SSD à OSD de 6:1.
     </para>
    </listitem>
   </itemizedlist>
   <note>
    <para>
     Assurez-vous qu&apos;aucun périphérique de bloc en réseau n&apos;est assigné aux noeuds OSD, tels que des images de périphérique de bloc RADOS ou iSCSI.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Taille minimale du disque</title>
   <para>
    Deux types d&apos;espace disque sont nécessaires pour l&apos;exécution sur un OSD : l&apos;espace pour le périphérique WAL/DB et l&apos;espace primaire pour les données stockées. La valeur minimale (et par défaut) pour le périphérique WAL/DB est de 6 Go. L&apos;espace minimum pour les données est de 5 Go, car les partitions inférieures à 5 Go sont automatiquement assignées à une pondération de 0.
   </para>
   <para>
    Ainsi, bien que l&apos;espace disque minimal pour un OSD soit de 11 Go, il est recommandé de ne pas utiliser un disque inférieur à 20 Go, même à des fins de test.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Taille recommandée pour les périphériques WAL et DB de BlueStore</title>
   <tip>
    <title>supplément d&apos;informations</title>
    <para>
     Reportez-vous à la <xref linkend="about-bluestore"/> pour plus d&apos;informations sur BlueStore.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      Nous vous recommandons de réserver 4 Go pour le périphérique WAL. Alors que la taille minimale de la base de données est de 64 Go pour les workloads RBD uniquement, la taille recommandée pour les workloads Object Gateway et CephFS est de 2 % de la capacité du périphérique principal (mais au moins 196 Go).
     </para>
     <important>
      <para>
       Nous recommandons des volumes de base de données plus importants pour les déploiements à charge élevée, en particulier si l&apos;utilisation de RGW ou de CephFS est importante. Réservez une certaine capacité (emplacements) pour installer plus de matériel et plus d&apos;espace DB si nécessaire.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
      Si vous envisagez de placer les périphériques WAL et DB sur le même disque, il est recommandé d&apos;utiliser une partition unique pour les deux périphériques, plutôt que d&apos;utiliser une partition distincte pour chacun. Cela permet à Ceph d&apos;utiliser également le périphérique DB pour les opérations WAL. La gestion de l&apos;espace disque est donc plus efficace, car Ceph n&apos;utilise la partition DB pour le WAL qu&apos;en cas de nécessité. Un autre avantage est que la probabilité que la partition WAL soit pleine est très faible, et lorsqu&apos;elle n&apos;est pas entièrement utilisée, son espace n&apos;est pas gaspillé, mais utilisé pour les opérations de DB.
     </para>
     <para>
      Pour partager le périphérique DB avec le WAL, ne spécifiez <emphasis>pas</emphasis> le périphérique WAL et indiquez uniquement le périphérique DB.
     </para>
     <para>
      Pour plus d&apos;informations sur la spécification d&apos;une disposition OSD, reportez-vous au <xref linkend="drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>SSD pour les partitions WAL/DB</title>
   <para>
    Les disques SSD n&apos;ont pas de pièces mobiles. Cela réduit le temps d&apos;accès aléatoire et la latence de lecture tout en accélérant le débit de données. Comme leur prix par Mo est significativement plus élevé que le prix des disques durs tournants, les disques SSD ne conviennent que pour un stockage plus petit.
   </para>
   <para>
    Les OSD peuvent constater une amélioration significative de leurs performances en stockant leurs partitions sur un disque SSD et les données d&apos;objet sur un disque dur distinct.
   </para>
   <tip>
    <title>partage d&apos;un disque SSD pour plusieurs partitions WAL/DB</title>
    <para>
     Étant donné que les partitions WAL/DB occupent relativement peu d&apos;espace, vous pouvez partager un disque SSD avec plusieurs partitions WAL/DB. N&apos;oubliez pas qu&apos;avec chaque partition WAL/DB, les performances du disque SSD se dégradent. Il est déconseillé de partager plus de six partitions WAL/DB sur le même disque SSD et 12 sur des disques NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Nombre maximal recommandé de disques</title>
   <para>
    Vous pouvez avoir autant de disques sur un serveur que ce dernier l&apos;autorise. Il existe quelques aspects à considérer lorsque vous planifiez le nombre de disques par serveur :
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Bande passante du réseau.</emphasis> Plus vous avez de disques sur un serveur, plus il y a de données à transférer via la ou les cartes réseau pour les opérations d&apos;écriture sur disque.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Mémoire.</emphasis> Plus de 2 Go de RAM sont utilisés pour le cache BlueStore. Avec la valeur par défaut de l&apos;option <option>osd_memory_target</option>, à savoir 4 Go, le système dispose d&apos;une taille de cache de départ raisonnable pour les supports rotatifs. Si vous utilisez des disques SSD ou NVMe, pensez à augmenter la taille du cache et l&apos;allocation de RAM par OSD afin d&apos;optimiser les performances.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolérance aux pannes.</emphasis> Si le serveur complet tombe en panne, plus il comporte de disques, plus le nombre d&apos;OSD perdus temporairement par la grappe est important. En outre, pour maintenir les règles de réplication en cours d&apos;exécution, vous devez copier toutes les données du serveur en panne vers les autres noeuds de la grappe.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Noeuds de moniteur</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Au moins trois noeuds MON sont requis. Le nombre de moniteurs doit toujours être impair (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 Go de RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processeur à quatre coeurs logiques.
    </para>
   </listitem>
   <listitem>
    <para>
     Un SSD ou un autre type de support de stockage suffisamment rapide est vivement recommandé pour les moniteurs, en particulier pour le chemin <filename>/var/lib/ceph</filename> sur chaque noeud de moniteur, car le quorum peut être instable avec des latences de disque élevées. Deux disques en configuration RAID 1 sont recommandés pour la redondance. Il est recommandé d&apos;utiliser des disques distincts ou au moins des partitions de disque distinctes pour les processus Monitor, afin de protéger l&apos;espace disque disponible du moniteur contre des phénomènes tels que le grossissement excessif du fichier journal.
    </para>
   </listitem>
   <listitem>
    <para>
     Il ne doit y avoir qu&apos;un seul processus Monitor par noeud.
    </para>
   </listitem>
   <listitem>
    <para>
     La combinaison des noeuds OSD, Monitor ou Object Gateway n&apos;est prise en charge que s&apos;il y a suffisamment de ressources matérielles disponibles. Cela signifie que les besoins de tous les services doivent être additionnés.
    </para>
   </listitem>
   <listitem>
    <para>
     Deux interfaces réseau liées à plusieurs commutateurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Noeuds Object Gateway</title>

  <para>
   Les noeuds Object Gateway doivent avoir au moins six coeurs de processeur et 32 Go de RAM. Lorsque d&apos;autres processus sont colocalisés sur la même machine, leurs besoins doivent être additionnés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Noeuds de serveur de métadonnées</title>

  <para>
   Le dimensionnement approprié des noeuds MDS (Metadata Server, Serveur de métadonnées) dépend du cas d&apos;utilisation spécifique. En règle générale, plus le nombre de fichiers ouverts que le serveur de métadonnées doit traiter est important, plus l&apos;UC et la RAM requises sont importantes. La configuration minimale requise est la suivante :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     4 Go de RAM pour chaque daemon MDS.
    </para>
   </listitem>
   <listitem>
    <para>
     Interface réseau liée.
    </para>
   </listitem>
   <listitem>
    <para>
     2,5 GHz d&apos;UC avec au moins 2 coeurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-admin-node">
  <title>Noeud Admin</title>

  <para>
   Au moins 4 Go de RAM et une UC quadruple coeur sont requis. Cela inclut l&apos;exécution de Salt Master sur le noeud Admin. Pour les grappes de grande taille avec des centaines de noeuds, 6 Go de RAM sont conseillés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>Noeuds de passerelle iSCSI</title>

  <para>
   Les noeuds de passerelle iSCSI doivent avoir au moins six coeurs de processeur et 16 Go de RAM.
  </para>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SES et autres produits SUSE</title>

  <para>
   Cette section contient des informations importantes concernant l&apos;intégration de SES avec d&apos;autres produits SUSE.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager et SUSE Enterprise Storage n&apos;étant pas intégrés, SUSE Manager ne peut actuellement pas gérer une grappe SES.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Limitations concernant les noms</title>

  <para>
   Ceph ne prend généralement pas en charge les caractères non-ASCII dans les fichiers de configuration, les noms de réserve, les noms d&apos;utilisateur, etc. Lors de la configuration d&apos;une grappe Ceph, il est recommandé d&apos;utiliser uniquement des caractères alphanumériques simples (A-Z, a-z, 0-9) et des ponctuations minimales (« . », « - », « _ ») dans tous les noms d&apos;objet/de configuration Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>OSD et Monitor partageant un même serveur</title>

  <para>
   Bien qu&apos;il soit techniquement possible d&apos;exécuter des OSD et des instances Monitor sur le même serveur dans des environnements de test, il est vivement recommandé de disposer d&apos;un serveur distinct pour chaque noeud de moniteur en production. Le principal motif concerne la performance : plus la grappe contient d&apos;OSD, plus les noeuds MON doivent effectuer un grand nombre d&apos;opérations d&apos;E/S. Et lorsqu&apos;un serveur est partagé entre un noeud MON et plusieurs OSD, les opérations d&apos;E/S des OSD constituent un facteur limitant pour le noeud de moniteur.
  </para>

  <para>
   Un autre aspect consiste à déterminer s&apos;il convient de partager des disques entre un OSD, un noeud MON et le système d&apos;exploitation sur le serveur. La réponse est simple : si possible, dédiez un disque distinct à l&apos;OSD et un serveur distinct au noeud de moniteur.
  </para>

  <para>
   Bien que Ceph prenne en charge les OSD basés sur les répertoires, un OSD doit toujours avoir un disque dédié autre que celui du système d&apos;exploitation.
  </para>

  <tip>
   <para>
    S&apos;il est <emphasis>vraiment</emphasis> nécessaire d&apos;exécuter l&apos;OSD et le noeud MON sur le même serveur, exécutez MON sur un disque distinct en montant le disque dans le répertoire <filename>/var/lib/ceph/mon</filename> pour obtenir des performances légèrement meilleures.
   </para>
  </tip>
 </sect1>
</chapter>
