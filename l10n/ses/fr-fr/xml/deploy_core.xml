<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_core.xml" version="5.0" xml:id="deploy-core">
 <info>
  <title>Déploiement des services essentiels restants à l&apos;aide de cephadm</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Après avoir déployé la grappe Ceph de base, déployez les services principaux sur d&apos;autres noeuds de la grappe. Pour que les clients puissent accéder aux données de la grappe, déployez aussi des services supplémentaires.
 </para>
 <para>
  Actuellement, nous prenons en charge le déploiement des services Ceph sur la ligne de commande à l&apos;aide de l&apos;orchestrateur Ceph (sous-commandes <command>ceph orch</command>).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>Commande <command>ceph orch</command></title>

  <para>
   La commande de l&apos;orchestrateur Ceph <command>ceph orch</command>, qui sert d&apos;interface avec le module cephadm, se charge d&apos;établir la liste les composants de la grappe et de déployer les services Ceph sur les nouveaux noeuds de la grappe.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Affichage de l&apos;état de l&apos;orchestrateur</title>
   <para>
    La commande suivante affiche le mode et l&apos;état actuels de l&apos;orchestrateur Ceph.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Liste des périphériques, services et daemons</title>
   <para>
    Pour obtenir la liste de tous les périphériques de disque, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>services et daemons</title>
    <para>
     <emphasis>Service</emphasis> est un terme général désignant un service Ceph d&apos;un type spécifique, par exemple Ceph Manager.
    </para>
    <para>
     <emphasis>Daemon</emphasis> est une instance spécifique d&apos;un service, par exemple un processus <literal>mgr.ses-min1.gdlcik</literal> exécuté sur un noeud <literal>appelé ses-min1</literal>.
    </para>
   </tip>
   <para>
    Pour obtenir la liste de tous les services connus de cephadm, exécutez :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     Vous pouvez limiter la liste aux services d&apos;un noeud en particulier avec le paramètre facultatif <option>&#x2011;&#x2011;host</option> et aux services d&apos;un type particulier avec le paramètre facultatif <option>&#x2011;&#x2011;service-type</option>. Les types acceptables sont <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>, <literal>mds</literal> et <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    Pour obtenir la liste de tous les daemons en cours d&apos;exécution déployés par cephadm, exécutez :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     Pour interroger l&apos;état d&apos;un daemon en particulier, utilisez <option>--daemon_type</option> 
et <option>--daemon_id</option>. Pour les OSD, l&apos;ID est l&apos;ID numérique de l&apos;OSD. Pour MDS, l&apos;ID est le nom du système de fichiers :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd --daemon_id 0
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Spécification de service et de placement</title>

  <para>
   La méthode recommandée pour spécifier le déploiement des services Ceph consiste à créer un fichier au format YAML spécifiant les services que vous avez l&apos;intention de déployer.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Création de spécifications de service</title>
   <para>
    Vous pouvez créer un fichier de spécifications distinct pour chaque type de service, par exemple :
   </para>
<screen>
<prompt>root@master # </prompt>cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Vous pouvez également spécifier plusieurs (ou tous les) types de service dans un fichier (par exemple, <filename>cluster.yml</filename>) pour décrire les noeuds qui exécuteront des services spécifiques. N&apos;oubliez pas de séparer les différents types de services par trois tirets (<literal>---</literal>) :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    Les propriétés susmentionnées ont la signification suivante :
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       Type de service. Il peut s&apos;agir d&apos;un service Ceph (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>, <literal>crash</literal>, <literal>osd</literal> ou <literal>rbd-mirror</literal>), d&apos;une passerelle (<literal>nfs</literal> ou <literal>rgw</literal>) ou d&apos;une partie de la pile de surveillance (<literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> ou <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       Nom du service. Les spécifications de type <literal>mon</literal>, <literal>mgr</literal>, <literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> et <literal>prometheus</literal> ne nécessitent pas la propriété <literal>service_id</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Spécifie les noeuds qui exécuteront le service. Reportez-vous à la <xref linkend="cephadm-placement-specs"/> pour plus de détails.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Spécification supplémentaire pertinente pour le type de service.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>application de services spécifiques</title>
    <para>
     Les services de grappe Ceph ont généralement un certain nombre de propriétés intrinsèques. Pour obtenir des exemples et des détails sur la spécification des différents services, reportez-vous à la <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Création d&apos;une spécification de placement</title>
   <para>
    Pour déployer les services Ceph, cephadm doit savoir sur quels noeuds les déployer. Utilisez la propriété <literal>placement</literal> et répertoriez les noms d&apos;hôte courts des noeuds auxquels le service s&apos;applique :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Application de la spécification de grappe</title>
   <para>
    Après avoir créé un fichier <filename>cluster.yml</filename> complet avec les spécifications de tous les services et leur placement, vous pouvez appliquer la grappe en exécutant la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yml</screen>
   <para>
    Pour afficher l&apos;état de la grappe, exécutez la commande <command>ceph orch status</command>. Pour plus de détails, reportez-vous à la <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exportation de la spécification d&apos;une grappe en cours d&apos;exécution</title>
   <para>
    Bien que vous ayez déployé des services sur la grappe Ceph à l&apos;aide des fichiers de spécifications comme décrit à la <xref linkend="cephadm-service-and-placement-specs"/>, la configuration de la grappe peut être différente de la spécification d&apos;origine lors de son fonctionnement. Il se peut également que vous ayez accidentellement supprimé les fichiers de spécifications.
   </para>
   <para>
    Pour récupérer une spécification complète d&apos;une grappe en cours d&apos;exécution, exécutez :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     Vous pouvez ajouter l&apos;option <option>--format</option> pour modifier le format de sortie <literal>yaml</literal> par défaut. Vous pouvez choisir entre <literal>json</literal>, <literal>json-attractive</literal> ou <literal>yaml</literal>. Par exemple :
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Déploiement des services Ceph</title>

  <para>
   Une fois la grappe de base en cours d&apos;exécution, vous pouvez déployer des services Ceph sur d&apos;autres noeuds.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Déploiement d&apos;instances de Monitor et Ceph Manager</title>
   <para>
    La grappe Ceph comporte trois ou cinq instances MON déployées sur différents noeuds. Si la grappe contient au moins cinq noeuds, il est recommandé de déployer cinq instances MON. Une bonne pratique consiste à déployer les instances MGR sur les mêmes noeuds que les instances MON.
   </para>
   <important>
    <title>inclusion de l&apos;instance MON de démarrage</title>
    <para>
     Lorsque vous déployez des instances MON et MGR, n&apos;oubliez pas d&apos;inclure la première instance MON que vous avez ajoutée lors de la configuration de la grappe de base à la <xref linkend="deploy-cephadm-configure-mon"/>.
    </para>
   </important>
   <para>
    Pour déployer des instances MON, appliquez la spécification suivante :
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     Si vous devez ajouter un autre noeud, ajoutez le nom d&apos;hôte à la même liste YAML. Par exemple :
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    De la même manière, pour déployer des instances MGR, appliquez la spécification suivante :
   </para>
   <important>
    <para>
     Assurez-vous que votre déploiement contient au moins trois instances Ceph Manager dans chaque déploiement.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     Si les instances MON ou MGR ne se trouvent <emphasis>pas</emphasis> dans le même sous-réseau, vous devez ajouter les adresses de sous-réseau. Par exemple :
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Déploiement des OSD Ceph</title>
   <important>
    <title>lorsque le périphérique de stockage est disponible</title>
    <para>
     Un périphérique de stockage est considéré comme <emphasis>disponible</emphasis> si toutes les conditions suivantes sont remplies :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Le périphérique n&apos;a pas de partitions.
      </para>
     </listitem>
     <listitem>
      <para>
       Le périphérique n&apos;a pas d&apos;état LVM.
      </para>
     </listitem>
     <listitem>
      <para>
       Le périphérique n&apos;est pas monté.
      </para>
     </listitem>
     <listitem>
      <para>
       Le périphérique ne contient pas de système de fichiers.
      </para>
     </listitem>
     <listitem>
      <para>
       Le périphérique ne contient pas d&apos;OSD BlueStore.
      </para>
     </listitem>
     <listitem>
      <para>
       La taille du périphérique est supérieure à 5 Go.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Si les conditions ci-dessus ne sont pas remplies, Ceph refuse de provisionner ces OSD.
    </para>
   </important>
   <para>
    Deux méthodes permettent de déployer des OSD :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Indiquez à Ceph de consommer tous les périphériques de stockage disponibles et inutilisés :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Utilisez DriveGroups (reportez-vous au <xref linkend="drive-groups"/>) pour créer une spécification OSD décrivant les périphériques qui seront déployés sur la base de leurs propriétés, telles que le type de périphérique (SSD ou HDD), les noms de modèle de périphérique, la taille ou les noeuds sur lesquels les périphériques existent. Appliquez ensuite la spécification en exécutant la commande suivante :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Déploiement de serveurs de métadonnées</title>
   <para>
    CephFS nécessite un ou plusieurs services de serveur de métadonnées (MDS). Pour créer un CephFS, commencez par créer des serveurs MDS en appliquant la spécification suivante :
   </para>
   <note>
    <para>
     Assurez-vous d&apos;avoir créé au moins deux réserves, l&apos;une pour les données CephFS et l&apos;autre pour les métadonnées CephFS, avant d&apos;appliquer la spécification suivante.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    Une fois que les MDS sont fonctionnels, créez le CephFS :
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Déploiement d&apos;instances Object Gateway</title>
   <para>
    cephadm déploie Object Gateway en tant qu&apos;ensemble de daemons qui gèrent un <emphasis>domaine</emphasis> et une <emphasis>zone</emphasis> spécifique.
   </para>
   <para>
    Vous pouvez soit associer un service Object Gateway à une zone et un domaine préexistants (reportez-vous au <xref linkend="ceph-rgw-fed"/> pour plus de détails) ou vous pouvez spécifier des noms <replaceable>NOM_DOMAINE</replaceable> et <replaceable>NOM_ZONE</replaceable> n&apos;existant pas encore. Ils seront créés automatiquement après avoir appliqué la configuration suivante :
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Utilisation d&apos;un accès SSL sécurisé</title>
    <para>
     Pour utiliser une connexion SSL sécurisée à l&apos;instance Object Gateway, vous avez besoin d&apos;une paire de certificats SSL et de fichiers de clé valides (reportez-vous au <xref linkend="ceph-rgw-https"/> pour plus de détails). Vous devez activer SSL, spécifier un numéro de port pour les connexions SSL, ainsi que le certificat SSL et les fichiers de clé.
    </para>
    <para>
     Pour activer SSL et spécifier le numéro de port, incluez les éléments suivants dans votre spécification :
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     Pour spécifier le certificat et la clé SSL, vous pouvez coller leur contenu directement dans le fichier de spécifications YAML. Le signe de barre verticale (<literal>|</literal>) à la fin de la ligne indique à l&apos;analyseur de s&apos;attendre à une valeur contenant une chaîne s&apos;étendant sur plusieurs lignes. Par exemple :
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      Au lieu de coller le contenu du certificat SSL et des fichiers de clé, vous pouvez omettre les mots-clés <literal>rgw_frontend_ssl_certificate:</literal> et <literal>rgw_frontend_ssl_key:</literal> et les télécharger dans la base de données de configuration :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>Configuration de la passerelle Object Gateway pour une écoute sur les ports 443 et 80</title>
     <para>
      Pour configurer la passerelle Object Gateway afin qu&apos;elle écoute sur les ports 443 (HTTPS) et 80 (HTTP), procédez comme suit :
     </para>
     <note>
      <para>
       Les commandes de la procédure utilisent le domaine et la zone <literal>default</literal>.
      </para>
     </note>
     <procedure>
      <step>
       <para>
        Déployez la passerelle Object Gateway en fournissant un fichier de spécifications. Reportez-vous à la <xref linkend="deploy-cephadm-day2-service-ogw"/> pour plus de détails sur la spécification Object Gateway. Utilisez la commande suivante :
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        Si les certificats SSL ne sont pas fournis dans le fichier de spécifications, ajoutez-les à l&apos;aide de la commande suivante :
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        Modifiez la valeur par défaut de l&apos;option <option>rgw_frontends</option> :
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      
      <step>
       <para>
        Supprimez la configuration spécifique créée par cephadm. Identifiez la cible pour laquelle l&apos;option <option>rgw_frontends</option> a été configurée en exécutant la commande :
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config dump | grep rgw</screen>
       <para>
        Par exemple, la cible est <literal>client.rgw.default.default.node4.yiewdu</literal>. Supprimez la valeur <option>rgw_frontends</option> spécifique actuelle :
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         Au lieu de supprimer une valeur pour <option>rgw_frontends</option>, vous pouvez la spécifier. Par exemple :
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      
      <step>
       <para>
        Redémarrez les passerelles Object Gateway :
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Déploiement avec une sous-grappe</title>
    <para>
     Les <emphasis>sous-grappes</emphasis> vous aident à organiser les noeuds de vos grappes afin d&apos;isoler les workloads et de gagner en flexibilité. Si vous effectuez un déploiement avec une sous-grappe, appliquez la configuration suivante :
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Déploiement de passerelles iSCSI</title>
   <para>
    cephadm déploie une passerelle iSCSI, à savoir un protocole SAN (Storage area network) qui permet aux clients (appelés initiateurs) d&apos;envoyer des commandes SCSI aux périphériques de stockage SCSI (cibles) sur des serveurs distants.
   </para>
   <para>
    Appliquez la configuration suivante pour effectuer le déploiement. Assurez-vous que la liste <literal>trusted_ip_list</literal> contient les adresses IP de tous les noeuds de passerelle iSCSI et Ceph Manager (comme dans l&apos;exemple de sortie ci-dessous).
   </para>
   <note>
    <para>
     Vérifiez que la réserve a été créée avant d&apos;appliquer la spécification suivante.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Assurez-vous que les adresses IP répertoriées pour <literal>trusted_ip_list</literal> ne contiennent <emphasis>pas</emphasis> d&apos;espace après la séparation par une virgule.
    </para>
   </note>
   <sect3>
    <title>Configuration SSL sécurisée</title>
    <para>
     Pour utiliser une connexion SSL sécurisée entre Ceph Dashboard et l&apos;API cible iSCSI, vous avez besoin d&apos;une paire de fichiers de clés et de certificats SSL valides. Ceux-ci peuvent être émis par une autorité de certification ou auto-signés (reportez-vous au <xref linkend="self-sign-certificates"/>). Pour activer SSL, ajoutez le paramètre <literal>api_secure: true</literal> à votre fichier de spécifications :
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     Pour spécifier le certificat et la clé SSL, vous pouvez coller le contenu directement dans le fichier de spécifications YAML. Le signe de barre verticale (<literal>|</literal>) à la fin de la ligne indique à l&apos;analyseur de s&apos;attendre à une valeur contenant une chaîne s&apos;étendant sur plusieurs lignes. Par exemple :
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Déploiement de NFS Ganesha</title>
    
<important>
 <para>
  NFS Ganesha prend en charge les versions 4.1 et ultérieures de NFS. Il ne prend pas en charge la version 3 de NFS.
 </para>
</important>

    <para>
    cephadm déploie NFS Ganesha à l&apos;aide d&apos;une réserve RADOS prédéfinie et d&apos;un espace de nom facultatif. Pour déployer NFS Ganesha, appliquez la spécification suivante :
   </para>
   <note>
    <para>
     Vous devez disposer d&apos;une réserve RADOS prédéfinie, sinon l&apos;opération <command>ceph orch apply</command> échouera. Pour plus d&apos;informations sur la création d&apos;une, reportez-vous au <xref linkend="ceph-pools-operate-add-pool"/>.
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      <replaceable>EXEMPLE_NFS</replaceable> avec une chaîne arbitraire qui identifie l&apos;exportation NFS.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXEMPLE_RÉSERVE</replaceable> avec le nom de la réserve dans laquelle l&apos;objet de configuration NFS Ganesha RADOS sera stocké.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXEMPLE_ESPACE_DE_NOM</replaceable> (facultatif) avec l&apos;espace de nom NFS Object Gateway souhaité (par exemple, <literal>ganesha</literal>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Déploiement de <systemitem class="daemon">rbd-mirror</systemitem></title>
   <para>
    Le service <systemitem class="daemon">rbd-mirror</systemitem> prend en charge la synchronisation des images de périphériques de bloc RADOS entre deux grappes Ceph (pour plus de détails, reportez-vous au <xref linkend="ceph-rbd-mirror"/>). Pour déployer <systemitem class="daemon">rbd-mirror</systemitem>, utilisez la spécification suivante :
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Déploiement de la pile de surveillance</title>
   <para>
    La pile de surveillance se compose de Prometheus, d&apos;exportateurs Prometheus, de Prometheus Alertmanager et de Grafana. Ceph Dashboard utilise ces composants pour stocker et visualiser des mesures détaillées concernant l&apos;utilisation et les performances de la grappe.
   </para>
   <tip>
    <para>
     Si votre déploiement nécessite des images de conteneur personnalisées ou mises à disposition localement pour les services de pile de surveillance, reportez-vous au <xref linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    Pour déployer la pile de surveillance, procédez comme suit :
   </para>
   <procedure>
    <step>
     <para>
      Activez le module <literal>prometheus</literal> dans le daemon Ceph Manager. Cela permet de rendre visibles les mesures Ceph internes afin que Prometheus puisse les lire :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Assurez-vous que cette commande est exécutée avant le déploiement de Prometheus. Si la commande n&apos;a pas été exécutée avant le déploiement, vous devez redéployer Prometheus pour mettre à jour la configuration de Prometheus :
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Créez un fichier de spécifications (par exemple, <filename>monitoring.yaml</filename>) dont le contenu ressemble à ce qui suit :
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Appliquez les services de surveillance en exécutant :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i monitoring.yaml</screen>
     <para>
      Le déploiement des services de surveillance peut prendre une à deux minutes.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     Prometheus, Grafana et Ceph Dashboard sont tous automatiquement configurés pour pouvoir communiquer entre eux, ce qui permet une intégration Grafana entièrement fonctionnelle dans Ceph Dashboard lorsque le déploiement a été effectué comme décrit ci-dessus.
    </para>
    <para>
     La seule exception à cette règle est la surveillance avec des images RBD. Pour plus d&apos;informations, reportez-vous au <xref linkend="monitoring-rbd-image"/>.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
