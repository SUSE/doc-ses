<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Mise à niveau de SUSE Enterprise Storage 6 vers la version 7.1</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ce chapitre présente les étapes requises pour mettre à niveau SUSE Enterprise Storage 6 vers la version 7.1.
 </para>
 <para>
  La mise à niveau inclut les tâches suivantes :
 </para>
 <itemizedlist>
  <listitem>
   <para>
    mise à niveau de Ceph Nautilus vers Pacific ;
   </para>
  </listitem>
  <listitem>
   <para>
    passage de l&apos;installation et de l&apos;exécution de Ceph via des paquetages RPM à l&apos;exécution dans des conteneurs ;
   </para>
  </listitem>
  <listitem>
   <para>
    suppression complète de DeepSea et remplacement par <systemitem class="resource">ceph-salt</systemitem> et cephadm.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   Les informations de mise à niveau de ce chapitre s&apos;appliquent <emphasis>uniquement</emphasis> aux mises à niveau de DeepSea vers cephadm. N&apos;essayez pas de suivre ces instructions si vous souhaitez déployer SUSE Enterprise Storage sur la plate-forme SUSE CaaS.
  </para>
 </warning>
 <important>
  <para>
   La mise à niveau des versions SUSE Enterprise Storage antérieures à la version 6 n&apos;est pas prise en charge. Vous devez d&apos;abord passer à la dernière version SUSE Enterprise Storage 6, puis suivre les étapes de ce chapitre.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Avant la mise à niveau</title>

  <para>
   Les tâches suivantes <emphasis>doivent</emphasis> être effectuées avant de commencer la mise à niveau. Cette opération peut être effectuée à tout moment pendant la durée de vie de SUSE Enterprise Storage 6.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     La migration OSD de FileStore vers BlueStore <emphasis>doit</emphasis> avoir été effectuée avant la mise à niveau, car FileStore n&apos;est pas pris en charge dans SUSE Enterprise Storage 7.1. Pour plus d&apos;informations concernant BlueStore et sur la procédure de migration à partir de FileStore, consultez le site <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>
    </para>
   </listitem>
   <listitem>
    <para>
     Si vous exécutez une grappe plus ancienne qui utilise toujours des OSD <literal>ceph-disk</literal>, vous <emphasis>devez</emphasis> passer à <literal>ceph-volume</literal> avant la mise à niveau. Pour plus de détails, consultez le site <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Considérations</title>
   <para>
    Avant la mise à niveau, veillez à lire les sections suivantes pour vous assurer que vous comprenez toutes les tâches à exécuter.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Lisez les notes de version</emphasis>. Elles contiennent des informations supplémentaires sur les modifications apportées depuis la version précédente de SUSE Enterprise Storage. Consultez les notes de version pour vérifier les aspects suivants :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        votre matériel doit tenir compte de certaines considérations spéciales ;
       </para>
      </listitem>
      <listitem>
       <para>
        les paquetages logiciels utilisés ont été considérablement modifiés ;
       </para>
      </listitem>
      <listitem>
       <para>
        des précautions spéciales sont nécessaires pour votre installation.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Les notes de version incluent également des informations de dernière minute qui, faute de temps, n&apos;ont pas pu être intégrées au manuel. Elles contiennent également des notes concernant les problèmes connus.
     </para>
     <para>
      Les notes de version de SES 7.1 sont disponibles en ligne à l&apos;adresse <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      De plus, après avoir installé le paquetage <package>release-notes-ses</package> à partir du dépôt SES 7.1, les notes de version sont disponibles en local dans le répertoire <filename>/usr/share/doc/release-notes</filename> ou en ligne à l&apos;adresse <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Lisez la <xref linkend="ses-deployment"/> pour vous familiariser avec <systemitem class="resource">ceph-salt</systemitem> et l&apos;orchestrateur Ceph, et en particulier prendre connaissance des informations sur les spécifications de service.
     </para>
    </listitem>
    <listitem>
     <para>
      La mise à niveau de la grappe peut prendre beaucoup de temps : environ le temps nécessaire pour mettre à niveau une machine multiplié par le nombre de noeuds de la grappe.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous devez d&apos;abord mettre à niveau Salt Master, puis remplacer DeepSea par <systemitem class="resource">ceph-salt</systemitem> et cephadm. Vous ne pourrez <emphasis>pas</emphasis> commencer à utiliser le module orchestrateur cephadm tant que vous n&apos;aurez pas mis à niveau tous les noeuds de Ceph Manager.
     </para>
    </listitem>
    <listitem>
     <para>
      La mise à niveau de l&apos;utilisation des RPM Nautilus vers les conteneurs Pacific doit s&apos;effectuer en une seule étape. Il convient donc de mettre à niveau un noeud entier à la fois, et non un daemon à la fois.
     </para>
    </listitem>
    <listitem>
     <para>
      La mise à niveau des services principaux (MON, MGR, OSD) s&apos;effectue de façon ordonnée. Chaque service est disponible pendant la mise à niveau. Les services de passerelle (Metadata Server, Object Gateway, NFS Ganesha, iSCSI Gateway) doivent être redéployés après la mise à niveau des services principaux. Un certain temps hors service existe pour chacun des services suivants :
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         Les instances de Metadata Server et d&apos;Object Gateway sont hors service à partir du moment où les noeuds sont mis à niveau de SUSE Linux Enterprise Server 15 SP1 vers SUSE Linux Enterprise Server 15 SP3 jusqu&apos;au redéploiement des services à la fin de la procédure de mise à niveau. Il est particulièrement important de garder cela à l&apos;esprit si ces services cohabitent avec des MON, des MGR ou des OSD, car ces derniers risquent aussi d&apos;être hors service pendant toute la durée de la mise à niveau de la grappe. Si cette mise hors service vous semble problématique, envisagez de déployer ces services séparément sur des noeuds supplémentaires avant de procéder à la mise à niveau, afin leur mise hors service dure moins longtemps. Il s&apos;agit de la durée de la mise à niveau des noeuds de passerelle, et non de la durée de la mise à niveau de l&apos;ensemble de la grappe.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        Les instances NFS Ganesha et iSCSI Gateway ne sont hors service que pendant le redémarrage des noeuds lors de la mise à niveau de SUSE Linux Enterprise Server 15 SP1 vers SUSE Linux Enterprise Server 15 SP3, puis brièvement lors du redéploiement de chaque service en mode conteneurisé.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Sauvegarde de la configuration et des données de grappe</title>
   <para>
    Nous vous recommandons vivement de sauvegarder l&apos;ensemble de la configuration et des données de la grappe avant de commencer votre mise à niveau vers SUSE Enterprise Storage 7.1. Pour savoir comment sauvegarder toutes vos données, consultez le site <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Vérification des étapes de la mise à niveau précédente</title>
   <para>
    Si vous avez au préalable effectué une mise à niveau à partir de la version 5, vérifiez que la mise à niveau vers la version 6 a bien abouti :
   </para>
   <para>
    Vérifiez l&apos;existence du fichier <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
   </para>
   <para>
    Ce fichier est créé par le processus engulf lors de la mise à niveau de SUSE Enterprise Storage 5 vers la version 6. L&apos;option <option>configuration_init: default-import</option> est définie dans le répertoire <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    Si l&apos;option <option>configuration_init</option> reste définie sur <option>default-import</option>, la grappe utilise <filename>ceph.conf.import</filename> comme fichier de configuration, au lieu du fichier <filename>ceph.conf</filename> par défaut de DeepSea, qui est compilé à partir des fichiers dans le répertoire <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Par conséquent, vous devez rechercher les configurations personnalisées potentielles dans <filename>ceph.conf.import</filename> et, éventuellement, les déplacer dans l&apos;un des fichiers du répertoire <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Supprimez ensuite la ligne <option>configuration_init: default-import</option> du répertoire <filename>/srv/pilleur/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Mise à jour des noeuds de la grappe et vérification de l&apos;état de santé de la grappe</title>
   <para>
    Vérifiez que les dernières mises à jour de SUSE Linux Enterprise Server 15 SP1 et SUSE Enterprise Storage 6 sont toutes appliquées à tous les noeuds de la grappe :
   </para>
<screen><prompt role="root"># </prompt>zypper refresh &amp;&amp; zypper patch</screen>
   <tip>
    <para>
     Reportez-vous au site <link xlink:href="https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates"/> pour obtenir des informations détaillées sur la mise à jour des noeuds de grappe.
    </para>
   </tip>
   <para>
    Une fois les mises à jour appliquées, redémarrez Salt Master, synchronisez les nouveaux modules Salt et vérifiez l&apos;état de santé de la grappe :
   </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
<prompt>cephuser@adm &gt; </prompt>ceph -s
</screen>
   <sect3 xml:id="upgrade-disable-insecure">
    <title>Désactivation des clients non sécurisés</title>
    <para>
     Depuis la version 14.2.20 de Nautilus, un nouvel avertissement relatif à l&apos;état de santé vous informe que les clients non sécurisés sont autorisés à rejoindre la grappe. Cet avertissement est <emphasis>activé</emphasis> par défaut. Ceph Dashboard affiche la grappe dans l&apos;état <literal>HEALTH_WARN</literal>. La ligne de commande vérifie l&apos;état de la grappe comme suit :
    </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph status
 cluster:
   id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
   health: HEALTH_WARN
   mons are allowing insecure global_id reclaim
 [...]
 </screen>
    <para>
     Cet avertissement signifie que les moniteurs Ceph autorisent toujours les anciens clients non corrigés à se connecter à la grappe. De cette façon, les clients existants peuvent toujours se connecter pendant la mise à niveau de la grappe, mais le système vous avertit qu&apos;un problème doit être résolu. Une fois la grappe et tous les clients mis à niveau vers la dernière version de Ceph, interdisez les clients non corrigés en exécutant la commande suivante :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Vérification de l&apos;accès aux dépôts logiciels et aux images de conteneur</title>
   <para>
    Vérifiez que chaque noeud de la grappe a accès aux dépôts logiciels SUSE Linux Enterprise Server 15 SP3 et SUSE Enterprise Storage 7.1, ainsi qu&apos;au registre des images de conteneur.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Dépôts logiciels</title>
    <para>
     Si tous les noeuds sont enregistrés auprès de SCC, vous pouvez utiliser la commande <command>zypper migration</command> pour effectuer la mise à niveau. Pour plus d&apos;informations, consultez le site <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/>.
    </para>
    <para>
     Si les noeuds ne sont <emphasis role="bold">pas</emphasis> enregistrés auprès de SCC, désactivez tous les dépôts logiciels existants et ajoutez les dépôts <literal>Pool</literal> et <literal>Updates</literal> pour chacune des extensions suivantes :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7.1
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Images de conteneur</title>
    <para>
     Tous les noeuds de la grappe doivent accéder au registre des images de conteneur. Dans la plupart des cas, vous utiliserez le registre SUSE public à l&apos;adresse <literal>registration.suse.com</literal>. Les images suivantes sont nécessaires :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Vous pouvez également, par exemple pour les déploiements à vide, configurer un registre local et vérifier que vous disposez de l&apos;ensemble approprié d&apos;images de conteneur. Pour plus d&apos;informations sur la configuration d&apos;un registre d&apos;images de conteneur local, reportez-vous à la <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Mise à niveau de Salt Master</title>

  <para>
   La procédure suivante décrit le processus de mise à niveau de Salt Master :
  </para>

  <procedure>
   <step>
    <para>
     Mettez à niveau le système d&apos;exploitation sous-jacent vers SUSE Linux Enterprise Server 15 SP3 :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Pour les grappes dont l&apos;ensemble des noeuds sont enregistrés auprès de SCC, exécutez <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Pour les grappes dont les noeuds ont des dépôts logiciels assignés manuellement, exécutez <command>zypper dup</command> suivi de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Désactivez les phases DeepSea pour éviter toute utilisation accidentelle. Ajoutez le contenu suivant à <filename>/srv/pillar/ceph/stack/global.yml</filename> :
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Enregistrez le fichier et appliquez les modifications :
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     Si vous n&apos;utilisez <emphasis role="bold">pas</emphasis> d&apos;images de conteneur de <literal>registry.suse.com</literal>, mais plutôt le registre configuré localement, modifiez <filename>/srv/pillar/ceph/stack/global.yml</filename> pour indiquer à DeepSea l&apos;image de conteneur Ceph et le registre utiliser. Par exemple, pour utiliser <literal>192.168.121.1:5000/my/ceph/image</literal>, ajoutez les lignes suivantes :
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Si vous devez spécifier des informations d&apos;authentification pour le registre, ajoutez le bloc <literal>ses7_container_registry_auth:</literal>, par exemple :
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
ses7_container_registry_auth:
  registry: 192.168.121.1:5000
  username: <replaceable>USER_NAME</replaceable>
  password: <replaceable>PASSWORD</replaceable>
</screen>
    <para>
     Enregistrez le fichier et appliquez les modifications :
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Assimilez la configuration existante :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Vérifiez l&apos;état de la mise à niveau. Il est possible que votre sortie soit différente en fonction de la configuration de votre grappe :
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 16.2.7-640-gceb23c7491b (ceb23c7491bd96ab7956111374219a4cdcf6f8f4) pacific (stable)
 os: SUSE Linux Enterprise Server 15 SP3

Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)

Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Mise à niveau des noeuds MON, MGR et OSD</title>

  <para>
   Mettez à niveau les noeuds Ceph Monitor, Ceph Manager et OSD un par un. Pour chaque service, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Avant d&apos;adopter un noeud OSD, vous devez effectuer une conversion de format des noeuds OSD pour améliorer la comptabilisation des données OMAP. Vous pouvez le faire en exécutant la commande suivante sur le noeud Admin :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set osd bluestore_fsck_quick_fix_on_mount true</screen>
    <para>
     Les noeuds OSD seront convertis automatiquement une fois leur adoption terminée.
    </para>
    <note>
     <para>
      La conversion peut prendre un certain temps (quelques minutes à plusieurs heures), selon la quantité de données OMAP que contient le disque dur associé. Pour plus d&apos;informations, reportez-vous à la page <link xlink:href="https://docs.ceph.com/en/latest/releases/pacific/#upgrading-non-cephadm-clusters"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Si vous mettez à niveau un noeud OSD, évitez de marquer l&apos;OSD comme <literal>sorti</literal> pendant la mise à niveau en exécutant la commande suivante :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Remplacez <replaceable>SHORT_NODE_NAME</replaceable> par le nom abrégé du noeud tel qu&apos;il apparaît dans la sortie de la commande <command>ceph osd tree</command>. Dans l&apos;entrée suivante, les noms d&apos;hôte abrégés sont <literal>ses-min1</literal> et <literal>ses-min2</literal>
    </para>
<screen>
<prompt>root@master # </prompt>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Mettez à niveau le système d&apos;exploitation sous-jacent vers SUSE Linux Enterprise Server 15 SP3 :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Si les noeuds de la grappe sont tous enregistrés auprès de SCC, exécutez <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Si les noeuds de la grappe ont des dépôts logiciels assignés manuellement, exécutez <command>zypper dup</command> suivi de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Une fois le noeud redémarré, conteneurisez tous les daemons MON, MGR et OSD existants sur ce noeud en exécutant la commande suivante sur Salt Master :
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Remplacez <replaceable>MINION_ID</replaceable> par l&apos;ID du minion que vous mettez à niveau. Vous pouvez obtenir la liste des ID de minion en exécutant la commande <command>salt-key -L</command> sur Salt Master.
    </para>
    <tip>
     <para>
      Pour voir l&apos;état et la progression de l&apos;<emphasis>adoption</emphasis>, consultez Ceph Dashboard ou exécutez l&apos;une des commandes suivantes sur Salt Master :
     </para>
<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     Une fois l&apos;adoption terminée, désélectionnez l&apos;indicateur <literal>noout</literal> si vous mettez à niveau un noeud OSD :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Mise à niveau des noeuds de passerelle</title>

  <para>
   Mettez ensuite à niveau vos noeuds de passerelle distincts (passerelle Samba, serveur de métadonnées, Object Gateway, NFS Ganesha ou passerelle iSCSI). Mettez à niveau le système d&apos;exploitation sous-jacent vers SUSE Linux Enterprise Server 15 SP3 pour chaque noeud :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Si les noeuds de la grappe sont tous enregistrés auprès de SUSE Customer Center, exécutez la commande <command>zypper migration</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Si les noeuds de la grappe ont des dépôts logiciels assignés manuellement, exécutez la commande <command>zypper dup</command> suivie de <command>reboot</command>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Cette étape s&apos;applique également à tous les noeuds qui font partie de la grappe, mais qui n&apos;ont pas encore de rôle assigné (en cas de doute, consultez la liste des hôtes sur Salt Master fournie par la commande <command>salt-key -L</command> et comparez-la à la sortie de la commande <command>salt-run upgrade.status</command>).
  </para>

  <para>
   Lorsque le système d&apos;exploitation a été mis à niveau sur tous les noeuds de la grappe, l&apos;étape suivante consiste à installer le paquetage <package>ceph-salt</package> et à appliquer la configuration de la grappe. Les services de passerelle proprement dits sont redéployés en mode conteneurisé à la fin de la procédure de mise à niveau.
  </para>

  <note>
   <para>
    Les services Metadata Server et Object Gateway sont indisponibles à partir de la mise à niveau vers SUSE Linux Enterprise Server 15 SP3 jusqu&apos;à leur redéploiement à la fin de la procédure de mise à niveau.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Installation de <systemitem class="resource">ceph-salt</systemitem> et application de la configuration de la grappe</title>

  <para>
   Avant de lancer la procédure d&apos;installation de <systemitem class="resource">ceph-salt</systemitem> et d&apos;appliquer la configuration de la grappe, vérifiez l&apos;état de la grappe et de la mise à niveau en exécutant les commandes suivantes :
  </para>

<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Supprimez les tâches cron <literal>rbd_exporter</literal> et <literal>rgw_exporter</literal> créées par DeepSea. Sur Salt Master en tant que <systemitem class="username">root</systemitem>, exécutez la commande <command>crontab -e</command> pour modifier le fichier crontab. Supprimez les éléments suivants, le cas échéant :
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null
</screen>
   </step>
   <step>
    <para>
     Exportez la configuration de la grappe de DeepSea en exécutant les commandes suivantes :
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<prompt>root@master # </prompt>salt-run upgrade.generate_service_specs &gt; specs.yaml
</screen>
   </step>
   <step>
    <para>
     Désinstallez DeepSea et installez <systemitem class="resource">ceph-salt</systemitem> sur Salt Master :
    </para>
<screen>
<prompt>root@master # </prompt>zypper remove 'deepsea*'
<prompt>root@master # </prompt>zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Redémarrez Salt Master et synchronisez les modules Salt :
    </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Importez la configuration de la grappe de DeepSea dans <systemitem class="resource">ceph-salt</systemitem> :
    </para>
<screen><prompt>root@master # </prompt>ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Générez des clés SSH pour la communication des noeuds de la grappe :
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Vérifiez que la configuration de la grappe a bien été importée de DeepSea et spécifiez les options potentiellement manquantes :
     </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
     <para>
      Pour une description complète de la configuration de la grappe, reportez-vous à la <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Appliquez la configuration et activez cephamp :
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </step>
   <step>
    <para>
     Si vous devez fournir une URL de registre local de conteneurs et des informations d&apos;identification d&apos;accès, suivez les étapes décrites à la <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     Si vous n&apos;utilisez <emphasis role="bold">pas</emphasis> d&apos;images de conteneur de <literal>registry.suse.com</literal>, mais plutôt le registre configuré en local, indiquez à Ceph quelle image de conteneur utiliser en exécutant
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Arrêtez et désactivez les daemons <systemitem class="daemon">ceph-crash</systemitem> de SUSE Enterprise Storage 6. Les nouvelles formes conteneurisées de ces daemons seront lancées automatiquement plus tard.
    </para>
<screen>
<prompt>root@master # </prompt>salt '*' service.stop ceph-crash
<prompt>root@master # </prompt>salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Mise à niveau et adoption de la pile de surveillance</title>

  <para>
   La procédure suivante adopte tous les composants de la pile de surveillance (voir <xref linkend="monitoring-alerting"/> pour plus de détails).
  </para>

  <procedure>
   <step>
    <para>
     Mettez l&apos;orchestrateur en pause :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch pause</screen>
   </step>
   <step>
    <para>
     Sur le noeud qui exécute Prometheus, Grafana et Alertmanager (l&apos;instance Salt Master par défaut), exécutez les commandes suivantes :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      Si vous n&apos;exécutez <emphasis role="bold">pas</emphasis> le registre d&apos;images de conteneur par défaut <literal>registration.suse.com</literal>, vous devez spécifier l&apos;image à utiliser sur chaque commande, par exemple :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-server:2.32.1 \
  adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-alertmanager:0.21.0 \
  adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/grafana:7.5.12 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      Les images de conteneur requises et leurs versions respectives sont répertoriées dans le <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Supprimez Node-Exporter de <emphasis role="bold">tous</emphasis> les noeuds. Node-Exporter ne doit pas nécessairement être migré et est réinstallé en tant que conteneur lorsque le fichier <filename>specs.yaml</filename> est appliqué.
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> zypper rm golang-github-prometheus-node_exporter</screen>
    <para>
     Vous pouvez également supprimer Node-Exporter de tous les noeuds simultanément à l&apos;aide de Salt sur le noeud Admin :
    </para>
<screen><prompt>root@master # </prompt>salt '*' pkg.remove golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Appliquez les spécifications de service que vous avez précédemment exportées de DeepSea :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i specs.yaml</screen>
    <tip>
     <para>
      Si vous n&apos;exécutez <emphasis role="bold">pas</emphasis> le registre d&apos;images de conteneur par défaut <literal>registration.suse.com</literal>, mais un registre de conteneurs local, configurez cephadm pour utiliser l&apos;image de conteneur du registre local pour le déploiement de Node-Exporter avant de déployer Node-Exporter. Dans le cas contraire, vous pouvez sans problème passer cette étape et ignorer l&apos;avertissement suivant.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/cephadm/container_image_node_exporter <replaceable>QUALIFIED_IMAGE_PATH</replaceable></screen>
     <para>
      Assurez-vous que toutes les images de conteneur des services de surveillance pointent vers le registre local, et pas seulement vers celui de Node-Exporter. Cette étape vous oblige à le faire uniquement pour Node-Exporter, mais il est conseillé de définir toutes les images de conteneur de surveillance dans cephadm pour qu&apos;elles pointent vers le registre local à ce stade.
     </para>
     <para>
      Si vous ne le faites pas, les nouveaux déploiements de services de surveillance ainsi que les redéploiements utiliseront la configuration par défaut de cephadm et il se peut que vous ne puissiez plus déployer de services (dans le cas de déploiements à vide) ou avec des services déployés avec des versions mixtes.
     </para>
     <para>
      La procédure de configuration de cephadm pour utiliser les images de conteneur du registre local est décrite dans le <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Relancez l&apos;orchestrateur :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Redéploiement du service de passerelle</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Mise à niveau de la passerelle Object Gateway</title>
   <para>
    Dans SUSE Enterprise Storage 7.1, les passerelles Object Gateway sont toujours configurées avec un domaine, ce qui permet une utilisation sur plusieurs sites (voir <xref linkend="ceph-rgw-fed"/> pour plus de détails) à l&apos;avenir. Si vous avez utilisé une configuration Object Gateway monosite dans SUSE Enterprise Storage 6, procédez comme suit pour ajouter un domaine. Si vous ne prévoyez pas d&apos;utiliser la fonctionnalité multisite, vous pouvez utiliser la <literal>valeur par défaut</literal> pour les noms de domaine, de groupe de zones et de zone.
   </para>
   <procedure>
    <step>
     <para>
      Créez un domaine :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Vous pouvez éventuellement renommer la zone et le groupe de zones par défaut.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configurez le groupe de zones maître :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configurez la zone maître. Pour ce faire, vous aurez besoin des clés ACCESS_KEY et SECRET_KEY d&apos;un utilisateur Object Gateway avec l&apos;indicateur <option>system</option> activé. Il s&apos;agit généralement de l&apos;utilisateur <literal>admin</literal>. Pour obtenir les clés ACCESS_KEY et SECRET_KEY, exécutez <command>radosgw-admin user info --uid admin --rgw-zone=<replaceable>NOM_ZONE</replaceable></command>.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Validez la configuration mise à jour :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    Pour conteneuriser le service Object Gateway, créez son fichier de spécification comme décrit à la <xref linkend="deploy-cephadm-day2-service-ogw"/> et appliquez-le.
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Mise à niveau de NFS Ganesha</title>
   
<important>
 <para>
  NFS Ganesha prend en charge les versions 4.1 et ultérieures de NFS. Il ne prend pas en charge la version 3 de NFS.
 </para>
</important>

   <para>
    Les paragraphes qui suivent expliquent comment migrer un service NFS Ganesha existant qui exécute Ceph Nautilus vers un conteneur NFS Ganesha qui exécute Ceph Octopus.
   </para>
   <warning>
    <para>
     La documentation suivante implique que vous ayez déjà mis à niveau les services Ceph principaux.
    </para>
   </warning>
   <para>
    NFS Ganesha stocke la configuration par daemon supplémentaire et l&apos;exporte dans une réserve RADOS. La réserve RADOS configurée se trouve sur la ligne <literal>watch_url</literal> du bloc <literal>RADOS_URLS</literal> dans le fichier <filename>ganesha.conf</filename>. Par défaut, cette réserve est nommée <literal>ganesha_config</literal>
   </para>
   <para>
    Avant de tenter une migration, nous vous recommandons vivement de faire une copie des objets de configuration d&apos;exportation et daemon situés dans la réserve RADOS. Pour localiser la réserve RADOS configurée, exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    Pour répertorier le contenu de la réserve RADOS :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    Pour copier les objets RADOS :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>OBJS=$(rados $RADOS_ARGS ls)
<prompt>cephuser@adm &gt; </prompt>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    Pour chaque noeud, tout service NFS Ganesha existant doit être arrêté, puis remplacé par un conteneur géré par cephadm.
   </para>
   <procedure>
    <step>
     <para>
      Arrêtez et désactivez le service NFS Ganesha existant :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>systemctl stop nfs-ganesha
<prompt>cephuser@adm &gt; </prompt>systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      Une fois le service NFS Ganesha existant arrêté, vous pouvez en déployer un nouveau dans un conteneur à l&apos;aide de cephadm. Pour ce faire, vous devez créer une spécification de service contenant un <literal>service_id</literal> qui sera utilisé pour identifier cette nouvelle grappe NFS, le nom d&apos;hôte du noeud que nous migrons répertorié comme hôte dans la spécification de placement, ainsi que la réserve RADOS et l&apos;espace de noms qui contient les objets d&apos;exportation NFS configurés. Par exemple :
     </para>
<screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      Pour plus d&apos;informations sur la création d&apos;une spécification de placement, reportez-vous à la <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Appliquez la spécification de placement :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirmez que le daemon NFS Ganesha est en cours d&apos;exécution sur l&apos;hôte :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7.1/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Répétez ces étapes pour chaque noeud NFS Ganesha. Il n&apos;est pas nécessaire de créer une spécification de service distincte pour chaque noeud. Il suffit d&apos;ajouter le nom d&apos;hôte de chaque noeud à la spécification de service NFS existante et de l&apos;appliquer à nouveau.
     </para>
    </step>
   </procedure>
   <para>
    Les exportations existantes peuvent être migrées de deux manières différentes :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      recréation ou réassignation manuelle à l&apos;aide de Ceph Dashboard ;
     </para>
    </listitem>
    <listitem>
     <para>
      copie manuelle du contenu de chaque objet RADOS par daemon dans la configuration commune NFS Ganesha nouvellement créée.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Copie manuelle des exportations dans le fichier de configuration commun NFS Ganesha</title>
    <step>
     <para>
      Déterminez la liste des objets RADOS par daemon :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Copiez les objets RADOS par daemon :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Triez et fusionnez en une seule liste d&apos;exportations :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat conf-* | sort -u &gt; conf-nfs.<replaceable>SERVICE_ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Écrivez le nouveau fichier de configuration commun NFS Ganesha :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Notifiez le daemon NFS Ganesha :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       Cette opération entraîne le rechargement de la configuration par le daemon.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    Une fois le service migré, le service NFS Ganesha basé sur Nautilus peut être supprimé.
   </para>
   <procedure>
    <step>
     <para>
      Supprimez NFS Ganesha :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Supprimez les paramètres hérités de la grappe de l&apos;instance Ceph Dashboard :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Mise à niveau du serveur de métadonnées</title>
   <para>
    Contrairement aux instances MON, MGR et OSD, le serveur de métadonnées ne peut pas être adopté sur place. Vous devez le redéployer dans des conteneurs à l&apos;aide de l&apos;orchestrateur Ceph.
   </para>
   <procedure>
    <step>
     <para>
      Exécutez la commande <command>ceph fs ls</command> pour obtenir le nom de votre système de fichiers, par exemple :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Créez un nouveau fichier de spécification de service <filename>mds.yml</filename> comme décrit à la <xref linkend="deploy-cephadm-day2-service-mds"/> en utilisant le nom du système de fichiers comme <option>service_id</option> et en spécifiant les hôtes qui exécuteront les daemons MDS. Par exemple :
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Exécutez la commande <command>ceph orch apply -i mds.yml</command> pour appliquer la spécification de service et démarrer les daemons MDS.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Mise à niveau de la passerelle iSCSI</title>
   <para>
    Pour mettre à niveau la passerelle iSCSI, vous devez la redéployer dans des conteneurs à l&apos;aide de l&apos;orchestrateur Ceph. Si vous disposez de plusieurs passerelles iSCSI, vous devez les redéployer une par une pour réduire le temps d&apos;indisponibilité du service.
   </para>
   <procedure>
    <step>
     <para>
      Arrêtez et désactivez les daemons iSCSI existants sur chaque noeud de passerelle iSCSI :
     </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> systemctl stop rbd-target-gw
<prompt>&gt; </prompt><command>sudo</command> systemctl disable rbd-target-gw
<prompt>&gt; </prompt><command>sudo</command> systemctl stop rbd-target-api
<prompt>&gt; </prompt><command>sudo</command> systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Créez une spécification de service pour la passerelle iSCSI comme décrit à la <xref linkend="deploy-cephadm-day2-service-igw"/>. Pour ce faire, vous avez besoin des paramètres <option>pool</option>, <option>trusted_ip_list</option> et <option>api_*</option> du fichier <filename>/etc/ceph/iscsi-gateway.cfg</filename> existant. Si la prise en charge SSL est activée (<literal>api_secure = true</literal>), vous avez également besoin du certificat SSL (<filename>/etc/ceph/iscsi-gateway.crt</filename>) et de la clé (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      Par exemple, si le fichier <filename>/etc/ceph/iscsi-gateway.cfg</filename> contient ce qui suit :
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Vous devez créer le fichier de spécification de service <filename>iscsi.yml</filename> suivant :
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       Les paramètres <option>pool</option>, <option>trusted_ip_list</option>, <option>api_port</option>, <option>api_user</option>, <option>api_password</option>, <option>api_secure</option> sont identiques à ceux du fichier <filename>/etc/ceph/iscsi-gateway.cfg</filename>. Les valeurs <option>ssl_cert</option> et <option>ssl_key</option> peuvent être copiées à partir du certificat SSL et des fichiers de clé existants. Vérifiez qu&apos;elles sont correctement indentées et que le caractère <emphasis>barre verticale</emphasis> (<literal>|</literal>) apparaît à la fin des lignes <literal>ssl_cert:</literal> et <literal>ssl_key:</literal> (voir le contenu du fichier <filename>iscsi.yml</filename> ci-dessus).
      </para>
     </note>
    </step>
    <step>
     <para>
      Exécutez la commande <command>ceph orch apply -i iscsi.yml</command> pour appliquer la spécification de service et démarrer les daemons de la passerelle iSCSI.
     </para>
    </step>
    <step>
     <para>
      Supprimez l&apos;ancien paquetage <package>ceph-iscsi</package> de chacun des noeuds de passerelle iSCSI existants :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Nettoyage après mise à niveau</title>

  <para>
   Après la mise à niveau, effectuez les opérations de nettoyage suivantes :
  </para>

  <procedure>
   <step>
    <para>
     Vérifiez que la grappe a bien été mise à niveau en vérifiant la version actuelle de Ceph :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph versions</screen>
   </step>
   <step>
    <para>
     Assurez-vous qu&apos;aucun ancien OSD ne rejoindra la grappe :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd require-osd-release pacific</screen>
   </step>
   <step>
    <para>
     Définissez le mode <option>pg_autoscale_mode</option> des réserves existantes si nécessaire :
    </para>
    <important>
     <para>
      Dans SUSE Enterprise Storage 6, les réserves avaient le paramètre <option>pg_autoscale_mode</option> défini sur <option>warn</option> par défaut. Cela entraînait un message d&apos;avertissement si le nombre de groupes de placement était sous-optimal, mais la mise à l&apos;échelle automatique ne se produisait pas. Dans SUSE Enterprise Storage 7.1, l&apos;option <option>pg_autoscale_mode</option> est définie sur <option>on</option> par défaut pour les nouvelles réserves et les groupes de placement seront mis à l&apos;échelle automatiquement. Le processus de mise à niveau ne modifie pas automatiquement le paramètre <option>pg_autoscale_mode</option> des réserves existantes. Si vous souhaitez le régler sur <option>on</option> pour tirer pleinement parti de la mise à l&apos;échelle automatique, reportez-vous aux instructions figurant dans le <xref linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Pour plus de détails, reportez-vous au <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Évitez les clients de versions antérieures à Luminous :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Activez le module de l&apos;équilibreur :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph balancer mode upmap
<prompt>cephuser@adm &gt; </prompt>ceph balancer on
</screen>
    <para>
     Pour plus de détails, reportez-vous au <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Vous pouvez éventuellement activer le module de télémétrie :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph mgr module enable telemetry
<prompt>cephuser@adm &gt; </prompt>ceph telemetry on
 </screen>
    <para>
     Pour plus de détails, reportez-vous au <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
