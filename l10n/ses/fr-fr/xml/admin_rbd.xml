<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>Périphérique de bloc RADOS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>oui</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Un bloc est une séquence d'octets, par exemple un bloc de 4 Mo de données.
  Les interfaces de stockage basées sur des blocs constituent le moyen le plus
  courant de stocker des données sur des supports rotatifs, tels que des
  disques durs, des CD ou des disquettes. L'omniprésence des interfaces de
  périphériques de bloc fait d'un périphérique de bloc virtuel un candidat
  idéal pour interagir avec un système de stockage de données de masse, tel que
  Ceph.
 </para>
 <para>
  Les périphériques de bloc Ceph permettent le partage de ressources physiques
  et sont redimensionnables. Ils stockent les données réparties sur plusieurs
  OSD dans une grappe Ceph. Les périphériques de bloc Ceph exploitent les
  fonctionnalités RADOS, telles que les instantanés, la réplication et la
  cohérence. Les périphériques de bloc RADOS (RADOS Block Devices, RBD) Ceph
  interagissent avec les OSD utilisant des modules de kernel ou la bibliothèque
  <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocole RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Les périphériques de bloc Ceph offrent des performances exceptionnelles ainsi
  qu'une évolutivité infinie des modules de kernel. Ils prennent en charge des
  solutions de virtualisation, telles que QEMU, ou des systèmes basés sur le
  cloud, tels qu'OpenStack, qui reposent sur
  <systemitem class="library">libvirt</systemitem>. Vous pouvez utiliser la
  même grappe pour faire fonctionner Object Gateway, CephFS et les
  périphériques de bloc RADOS simultanément.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Commandes de périphériques de bloc</title>

  <para>
   La commande <command>rbd</command> permet de créer, de répertorier,
   d'explorer et de supprimer des images de périphérique de bloc. Vous pouvez
   également l'utiliser, par exemple, pour cloner des images, créer des
   instantanés, restaurer une image dans un instantané ou afficher un
   instantané.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Création d'une image de périphérique de bloc dans une réserve répliquée</title>
   <para>
    Avant de pouvoir ajouter un périphérique de bloc à un client, vous devez
    créer une image associée dans une réserve existante (voir
    <xref linkend="ceph-pools"/>) :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Par exemple, pour créer une image de 1 Go nommée « myimage » qui stocke des
    informations dans une réserve nommée « mypool », exécutez la commande
    suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>unités de taille d'image</title>
    <para>
     Si vous n'indiquez pas de raccourci d'unité de taille (« G » ou « T »), la
     taille de l'image est en mégaoctets. Indiquez « G » ou « T » après le
     chiffre de la taille pour spécifier des gigaoctets ou des téraoctets.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Création d'une image de périphérique de bloc dans une réserve codée à effacement</title>
   <para>
    Il est possible de stocker les données d'une image de périphérique de bloc
    directement dans des réserves codées à effacement (Erasure Coded, EC). Une
    image de périphérique de bloc RADOS se compose de
    <emphasis>données</emphasis> et de <emphasis>métadonnées</emphasis>. Seules
    les données d'une image de périphérique de bloc peuvent être stockées dans
    une réserve EC. Le drapeau <option>overwrite</option> (écraser) de la
    réserve doit être défini sur <emphasis>true</emphasis> (vrai), ce qui est
    possible uniquement si tous les OSD sur lesquels la réserve est stockée
    utilisent BlueStore.
   </para>
   <para>
    La partie « métadonnées » de l'image ne peut pas être stockée dans une
    réserve EC. Vous pouvez spécifier la réserve répliquée pour stocker les
    métadonnées de l'image à l'aide de l'option <option>--pool=</option> de la
    commande <command>rbd create</command> ou spécifier <option>pool/</option>
    comme préfixe du nom de l'image.
   </para>
   <para>
    Créez une réserve EC :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Spécifiez la réserve répliquée dans laquelle stocker les métadonnées :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    Ou :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Liste des images de périphériques de bloc</title>
   <para>
    Pour lister les périphériques de bloc dans une réserve nommée « mypool »,
    exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Récupération d'informations sur l'image</title>
   <para>
    Pour récupérer des informations à partir d'une image « myimage » dans une
    réserve nommée « mypool », exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Redimensionnement d'une image de périphérique de bloc</title>
   <para>
    Les images de périphérique de bloc RADOS sont provisionnées dynamiquement :
    en effet, elles n'utilisent aucun stockage physique tant que vous n'y avez
    pas enregistré des données. Cependant, elles possèdent une capacité
    maximale que vous définissez à l'aide de l'option <option>--size</option>.
    Si vous souhaitez augmenter (ou diminuer) la taille maximale de l'image,
    exécutez la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Suppression d'une image de périphérique de bloc</title>
   <para>
    Pour supprimer un périphérique de bloc qui correspond à une image
    « myimage » dans une réserve nommée « mypool », exécutez la commande
    suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montage et démontage</title>

  <para>
   Après avoir créé un périphérique de bloc RADOS, vous pouvez l'utiliser comme
   n'importe quel autre périphérique de disque et le formater, le monter pour
   pouvoir échanger des fichiers et le démonter une fois que vous avez terminé.
  </para>

  <para>
   La commande <command>rbd</command> accède par défaut à la grappe à l'aide du
   compte utilisateur Ceph <literal>admin</literal>. Ce compte dispose d'un
   accès administratif complet à la grappe. Il existe un risque de causer
   accidentellement des dommages, comme lorsque vous vous connectez à un poste
   de travail Linux en tant que <systemitem class="username">root</systemitem>.
   Par conséquent, il est préférable de créer des comptes utilisateur avec
   moins de privilèges et d'utiliser ces comptes pour un accès normal en
   lecture/écriture aux périphériques de bloc RADOS.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Création d'un compte utilisateur Ceph</title>
   <para>
    Pour créer un nouveau compte utilisateur avec les fonctionnalités Ceph
    Manager, Ceph Monitor et Ceph OSD, utilisez la commande
    <command>ceph</command> avec la sous-commande <command>auth
    get-or-create</command> :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Par exemple, pour créer un utilisateur appelé
    <replaceable>qemu</replaceable> avec un accès en lecture-écriture aux
    <replaceable>vms</replaceable> de la réserve et un accès en lecture seule
    aux <replaceable>images</replaceable> de la réserve, exécutez la commande
    suivante :
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    La sortie de la commande <command>ceph auth get-or-create</command> sera le
    trousseau de clés de l'utilisateur spécifié, qui peut être écrit dans
    <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     Lorsque vous utilisez la commande <command>rbd</command>, vous pouvez
     spécifier l'ID utilisateur en fournissant l'argument facultatif
     <command>--id</command> <replaceable>ID</replaceable>.
    </para>
   </note>
   <para>
    Pour plus d'informations sur la gestion des comptes utilisateur Ceph,
    reportez-vous au <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Authentification des utilisateurs</title>
   <para>
    Pour indiquer un nom d'utilisateur, utilisez <option>--id
    <replaceable>nom-utilisateur</replaceable></option>. Si vous utilisez
    l'authentification <systemitem>cephx</systemitem>, vous devez également
    indiquer un secret. Il peut provenir d'un trousseau de clés ou d'un fichier
    contenant le secret :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    ou
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Préparation du périphérique de bloc RADOS à utiliser</title>
   <procedure>
    <step>
     <para>
      Assurez-vous que votre grappe Ceph inclut une réserve avec l'image disque
      que vous souhaitez assigner. Supposons que la réserve soit appelée
      <literal>mypool</literal> et l'image <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Assignez l'image à un nouveau périphérique de bloc:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Dressez la liste de tous les périphériques assignés :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      Le périphérique sur lequel nous voulons travailler est
      <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>chemin du périphérique RBD</title>
      <para>
       Au lieu de
       <filename>/dev/rbd<replaceable>NUMÉRO_PÉRIPHÉRIQUE</replaceable></filename>,
       vous pouvez utiliser
       <filename>/dev/rbd/<replaceable>NOM_RÉSERVE</replaceable>/<replaceable>NOM_IMAGE</replaceable></filename>
       comme chemin de périphérique persistant. Par exemple :
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Créez un système de fichiers XFS sur le périphérique
      <filename>/dev/rbd0:</filename>
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      En remplaçant <filename>/mnt</filename> par votre point de montage,
      montez le périphérique et vérifiez qu'il est correctement monté :
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Vous pouvez maintenant déplacer des données vers et depuis le
      périphérique comme s'il s'agissait d'un répertoire local.
     </para>
     <tip>
      <title>augmentation de la taille du périphérique RBD</title>
      <para>
       Si vous trouvez que la taille du périphérique RBD n'est plus suffisante,
       vous pouvez facilement l'augmenter.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Augmentez la taille de l'image RBD, par exemple jusqu'à 10 Go.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Développez le système de fichiers à la nouvelle taille du
         périphérique:
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Après avoir accédé au périphérique, vous pouvez annuler son assignation
      et le démonter.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>montage et démontage manuels</title>
    <para>
     Un script <command>rbdmap</command> et une unité
     <systemitem class="daemon">systemd</systemitem> sont fournis pour
     faciliter le processus d'assignation et de montage des RBD après le
     démarrage et de leur démontage avant l'arrêt. Reportez-vous à la
     <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command> : assignation de périphériques RBD au moment du démarrage</title>
   <para>
    <command>rbdmap</command> est un script shell qui automatise les opérations
    <command>rbd map</command> et <command>rbd unmap</command> sur une ou
    plusieurs images RBD. Bien que vous puissiez exécuter le script
    manuellement à tout moment, les principaux avantages sont l'assignation et
    le montage automatiques des images RBD au démarrage (ainsi que le démontage
    et la désassignation à l'arrêt) déclenchés par le système Init. À cet
    effet, un fichier unité <systemitem class="daemon">systemd</systemitem>,
    <filename>rbdmap.service</filename>, est fourni avec le paquetage
    <systemitem>ceph-common</systemitem>.
   </para>
   <para>
    Le script accepte un argument unique, qui peut être <option>map</option> ou
    <option>unmap</option>. Dans les deux cas, le script analyse un fichier de
    configuration. Il pointe vers <filename>/etc/ceph/rbdmap</filename> par
    défaut, mais peut être remplacé par le biais d'une variable d'environnement
    <literal>RBDMAPFILE</literal>. Chaque ligne du fichier de configuration
    correspond à une image RBD qui doit être assignée ou dont l'assignation
    doit être annulée.
   </para>
   <para>
    Le fichier de configuration possède le format suivant :
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Chemin d'accès à une image dans une réserve. Indiquez-le en tant que
       <replaceable>nom_réserve</replaceable>/<replaceable>nom_image</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Liste facultative de paramètres à transmettre à la commande <command>rbd
       device map</command> sous-jacente. Ces paramètres et leurs valeurs
       doivent être indiqués en tant que chaîne séparée par des virgules, par
       exemple :
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       Dans cet exemple suivant, le script <command>rbdmap</command> exécute la
       commande suivante :
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       L'exemple suivant illustre comment spécifier un nom d'utilisateur et un
       trousseau de clés avec un secret correspondant :
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Lorsqu'il est exécuté en tant que <command>rdbmap map</command>, le script
    analyse le fichier de configuration et, pour chaque image RBD indiquée,
    tente d'abord d'assigner l'image (à l'aide de la commande <command>rbd
    device map</command>), puis de la monter.
   </para>
   <para>
    Lorsqu'elles sont exécutées en tant que <command>rbdmap unmap</command>,
    les images répertoriées dans le fichier de configuration seront démontées
    et désassignées.
   </para>
   <para>
    <command>rbdmap unmap-all</command> tente de démonter puis de désassigner
    toutes les images RBD actuellement assignées, qu'elles soient ou non
    répertoriées dans le fichier de configuration.
   </para>
   <para>
    En cas de réussite, l'opération <command>rbd device map</command> assigne
    l'image à un périphérique <filename>/dev/rbdX</filename> ; une règle udev
    est alors déclenchée afin de créer un lien symbolique de nom de
    périphérique convivial
    <filename>/dev/rbd/<replaceable>nom_réserve</replaceable>/<replaceable>nom_image</replaceable></filename>
    pointant vers le périphérique réellement assigné.
   </para>
   <para>
    Pour que le montage et le démontage réussissent, le nom de périphérique
    « convivial » doit être répertorié dans le fichier
    <filename>/etc/fstab</filename>. Lors de l'écriture d'entrées
    <filename>/etc/fstab</filename> pour les images RBD, indiquez l'option de
    montage « noauto » (ou « nofail »). Cela empêche le système Init d'essayer
    de monter le périphérique trop tôt, avant même que le périphérique en
    question existe, car <filename>rbdmap.service</filename> est généralement
    déclenché assez tard dans la séquence de démarrage.
   </para>
   <para>
    Pour obtenir la liste complète des options de <command>rbd</command>,
    reportez-vous à la page de manuel <command>rbd</command> (<command>man 8
    rbd</command>).
   </para>
   <para>
    Pour obtenir des exemples de l'utilisation de <command>rbd</command>,
    reportez-vous à la page de manuel de <command>rbd</command> (<command>man 8
    rbd</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Augmentation de la taille des périphériques RBD</title>
   <para>
    Si vous trouvez que la taille du périphérique RBD n'est plus suffisante,
    vous pouvez facilement l'augmenter.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Augmentez la taille de l'image RBD, par exemple jusqu'à 10 Go.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Développez le système de fichiers à la nouvelle taille du périphérique.
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Images instantanées</title>

  <para>
   Un instantané RBD est un instantané d'une image de périphérique de bloc
   RADOS. Avec les instantanés, vous conservez l'historique de l'état de
   l'image. Ceph prend également en charge la superposition d'instantanés, ce
   qui vous permet de cloner des images de machine virtuelle rapidement et
   facilement. Ceph prend en charge les instantanés de périphériques de bloc en
   utilisant la commande <command>rbd</command> et de nombreuses interfaces de
   niveau supérieur, notamment QEMU, <systemitem>libvirt</systemitem>,
   OpenStack et CloudStack.
  </para>

  <note>
   <para>
    Arrêtez les opérations d'entrée et de sortie et videz toutes les écritures
    en attente avant de créer l'instantané d'une image. Si l'image contient un
    système de fichiers, celui-ci doit être cohérent lors de la création de
    l'instantané.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Activation et configuration de <systemitem>cephx</systemitem></title>
   <para>
    Quand <systemitem>cephx</systemitem> est activé, vous devez spécifier un
    nom ou un ID d'utilisateur et un chemin d'accès au trousseau de clés
    contenant la clé correspondante pour l'utilisateur. Pour plus
    d'informations, reportez-vous au <xref linkend="cha-storage-cephx"/>. Vous
    pouvez également ajouter la variable d'environnement
    <systemitem>CEPH_ARGS</systemitem> pour ne pas avoir à saisir à nouveau les
    paramètres suivants.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Ajoutez l'utilisateur et le secret à la variable d'environnement
     <systemitem>CEPH_ARGS</systemitem> afin de ne pas avoir à les saisir à
     chaque fois.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Notions de base sur les instantanés</title>
   <para>
    Les procédures suivantes montrent comment créer, répertorier et supprimer
    des instantanés à l'aide de la commande <command>rbd</command> sur la ligne
    de commande.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Création d'instantanés</title>
    <para>
     Pour créer un instantané avec <command>rbd</command>, indiquez l'option
     <option>snap create</option>, le nom de la réserve et le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Liste des instantanés</title>
    <para>
     Pour répertorier les instantanés d'une image, spécifiez le nom de la
     réserve et le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Restauration de l'état initial des instantanés</title>
    <para>
     Pour rétablir l'état initial d'un instantané avec <command>rbd</command>,
     indiquez l'option <option>snap rollback</option>, le nom de la réserve, le
     nom de l'image et le nom de l'instantané.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Le retour à l'état initial d'une image dans un instantané revient à
      écraser la version actuelle de l'image avec les données d'un instantané.
      Le temps nécessaire à l'exécution d'un retour à l'état initial augmente
      avec la taille de l'image. Il est <emphasis>plus rapide de
      cloner</emphasis> à partir d'un instantané <emphasis>que de
      rétablir</emphasis> une image vers un instantané, cette méthode étant
      recommandée pour revenir à un état préexistant.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Suppression d'un instantané</title>
    <para>
     Pour supprimer un instantané avec <command>rbd</command>, indiquez
     l'option <option>snap rm</option>, le nom de la réserve, le nom de l'image
     et le nom de l'utilisateur.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Les instances Ceph OSD suppriment les données de manière asynchrone de
      sorte que la suppression d'un instantané ne libère pas immédiatement
      l'espace disque.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Purge des instantanés</title>
    <para>
     Pour supprimer tous les instantanés d'une image avec
     <command>rbd</command>, indiquez l'option <option>snap purge</option> et
     le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Superposition d'instantanés</title>
   <para>
    Ceph prend en charge la possibilité de créer plusieurs clones de copie à
    l'écriture (COW) d'un instantané de périphérique de bloc. La superposition
    d'instantanés donne aux clients de périphériques de bloc Ceph les moyens de
    créer des images très rapidement. Par exemple, vous pouvez créer une image
    de périphérique de bloc avec une machine virtuelle Linux écrite, puis
    capturer l'image, protéger l'instantané et créer autant de clones COW que
    vous le souhaitez. Un instantané étant en lecture seule, le clonage d'un
    instantané simplifie la sémantique et permet de créer rapidement des
    clones.
   </para>
   <note>
    <para>
     Dans les exemples de ligne de commande ci-dessous, les termes « parent »
     et « child » (enfant) désignent un instantané de périphérique de bloc Ceph
     (parent) et l'image correspondante clonée à partir de l'instantané
     (enfant).
    </para>
   </note>
   <para>
    Chaque image clonée (enfant) stocke une référence à son image parent, ce
    qui permet à l'image clonée d'ouvrir l'instantané parent et de le lire.
   </para>
   <para>
    Un clone COW d'un instantané se comporte exactement comme n'importe quelle
    autre image de périphérique Ceph. Vous pouvez lire, écrire, cloner et
    redimensionner des images clonées. Il n'y a pas de restrictions spéciales
    avec les images clonées. Cependant, le clone copy-on-write d'un instantané
    fait référence à l'instantané, donc vous <emphasis>devez</emphasis>
    protéger l'instantané avant de le cloner.
   </para>
   <note>
    <title><option>--image-format 1</option> non pris en charge</title>
    <para>
     Vous ne pouvez pas créer d'instantanés d'images créés avec l'option
     <command>rbd create ‑‑image-format 1</command> obsolète. Ceph ne prend en
     charge que le clonage des images <emphasis>format 2</emphasis> par défaut.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Démarrage de la superposition</title>
    <para>
     La superposition de périphériques de bloc Ceph est un processus simple.
     Vous devez disposer d'une image. Vous devez créer un instantané de
     l'image. Vous devez protéger l'instantané. Après avoir effectué ces
     étapes, vous pouvez commencer le clonage de l'instantané.
    </para>
    <para>
     L'image clonée contient une référence à l'instantané parent et inclut l'ID
     de la réserve, l'ID de l'image et l'ID de l'instantané. L'inclusion de
     l'ID de réserve signifie que vous pouvez cloner des instantanés d'une
     réserve vers des images d'une autre réserve.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Modèle d'image</emphasis> : un cas d'utilisation courant de la
       superposition de périphériques de bloc consiste à créer une image
       principale et un instantané servant de modèle aux clones. Par exemple,
       un utilisateur peut créer une image pour une distribution Linux (par
       exemple, SUSE Linux Enterprise Server (SLES)) et créer un instantané
       correspondant. Périodiquement, l'utilisateur peut mettre à jour l'image
       et créer un instantané (par exemple, <command>zypper ref &amp;&amp;
       zypper patch</command> suivi de <command>rbd snap create</command>). Au
       fur et à mesure que l'image mûrit, l'utilisateur peut cloner l'un des
       instantanés.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Modèle étendu</emphasis> : un cas d'utilisation plus avancée
       inclut l'extension d'une image modèle fournissant plus d'informations
       qu'une image de base. Par exemple, un utilisateur peut cloner une image
       (un modèle de machine virtuelle) et installer d'autres logiciels (par
       exemple, une base de données, un système de gestion de contenu ou un
       système d'analyse), puis prendre un instantané de l'image agrandie, qui
       peut elle-même être mise à jour de la même manière que l'image de base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Réserve de modèles</emphasis> : une façon d'utiliser la
       superposition de périphériques de bloc consiste à créer une réserve
       contenant des images principales agissant comme des modèles et des
       instantanés de ces modèles. Vous pouvez ensuite étendre les privilèges
       de lecture seule aux utilisateurs afin qu'ils puissent cloner les
       instantanés sans possibilité d'écriture ou d'exécution dans la réserve.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migration/récupération d'image</emphasis> : une façon
       d'utiliser la superposition de périphériques de bloc consiste à migrer
       ou récupérer des données d'une réserve vers une autre.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protection d'un instantané</title>
    <para>
     Les clones accèdent aux instantanés parents. Tous les clones seraient
     endommagés si un utilisateur supprimait par inadvertance l'instantané
     parent. Pour éviter toute perte de données, vous devez protéger
     l'instantané avant de pouvoir le cloner.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Vous ne pouvez pas supprimer un instantané protégé.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Clonage d'un instantané</title>
    <para>
     Pour cloner un instantané, vous devez spécifier la réserve parent,
     l'image, l'instantané, la réserve enfant et le nom de l'image. Vous devez
     protéger l'instantané avant de pouvoir le cloner.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Vous pouvez cloner un instantané d'une réserve vers une image d'une autre
      réserve. Par exemple, vous pouvez gérer des images en lecture seule et
      des instantanés en tant que modèles dans une réserve, d'une part, et des
      clones inscriptibles dans une autre réserve, d'autre part.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Suppression de la protection d'un instantané</title>
    <para>
     Avant de pouvoir supprimer un instantané, vous devez d'abord le
     déprotéger. En outre, vous pouvez <emphasis>ne pas</emphasis> supprimer
     des instantanés contenant des références de clones. Vous devez fusionner
     chaque clone d'un instantané avant de pouvoir supprimer celui-ci.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Liste des enfants d'un instantané</title>
    <para>
     Pour dresser la liste des enfants d'un instantané, exécutez :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Mise à plat d'une image clonée</title>
    <para>
     Les images clonées conservent une référence à l'instantané parent. Lorsque
     vous supprimez la référence du clone enfant dans l'instantané parent, vous
     « aplatissez » (fusionnez) l'image en copiant les informations de
     l'instantané sur le clone. Le temps nécessaire à la fusion d'un clone
     augmente avec la taille de l'instantané. Pour supprimer un instantané,
     vous devez d'abord fusionner les images enfant.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Comme une image fusionnée contient toutes les informations de
      l'instantané, elle occupe plus d'espace de stockage qu'un clone en
      couches.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Miroirs d'image RBD</title>

  <para>
   Les images RBD peuvent être mises en miroir de manière asynchrone entre deux
   grappes Ceph. Cette fonctionnalité est disponible en deux modes :
  </para>

  <variablelist>
   <varlistentry>
    <term>Mode basé sur un journal</term>
    <listitem>
     <para>
      Ce mode utilise la fonctionnalité de journalisation de l'image RBD afin
      de garantir une réplication ponctuelle, cohérente entre les grappes en
      cas de panne. Chaque écriture dans l'image RBD est d'abord enregistrée
      dans le journal associé avant de réellement modifier l'image. La grappe
      <literal>remote</literal> lira le journal et relira les mises à jour de
      sa copie locale de l'image. Étant donné que chaque écriture dans l'image
      RBD entraîne deux écritures dans la grappe Ceph, attendez-vous à ce que
      les temps de latence en écriture soient pratiquement multipliés par deux
      lorsque vous utilisez la fonctionnalité de journalisation de l'image RBD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mode basé sur des instantanés</term>
    <listitem>
     <para>
      Ce mode utilise des instantanés en miroir d'image RBD planifiés ou créés
      manuellement pour répliquer des images RBD cohérentes entre les grappes
      en cas de panne. La grappe <literal>remote</literal> détermine toutes les
      mises à jour de données ou de métadonnées entre deux instantanés-miroir
      et copie les différences dans sa copie locale de l'image. La
      fonctionnalité d'image RBD fast-diff permet de calculer rapidement les
      blocs de données mis à jour sans devoir analyser toute l'image RBD. Étant
      donné que ce mode n'est pas cohérent à un moment donné, la différence de
      l'instantané complet devra être synchronisée avant d'être utilisée
      pendant un scénario de basculement. Toutes les différences d'instantanés
      partiellement appliquées sont restaurées vers l'état du dernier
      instantané entièrement synchronisé avant utilisation.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   La mise en miroir est configurée réserve par réserve au sein des grappes
   homologues. Elle peut être configurée sur un sous-ensemble spécifique
   d'images dans la réserve ou configurée pour mettre en miroir automatiquement
   toutes les images d'une réserve lorsque vous utilisez la mise en miroir
   basée sur le journal uniquement. La mise en miroir est configurée à l'aide
   de la commande <command>rbd</command>. Le daemon
   <systemitem class="daemon">rbd-mirror</systemitem> est chargé d'extraire les
   mises à jour de l'image de la grappe homologue <literal>remote</literal>
   (distante) et de les appliquer à l'image dans la grappe
   <literal>local</literal> (locale).
  </para>

  <para>
   Selon les besoins de réplication souhaités, la mise en miroir RBD peut être
   configurée pour une réplication unidirectionnelle ou bidirectionnelle :
  </para>

  <variablelist>
   <varlistentry>
    <term>Réplication unidirectionnelle</term>
    <listitem>
     <para>
      Lorsque les données sont mises en miroir uniquement à partir d'une grappe
      primaire vers une grappe secondaire, le daemon
      <systemitem class="daemon">rbd-mirror</systemitem> s'exécute uniquement
      sur la grappe secondaire.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Réplication bidirectionnelle</term>
    <listitem>
     <para>
      Lorsque les données sont mises en miroir à partir des images primaires
      sur une grappe vers des images non primaires sur une autre grappe (et
      inversement), le daemon
      <systemitem class="daemon">rbd-mirror</systemitem> s'exécute sur les deux
      grappes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Chaque instance du daemon
    <systemitem class="daemon">rbd-mirror</systemitem> doit pouvoir se
    connecter simultanément aux grappes Ceph locales (<literal>local</literal>)
    et distantes (<literal>remote</literal>). Par exemple, tous les hôtes du
    moniteur et OSD. En outre, le réseau doit disposer de suffisamment de bande
    passante entre les deux centres de données pour gérer le workload en
    miroir.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configuration de la réserve</title>
   <para>
    Les procédures suivantes montrent comment effectuer les tâches
    d'administration de base pour configurer la mise en miroir à l'aide de la
    commande <command>rbd</command>. La mise en miroir est configurée réserve
    par réserve au sein des grappes Ceph.
   </para>
   <para>
    Vous devez effectuer les étapes de configuration de la réserve sur les deux
    grappes homologues. Ces procédures supposent que deux grappes, nommées
    <literal>local</literal> et <literal>remote</literal>, sont accessibles
    depuis un seul hôte pour plus de clarté.
   </para>
   <para>
    Reportez-vous à la page de manuel <command>rbd</command> (<command>man 8
    rbd</command>) pour plus de détails sur la procédure de connexion à des
    grappes Ceph différentes.
   </para>
   <tip>
    <title>grappes multiples</title>
    <para>
     Le nom de la grappe dans les exemples suivants correspond à un fichier de
     configuration Ceph du même nom <filename>/etc/ceph/remote.conf</filename>
     et à un fichier de trousseau de clés Ceph du même nom
     <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Activation de la mise en miroir sur une réserve</title>
    <para>
     Pour activer la mise en miroir sur une grappe, indiquez la sous-commande
     <command>mirror pool enable</command>, le nom de la réserve et le mode de
     mise en miroir. Le mode de mise en miroir peut être pool ou image :
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Toutes les images de la réserve dont la fonctionnalité de
        journalisation est activée sont mises en miroir.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        La mise en miroir doit être explicitement activée sur chaque image.
        Pour plus d'informations, reportez-vous à la
        <xref linkend="rbd-mirror-enable-image-mirroring"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Désactivation de la mise en miroir</title>
    <para>
     Pour désactiver la mise en miroir sur une grappe, indiquez la
     sous-commande <command>mirror pool disable</command> et le nom de la
     réserve. Lorsque la mise en miroir est désactivée sur une réserve de cette
     manière, la mise en miroir est également désactivée sur toutes les images
     (dans la réserve) pour lesquelles la mise en miroir a été explicitement
     activée.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Démarrage des homologues</title>
    <para>
     Pour que le daemon <systemitem class="daemon">rbd-mirror</systemitem>
     découvre sa grappe homologue, l'homologue doit être enregistré dans la
     réserve et un compte utilisateur doit être créé. Ce processus peut être
     automatisé avec <command>rbd</command> et les commandes <command>mirror
     pool peer bootstrap create</command> ainsi que<command> mirror pool peer
     bootstrap import</command>.
    </para>
    <para>
     Pour créer manuellement un nouveau jeton de démarrage avec
     <command>rbd</command>, spécifiez la commande <command>mirror pool peer
     bootstrap create</command>, un nom de réserve, ainsi qu'un nom de site
     convivial facultatif pour décrire la grappe <literal>local</literal> :
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     La sortie de la commande <command>mirror pool peer bootstrap
     create</command> sera un jeton qui doit être fourni à la commande
     <command>mirror pool peer bootstrap import</command>. Par exemple, sur la
     grappe <literal>local</literal> :
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Pour importer manuellement le jeton de démarrage créé par une autre grappe
     avec la commande <command>rbd</command>, utilisez la syntaxe suivante :
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Où :
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        Nom facultatif convivial du site pour décrire la grappe
        <literal>local</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Direction de la mise en miroir. La valeur par défaut est
        <literal>rx-tx</literal> pour la mise en miroir bidirectionnelle, mais
        peut également être définie sur <literal>rx-only</literal> pour la mise
        en miroir unidirectionnelle.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        Nom de la réserve.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        Chemin du fichier pour accéder au jeton créé (ou <literal>-</literal>
        pour le lire à partir de l'entrée standard).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple, sur la grappe <literal>remote</literal> :
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Ajout manuel d'un homologue de grappe</title>
    <para>
     Au lieu de démarrer des homologues comme décrit à la
     <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>, vous pouvez spécifier
     des homologues manuellement. Le daemon
     <systemitem class="daemon">rbd-mirror</systemitem> distant doit accéder à
     la grappe locale pour effectuer la mise en miroir. Créez un nouvel
     utilisateur Ceph local que le daemon
     <systemitem class="daemon">rbd-mirror</systemitem> distant utilisera, par
     exemple, <literal>rbd-mirror-peer</literal> :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Utilisez la syntaxe suivante pour ajouter une grappe Ceph homologue en
     miroir avec la commande <command>rbd</command> :
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Par défaut, le daemon <systemitem class="daemon">rbd-mirror</systemitem>
     doit avoir accès au fichier de configuration Ceph situé à l'emplacement
     <filename>/etc/ceph/.<replaceable>NOM_GRAPPE</replaceable>.conf</filename>.
     Il fournit les adresses IP des instances MON de la grappe homologue et un
     trousseau de clés pour un client nommé
     <replaceable>NOM_CLIENT</replaceable> situé dans les chemins de recherche
     par défaut ou personnalisés du trousseau de clés, par exemple
     <filename>/etc/ceph/<replaceable>NOM_GRAPPE</replaceable>.<replaceable>NOM_CLIENT</replaceable>.keyring</filename>.
    </para>
    <para>
     Sinon, l'instance MON et/ou la clé du client de la grappe homologue peut
     être stockée en toute sécurité dans la zone de stockage locale config-key
     de Ceph. Pour spécifier les attributs de connexion de la grappe homologue
     lors de l'ajout d'un homologue en miroir, utilisez les options
     <option>--remote-mon-host</option> et <option>--remote-key-file</option>.
     Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Suppression d'un homologue de grappe</title>
    <para>
     Pour supprimer une grappe homologue de mise en miroir, indiquez la
     sous-commande <command>mirror pool peer remove</command>, le nom de la
     réserve et l'UUID de l'homologue (disponible dans le résultat de la
     commande <command>rbd mirror pool info</command>) :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Réserves de données</title>
    <para>
     Lors de la création d'images dans la grappe cible,
     <systemitem class="daemon">rbd-mirror</systemitem> sélectionne une réserve
     de données comme suit :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Si une réserve de données par défaut est configurée pour la grappe cible
       (avec l'option de configuration <option>rbd_default_data_pool</option>),
       cette réserve sera utilisée.
      </para>
     </listitem>
     <listitem>
      <para>
       Dans le cas contraire, si l'image source utilise une réserve de données
       distincte et qu'une réserve portant le même nom existe sur la grappe
       cible, cette réserve est utilisée.
      </para>
     </listitem>
     <listitem>
      <para>
       Si aucune des conditions ci-dessus n'est vraie, aucune réserve de
       données n'est configurée.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configuration de l'image RBD</title>
   <para>
    Contrairement à la configuration de réserve, la configuration d'image ne
    doit être effectuée que par rapport à une seule grappe Ceph homologue de
    mise en miroir.
   </para>
   <para>
    Les images RBD en miroir sont désignées comme étant soit
    <emphasis>primaires</emphasis>, soit <emphasis>non primaires</emphasis>. Il
    s'agit d'une propriété de l'image et non pas de la réserve. Les images qui
    sont désignées comme non primaires ne sont pas modifiables.
   </para>
   <para>
    Les images sont automatiquement promues au rang d'images primaires lorsque
    la mise en miroir est activée pour la première fois sur une image (soit
    implicitement si le mode de mise en miroir de la réserve était « pool » et
    que la fonctionnalité de journalisation de l'image a été activée, soit
    explicitement – reportez-vous à la
    <xref linkend="rbd-mirror-enable-image-mirroring"/> – à l'aide de la
    commande <command>rbd</command>).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Activation de la mise en miroir d'images</title>
    <para>
     Si la mise en miroir est configurée en mode <literal>image</literal>, il
     est nécessaire d'activer explicitement la mise en miroir pour chaque image
     de la réserve. Pour activer la mise en miroir d'une image en particulier
     avec la commande <command>rbd</command>, indiquez la sous-commande
     <command>mirror image enable</command> ainsi que le nom de la réserve et
     le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Le mode d'image en miroir peut être <literal>journal</literal> ou
     <literal>snapshot</literal> :
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (valeur par défaut)</term>
      <listitem>
       <para>
        Lorsqu'elle est configurée en mode <literal>journal</literal>, la mise
        en miroir utilise la fonctionnalité de journalisation de l'image RBD
        pour répliquer le contenu de l'image. Si la fonction de journalisation
        de l'image RBD n'est pas encore activée sur l'image, elle sera activée
        automatiquement.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        Lorsqu'elle est configurée en mode <literal>snapshot</literal>
        (instantané), la mise en miroir utilise des instantanés-miroir de
        l'image RBD pour répliquer le contenu de l'image. Une fois activé, un
        instantané-miroir initial est automatiquement créé. Des
        instantanés-miroir de l'image RBD supplémentaires peuvent être créés à
        l'aide de la commande <command>rbd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Activation de la fonctionnalité de journalisation des images</title>
    <para>
     La mise en miroir RBD utilise la fonctionnalité de journalisation RBD pour
     garantir que l'image répliquée est préservée en cas de panne. Lorsque vous
     utilisez le mode de mise en miroir <literal>image</literal>, la
     fonctionnalité de journalisation est automatiquement activée si la mise en
     miroir est activée sur l'image. Lorsque vous utilisez le mode de mise en
     miroir <literal>pool</literal>, avant qu'une image puisse être mise en
     miroir sur une grappe homologue, la fonction de journalisation d'image RBD
     doit être activée. La fonctionnalité peut être activée au moment de la
     création de l'image en indiquant l'option <option>--image-feature
     exclusive-lock,journaling</option> dans la commande
     <command>rbd</command>.
    </para>
    <para>
     Le cas échéant, la fonction de journalisation peut être dynamiquement
     activée sur des images RBD préexistantes. Pour activer la journalisation,
     indiquez la sous-commande <command>feature enable</command>, le nom de la
     réserve et de l'image, et le nom de l'entité :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>dépendance des options</title>
     <para>
      La fonctionnalité <option>journaling</option> dépend de la fonctionnalité
      <option>exclusive-lock</option>. Si la fonctionnalité
      <option>exclusive-lock</option> n'est pas encore activée, vous devez
      l'activer avant la fonctionnalité <option>journaling</option>.
     </para>
    </note>
    <tip>
     <para>
      Vous pouvez activer la journalisation sur toutes les nouvelles images par
      défaut en ajoutant <option>rbd default features =
      layering,exclusive-lock,object-map,deep-flatten,journaling</option> à
      votre fichier de configuration Ceph.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Création d'instantanés-miroir d'images</title>
    <para>
     Lors de l'utilisation de la mise en miroir basée sur des instantanés, des
     instantanés-miroir doivent être créés chaque fois que vous souhaitez
     mettre en miroir le contenu modifié de l'image RBD. Pour créer
     manuellement un instantané-miroir à l'aide de la commande
     <command>rbd</command>, spécifiez la commande <command>mirror image
     snapshot</command> ainsi que le nom de la réserve et de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Par défaut, seuls trois instantanés-miroir sont créés par image.
     L'instantané-miroir le plus récent est automatiquement nettoyé si la
     limite est atteinte. La limite peut être remplacée par l'option de
     configuration <option>rbd_mirroring_max_mirroring_snapshots</option> si
     nécessaire. En outre, les instantanés-miroir sont automatiquement
     supprimés lors du retrait de l'image ou de la désactivation de la mise en
     miroir.
    </para>
    <para>
     Des instantanés-miroir peuvent également être créés automatiquement à
     intervalles réguliers si des planifications d'instantanés-miroir sont
     définies. L'instantané-miroir peut être planifié de manière globale, par
     réserve ou par image. Plusieurs planifications d'instantanés-miroir
     peuvent être définies à n'importe quel niveau, mais seules les
     planifications d'instantanés les plus spécifiques qui correspondent à une
     image en miroir individuelle seront exécutées.
    </para>
    <para>
     Pour créer une planification d'instantanés-miroir avec
     <command>rbd</command>, spécifiez la commande <command>mirror snapshot
     schedule add</command> ainsi qu'un nom de réserve ou d'image, un
     intervalle et une heure de début facultative.
    </para>
    <para>
     L'intervalle peut être spécifié en jours, heures ou minutes respectivement
     à l'aide des suffixes <option>d</option>, <option>h</option> ou
     <option>m</option>. L'heure de début facultative peut être spécifiée à
     l'aide du format d'heure ISO 8601. Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Pour supprimer une planification d'instantané-miroir avec
     <command>rbd</command>, spécifiez la commande <command>mirror snapshot
     schedule remove</command> avec des options qui correspondent à la commande
     d'ajout de planification correspondante.
    </para>
    <para>
     Pour répertorier toutes les planifications d'instantanés d'un niveau
     spécifique (global, réserve ou image) avec la commande
     <command>rbd</command>, spécifiez la commande <command>mirror snapshot
     schedule ls</command> avec un nom facultatif de réserve ou d'image. En
     outre, l'option <option>--recursive</option> peut être spécifiée pour
     répertorier toutes les planifications du niveau spécifié et des niveaux
     inférieurs. Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Pour savoir quand les prochains instantanés seront créés pour les images
     RBD en miroir basées sur des instantanés avec <command>rbd</command>,
     spécifiez la commande <command>mirror snapshot schedule status</command>
     ainsi qu'un nom de réserve ou d'image facultatif. Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Désactivation de la mise en miroir d'images</title>
    <para>
     Pour désactiver la mise en miroir d'une image en particulier, indiquez la
     sous-commande <command>mirror image disable</command> avec le nom de la
     réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Promotion et rétrogradation d'images</title>
    <para>
     Dans un scénario de basculement où la désignation principale doit être
     déplacée sur l'image dans la grappe homologue, vous devez arrêter l'accès
     à l'image primaire, rétrograder l'image primaire actuelle, promouvoir la
     nouvelle image primaire et reprendre l'accès à l'image sur la grappe
     alternative.
    </para>
    <note>
     <title>promotion forcée</title>
     <para>
      La promotion peut être forcée à l'aide de l'option
      <option>--force</option>. La promotion forcée est nécessaire lorsque la
      rétrogradation ne peut pas être propagée à la grappe homologue (par
      exemple, en cas d'échec de la grappe ou de panne de communication). Cela
      se traduira par un scénario de divergence entre les deux homologues, et
      l'image ne sera plus synchronisée jusqu'à l'émission de la sous-commande
      <command>resync</command>.
     </para>
    </note>
    <para>
     Pour rétrograder une image non primaire spécifique, indiquez la
     sous-commande <command>mirror image demote</command> ainsi que le nom de
     la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Pour rétrograder toutes les images primaires, indiquez la sous-commande
     <command>mirror image demote</command> ainsi que le nom de la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Pour promouvoir une image spécifique au rang d'image primaire, indiquez la
     sous-commande <command>mirror image promote</command> ainsi que le nom de
     la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Pour promouvoir toutes les images non primaires d'une réserve au rang
     d'images primaires, indiquez la sous-commande <command>mirror image
     promote</command> ainsi que le nom de la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>division de la charge d'E/S</title>
     <para>
      Comme l'état primaire ou non primaire s'applique au niveau de l'image, il
      est possible que deux grappes divisent le chargement des E/S et le
      basculement ou la restauration par phases.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Resynchronisation forcée de l'image</title>
    <para>
     Si le daemon <systemitem class="daemon">rbd-mirror</systemitem> détecte un
     événement de divergence, il n'y aura pas de tentative de mettre en miroir
     l'image concernée jusqu'à ce que celle-ci soit corrigée. Pour reprendre la
     mise en miroir d'une image, commencez par rétrograder l'image jugée
     obsolète, puis demandez une resynchronisation avec l'image principale.
     Pour demander une resynchronisation de l'image, indiquez la sous-commande
     <command>mirror image resync</command> avec le nom de la réserve et le nom
     de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Vérification de l'état du miroir</title>
   <para>
    L'état de réplication de la grappe homologue est stocké pour chaque image
    en miroir principale. Cet état peut être récupéré à l'aide des
    sous-commandes <command>mirror image status</command> et <command>mirror
    pool status</command> :
   </para>
   <para>
    Pour demander l'état de l'image miroir, indiquez la sous-commande
    <command>mirror image status</command> avec le nom de la réserve et le nom
    de l'image :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Pour demander l'état du résumé de la réserve miroir, indiquez la
    sous-commande <command>mirror pool status</command> avec le nom de la
    réserve :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     L'option <option>--verbose</option> de la sous-commande <command>mirror
     pool status</command> permet d'afficher des informations détaillées sur
     l'état de chaque image de mise en miroir présente dans la réserve.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Paramètres de cache</title>

  <para>
   L'implémentation de l'espace utilisateur du périphérique de bloc Ceph
   (<systemitem>librbd</systemitem>) ne peut pas profiter du cache de page
   Linux. Il comprend donc son propre caching en mémoire. Le caching RBD se
   comporte comme le caching de disque dur. Lorsque le système d'exploitation
   envoie une demande de barrière ou de vidage, toutes les données altérées
   (« dirty ») sont écrites sur l'OSD. Cela signifie que l'utilisation du
   caching à écriture différée est tout aussi sûre que celle d'un disque dur
   physique correct avec une machine virtuelle qui envoie correctement des
   demandes de vidage. Le cache utilise un algorithme <emphasis>Moins récemment
   utilisée</emphasis> (LRU) et peut, en mode d'écriture différée, fusionner
   les demandes adjacentes pour un meilleur débit.
  </para>

  <para>
   Ceph prend en charge le caching à écriture différée pour RBD. Pour
   l'activer, exécutez
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Par défaut, <systemitem>librbd</systemitem> n'effectue aucun caching. Les
   écritures et les lectures sont envoyées directement à la grappe de stockage,
   et les écritures ne reviennent que lorsque les données sont sur disque sur
   toutes les répliques. Lorsque le caching est activé, les écritures
   reviennent immédiatement sauf si le volume d'octets non vidés est supérieur
   à celui défini par l'option <option>rbd cache max dirty</option>. Dans un
   tel cas, l'écriture déclenche l'écriture différée et les blocs jusqu'à ce
   que suffisamment d'octets soient vidés.
  </para>

  <para>
   Ceph prend en charge le caching à écriture immédiate pour RBD. Vous pouvez
   définir la taille du cache ainsi que des objectifs et des limites pour
   passer du caching à écriture différée au caching à écriture immédiate. Pour
   activer le mode d'écriture immédiate, exécutez
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Cela signifie que les écritures ne reviennent que lorsque les données sont
   sur disque sur toutes les répliques, mais que les lectures peuvent provenir
   du cache. Le cache est en mémoire sur le client, et chaque image RBD a son
   propre cache. Étant donné que le cache est en local sur le client, il n'y a
   pas de cohérence s'il y a d'autres accès à l'image. L'exécution de GFS ou
   d'OCFS sur RBD ne fonctionnera pas avec le caching activé.
  </para>

  <para>
   Les paramètres suivants affectent le comportement des périphériques de bloc
   RADOS. Pour les définir, utilisez la catégorie <literal>client</literal> :
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Permet d'activer le caching pour le périphérique de bloc RADOS (RBD). La
      valeur par défaut est « true ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      Taille du cache RBD en octets. La valeur par défaut est 32 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      Limite « dirty » en octets à laquelle le cache déclenche l'écriture
      différée. <option>rbd cache max dirty</option> doit être inférieur à
      <option>rbd cache size</option>. Si la valeur est définie sur 0, le
      caching à écriture immédiate est utilisé. La valeur par défaut est 24 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      Valeur « dirty target » avant que le cache commence à écrire des données
      sur le stockage de données. Ne bloque pas les écritures dans le cache. La
      valeur par défaut est 16 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Temps en secondes pendant lequel les données altérées sont dans le cache
      avant le début de l'écriture différée. La valeur par défaut est 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Indique de commencer en mode d'écriture immédiate et de passer à
      l'écriture différée après la réception de la première demande de vidage.
      Cette configuration classique est judicieuse lorsque les machines
      virtuelles qui s'exécutent sur <systemitem>rbd</systemitem> sont trop
      anciennes pour envoyer des vidages (par exemple, le pilote virtio dans
      Linux avant le kernel 2.6.32). La valeur par défaut est « true ».
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>Paramètres QoS</title>

  <para>
   En règle générale, la qualité de service (QoS) fait référence aux méthodes
   de priorisation du trafic et de réservation des ressources. Elle est
   particulièrement importante pour le transport du trafic avec des exigences
   spéciales.
  </para>

  <important>
   <title>non pris en charge par iSCSI</title>
   <para>
    Les paramètres QoS suivants sont utilisés uniquement par l'implémentation
    RBD de l'espace utilisateur <systemitem class="daemon">librbd</systemitem>
    et <emphasis>non</emphasis> par l'implémentation
    <systemitem>kRBD</systemitem>. Étant donné qu'iSCSI utilise
    <systemitem>kRBD</systemitem>, il n'emploie pas les paramètres QoS.
    Toutefois, pour iSCSI, vous pouvez configurer la qualité de service sur la
    couche des périphériques de bloc du kernel à l'aide des fonctionnalités
    standard du kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations d'E/S par seconde. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée d'octets en E/S par seconde. La valeur par défaut est 0
      (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations de lecture par seconde. La valeur par
      défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations d'écriture par seconde. La valeur par
      défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des octets en lecture par seconde. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des octets en écriture par seconde. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations d'E/S. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en E/S. La valeur par défaut est 0
      (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations de lecture. La valeur par
      défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations d'écriture. La valeur par
      défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en lecture. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en écriture. La valeur par défaut
      est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      Cycle d'horloge de planification minimal (en millisecondes) pour la
      qualité de service. La valeur par défaut est 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Paramètres de la lecture anticipée</title>

  <para>
   Le périphérique de bloc RADOS prend en charge la lecture anticipée/la
   prérécupération pour optimiser les petites lectures séquentielles. Ces
   opérations devraient normalement être gérées par le système d'exploitation
   invité dans le cas d'une machine virtuelle, mais les chargeurs de démarrage
   peuvent ne pas émettre des lectures efficaces. La lecture anticipée est
   automatiquement désactivée si le caching est désactivé.
  </para>

  <important>
   <title>non pris en charge par iSCSI</title>
   <para>
    Les paramètres de lecture anticipée suivants sont utilisés uniquement par
    l'implémentation RBD de l'espace utilisateur
    <systemitem class="daemon">librbd</systemitem> et <emphasis>non</emphasis>
    par l'implémentation <systemitem>kRBD</systemitem>. Étant donné qu'iSCSI
    utilise <systemitem>kRBD</systemitem>, il n'emploie pas les paramètres de
    lecture anticipée. Toutefois, pour iSCSI, vous pouvez configurer la lecture
    anticipée sur la couche des périphériques de bloc du kernel à l'aide des
    fonctionnalités standard du kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Nombre de demandes de lecture séquentielle nécessaires pour déclencher la
      lecture anticipée. La valeur par défaut est 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Taille maximale d'une demande de lecture anticipée. Lorsque la valeur
      est 0, la lecture anticipée est désactivée. La valeur par défaut est
      512 Ko.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Après la lecture de tous ces octets à partir d'une image RBD, la lecture
      anticipée est désactivée pour cette image jusqu'à ce qu'elle soit fermée.
      Cela permet à l'OS invité de prendre en charge la lecture anticipée quand
      il est démarré. Lorsque la valeur est 0, la lecture anticipée reste
      activée. La valeur par défaut est 50 Mo.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Fonctions avancées</title>

  <para>
   Le périphérique de bloc RADOS prend en charge les fonctions avancées qui
   améliorent la fonctionnalité des images RBD. Vous pouvez spécifier les
   fonctions sur la ligne de commande lors de la création d'une image RBD ou
   dans le fichier de configuration Ceph à l'aide de l'option
   <option>rbd_default_features</option>.
  </para>

  <para>
   Vous pouvez spécifier les valeurs de l'option
   <option>rbd_default_features</option> de deux façons :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Comme une somme de valeurs internes des fonctions. Chaque fonction a sa
     propre valeur interne, par exemple 1 pour « layering » et 16 pour
     « fast-diff ». Par conséquent, pour activer ces deux fonctions par défaut,
     incluez la ligne suivante :
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     Comme une liste de fonctions séparées par des virgules. L'exemple
     précédent se présentera comme suit :
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>fonctions non prises en charge par iSCSI</title>
   <para>
    Les images RBD avec les fonctions suivantes ne seront pas prises en charge
    par iSCSI : <option>deep-flatten</option>, <option>object-map</option>,
    <option>journaling</option>, <option>fast-diff</option> et
    <option>striping</option>.
   </para>
  </note>

  <para>
   Voici une liste de fonctions RBD avancées :
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      La création de couches, ou superposition (layering), permet d'utiliser le
      clonage.
     </para>
     <para>
      La valeur interne est 1, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      La segmentation (striping) propage les données sur plusieurs objets et
      contribue au parallélisme pour les workloads séquentiels de
      lecture/écriture. Elle empêche les goulots d'étranglement de noeud unique
      pour les périphériques de bloc RADOS volumineux ou fort occupés.
     </para>
     <para>
      La valeur interne est 2, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Lorsque cette fonction est activée, il faut qu'un client obtienne un
      verrouillage sur un objet avant d'effectuer une écriture. Activez le
      verrouillage exclusif uniquement lorsqu'un seul client accède à une image
      en même temps. La valeur interne est 4. La valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      La prise en charge de l'assignation d'objet dépend de la prise en charge
      du verrouillage exclusif. Les périphériques de bloc sont provisionnés
      dynamiquement, ce qui signifie qu'ils ne stockent que les données qui
      existent réellement. La prise en charge de l'assignation d'objet permet
      de suivre quels objets existent réellement (ont des données stockées sur
      un disque). L'activation de la prise en charge de l'assignation d'objet
      permet d'accélérer les opérations d'E/S pour le clonage, l'importation et
      l'exportation d'une image peu peuplée, et pour la suppression.
     </para>
     <para>
      La valeur interne est 8, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      La prise en charge de la fonction fast-diff dépend de la prise en charge
      de l'assignation d'objet et du verrouillage exclusif. Elle ajoute une
      propriété à l'assignation d'objet, ce qui la rend beaucoup plus rapide
      pour générer des différentiels entre les instantanés d'une image et
      l'utilisation réelle des données d'un instantané.
     </para>
     <para>
      La valeur interne est 16, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      La fonction deep-flatten rend <command>rbd flatten</command> (voir la
      <xref linkend="rbd-flatten-cloned-image"/>) opérationnel sur tous les
      instantanés d'une image, en plus de l'image elle-même. Sans elle, les
      instantanés d'une image s'appuieront toujours sur le parent, et vous ne
      pourrez pas supprimer l'image parent avant que les instantanés soient
      supprimés. La fonction deep-flatten rend un parent indépendant de ses
      clones, même s'ils ont des instantanés.
     </para>
     <para>
      La valeur interne est 32, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      La prise en charge de la fonction de journalisation (journaling) dépend
      de la prise en charge du verrouillage exclusif. La journalisation
      enregistre toutes les modifications d'une image dans l'ordre où elles se
      produisent. La mise en miroir RBD (voir la
      <xref linkend="ceph-rbd-mirror"/>) utilise le journal pour répliquer une
      image cohérente sur une grappe distante en cas de panne.<literal/>
     </para>
     <para>
      La valeur interne est 64, la valeur par défaut est « no ».
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Assignation RBD à l'aide d'anciens clients de kernel</title>

  <para>
   Les anciens clients (par exemple, SLE 11 SP4) peuvent ne pas être en mesure
   d'assigner les images RBD parce qu'une grappe déployée avec SUSE Enterprise
   Storage 7 force certaines fonctions (à la fois les fonctions de niveau
   image RBD et celles de niveau RADOS) que ces anciens clients ne prennent pas
   en charge. Dans ce cas, les journaux OSD afficheront des messages semblables
   à ce qui suit :
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>la modification des types de compartiment de carte CRUSH provoque un rééquilibrage massif</title>
   <para>
    Si vous avez l'intention de commuter les types de compartiment de carte
    CRUSH « straw » et « straw2 », procédez de manière méthodique.
    Attendez-vous à un impact significatif sur la charge de la grappe, car un
    tel changement provoque un rééquilibrage massif des grappes.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Désactivez toutes les fonctions d'image RBD qui ne sont pas prises en
     charge. Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Remplacez les types de compartiment de carte CRUSH « straw2 » par
     « straw » :
    </para>
    <substeps>
     <step>
      <para>
       Enregistrez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Décompilez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Modifiez la carte CRUSH et remplacez « straw2 » par « straw ».
      </para>
     </step>
     <step>
      <para>
       Recompilez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Définissez la nouvelle carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Activation des périphériques de bloc et de Kubernetes</title>

  <para>
   Vous pouvez utiliser Ceph RBD avec Kubernetes v1.13 et versions ultérieures
   via le pilote <literal>ceph-csi</literal>. Ce pilote provisionne
   dynamiquement des images RBD pour soutenir les volumes Kubernetes et assigne
   ces images RBD en tant que périphériques de bloc (éventuellement en montant
   un système de fichiers contenu dans l'image) sur des noeuds de travail
   exécutant des pods faisant référence à un volume soutenu par RBD.
  </para>

  <para>
   Pour utiliser des périphériques de bloc Ceph avec Kubernetes, vous devez
   installer et configurer <literal>ceph-csi</literal> dans votre environnement
   Kubernetes.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> utilise les modules de kernel RBD par défaut
    qui peuvent ne pas prendre en charge tous les paramètres Ceph CRUSH ou les
    fonctions d'image RBD.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Par défaut, les périphériques de bloc Ceph utilisent la réserve RBD. Créez
     une réserve pour stocker les volumes Kubernetes. Assurez-vous que votre
     grappe Ceph est en cours d'exécution, puis créez la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Utilisez l'outil RBD pour initialiser la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Créez un nouvel utilisateur pour Kubernetes et
     <literal>ceph-csi</literal>. Exécutez la commande suivante et enregistrez
     la clé générée :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> nécessite un objet ConfigMap stocké dans
     Kubernetes pour définir les adresses de moniteur Ceph pour la grappe Ceph.
     Collectez le fsid unique de la grappe Ceph et les adresses de moniteur :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Générez un fichier <filename>csi-config-map.yaml</filename> similaire à
     l'exemple ci-dessous, en remplaçant le FSID par
     <literal>clusterID</literal> et les adresses de moniteur pour
     <literal>monitors</literal> :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Une fois généré, stockez le nouvel objet ConfigMap dans Kubernetes :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> a besoin des informations d'identification
     cephx pour communiquer avec la grappe Ceph. Générez un fichier
     <filename>csi-rbd-secret.yaml</filename> similaire à l'exemple ci-dessous,
     en utilisant l'ID utilisateur Kubernetes et la clé cephx que vous venez de
     créer :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Une fois généré, stockez le nouvel objet Secret dans Kubernetes :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Créez les objets ServiceAccount et RBAC ClusterRole/ClusterRoleBinding
     Kubernetes requis. Ces objets ne doivent pas nécessairement être
     personnalisés pour votre environnement Kubernetes et peuvent donc être
     utilisés directement à partir des fichiers YAML de déploiement
     <literal>ceph-csi</literal> :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Créez l'outil de déploiement <literal>ceph-csi</literal> et les plug-ins
     de noeud :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Par défaut, les fichiers YAML de l'outil de déploiement et du plug-in de
      noeud récupèrent la version de développement du conteneur
      <literal>ceph-csi</literal>. Les fichiers YAML doivent être mis à jour
      pour utiliser une version commerciale.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Utilisation de périphériques de bloc Ceph dans Kubernetes</title>
   <para>
    Kubernetes StorageClass définit une classe de stockage. Plusieurs objets
    StorageClass peuvent être créés pour être assignés à différents niveaux et
    fonctionnalités de qualité de service. Par exemple, NVMe par rapport aux
    réserves sur disque dur.
   </para>
   <para>
    Pour créer une classe de stockage <literal>ceph-csi</literal> assignée à la
    réserve Kubernetes créée ci-dessus, le fichier YAML suivant peut être
    utilisé, après avoir vérifié que la propriété <literal>clusterID</literal>
    correspond au FSID de votre grappe Ceph :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    <literal>PersistentVolumeClaim</literal> est une requête de ressources de
    stockage abstrait émise par un utilisateur. Le paramètre
    <literal>PersistentVolumeClaim</literal> serait alors associé à une
    ressource de pod pour provisionner un volume
    <literal>PersistentVolume</literal>, qui serait soutenu par une image de
    bloc Ceph. Un mode de volume <option>volumeMode</option> facultatif peut
    être inclus pour choisir entre un système de fichiers monté (par défaut) ou
    un volume basé sur un périphérique de bloc brut.
   </para>
   <para>
    À l'aide de <literal>ceph-csi</literal>, la spécification de
    <option>Filesystem</option> pour <option>volumeMode</option> peut prendre
    en charge les réclamations <literal>ReadWriteOnce</literal> et
    <literal>ReadOnlyMode accessMode</literal> et la spécification de
    <option>Block</option> pour <option>volumeMode</option> peut prendre en
    charge les réclamations <literal>ReadWriteOnce</literal>,
    <literal>ReadWriteMany</literal> et <literal>ReadOnlyMany
    accessMode</literal>.
   </para>
   <para>
    Par exemple, pour créer une réclamation
    <literal>PersistentVolumeClaim</literal> basée sur des blocs qui utilise la
    classe <literal>ceph-csi-based StorageClass</literal> créée ci-dessus, le
    fichier YAML suivant peut être utilisé pour demander un stockage de bloc
    brut à partir de la classe de stockage <literal>csi-rbd-sc
    StorageClass</literal> :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    L'exemple suivant illustre la liaison d'une réclamation
    <literal>PersistentVolumeClaim</literal> à une ressource de pod en tant que
    périphérique de bloc brut :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Pour créer une réclamation <literal>PersistentVolumeClaim</literal> basée
    sur le système de fichiers qui utilise la classe <literal>ceph-csi-based
    StorageClass</literal> créée ci-dessus, le fichier YAML suivant peut être
    utilisé pour demander un système de fichiers monté (soutenu par une image
    RBD) à partir de la classe de stockage <literal>csi-rbd-sc
    StorageClass</literal> :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    L'exemple suivant illustre la liaison d'une réclamation
    <literal>PersistentVolumeClaim</literal> à une ressource de pod en tant que
    système de fichiers monté :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
