<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ds_backup.xml" version="5.0" xml:id="cha-deployment-backup">
 <title>备份和恢复</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  本章说明您应当备份 Ceph 集群的哪些部分才能恢复集群功能。
 </para>
 <sect1 xml:id="backrest-ceph">
  <title>备份集群配置和数据</title>

  <sect2 xml:id="backrest-ceph-cephsalt">
   <title>备份 <systemitem class="resource">ceph-salt</systemitem> 配置</title>
   <para>
    导出集群配置。有关详细信息，请参见<xref linkend="deploy-cephadm-configure-export"/>。
   </para>
  </sect2>

  <sect2 xml:id="backup-ceph">
   <title>备份 Ceph 配置</title>
   <para>
    备份 <filename>/etc/ceph</filename> 目录。该目录包含至关重要的集群配置。例如，当您需要替换管理节点时，就需要备份 <filename>/etc/ceph</filename>。
   </para>
  </sect2>

  <sect2 xml:id="sec-deployment-backup-salt">
   <title>备份 Salt 配置</title>
   <para>
    您需要备份 <filename>/etc/salt/</filename> 目录。该目录包含 Salt 配置文件，例如 Salt 主控端密钥和已接受的客户端密钥。
   </para>
   <para>
    从严格意义上来说，备份管理节点并不需要备份 Salt 文件，但这些文件能够简化 Salt 集群的重新部署。如果不备份这些文件，就需要在新管理节点上重新注册 Salt 受控端。
   </para>
   <note>
    <title>Salt 主控端私用密钥的安全性</title>
    <para>
     务必将 Salt 主控端私用密钥的备份存储在安全位置。Salt 主控端密钥可用于操纵所有集群节点。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="backup-config-files">
   <title>备份自定义配置</title>
   <itemizedlist>
    <listitem>
     <para>
      Prometheus 数据和自定义。
     </para>
    </listitem>
    <listitem>
     <para>
      Grafana 自定义。
     </para>
    </listitem>
    <listitem>
     <para>
      手动更改 iSCSI 配置。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph 密钥。
     </para>
    </listitem>
    <listitem>
     <para>
      CRUSH 索引和 CRUSH 规则。通过运行以下命令将包含 CRUSH 规则的反编译 CRUSH 索引保存到 <filename>crushmap-backup.txt</filename> 中：
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap | crushtool -d - -o crushmap-backup.txt</screen>
    </listitem>
    <listitem>
     <para>
      Samba 网关配置。如果您使用的是单个网关，请备份 <filename>/etc/samba/smb.conf</filename>。如果您使用的是 HA 设置，还需要备份 CTDB 和 Pacemaker 配置文件。有关 Samba 网关所使用配置的详细信息，请参见<xref linkend="cha-ses-cifs"/>。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha 配置。仅当使用 HA 设置时需要备份。有关 NFS Ganesha 所使用配置的详细信息，请参见<xref linkend="cha-ceph-nfsganesha"/>。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="restore-ceph">
  <title>恢复 Ceph 节点</title>

  <para>
   从备份中恢复节点的过程就是重新安装节点，替换其配置文件，然后重新编制集群，以便重新添加替换节点。
  </para>

  <para>
   如果您需要重新部署管理节点，请参见<xref linkend="moving-saltmaster"/>。
  </para>

  <para>
   对于受控端 ，通常更容易简化重构建和重新部署。
  </para>

  <orderedlist>
   <listitem>
    <para>
     重新安装节点。有关详细信息，请参见<xref linkend="deploy-sles"/>
    </para>
   </listitem>
   <listitem>
    <para>
     安装 Salt。有关详细信息，请参见<xref linkend="deploy-salt"/>
    </para>
   </listitem>
   <listitem>
    <para>
     从备份恢复 <filename>/etc/salt</filename> 目录后，启用并重启动适用的 Salt 服务，例如：
    </para>
<screen>
<prompt>root@master # </prompt><command>systemctl</command> enable salt-master
<prompt>root@master # </prompt><command>systemctl</command> start salt-master
<prompt>root@master # </prompt><command>systemctl</command> enable salt-minion
<prompt>root@master # </prompt><command>systemctl</command> start salt-minion
</screen>
   </listitem>
   <listitem>
    <para>
     从所有受控端中删除旧 Salt 主控端节点的公共主控端密钥。
    </para>
<screen>
<prompt>root@master # </prompt><command>rm</command> /etc/salt/pki/minion/minion_master.pub
<prompt>root@master # </prompt><command>systemctl</command> restart salt-minion
</screen>
   </listitem>
   <listitem>
    <para>
     恢复管理节点的所有本地内容。
    </para>
   </listitem>
   <listitem>
    <para>
     从之前导出的 JSON 文件导入集群配置。有关更多详细信息，请参见<xref linkend="deploy-cephadm-configure-export"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     应用导入的集群配置：
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </listitem>
  </orderedlist>
 </sect1>
</chapter>
