<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>RADOS 块设备</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>是</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  一个块就是由若干字节组成的序列，例如 4 MB
  的数据块。基于块的存储接口是使用旋转媒体（例如硬盘、CD、软盘）存储数据最常见的方式。块设备接口的普及，也使得虚拟块设备成为与大量数据存储系统（例如
  Ceph）进行交互的理想选择。
 </para>
 <para>
  Ceph 块设备允许共享物理资源，并且可以调整大小。它们会在 Ceph 集群中的多个 OSD 上等量存储数据。Ceph 块设备会利用 RADOS
  功能，例如创建快照、复制和一致性。Ceph 的 RADOS 块设备 (RBD) 使用内核扩展模块或
  <systemitem>librbd</systemitem> 库与 OSD 交互。
 </para>
 <figure>
  <title>RADOS 协议</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Ceph 的块设备为内核扩展模块提供高性能及无限的可扩展性。它们支持虚拟化解决方案（例如 QEMU）或依赖于
  <systemitem class="library">libvirt</systemitem> 的基于云的计算系统（例如
  OpenStack）。您可以使用同一个集群来同时操作对象网关、CephFS 和 RADOS 块设备。
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>块设备命令</title>

  <para>
   <command>rbd</command>
   命令可让您创建、列出、内省和删除块设备映像。您还可以使用它来执行其他操作，例如，克隆映像、创建快照、将映像回滚到快照或查看快照。
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>在副本存储池中创建块设备映像</title>
   <para>
    将块设备添加到客户端之前，您需要在现有存储池中创建一个相关的映像（请参见<xref linkend="ceph-pools"/>）：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    例如，要创建名为“myimage”的 1 GB 映像，并使其将信息存储在名为“mypool”的存储池中，请执行以下命令：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>映像大小单位</title>
    <para>
     如果您省略了大小单位缩写（“G”或“T”），映像大小将以兆字节为单位。请在大小数值之后使用“G”或“T”来指定千兆字节或太字节。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>在纠删码存储池中创建块设备映像</title>
   <para>
    可以将块设备映像的数据直接存储到纠删码 (EC) 存储池中。RADOS
    块设备映像由<emphasis>数据</emphasis>和<emphasis>元数据</emphasis>两部分组成。您只能将 RADOS
    块设备映像的数据部分存储到 EC 存储池中。要执行此操作，存储池的 <option>overwrite</option> 标志需要设置为
    <emphasis>true</emphasis>，并且用于存储存储池的所有 OSD 都必须使用 BlueStore。
   </para>
   <para>
    不能将映像的元数据部分存储在 EC 存储池中。您可以使用 <command>rbd create</command> 命令的
    <option>--pool=</option> 选项指定用于存储映像元数据的副本存储池，也可以将 <option>pool/</option>
    指定为映像名称的前缀。
   </para>
   <para>
    创建 EC 存储池：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    指定用于存储元数据的副本存储池：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    或：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>列出块设备映像</title>
   <para>
    要列出名为“mypool”的存储池中的块设备，请执行以下命令：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>检索映像信息</title>
   <para>
    要从名为“mypool”的存储池内的映像“myimage”检索信息，请运行以下命令：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>调整块设备映像的大小</title>
   <para>
    RADOS 块设备映像是瘦配置 — 在您开始将数据保存到这些映像之前，它们实际上并不会使用任何物理存储。但是，这些映像具有您使用
    <option>--size</option> 选项设置的最大容量。如果您要增大（或减小）映像的最大大小，请运行以下命令：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>删除块设备映像</title>
   <para>
    要删除与“mypool”存储池内的映像“myimage”对应的块设备，请运行以下命令：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>装入和卸载</title>

  <para>
   创建 RADOS 块设备之后，便可以像任何其他磁盘设备一样使用它：进行格式化、将其装入以便能够交换文件，以及在完成时将其卸载。
  </para>

  <para>
   <command>rbd</command> 命令默认使用 Ceph <literal>admin</literal>
   用户帐户访问集群。此帐户拥有集群的完全管理访问权限。这会带来造成损害的意外风险，类似于以
   <systemitem class="username">root</systemitem> 身份登录 Linux
   工作站之类。因此，最好创建具有较少特权的用户帐户，并将这些帐户用于正常的读/写 RADOS 块设备访问。
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>创建 Ceph 用户帐户</title>
   <para>
    要创建拥有 Ceph Manager、Ceph Monitor 和 Ceph OSD 用户权限的新用户帐户，请将
    <command>ceph</command> 命令与 <command>auth get-or-create</command> 子命令一起使用：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    例如，要创建一个名为 <replaceable>qemu</replaceable> 的用户，该用户拥有对存储池
    <replaceable>vms</replaceable> 的读写访问权限以及对存储池
    <replaceable>images</replaceable> 的只读访问权限，请执行以下命令：
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    <command>ceph auth get-or-create</command> 命令的输出将是指定用户的密钥环，可将其写入
    <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>。
   </para>
   <note>
    <para>
     使用 <command>rbd</command> 命令时，可以通过提供可选的 <command>--id</command>
     <replaceable>ID</replaceable> 自变量来指定用户 ID。
    </para>
   </note>
   <para>
    有关管理 Ceph 用户帐户的更多详细信息，请参见<xref linkend="cha-storage-cephx"/>。
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>用户身份验证</title>
   <para>
    要指定用户名，请使用 <option>--id <replaceable>user-name</replaceable></option>。如果您使用
    <systemitem>cephx</systemitem> 身份验证，则还需要指定密钥。该机密可能来自密钥环，或某个包含机密的文件：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    或
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>准备 RADOS 块设备以供使用</title>
   <procedure>
    <step>
     <para>
      确保您的 Ceph 集群的存储池中包含要映射的磁盘映像。假设存储池名为 <literal>mypool</literal>，映像名为
      <literal>myimage</literal>。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      将映像映射到新的块设备：
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      列出所有映射的设备：
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      我们要使用的设备是 <filename>/dev/rbd0</filename>。
     </para>
     <tip>
      <title>RBD 设备路径</title>
      <para>
       您可以使用
       <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename>作为永久设备路径来代替
       <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>。例如：
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      在 <filename>/dev/rbd0</filename> 设备上创建 XFS 文件系统：
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      使用您的装入点替换 <filename>/mnt</filename>，装入设备并检查其是否正确装入：
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      现在，您便可以将数据移入/移出设备，就如同它是本地目录一样。
     </para>
     <tip>
      <title>增大 RBD 设备的大小</title>
      <para>
       如果您发现 RBD 设备的大小不再够用，可以轻松增大大小。
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         增大 RBD 映像的大小，例如增大到 10GB。
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         扩大文件系统以填充设备的新大小：
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      当您访问完设备后，可以将其取消映射并卸载。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>手动装入和卸载</title>
    <para>
     提供了 <command>rbdmap</command> 脚本和
     <systemitem class="daemon">systemd</systemitem> 单元，可更加顺畅地在引导后映射和装入
     RBD，并在关机前将其卸载。有关详细信息，请参见<xref linkend="ceph-rbd-rbdmap"/>。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command>：在引导时映射 RBD 设备</title>
   <para>
    <command>rbdmap</command> 是一个外壳脚本，可对一个或多个 RBD 映像自动执行 <command>rbd
    map</command> 和 <command>rbd device unmap</command>
    操作。虽然您随时都可以手动运行该脚本，但其主要优势是在引导时自动映射和装入 RBD 映像（以及在关机时卸载和取消映射），此操作由 Init
    系统触发。<systemitem>ceph-common</systemitem> 包中随附了一个
    <systemitem class="daemon">systemd</systemitem> 单元文件
    <filename>rbdmap.service</filename> 用于执行此操作。
   </para>
   <para>
    该脚本使用单个自变量，可以是 <option>map</option> 或
    <option>unmap</option>。使用任一自变量时，该脚本都会分析配置文件。它默认为
    <filename>/etc/ceph/rbdmap</filename>，但可通过环境变量
    <literal>rbdmapFILE</literal> 覆盖。该配置文件的每一行相当于一个要映射或取消映射的 RBD 映像。
   </para>
   <para>
    配置文件采用以下格式：
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       存储池中映像的路径。以
       <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>
       格式指定。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       要传递给底层 <command>rbd device map</command>
       命令的参数的可选列表。这些参数及其值应该以逗号分隔的字符串形式指定，例如：
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       该示例让 <command>rbdmap</command> 脚本运行以下命令：
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       下面的示例中介绍了如何使用相应的密钥指定用户名和密钥环：
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    以 <command>rbdmap map</command> 形式运行时，该脚本会分析配置文件，并且对于每个指定的 RBD
    映像，它会尝试先映射映像（使用 <command>rbd device map</command> 命令），再装入映像。
   </para>
   <para>
    以 <command>rbdmap unmap</command> 形式运行时，配置文件中列出的映像将卸载并取消映射。
   </para>
   <para>
    <command>rbdmap unmap-all</command> 会尝试卸载然后取消映射所有当前已映射的 RBD
    映像，而不论它们是否列在配置文件中。
   </para>
   <para>
    如果成功，<command>rbd device map</command> 操作会将映像映射到
    <filename>/dev/rbdX</filename> 设备，此时会触发 udev 规则，以创建易记设备名称符号链接
    <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename>，该链接指向实际映射的设备。
   </para>
   <para>
    为了装入和卸载成功，易记设备名称在 <filename>/etc/fstab</filename> 中需有对应项。写入 RBD 映像的
    <filename>/etc/fstab</filename> 项时，指定“noauto”（或“nofail”）装入选项。这可防止 Init
    系统过早（尚未出现有问题的设备时）尝试装入设备，因为 <filename>rbdmap.service</filename>
    通常是在引导序列中相当靠后的时间触发。
   </para>
   <para>
    有关 <command>rbd</command> 选项的完整列表，请参见 <command>rbd</command> 手册页
    (<command>man 8 rbd</command>)。
   </para>
   <para>
    有关 <command>rbdmap</command> 用法的示例，请参见 <command>rbdmap</command> 手册页
    (<command>man 8 rbdmap</command>)。
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>增大 RBD 设备的大小</title>
   <para>
    如果您发现 RBD 设备的大小不再够用，可以轻松增大大小。
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      增大 RBD 映像的大小，例如增大到 10GB。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      扩大文件系统以填入设备的新大小。
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>快照</title>

  <para>
   RBD 快照是 RADOS 块设备映像的快照。通过快照，您可以保留映像状态的历史。Ceph 还支持快照分层，这可让您轻松快速地克隆 VM 映像。Ceph
   使用 <command>rbd</command> 命令和许多高级接口（包括
   QEMU、<systemitem>libvirt</systemitem>、OpenStack 和 CloudStack）支持块设备快照。
  </para>

  <note>
   <para>
    在创建映像的快照之前，请停止输入和输出操作，并刷新所有待处理写操作。如果映像包含文件系统，则在创建快照时，文件系统必须处于一致状态。
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>启用和配置 <systemitem>cephx</systemitem></title>
   <para>
    如果启用了 <systemitem>cephx</systemitem>，则必须指定用户名或
    ID，以及包含用户相应密钥的密钥环的路径。有关详细信息，请参见<xref linkend="cha-storage-cephx"/>。您还可以添加
    <systemitem>CEPH_ARGS</systemitem> 环境变量，以免重新输入以下参数。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     将用户和机密添加到 <systemitem>CEPH_ARGS</systemitem> 环境变量，如此您便无需每次都输入它们。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>快照基础知识</title>
   <para>
    下面的过程说明如何在命令行上使用 <command>rbd</command> 命令创建、列出和删除快照。
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>创建快照</title>
    <para>
     要使用 <command>rbd</command> 创建快照，请指定 <option>snap create</option>
     选项、存储池名称和映像名称。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>列出快照</title>
    <para>
     要列出映像的快照，请指定存储池名称和映像名称。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>回滚快照</title>
    <para>
     要使用 <command>rbd</command> 回滚快照，请指定 <option>snap rollback</option>
     选项、存储池名称、映像名称和快照名称。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      将映像回滚到快照意味着会使用快照中的数据重写当前版本的映像。执行回滚所需的时间将随映像大小的增加而延长。从快照<emphasis>克隆较快</emphasis>，而从映像到快照的<emphasis>回滚较慢</emphasis>，因此克隆是返回先前存在状态的首选方法。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>删除快照</title>
    <para>
     要使用 <command>rbd</command> 删除快照，请指定 <option>snap rm</option>
     选项、存储池名称、映像名称和用户名。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Ceph OSD 会以异步方式删除数据，因此删除快照不能立即释放磁盘空间。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>清除快照</title>
    <para>
     要使用 <command>rbd</command> 删除映像的所有快照，请指定 <option>snap purge</option>
     选项和映像名称。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>快照分层</title>
   <para>
    Ceph 支持为一个块设备快照创建多个写入时复制 (COW) 克隆的功能。快照分层可让 Ceph
    块设备客户端能够极快地创建映像。例如，您可以创建块设备映像并将 Linux VM
    写入其中，然后创建映像的快照、保护快照，并创建您所需数量的“写入时复制”克隆。快照是只读的，因此克隆快照简化了语义，如此可快速创建克隆。
   </para>
   <note>
    <para>
     下面的命令行示例中提到的“父”和“子”这两个术语是指 Ceph 块设备快照（父）和从快照克隆的相应映像（子）。
    </para>
   </note>
   <para>
    每个克隆的映像（子）都存储了对其父映像的引用，这可让克隆的映像打开父快照并读取其内容。
   </para>
   <para>
    快照的 COW 克隆的行为方式与任何其他 Ceph
    块设备映像完全相同。可针对克隆的映像执行读取、写入、克隆和调整大小操作。系统对克隆的映像没有特殊限制。但是，快照的写入时复制克隆会引用快照，因此您<emphasis>必须</emphasis>在克隆快照之前保护快照。
   </para>
   <note>
    <title>不支持 <option>--image-format 1</option></title>
    <para>
     您无法为通过弃用的 <command>rbd create --image-format 1</command> 选项创建的映像创建快照。Ceph
     仅支持克隆默认的 <emphasis>format 2</emphasis> 映像。
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>分层入门</title>
    <para>
     Ceph 块设备分层是一个简单的过程。您必须有一个映像。您必须创建映像的快照。您必须保护快照。在您执行这些步骤之后，就可以开始克隆快照了。
    </para>
    <para>
     克隆的映像具有对父快照的引用，并且包含存储池 ID、映像 ID 和快照 ID。包含存储池 ID
     意味着您可以将快照从一个存储池克隆到另一个存储池中的映像。
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>映像模板</emphasis>：一种常见的块设备分层用例是创建主映像和用作克隆模板的快照。例如，用户可为 Linux
       发行套件（如 SUSE Linux Enterprise Server）创建映像并为它创建快照。用户可以定期更新映像和创建新的快照（例如，先执行
       <command>zypper ref &amp;&amp; zypper patch</command>，接着执行 <command>rbd
       snap create</command>）。随着映像日趋成熟，用户可以克隆任何一个快照。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>扩展模板</emphasis>：更高级的用例包括扩展比基本映像提供的信息更多的模板映像。例如，用户可以克隆映像（VM
       模板）并安装其他软件（例如，数据库、内容管理系统或分析系统），然后创建扩展映像的快照，这个扩展映像可以如基本映像一样更新。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>模板池</emphasis>：使用块设备分层的一种方法是创建包含主映像（用作模板）的池，然后创建这些模板的快照。之后，您便可以扩大用户的只读特权，使他们可以克隆快照，却不能写入存储池或在存储池中执行。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>映像迁移/恢复</emphasis>：使用块设备分层的一种方法是将数据从一个存储池迁移或恢复到另一个存储池。
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>保护快照</title>
    <para>
     克隆会访问父快照。如果用户意外删除了父快照，则所有克隆都会损坏。为了防止数据丢失，您需要先保护快照，然后才能克隆它。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      您无法删除受保护的快照。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>克隆快照</title>
    <para>
     要克隆快照，您需要指定父存储池、映像、快照、子存储池和映像名称。您需要先保护快照，然后才能克隆它。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      您可以将快照从一个存储池克隆到另一个存储池中的映像。例如，可以在一个存储池中将只读映像和快照作为模板维护，而在另一个存储池中维护可写入克隆。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>取消保护快照</title>
    <para>
     必须先取消保护快照，然后才能删除它。另外，您<emphasis>无法</emphasis>删除克隆所引用的快照。您需要先平展快照的每个克隆，然后才能删除快照。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>列出快照的子项</title>
    <para>
     要列出快照的子项，请执行以下命令：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>平展克隆的映像</title>
    <para>
     克隆的映像会保留对父快照的引用。删除子克隆对父快照的引用时，可通过将信息从快照复制到克隆，高效“平展”映像。平展克隆所需的时间随着映像大小的增加而延长。要删除快照，必须先平展子映像。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      由于平展的映像包含快照中的所有信息，平展的映像占用的存储空间将比分层克隆多。
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>RBD 映像镜像</title>

  <para>
   RBD 映像可以在两个 Ceph 集群之间异步镜像。此功能有两种模式：
  </para>

  <variablelist>
   <varlistentry>
    <term>基于日志</term>
    <listitem>
     <para>
      此模式使用 RBD 日志映像功能来确保集群之间的复制在时间点和崩溃时保持一致。在修改实际映像之前，向 RBD
      映像的每次写入都会先记录到关联日志中。<literal>remote</literal> 集群将从日志中读取并向其本地映像副本重放更新。由于向
      RBD 映像的每次写入都将导致向 Ceph 集群的两次写入，因此在使用 RBD 日志映像功能时，预计写入延迟将增加近一倍。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>基于快照</term>
    <listitem>
     <para>
      此模式使用定期安排或手动创建的 RBD 映像镜像快照，以在集群之间复制崩溃时保持一致的 RBD
      映像。<literal>remote</literal> 集群将决定两个镜像快照之间的任何数据或元数据更新，并将增量复制到映像的本地副本。借助
      RBD fast-diff 映像功能，可以快速计算更新的数据块，而无需扫描完整的 RBD
      映像。由于此模式不确保在时间点保持一致，因此在故障转移期间使用该模式之前，需要同步完整的快照增量。任何部分应用的快照增量都将回滚到使用该模式前最后一个完全同步的快照。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   基于对等集群中的每个存储池配置镜像。可以对存储池中的特定映像子集进行配置，也可以配置为在仅使用基于日志的镜像时自动镜像存储池中的所有映像。镜像使用
   <command>rbd</command>
   命令进行配置。<systemitem class="daemon">rbd-mirror</systemitem> 守护进程负责从
   <literal>remote</literal> 对等集群提取映像更新，并将它们应用于 <literal>local</literal>
   集群中的映像。
  </para>

  <para>
   根据所需的复制需求，RBD 镜像可以配置为单向或双向复制：
  </para>

  <variablelist>
   <varlistentry>
    <term>单向复制</term>
    <listitem>
     <para>
      当数据仅会从主集群镜像到次集群时，<systemitem class="daemon">rbd-mirror</systemitem>
      守护进程仅在次集群上运行。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>双向复制</term>
    <listitem>
     <para>
      当数据从一个集群上的主映像镜像到另一个集群上的非主映像（反之亦然）时，<systemitem class="daemon">rbd-mirror</systemitem>
      守护进程将在两个集群上运行。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    <systemitem class="daemon">rbd-mirror</systemitem> 守护进程的每个实例需要能够同时连接到
    <literal>local</literal> 和 <literal>remote</literal> Ceph 集群。例如，所有 Monitor
    和 OSD 主机。此外，网络需要两个数据中心之间有足够的带宽来处理镜像工作负载。
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>存储池配置</title>
   <para>
    以下过程说明如何使用 <command>rbd</command> 命令来执行配置镜像的基本管理任务。镜像在 Ceph 集群中逐池进行配置。
   </para>
   <para>
    您需要在两个同伴集群上执行存储池配置步骤。为清楚起见，这些过程假设可从单台主机访问名为 <literal>local</literal> 和
    <literal>remote</literal> 的两个集群。
   </para>
   <para>
    有关如何连接到不同的 Ceph 集群的更多详细信息，请参见 <command>rbd</command> 手册页 (<command>man 8
    rbd</command>)。
   </para>
   <tip>
    <title>多个集群</title>
    <para>
     以下示例中的集群名称对应于同名 <filename>/etc/ceph/remote.conf</filename> 的 Ceph 配置文件以及同名
     <filename>/etc/ceph/remote.client.admin.keyring</filename> 的 Ceph 密钥环文件。
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>在存储池上启用镜像</title>
    <para>
     要针对存储池启用镜像，请指定 <command>mirror pool enable</command>
     子命令、存储池名称和镜像模式。镜像模式可以是存储池或映像：
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        将会镜像启用了日志特性的存储池中的所有映像。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        需要针对每个映像明确启用镜像。有关更多信息，请参见<xref linkend="rbd-mirror-enable-image-mirroring"/>。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>禁用镜像</title>
    <para>
     要对存储池禁用镜像，请指定 <command>mirror pool disable</command>
     子命令和存储池名称。使用这种方法对存储池禁用镜像时，还会对已为其明确启用镜像的所有映像（该存储池中）禁用镜像。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>引导对等</title>
    <para>
     为了使 <systemitem class="daemon">rbd-mirror</systemitem>
     守护进程发现其对等集群，需要将对等注册到存储池，并需要创建用户帐户。此过程可以通过
     <command>rbd</command>、<command>mirror pool peer bootstrap
     create</command> 和 <command>mirror pool peer bootstrap import</command>
     命令自动完成。
    </para>
    <para>
     要使用 <command>rbd</command> 手动创建新的引导令牌，请指定 <command>mirror pool peer
     bootstrap create</command> 命令、存储池名称以及描述 <literal>local</literal>
     集群的可选易记站点名称：
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     <command>mirror pool peer bootstrap create</command> 的输出将是应提供给
     <command>mirror pool peer bootstrap import</command> 命令的令牌。例如，在
     <literal>local</literal> 集群上：
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     要使用 <command>rbd</command> 命令手动导入另一个集群创建的引导令牌，请使用以下语法：
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     位置:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        用于描述 <literal>local</literal> 集群的可选易记站点名称。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        镜像方向。默认设为 <literal>rx-tx</literal> 进行双向镜像，但也可设为
        <literal>rx-only</literal> 进行单向镜像。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        存储池的名称。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        指向所创建令牌的文件路径（或设为 <literal>-</literal> 以通过标准输入来读取）。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     例如，在 <literal>remote</literal> 集群上：
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>手动添加集群对等</title>
    <para>
     除了按<xref linkend="ceph-rbd-mirror-bootstrap-peer"/>中所述引导对等之外，您还可以手动指定对等。远程
     <systemitem class="daemon">rbd-mirror</systemitem> 守护进程需要访问本地集群才能执行镜像。创建远程
     <systemitem class="daemon">rbd-mirror</systemitem> 守护进程将使用的新的本地 Ceph 用户，例如
     <literal>rbd-mirror-peer</literal>：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     使用以下语法通过 <command>rbd</command> 命令添加镜像对等 Ceph 集群：
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     例如：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     默认情况下，<systemitem class="daemon">rbd-mirror</systemitem> 守护进程需要有权访问位于
     <filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename>
     的 Ceph 配置文件。它提供对等集群的 MON 的 IP 地址和名为 <replaceable>CLIENT_NAME</replaceable>
     的客户端的密钥环（位于默认或自定义密钥环搜索路径中，例如
     <filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>）。
    </para>
    <para>
     或者，对等集群的 MON 和/或客户端密钥可以安全地存储在本地 Ceph config-key
     存储区中。要在添加镜像对等时指定对等集群连接属性，请使用 <option>--remote-mon-host</option> 和
     <option>--remote-key-file</option> 选项。例如：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>删除集群对等</title>
    <para>
     要删除镜像同伴集群，请指定 <command>mirror pool peer remove</command> 子命令、存储池名称和对等
     UUID（可通过 <command>rbd mirror pool info</command> 命令获得）：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>数据存储池</title>
    <para>
     在目标集群中创建映像时，<systemitem class="daemon">rbd-mirror</systemitem>
     会按如下所述选择数据存储池：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       如果目标集群配置了默认数据存储池（使用 <option>rbd_default_data_pool</option>
       配置选项），则会使用该默认数据存储池。
      </para>
     </listitem>
     <listitem>
      <para>
       否则，如果源映像使用单独的数据存储池，并且目标集群上存在同名的存储池，则将使用该存储池。
      </para>
     </listitem>
     <listitem>
      <para>
       如果以上两种情况都不成立，将不会设置数据存储池。
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>RBD 映像配置</title>
   <para>
    与存储池配置不同，映像配置只需要针对单个镜像同伴 Ceph 集群执行。
   </para>
   <para>
    系统会将镜像的 RBD
    映像指定为<emphasis>主要</emphasis>或<emphasis>非主要</emphasis>。这是映像的属性，而不是存储池的属性。不能修改指定为非主要的映像。
   </para>
   <para>
    当首次对某个映像启用镜像时（如果存储池镜像模式是“存储池”并且映像已启用日志映像功能，则为隐式启用，或可通过
    <command>rbd</command>
    命令显式启用（请参见<xref linkend="rbd-mirror-enable-image-mirroring"/>）），映像会自动升级为主要映像。
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>启用映像镜像</title>
    <para>
     如果镜像配置为使用 <literal>image</literal> 模式，则需要为存储池中的每个映像明确启用镜像。要使用
     <command>rbd</command> 为特定映像启用镜像，请指定 <command>mirror image
     enable</command> 子命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     镜像映像模式可以是 <literal>journal</literal>，也可以是 <literal>snapshot</literal>：
    </para>
    <variablelist>
     <varlistentry>
      <term>journal（默认模式）</term>
      <listitem>
       <para>
        如果配置为使用 <literal>journal</literal> 模式，镜像将使用 RBD
        日志映像功能来复制映像内容。如果尚未在映像上启用 RBD 日志映像功能，该功能将自动启用。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        如果配置为使用 <literal>snapshot</literal> 模式，镜像将使用 RBD
        映像镜像快照来复制映像内容。启用后，将自动创建初始镜像快照。可通过 <command>rbd</command> 命令创建其他 RBD
        映像镜像快照。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>启用映像日志功能</title>
    <para>
     RBD 镜像使用 RBD 日志功能来确保复制的映像始终在崩溃时保持一致状态。使用 <literal>image</literal>
     镜像模式时，如果在映像上启用了镜像，则将自动启用日志功能。使用 <literal>pool</literal> 镜像模式时，必须先启用 RBD
     映像日志功能，然后才能将映像镜像到对等集群。在创建映像时，可以通过将 <option>--image-feature
     exclusive-lock,journaling</option> 选项提供给 <command>rbd</command> 命令来启用该功能。
    </para>
    <para>
     或者，日志功能可以针对预先存在的 RBD 映像动态启用。要启用日记，请指定 <command>feature enable</command>
     子命令、存储池和映像名称以及功能名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>选项依赖性</title>
     <para>
      <option>journaling</option> 功能依赖于 <option>exclusive-lock</option> 功能。如果
      <option>exclusive-lock</option> 功能尚未启用，则您需要先启用它，再启用
      <option>journaling</option> 功能。
     </para>
    </note>
    <tip>
     <para>
      默认情况下，您可以通过将 <option>rbd default features =
      layering,exclusive-lock,object-map,deep-flatten,journaling</option> 添加到
      Ceph 配置文件，在所有新映像上启用日志功能。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>创建映像镜像快照</title>
    <para>
     使用基于快照的镜像时，每当要镜像 RBD 映像的已更改内容，都需要创建镜像快照。要使用 <command>rbd</command>
     手动创建镜像快照，请指定 <command>mirror image snapshot</command> 命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     默认情况下，每个映像只能创建三个镜像快照。如果达到此限制，将自动修剪最近的镜像快照。可以视需要通过
     <option>rbd_mirroring_max_mirroring_snapshots</option>
     配置选项覆盖该限制。此外，删除映像或禁用镜像时，会自动删除镜像快照。
    </para>
    <para>
     如果定义了镜像快照日程安排，也可以定期自动创建镜像快照。可以在全局、存储池或映像级别安排镜像快照。可以在任何级别定义多个镜像快照日程安排，但只有与单个镜像映像相匹配的最具针对性的快照日程安排才会运行。
    </para>
    <para>
     要使用 <command>rbd</command> 创建镜像快照日程安排，请指定 <command>mirror snapshot
     schedule add</command> 命令以及可选的存储池或映像名称、间隔和可选的开始时间。
    </para>
    <para>
     可以分别使用后缀 <option>d</option>、<option>h</option> 或 <option>m</option>
     指定以天、小时或分钟为单位的间隔。可使用 ISO 8601 时间格式指定可选的开始时间。例如：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     要使用 <command>rbd</command> 删除镜像快照日程安排，请指定 <command>mirror snapshot
     schedule remove</command> 命令以及与相应的添加日程安排命令相匹配的选项。
    </para>
    <para>
     要使用 <command>rbd</command> 列出特定级别（全局、存储池或映像）的所有快照日程安排，请指定 <command>mirror
     snapshot schedule ls</command> 命令以及可选的存储池或映像名称。此外，还可以指定
     <option>--recursive</option> 选项，以列出指定及以下级别的所有日程安排。例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     要获知使用 <command>rbd</command> 为基于快照的镜像 RBD 映像创建下一个快照的时间，请指定 <command>mirror
     snapshot schedule status</command> 命令以及可选的存储池或映像名称。例如：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>禁用映像镜像</title>
    <para>
     要为特定映像禁用镜像，请指定 <command>mirror image disable</command> 子命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>升级和降级映像</title>
    <para>
     在需要将主要指定移动到同伴集群中映像的故障转移情况下，您需要停止访问主要映像、降级当前主要映像、升级新的主要映像，然后继续访问替代集群上的映像。
    </para>
    <note>
     <title>强制升级</title>
     <para>
      可以使用 <option>--force</option>
      选项强制升级。降级不能传播到同伴集群时（例如，当集群发生故障或通讯中断时），就需要强制升级。这将导致两个同伴集群之间出现节点分裂情况，并且映像不再同步，直到发出了
      <command>resync</command> 子命令。
     </para>
    </note>
    <para>
     要将特定映像降级为非主要映像，请指定 <command>mirror image demote</command> 子命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     要将存储池中的所有主要映像都降级为非主要映像，请指定 <command>mirror pool demote</command>
     子命令以及存储池名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     要将特定映像升级为主要映像，请指定 <command>mirror image promote</command> 子命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     要将存储池中的所有非主要映像都升级为主要映像，请指定 <command>mirror pool promote</command>
     子命令以及存储池名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>拆分 I/O 负载</title>
     <para>
      因为主要或非主要状态是针对映像的，所以可以使用两个集群来分割 I/O 负载并进行故障转移或故障回复。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>强制映像重新同步</title>
    <para>
     如果 <systemitem class="daemon">rbd-mirror</systemitem>
     守护进程检测到分区事件，则在该情况解决之前，它不会尝试镜像受影响的映像。要继续镜像映像，请先降级确定过期的映像，然后请求与主要映像重新同步。要请求映像重新同步，请指定
     <command>mirror image resync</command> 子命令以及存储池和映像名称：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>检查镜像状态</title>
   <para>
    系统会存储每个主要镜像映像的同伴集群复制状态。此状态可使用 <command>mirror image status</command> 和
    <command>mirror pool status</command> 子命令检索：
   </para>
   <para>
    要请求镜像映像状态，请指定 <command>mirror image status</command> 子命令以及存储池和映像名称：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    要请求镜像存储池摘要状态，请指定 <command>mirror pool status</command> 子命令以及存储池名称：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     将 <option>--verbose</option> 选项添加到 <command>mirror pool status</command>
     子命令会额外地输出存储池中每个镜像映像的状态详细信息。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>缓存设置</title>

  <para>
   Ceph 块设备的用户空间实现 (<systemitem>librbd</systemitem>) 无法利用 Linux
   页面缓存。因此，它具有自己的内存中缓存。RBD 缓存的行为与硬盘缓存类似。当 OS 发送屏障或刷新请求时，所有“脏”数据都会写入
   OSD。这意味着只要虚拟机可以正确发送刷新请求，使用写回缓存与使用运行良好的物理硬盘一样安全。该缓存运用<emphasis>最久未使用</emphasis>
   (LRU) 算法，并且在写回模式下可以合并相邻请求以提高吞吐量。
  </para>

  <para>
   Ceph 支持为 RBD 提供写回缓存。要启用该功能，请运行
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   <systemitem>librbd</systemitem>
   默认不会执行任何缓存。写和读都直接到达存储集群，并且只有当数据的所有副本都写入磁盘后写操作才会返回。如果启用了缓存，写操作会立即返回，除非未刷新的字节数大于
   <option>rbd cache max dirty</option>
   选项中设置的数值。在此情况下，写操作会触发写回机制并一直阻塞，直至有足够多的字节得到刷新。
  </para>

  <para>
   Ceph 支持为 RBD 提供直写缓存。您可以设置缓存的大小，以及从写回缓存切换到直写缓存的目标和限值。要启用直写模式，请运行
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   这意味着只有当数据的所有副本都写入磁盘后写操作才会返回，但可能会从缓存中读取数据。缓存信息保存在客户端的内存中，并且每个 RBD
   都有自己的缓存。由于对客户端而言缓存位于本地，因此如果有其他客户端访问映像，不会存在缓存一致性的问题。如果启用了缓存，在 RBD 上将不能运行 GFS
   或 OCFS。
  </para>

  <para>
   以下参数会影响 RADOS 块设备的行为。要设置这些参数，请使用 <literal>client</literal> 类别：
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      对 RADOS 块设备 (RBD) 启用缓存。默认值为“true”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      RBD 缓存大小（以字节为单位）。默认值为 32 MB。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      使缓存触发写回机制的“脏”数据上限（以字节为单位）。<option>rbd cache max dirty</option> 的值必须小于
      <option>rbd cache size</option> 的值。如果设置为 0，将使用直写缓存。默认值为 24 MB。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      达到该“脏数据目标”后，缓存即会开始向数据存储空间写入数据。该设置不会使写入缓存的操作阻塞。默认值为 16 MB。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      写回开始前，脏数据在缓存中暂存的时间（以秒为单位）。默认值为 1。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      开始进入直写模式，在收到第一条刷新请求后切换到写回模式。启用此设置虽然较为保守，但却是一种安全的做法，如此可应对在
      <systemitem>rbd</systemitem> 上运行的虚拟机太旧而无法发送刷新请求的情况（例如，内核低于 2.6.32 的 Linux
      中的 Virtio 驱动程序）。默认值为“true”。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>QoS 设置</title>

  <para>
   一般而言，服务质量 (QoS) 指的是流量优先级设置和资源预留方法。它对于具有特殊要求的流量传输尤为重要。
  </para>

  <important>
   <title>不受 iSCSI 支持</title>
   <para>
    只有用户空间 RBD 实现 <systemitem class="daemon">librbd</systemitem> 会使用下列 QoS
    设置，<systemitem>kRBD</systemitem> 实现<emphasis>不</emphasis>使用这些设置。由于 iSCSI
    使用的是 <systemitem>kRBD</systemitem>，因此不使用 QoS 设置。不过，对于
    iSCSI，您可以使用标准内核工具在内核块设备层上配置 QoS。
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      指定的每秒 I/O 操作次数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      指定的每秒 I/O 字节数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      指定的每秒读操作次数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      指定的每秒写操作次数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      指定的每秒内读取的字节数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      指定的每秒内写入的字节数上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      指定的 I/O 操作次数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      指定的 I/O 字节数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      指定的读操作次数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      指定的写操作次数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      指定的读取的字节数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      指定的写入的字节数突发上限。默认值为 0（无限制）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      QoS 的最小时间表刻度（以毫秒为单位）。默认值为 50。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>预读设置</title>

  <para>
   RADOS 块设备支持预读/预提取功能，以优化小块的顺序读取。如果使用虚拟机，此操作通常应由 guest
   操作系统处理，但引导加载程序可能不会发出有效的读请求。如果禁用缓存，则会自动禁用预读功能。
  </para>

  <important>
   <title>不受 iSCSI 支持</title>
   <para>
    只有用户空间 RBD 实现 <systemitem class="daemon">librbd</systemitem>
    会使用下列预读设置，<systemitem>kRBD</systemitem> 实现<emphasis>不</emphasis>使用这些设置。由于
    iSCSI 使用的是 <systemitem>kRBD</systemitem>，因此不使用预读设置。不过，对于
    iSCSI，您可以使用标准内核工具在内核块设备层上配置预读。
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      触发预读所必需的顺序读取请求数。默认值为 10。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      预读请求的最大大小。如果设置为 0，则会禁用预读功能。默认值为 512kB。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      从 RBD 映像读取该数量的字节后，该映像的预读功能将会禁用，直至其关闭。使用此设置，guest 操作系统引导时便可接管预读工作。如果设置为
      0，预读将始终处于启用状态。默认值为 50 MB。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>高级特性</title>

  <para>
   RADOS 块设备支持可增强 RBD 映像功能的高级特性。您可以在创建 RBD 映像时在命令行上指定这些特性，或者在 Ceph 配置文件中使用
   <option>rbd_default_features</option> 选项来指定。
  </para>

  <para>
   您可以通过以下两种方式指定 <option>rbd_default_features</option> 选项的值：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     指定为相应特性的内部值之和。每项特性都有自己的内部值，例如，“layering”的内部值为 1，“fast-diff”的内部值为
     16。因此，若要默认激活这两项特性，请指定以下选项：
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     指定为各特性的逗号分隔列表。上面的示例应如下所示：
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>iSCSI 不支持的特性</title>
   <para>
    iSCSI 不支持具有以下特性的 RBD
    映像：<option>deep-flatten</option>、<option>object-map</option>、<option>journaling</option>、<option>fast-diff</option>、<option>striping</option>
   </para>
  </note>

  <para>
   以下是 RBD 的高级特性列表：
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      分层允许您使用克隆。
     </para>
     <para>
      内部值为 1，默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      条带特性会将数据分布在多个对象之间，有助于提升顺序读取/写入工作负载的并行度。它可防止大型或繁忙的 RADOS 块设备出现单节点瓶颈。
     </para>
     <para>
      内部值为 2，默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      如果启用，客户端需要在写入数据之前锁定对象。仅当一次只有一个客户端在访问映像时才应启用互斥锁。内部值为 4。默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      对象映射支持依赖于互斥锁支持。块设备采用的是精简供给，也就是说这些设备只存储实际存在的数据。对象映射支持有助于跟踪哪些对象实际存在（在驱动器上存储了数据）。启用对象映射支持可以加快克隆、导入和导出数据稀疏的映像以及删除所需的
      I/O 操作。
     </para>
     <para>
      内部值为 8，默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Fast-diff 支持依赖于对象映射支持和互斥锁支持。它会向对象映射添加另一个属性，使其更快地生成映像各快照之间的差异以及快照的实际数据使用率。
     </para>
     <para>
      内部值为 16，默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      Deep-flatten 使 <command>rbd
      flatten</command>（请参见<xref linkend="rbd-flatten-cloned-image"/>）除了对映像本身有效外，还对映像的所有快照有效。如果没有该特性，映像的快照将仍然依赖于其父映像，因而如果未删除快照，您将无法删除父映像。Deep-flatten
      使父映像可独立于其克隆，即使这些克隆有快照也不例外。
     </para>
     <para>
      内部值为 32，默认值为“yes”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      日志支持依赖于互斥锁支持。日志会按修改发生的顺序记录映像的所有修改。RBD
      镜像（请参见<xref linkend="ceph-rbd-mirror"/>）会使用日志将崩溃一致性映像复制到<literal>远程</literal>集群。
     </para>
     <para>
      内部值为 64，默认值为“no”。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>使用旧内核客户端映射 RBD</title>

  <para>
   旧客户端（例如 SLE11 SP4）可能无法映射 RBD 映像，因为使用 SUSE Enterprise Storage 7
   部署的集群会强制执行一些旧客户端不支持的特性（RBD 映像级特性和 RADOS 级特性）。发生此情况时，OSD 日志将显示类似如下的讯息：
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>更改 CRUSH 索引存储桶类型将导致大规模重新平衡</title>
   <para>
    如果您打算在 CRUSH
    索引存储桶类型“straw”与“straw2”之间切换，请做好相应规划。这样做预计会对集群负载产生重大影响，因为更改存储桶类型将导致集群大规模重新平衡。
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     禁用任何不支持的 RBD 映像特性。例如：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     将 CRUSH 索引存储桶类型由“straw2”更改为“straw”：
    </para>
    <substeps>
     <step>
      <para>
       保存 CRUSH 索引：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       反编译 CRUSH 索引：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       编辑 CRUSH 索引，并用“straw”替换“straw2”。
      </para>
     </step>
     <step>
      <para>
       重新编译 CRUSH 索引：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       设置新 CRUSH 索引：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>启用块设备和 Kubernetes</title>

  <para>
   您可以通过 <literal>ceph-csi</literal> 驱动程序将 Ceph RBD 与 Kubernetes v1.13
   及更高版本结合使用。此驱动程序会动态供给 RBD 映像以支持 Kubernetes 卷，并在引用 RBD 支持卷的工作节点运行中 pod
   上作为块设备映射这些 RBD 映像（选择性地装入映像中包含的文件系统）。
  </para>

  <para>
   要将 Ceph 块设备与 Kubernetes 结合使用，必须在您的 Kubernetes 环境中安装和配置
   <literal>ceph-csi</literal>。
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> 默认使用 RBD 内核模块，它可能不支持所有 Ceph CRUSH 可调变量或 RBD
    映像功能。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     默认情况下，Ceph 块设备使用 RBD 存储池。为 Kubernetes 卷存储创建存储池。确保 Ceph 集群正在运行，然后创建存储池：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     使用 RBD 工具初始化存储池：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     为 Kubernetes 和 <literal>ceph-csi</literal> 创建一个新用户。执行以下命令并记录生成的密钥：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> 需要存储在 Kubernetes 中的 ConfigMap 对象来定义 Ceph 集群的
     Ceph Monitor 地址。收集 Ceph 集群的唯一 fsid 和 Monitor 地址：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     生成 <filename>csi-config-map.yaml</filename> 文件，替换
     <literal>clusterID</literal> 的 FSID，并替换 <literal>monitors</literal> 的
     Monitor 地址，如下例所示：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     生成后，将新的 ConfigMap 对象存储在 Kubernetes 中：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> 需要 cephx 身份凭证来与 Ceph 集群通讯。使用新创建的 Kubernetes 用户
     ID 和 cephx 密钥生成 <filename>csi-rbd-secret.yaml</filename> 文件，如下例所示：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     生成后，将新的秘密密钥对象存储在 Kubernetes 中：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     创建所需的 ServiceAccount 和 RBAC ClusterRole/ClusterRoleBinding Kubernetes
     对象。这些对象不一定需要根据您的 Kubernetes 环境进行自定义，因此可以直接从 <literal>ceph-csi</literal> 部署
     YAML 文件使用：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     创建 <literal>ceph-csi</literal> 供给程序和节点插件：
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      默认情况下，供给程序和节点插件 YAML 文件将提取 <literal>ceph-csi</literal> 容器的开发版本。应更新 YAML
      文件以使用发布版本。
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>在 Kubernetes 中使用 Ceph 块设备</title>
   <para>
    Kubernetes StorageClass 定义了一个存储类。可以创建多个 StorageClass
    对象，以映射到不同的服务质量级别和功能。例如，NVMe 与基于 HDD 的存储池。
   </para>
   <para>
    要创建映射到上面所创建 Kubernetes 存储池的 <literal>ceph-csi</literal> StorageClass，在确保
    <literal>clusterID</literal> 属性与您的 Ceph 集群的 FSID 相匹配之后，可以使用以下 YAML文件：
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    <literal>PersistentVolumeClaim</literal>
    是用户发出的抽象存储资源请求。然后，<literal>PersistentVolumeClaim</literal> 将与 pod 资源相关联以供应
    <literal>PersistentVolume</literal>，它将由 Ceph 块映像提供支持。可以包含可选的
    <option>volumeMode</option>，以便在装入的文件系统（默认）或基于块设备的原始卷之间进行选择。
   </para>
   <para>
    使用 <literal>ceph-csi</literal>，为 <option>volumeMode</option> 指定
    <option>Filesystem</option> 可支持 <literal>ReadWriteOnce</literal> 和
    <literal>ReadOnlyMany accessMode</literal> 声明，为 <option>volumeMode</option>
    指定 <option>Block</option> 可支持
    <literal>ReadWriteOnce</literal>、<literal>ReadWriteMany</literal> 和
    <literal>ReadOnlyMany accessMode</literal> 声明。
   </para>
   <para>
    例如，要创建使用上述创建的 <literal>ceph-csi-based StorageClass</literal> 的基于块的
    <literal>PersistentVolumeClaim</literal>，可以使用以下 YAML 文件通过
    <literal>csi-rbd-sc StorageClass</literal> 请求原始块存储：
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    下面的示例演示了如何将上述 <literal>PersistentVolumeClaim</literal> 作为原始块设备绑定到 pod 资源：
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    要创建使用上述创建的 <literal>ceph-csi-based StorageClass</literal> 的基于文件系统的
    <literal>PersistentVolumeClaim</literal>，可以使用以下 YAML 文件通过
    <literal>csi-rbd-sc StorageClass</literal> 请求装入的文件系统（由 RBD 映像提供支持）：
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    下面的示例演示了如何将上述 <literal>PersistentVolumeClaim</literal> 作为装入的文件系统绑定到 pod 资源：
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
