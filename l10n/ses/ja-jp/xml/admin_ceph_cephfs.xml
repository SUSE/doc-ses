<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>クラスタファイルシステム</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  この章では、通常はクラスタの設定とCephFSのエクスポート後に実行する管理タスクについて説明します。CephFSの設定の詳細については、<xref linkend="deploy-cephadm-day2-service-mds"/>を参照してください。
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>CephFSのマウント</title>

  <para>
   ファイルシステムが作成されてMDSがアクティブになったら、クライアントホストからファイルシステムをマウントできます。
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>クライアントの準備</title>
   <para>
    クライアントホストがSUSE Linux Enterprise 12 SP2以降を実行している場合、システムはCephFSをそのまますぐにマウントできます。
   </para>
   <para>
    クライアントホストがSUSE Linux Enterprise 12 SP1を実行している場合は、CephFSをマウントする前にすべての最新パッチを適用する必要があります。
   </para>
   <para>
    いずれの場合も、CephFSをマウントするのに必要なものはすべてSUSE Linux Enterpriseに付属しています。SUSE Enterprise Storage 7製品は必要ありません。
   </para>
   <para>
    完全な<command>mount</command>構文をサポートするには、CephFSのマウントを試みる前に、
    <package>ceph-common</package> パッケージ(SUSE Linux Enterpriseに付属)をインストールする必要があります。
   </para>
   <important>
    <para>
     パッケージ <package>ceph-common</package> がない場合(つまり、<command>mount.ceph</command>ヘルパーがない場合)、モニターのホスト名ではなく、IPアドレスが必要です。これは、カーネルクライアントがネームレゾリューションを実行できないためです。
    </para>
    <para>
     基本的な構文は、次の通りです。
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>シークレットファイルの作成</title>
   <para>
    Cephクラスタは、デフォルトで認証がオンの状態で動作します。秘密鍵(キーリングそのものではない)を保存するファイルを作成する必要があります。特定のユーザの秘密鍵を入手してファイルを作成するには、次の操作を行います。
   </para>
   <procedure>
    <title>秘密鍵の作成</title>
    <step>
     <para>
      キーリングファイル内の特定のユーザの鍵を表示します。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      マウントしたCephFS (Ceph File System)を使用するユーザの鍵をコピーします。通常、鍵は次のような形式です。
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      ファイル名の部分にユーザ名を使用してファイルを作成します。たとえば、ユーザ<emphasis>admin</emphasis>の場合は、<filename>/etc/ceph/admin.secret</filename>のようになります。
     </para>
    </step>
    <step>
     <para>
      前の手順で作成したファイルに鍵の値を貼り付けます。
     </para>
    </step>
    <step>
     <para>
      ファイルに適切なアクセス権を設定します。このユーザは、ファイルを読み込める唯一のユーザである必要があります。ほかのユーザは一切アクセス権を持つことはできません。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>CephFSのマウント</title>
   <para>
    <command>mount</command>コマンドでCephFSをマウントできます。Monitorのホスト名またはIPアドレスを指定する必要があります。SUSE Enterprise Storageでは<systemitem>cephx</systemitem>認証がデフォルトで有効になっているため、ユーザ名とその関連シークレットも指定する必要があります。
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    以前のコマンドはシェルの履歴に残るため、ファイルからシークレットを読み込むアプローチの方が安全です。
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    シークレットファイルには実際のキーリングシークレットだけが含まれる必要があることに注意してください。この例では、ファイルに含まれるのは次の行だけです。
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>複数のMonitorの指定</title>
    <para>
     マウント時に特定のMonitorがダウンしている事態に備え、<command>mount</command>コマンドラインで複数のMonitorをコンマで区切って指定することをお勧めします。各Monitorのアドレスは<literal>host[:port]</literal>という形式です。ポートを指定しない場合は、デフォルトで6789が使用されます。
    </para>
   </tip>
   <para>
    ローカルホストでマウントポイントを作成します。
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    CephFSをマウントします。
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    ファイルシステムのサブセットをマウントする場合は、サブディレクトリ<filename>subdir</filename>を指定できます。
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    <command>mount</command>コマンドで複数のMonitorホストを指定できます。
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>ルートディレクトリに対する読み込みアクセス</title>
    <para>
     パス制約付きのクライアントを使用する場合は、MDSのケーパビリティにルートディレクトリに対する読み込みアクセスを含める必要があります。たとえば、キーリングは次のようになります。
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     <literal>allow r path=/</literal>の部分は、パス制約付きのクライアントは、ルートボリュームを表示できても書き込みはできないことを意味します。これは、完全な分離が要件である使用事例で問題になることがあります。
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>CephFSのアンマウント</title>

  <para>
   CephFSをアンマウントするには、<command>umount</command>コマンドを使用します。
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title><filename>/etc/fstab</filename>でのCephFSのマウント</title>

  <para>
   クライアントの起動時にCephFSを自動的にマウントするには、対応する行をファイルシステムテーブル<filename>/etc/fstab</filename>に挿入します。
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>複数のアクティブMDSデーモン(アクティブ-アクティブMDS)</title>

  <para>
   CephFSは、デフォルトでは単一のアクティブMDSデーモン用に設定されています。大規模システム用にメタデータのパフォーマンスを拡張する場合、複数のアクティブMDSデーモンを有効にできます。これにより、各デーモンがお互いにメタデータワークロードを共有します。
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>アクティブ-アクティブMDSの使用</title>
   <para>
    デフォルトの単一のMDSではメタデータのパフォーマンスがボトルネックになる場合、複数のアクティブMDSデーモンの使用を検討します。
   </para>
   <para>
    デーモンを追加しても、すべてのワークロードタイプのパフォーマンスが向上するわけではありません。たとえば、単一のクライアント上で動作している単一のアプリケーションの場合、そのアプリケーションが大量のメタデータ操作を並列で実行していない限り、MDSデーモンの数を増やしてもメリットはありません。
   </para>
   <para>
    一般的に大量のアクティブMDSデーモンのメリットを受けられるワークロードは、クライアントが複数あり、多数の別個のディレクトリを操作する可能性が高いワークロードです。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>MDSのアクティブクラスタサイズの増加</title>
   <para>
    各CephFSファイルシステムには、作成するランクの数を制御する<option>max_mds</option>設定があります。ファイルシステム内の実際のランク数は、新しいランクを引き受けるスペアデーモンが利用可能な場合にのみ増やされます。たとえば、実行中のMDSデーモンが1つだけで、<option>max_mds</option>が2に設定されている場合、2番目のランクは作成されません。
   </para>
   <para>
    次の例では、<option>max_mds</option>オプションを2に設定して、デフォルトのランクとは別の新しいランクを作成します。変更を確認するには、<option>max_mds</option>の設定前と設定後に<command>ceph status</command>を実行し、<literal>fsmap</literal>が含まれる行を確認します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    新しく作成されたランク(1)は、「creating (作成中)」状態を経由して「active (アクティブ)」状態になります。
   </para>
   <important>
    <title>スタンバイデーモン</title>
    <para>
     複数のアクティブMDSデーモンを使用していても、高可用性システムには、アクティブデーモンを実行するサーバに障害が発生した場合に処理を引き継ぐスタンバイデーモンも必要です。
    </para>
    <para>
     そのため、高可用性システムの<option>max_mds</option>の実用的な最大数は、システムのMDSサーバの合計数から1を引いた数になります。複数のサーバ障害時に可用性を維持するには、切り抜ける必要があるサーバ障害の数に一致するようにシステムのスタンバイデーモンの数を増やします。
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>ランク数の減少</title>
   <para>
    最初に、すべてのランク(削除するランクを含む)がアクティブになっている必要があります。つまり、少なくとも<option>max_mds</option> MDSデーモンが利用可能である必要があります。
   </para>
   <para>
    最初に、<option>max_mds</option>をより低い数字に設定します。たとえば、単一のアクティブMDSに戻します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>ランクへのディレクトリツリーの手動固定</title>
   <para>
    複数のアクティブメタデータサーバ設定では、バランサが動作し、メタデータの負荷をクラスタに均等に分散します。これは通常、ほとんどのユーザにとって十分有効に機能しますが、メタデータを特定のランクに明示的にマッピングして動的バランサを無効にした方が良い場合もあります。これにより、管理者やユーザは、アプリケーションの負荷を均等に分散したり、ユーザのメタデータ要求によるクラスタ全体への影響を抑えたりできます。
   </para>
   <para>
    このために提供されているメカニズムを「エクスポートピン」と呼びます。これはディレクトリの拡張属性です。この拡張属性の名前は<literal>ceph.dir.pin</literal>です。標準のコマンドを使用して、この属性を設定できます。
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    拡張属性の値(<option>-v</option>)は、ディレクトリサブツリーの割り当て先となるランクです。デフォルト値-1は、ディレクトリが固定されないことを示します。
   </para>
   <para>
    ディレクトリのエクスポートピンは、設定されているエクスポートピンを持つ最も近い親から継承されます。したがって、ディレクトリにエクスポートピンを設定すると、そのすべての子に影響します。ただし、子ディレクトリのエクスポートピンを設定して親のピンを上書きできます。例:
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>フェールオーバーの管理</title>

  <para>
   MDSデーモンがMonitorとの通信を停止した場合、そのMonitorは<option>mds_beacon_grace</option>の秒数(デフォルトは15秒)待機してから、デーモンを「遅延」<emphasis/>としてマークします。MDSデーモンのフェールオーバー中に処理を引き継ぐ「スタンバイ」デーモンを1つ以上設定できます。
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>スタンバイ再生の設定</title>
   <para>
    各CephFSファイルシステムはスタンバイ再生デーモンを追加するように設定することもできます。こうしたスタンバイデーモンはアクティブMDSのメタデータジャーナルを追跡して、アクティブMDSが利用不能になるようなイベントが発生した場合のフェールオーバー時間を短縮します。各アクティブMDSに設定できる、追跡用のスタンバイ再生デーモンは一つだけです。
   </para>
   <para>
    ファイルシステムにスタンバイ再生を設定するには、次のコマンドを使用します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    設定された場合、モニターは使用可能なスタンバイデーモンをファイルシステムのアクティブMDSを追跡するように割り当てます。
   </para>
   <para>
    ひとたびMDSがスタンバイ再生状態になると、そのMDSは追跡しているランクのスタンバイとしてのみ使用されます。別のランクが失敗し、他に利用可能なスタンバイがない場合でも、このスタンバイ再生デーモンは代わりとして使用されません。そのため、スタンバイ再生機能を使用する場合は、すべてのアクティブMDSにスタンバイ再生デーモンを設定することをお勧めします。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>CephFSのクォータの設定</title>

  <para>
   Cephファイルシステムの任意のサブディレクトリにクォータを設定できます。クォータは、ディレクトリ階層の指定したポイントの下層に保存される「バイト」または「ファイル」の数を制限します。<emphasis role="bold"/><emphasis role="bold"/>
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>CephFSのクォータの制限</title>
   <para>
    CephFSでのクォータの使用には、次の制限があります。
   </para>
   <variablelist>
    <varlistentry>
     <term>クォータは協調的で非競合</term>
     <listitem>
      <para>
       Cephクォータは、ファイルシステムをマウントしているクライアントに依存し、制限に達すると書き込みを停止します。サーバ側では、悪意のあるクライアントが必要なだけデータを書き込むのを防止することはできません。クライアントが完全に信頼されていない環境では、ファイルシステムがいっぱいになるのを防ぐため、クォータを使用しないでください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>クォータは正確ではない</term>
     <listitem>
      <para>
       ファイルシステムへの書き込み中のプロセスは、クォータ制限に達した直後に停止されます。そのため必然的に、設定された制限を超える量のデータを書き込むことができます。クライアントのライタは、設定された制限を超えてから1/10秒以内に停止されます。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>バージョン4.17からクォータはカーネルクライアントに実装される</term>
     <listitem>
      <para>
       クォータは、ユーザスペースクライアント(libcephfs、ceph-fuse)によってサポートされます。Linuxカーネルクライアント4.17以降は、SUSE Enterprise Storage 7クラスタ上のCephFSクォータをサポートします。カーネルクライアントが最新バージョンであっても、クォータ拡張属性を設定できても、古いクラスタ上のクォータは処理できません。SLE12-SP3以降のカーネルは、クォータの処理に必要なバックポートをすでに備えています。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>パスベースのマウント制限とともに使用する場合はクォータを慎重に設定する</term>
     <listitem>
      <para>
       クライアントは、クォータを適用するには、クォータが設定されているディレクトリiノードにアクセスできる必要があります。クライアントがMDSの機能に基づいて特定のパス(たとえば、<filename>/home/user</filename>)へのアクセスを制限されている場合に、そのクライアントがアクセスできない祖先ディレクトリ(<filename>/home</filename>)にクォータが設定されているときは、クライアントはクォータを適用しません。パスベースのアクセス制限を使用する場合、クライアントがアクセスできるディレクトリにクォータを設定することを忘れないでください(たとえば、<filename>/home/user</filename>や<filename>/home/user/quota_dir</filename>)。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>CephFSのクォータの設定</title>
   <para>
    仮想拡張属性を使用して、CephFSクォータを設定できます。
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       「ファイル」制限を設定します。<emphasis/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       「バイト」制限を設定します。<emphasis/>
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    これらの属性がディレクトリiノード上に存在する場合、そこにクォータが設定されています。存在しない場合は、そのディレクトリにクォータは設定されていません(ただし、親ディレクトリに設定されている場合があります)。
   </para>
   <para>
    100MBのクォータを設定するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    10,000ファイルのクォータを設定するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    クォータ設定を表示するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>クォータが設定されない</title>
    <para>
     拡張属性の値が「0」の場合、クォータは設定されません。
    </para>
   </note>
   <para>
    クォータを削除するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>CephFSスナップショットの管理</title>

  <para>
   CephFSスナップショットは、スナップショットを作成した時点でのファイルシステムの読み込み専用ビューを作成します。スナップショットは任意のディレクトリに作成できます。スナップショットでは、ファイルシステムの指定ディレクトリの下層にあるすべてのデータが対象になります。スナップショットの作成後、バッファされたデータはさまざまなクライアントから非同期にフラッシュされます。その結果、スナップショットの作成は非常に高速です。
  </para>

  <important>
   <title>複数のファイルシステム</title>
   <para>
    複数のCephFSファイルシステムが(ネームスペースを介して) 1つのプールを共有している場合、それらのスナップショットは競合し、1つのスナップショットを削除すると同じプールを共有している他のスナップショットのファイルデータがなくなります。
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>スナップショットの作成</title>
   <para>
    CephFSスナップショット機能は、新しいファイルシステムではデフォルトで有効になっています。既存のファイルシステムでこの機能を有効にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    スナップショットを有効にすると、CephFSのすべてのディレクトリに特別な<filename>.snap</filename>サブディレクトリが作成されます。
   </para>
   <note>
    <para>
     これは「仮想」<emphasis/>サブディレクトリです。このサブディレクトリは親ディレクトリのディレクトリ一覧には表示されませんが、<filename>.snap</filename>という名前はファイル名やディレクトリ名として使用できません。<filename>.snap</filename>ディレクトリにアクセスするには、明示的にアクセスする必要があります。たとえば、次のようなコマンドを使用します。
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>カーネルクライアントの制限</title>
    <para>
     CephFSカーネルクライアントには、1つのファイルシステム内で400を超えるスナップショットを処理できないという制限があります。スナップショットの数は、使用しているクライアントに関係なく、常にこの制限を下回るようにする必要があります。SLE12-SP3などの古いCephFSクライアントを使用する場合は、スナップショットが400を超えるとクライアントがクラッシュするため、操作に害を及ぼすことに注意してください。
    </para>
   </important>
   <tip>
    <title>カスタムスナップショットサブディレクトリ名</title>
    <para>
     <option>client snapdir</option>設定により、スナップショットサブディレクトリに異なる名前を設定できます。
    </para>
   </tip>
   <para>
    スナップショットを作成するには、<filename>.snap</filename>ディレクトリに、カスタム名を持つサブディレクトリを作成します。たとえば、<filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>ディレクトリのスナップショットを作成するには、次のコマンドを実行します。
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>スナップショットの削除</title>
   <para>
    スナップショットを削除するには、<filename>.snap</filename>ディレクトリ内にあるそのサブディレクトリを削除します。
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
