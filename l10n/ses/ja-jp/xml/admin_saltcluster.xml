<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>運用タスク</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>クラスタ設定の変更</title>

  <para>
   既存のCephクラスタの設定を変更するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     現在のクラスタの設定をファイルにエクスポートします。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     設定が記載されたファイルを編集し、関連する行を更新します。仕様の例については、<xref linkend="deploy-core"/>と<xref linkend="drive-groups"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     新しい設定を適用します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>ノードの追加</title>

  <para>
   Cephクラスタに新しいノードを追加するには、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     SUSE Linux Enterprise ServerとSUSE Enterprise Storageを新規ホストにインストールします。詳細については、<xref linkend="deploy-sles"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     ホストを既存のSalt MasterのSalt Minionとして設定します。詳細については、<xref linkend="deploy-salt"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     新しいホストを<systemitem class="resource">ceph-salt</systemitem>に追加し、cephadmにホストを認識させます。たとえば、次のコマンドを実行します。
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     詳細については、<xref linkend="deploy-cephadm-configure-minions"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     <systemitem class="resource">ceph-salt</systemitem>にノードが追加されたことを確認します。
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     新しいクラスタホストに設定を適用します。
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     新しく追加したホストがcephadm環境に属していることを確認します。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>ノードの削除</title>

  <tip>
   <title>OSDの削除</title>
   <para>
    削除しようとしているノードがOSDを実行している場合、まずOSDを削除してからそのノード上でOSDが実行されていないことを確認してください。OSDを削除する方法の詳細については、<xref linkend="removing-node-osds"/>を参照してください。
   </para>
  </tip>

  <para>
   クラスタからノードを削除するには、次の手順に従います。
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     <literal>node-exporter</literal>と<literal>crash</literal>を除くすべてのCephサービスタイプについて、ノードのホスト名をクラスタの配置仕様ファイル(<filename>cluster.yml</filename>など)から削除します。詳細については、<xref linkend="cephadm-service-and-placement-specs"/>を参照してください。たとえば、<literal>ses-min2</literal>という名前のホストを削除する場合、すべての<literal>placement:</literal>セクションから、<literal>- ses-min2</literal>という記載をすべて削除します。
    </para>
    <para>
     この状態から
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     次のように変更してください。
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     変更内容を設定ファイルに適用します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     cephadm環境からノードを削除します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     ノードが<literal>crash.osd.1</literal>と<literal>crash.osd.2</literal>というサービスを実行している場合、ホスト上で次のコマンドを実行してサービスを削除します。
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     例:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     削除したいミニオンからすべての役割を削除します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     削除したいミニオンがブートストラップミニオンの場合、ブートストラップの役割も削除する必要があります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     1つのホストからすべてのOSDを削除したら、そのホストをCRUSHマップから削除します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      バケット名はホスト名と同じであるはずです。
     </para>
    </note>
   </step>
   <step>
    <para>
     これで、クラスタからミニオンを削除できるようになります。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    障害イベント中で、削除しようとしているミニオンが永続的な電源オフ状態である場合、Salt Masterからノードを削除する必要があります。
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    その後、<filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>からノードを手動で削除します。このファイルは通常、<filename>/srv/pillar/ceph-salt.sls</filename>に置かれています。
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSDの管理</title>

  <para>
   このセクションではCephクラスタにOSDを追加、消去、削除する方法を説明します。
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>ディスクデバイスの一覧</title>
   <para>
    すべてのクラスタノード上のディスクデバイスが使用中か未使用かを確認するには、次のコマンドを実行してディスクを一覧にしてください。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>ディスクデバイスの消去</title>
   <para>
    ディスクデバイスを再利用するには、まず内容を消去<emphasis></emphasis>する必要があります。
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    例:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     <literal>unmanaged</literal>フラグが設定されておらず、以前OSDを展開した際にDriveGroupsまたは<option>--all-available-devices</option>オプションを使用していた場合、消去後にcephadmが自動的にこれらのOSDを展開します。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>DriveGroups仕様を用いたOSDの追加</title>
   <para>
    <emphasis></emphasis>「DriveGroups」では、CephクラスタのOSDのレイアウトを指定します。レイアウトは単独のYAMLファイルで定義します。このセクションでは、例として<filename>drive_groups.yml</filename>を使用します。
   </para>
   <para>
    管理者は、相互に関連するOSDのグループ(HDDとSSDの混成環境に展開されるハイブリッドOSD)を手動で指定するか、同じ展開オプション(たとえば、同じオブジェクトストア、同じ暗号化オプション、スタンドアロンOSDなど)を共有する必要があります。デバイスが明示的に一覧にされないようにするため、DriveGroupsでは、<command>ceph-volume</command>インベントリレポートで選択した数個のフィールドに対応するフィルタ項目のリストを使用します。cephadmでは、これらのDriveGroupsを実際のデバイスリストに変換してユーザが調べられるようにするコードを提供します。
   </para>
   <para>
    OSDの仕様をクラスタに適用するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    アクションのプレビューを確認する場合や、アプリケーションをテストする場合には、<option>--dry-run</option>オプションを付加した<command>ceph orch apply osd</command>コマンドを使用できます。次に例を示します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    <option>--dry-run</option>の出力が想定通りなら、<option>--dry-run</option>オプションを外して再度コマンドを実行するだけです。
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>アンマネージドOSD</title>
    <para>
     DriveGroups仕様と一致する、利用可能なすべてのクリーンディスクデバイスは、クラスタに追加すると自動的にOSDとして使用されます。この動作を「マネージド」<emphasis></emphasis>モードと呼びます。
    </para>
    <para>
     「マネージド」<emphasis></emphasis>モードを無効化するには、<literal>unmanaged: true</literal>行を関連する仕様に追加してください。次に例を示します。
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      すでに展開済みのOSDを「マネージド」<emphasis></emphasis>モードから「アンマネージド」<emphasis></emphasis>モードに切り替えるには、<xref linkend="modifying-cluster-configuration"/>で説明した手順の中で<literal>unmanaged: true</literal>行を追加してください。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>DriveGroups仕様</title>
    <para>
     DriveGroups仕様ファイルの例を次に示します。
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
    <note>
     <para>
      かつてのDeepSeaで「encryption」と呼ばれていたオプションは、「encrypted」に名前が変更されました。SUSE Enterprise Storage 7にDriveGroupに適用する場合は、サービス仕様にこの新しい用語が使われているかを確認してください。さもなければ、<command>ceph orch apply</command>操作は失敗します。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>一致するディスクデバイス</title>
    <para>
     次のフィルタを使用して指定を記述できます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       ディスクモデル別。
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       ディスクベンダー別。
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        <replaceable>DISK_VENDOR_STRING</replaceable>は必ず小文字で入力してください。
       </para>
      </tip>
      <para>
       ディスクモデルとディスクベンダーの詳細を取得するには、次のコマンドの出力を確認してください。
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       ディスクが回転型かどうか。SSDとNVMeドライブは回転型ではありません。
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       OSDで使用可能な「すべての」ドライブを使用してノードを展開します。<emphasis></emphasis>
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       また、一致するディスクの数を制限します。
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>サイズによるデバイスのフィルタリング</title>
    <para>
     ディスクデバイスをサイズでフィルタできます(正確なサイズ、またはサイズの範囲)。<option>size:</option>パラメータには、次の形式の引数を指定できます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &apos;10G&apos; -正確にこのサイズのディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       &apos;10G:40G&apos; - この範囲内のサイズのディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       &apos;:10G&apos; - サイズが10GB以下のディスクを含めます。
      </para>
     </listitem>
     <listitem>
      <para>
       &apos;40G:&apos; - サイズが40GB以上のディスクを含めます。
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>ディスクサイズによる一致</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>引用符が必要</title>
     <para>
      区切り文字「:」を使用する場合は、サイズを引用符で囲む必要があります。そうしないと、「:」記号は新しい設定のハッシュであると解釈されます。
     </para>
    </note>
    <tip>
     <title>単位のショートカット</title>
     <para>
      ギガバイト(G)の代わりに、メガバイト(M)やテラバイト(T)でもサイズを指定できます。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>DriveGroupsの例</title>
    <para>
     このセクションでは、さまざまなOSDセットアップの例を示します。
    </para>
    <example>
     <title>単純なセットアップ</title>
     <para>
      この例では、同じセットアップを使用する2つのノードについて説明します。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      対応する<filename>drive_groups.yml</filename>ファイルは次のようになります。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      このような設定は単純で有効です。問題は、管理者が将来、別のベンダーのディスクを追加することがあっても、それらのディスクが含まれない点です。この設定を向上させるには、ドライブのコアプロパティのフィルタを減らします。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      前の例では、回転型デバイスはすべて「データデバイス」として宣言し、非回転型デバイスはすべて「共有デバイス」(wal、db)として使用します。
     </para>
     <para>
      2TBを超えるドライブが常に低速のデータデバイスであることがわかっている場合は、サイズでフィルタできます。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>詳細セットアップ</title>
     <para>
      この例では、2つの別個のセットアップについて説明します。20台のHDDで2台のSSDを共有するセットアップと、10台のSSDで2台のNVMeを共有するセットアップです。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      このようなセットアップは、次のような2つのレイアウトで定義できます。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>不均一なノードを使用した詳細セットアップ</title>
     <para>
      前の例では、すべてのノードに同じドライブがあることを想定しています。ただし、常にこれが当てはまるとは限りません。
     </para>
     <para>
      ノード1～5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      ノード6～10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      レイアウトに「target」キーを使用して、特定のノードをターゲットに設定できます。Saltのターゲット表記を使用すると、内容をシンプルに保つことができます。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      続いて以下を設定します。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>エキスパートセットアップ</title>
     <para>
      前の事例はすべて、WALとDBが同じデバイスを使用することを想定していました。ただし、WALを専用のデバイスに展開することもできます。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>複雑な(可能性が低い)セットアップ</title>
     <para>
      次のセットアップでは、以下を定義してみます。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1つのNVMeを利用する20台のHDD
       </para>
      </listitem>
      <listitem>
       <para>
        1台のSSD (db)と1つのNVMe (wal)を利用する2台のHDD
       </para>
      </listitem>
      <listitem>
       <para>
        1つのNVMeを利用する8台のSSD
       </para>
      </listitem>
      <listitem>
       <para>
        2台SSDスタンドアロン(暗号化)
       </para>
      </listitem>
      <listitem>
       <para>
        1台のHDDはスペアで、展開しない
       </para>
      </listitem>
     </itemizedlist>
     <para>
      使用するドライブの概要は次のとおりです。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23台のHDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 4TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10台のSSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 512GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1つのNVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          ベンダー: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          モデル: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          サイズ: 256GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      DriveGroupsの定義は次のようになります。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      ファイルが上から下へ解析されると、HDDが1台残ります。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>OSDの削除</title>
   <para>
    クラスタからOSDノードを削除する前に、クラスタの空きディスク容量が、削除予定のOSDディスクの容量以上であることを確認してください。OSDを削除すると、クラスタ全体のリバランスが発生することに注意してください。
   </para>
   <procedure>
    <step>
     <para>
      IDを取得して、削除するOSDを特定します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      クラスタから1つ以上のOSDを削除します。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      例:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      削除操作の状態をクエリすることができます。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>OSD削除の中止</title>
    <para>
     OSDの削除をスケジュールした後、必要に応じて削除を停止できます。次のコマンドを実行すると、OSDの初期状態がリセットされ、キューから削除されます。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>OSDの交換</title>
   <para>
    さまざまな理由でOSDディスクを交換しなければならないことがあります。例:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      OSDディスクに障害が発生しているか、SMART情報によると間もなく障害が発生しそうで、そのOSDディスクを使用してデータを安全に保存できなくなっている。
     </para>
    </listitem>
    <listitem>
     <para>
      サイズの増加などのため、OSDディスクをアップグレードする必要がある。
     </para>
    </listitem>
    <listitem>
     <para>
      OSDディスクのレイアウトを変更する必要がある。
     </para>
    </listitem>
    <listitem>
     <para>
      非LVMからLVMベースのレイアウトに移行することを計画している。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    IDを維持したままOSDを交換するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    例:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    OSDの交換は、OSDがCRUSH階層から永久に削除されることがなく、代わりに<literal>destroyed</literal>フラグが割り当てられることを除けば、OSDの削除と同じです(詳細については、<xref linkend="removing-node-osds"/>を参照してください)。
   </para>
   <para>
    <literal>destroyed</literal>フラグは、次回OSDを展開した際に再利用するOSD IDを特定するために使用されます。新しく追加されたディスクがDriveGroups仕様と一致する場合、そのディスクには交換されたディスクのOSD IDが割り当てられます(詳細については、<xref linkend="drive-groups"/>を参照してください)。
   </para>
   <tip>
    <para>
     <option>--dry-run</option>オプションを付加すると、実際の交換は行われませんが、通常発生する手順をプレビューします。
    </para>
   </tip>
   <note>
    <para>
     障害後にOSDを交換する場合は、配置グループのディープスクラブをトリガすることを強くお勧めします。詳しくは「<xref linkend="scrubbing-pgs"/>」を参照してください。
    </para>
    <para>
     次のコマンドを実行して、ディープスクラブを開始します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd deep-scrub osd.<replaceable>OSD_NUMBER</replaceable></screen>
   </note>
   <important>
    <title>共有デバイスの障害</title>
    <para>
     DB/WALの共有デバイスに障害が発生した場合は、障害が発生したデバイスを共有するすべてのOSDの交換手順を実行する必要があります。
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>新しいノードへのSalt Masterの移動</title>

  <para>
   Salt Masterのホストを新しいものに交換する必要がある場合、次の手順に従います。
  </para>

  <procedure>
   <step>
    <para>
     クラスタ設定をエクスポートし、エクスポートされたJSONファイルをバックアップします。詳細については、<xref linkend="deploy-cephadm-configure-export"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     古いSalt Masterがクラスタで唯一の管理ノードでもある場合、<filename>/etc/ceph/ceph.client.admin.keyring</filename>と<filename>/etc/ceph/ceph.conf</filename>を新しいSalt Masterに手動で移動します。
    </para>
   </step>
   <step>
    <para>
     古いSalt Masterノードで、Salt Masterの<systemitem class="daemon">systemd</systemitem>サービスを停止し無効化します。
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     古いSalt Masterノードがすでにクラスタ内に存在しない場合、Salt Minionの<systemitem class="daemon">systemd</systemitem>サービスも停止し無効化します。
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      古いSalt MasterノードがいずれかのCephデーモン(MON、MGR、OSD、MDS、ゲートウェイ、監視)を実行している場合は、<literal>salt-minion.service</literal>の停止と無効化を行わないでください。
     </para>
    </warning>
   </step>
   <step>
    <para>
     SUSE Linux Enterprise Server 15 SP3を新しいSalt Masterにインストールします。手順は<xref linkend="deploy-sles"/>を参照してください。
    </para>
    <tip>
     <title>Salt Minionの移行</title>
     <para>
      Salt Minionを新しいSalt Masterへ簡単に移行するには、各Minionから元のSalt Masterの公開鍵を削除します。
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     <package>salt-master</package>パッケージをインストールし、該当する場合は、新しいSalt Masterに<package>salt-minion</package>パッケージをインストールします。
    </para>
   </step>
   <step>
    <para>
     新しいSalt Masterノードに<systemitem class="resource">ceph-salt</systemitem>をインストールします。
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      次の手順に進む前に、3つすべてのコマンドを実行したか確認してください。これらのコマンドはべき等です。つまり、同じコマンドを複数回実行しても問題ありません。
     </para>
    </important>
   </step>
   <step>
    <para>
     新しいSalt Masterをクラスタに取り込みます。手順は<xref linkend="deploy-cephadm-cephsalt"/>と<xref linkend="deploy-cephadm-configure-minions"/>、<xref linkend="deploy-cephadm-configure-admin"/>を参照してください。
    </para>
   </step>
   <step>
    <para>
     バックアップしたクラスタ設定をインポートして適用します。
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      インポートする前に、エクスポートされた<filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename>ファイルに記載されたSalt Masterの<literal>minion id</literal>の名前を変更します。
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>クラスタノードの更新</title>

  <para>
   ローリングアップデートを定期的に適用して、Cephクラスタノードを最新の状態に保ちます。
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>ソフトウェアリポジトリ</title>
   <para>
    最新のソフトウェアパッケージのパッチをクラスタに適用する前に、クラスタのすべてのノードが関連するリポジトリにアクセスできることを確認します。必要なリポジトリの完全なリストについては、<xref linkend="verify-previous-upgrade-patch-repos-repos"/>を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>リポジトリのステージング</title>
   <para>
    クラスタノードにソフトウェアリポジトリを提供するステージングツール(SUSE Manager、RMT (Repository Management Tool)など)を使用する場合、SUSE Linux Enterprise ServerとSUSE Enterprise Storageの両方の「更新」リポジトリのステージが同じ時点で作成されていることを確認します。
   </para>
   <para>
    ステージングツールを使用して、パッチレベルが<literal>frozen</literal>または<literal>staged</literal>のパッチを適用することを強く推奨します。これにより、クラスタに参加している新しいノードと、クラスタですでに動作しているノードが確実に同じパッチレベルになるようにします。また、新しいノードがクラスタに参加する前に、クラスタのすべてのノードに最新のパッチを適用する必要がなくなります。
   </para>
  </sect2>

  <sect2>
   <title>Cephサービスのダウンタイム</title>
   <para>
    設定によっては、更新中にクラスタノードが再起動される場合があります。Object Gateway、Samba Gateway、NFS Ganesha、iSCSIなど、サービスの単一障害点があると、再起動されるノードに存在するサービスからクライアントマシンが一時的に切断される場合があります。
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>更新の実行</title>
   <para>
    すべてのクラスタノードでソフトウェアパッケージを最新バージョンに更新するには、次のコマンドを実行します。
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Cephの更新</title>

  <para>
   cephadmに対して、Cephを更新して各バグフィックスリリースを適用するように指示できます。Cephサービスの自動更新は、推奨される順番を尊重します。つまり、まずはCeph ManagerとCeph Monitorから開始し、その後、Ceph OSD、メタデータサーバ、Object Gatewayなどのその他のサービスに進みます。クラスタの利用を継続できることをCephが示すまで、各デーモンは再起動されません。
  </para>

  <note>
   <para>
    以下の更新手順では、<command>ceph orch upgrade</command>コマンドを使用します。以下の手順は、ある製品バージョンのCephクラスタを更新する方法を詳細に説明したもので(たとえば、保守更新)、クラスタをある製品バージョンから別の製品バージョンにアップグレードする方法を説明したものではない<emphasis></emphasis>ことに注意してください。
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>更新の開始</title>
   <para>
    更新を開始する前に、すべてのノードが現在オンラインであり、クラスタが正常な状態であることを確認してください。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    特定のCephリリースに更新するには、次のコマンドを使用します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    例:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7.1/ceph/ceph:latest</screen>
   <para>
    ホスト上でパッケージをアップグレードします。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>更新の監視</title>
   <para>
    更新が進行中かどうかを確認するには、次のコマンドを実行します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    更新が進行中であれば、Cephの状態出力でプログレスバーを確認できます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7.1/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    cephadmのログを監視することもできます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>更新のキャンセル</title>
   <para>
    更新処理はいつでも中止できます。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>クラスタの停止または再起動</title>

  <para>
   場合によっては、クラスタ全体を停止または再起動しなければならないことがあります。実行中のサービスの依存関係を入念に確認することをお勧めします。次の手順では、クラスタの停止と起動の概要を説明します。
  </para>

  <procedure>
   <step>
    <para>
     OSDにoutのマークを付けないようCephクラスタに指示します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     次の順序でデーモンとノードを停止します。
    </para>
    <orderedlist>
     <listitem>
      <para>
       ストレージクライアント
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイ(たとえば、NFS Ganesha、Object Gateway)
      </para>
     </listitem>
     <listitem>
      <para>
       メタデータサーバ
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     必要に応じて、保守タスクを実行します。
    </para>
   </step>
   <step>
    <para>
     ノードとサーバをシャットダウンプロセスの逆の順序で起動します。
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       メタデータサーバ
      </para>
     </listitem>
     <listitem>
      <para>
       ゲートウェイ(たとえば、NFS Ganesha、Object Gateway)
      </para>
     </listitem>
     <listitem>
      <para>
       ストレージクライアント
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     nooutフラグを削除します。
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Cephクラスタ全体の削除</title>

  <para>
   <command>ceph-salt purge</command>コマンドを実行すると、Cephクラスタ全体が削除されます。複数のCephクラスタを展開している場合は、<command>ceph -s</command>コマンドでレポートされるクラスタがパージされます。これにより、異なる設定をテストする際にクラスタ環境をクリーンにすることができます。
  </para>

  <para>
   誤って削除されることがないよう、オーケストレーションは、セキュリティ対策が解除されているかどうかをチェックします。次のコマンドを実行して、セキュリティ対策を解除してCephクラスタを削除できます。
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
