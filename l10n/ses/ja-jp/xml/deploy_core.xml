<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_core.xml" version="5.0" xml:id="deploy-core">
 <info>
  <title>cephadmを使用して残りのコアサービスを展開する</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  基本的なCephクラスタを展開した後、より多くのクラスタノードにコアサービスを展開します。クライアントからクラスタのデータにアクセスできるようにするには、追加のサービスも展開します。
 </para>
 <para>
  現時点では、Cephオーケストレータ(<command>ceph orch</command>サブコマンド)を使用したコマンドライン上でのCephサービスの展開がサポートされています。
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title><command>ceph orch</command>コマンド</title>

  <para>
   Cephオーケストレータコマンドである<command>ceph orch</command>は、新しいクラスタノード上で、クラスタコンポーネントの一覧とCephサービスの展開を行います。このコマンドはcephadmモジュールのインターフェイスです。
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>オーケストレータステータスの表示</title>
   <para>
    次のコマンドは、Cephオーケストレータの現在モードとステータスを表示します。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>デバイス、サービス、デーモンの一覧</title>
   <para>
    すべてのディスクデバイスを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>サービスとデーモン</title>
    <para>
     「サービス」<emphasis></emphasis>とは、特定のタイプのCephサービスを指す総称です。たとえば、Ceph Managerなどです。
    </para>
    <para>
     「デーモン」<emphasis></emphasis>とは、サービスの特定のインスタンスを指します。たとえば、<literal>ses-min1</literal>という名前のノードで実行される<literal>mgr.ses-min1.gdlcik</literal>プロセスなどです。
    </para>
   </tip>
   <para>
    cephadmが認識しているすべてのサービスを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     リストに特定のノードのサービスだけを表示するには、オプションの<option>-–host</option>パラメータを使用します。特定のタイプのサービスだけを表示するには、オプションの<option>--service-type</option>パラメータを使用します。指定できるタイプは、<literal>mon</literal>、<literal>osd</literal>、<literal>mgr</literal>、<literal>mds</literal>、および<literal>rgw</literal>です。
    </para>
   </tip>
   <para>
    cephadmが展開した実行中のすべてのデーモンを一覧にするには、次のコマンドを実行します。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     特定のデーモンのステータスを照会するには、<option>--daemon_type</option>と<option>--daemon_id</option>を使用します。OSDの場合、IDは数字のOSD IDです。MDSの場合、IDはファイルシステム名です。
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd --daemon_id 0
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>サービス仕様と配置仕様</title>

  <para>
   Cephサービスの展開内容を指定する方法としては、YAMLフォーマットのファイルを作成して、展開したいサービスの仕様を記載することをお勧めします。
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>サービス仕様の作成</title>
   <para>
    サービスタイプごとに個別の仕様ファイルを作成できます。以下に例を示します。
   </para>
<screen>
<prompt>root@master # </prompt>cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    もしくは、各サービスを実行するノードを記載した単一のファイル(<filename>cluster.yml</filename>など)により、複数の(または、すべての)サービスタイプを指定することもできます。それぞれのサービスタイプを3つのダッシュ記号(<literal>---</literal>)で区切ることを忘れないでください。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    各プロパティが意味するものは、以下の通りです。
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       サービスのタイプです。次のいずれかを指定できます。Cephサービス(<literal>mon</literal>、<literal>mgr</literal>、<literal>mds</literal>、<literal>crash</literal>、<literal>osd</literal>、<literal>rbd-mirror</literal>)、ゲートウェイ(<literal>nfs</literal> 、<literal>rgw</literal>)、監視スタックの一部(<literal>alertmanager</literal>、<literal>grafana</literal>、<literal>node-exporter</literal>、<literal>prometheus</literal>)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       サービスの名前です。次のサービスタイプについては、<literal>service_id</literal>プロパティは不要です。<literal>mon</literal>、<literal>mgr</literal>、<literal>alertmanager</literal>、<literal>grafana</literal>、<literal>node-exporter</literal>、<literal>prometheus</literal>。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       どのノードがサービスを実行するかを指定します。詳細については、<xref linkend="cephadm-placement-specs"/>を参照してください。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       サービスタイプに関連する、追加仕様です。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>特定のサービスを適用する</title>
    <para>
     通常、Cephクラスタのサービスには、いくつかの固有のプロパティがあります。個別のサービス仕様の例と詳細については、<xref linkend="deploy-cephadm-day2-services"/>を参照してください。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>配置仕様の作成</title>
   <para>
    Cephサービスを展開するには、サービスの展開先ノードをcephadmが認識する必要があります。<literal>placement</literal>プロパティを使用して、サービスを適用するノードのホスト名の略称を列挙してください。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>クラスタ仕様の適用</title>
   <para>
    すべてのサービス仕様とサービスの配置仕様を記載した完全な<filename>cluster.yml</filename>ファイルの作成が完了したら、次のコマンドを実行して、クラスタに仕様を適用してください。
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yml</screen>
   <para>
    クラスタのステータスを確認するには、<command>ceph orch status</command>コマンドを実行します。詳細については、「<xref linkend="deploy-cephadm-day2-orch-status"/>」を参照してください。
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>実行中のクラスタ仕様のエクスポート</title>
   <para>
    <xref linkend="cephadm-service-and-placement-specs"/>で説明した仕様ファイルを用いてCephクラスタにサービスを展開したにもかかわらず、運用中にクラスタの設定が元の仕様から変わる場合もあります。また、誤って仕様ファイルを削除してしまうことも考えられます。
   </para>
   <para>
    実行中のクラスタからすべての仕様を取得するには、次のコマンドを実行してください。
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     <option>--format</option>オプションを付加することで、デフォルトの<literal>yaml</literal>出力フォーマットを変更できます。選択できるフォーマットは、<literal>json</literal>、<literal>json-pretty</literal>、<literal>yaml</literal>です。以下に例を示します。
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Cephサービスの展開</title>

  <para>
   基本的なクラスタの実行後、他のノードにCephサービスを展開できます。
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Ceph MonitorとCeph Managerの展開</title>
   <para>
    Cephクラスタでは、3個または5個のMONを異なるノードに展開します。クラスタに5個以上のノードが含まれる場合、5個のMONを展開することをお勧めします。MONと同じノードにMGRを展開すると良いでしょう。
   </para>
   <important>
    <title>ブートストラップMONを含める</title>
    <para>
     MONとMGRを展開する際は、<xref linkend="deploy-cephadm-configure-mon"/>で基本的なクラスタを構成した際に追加した、最初のMONを忘れずに含めてください。
    </para>
   </important>
   <para>
    MONを展開するには、次の仕様を適用してください。
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     別のノードを追加する必要がある場合は、同じYAMLリストにホスト名を付加してください。以下に例を示します。
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    同様に、MGRを展開するには次の仕様を適用してください。
   </para>
   <important>
    <para>
     展開ごとに、少なくとも3個のCeph Managerが展開されているかを確認してください。
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     MONまたはMGRが同じサブネット上に存在しない<emphasis></emphasis>場合、サブネットアドレスを付加する必要があります。以下に例を示します。
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Ceph OSDの展開</title>
   <important>
    <title>ストレージデバイスが使用可能となる条件</title>
    <para>
     以下の条件をすべて満たす場合、ストレージデバイスは「使用可能」<emphasis></emphasis>とみなされます。
    </para>
    <itemizedlist>
     <listitem>
      <para>
       デバイスにパーティションが作成されていない。
      </para>
     </listitem>
     <listitem>
      <para>
       デバイスがLVM状態ではない。
      </para>
     </listitem>
     <listitem>
      <para>
       デバイスがマウント先になっていない。
      </para>
     </listitem>
     <listitem>
      <para>
       デバイスにファイルシステムが含まれない。
      </para>
     </listitem>
     <listitem>
      <para>
       デバイスにBlueStore OSDが含まれない。
      </para>
     </listitem>
     <listitem>
      <para>
       デバイスのサイズが5GBを超えている。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     これらの条件が満たされない場合、CephはそのOSDのプロビジョニングを拒否します。
    </para>
   </important>
   <para>
    OSDを展開する方法は2つあります。
   </para>
   <itemizedlist>
    <listitem>
     <para>
      「使用可能」とみなされた未使用のストレージデバイスをすべて使用するよう、Cephに指示する方法。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      DriveGroupsを使用してデバイスを記述したOSD仕様を作成し、そのプロパティを基にデバイスを展開する方法(<xref linkend="drive-groups"/>を参照してください)。プロパティの例としては、デバイスの種類(SSDまたはHDD)、デバイスのモデル名、サイズ、デバイスが存在するノードなどがあります。仕様の作成後、次のコマンドを実行して仕様を適用します。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>メタデータサーバの展開</title>
   <para>
    CephFSは1つ以上のMDS(メタデータサーバ)サービスを必要とします。CephFSを作成するには、まず以下の仕様を適用して、MDSサーバを作成する必要があります。
   </para>
   <note>
    <para>
     最低でも2つのプールを作成してから以下の仕様を適用してください。1つはCephFSのデータ用、もう1つはCephFSのメタデータ用のプールです。
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    MDSが機能したら、CephFSを作成します。
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Object Gatewayの展開</title>
   <para>
    cephadmはObject Gatewayを、特定の「レルム」<emphasis></emphasis>と「ゾーン」<emphasis></emphasis>を管理するデーモンのコレクションとして展開します。
   </para>
   <para>
    Object Gatewayサービスを既存のレルムとゾーンに関連付けることも(詳細については、<xref linkend="ceph-rgw-fed"/>を参照してください)、存在しない<replaceable>REALM_NAME</replaceable>と<replaceable>ZONE_NAME</replaceable>を指定することもできます。後者の場合、次の設定を適用すると自動的にゾーンとレルムが作成されます。
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>セキュアなSSLアクセスの使用</title>
    <para>
     Object Gatewayへの接続にセキュアなSSL接続を使用するには、有効なSSL証明書とキーファイルのペアが必要です(詳細については、<xref linkend="ceph-rgw-https"/>を参照してください)。必要な作業は、SSLの有効化、SSL接続のポート番号の指定、SSL証明書とキーファイルの指定です。
    </para>
    <para>
     SSLを有効化し、ポート番号を指定するには、仕様に次の内容を記載します。
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     SSL証明書とキーを指定するには、YAML仕様ファイルに内容を直接ペーストすることができます。行末のパイプ記号(<literal>|</literal>)は、構文解析の際に複数行にまたがる文字列を1つの値として認識させるためのものです。以下に例を示します。
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      SSL証明書とキーファイルの内容をペーストする代わりに、<literal>rgw_frontend_ssl_certificate:</literal>キーワードと<literal>rgw_frontend_ssl_key:</literal>キーワードを削除して、設定データベースにSSL証明書とキーファイルをアップロードすることもできます。
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>ポート443と80の両方でリスンするようにObject Gatewayを設定する</title>
     <para>
      ポート443 (HTTPS)とポート80 (HTTP)の両方でリスンするようにObject Gatewayを設定するには、次の手順に従います。
     </para>
     <note>
      <para>
       この手順のコマンドは、レルムとゾーンの<literal>default</literal>を使用します。
      </para>
     </note>
     <procedure>
      <step>
       <para>
        仕様ファイルを提供して、Object Gatewayを展開します。Object Gateway仕様の詳細については、<xref linkend="deploy-cephadm-day2-service-ogw"/>を参照してください。次のコマンドを実行します。
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        SSL証明書が仕様ファイルで提供されていない場合は、次のコマンドを使用してSSL証明書を追加します。
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        <option>rgw_frontends</option>オプションのデフォルト値を変更します。
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      
      <step>
       <para>
        cephadmによって作成された特定の設定を削除します。次のコマンドを実行して、<option>rgw_frontends</option>オプションが設定されているターゲットを特定します。
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config dump | grep rgw</screen>
       <para>
        たとえば、ターゲットは<literal>client.rgw.default.default.node4.yiewdu</literal>です。現在の特定の<option>rgw_frontends</option>値を削除します。
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         <option>rgw_frontends</option>の値を削除する代わりに指定できます。例:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      
      <step>
       <para>
        Object Gatewayを再起動します。
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>サブクラスタを使用した展開</title>
    <para>
     「サブクラスタ」<emphasis></emphasis>はクラスタ内のノードの整理に役立ちます。これによりワークロードを分離することで、弾力的な拡張が容易になります。サブクラスタを使用して展開する場合は、次の設定を適用します。
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>iSCSI Gatewayの展開</title>
   <para>
    cephadmが展開するiSCSI Gatewayは、クライアント(「イニシエータ」)から、リモートサーバ上のSCSIストレージデバイス(「ターゲット」)にSCSIコマンドを送信できるようにする、SAN(ストレージエリアネットワーク)プロトコルです。
   </para>
   <para>
    展開するには以下の設定を適用します。<literal>trusted_ip_list</literal>にすべてのiSCSI GatewayノードとCeph ManagerノードのIPアドレスが含まれているか確認してください(以下の出力例を参照してください)。
   </para>
   <note>
    <para>
     以下の仕様を適用する前に、プールが作成されているか確認してください。
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     <literal>trusted_ip_list</literal>に列挙されたIPについて、カンマ区切りの後にスペースが入っていない<emphasis></emphasis>ことを確認してください。
    </para>
   </note>
   <sect3>
    <title>セキュアなSSLの設定</title>
    <para>
     セキュアなSSL接続をCephダッシュボードとiSCSIターゲットAPIの間で使用するには、有効なSSL証明書とキーファイルのペアが必要です。証明書とキーファイルは、CAが発行したものか自己署名したものを使用します(<xref linkend="self-sign-certificates"/>を参照してください)。SSLを有効化するには、仕様ファイルに<literal>api_secure: true</literal>設定を含めます。
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     SSL証明書とキーを指定するには、YAML仕様ファイルに内容を直接ペーストすることができます。行末のパイプ記号(<literal>|</literal>)は、構文解析の際に複数行にまたがる文字列を1つの値として認識させるためのものです。以下に例を示します。
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>NFS Ganeshaの展開</title>
    
<important>
 <para>
  NFS Ganeshaは、NFSバージョン4.1以降をサポートしています。NFSバージョン3はサポートしていません。
 </para>
</important>

    <para>
    cephadmはNFS Ganeshaの展開に、事前定義されたRADOSプールとオプションのネームスペースを使用します。NFS Ganeshaを展開するには、次の仕様を適用してください。
   </para>
   <note>
    <para>
     事前定義されたRADOSプールが必要です。これが存在しない場合は、<command>ceph orch apply</command>処理に失敗します。プールの作成の詳細については、<xref linkend="ceph-pools-operate-add-pool"/>を参照してください。
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NFS</replaceable>にはNFSエクスポートを識別する任意の文字列を指定します。
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_POOL</replaceable>にはNFS GaneshaのRADOS設定オブジェクトを保存するプール名を指定します。
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NAMESPACE</replaceable>(オプション)には、希望するObject GatewayのNFSネームスペースを指定します(<literal>ganesha</literal>など)。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>配備 <systemitem class="daemon">rbd-mirror</systemitem></title>
   <para>
    <systemitem class="daemon">rbd-mirror</systemitem>サービスは2つのCephクラスタ間でRADOS Block Deviceイメージの同期を行います(詳細については<xref linkend="ceph-rbd-mirror"/>を参照してください)。<systemitem class="daemon">rbd-mirror</systemitem>を展開するには、次の仕様を使用してください。
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>監視スタックの展開</title>
   <para>
    監視スタックは、Prometheus、Prometheusエクスポータ、Prometheus Alertmanager、Grafanaから構成されます。Cephダッシュボードはこうしたコンポーネントを利用して、クラスタの使用量やパフォーマンスの詳細なメトリクスの保存と視覚化を行います。
   </para>
   <tip>
    <para>
     展開に監視スタックサービスのカスタムコンテナイメージやローカルコンテナイメージを必要とする場合は、<xref linkend="monitoring-custom-images"/>を参照してください。
    </para>
   </tip>
   <para>
    監視スタックを展開するには、以下の手順に従ってください。
   </para>
   <procedure>
    <step>
     <para>
      Ceph Managerデーモンで<literal>prometheus</literal>モジュールを有効化します。これにより、Cephの内部メトリクスが公開され、Prometheusから読み取れるようになります。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable prometheus</screen>
     <note>
      <para>
       このコマンドはPrometheusの展開前に実行してください。展開前にコマンドを実行していない場合、Prometheusを再展開してPrometheusの設定を更新する必要があります。
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      次のような内容を含む仕様ファイル(<filename>monitoring.yaml</filename>など)を作成します。
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      次のコマンドを実行して、監視サービスを適用します。
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i monitoring.yaml</screen>
     <para>
      監視サービスの展開には1、2分かかる場合があります。
     </para>
    </step>
   </procedure>
   <important>
    <para>
     Prometheus、Grafana、Cephダッシュボードは、お互いに通信できるようにすべて自動的に設定されます。そのため、この手順で展開されたとき、Cephダッシュボードには完全に機能するGrafanaが統合されています。
    </para>
    <para>
     このルールの例外は、RBDイメージの監視だけです。詳細については、<xref linkend="monitoring-rbd-image"/>を参照してください。
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
