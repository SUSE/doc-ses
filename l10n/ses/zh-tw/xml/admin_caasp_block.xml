<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_caasp_block.xml" version="5.0" xml:id="admin-caasp-block-storage">

 <title>Block Storage</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Block Storage allows a single pod to mount storage. This guide shows how to
  create a simple, multi-tier web application on Kubernetes using persistent
  volumes enabled by Rook.
 </para>
 <sect1 xml:id="rook-block-storage-provision-storage">
  <title>Provisioning Block Storage</title>

  <para>
   Before Rook can provision storage, a <literal>StorageClass</literal> and a
   <literal>CephBlockPool</literal> need to be created. This will allow Kubernetes
   to interoperate with Rook when provisioning persistent volumes.
  </para>

  <note>
   <para>
    This sample requires <emphasis>at least one OSD per node</emphasis>, with
    each OSD located on <emphasis>three different nodes</emphasis>.
   </para>
  </note>

  <para>
   Each OSD must be located on a different node, because the
   <link xlink:href="https://github.com/rook/rook/blob/release-1.4/Documentation/ceph-pool-crd.md#spec"><literal>failureDomain</literal></link>
   is set to <literal>host</literal> and the <literal>replicated.size</literal>
   is set to <literal>3</literal>.
  </para>

  <note>
   <para>
    This example uses the CSI driver, which is the preferred driver going
    forward for Kubernetes 1.13 and newer. Examples are found in the
    <link xlink:href="https://github.com/rook/rook/tree/release-1.4/cluster/examples/kubernetes/ceph/csi/rbd">CSI
    RBD</link> directory.
   </para>
  </note>

  <para>
   Save this <literal>StorageClass</literal> definition as
   <literal>storageclass.yaml</literal>:
  </para>

<screen>
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  failureDomain: host
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
# Change "rook-ceph" provisioner prefix to match the operator namespace if needed
provisioner: rook-ceph.rbd.csi.ceph.com
parameters:
    # clusterID is the namespace where the rook cluster is running
    clusterID: rook-ceph
    # Ceph pool into which the RBD image shall be created
    pool: replicapool

    # RBD image format. Defaults to "2".
    imageFormat: "2"

    # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only `layering` feature.
    imageFeatures: layering

    # The secrets contain Ceph admin credentials.
    csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
    csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
    csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    # Specify the filesystem type of the volume. If not specified, csi-provisioner
    # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    # in hyperconverged settings where the volume is mounted on the same node as the osds.
    csi.storage.k8s.io/fstype: ext4

# Delete the rbd volume when a PVC is deleted
reclaimPolicy: Delete
</screen>

  <para>
   If you have deployed the Rook operator in a namespace other than
   <quote>rook-ceph</quote>, change the prefix in the provisioner to match the
   namespace you used. For example, if the Rook operator is running in the
   namespace <quote>my-namespace</quote> the provisioner value should be
   <quote>my-namespace.rbd.csi.ceph.com</quote>.
  </para>

  <para>
   Create the storage class.
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>kubectl create -f cluster/examples/kubernetes/ceph/csi/rbd/storageclass.yaml</screen>

  <note>
   <para>
    As
    <link xlink:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain">specified
    by Kubernetes</link>, when using the <literal>Retain</literal> reclaim policy,
    any Ceph RBD image that is backed by a
    <literal>PersistentVolume</literal> will continue to exist even after the
    <literal>PersistentVolume</literal> has been deleted. These Ceph RBD
    images will need to be cleaned up manually using <literal>rbd rm</literal>.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="consume-the-storage-wordpress-sample">
  <title>Consuming storage: WordPress sample</title>

  <para>
   In this example, we will create a sample application to consume the block
   storage provisioned by Rook with the classic WordPress and MySQL apps.
   Both of these applications will make use of block volumes provisioned by
   Rook.
  </para>

  <para>
   Start MySQL and WordPress from the
   <literal>cluster/examples/kubernetes</literal> folder:
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>kubectl create -f mysql.yaml
kubectl create -f wordpress.yaml</screen>

  <para>
   Both of these applications create a block volume, and mount it to their
   respective pod. You can see the Kubernetes volume claims by running the
   following:
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>kubectl get pvc
NAME             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
mysql-pv-claim   Bound     pvc-95402dbc-efc0-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m
wp-pv-claim      Bound     pvc-39e43169-efc1-11e6-bc9a-0cc47a3459ee   20Gi       RWO           1m</screen>

  <para>
   Once the WordPress and MySQL pods are in the <literal>Running</literal>
   state, get the cluster IP of the WordPress app and enter it in your browser:
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>kubectl get svc wordpress
NAME        CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE
wordpress   10.3.0.155   &lt;pending&gt;     80:30841/TCP   2m</screen>

  <para>
   You should see the WordPress application running.
  </para>

  <para>
   If you are using Minikube, the WordPress URL can be retrieved with this
   one-line command:
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>echo http://$(minikube ip):$(kubectl get service wordpress -o jsonpath='{.spec.ports[0].nodePort}')</screen>

  <note>
   <para>
    When running in a Vagrant environment, there will be no external IP address
    to reach WordPress with. You will only be able to reach WordPress via the
    <literal>CLUSTER-IP</literal> from inside the Kubernetes cluster.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="consume-the-storage-toolbox">
  <title>Consuming the storage: Toolbox</title>

  <para>
   With the pool that was created above, we can also create a block image and
   mount it directly in a pod.
  </para>
 </sect1>
 <sect1 xml:id="rook-block-storage-teardown">
  <title>Teardown</title>

  <para>
   To clean up all the artifacts created by the block-storage demonstration:
  </para>

<screen><prompt>kubectl@adm &gt; </prompt>kubectl delete -f wordpress.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl delete -f mysql.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl delete -n rook-ceph cephblockpools.ceph.rook.io replicapool
<prompt>kubectl@adm &gt; </prompt>kubectl delete storageclass rook-ceph-block</screen>
 </sect1>
 <sect1 xml:id="advanced-example-erasure-coded-block-storage">
  <title>Advanced Example: Erasure-Coded Block Storage</title>

  <para>
   If you want to use erasure-coded pools with RBD, your OSDs must use
   <literal>bluestore</literal> as their <literal>storeType</literal>.
   Additionally, the nodes that will mount the erasure-coded RBD block storage
   must have Linux kernel <literal>4.11</literal> or above.
  </para>

  <para>
   This example requires <emphasis>at least three bluestore OSDs</emphasis>,
   with each OSD located on a <emphasis>different node</emphasis>.
  </para>

  <para>
   The OSDs must be located on different nodes, because the
   <literal>failureDomain</literal> is set to <literal>host</literal> and the
   <literal>erasureCoded</literal> chunk settings require at least three
   different OSDs (two <literal>dataChunks</literal> plus one
   <literal>codingChunk</literal>).
  </para>

  <para>
   To be able to use an erasure-coded pool, you need to create two pools (as
   seen below in the definitions): one erasure-coded, and one replicated.
  </para>

  <sect2 xml:id="erasure-coded-csi-driver">
   <title>Erasure coded CSI driver</title>
   <para>
    The erasure-coded pool must be set as the <literal>dataPool</literal>
    parameter in
    <link xlink:href="https://github.com/rook/rook/blob/release-1.4/cluster/examples/kubernetes/ceph/csi/rbd/storageclass-ec.yaml"><literal>storageclass-ec.yaml</literal></link>
    It is used for the data of the RBD images.
   </para>
  </sect2>
 </sect1>
</chapter>
