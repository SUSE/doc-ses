<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_bootstrap.xml" version="5.0" xml:id="deploy-bootstrap">
 <info>
  <title>Implantando o cluster de boot usando <systemitem class="resource">ceph-salt</systemitem></title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Esta seção orienta você pelo processo de implantação de um cluster básico do Ceph. Leia as subseções a seguir com atenção e execute os comandos incluídos na ordem indicada.
 </para>
 <sect1 xml:id="deploy-cephadm-cephsalt">
  <title>Instalando <systemitem class="resource">ceph-salt</systemitem></title>

  <para>
   O <systemitem class="resource">ceph-salt</systemitem> inclui ferramentas para implantar clusters do Ceph gerenciados pelo cephadm. O <systemitem class="resource">ceph-salt</systemitem> usa a infraestrutura do Salt para executar o gerenciamento de OS; por exemplo, atualizações de software ou sincronização de horário, e definir funções para os Minions Salt.
  </para>

  <para>
   No Master Salt, instale o pacote <package>ceph-salt</package>:
  </para>

<screen><prompt>root@master # </prompt>zypper install ceph-salt</screen>

  <para>
   O comando acima instalou <package>ceph-salt-formula</package> como uma dependência que modificou a configuração do Master Salt inserindo arquivos adicionais no diretório <filename>/etc/salt/master.d</filename>. Para aplicar as mudanças, reinicie o <systemitem class="daemon">salt-master.service</systemitem> e sincronize os módulos do Salt:
  </para>

<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
 </sect1>
 <sect1 xml:id="deploy-cephadm-configure">
  <title>Configurando as propriedades do cluster</title>

  <para>
   Use o comando <command>ceph-salt config</command> para configurar as propriedades básicas do cluster.
  </para>

  <important>
   <para>
    O arquivo <filename>/etc/ceph/ceph.conf</filename> é gerenciado pelo cephadm e os usuários <emphasis>não devem</emphasis> editá-lo. Os parâmetros de configuração do Ceph devem ser definidos usando o novo comando <command>ceph config</command>. Consulte a <xref linkend="cha-ceph-configuration-db"/> para obter mais informações.
   </para>
  </important>

  <sect2 xml:id="deploy-cephadm-configure-shell">
   <title>Usando o shell do <systemitem class="resource">ceph-salt</systemitem></title>
   <para>
    Se você executar o <command> config</command> sem nenhum caminho ou subcomando, digitará um shell interativo do <systemitem class="resource">ceph-salt</systemitem>ceph-salt. O shell é prático se você precisa configurar várias propriedades em um lote e não deseja digitar a sintaxe completa do comando.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config
<prompt>/&gt;</prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
   <para>
    Como você pode ver na saída do comando <systemitem class="resource">ceph-salt</systemitem>ls<command> do </command>, a configuração do cluster está organizada em uma estrutura de árvore. Para configurar uma propriedade específica do cluster no shell do <systemitem class="resource">ceph-salt</systemitem>, você tem duas opções:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Execute o comando a partir da posição atual e digite o caminho absoluto para a propriedade como o primeiro argumento:
     </para>
<screen>
<prompt>/&gt;</prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/&gt; /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
    </listitem>
    <listitem>
     <para>
      Mude para o caminho com a propriedade que você precisa configurar e execute o comando:
     </para>
<screen>
<prompt>/&gt;</prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions&gt;</prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
    </listitem>
   </itemizedlist>
   <tip>
    <title>Preenchimento automático de trechos da configuração</title>
    <para>
     Enquanto estiver em um shell do <systemitem class="resource">ceph-salt</systemitem>, você poderá usar o recurso de preenchimento automático semelhante ao do shell normal do Linux (Bash). Ele preenche os caminhos de configuração, subcomandos ou nomes de Minion Salt. Ao preencher automaticamente um caminho de configuração, você tem duas opções:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Para permitir que o shell termine um caminho relativo à sua posição atual, pressione a tecla TAB <keycap function="tab"/> duas vezes.
      </para>
     </listitem>
     <listitem>
      <para>
       Para permitir que o shell termine um caminho absoluto, digite <keycap>/</keycap> e pressione a tecla TAB <keycap function="tab"/> duas vezes.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <tip>
    <title>Navegando com as teclas do cursor</title>
    <para>
     Se você digitar <command>cd</command> no shell do <systemitem class="resource">ceph-salt</systemitem> sem nenhum caminho, o comando imprimirá uma estrutura de árvore da configuração do cluster com a linha do caminho atual ativo. Você pode usar as teclas do cursor para cima e para baixo para navegar pelas linhas individuais. Depois de confirmar com <keycap function="enter"/>, o caminho de configuração mudará para o último caminho ativo.
    </para>
   </tip>
   <important>
    <title>Convenção</title>
    <para>
     Para manter a documentação consistente, usaremos uma única sintaxe de comando sem inserir o shell do <systemitem class="resource">ceph-salt</systemitem>. Por exemplo, você pode listar a árvore de configuração do cluster usando o seguinte comando:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
   </important>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-minions">
   <title>Adicionando minions Salt</title>
   <para>
    Inclua todos ou um subconjunto de Minions Salt que implantamos e aceitamos na <xref linkend="deploy-salt"/> na configuração do cluster do Ceph. Você pode especificar os Minions Salt usando os nomes completos ou as expressões glob &quot;*&quot; e &quot;?&quot; para incluir vários Minions Salt de uma vez. Use o subcomando <command>add</command> no caminho <literal>/ceph_cluster/minions</literal>. O seguinte comando inclui todos os Minions Salt aceitos:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add '*'</screen>
   <para>
    Verifique se os Minions Salt especificados foram adicionados:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-cephadm">
   <title>Especificando minions Salt gerenciados pelo cephadm</title>
   <para>
    Especifique os nós que pertencerão ao cluster do Ceph e serão gerenciados pelo cephadm. Inclua todos os nós que executarão os serviços do Ceph e também o Nó de Admin:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-admin">
   <title>Especificando o nó de admin</title>
   <para>
    O Nó de Admin é o nó em que o arquivo de configuração <filename>ceph.conf</filename> e o chaveiro admin do Ceph estão instalados. Geralmente, você executa os comandos relacionados ao Ceph no Nó de Admin.
   </para>
   <tip>
    <title>Master Salt e nó de admin no mesmo nó</title>
    <para>
     Em um ambiente homogêneo onde todos ou a maioria dos hosts pertencem ao SUSE Enterprise Storage, recomendamos manter o Nó de Admin no mesmo host que o Master Salt.
    </para>
    <para>
     Em um ambiente heterogêneo onde uma infraestrutura do Salt hospeda mais de um cluster, por exemplo, o SUSE Enterprise Storage junto com o SUSE Manager, <emphasis>não</emphasis> coloque o Nó de Admin no mesmo host que o Master Salt.
    </para>
   </tip>
   <para>
    Para especificar o Nó de Admin, execute o seguinte comando:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
   <tip>
    <title>Instalar o <filename>ceph.conf</filename> e o chaveiro admin em vários nós</title>
    <para>
     Você pode instalar o arquivo de configuração do Ceph e o chaveiro admin em vários nós, se sua implantação exigir isso. Por motivos de segurança, evite instalá-los em todos os nós do cluster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-mon">
   <title>Especificando o primeiro nó MON/MGR</title>
   <para>
    Você precisa especificar qual dos Minions Salt do cluster inicializará o cluster. Esse minion será o primeiro a executar os serviços Ceph Monitor e Ceph Manager.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
   <para>
    Além disso, você precisa especificar o endereço IP do MON de boot na rede pública para garantir que o parâmetro <option>public_network</option> seja definido corretamente, por exemplo:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-tuned-profiles">
   <title>Especificando perfis ajustados</title>
   <para>
    Você precisa especificar quais dos minions do cluster têm os perfis ajustados ativamente. Para fazer isso, adicione estas funções explicitamente com os seguintes comandos:
   </para>
   <note>
    <para>
     Um minion não pode ter as duas funções <literal>latency</literal> e <literal>throughput</literal>.
    </para>
   </note>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ssh">
   <title>Gerando um par de chaves SSH</title>
   <para>
    O cephadm usa o protocolo SSH para se comunicar com os nós do cluster. Uma conta do usuário chamada <literal>cephadm</literal> é criada automaticamente e usada para comunicação por SSH.
   </para>
   <para>
    Você precisa gerar a parte particular e a pública do par de chaves SSH:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ssh generate
Key pair generated.
<prompt>root@master # </prompt>ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ntp">
   <title>Configurando o servidor de horário</title>
   <para>
    Todos os nós do cluster precisam ter o horário sincronizado com uma fonte de horário confiável. Há vários cenários para realizar a sincronização de horário:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Se todos os nós do cluster já estiverem configurados para sincronizar o horário usando um serviço NTP de sua escolha, desabilite completamente o processamento do servidor de horário:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server disable
</screen>
    </listitem>
    <listitem>
     <para>
      Se o seu site já tiver uma fonte de horário única, especifique o nome de host dessa fonte:
     </para>
<screen>
 <prompt>root@master # </prompt>ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
    </listitem>
    <listitem>
     <para>
      Se preferir, o <systemitem class="resource">ceph-salt</systemitem> pode configurar um dos Minions Salt para agir como o servidor de horário para o restante do cluster. Esse recurso às vezes é chamado de &quot;servidor de horário interno&quot;. Nesse cenário, o <systemitem class="resource">ceph-salt</systemitem> configura o servidor de horário interno (que deve ser um dos Minions Salt) para sincronizar seu horário com um servidor de horário externo, como <literal>pool.ntp.org</literal>, e configura todos os outros minions para obter o horário do servidor de horário interno. Isso pode ser feito da seguinte maneira:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/servers add ses-master.example.com
<prompt>root@master # </prompt>ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
     <para>
      A opção <option>/time_server/subnet</option> especifica a sub-rede da qual os clientes NTP têm permissão para acessar o servidor NTP. Ela é definida automaticamente quando você especifica <option>/time_server/servers</option>. Se você precisar mudá-la ou especificá-la manualmente, execute:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
    </listitem>
   </itemizedlist>
   <para>
    Verifique as configurações do servidor de horário:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
   <para>
    Encontre mais informações sobre como configurar a sincronização de horário em <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
   </para>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-dashboardlogin">
   <title>Configurando as credenciais de login do Ceph Dashboard</title>
   <para>
    O Ceph Dashboard estará disponível após a implantação do cluster básico. Para acessá-lo, você precisa definir um nome de usuário e uma senha válidos, por exemplo:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/username set admin
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
   <tip>
    <title>Forçando atualização da senha</title>
    <para>
     Por padrão, o primeiro usuário do painel de controle será forçado a mudar sua senha ao efetuar o primeiro login. Para desabilitar esse recurso, execute o seguinte comando:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-registry">
   <title>Usando o registro do container</title>
   <para>
    O cluster do Ceph precisa ter acesso a um registro de container para que possa fazer download e implantar os serviços do Ceph em container. Há duas maneiras de acessar o registro:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Se o cluster puder acessar o registro padrão em <literal>registration.suse.com</literal> (diretamente ou por proxy), você poderá apontar o <systemitem class="resource">ceph-salt</systemitem> diretamente para esse URL sem criar um registro local. Continue seguindo as etapas em <xref linkend="deploy-cephadm-configure-imagepath"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Se o cluster não puder acessar o registro padrão, por exemplo, para uma implantação isolada (air-gapped), você precisará configurar um registro de container local. Depois que o registro local for criado e configurado, você precisará apontar o <systemitem class="resource">ceph-salt</systemitem> para ele.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="updating-ceph-local-registry">
    <title>Criando e configurando o registro local (opcional)</title>
    <important>
     <para>
      Há vários métodos de criação de um registro local. As instruções nesta seção são exemplos de criação de registros seguros e não seguros. Para obter informações gerais sobre a execução de um registro de imagem de container, consulte <link xlink:href="https://documentation.suse.com/sles/15-SP3/single-html/SLES-container/#sec-docker-registry-installation"/>.
     </para>
    </important>
    <tip>
     <title>Posicionamento e uso de porta</title>
     <para>
      Implante o registro em uma máquina acessível a todos os nós no cluster. Recomendamos o Nó de Admin. Por padrão, o registro escuta na porta 5000.
     </para>
     <para>
      No nó de registro, use o seguinte comando para garantir que a porta esteja livre:
     </para>
<screen>ss -tulpn | grep :5000</screen>
     <para>
      Se outros processos (como <literal>iscsi-tcmu</literal>) já estiverem escutando na porta 5000, determine outra porta livre que possa ser usada para mapear para a porta 5000 no container de registro.
     </para>
    </tip>
    <procedure>
     <title>Criando o registro local</title>
     <step>
      <para>
       Verifique se a extensão <package>Containers Module</package> está habilitada:
      </para>
<screen>
<prompt>&gt; </prompt>SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP3 x86_64 (Activated)
</screen>
     </step>
     <step>
      <para>
       Verifique se os seguintes pacotes estão instalados: <package>apache2-utils</package> (no caso de habilitar um registro seguro), <package>cni</package>, <package>cni-plugins</package>, <package>podman</package>, <package>podman-cni-config</package> e <package>skopeo</package>.
      </para>
     </step>
     <step>
      <para>
       Colete as seguintes informações:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Nome de domínio completo e qualificado do host de registro (<option>REG_HOST_FQDN</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Um número de porta disponível usado para mapear para a porta 5000 do container de registro (<option>REG_HOST_PORT</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Se o registro será seguro ou não seguro (<option>insecure=[true|false]</option>).
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Para iniciar um registro não seguro (sem criptografia SSL), siga estas etapas:
      </para>
      <substeps>
       <step>
        <para>
         Configure o <systemitem class="resource">ceph-salt</systemitem> para o registro não seguro:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf enable
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf/registries \
 add prefix=<option>REG_HOST_FQDN</option> insecure=true \
 location=<option>REG_HOST_PORT</option>:5000
</screen>
       </step>
       <step>
        <para>
         Inicie o registro não seguro criando o diretório necessário (por exemplo, <filename>/var/lib/registry</filename>) e iniciando o registro com o comando <command>podman</command>:
        </para>
<screen>
<prompt role="root"># </prompt>mkdir -p /var/lib/registry
<prompt role="root"># </prompt>podman run --privileged -d --name registry \
 -p <option>REG_HOST_PORT</option>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
       </step>
       <step>
        <para>
         Para que o registro seja iniciado após uma reinicialização, crie um arquivo da unidade <systemitem class="daemon">systemd</systemitem> para ele e habilite-o:
        </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> podman generate systemd --files --name registry
<prompt>&gt; </prompt><command>sudo</command> mv container-registry.service /etc/systemd/system/
<prompt>&gt; </prompt><command>sudo</command> systemctl enable container-registry.service
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Para iniciar um registro seguro, siga estas etapas:
      </para>
      <substeps>
       <step>
        <para>
         Crie os diretórios necessários:
        </para>
<screen><prompt role="root"># </prompt>mkdir -p /var/lib/registry/{auth,certs}</screen>
       </step>
       <step>
        <para>
         Gerar um certificado SSL:
        </para>
<screen>
<prompt role="root"># </prompt>openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt
</screen>
        <note>
         <para>
          Defina o valor <literal>CN=[value]</literal> como o nome de domínio completo e qualificado do host ([<option>REG_HOST_FQDN</option>]).
         </para>
        </note>
       </step>
       <step>
        <para>
         Copie o certificado para todos os nós do cluster e atualize o cache do certificado:
        </para>
<screen>
<prompt role="root"># </prompt>salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
<prompt role="root"># </prompt>salt '*' cmd.shell "update-ca-certificates"
</screen>
       </step>
       <step>
        <para>
         Gere uma combinação de nome de usuário e senha para autenticação no registro:
        </para>
<screen>
<prompt role="root"># </prompt>htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <option>REG_USERNAME</option> <option>REG_PASSWORD</option>
</screen>
       </step>
       <step>
        <para>
         Inicie o registro seguro. Use o flag <option>REGISTRY_STORAGE_DELETE_ENABLED=true</option> para que você possa apagar imagens posteriormente com o comando <command>skopeo delete</command>.
        </para>
<screen>
podman run --name myregistry -p <option>REG_HOST_PORT</option>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_STORAGE_DELETE_ENABLED=true \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
       </step>
       <step>
        <para>
         Teste o acesso seguro ao registro:
        </para>
<screen>
<prompt>&gt; </prompt>curl https://<option>REG_HOST_FQDN</option>:<option>REG_HOST_PORT</option>/v2/_catalog \
 -u <option>REG_USERNAME</option>:<option>REG_PASSWORD</option>
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Quando o registro local é criado, você precisa sincronizar as imagens de container do registro oficial do SUSE em <literal>registry.suse.com</literal> com o registro local. Você pode usar o comando <command>skopeo sync</command> incluído no pacote <package>skopeo</package> para essa finalidade. Para obter mais detalhes, consulte a página de manual (<command>man 1 skopeo-sync</command>). Considere estes exemplos:
      </para>
      <example>
       <title>Vendo arquivos de manifesto</title>
<screen>
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/ceph | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/grafana | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 | jq .RepoTags
</screen>
      </example>
      <example>
       <title>Sincronizar com um diretório</title>
       <para>
        Sincronizar todas as imagens do Ceph:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph /root/images/</screen>
       <para>
        Sincronizar apenas as imagens mais recentes:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph:latest /root/images/</screen>
      </example>
      <example>
       <title>Sincronizar as imagens do Grafana:</title>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana /root/images/</screen>
       <para>
        Sincronizar apenas as imagens mais recentes do Grafana:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana:latest /root/images/</screen>
      </example>
      <example>
       <title>Sincronizar as imagens mais recentes do Prometheus</title>
<screen>
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 /root/images/
</screen>
      </example>
     </step>
    </procedure>
    <procedure>
     <title>Configurar o registro local e as credenciais de acesso</title>
     <step>
      <para>
       Configure o URL do registro local:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/registry set <replaceable>REG_HOST_URL</replaceable></screen>
     </step>
     <step>
      <para>
       Configure o nome de usuário e a senha para acessar o registro local:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable></screen>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable></screen>
     </step>
    </procedure>
    <tip>
     <title>Cache de registro</title>
     <para>
      Para evitar a ressincronização do registro local quando novos containers atualizados forem exibidos, você pode configurar um <emphasis>cache de registro</emphasis>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configurando o caminho para imagens de container</title>
    <important>
     <para>
      Esta seção ajuda você a configurar o caminho para as imagens de container do cluster de boot (implantação do primeiro par de Ceph Monitor e Ceph Manager). O caminho não se aplica a imagens de container de serviços adicionais, por exemplo, a pilha de monitoramento.
     </para>
    </important>
    <tip>
     <title>Configurando o proxy HTTPS</title>
     <para>
      Se você precisa usar um proxy para se comunicar com o servidor de registro do container, execute as seguintes etapas de configuração em todos os nós do cluster:
     </para>
     <procedure>
      <step>
       <para>
        Copie o arquivo de configuração dos containers:
       </para>
<screen><prompt>&gt; </prompt><command>sudo</command> cp /usr/share/containers/containers.conf /etc/containers/containers.conf</screen>
      </step>
      <step>
       <para>
        Edite o arquivo recém-copiado e adicione a configuração <option>http_proxy</option> a esta seção <literal>[engine]</literal>, por exemplo:
       </para>
<screen><prompt>&gt; </prompt>cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]
 </screen>
      </step>
     </procedure>
    </tip>
    <para>
     O cephadm precisa saber um caminho de URI válido para as imagens de container. Verifique a configuração padrão executando:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
    <para>
     Se você não precisa de um registro alternativo ou local, especifique o registro de container do SUSE padrão:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7.1/ceph/ceph</screen>
    <para>
     Se a implantação exigir um caminho específico, por exemplo, um caminho para um registro local, configure-o da seguinte maneira:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-inflight-encryption">
   <title>Habilitando a criptografia de dados em trânsito (msgr2)</title>
   <para>
    O Messenger v2 (MSGR2) é o protocolo on-wire do Ceph. Ele oferece um modo de segurança que criptografa todos os dados que passam pela rede, encapsulamento de payloads de autenticação e habilitação da integração futura de novos modos de autenticação (como Kerberos).
   </para>
   <important>
    <para>
     Atualmente, o msgr2 não é suportado pelos clientes Ceph do kernel do Linux, como CephFS e Dispositivo de Blocos RADOS.
    </para>
   </important>
   <para>
    Os daemons do Ceph podem se vincular a várias portas, permitindo que os clientes Ceph legados e os novos clientes compatíveis com a versão 2 se conectem ao mesmo cluster. Por padrão, os MONs agora se vinculam à nova porta 3300 atribuída pela IANA (CE4h ou 0xCE4) para o novo protocolo v2 e também à porta antiga padrão 6789 para o protocolo v1 legado.
   </para>
   <para>
    O protocolo v2 (MSGR2) suporta dois modos de conexão:
   </para>
   <variablelist>
    <varlistentry>
     <term>modo crc</term>
     <listitem>
      <para>
       Uma autenticação inicial forte quando a conexão é estabelecida e uma verificação de integridade CRC32C.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>modo seguro</term>
     <listitem>
      <para>
       Uma autenticação inicial forte quando a conexão é estabelecida e a criptografia completa de todo o tráfego pós-autenticação, incluindo uma verificação de integridade criptográfica.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para a maioria das conexões, há opções que controlam os modos que são usados:
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_cluster_mode</term>
     <listitem>
      <para>
       O modo de conexão (ou modos permitidos) usado para comunicação intracluster entre os daemons do Ceph. Se houver vários modos na lista, a preferência será dos que forem listados primeiro.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_service_mode</term>
     <listitem>
      <para>
       Uma lista de modos permitidos para os clientes usarem na conexão com o cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_client_mode</term>
     <listitem>
      <para>
       Uma lista de modos de conexão, em ordem de preferência, para os clientes usarem (ou permitirem) na comunicação com um cluster do Ceph.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Há um conjunto paralelo de opções que se aplicam especificamente aos monitores, permitindo que os administradores definam requisitos diferentes (geralmente mais seguros) para comunicação com os monitores.
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_mon_cluster_mode</term>
     <listitem>
      <para>
       O modo de conexão (ou modos permitidos) que será usado entre os monitores.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_service_mode</term>
     <listitem>
      <para>
       Uma lista de modos permitidos para clientes ou outros daemons do Ceph usarem na conexão com monitores.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_client_mode</term>
     <listitem>
      <para>
       Uma lista de modos de conexão, em ordem de preferência, para clientes ou daemons que não são de monitor usarem na conexão com monitores.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para habilitar o modo de criptografia MSGR2 durante a implantação, você precisa adicionar algumas opções à configuração do <systemitem class="resource">ceph-salt</systemitem> antes de executar o <command>ceph-salt apply</command>.
   </para>
   <para>
    Para usar o modo <literal>secure</literal>, execute os comandos a seguir.
   </para>
   <para>
    Adicione a seção global ao <filename>ceph_conf</filename> na ferramenta de configuração do <systemitem class="resource">ceph-salt</systemitem>:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
   <para>
    Defina as seguintes opções:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
   <note>
    <para>
     Certifique-se de que <literal>secure</literal> venha antes de <literal>crc</literal>.
    </para>
   </note>
   <para>
    Para <emphasis>forçar o modo</emphasis> <literal>secure</literal>, execute os seguintes comandos:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
   <tip xml:id="update-inflight-encryption-settings">
    <title>Atualizando as configurações</title>
    <para>
     Para mudar qualquer uma das configurações acima, defina as mudanças de configuração no armazenamento de configuração do monitor. Isso é feito usando o comando <command>ceph config set</command>.
    </para>
<screen><prompt>root@master # </prompt>ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global ms_cluster_mode "secure crc"</screen>
    <para>
     Para verificar o valor atual, incluindo o valor padrão, execute o seguinte comando:
    </para>
<screen><prompt>root@master # </prompt>ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
    <para>
     Por exemplo, para obter o <literal>ms_cluster_mode</literal> dos OSD&apos;s, execute:
    </para>
<screen><prompt>root@master # </prompt>ceph config get osd ms_cluster_mode</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-enable-network">
   <title>Configurando a rede do cluster</title>
   <para>
    Opcionalmente, se você executar uma rede de cluster separada, talvez seja necessário definir o endereço IP da rede do cluster seguido pela parte da máscara de sub-rede após a barra, por exemplo, <literal>192.168.10.22/24</literal>.
   </para>
   <para>
    Execute os seguintes comandos para habilitar <literal>cluster_network</literal>:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-verify">
   <title>Verificando a configuração do cluster</title>
   <para>
    A configuração mínima do cluster foi concluída. Verifique se há erros óbvios:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .............. [registry.suse.com/ses/7.1/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
   <tip>
    <title>Status da configuração do cluster</title>
    <para>
     Você pode verificar se a configuração do cluster é válida executando o seguinte comando:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-export">
   <title>Exportando as configurações do cluster</title>
   <para>
    Depois de configurar o cluster básico e sua configuração estiver válida, convém exportá-la para um arquivo:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt export &gt; cluster.json</screen>
   <warning>
    <para>
     A saída do comando <command>ceph-salt export</command> inclui a chave privada SSH. Se você estiver preocupado com as implicações de segurança, não execute esse comando sem tomar as devidas precauções.
    </para>
   </warning>
   <para>
    Caso você danifique a configuração do cluster e tenha de reverter para um estado de backup, execute:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt import cluster.json</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-deploy">
  <title>Atualizando os nós e o cluster mínimo de boot</title>

  <para>
   Antes de implantar o cluster, atualize todos os pacotes de software em todos os nós:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update</screen>

  <para>
   Se um nó relatar que a <literal>Reinicialização é necessária</literal> durante a atualização, os pacotes importantes do OS, como o kernel, foram atualizados para uma versão mais recente, e você precisa reinicializar o nó para aplicar as mudanças.
  </para>

  <para>
   Para reinicializar todos os nós que exigem reinicialização, anexe a opção <option>--reboot</option>
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update --reboot</screen>

  <para>
   Se preferir, reinicialize-os em uma etapa separada:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt reboot</screen>

  <important>
   <para>
    O Master Salt nunca é reinicializado pelos comandos <command>ceph-salt update --reboot</command> ou <command>ceph-salt reboot</command>. Se o Master Salt precisar ser reinicializado, você deverá reinicializá-lo manualmente.
   </para>
  </important>

  <para>
   Após a atualização dos nós, inicialize o cluster mínimo:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt apply</screen>

  <note>
   <para>
    Quando a inicialização for concluída, o cluster terá um Ceph Monitor e um Ceph Manager.
   </para>
  </note>

  <para>
   O comando acima abrirá uma interface do usuário interativa que mostra o andamento atual de cada minion.
  </para>

  <figure>
   <title>Implantação de um cluster mínimo</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>Modo não interativo</title>
   <para>
    Se você precisa aplicar a configuração de um script, também há um modo de implantação não interativo. Ele também é útil para implantar o cluster de uma máquina remota, porque a atualização constante das informações de andamento na tela pela rede pode provocar distração:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt apply --non-interactive</screen>
  </tip>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-final-steps">
  <title>Revisando as etapas finais</title>

  <para>
   Após a conclusão do comando <command>ceph-salt apply</command>, você deverá ter um Ceph Monitor e um Ceph Manager. Você deve conseguir executar o comando <command>ceph status</command> com êxito em qualquer um dos minions que receberam a função <literal>admin</literal> como <literal>root</literal> ou o usuário <literal>cephadm</literal> por meio do <literal>sudo</literal>.
  </para>

  <para>
   As próximas etapas envolvem o uso do cephadm para implantar mais Ceph Monitor, Ceph Manager, OSDs, Pilha de Monitoramento e Gateways.
  </para>

  <para>
   Antes de continuar, revise as novas configurações de rede do cluster. Neste ponto, a configuração <literal>public_network</literal> foi preenchida com base no que foi inserido para <literal>/cephadm_bootstrap/mon_ip</literal> na configuração do <literal>ceph-salt</literal>. No entanto, essa configuração foi aplicada apenas ao Ceph Monitor. Você pode revisá-la com o seguinte comando:
  </para>

<screen><prompt>root@master # </prompt>ceph config get mon public_network</screen>

  <para>
   Esse é o mínimo necessário para o Ceph funcionar, mas recomendamos tornar essa configuração <literal>public_network</literal> <literal>global</literal>, o que significa que ela será aplicada a todos os tipos de daemons do Ceph, e não apenas aos MONs:
  </para>

<screen><prompt>root@master # </prompt>ceph config set global public_network "$(ceph config get mon public_network)"</screen>

  <note>
   <para>
    Essa etapa não é obrigatória. No entanto, se você não usar essa configuração, os Ceph OSDs e outros daemons (exceto o Ceph Monitor) escutarão em <emphasis>todos os endereços</emphasis>.
   </para>
   <para>
    Para que seus OSDs se comuniquem entre si usando uma rede completamente separada, execute o seguinte comando:
   </para>
<screen><prompt>root@master # </prompt>ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
   <para>
    A execução desse comando garante que os OSDs criados em sua implantação usem a rede de cluster pretendida desde o início.
   </para>
  </note>

  <para>
   Se o cluster estiver definido para ter nós densos (mais de 62 OSDs por host), atribua portas suficientes aos Ceph OSDs. Atualmente, a faixa padrão (6800-7300) não permite mais do que 62 OSDs por host. Para um cluster com nós densos, ajuste a configuração <literal>ms_bind_port_max</literal> para um valor adequado. Cada OSD consumirá oito portas adicionais. Por exemplo, um host definido para executar 96 OSDs requer 768 portas. <literal>ms_bind_port_max</literal> deve ser definido, no mínimo, como 7568 executando o seguinte comando:
  </para>

<screen><prompt>root@master # </prompt>ceph config set osd.* ms_bind_port_max 7568</screen>

  <para>
   Você precisará ajustar as configurações de firewall de acordo para que isso funcione. Consulte a <xref linkend="storage-bp-net-firewall"/> para obter mais informações.
  </para>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-disable-insecure">
  <title>Desabilitar clientes não seguros</title>

  <para>
   Desde o Pacific v15.2.11, um novo aviso de saúde foi implementado para informar a você que clientes não seguros têm permissão para ingressar no cluster. Por padrão, esse aviso está <emphasis>ativado</emphasis>. O Ceph Dashboard mostrará o cluster no status <literal>HEALTH_WARN</literal>, e a verificação do status do cluster na linha de comando informará o seguinte:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>

  <para>
   Esse aviso significa que os Ceph Monitors ainda permitem que clientes antigos e sem patch se conectem ao cluster. Isso garante que os clientes existentes ainda consigam se conectar durante o upgrade do cluster, mas avisa você de que há um problema que precisa ser resolvido. Quando o upgrade do cluster e de todos os clientes for feito para a versão mais recente do Ceph, execute o seguinte comando para não permitir os clientes sem patch:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
 </sect1>
</chapter>
