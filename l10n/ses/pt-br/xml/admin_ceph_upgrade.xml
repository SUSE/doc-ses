<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade do SUSE Enterprise Storage 6 para 7.1</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Este capítulo apresenta as etapas para fazer upgrade do SUSE Enterprise Storage 6 para a versão 7.1.
 </para>
 <para>
  O upgrade inclui as seguintes tarefas:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Fazer upgrade do Ceph Nautilus para o Pacific.
   </para>
  </listitem>
  <listitem>
   <para>
    Alternar da instalação e execução do Ceph por meio de pacotes RPM para a execução em containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Remoção completa do DeepSea e substituição pelo <systemitem class="resource">ceph-salt</systemitem> e cephadm.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   As informações de upgrade neste capítulo são válidas <emphasis>apenas</emphasis> para upgrades do DeepSea para o cephadm. Não tente seguir estas instruções para implantar o SUSE Enterprise Storage na Plataforma SUSE CaaS.
  </para>
 </warning>
 <important>
  <para>
   O upgrade de versões do SUSE Enterprise Storage mais antigas do que a 6 não é suportado. Você deve primeiro fazer upgrade para a versão mais recente do SUSE Enterprise Storage 6 e, na sequência, seguir as etapas neste capítulo.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Antes de fazer upgrade</title>

  <para>
   As tarefas a seguir <emphasis>devem</emphasis> ser concluídas antes de você iniciar o upgrade. Isso pode ser feito a qualquer momento durante a vida útil do SUSE Enterprise Storage 6.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A migração do OSD do FileStore para o BlueStore <emphasis>deve</emphasis> ser feita antes do upgrade porque o FileStore é incompatível com o SUSE Enterprise Storage 7.1. Há mais detalhes sobre o BlueStore e como migrar do FileStore em <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Se você executa um cluster mais antigo que ainda usa OSDs do <literal>ceph-disk</literal>, <emphasis>precisa</emphasis> alternar para o <literal>ceph-volume</literal> antes do upgrade. Encontre mais detalhes na <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Pontos a serem considerados</title>
   <para>
    Antes de fazer upgrade, leia as seções a seguir para garantir que você entenda todas as tarefas que precisam ser executadas.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Ler os detalhes da versão</emphasis>. Neles, você encontra mais informações sobre o que mudou desde a versão anterior do SUSE Enterprise Storage. Consulte os detalhes da versão para verificar se:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Seu hardware precisa de considerações especiais.
       </para>
      </listitem>
      <listitem>
       <para>
        Qualquer pacote de software usado foi significativamente modificado.
       </para>
      </listitem>
      <listitem>
       <para>
        São necessárias precauções especiais para a instalação.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Os detalhes da versão também apresentam informações que não puderam ser incluídas a tempo no manual. Eles também incluem notas sobre problemas conhecidos.
     </para>
     <para>
      Você encontra os detalhes da versão do SES 7.1 online em <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Além disso, após a instalação do pacote <package>release-notes-ses</package> do repositório SES 7.1, encontre os detalhes da versão localmente no diretório <filename>/usr/share/doc/release-notes</filename> ou online em <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Leia o <xref linkend="ses-deployment"/> para se familiarizar com o <systemitem class="resource">ceph-salt</systemitem> e o orquestrador do Ceph, especialmente as informações sobre as especificações do serviço.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade do cluster pode levar algum tempo, aproximadamente o tempo necessário para fazer upgrade de uma máquina multiplicado pelo número de nós do cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Você precisa primeiro fazer upgrade do Master Salt e depois substituir o DeepSea pelo <systemitem class="resource">ceph-salt</systemitem> e cephadm. Você <emphasis>não</emphasis> poderá começar a usar o módulo de orquestrador cephadm até que seja feito o upgrade de todos os nós do Ceph Manager.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade do uso de RPMs do Nautilus para containers do Pacific precisa ser feito em uma única etapa. Isso significa fazer upgrade de um nó inteiro de uma vez, e não um daemon de cada vez.
     </para>
    </listitem>
    <listitem>
     <para>
      O upgrade dos serviços básicos (MON, MGR, OSD) é feito de forma ordenada. Cada serviço fica disponível durante o upgrade. Os serviços de gateway (Servidor de Metadados, Gateway de Objetos, NFS Ganesha, iSCSI Gateway) precisarão ser reimplantados após o upgrade dos serviços básicos. Há um determinado tempo de espera para cada um dos seguintes serviços:
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         Os Servidores de Metadados e os Gateways de Objetos ficam inativos a partir do momento em que o upgrade dos nós é feito do SUSE Linux Enterprise Server 15 SP1 para o SUSE Linux Enterprise Server 15 SP3 até a reimplantação dos serviços no final do procedimento de upgrade. É importante ter isso em mente principalmente quando esses serviços estão combinados com MONs, MGRs ou OSDs, porque eles podem ficar inativos durante o upgrade do cluster. Se isso for um problema, considere a implantação desses serviços separadamente em nós adicionais antes do upgrade, de modo que eles fiquem inativos pelo menor tempo possível. Essa é a duração do upgrade dos nós de gateway, não a duração do upgrade de todo o cluster.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        O NFS Ganesha e os iSCSI Gateways ficam inativos apenas no período de reinicialização dos nós durante o upgrade do SUSE Linux Enterprise Server 15 SP1 para o SUSE Linux Enterprise Server 15 SP3, e rapidamente depois quando cada serviço é reimplantado no modo de container.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Fazendo backup da configuração e dos dados do cluster</title>
   <para>
    É altamente recomendável fazer backup de todas as configurações e os dados do cluster antes de iniciar o upgrade para o SUSE Enterprise Storage 7.1. Para obter instruções sobre como fazer backup de todos os seus dados, consulte <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verificando etapas do upgrade anterior</title>
   <para>
    Se você fez upgrade da versão 5, verifique se o upgrade para a versão 6 foi concluído com êxito:
   </para>
   <para>
    Verifique se existe o arquivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>.
   </para>
   <para>
    Esse arquivo é criado pelo processo de integração durante o upgrade do SUSE Enterprise Storage 5 para 6. A opção <option>configuration_init: default-import</option> é definida em <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    Se <option>configuration_init</option> ainda está definido como <option>default-import</option>, o cluster usa <filename>ceph.conf.import</filename> como arquivo de configuração, e não o <filename>ceph.conf</filename> padrão do DeepSea, que é compilado dos arquivos em <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Portanto, você precisa inspecionar se há qualquer configuração personalizada em <filename>ceph.conf.import</filename> e, possivelmente, mover a configuração para um dos arquivos em <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Em seguida, remova a linha <option>configuration_init: default-import</option> de <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Atualizando os nós e verificando a saúde do cluster</title>
   <para>
    Verifique se todas as atualizações mais recentes do SUSE Linux Enterprise Server 15 SP1 e do SUSE Enterprise Storage 6 foram aplicadas a todos os nós do cluster:
   </para>
<screen><prompt role="root"># </prompt>zypper refresh &amp;&amp; zypper patch</screen>
   <tip>
    <para>
     Consulte <link xlink:href="https://documentation.suse.com/ses/6/html/ses-all/storage-salt-cluster.html#deepsea-rolling-updates"/> para obter informações detalhadas sobre como atualizar os nós do cluster.
    </para>
   </tip>
   <para>
    Depois que as atualizações forem aplicadas, reinicie o Master Salt, sincronize os novos módulos do Salt e verifique a saúde do cluster:
   </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
<prompt>cephuser@adm &gt; </prompt>ceph -s
</screen>
   <sect3 xml:id="upgrade-disable-insecure">
    <title>Desabilitar clientes não seguros</title>
    <para>
     Desde o Nautilus v14.2.20, um novo aviso de saúde foi implementado para informar a você que clientes não seguros têm permissão para ingressar no cluster. Por padrão, esse aviso está <emphasis>ativado</emphasis>. O Ceph Dashboard mostrará o cluster no status <literal>HEALTH_WARN</literal>. A linha de comando verifica o status do cluster da seguinte maneira:
    </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph status
 cluster:
   id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
   health: HEALTH_WARN
   mons are allowing insecure global_id reclaim
 [...]
 </screen>
    <para>
     Esse aviso significa que os Ceph Monitors ainda permitem que clientes antigos e sem patch se conectem ao cluster. Isso garante que os clientes existentes ainda consigam se conectar durante o upgrade do cluster, mas avisa você de que há um problema que precisa ser resolvido. Quando o upgrade do cluster e de todos os clientes for feito para a versão mais recente do Ceph, execute o seguinte comando para não permitir os clientes sem patch:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verificando o acesso a repositórios do software e imagens de container</title>
   <para>
    Verifique se cada nó do cluster tem acesso aos repositórios do software SUSE Linux Enterprise Server 15 SP3 e SUSE Enterprise Storage 7.1 e também ao registro de imagens de container.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Repositórios do software</title>
    <para>
     Se todos os nós foram registrados no SCC, você poderá usar o comando <command>zypper migration</command> para fazer upgrade. Consulte <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/> para obter mais detalhes.
    </para>
    <para>
     Se os nós <emphasis role="bold">não</emphasis> foram registrados no SCC, desabilite todos os repositórios existentes do software e adicione os dois repositórios <literal>Pool</literal> e <literal>Updates</literal> para cada uma das seguintes extensões:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP3
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7.1
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Imagens de container</title>
    <para>
     Todos os nós do cluster precisam acessar o registro de imagens de container. Na maioria dos casos, você usará o registro público do SUSE em <literal>registry.suse.com</literal>. Você precisa das seguintes imagens:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7.1/ceph/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Por exemplo, para implantações isoladas (air-gapped), uma alternativa é configurar um registro local e verificar se você tem o conjunto correto de imagens de container disponível. Consulte a <xref linkend="deploy-cephadm-configure-registry"/> para obter mais detalhes sobre como configurar um registro de imagens de container local.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Fazendo upgrade do Master Salt</title>

  <para>
   O procedimento a seguir descreve o processo de upgrade do Master Salt:
  </para>

  <procedure>
   <step>
    <para>
     Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP3:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Para um cluster em que todos os nós foram registrados no SCC, execute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Para um cluster em que os nós têm repositórios de software atribuídos manualmente, execute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Desabilite as fases do DeepSea para evitar o uso acidental. Adicione o seguinte conteúdo a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Grave o arquivo e aplique as mudanças:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     Se você <emphasis role="bold">não</emphasis> usa imagens de container do <literal>registry.suse.com</literal>, mas sim o registro configurado localmente, edite o <filename>/srv/pillar/ceph/stack/global.yml</filename> para especificar a imagem de container e o registro do Ceph que o DeepSea usará. Por exemplo, para usar <literal>192.168.121.1:5000/my/ceph/image</literal>, adicione as seguintes linhas:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Se você precisar especificar informações de autenticação para o registro, adicione o bloco <literal>ses7_container_registry_auth:</literal>, por exemplo:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
ses7_container_registry_auth:
  registry: 192.168.121.1:5000
  username: <replaceable>USER_NAME</replaceable>
  password: <replaceable>PASSWORD</replaceable>
</screen>
    <para>
     Grave o arquivo e aplique as mudanças:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Observe a configuração existente:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verifique o status do upgrade. A saída pode ser diferente dependendo da configuração do seu cluster:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 16.2.7-640-gceb23c7491b (ceb23c7491bd96ab7956111374219a4cdcf6f8f4) pacific (stable)
 os: SUSE Linux Enterprise Server 15 SP3

Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)

Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Fazendo upgrade dos nós MON, MGR e OSD</title>

  <para>
   Faça upgrade do Ceph Monitor, do Ceph Manager e dos nós OSD, um de cada vez. Para cada serviço, siga estas etapas:
  </para>

  <procedure>
   <step>
    <para>
     Antes de adotar qualquer nó OSD, você precisa executar uma conversão de formato dos nós OSD para melhorar a contabilização dos dados OMAP. Você pode fazer isso executando o seguinte comando no Nó de Admin:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set osd bluestore_fsck_quick_fix_on_mount true</screen>
    <para>
     Os nós OSD serão convertidos automaticamente após o término da adoção.
    </para>
    <note>
     <para>
      A conversão pode levar de minutos a horas, dependendo da quantidade de dados OMAP contida no disco rígido relacionado. Para obter mais detalhes, visite <link xlink:href="https://docs.ceph.com/en/latest/releases/pacific/#upgrading-non-cephadm-clusters"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Se o nó do qual você está fazendo upgrade é um OSD, execute o comando a seguir para evitar marcá-lo como <literal>out</literal>:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Substitua <replaceable>SHORT_NODE_NAME</replaceable> pelo nome abreviado do nó da forma como ele aparece na saída do comando <command>ceph osd tree</command>. Na entrada a seguir, os nomes abreviados de host são <literal>ses-min1</literal> e <literal>ses-min2</literal>
    </para>
<screen>
<prompt>root@master # </prompt>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP3:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Se todos os nós do cluster foram registrados no SCC, execute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Se os nós do cluster tiveram repositórios de software atribuídos manualmente, execute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Após a reinicialização do nó, coloque em contêiner todos os daemons MON, MGR e OSD existentes nesse nó executando o seguinte comando no Master Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Substitua <replaceable>MINION_ID</replaceable> pelo ID do minion do qual você está fazendo upgrade. Você pode gerar uma lista de IDs de minions ao executar o comando <command>salt-key -L</command> no Master Salt.
    </para>
    <tip>
     <para>
      Para ver o status e o andamento da <emphasis>adoção</emphasis>, verifique o Ceph Dashboard ou execute um dos seguintes comandos no Master Salt:
     </para>
<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     Após a conclusão bem-sucedida da adoção, cancele a definição do flag <literal>noout</literal> se o nó do qual você estiver fazendo upgrade for OSD:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Fazendo upgrade de nós de gateway</title>

  <para>
   A seguir, faça upgrade dos nós de gateway separados (Gateway do Samba, Servidor de Metadados, Gateway de Objetos, NFS Ganesha ou iSCSI Gateway). Faça upgrade do OS subjacente para o SUSE Linux Enterprise Server 15 SP3 para cada nó:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Se todos os nós do cluster foram registrados no SUSE Customer Center, execute o comando <command>zypper migration</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Se os nós do cluster tiveram repositórios de software atribuídos manualmente, execute o comando <command>zypper dup</command> seguido do comando <command>reboot</command>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Esta etapa também vale para qualquer nó que faça parte do cluster, mas que ainda não recebeu nenhuma função (em caso de dúvida, verifique a lista de hosts no Master Salt gerada pelo comando <command>salt-key -L</command> e compare-a com a saída do comando <command>salt-run upgrade.status</command>).
  </para>

  <para>
   Quando o upgrade do OS é feito em todos os nós do cluster, a próxima etapa é instalar o pacote <package>ceph-salt</package> e aplicar a configuração do cluster. Os serviços de gateway reais são reimplantados no modo em container no final do procedimento de upgrade.
  </para>

  <note>
   <para>
    Os serviços Servidor de Metadados e Gateway de Objetos ficam indisponíveis a partir do momento do upgrade para o SUSE Linux Enterprise Server 15 SP3 até serem reimplantados no final do procedimento de upgrade.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Instalando o <systemitem class="resource">ceph-salt</systemitem> e aplicando a configuração do cluster</title>

  <para>
   Antes de iniciar o procedimento de instalação do <systemitem class="resource">ceph-salt</systemitem> e aplicar a configuração do cluster, verifique o status do cluster e do upgrade executando os seguintes comandos:
  </para>

<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Remova os Crons <literal>rbd_exporter</literal> e <literal>rgw_exporter</literal> criados pelo DeepSea. No Master Salt, execute o comando <systemitem class="username">root</systemitem>crontab -e<command> como </command> para editar o crontab. Apague os seguintes itens, se houver:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null
</screen>
   </step>
   <step>
    <para>
     Exporte a configuração do cluster do DeepSea executando os seguintes comandos:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<prompt>root@master # </prompt>salt-run upgrade.generate_service_specs &gt; specs.yaml
</screen>
   </step>
   <step>
    <para>
     Desinstale o DeepSea e instale o <systemitem class="resource">ceph-salt</systemitem> no Master Salt:
    </para>
<screen>
<prompt>root@master # </prompt>zypper remove 'deepsea*'
<prompt>root@master # </prompt>zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Reinicie o Master Salt e sincronize os módulos do Salt:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Importe a configuração do cluster do DeepSea para <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Gere as chaves SSH para comunicação do nó do cluster:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verifique se a configuração do cluster foi importada do DeepSea e especifique as opções que possam ter sido perdidas:
     </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
     <para>
      Para ver uma descrição completa da configuração do cluster, consulte a <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Aplique a configuração e habilite o cephadm:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </step>
   <step>
    <para>
     Se for necessário inserir o URL de registro do container local e as credenciais de acesso, siga as etapas descritas na <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     Se você <emphasis role="bold">não</emphasis> usa imagens de container do <literal>registry.suse.com</literal>, mas usa o registro configurado localmente, informe a imagem de container que o Ceph usará executando o comando:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Pare e desabilite os daemons <systemitem class="daemon">ceph-crash</systemitem> do SUSE Enterprise Storage 6. Novos formulários em container desses daemons são iniciados automaticamente mais tarde.
    </para>
<screen>
<prompt>root@master # </prompt>salt '*' service.stop ceph-crash
<prompt>root@master # </prompt>salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Fazendo upgrade e adotando a pilha de monitoramento</title>

  <para>
   O procedimento a seguir adota todos os componentes da pilha de monitoramento (consulte o <xref linkend="monitoring-alerting"/> para obter mais detalhes).
  </para>

  <procedure>
   <step>
    <para>
     Pause o orquestrador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch pause</screen>
   </step>
   <step>
    <para>
     Em qualquer nó que esteja executando o Prometheus, o Grafana e o Alertmanager (por padrão, o Master Salt), execute os seguintes comandos:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      Se você <emphasis role="bold">não</emphasis> executa o registro de imagem de container padrão <literal>registry.suse.com</literal>, precisa especificar a imagem que será usada em cada comando, por exemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-server:2.32.1 \
  adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/prometheus-alertmanager:0.21.0 \
  adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7.1/ceph/grafana:7.5.12 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      As imagens de container necessárias e suas respectivas versões estão listadas em <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Remova Node-Exporter de <emphasis role="bold">todos</emphasis> os nós. O Node-Exporter não precisa ser migrado e será reinstalado como um container quando o arquivo <filename>specs.yaml</filename> for aplicado.
    </para>
<screen><prompt>&gt; </prompt><command>sudo</command> zypper rm golang-github-prometheus-node_exporter</screen>
    <para>
     Se preferir, remova o Node-Exporter de todos os nós simultaneamente usando o Salt no nó de admin:
    </para>
<screen><prompt>root@master # </prompt>salt '*' pkg.remove golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Aplique as especificações de serviço que você já exportou do DeepSea:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i specs.yaml</screen>
    <tip>
     <para>
      Se você <emphasis role="bold">não</emphasis> executa o registro de imagem de container padrão <literal>registry.suse.com</literal>, mas um registro de container local, configure o cephadm para usar a imagem de container do registro local para a implantação do Node-Exporter antes de implantá-lo. Do contrário, você poderá ignorar com segurança esta etapa e o aviso a seguir.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mgr mgr/cephadm/container_image_node_exporter <replaceable>QUALIFIED_IMAGE_PATH</replaceable></screen>
     <para>
      Verifique se todas as imagens de container dos serviços de monitoramento apontam para o registro local, não apenas para o Node-Exporter. Esta etapa requer que você faça isso apenas para o Node-Exporter, mas é recomendável definir todas as imagens de container de monitoramento no cephadm para apontar para o registro local neste ponto.
     </para>
     <para>
      Se você não fizer isso, as novas implantações de serviços de monitoramento e as reimplantações usarão a configuração cephadm padrão, e você talvez não consiga implantar serviços (no caso de implantações isoladas (air-gapped)) ou tenha serviços implantados com versões misturadas.
     </para>
     <para>
      Encontre uma descrição de como o cephadm precisa ser configurado para usar imagens de container do registro local em <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Continue o orquestrador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Reimplantação do serviço de gateway</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Fazendo upgrade do Gateway de Objetos</title>
   <para>
    No SUSE Enterprise Storage 7.1, os Gateways de Objetos são sempre configurados com um domínio, o que permite multissite no futuro (consulte o <xref linkend="ceph-rgw-fed"/> para obter mais detalhes). Se você usou uma configuração de site único do Gateway de Objetos no SUSE Enterprise Storage 6, siga estas etapas para adicionar um domínio. Se você não planeja usar a funcionalidade multissite, pode usar o <literal>padrão</literal> para os nomes de domínio, grupo de zonas e zona.
   </para>
   <procedure>
    <step>
     <para>
      Crie um novo domínio:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Opcionalmente, renomeie a zona padrão e o grupo de zonas.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure o grupo de zonas master:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure a zona master. Para isso, você precisará da ACCESS_KEY e da SECRET_KEY de um usuário do Gateway de Objetos com o flag <option>system</option> habilitado. O usuário costuma ser o <literal>admin</literal>. Para obter a ACCESS_KEY e a SECRET_KEY, execute <command>radosgw-admin user info --uid admin --rgw-zone=<replaceable>NOME_DA_ZONA</replaceable></command>.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Confirme a configuração atualizada:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    Para colocar o serviço Gateway de Objetos em container, crie o respectivo arquivo de especificação conforme descrito na <xref linkend="deploy-cephadm-day2-service-ogw"/> e aplique-o.
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Fazendo upgrade do NFS Ganesha</title>
   
<important>
 <para>
  O NFS Ganesha suporta o NFS versão 4.1 e mais recente. Ele não suporta o NFS versão 3.
 </para>
</important>

   <para>
    Veja a seguir como migrar um serviço NFS Ganesha existente que executa o Ceph Nautilus para um container do NFS Ganesha que executa o Ceph Octopus.
   </para>
   <warning>
    <para>
     A documentação a seguir requer que você já tenha feito upgrade dos serviços básicos do Ceph.
    </para>
   </warning>
   <para>
    O NFS Ganesha armazena a configuração adicional por daemon e a exporta para um pool RADOS. O pool RADOS configurado pode ser encontrado na linha <literal>watch_url</literal> do bloco <literal>RADOS_URLS</literal> no arquivo <filename>ganesha.conf</filename>. Por padrão, esse pool será denominado <literal>ganesha_config</literal>.
   </para>
   <para>
    Antes de tentar qualquer migração, é altamente recomendável fazer uma cópia dos objetos de configuração de exportação e daemon localizados no pool RADOS. Para localizar o pool RADOS configurado, execute o seguinte comando:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    Para listar o conteúdo do pool RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    Para copiar os objetos RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>OBJS=$(rados $RADOS_ARGS ls)
<prompt>cephuser@adm &gt; </prompt>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    Para cada nó, qualquer serviço existente do NFS Ganesha precisa ser interrompido e, em seguida, substituído por um container gerenciado pelo cephadm.
   </para>
   <procedure>
    <step>
     <para>
      Pare e desabilite o serviço NFS Ganesha existente:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>systemctl stop nfs-ganesha
<prompt>cephuser@adm &gt; </prompt>systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      Depois que o serviço NFS Ganesha existente for interrompido, um novo serviço poderá ser implantado em um container usando o cephadm. Para fazer isso, você precisa criar uma especificação de serviço que contenha um <literal>service_id</literal>, que será usado para identificar esse novo cluster do NFS, o nome de host do nó que estamos migrando listado como host na especificação de posicionamento e o pool RADOS e o namespace que contêm os objetos de exportação do NFS configurados. Por exemplo:
     </para>
<screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      Para obter mais informações sobre como criar uma especificação de posicionamento, consulte <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Aplique a especificação de posicionamento:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirme se o daemon do NFS Ganesha está em execução no host:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7.1/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Repita essas etapas para cada nó do NFS Ganesha. Você não precisa criar uma especificação de serviço separada para cada nó. É suficiente adicionar o nome de host de cada nó à especificação do serviço NFS existente e reaplicá-la.
     </para>
    </step>
   </procedure>
   <para>
    É possível migrar as exportações existentes de duas maneiras diferentes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Recriação ou reatribuição manual usando o Ceph Dashboard.
     </para>
    </listitem>
    <listitem>
     <para>
      Cópia manual do conteúdo de cada objeto RADOS por daemon para a configuração comum recém-criada do NFS Ganesha.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Copiando manualmente as exportações para o arquivo de configuração comum do NFS Ganesha</title>
    <step>
     <para>
      Determine a lista de objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Faça uma cópia dos objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Classifique e faça a fusão em uma única lista de exportações:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat conf-* | sort -u &gt; conf-nfs.<replaceable>SERVICE_ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Grave o novo arquivo de configuração comum do NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Notifique o daemon do NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       Essa ação fará com que o daemon recarregue a configuração.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    Após a migração bem-sucedida do serviço, será possível remover o serviço NFS Ganesha baseado no Nautilus.
   </para>
   <procedure>
    <step>
     <para>
      Remova o NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Remova as configurações do cluster legado do Ceph Dashboard:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Fazendo upgrade do servidor de metadados</title>
   <para>
    Ao contrário dos MONs, MGRs e OSDs, o Servidor de Metadados não pode ser adotado no local. Em vez disso, você precisa reimplantá-lo em containers usando o orquestrador do Ceph.
   </para>
   <procedure>
    <step>
     <para>
      Execute o comando <command>ceph fs ls</command> para obter o nome do seu sistema de arquivos, por exemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Crie um novo arquivo de especificação de serviço <filename>mds.yml</filename>, conforme descrito na <xref linkend="deploy-cephadm-day2-service-mds"/>, usando o nome do sistema de arquivos como o <option>service_id</option> e especificando os hosts que executarão os daemons MDS. Por exemplo:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Execute o comando <command>ceph orch apply -i mds.yml</command> para aplicar a especificação de serviço e iniciar os daemons MDS.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Fazendo upgrade do iSCSI Gateway</title>
   <para>
    Para fazer upgrade do iSCSI Gateway, é necessário reimplantá-lo em containers usando o orquestrador do Ceph. Se você tiver vários iSCSI Gateways, precisará reimplantá-los um por um para reduzir o tempo de espera do serviço.
   </para>
   <procedure>
    <step>
     <para>
      Pare e desabilite os daemons iSCSI existentes em cada nó do iSCSI Gateway:
     </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> systemctl stop rbd-target-gw
<prompt>&gt; </prompt><command>sudo</command> systemctl disable rbd-target-gw
<prompt>&gt; </prompt><command>sudo</command> systemctl stop rbd-target-api
<prompt>&gt; </prompt><command>sudo</command> systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Crie uma especificação de serviço para o iSCSI Gateway conforme descrito na <xref linkend="deploy-cephadm-day2-service-igw"/>. Para isso, você precisa das configurações <option>pool</option>, <option>trusted_ip_list</option> e <option>api_*</option> do arquivo <filename>/etc/ceph/iscsi-gateway.cfg</filename> existente. Se você tem o suporte a SSL habilitado (<literal>api_secure = true</literal>), precisa também do certificado SSL (<filename>/etc/ceph/iscsi-gateway.crt</filename>) e da chave (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      Por exemplo, se <filename>/etc/ceph/iscsi-gateway.cfg</filename> contém o seguinte:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Você precisa criar o seguinte arquivo de especificação de serviço <filename>iscsi.yml</filename>:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       As configurações <option>pool</option>, <option>trusted_ip_list</option>, <option>api_port</option>, <option>api_user</option>, <option>api_password</option> e <option>api_secure</option> são idênticas às do arquivo <filename>/etc/ceph/iscsi-gateway.cfg</filename>. É possível copiar os valores <option>ssl_cert</option> e <option>ssl_key</option> dos arquivos de chave e de certificado SSL existentes. Verifique se eles estão indentados corretamente e se o caractere <emphasis>barra vertical</emphasis> <literal>|</literal> aparece no final das linhas <literal>ssl_cert:</literal> e <literal>ssl_key:</literal> (veja o conteúdo do arquivo <filename>iscsi.yml</filename> acima).
      </para>
     </note>
    </step>
    <step>
     <para>
      Execute o comando <command>ceph orch apply -i iscsi.yml</command> para aplicar a especificação de serviço e iniciar os daemons do iSCSI Gateway.
     </para>
    </step>
    <step>
     <para>
      Remova o pacote <package>ceph-iscsi</package> antigo de cada um dos nós de gateway iSCSI existentes:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Limpeza após o upgrade</title>

  <para>
   Após o upgrade, execute as seguintes etapas de limpeza:
  </para>

  <procedure>
   <step>
    <para>
     Verifique a versão atual do Ceph para conferir se o upgrade do cluster foi bem-sucedido:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph versions</screen>
   </step>
   <step>
    <para>
     Garanta que nenhum OSD antigo ingresse no cluster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd require-osd-release pacific</screen>
   </step>
   <step>
    <para>
     Defina o <option>pg_autoscale_mode</option> dos pools existentes, se necessário:
    </para>
    <important>
     <para>
      Por padrão, os pools no SUSE Enterprise Storage 6 tinham o <option>pg_autoscale_mode</option> definido como <option>warn</option>. Isso resultava em uma mensagem de aviso em caso de número de PGs abaixo do ideal, mas o dimensionamento automático não era feito. O padrão no SUSE Enterprise Storage 7.1 é a opção <option>pg_autoscale_mode</option> definida como <option>on</option> para que os novos pools e os PGs sejam dimensionados automaticamente. O processo de upgrade não muda automaticamente o <option>pg_autoscale_mode</option> dos pools existentes. Para mudá-lo para <option>on</option> e aproveitar todos os benefícios do dimensionador automático, consulte as instruções no <xref linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Encontre mais detalhes na <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Impedir clientes pré-Luminous:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Habilite o módulo de balanceador:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph balancer mode upmap
<prompt>cephuser@adm &gt; </prompt>ceph balancer on
</screen>
    <para>
     Encontre mais detalhes na <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Se preferir, habilite o módulo de telemetria:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph mgr module enable telemetry
<prompt>cephuser@adm &gt; </prompt>ceph telemetry on
 </screen>
    <para>
     Encontre mais detalhes na <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
