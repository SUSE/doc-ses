<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>Gerenciar pools de armazenamento</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  O Ceph armazena dados em pools. Pools são grupos lógicos para armazenamento
  de objetos. Quando você implanta um cluster pela primeira vez sem criar um
  pool, o Ceph usa os pools padrão para armazenar os dados. Os destaques
  importantes a seguir são relacionados aos pools do Ceph:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resiliência</emphasis>: Os pools do Ceph oferecem resiliência
    replicando ou codificando os dados contidos neles. É possível definir cada
    pool como <literal>replicado</literal> ou <literal>codificação de
    eliminação</literal>. Para pools replicados, você ainda define o número de
    réplicas, ou cópias, que cada objeto de dados terá no pool. O número de
    cópias (OSDs, compartimentos de memória/folhas CRUSH) que podem ser
    perdidas é um a menos que o número de réplicas. Com a codificação de
    eliminação, você define os valores de <option>k</option> e
    <option>m</option>, em que <option>k</option> é o número de pacotes de
    dados e <option>m</option> é o número de pacotes de codificação. Para os
    pools codificados para eliminação, esse é o número de pacotes de
    codificação que determina quantos OSDs (compartimentos de memória/folhas
    CRUSH) podem ser perdidos sem perda de dados.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Grupos de Posicionamento</emphasis>: Você pode definir o número
    de grupos de posicionamento para o pool. Uma configuração típica usa
    aproximadamente 100 grupos de posicionamento por OSD para possibilitar o
    equilíbrio ideal sem usar muitos recursos de computação. Ao configurar
    vários pools, tenha cuidado para garantir que você defina um número
    adequado de grupos de posicionamento para o pool e o cluster como um todo.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Regras CRUSH</emphasis>: Quando você armazena dados em um pool,
    os objetos e suas réplicas (ou blocos, no caso de pools codificados para
    eliminação) são posicionados de acordo com o conjunto de regras CRUSH
    mapeado para o pool. Você pode criar uma regra CRUSH personalizada para o
    pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Instantâneos</emphasis>: Ao criar instantâneos com <command>ceph
    osd pool mksnap</command>, você efetivamente captura um instantâneo de
    determinado pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Para organizar dados em pools, você pode listar, criar e remover pools. Você
  também pode ver as estatísticas de uso para cada pool.
 </para>
 <sect1 xml:id="ceph-pools-operate-add-pool">
  <title>Criando um pool</title>

  <para>
   Um pool pode ser criado como <literal>replicated</literal> para recuperar
   OSDs perdidos mantendo várias cópias dos objetos, ou como
   <literal>erasure</literal> para ter um recurso RAID5 ou 6 generalizado. Os
   pools replicados exigem mais armazenamento bruto, enquanto os pools
   codificados para eliminação exigem menos armazenamento bruto. A configuração
   padrão é <literal>replicated</literal>. Para obter mais informações sobre
   pools codificados para eliminação, consulte o
   <xref linkend="cha-ceph-erasure"/>.
  </para>

  <para>
   Para criar um pool replicado, execute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable></screen>

  <note>
   <para>
    O dimensionador automático cuidará dos argumentos opcionais restantes. Para
    obter mais informações, consulte a <xref linkend="op-pgs-autoscaler"/>.
   </para>
  </note>

  <para>
   Para criar um pool codificado para eliminação, execute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable> erasure <replaceable>CRUSH_RULESET_NAME</replaceable> \
<replaceable>EXPECTED_NUM_OBJECTS</replaceable></screen>

  <para>
   O comando <command>ceph osd pool create</command> poderá falhar se você
   exceder o limite de grupos de posicionamento por OSD. O limite é definido
   com a opção <option>mon_max_pg_per_osd</option>.
  </para>

  <variablelist>
   <varlistentry>
    <term>POOL_NAME</term>
    <listitem>
     <para>
      O nome do pool. Ele deve ser exclusivo. Essa opção é obrigatória.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_TYPE</term>
    <listitem>
     <para>
      O tipo de pool, que pode ser <literal>replicated</literal> para
      recuperação de OSDs perdidos mantendo várias cópias dos objetos, ou
      <literal>erasure</literal> para aplicar um tipo de recurso RAID5
      generalizado. Os pools replicados exigem mais armazenamento bruto, porém
      implementam todas as operações do Ceph. Os pools de eliminação exigem
      menos armazenamento bruto, porém implementam apenas um subconjunto das
      operações disponíveis. O padrão de <literal>POOL_TYPE</literal> é
      <literal>replicated</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CRUSH_RULESET_NAME</term>
    <listitem>
     <para>
      O nome do conjunto de regras CRUSH para este pool. Se o conjunto de
      regras especificado não existir, haverá falha na criação dos pools
      replicados com -ENOENT. Para pools replicados, trata-se do conjunto de
      regras especificado pela variável de configuração <varname>osd pool
      default CRUSH replicated ruleset</varname>. Esse conjunto de regras deve
      existir. Para pools de eliminação, trata-se do “erasure-code”, se o
      perfil de código de eliminação for usado ou, do contrário,
      <replaceable>POOL_NAME</replaceable>. Esse conjunto de regras será criado
      implicitamente se ainda não existir.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>erasure_code_profile=profile</term>
    <listitem>
     <para>
      Apenas para pools codificados para eliminação. Use o perfil de código de
      eliminação. Ele deve ser um perfil existente, conforme definido por
      <command>osd erasure-code-profile set</command>.
     </para>
     <note>
      <para>
       Por qualquer motivo, se o dimensionador automático tiver sido
       desabilitado (<literal>pg_autoscale_mode</literal> definido como
       desativado) em um pool, você poderá calcular e definir os números de PGs
       manualmente. Consulte a <xref linkend="op-pgs"/> para obter detalhes
       sobre como calcular um número apropriado de grupos de posicionamento
       para seu pool.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>EXPECTED_NUM_OBJECTS</term>
    <listitem>
     <para>
      O número esperado de objetos para este pool. Ao definir esse valor
      (juntamente com um <option>limite de fusão de armazenamento de
      arquivos</option> negativo), a divisão da pasta PG é feita no momento da
      criação do pool. Isso evita o impacto da latência com uma divisão de
      pasta em tempo de execução.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-listing-pools">
  <title>Listando os pools</title>

  <para>
   Para listar os pools do cluster, execute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool ls</screen>
 </sect1>
 <sect1 xml:id="ceph-renaming-pool">
  <title>Renomeando um pool</title>

  <para>
   Para renomear um pool, execute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rename <replaceable>CURRENT_POOL_NAME</replaceable> <replaceable>NEW_POOL_NAME</replaceable></screen>

  <para>
   Se você renomear um pool e tiver recursos por pool para um usuário
   autenticado, deverá atualizar os recursos do usuário com o novo nome do
   pool.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pools-operate-del-pool">
  <title>Apagando um pool</title>

  <warning>
   <title>A exclusão do pool não é reversível</title>
   <para>
    Os pools podem conter dados importantes. Apagar um pool faz com que todos
    os dados nele desapareçam, e não é possível recuperá-los.
   </para>
  </warning>

  <para>
   Como a exclusão acidental do pool é um perigo real, o Ceph implementa dois
   mecanismos que impedem que os pools sejam apagados. Os dois mecanismos devem
   ser desabilitados antes que um pool possa ser apagado.
  </para>

  <para>
   O primeiro mecanismo é o flag <literal>NODELETE</literal>. Cada pool tem
   esse flag, e seu valor padrão é “false”. Para saber o valor desse flag em um
   pool, execute o seguinte comando:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>

  <para>
   Se a saída for <literal>nodelete: true</literal>, não será possível apagar o
   pool até você mudar o flag usando o seguinte comando:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>

  <para>
   O segundo mecanismo é o parâmetro de configuração de todo o cluster
   <option>mon allow pool delete</option>, que assume como padrão “false”. Por
   padrão, isso significa que não é possível apagar um pool. A mensagem de erro
   exibida é:
  </para>

<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>

  <para>
   Para apagar o pool mesmo com essa configuração de segurança, você pode
   definir <option>mon allow pool delete</option> temporariamente como “true”,
   apagar o pool e, em seguida, reverter o parâmetro para “false”:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>cephuser@adm &gt; </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>

  <para>
   O comando <command>injectargs</command> exibe a seguinte mensagem:
  </para>

<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>

  <para>
   Trata-se apenas de uma confirmação de que o comando foi executado com êxito.
   Isso não é um erro.
  </para>

  <para>
   Se você criou seus próprios conjuntos de regras e suas próprias regras para
   um pool, convém removê-los quando ele não for mais necessário.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pool-other-operations">
  <title>Outras operações</title>

  <sect2 xml:id="ceph-pools-associate">
   <title>Associando pools a um aplicativo</title>
   <para>
    Antes de usar os pools, você precisa associá-los a um aplicativo. Os pools
    que serão usados com o CephFS ou os pools criados automaticamente pelo
    Gateway de Objetos são associados de forma automática.
   </para>
   <para>
    Nos outros casos, você pode associar manualmente um nome de aplicativo de
    formato livre a um pool:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application enable <replaceable>POOL_NAME</replaceable> <replaceable>APPLICATION_NAME</replaceable></screen>
   <tip>
    <title>Nomes de aplicativos padrão</title>
    <para>
     O CephFS usa o nome do aplicativo <literal>cephfs</literal>, o Dispositivo
     de Blocos RADOS usa o <literal>rbd</literal> e o Gateway de Objetos usa o
     <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    É possível associar um pool a vários aplicativos, e cada aplicativo tem
    seus próprios metadados. Para listar o(s) aplicativo(s) associado(s) a um
    pool, emita o seguinte comando:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-set-pool-quotas">
   <title>Definindo cotas de pool</title>
   <para>
    Você pode definir cotas do pool para o número máximo de bytes e/ou para o
    número máximo de objetos por pool.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>POOL_NAME</replaceable> <replaceable>MAX_OBJECTS</replaceable> <replaceable>OBJ_COUNT</replaceable> <replaceable>MAX_BYTES</replaceable> <replaceable>BYTES</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Para remover uma cota, defina o valor como 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph-showing-pool-statistics">
   <title>Mostrando as estatísticas do pool</title>
   <para>
    Para mostrar as estatísticas de uso de um pool, execute:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados df
 POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
 .rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
 cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
 cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
 default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
 default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
 default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
 default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
 example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
 iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
 mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
 pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
 pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
 pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
 </screen>
   <para>
    Veja a seguir uma descrição de cada coluna:
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       Número de bytes usados pelo pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       Número de objetos armazenados no pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       Número de clones armazenados no pool. Quando um instantâneo é criado e
       faz gravações em um objeto, em vez de modificar o objeto original, o
       clone dele é criado para que o conteúdo do objeto original do qual foi
       feito o instantâneo não seja modificado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>COPIES</term>
     <listitem>
      <para>
       Número de réplicas do objeto. Por exemplo, se um pool replicado com o
       fator de replicação 3 tiver “x” objetos, ele normalmente terá 3 * x
       cópias.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       Número de objetos no estado degradado (nem todas as cópias existem)
       enquanto a cópia está ausente no OSD principal.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       Número de objetos não encontrados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Número de objetos degradados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       Número total de operações de leitura solicitadas para este pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       Número total de bytes lidos deste pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       Número total de operações de gravação solicitadas para este pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       Número total de bytes gravados no pool. Observe que isso não é igual ao
       uso do pool porque você pode gravar no mesmo objeto várias vezes. O
       resultado é que o uso do pool permanecerá o mesmo, mas o número de bytes
       gravados no pool aumentará.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       Número de bytes alocados para dados comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       Número de bytes que os dados comprimidos ocupam quando não são
       comprimidos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-getting-pool-values">
   <title>Obtendo valores do pool</title>
   <para>
    Para obter um valor de um pool, execute o seguinte comando
    <command>get</command>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable></screen>
   <para>
    Você pode obter valores para as chaves listadas na
    <xref linkend="ceph-pools-values"/> e as chaves a seguir:
   </para>
   <variablelist>
    <varlistentry>
     <term>PG_NUM</term>
     <listitem>
      <para>
       O número de grupos de posicionamento para o pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PGP_NUM</term>
     <listitem>
      <para>
       O número efetivo de grupos de posicionamento a ser usado ao calcular o
       posicionamento dos dados. A faixa válida é igual a ou menor do que
       <option>PG_NUM</option>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Todos os valores de um pool</title>
    <para>
     Para listar todos os valores relacionados a um pool específico, execute:
    </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> all
 </screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>Definindo valores de um pool</title>
   <para>
    Para definir um valor para um pool, execute:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable> <replaceable>VALUE</replaceable></screen>
   <para>
    Veja a seguir uma lista de valores de pool classificados por tipo de pool:
   </para>
   <variablelist>
    <title>Valores comuns de pool</title>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       O número de segundos para permitir que os clientes reproduzam
       solicitações confirmadas, mas não comprometidas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       O número de grupos de posicionamento para o pool. Se você adicionar
       novos OSDs ao cluster, verifique o valor para os grupos de
       posicionamento em todos os pools direcionados para os novos OSDs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       O número efetivo de grupos de posicionamento a ser usado ao calcular o
       posicionamento dos dados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       O conjunto de regras a ser usado para mapear o posicionamento de objetos
       no cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Defina (1) ou não defina (0) o flag HASHPSPOOL em um pool específico. A
       habilitação desse flag muda o algoritmo para distribuir melhor os PGs
       pelos OSDs. Após a habilitação desse flag em um pool com o flag
       HASHPSPOOL definido como o padrão 0, o cluster iniciará o
       provisionamento para reposicionar todos os PGs corretamente. Saiba que
       isso pode gerar uma carga de E/S muito significativa em um cluster,
       portanto, não habilite o flag de 0 a 1 em clusters de produção altamente
       carregados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Impede que o pool seja removido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Impede que <option>pg_num</option> e <option>pgp_num</option> do pool
       sejam modificados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Desabilita a depuração (profunda) dos dados para o pool específico a fim
       de resolver uma alta carga de E/S temporária.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Defina ou cancele a definição do flag
       <literal>WRITE_FADVISE_DONTNEED</literal> nas solicitações de
       leitura/gravação de um determinado pool para ignorar a colocação dos
       dados em cache. O padrão é <literal>false</literal>. Aplica-se aos pools
       tanto replicados quanto EC.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       O intervalo mínimo em segundos para depuração do pool quando a carga do
       cluster está baixa. O padrão <literal>0</literal> significa que o valor
       <option>osd_scrub_min_interval</option> do arquivo de configuração do
       Ceph foi usado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       O intervalo máximo em segundos para depuração do pool, independentemente
       da carga do cluster. O padrão <literal>0</literal> significa que o valor
       <option>osd_scrub_max_interval</option> do arquivo de configuração do
       Ceph foi usado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       O intervalo em segundos para depuração do pool <emphasis>em
       detalhes</emphasis>. O padrão <literal>0</literal> significa que o valor
       <option>osd_deep_scrub</option> do arquivo de configuração do Ceph foi
       usado.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>Valores de pools replicados</title>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Define o número de réplicas para os objetos no pool. Consulte a
       <xref linkend="ceph-pools-options-num-of-replicas"/> para obter mais
       detalhes. Apenas pools replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Define o número mínimo de réplicas necessárias para E/S. Consulte a
       <xref linkend="ceph-pools-options-num-of-replicas"/> para obter mais
       detalhes. Apenas pools replicados.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Impede que o tamanho do pool seja modificado. Quando um pool é criado, o
       valor padrão é obtido do valor do parâmetro
       <option>osd_pool_default_flag_nosizechange</option>, que é
       <literal>false</literal> por padrão. Aplica-se apenas a pools replicados
       porque não é possível mudar o tamanho dos pools EC.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Habilita o monitoramento de conjunto de acertos para pools de cache.
       Consulte
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtro de
       Bloom</link> para obter informações adicionais. Essa opção pode ter os
       seguintes valores: <literal>bloom</literal>,
       <literal>explicit_hash</literal>, <literal>explicit_object</literal>. O
       padrão é <literal>bloom</literal>, os outros valores são apenas para
       teste.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       O número de conjuntos de acertos para armazenar nos pools de cache.
       Quanto maior o número, mais RAM é consumida pelo daemon
       <systemitem>ceph-osd</systemitem>. O padrão é <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       A duração em segundos de um período do conjunto de acertos para os pools
       de cache. Quanto maior o número, mais RAM é consumida pelo daemon
       <systemitem>ceph-osd</systemitem>. Quando um pool é criado, o valor
       padrão é obtido do valor do parâmetro
       <option>osd_tier_default_cache_hit_set_period</option>, que é
       <literal>1200</literal> por padrão. Aplica-se apenas a pools replicados
       porque os pools EC não podem ser usados como camada de cache.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       A probabilidade de falsos positivos para o tipo de conjunto de acertos
       bloom. Consulte
       <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtro de
       Bloom</link> para obter informações adicionais. A faixa válida é de 0,0
       a 1,0. O padrão é <literal>0,05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Force os OSDs a usar marcações de horário em GMT (Horário de Greenwich)
       ao criar um conjunto de acertos para camadas de cache. Isso garante que
       os nós em fusos horários diferentes retornem o mesmo resultado. O padrão
       é <literal>1</literal>. Esse valor não deve ser mudado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos modificados antes
       que o agente de camadas de cache os descarregue para o pool de
       armazenamento de suporte. O padrão é <literal>0.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos modificados antes
       que o agente de camadas de cache os descarregue para o pool de
       armazenamento de suporte com uma velocidade maior. O padrão é
       <literal>0.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       A porcentagem do pool de cache que contém os objetos não modificados
       (limpos) antes que o agente de camadas de cache os elimine do pool de
       cache. O padrão é <literal>0.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       O Ceph iniciará o descarregamento ou a eliminação de objetos quando o
       limite <option>max_bytes</option> for acionado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       O Ceph iniciará o descarregamento ou a eliminação de objetos quando o
       limite <option>max_objects</option> for acionado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Taxa de redução de temperatura entre dois <literal>hit_set</literal>s
       sucessivos. O padrão é <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Considera no máximo <literal>N</literal> aparições nos
       <literal>hit_set</literal>s para o cálculo da temperatura. O padrão é
       <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       O tempo (em segundos) antes que o agente de camadas de cache descarregue
       um objeto do pool de cache para o pool de armazenamento.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       O tempo (em segundos) antes que o agente de camadas de cache elimine um
       objeto do pool de cache.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist xml:id="pool-values-ec">
    <title>Valores de pool codificado para eliminação</title>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Se esse flag estiver habilitado nos pools com codificação de eliminação,
       a solicitação de leitura emitirá subleituras para todos os fragmentos e
       aguardará até receber fragmentos suficientes para decodificar e atender
       ao cliente. No caso dos plug-ins de eliminação
       <emphasis>jerasure</emphasis> e <emphasis>isa</emphasis>, quando as
       primeiras <literal>K</literal> respostas são retornadas, a solicitação
       do cliente é atendida imediatamente, usando os dados decodificados
       dessas respostas. Essa abordagem gera mais carga de CPU e menos carga de
       disco/rede. No momento, esse flag é suportado apenas para pools com
       codificação de eliminação. O padrão é <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>Definindo o número de réplicas do objeto</title>
   <para>
    Para definir o número de réplicas do objeto em um pool replicado, execute o
    seguinte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    O <replaceable>num-replicas</replaceable> inclui o próprio objeto. Por
    exemplo, se você deseja o objeto e duas cópias dele para um total de três
    instâncias do objeto, especifique 3.
   </para>
   <warning>
    <title>Não defina menos do que 3 réplicas</title>
    <para>
     Se você definir <replaceable>num-replicas</replaceable> como 2, haverá
     apenas <emphasis>uma</emphasis> cópia dos dados. Se você perder uma
     instância do objeto, precisará confiar que a outra cópia não foi
     corrompida desde a última depuração durante a recuperação, por exemplo
     (consulte a <xref linkend="scrubbing-pgs"/> para obter detalhes).
    </para>
    <para>
     A definição de um pool para uma réplica significa que existe exatamente
     <emphasis>uma</emphasis> instância do objeto de dados no pool. Se houver
     falha no OSD, você perderá os dados. Um uso possível para um pool com uma
     réplica é armazenar dados temporários por um curto período.
    </para>
   </warning>
   <tip>
    <title>Definindo mais do que 3 réplicas</title>
    <para>
     A definição de 4 réplicas para um pool aumenta a confiabilidade em 25%.
    </para>
    <para>
     No caso de dois data centers, você precisa definir pelo menos 4 réplicas
     para que um pool tenha duas cópias em cada data center. Desse modo, se um
     data center for perdido, ainda haverá duas cópias, e você poderá perder um
     disco sem perder os dados.
    </para>
   </tip>
   <note>
    <para>
     Um objeto pode aceitar E/S no modo degradado com menos do que
     <literal>pool size</literal> réplicas. Para definir um número mínimo de
     réplicas necessárias para E/S, você deve usar a configuração
     <literal>min_size</literal>. Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Isso garante que nenhum objeto no pool de dados receba E/S com menos do
     que <literal>min_size</literal> réplicas.
    </para>
   </note>
   <tip>
    <title>Obter o número de réplicas do objeto</title>
    <para>
     Para obter o número de réplicas do objeto, execute o seguinte:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump | grep 'replicated size'</screen>
    <para>
     O Ceph listará os pools, com o atributo <literal>replicated size</literal>
     realçado. Por padrão, o Ceph cria duas réplicas de um objeto (um total de
     três cópias, ou um tamanho de 3).
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>Migração de pool</title>

  <para>
   Ao criar um pool (consulte a <xref linkend="ceph-pools-operate-add-pool"/>),
   você precisa especificar os parâmetros iniciais, como o tipo de pool ou o
   número de grupos de posicionamento. Posteriomente, se você decidir mudar
   qualquer um desses parâmetros, por exemplo, ao converter um pool replicado
   em um codificado para eliminação ou reduzir o número de grupos de
   posicionamento, será necessário migrar os dados do pool para outro cujos
   parâmetros sejam mais adequados à sua implantação.
  </para>

  <para>
   Esta seção descreve dois métodos de migração: <emphasis>camada de
   cache</emphasis> para migração geral de dados do pool, e o método que usa os
   subcomandos <command>rbd migrate</command> para migrar imagens RBD para um
   novo pool. Cada método tem suas especificações e limitações.
  </para>

  <sect2 xml:id="pool-migrate-limits">
   <title>Limitações</title>
   <itemizedlist>
    <listitem>
     <para>
      Você pode usar o método de <emphasis>camada de cache</emphasis> para
      migrar de um pool replicado para um pool EC ou para outro replicado. A
      migração de um pool EC não é suportada.
     </para>
    </listitem>
    <listitem>
     <para>
      Não é possível migrar imagens RBD e exportações do CephFS de um pool
      replicado para um pool com EC. O motivo é que os pools EC não suportam
      <literal>omap</literal>, e o RBD e o CephFS usam o
      <literal>omap</literal> para armazenar seus metadados. Por exemplo,
      haverá falha ao descarregar o objeto de cabeçalho do RBD. Porém, você
      pode migrar os dados para o pool EC, deixando os metadados no pool
      replicado.
     </para>
    </listitem>
    <listitem>
     <para>
      O método <command>rbd migration</command> permite migrar imagens com
      tempo de espera mínimo do cliente. Você apenas precisa parar o cliente
      antes da etapa de <option>preparação</option> e iniciá-lo depois. Observe
      que apenas um cliente <systemitem>librbd</systemitem> com suporte a esse
      recurso (Ceph Nautilus ou mais recente) poderá abrir a imagem logo após a
      etapa de <option>preparação</option>. Os clientes
      <systemitem>librbd</systemitem> mais antigos ou os clientes
      <systemitem>krbd</systemitem> não poderão abrir a imagem antes da
      execução da etapa de <option>confirmação</option>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>Migração usando a camada de cache</title>
   <para>
    O princípio é simples: incluir o pool que você precisa migrar para a camada
    de cache na ordem inversa. O exemplo a seguir migra um pool replicado
    chamado “testpool” para um pool codificado para eliminação:
   </para>
   <procedure>
    <title>Migrando um pool replicado para um codificado para eliminação</title>
    <step>
     <para>
      Crie um novo pool codificado para eliminação chamado “newpool”. Consulte
      a <xref linkend="ceph-pools-operate-add-pool"/> para obter uma explicação
      detalhada dos parâmetros de criação de pool.
     </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool create newpool erasure default
</screen>
     <para>
      Verifique se o chaveiro do cliente usado oferece pelo menos os mesmos
      recursos do “testpool” para o “newpool”.
     </para>
     <para>
      Agora você tem dois pools: o “testpool” replicado original preenchido com
      dados e o novo “newpool” codificado para eliminação vazio:
     </para>
     <figure>
      <title>Pools antes da migração</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Configure a camada de cache e defina o pool replicado “testpool” como o
      pool de cache. A opção <option>-force-nonempty</option> permite adicionar
      uma camada de cache mesmo que o pool já tenha dados:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<prompt>cephuser@adm &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>cephuser@adm &gt; </prompt>ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>Configuração da camada de cache</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Force o pool de cache a mover todos os objetos para o novo pool:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Descarregando dados</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Até todos os dados serem descarregados para o novo pool codificado para
      eliminação, você precisa especificar uma sobreposição para que esses
      objetos sejam pesquisados no pool antigo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Com a sobreposição, todas as operações são encaminhadas para o “testpool”
      replicado antigo:
     </para>
     <figure>
      <title>Definindo a sobreposição</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Agora você pode alternar todos os clientes para acessar objetos no novo
      pool.
     </para>
    </step>
    <step>
     <para>
      Após a migração de todos os dados para o “newpool” codificado para
      eliminação, remova a sobreposição e o pool de cache antigo “testpool”:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migração concluída</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Execute:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>Migrando imagens RBD</title>
   <para>
    Veja a seguir a maneira recomendada de migrar imagens RBD entre dois pools
    replicados.
   </para>
   <procedure>
    <step>
     <para>
      Impeça os clientes (como uma máquina virtual) de acessar a imagem RBD.
     </para>
    </step>
    <step>
     <para>
      Crie uma nova imagem no pool de destino, com o pai definido como a imagem
      de origem:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>Migrar apenas dados para um pool codificado para eliminação</title>
      <para>
       Se você precisa migrar apenas os dados da imagem para um novo pool EC e
       deixar os metadados no pool replicado original, execute o seguinte
       comando:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
    </step>
    <step>
     <para>
      Permita que os clientes acessem a imagem no pool de destino.
     </para>
    </step>
    <step>
     <para>
      Migre os dados para o pool de destino:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      Remova a imagem antiga:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>Instantâneos de pool</title>

  <para>
   Os instantâneos de pool são capturados com base no estado do pool inteiro do
   Ceph. Com os instantâneos de pool, você pode manter o histórico de estado do
   pool. A criação de instantâneos de pool consome espaço de armazenamento
   proporcional ao tamanho do pool. Confira sempre se há espaço em disco
   suficiente no armazenamento relacionado antes de criar um instantâneo de um
   pool.
  </para>

  <sect2 xml:id="ceph-make-snapshot-pool">
   <title>Criando um instantâneo de um pool</title>
   <para>
    Para criar um instantâneo de um pool, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2 xml:id="ceph-listing-snapshots-pool">
   <title>Listando instantâneos de um pool</title>
   <para>
    Para listar os instantâneos existentes de um pool, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    Por exemplo:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2 xml:id="ceph-removing-snapshot-pool">
   <title>Removendo um instantâneo de um pool</title>
   <para>
    Para remover um instantâneo de um pool, execute:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>Compactação de dados</title>

  <para>
   O BlueStore (encontre mais detalhes no <xref linkend="about-bluestore"/>)
   oferece compactação de dados sob demanda para economizar espaço no disco. A
   taxa de compactação depende dos dados armazenados no sistema. Observe que a
   compactação/descompactação requer mais capacidade da CPU.
  </para>

  <para>
   Você pode configurar a compactação de dados globalmente (consulte a
   <xref linkend="sec-ceph-pool-bluestore-compression-options"/>) e, em
   seguida, anular as configurações de compactação específicas para cada pool
   individual.
  </para>

  <para>
   Você pode habilitar ou desabilitar a compactação de dados do pool ou mudar o
   algoritmo e o modo de compactação a qualquer momento, para um pool tanto com
   dados quanto sem dados.
  </para>

  <para>
   Nenhuma compactação será aplicada aos dados existentes após habilitar a
   compactação do pool.
  </para>

  <para>
   Após desabilitar a compactação de um pool, todos os dados dele serão
   descompactados.
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>Habilitando a compactação</title>
   <para>
    Para habilitar a compactação de dados para um pool denominado
    <replaceable>POOL_NAME</replaceable>, execute o seguinte comando:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>Desabilitando a compactação do pool</title>
    <para>
     Para desabilitar a compactação de dados para um pool, use “none” (nenhum)
     como o algoritmo de compactação:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>Opções de compactação do pool</title>
   <para>
    Uma lista completa de configurações de compactação:
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Os valores possíveis são <literal>none</literal>,
       <literal>zstd</literal> e <literal>snappy</literal>. O padrão é
       <literal>snappy</literal>.
      </para>
      <para>
       O algoritmo de compactação a ser usado depende do caso de uso
       específico. Veja a seguir várias recomendações:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Use o padrão <literal>snappy</literal> se não tiver um bom motivo para
         mudá-lo.
        </para>
       </listitem>
       <listitem>
        <para>
         O <literal>zstd</literal> oferece uma boa taxa de compactação, mas
         provoca alto overhead da CPU ao compactar pequenas quantidades de
         dados.
        </para>
       </listitem>
       <listitem>
        <para>
         Realize um benchmark desses algoritmos em uma amostra dos dados reais
         e observe o uso de CPU e memória do cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       Os valores possíveis são <literal>none</literal>,
       <literal>aggressive</literal>, <literal>passive</literal> e
       <literal>force</literal>. O padrão é <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: nunca comprimir
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: comprimir se houver a dica
         <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: comprimir, exceto se houver a dica
         <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: sempre comprimir
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valor: Duplo, Taxa = SIZE_COMPRESSED/SIZE_ORIGINAL. O padrão é
       <literal>0,875</literal>, o que significa que, se a compactação não
       reduzir o espaço ocupado em pelo menos 12,5%, o objeto não será
       comprimido.
      </para>
      <para>
       Os objetos acima dessa taxa não serão comprimidos por causa do baixo
       ganho líquido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>0</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>0</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>Opções globais de compactação</title>
   <para>
    As seguintes opções de configuração podem ser definidas na configuração do
    Ceph e aplicam-se a todos os OSDs, não apenas a um único pool. A
    configuração específica do pool listada na
    <xref linkend="sec-ceph-pool-compression-options"/> tem prioridade.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Consulte <xref linkend="compr-algorithm"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Consulte <xref linkend="compr-mode"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Consulte <xref linkend="compr-ratio"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>0</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos. Por padrão, a
       configuração é ignorada a favor de
       <option>bluestore_compression_min_blob_size_hdd</option> e
       <option>bluestore_compression_min_blob_size_ssd</option>. Ela tem
       prioridade quando definida como um valor diferente de zero.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>0</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que são comprimidos antes de serem divididos
       em blocos menores. Por padrão, a configuração é ignorada a favor de
       <option>bluestore_compression_max_blob_size_hdd</option> e
       <option>bluestore_compression_max_blob_size_ssd</option>. Ela tem
       prioridade quando definida como um valor diferente de zero.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>8K</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos e armazenados na
       unidade de estado sólido.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>64K</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que são comprimidos e armazenados em unidade
       de estado sólido antes de serem divididos em blocos menores.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>128K</literal>
      </para>
      <para>
       Tamanho mínimo dos objetos que serão comprimidos e armazenados em discos
       rígidos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valor: Número Inteiro Não Assinado, tamanho em bytes. Padrão:
       <literal>512K</literal>
      </para>
      <para>
       Tamanho máximo dos objetos que são comprimidos e armazenados em discos
       rígidos antes de serem divididos em blocos menores.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
