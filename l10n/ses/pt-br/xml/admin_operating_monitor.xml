<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph-monitor">
 <title>Determinar o estado do cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Quando você tem um cluster em execução, pode usar a ferramenta <command>ceph</command> para monitorá-lo. Normalmente, determinar o estado do cluster envolve verificar o status dos Ceph OSDs, Ceph Monitors, grupos de posicionamento e Servidores de Metadados.
 </para>
 <tip>
  <title>Modo interativo</title>
  <para>
   Para executar a ferramenta <command>ceph</command> no modo interativo, digite <command>ceph</command> na linha de comando sem argumentos. O modo interativo é o mais prático quando você pretende digitar mais comandos <command>ceph</command> em uma linha. Por exemplo:
  </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon stat</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>Verificando o status de um cluster</title>

  <para>
   Você pode detectar o estado imediato do cluster usando <command>ceph status</command> ou <command>ceph -s</command>:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph -s
cluster:
    id:     b4b30c6e-9681-11ea-ac39-525400d7702d
    health: HEALTH_OK

  services:
    mon: 5 daemons, quorum ses-min1,ses-master,ses-min2,ses-min4,ses-min3 (age 2m)
    mgr: ses-min1.gpijpm(active, since 3d), standbys: ses-min2.oopvyh
    mds: my_cephfs:1 {0=my_cephfs.ses-min1.oterul=up:active}
    osd: 3 osds: 3 up (since 3d), 3 in (since 11d)
    rgw: 2 daemons active (myrealm.myzone.ses-min1.kwwazo, myrealm.myzone.ses-min2.jngabw)

  task status:
    scrub status:
        mds.my_cephfs.ses-min1.oterul: idle

  data:
    pools:   7 pools, 169 pgs
    objects: 250 objects, 10 KiB
    usage:   3.1 GiB used, 27 GiB / 30 GiB avail
    pgs:     169 active+clean
</screen>

  <para>
   A saída apresenta as seguintes informações:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     ID do cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Status de saúde do cluster
    </para>
   </listitem>
   <listitem>
    <para>
     A época do mapa do monitor e o status do quorum do monitor
    </para>
   </listitem>
   <listitem>
    <para>
     A época do mapa OSD e o status dos OSDs
    </para>
   </listitem>
   <listitem>
    <para>
     O status dos Ceph Managers
    </para>
   </listitem>
   <listitem>
    <para>
     O status dos Gateways de Objetos
    </para>
   </listitem>
   <listitem>
    <para>
     A versão do mapa do grupo de posicionamento
    </para>
   </listitem>
   <listitem>
    <para>
     O número de grupos de posicionamento e pools
    </para>
   </listitem>
   <listitem>
    <para>
     A quantidade <emphasis>estimada</emphasis> de dados armazenados e o número de objetos armazenados
    </para>
   </listitem>
   <listitem>
    <para>
     A quantidade total de dados armazenados.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Como o Ceph calcula o uso de dados</title>
   <para>
    O valor <literal>usado</literal> reflete o valor real do armazenamento bruto utilizado. O valor <literal>xxx GB/xxx GB</literal> indica o valor disponível (o menor número) da capacidade de armazenamento geral do cluster. O número estimado reflete o tamanho dos dados armazenados antes de serem replicados, clonados ou capturados como instantâneos. Portanto, a quantidade de dados realmente armazenados costuma exceder o valor estimado armazenado, pois o Ceph cria réplicas dos dados e também pode usar a capacidade de armazenamento para fazer clonagem e criar instantâneos.
   </para>
  </tip>

  <para>
   Outros comandos que exibem informações de status imediatas são:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Para obter as informações atualizadas em tempo real, especifique qualquer um desses comandos (incluindo o <command>ceph -s</command>) como um argumento do comando <command>watch</command>:
  </para>

<screen><prompt role="root">root # </prompt>watch -n 10 'ceph -s'</screen>

  <para>
   Pressione <keycombo><keycap function="control"/><keycap>C</keycap></keycombo> quando estiver cansado de observar.
  </para>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>Verificando a saúde do cluster</title>

  <para>
   Após iniciar o cluster e antes de começar a leitura e/ou gravação de dados, verifique a saúde dele:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    Se você especificou locais diferentes do padrão em sua configuração ou no chaveiro, deve especificar estes locais:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   O cluster do Ceph retorna um dos seguintes códigos de saúde:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Um ou mais OSDs estão marcados como inativos. O daemon OSD pode ter sido parado ou os OSDs peers talvez não conseguem acessar o OSD pela rede. As causas comuns incluem um daemon parado ou com falha, um host inativo ou uma interrupção da rede.
     </para>
     <para>
      Verifique se o host está saudável, se o daemon foi iniciado e se a rede está funcionando. Se o daemon falhou, o arquivo de registro do daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) pode incluir informações de depuração.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>tipo de crush</replaceable>_DOWN. Por exemplo, OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Todos os OSDs em uma subárvore específica do CRUSH estão marcados como inativos. Por exemplo, todos os OSDs em um host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      Um OSD é referenciado na hierarquia do mapa CRUSH, mas não existe. O OSD pode ser removido da hierarquia do CRUSH com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Os limites de uso para <emphasis>backfillfull</emphasis> (padrão definido como 0,90), <emphasis>nearfull</emphasis> (padrão definido como 0,85), <emphasis>full</emphasis> (padrão definido como 0,95) e/ou <emphasis>failsafe_full</emphasis> não são ascendentes. Especificamente, esperamos <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> e <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      Para ler os valores atuais, execute:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      É possível ajustar os limites com os seguintes comandos:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Um ou mais OSDs excederam o limite de <emphasis>full</emphasis> e impedem o cluster de executar gravações. É possível verificar o uso por pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
     <para>
      É possível ver a cota <emphasis>full</emphasis> definida no momento com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Uma solução alternativa de curto prazo para resolver a disponibilidade de gravação é aumentar um pouco o valor do limite de full:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Adicione o novo armazenamento ao cluster implantando mais OSDs ou apague os dados existentes para liberar espaço.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Um ou mais OSDs excederam o limite de <emphasis>backfillfull</emphasis>, o que impede a redistribuição dos dados no dispositivo. Trata-se de um aviso antecipado de que a redistribuição talvez não possa ser concluída e de que o cluster está quase cheio. É possível verificar o uso por pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Um ou mais OSDs excederam o limite de <emphasis>nearfull</emphasis>. Trata-se de um aviso antecipado de que o cluster está quase cheio. É possível verificar o uso por pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Um ou mais flags de interesse do cluster foram definidos. Com exceção de <emphasis>full</emphasis>, é possível definir ou limpar esses flags com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Esses flags incluem:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         O cluster foi sinalizado como cheio e não pode realizar gravações.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Leituras ou gravações pausadas.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         Os OSDs não têm permissão para serem iniciados.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Os relatórios de falha do OSD estão sendo ignorados, portanto, os monitores não marcarão os OSDs como <emphasis>inativos</emphasis>.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Os OSDs já marcados como <emphasis>out</emphasis> não serão remarcados como <emphasis>in</emphasis> quando forem iniciados.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Os OSDs <emphasis>inativos</emphasis> não serão automaticamente marcados como <emphasis>out</emphasis> após o intervalo configurado.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         A recuperação ou a redistribuição de dados está suspensa.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         A depuração (consulte a <xref linkend="scrubbing-pgs"/>) está desabilitada.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         A atividade de camadas de cache foi suspensa.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Um ou mais OSDs têm um flag de interesse definido por OSD. Esses flags incluem:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         O OSD não tem permissão para ser iniciado.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Os relatórios de falha para este OSD serão ignorados.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Se este OSD já foi marcado como <emphasis>out</emphasis> automaticamente após uma falha, ele não será marcado como <emphasis>in</emphasis> quando for iniciado.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Se este OSD estiver inativo, ele não será automaticamente marcado como <emphasis>out</emphasis> após o intervalo configurado.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      É possível definir e limpar os flags por OSD com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      O Mapa CRUSH usa configurações muito antigas e deve ser atualizado. Os tunables mais antigos que podem ser usados (ou seja, a versão de cliente mais antiga que pode se conectar ao cluster) sem acionar este aviso de saúde são determinados pela opção de configuração <option>mon_crush_min_required_version</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      O Mapa CRUSH usa um método mais antigo que não é o ideal para calcular os valores de peso intermediários para compartimentos de memória straw. O Mapa CRUSH deve ser atualizado para usar o método mais recente (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Um ou mais pools de cache não estão configurados com um conjunto de acertos para monitorar o uso, o que impede o agente de camadas de identificar objetos frios para descarregar e eliminar do cache. É possível configurar conjuntos de acertos no pool de cache com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      Não há OSDs anteriores ao Luminous v12 em execução, mas o flag <option>sortbitwise</option> não foi definido. Você precisa definir o flag <option>sortbitwise</option> para que os OSDs do Luminous v12 ou mais recentes possam ser iniciados:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Um ou mais pools atingiram a cota e não permitem mais gravações. Você pode definir cotas e uso de pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph df detail</screen>
     <para>
      Você pode aumentar a cota do pool com
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      ou apagar alguns dados existentes para reduzir o uso.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      A disponibilidade de dados está reduzida, o que significa que o cluster não pode atender a possíveis solicitações de leitura ou gravação para alguns dados no cluster. Especificamente, o estado de um ou mais PGs não permite que as solicitações de E/S sejam atendidas. Os estados dos PGs problemáticos incluem <emphasis>emparelhamento</emphasis>, <emphasis>obsoleto</emphasis>, <emphasis>incompleto</emphasis> e a ausência de <emphasis>ativo</emphasis> (se essas condições não forem resolvidas rapidamente). As informações detalhadas sobre quais PGs são afetados estão disponíveis em:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph health detail</screen>
     <para>
      Na maioria dos casos, a causa raiz é que um ou mais OSDs estão inativos no momento. É possível consultar o estado dos PGs problemáticos específicos com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      A redundância de dados está reduzida para alguns dados, o que significa que o cluster não tem o número de réplicas desejado para todos os dados (em pools replicados) ou para fragmentos de código de eliminação (em pools codificados para eliminação). Especificamente, um ou mais PGs têm o flag <emphasis>degraded</emphasis> ou <emphasis>undersized</emphasis> definido (não há instâncias suficientes desse grupo de posicionamento no cluster) ou não tinham o flag <emphasis>clean</emphasis> definido durante determinado período. As informações detalhadas sobre quais PGs são afetados estão disponíveis em:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph health detail</screen>
     <para>
      Na maioria dos casos, a causa raiz é que um ou mais OSDs estão inativos no momento. É possível consultar o estado dos PGs problemáticos específicos com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      A redundância de dados pode estar reduzida ou em risco para alguns dados devido à ausência de espaço livre no cluster. Especificamente, um ou mais PGs têm o flag <emphasis>backfill_toofull</emphasis> ou <emphasis>recovery_toofull</emphasis> definido, o que significa que o cluster não pode migrar ou recuperar dados porque um ou mais OSDs estão acima do limite de <emphasis>backfillfull</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      A depuração de dados (consulte a <xref linkend="scrubbing-pgs"/>) descobriu alguns problemas com a consistência de dados no cluster. Especificamente, um ou mais PGs têm o flag <emphasis>inconsistent</emphasis> ou <emphasis>snaptrim_error</emphasis> definido, indicando que uma operação de depuração anterior detectou um problema, ou o flag <emphasis>repair</emphasis> definido, o que significa que um reparo para esse tipo de inconsistência está agora em andamento.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Depurações recentes de OSD revelaram inconsistências.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Um pool de camada de cache está quase cheio. Neste contexto, “full” é determinado pelas propriedades <emphasis>target_max_bytes</emphasis> e <emphasis>target_max_objects</emphasis> no pool de cache. Quando o pool atinge o limite de destino, as solicitações de gravação para o pool podem ser bloqueadas enquanto os dados são descarregados e eliminados do cache, um estado que normalmente gera latências muito altas e baixo desempenho. É possível ajustar o tamanho do destino do pool de cache com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      As atividades normais de descarregamento e eliminação de cache também podem ficar restritas por redução na disponibilidade ou no desempenho da camada de base ou do carregamento geral do cluster.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      O número de PGs em uso está abaixo do limite configurável de <option>mon_pg_warn_min_per_osd</option> PGs por OSD. Isso pode levar à distribuição e ao equilíbrio de dados abaixo do ideal em todos os OSDs no cluster, reduzindo o desempenho geral.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      O número de PGs em uso está acima do limite configurável de <option>mon_pg_warn_max_per_osd</option> PGs por OSD. Isso pode levar a um aumento do uso de memória dos daemons OSD, a uma redução do emparelhamento após mudanças no estado do cluster (por exemplo, reinicializações, adições ou remoções de OSD) e a um aumento da carga nos Ceph Managers e Ceph Monitors.
     </para>
     <para>
      Não é possível reduzir o valor <option>pg_num</option> dos pools existentes, mas é possível reduzir o valor <option>pgp_num</option>. Efetivamente, isso coloca alguns PGs nos mesmos conjuntos de OSDs, atenuando alguns dos impactos negativos descritos acima. É possível ajustar o valor <option>pgp_num</option> com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      Um ou mais pools têm um valor <option>pgp_num</option> menor do que <option>pg_num</option>. Normalmente, isso indica que a contagem de PGs foi aumentada sem aumentar também o comportamento de posicionamento. Isso costuma ser resolvido definindo <option>pgp_num</option> para corresponder a <option>pg_num</option>, o que aciona a migração de dados, com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Um ou mais pools têm um número médio de objetos por PG que é significativamente maior do que a média geral do cluster. O limite específico é controlado pelo valor da configuração <option>mon_pg_warn_max_object_skew</option>. Isso costuma ser uma indicação de que o(s) pool(s) com a maioria dos dados no cluster está(ão) com um número muito baixo de PGs, e/ou que outros pools sem tantos dados têm PGs em excesso. É possível aumentar o limite para silenciar o aviso de saúde ajustando a opção de configuração <option>mon_pg_warn_max_object_skew</option> nos monitores.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Existe um pool que contém um ou mais objetos, mas que não foi marcados para uso por determinado aplicativo. Resolva esse aviso identificando o pool para uso por um aplicativo. Por exemplo, se o pool é usado pelo RBD:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Se o pool é usado por um aplicativo personalizado “foo”, você também pode identificá-lo usando o comando de nível inferior:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Um ou mais pools atingiram (ou estão muito próximos de atingir) a cota. O limite para acionar essa condição de erro é controlado pela opção de configuração <option>mon_pool_quota_crit_threshold</option>. É possível aumentar ou reduzir (ou remover) as cotas de pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      A definição do valor como 0 desabilitará a cota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Um ou mais pools estão quase atingindo a cota. O limite para acionar essa condição de aviso é controlado pela opção de configuração <option>mon_pool_quota_warn_threshold</option>. É possível aumentar ou reduzir (ou remover) as cotas de pool com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      A definição do valor como 0 desabilitará a cota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Um ou mais objetos no cluster não são armazenados no nó em que o cluster deseja que eles sejam. Isso é uma indicação de que a migração de dados causada por alguma mudança recente no cluster ainda não foi concluída. Os dados incorretamente armazenados não representam uma condição de risco por si só. A consistência de dados nunca está em risco, e as cópias antigas de objetos serão removidas apenas quando houver o número desejado de novas cópias (nos locais esperados).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      Um ou mais objetos no cluster não foram encontrados. Especificamente, os OSDs sabem que uma cópia nova ou atualizada de um objeto deve existir, mas uma cópia dessa versão do objeto não foi encontrada nos OSDs que estão ativos no momento. As solicitações de leitura ou gravação para os objetos “não encontrados” serão bloqueadas. No cenário ideal, o OSD inativo com a cópia mais recente do objeto não encontrado pode ser reativado. É possível identificar os OSDs candidatos com base no estado do emparelhamento referente ao(s) PG(s) responsável(is) pelo objeto não encontrado:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      O processamento de uma ou mais solicitações OSD está levando muito tempo. Isso pode ser uma indicação de carga extrema, um dispositivo de armazenamento lento ou um bug de software. É possível consultar a fila de solicitações no(s) OSD(s) em questão com o seguinte comando executado do host OSD:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      Você pode ver um resumo das solicitações recentes mais lentas:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      Você pode encontrar o local de um OSD com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Uma ou mais solicitações OSD foram bloqueadas por um período relativamente longo. Por exemplo, 4096 segundos. Isso é uma indicação de que o cluster não esteve saudável por um longo período (por exemplo, não há OSDs suficientes em execução ou PGs inativos) ou de que existe algum problema interno com o OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      Um ou mais PGs não foram depurados (consulte a <xref linkend="scrubbing-pgs"/>) recentemente. Normalmente, os PGs são depurados a cada <option>mon_scrub_interval</option> segundos, e esse aviso será acionado após decorrer <option>mon_warn_not_scrubbed</option> segundos sem uma depuração. Os PGs não serão depurados se não forem sinalizados como limpos, o que poderá ocorrer se forem armazenados incorretamente ou estiverem degradados (consulte PG_AVAILABILITY e PG_DEGRADED acima). Você pode iniciar manualmente uma depuração de um PG limpo com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Um ou mais PGs não foram depurados em detalhes (consulte a <xref linkend="scrubbing-pgs"/>) recentemente. Normalmente, os PGs são depurados a cada <option>osd_deep_mon_scrub_interval</option> segundos, e esse aviso é acionado quando <option>mon_warn_not_deep_scrubbed</option> segundos decorreram sem uma depuração. Os PGs não serão depurados (em detalhes) se não forem sinalizados como limpos, o que poderá ocorrer se forem armazenados incorretamente ou estiverem degradados (consulte PG_AVAILABILITY e PG_DEGRADED acima). Você pode iniciar manualmente uma depuração de um PG limpo com:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Se você especificou locais diferentes do padrão em sua configuração ou no chaveiro, deve especificar estes locais:
   </para>
<screen><prompt role="root">root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Verificando as estatísticas de uso de um cluster</title>

  <para>
   Para verificar o uso e a distribuição dos dados de um cluster entre pools, use o comando <command>ceph df</command>. Para obter mais detalhes, use <command>ceph df detail</command>.
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph df
--- RAW STORAGE ---
CLASS  SIZE    AVAIL   USED     RAW USED  %RAW USED
hdd    30 GiB  27 GiB  121 MiB   3.1 GiB      10.40
TOTAL  30 GiB  27 GiB  121 MiB   3.1 GiB      10.40

--- POOLS ---
POOL                   ID  STORED   OBJECTS  USED     %USED  MAX AVAIL
device_health_metrics   1      0 B        0      0 B      0    8.5 GiB
cephfs.my_cephfs.meta   2  1.0 MiB       22  4.5 MiB   0.02    8.5 GiB
cephfs.my_cephfs.data   3      0 B        0      0 B      0    8.5 GiB
.rgw.root               4  1.9 KiB       13  2.2 MiB      0    8.5 GiB
myzone.rgw.log          5  3.4 KiB      207    6 MiB   0.02    8.5 GiB
myzone.rgw.control      6      0 B        8      0 B      0    8.5 GiB
myzone.rgw.meta         7      0 B        0      0 B      0    8.5 GiB
</screen>

  <para>
   A seção <literal>RAW STORAGE</literal> da saída apresenta uma visão geral da quantidade de armazenamento que seu cluster usa para os dados.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>CLASS</literal>: A classe de armazenamento do dispositivo. Consulte a <xref linkend="crush-devclasses"/> para obter mais detalhes sobre classes de dispositivo.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>SIZE</literal>: A capacidade de armazenamento geral do cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: A quantidade de espaço livre disponível no cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: O espaço (acumulado de todos os OSDs) alocado exclusivamente para objetos de dados mantidos em dispositivo de blocos.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: A soma do espaço “USED” e do espaço alocado/reservado no dispositivo de blocos para finalidade do Ceph. Por exemplo, parte do BlueFS para o BlueStore.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: A porcentagem usada de armazenamento bruto. Use esse número em conjunto com <literal>full ratio</literal> e <literal>near full ratio</literal> para garantir que você não atinja a capacidade do cluster. Consulte a <xref linkend="storage-capacity"/> para obter mais detalhes.
    </para>
    <note>
     <title>Nível de preenchimento do cluster</title>
     <para>
      Quando o nível de preenchimento de um armazenamento bruto está próximo de 100%, você precisa adicionar um novo armazenamento ao cluster. O uso mais alto pode resultar em OSDs únicos cheios e problemas de saúde do cluster.
     </para>
     <para>
      Use o comando <command>ceph osd df tree</command> para listar o nível de preenchimento de todos os OSDs.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   A seção <literal>POOLS</literal> da saída apresenta uma lista dos pools e o uso estimado de cada pool. A saída dessa seção <emphasis>não</emphasis> reflete réplicas, clones ou instantâneos. Por exemplo, se você armazenar um objeto com 1 MB de dados, o uso estimado será de 1 MB, mas o uso real poderá ser de 2 MB ou mais, dependendo do número de réplicas, clones e instantâneos.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>POOL</literal>: O nome do pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: O ID do pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>STORED</literal>: A quantidade de dados armazenados pelo usuário.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: O número de objetos armazenados por pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: A quantidade de espaço alocado exclusivamente para dados por todos os nós OSD em kB.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: A porcentagem estimada de armazenamento usado por pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: O espaço máximo disponível no pool especificado.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Os números na seção POOLS são estimativas. Eles não incluem o número de réplicas, instantâneos ou clones. Como resultado, a soma dos valores <literal>USED</literal>e %<literal>USED</literal> não incluirá os valores <literal>RAW USED</literal> e <literal>%RAW USED</literal> na seção <literal>RAW STORAGE</literal> da saída.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Verificando o status do OSD</title>

  <para>
   Você pode verificar os OSDs para garantir que estejam ativos e em execução:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd stat</screen>

  <para>
   ou
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump</screen>

  <para>
   Você também pode ver os OSDs de acordo com a posição deles no mapa CRUSH.
  </para>

  <para>
   O <command>ceph osd tree</command> imprimirá uma árvore CRUSH com um host, os OSDs, se eles estão ativos e o peso:
  </para>

<screen>
   <prompt>cephuser@adm &gt; </prompt>ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME              STATUS  REWEIGHT  PRI-AFF
-1      3  0.02939  root default
-3      3  0.00980    rack mainrack
-2      3  0.00980            host osd-host
0       1  0.00980                    osd.0   up   1.00000   1.00000
1       1  0.00980                    osd.1   up   1.00000   1.00000
2       1  0.00980                    osd.2   up   1.00000   1.00000
</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Verificando se há OSDs cheios</title>

  <para>
   O Ceph impede você de gravar em um OSD cheio para evitar a perda de dados. Em um cluster operacional, você deve receber um aviso quando o cluster está próximo cota máxima. O padrão do valor <command>mon osd full ratio</command> é de 0,95, ou 95% da capacidade antes de impedir que os clientes gravem dados. O padrão do valor <command>mon osd nearfull ratio</command> é de 0,85, ou 85% da capacidade, quando ele gera um aviso de saúde.
  </para>

  <para>
   O <command>ceph health</command> relata os nós OSD cheios:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   ou
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   A melhor maneira de resolver um cluster cheio é adicionar novos hosts/discos OSD, o que permite ao cluster redistribuir os dados para o armazenamento recém-disponibilizado.
  </para>

  <tip>
   <title>Evitando OSDs cheios</title>
   <para>
    Depois que um OSD fica cheio (usa 100% do espaço em disco), ele costuma falhar rapidamente sem aviso. Veja a seguir algumas dicas para se lembrar na hora de administrar nós OSD.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      O espaço em disco de cada OSD (normalmente montado em <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) precisa ser colocado em um disco ou partição subjacente dedicado.
     </para>
    </listitem>
    <listitem>
     <para>
      Verifique os arquivos de configuração do Ceph e certifique-se de que o Ceph não armazene o arquivo de registro em discos/partições dedicados para uso por OSDs.
     </para>
    </listitem>
    <listitem>
     <para>
      Confirme se nenhum outro processo faz gravações nos discos/partições dedicados para uso por OSDs.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Verificando o status do monitor</title>

  <para>
   Após iniciar o cluster e antes da primeira leitura e/ou gravação de dados, verifique o status do quorum dos Ceph Monitors. Quando o cluster já estiver processando solicitações, verifique o status dos Ceph Monitors periodicamente para garantir que estejam em execução.
  </para>

  <para>
   Para exibir o mapa do monitor, execute o seguinte:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph mon stat</screen>

  <para>
   ou
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump</screen>

  <para>
   Para verificar o status do quorum para o cluster do monitor, execute o seguinte:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph quorum_status</screen>

  <para>
   O Ceph retornará o status do quorum. Por exemplo, um cluster do Ceph com três monitores pode retornar o seguinte:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Verificando estados de grupos de posicionamento</title>

  <para>
   Os grupos de posicionamento mapeiam objetos para OSDs. Ao monitorar seus grupos de posicionamento, você deseja que eles estejam <literal>ativos</literal> e <literal>limpos</literal>. Para ver uma discussão detalhada, consulte a <xref linkend="op-mon-osd-pg"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage-capacity">
  <title>Capacidade de armazenamento</title>

  <para>
   Quando um cluster de armazenamento do Ceph está próximo da sua capacidade máxima, o Ceph impede você de gravar ou ler os Ceph OSDs como medida de segurança para evitar perda de dados. Portanto, permitir que um cluster de produção se aproxime da sua cota máxima não é uma boa prática, porque coloca em risco a alta disponibilidade. A cota máxima padrão está definida como 0,95, que significa 95% da capacidade. Essa é uma configuração muito agressiva para um cluster de teste com um número pequeno de OSDs.
  </para>

  <tip>
   <title>Aumentar a Capacidade de Armazenamento</title>
   <para>
    Ao monitorar o cluster, fique atento aos avisos relacionados à cota <literal>nearfull</literal>. Eles indicam que a falha de alguns OSDs pode resultar em interrupção temporária de serviço. Considere adicionar mais OSDs para aumentar a capacidade de armazenamento.
   </para>
  </tip>

  <para>
   Um cenário comum para clusters de teste envolve um administrador do sistema que remove um Ceph OSD do cluster de armazenamento do Ceph para observar a redistribuição do cluster. Em seguida, ele remove outro Ceph OSD, e assim por diante, até o cluster atingir a cota máxima e ser bloqueado. Recomendamos um pouco de planejamento de capacidade, mesmo com um cluster de teste. O planejamento permite estimar a quantidade de capacidade sobressalente que você precisará para manter a alta disponibilidade. O ideal é planejar uma série de falhas de Ceph OSDs em que o cluster possa se recuperar ao estado <literal>ativo + limpo</literal> sem substituir os Ceph OSDs imediatamente. Você pode executar um cluster no estado <literal>ativo + degradado</literal>, mas isso não é ideal em condições normais de operação.
  </para>

  <para>
   O diagrama a seguir retrata um cluster de armazenamento do Ceph simples contendo 33 nós do Ceph com um Ceph OSD por host, cada um deles lendo e gravando em uma unidade de 3 TB. Esse cluster de exemplo tem uma capacidade máxima real de 99 TB. A opção <option>mon osd full ratio</option> está definida como 0,95. Se o cluster atingir 5 TB da capacidade restante, ele não permitirá que os clientes leiam e gravem dados. Portanto, a capacidade operacional do cluster de armazenamento é de 95 TB, não de 99 TB.
  </para>

  <figure>
   <title>Cluster do Ceph</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   Nesse tipo de cluster, é normal haver falha de um ou dois OSDs. Um cenário menos frequente, mas considerável, envolve uma falha no roteador do rack ou no fornecimento de energia, o que desligaria vários OSDs ao mesmo tempo (por exemplo, OSDs 7-12). Nesse cenário, você ainda deve recorrer a um cluster capaz de se manter operante e atingir o estado <literal>ativo + limpo</literal>, mesmo que isso signifique adicionar alguns hosts com outros OSDs a curto prazo. Se o uso da capacidade for muito alto, talvez você não perca os dados. Porém, você ainda poderá colocar em risco a disponibilidade dos dados ao resolver uma interrupção em um domínio de falha, se o uso da capacidade do cluster exceder a cota máxima. Por esse motivo, recomendamos pelo menos algum planejamento de capacidade estimada.
  </para>

  <para>
   Identifique dois números para o cluster:
  </para>

  <orderedlist>
   <listitem>
    <para>
     O número de OSDs.
    </para>
   </listitem>
   <listitem>
    <para>
     A capacidade total do cluster.
    </para>
   </listitem>
  </orderedlist>

  <para>
   Se você dividir a capacidade total pelo número de OSDs do cluster, encontrará a capacidade média de um OSD no cluster. Considere multiplicar esse número pela quantidade de OSDs que podem falhar simultaneamente durante as operações normais (um número relativamente pequeno). Por fim, multiplique a capacidade do cluster pela cota máxima para chegar a uma capacidade operacional máxima. Em seguida, subtraia o número da quantidade de dados dos OSDs que podem falhar para chegar a uma cota máxima razoável. Repita o processo anterior com um número maior de falhas de OSD (um rack de OSDs) para chegar a um número razoável para uma cota quase máxima.
  </para>

  <para>
   As seguintes configurações aplicam-se apenas à criação de cluster e são armazenadas no mapa OSD:
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    Essas configurações aplicam-se apenas durante a criação do cluster. Depois disso, elas precisarão ser mudadas no Mapa OSD usando os comandos <command>ceph osd set-nearfull-ratio</command> e <command>ceph osd set-full-ratio</command>.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      A porcentagem de espaço em disco usado antes que um OSD seja considerado <literal>cheio</literal>. O padrão é 0,95
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      A porcentagem de espaço em disco usado antes que um OSD seja considerado muito <literal>cheio</literal> para provisionamento. O padrão é 0,90
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      A porcentagem de espaço em disco usado antes que um OSD seja considerado <literal>quase cheio</literal>. O padrão é 0,85
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>Verificar o peso do OSD</title>
   <para>
    Se alguns OSDs estiverem <literal>quase cheios</literal>, mas outros tiverem bastante capacidade, talvez você tenha problemas com o peso do CRUSH para os OSDs <literal>quase cheios</literal>.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op-mon-osd-pg">
  <title>Monitorando OSDs e grupos de posicionamento</title>

  <para>
   As altas disponibilidade e confiabilidade exigem uma abordagem tolerante a falhas para gerenciar problemas de hardware e software. O Ceph não tem um ponto único de falha e pode processar solicitações de dados no modo "degradado". O posicionamento de dados do Ceph introduz uma camada de indireção para garantir que os dados não sejam diretamente vinculados a determinados endereços de OSD. Isso significa que o monitoramento de falhas no sistema requer encontrar o grupo de posicionamento e os OSDs subjacentes na raiz do problema.
  </para>

  <tip>
   <title>Acesso em caso de falha</title>
   <para>
    Uma falha em uma parte do cluster pode impedir você de acessar um determinado objeto. Isso não significa que você não pode acessar outros objetos. Em caso de falha, siga as etapas para monitorar os OSDs e grupos de posicionamento. Em seguida, comece a solução de problemas.
   </para>
  </tip>

  <para>
   Em geral, o Ceph tem a funcionalidade de conserto automático. No entanto, quando os problemas persistem, o monitoramento de OSDs e grupos de posicionamento ajuda você a identificá-los.
  </para>

  <sect2 xml:id="op-mon-osds">
   <title>Monitorando OSDs</title>
   <para>
    O status de um OSD é <emphasis>no cluster</emphasis> (“in”) ou <emphasis>fora do cluster</emphasis> (“out”). Ao mesmo tempo, ele é <emphasis>em execução</emphasis> (“up”) ou <emphasis>inativo e não operante</emphasis> (“down”). Se um OSD é “up”, ele pode estar no cluster (você pode ler e gravar dados) ou fora do cluster. Se ele estava no cluster e foi recentemente removido do cluster, o Ceph migra os grupos de posicionamento para outros OSDs. Se um OSD estiver fora do cluster, o CRUSH não atribuirá grupos de posicionamento a ele. Se um OSD é “down”, ele também deve ser “out”.
   </para>
   <note>
    <title>Estado não saudável</title>
    <para>
     Se um OSD é “down” e “in”, há um problema, e o estado do cluster não é saudável.
    </para>
   </note>
   <para>
    Se você executar um comando, como <command>ceph health</command>, <command>ceph -s</command> ou <command>ceph -w</command>, poderá perceber que o cluster nem sempre retorna <literal>HEALTH OK</literal>. Em relação aos OSDs, você deve esperar que o cluster <emphasis>não</emphasis> retorne <literal>HEALTH OK</literal> nas seguintes circunstâncias:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Você ainda não iniciou o cluster (ele não responderá).
     </para>
    </listitem>
    <listitem>
     <para>
      Você iniciou ou reiniciou o cluster e ele ainda não está pronto, porque os grupos de posicionamento estão sendo criados e os OSDs estão no processo de emparelhamento.
     </para>
    </listitem>
    <listitem>
     <para>
      Você adicionou ou removeu um OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Você modificou o mapa do cluster.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Um aspecto importante do monitoramento de OSDs é garantir que, quando o cluster estiver em execução, todos os OSDs no cluster também estejam funcionando. Para ver se todos os OSDs estão funcionando, execute:
   </para>
<screen>
<prompt role="root">root # </prompt>ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    O resultado deve indicar o número total de OSDs (x), quantos estão “up” (y), quantos estão “in” (z) e a época do mapa (eNNNN). Se o número de OSDs que estão “in” no cluster for maior do que o número de OSDs que estão “up”, execute o seguinte comando para identificar os daemons <literal>ceph-osd</literal> que não estão funcionando:
   </para>
<screen>
<prompt role="root">root # </prompt>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    Por exemplo, se um OSD com ID 1 estiver desativado, inicie-o:
   </para>
<screen>
<prompt>cephuser@osd &gt; </prompt>sudo systemctl start ceph-<replaceable>CLUSTER_ID</replaceable>@osd.0.service
</screen>
   <para>
    Consulte o <xref linkend="bp-troubleshooting-not-running"/> para ver os problemas associados aos OSDs que estão parados ou que não serão reiniciados.
   </para>
  </sect2>

  <sect2 xml:id="op-pgsets">
   <title>Atribuindo conjuntos de grupos de posicionamento</title>
   <para>
    Quando o CRUSH atribui grupos de posicionamento a OSDs, ele analisa o número de réplicas para o pool e atribui o grupo de posicionamento aos OSDs de modo que cada réplica do grupo de posicionamento seja atribuída a um OSD diferente. Por exemplo, se o pool exigir três réplicas de um grupo de posicionamento, o CRUSH poderá atribuí-las a <literal>osd.1</literal>, <literal>osd.2</literal> e <literal>osd.3</literal>, respectivamente. Na verdade, o CRUSH busca um posicionamento pseudo-aleatório que considera os domínios de falha definidos no seu Mapa CRUSH. Sendo assim, você raramente verá grupos de posicionamento atribuídos aos OSDs vizinhos mais próximos em um cluster de grande porte. Nossa referência é ao conjunto de OSDs que deve incluir as réplicas de um determinado grupo de posicionamento como o <emphasis>conjunto de atuação</emphasis>. Em alguns casos, um OSD no conjunto de atuação está inativo ou, de alguma outra forma, não pode processar as solicitações para objetos no grupo de posicionamento. Nesses tipos de situação, um dos seguintes cenários pode ocorrer:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Você adicionou ou removeu um OSD. Em seguida, o CRUSH reatribuiu o grupo de posicionamento a outros OSDs e, portanto, mudou a composição do <emphasis>conjunto de atuação</emphasis>, provocando a migração dos dados com um processo de “provisionamento”.
     </para>
    </listitem>
    <listitem>
     <para>
      Um OSD estava “down”, foi reiniciado e agora está se recuperando.
     </para>
    </listitem>
    <listitem>
     <para>
      Um OSD no <emphasis>conjunto de atuação</emphasis> está “down” ou não pode processar as solicitações, e outro OSD assumiu temporariamente as tarefas dele.
     </para>
     <para>
      O Ceph processa uma solicitação de cliente usando o <emphasis>conjunto ativo</emphasis>, que é o conjunto de OSDs que processará de fato as solicitações. Na maioria dos casos, o <emphasis>conjunto ativo</emphasis> e o <emphasis>conjunto de atuação</emphasis> são praticamente idênticos. Quando não são, isso pode indicar que o Ceph está migrando dados, que um OSD está se recuperando ou que há um problema. Por exemplo, o Ceph costuma retornar o estado <literal>HEALTH WARN</literal> com a mensagem “stuck stale” em cenários assim.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Para recuperar uma lista de grupos de posicionamento, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    Para ver quais OSDs estão no <emphasis>conjunto de atuação</emphasis> ou no <emphasis>conjunto ativo</emphasis> para um determinado grupo de posicionamento, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg map <replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -&gt; up [0,1,2] acting [0,1,2]
</screen>
   <para>
    O resultado deve indicar a época do osdmap (eNNN), o número do grupo de posicionamento (<replaceable>PG_NUM</replaceable>), os OSDs no <emphasis>conjunto ativo</emphasis> (“up”) e os OSDs no <emphasis>conjunto de atuação</emphasis> (“acting”):
   </para>
   <tip>
    <title>Indicador de problema no cluster</title>
    <para>
     Se o <emphasis>conjunto ativo</emphasis> e o <emphasis>conjunto de atuação</emphasis> não forem os mesmos, isso pode ser um indicador de que o cluster está realizando a própria redistribuição ou de um possível problema com o cluster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-peering">
   <title>Emparelhamento</title>
   <para>
    Antes que você possa gravar dados em um grupo de posicionamento, ele deve estar no estado <literal>ativo</literal> e, de preferência, <literal>limpo</literal>. Para o Ceph determinar o estado atual de um grupo de posicionamento, o OSD principal do grupo de posicionamento (o primeiro OSD no <emphasis>conjunto de atuação</emphasis>), é emparelhado com os OSDs secundários e terciários para estabelecer um acordo em relação ao estado atual do grupo de posicionamento (considerando um pool com três réplicas do PG).
   </para>
   <figure>
    <title>Esquema de emparelhamento</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op-mon-pg-states">
   <title>Monitorando estados de grupos de posicionamento</title>
   <para>
    Se você executar um comando, como <command>ceph health</command>, <command>ceph -s</command> ou <command>ceph -w</command>, poderá perceber que o cluster nem sempre retorna a mensagem <literal>HEALTH OK</literal>. Após verificar se os OSDs estão em execução, verifique também os estados do grupo de posicionamento.
   </para>
   <para>
    É esperado que o cluster <emphasis role="bold">não</emphasis> retorne <literal>HEALTH OK</literal> em várias circunstâncias relacionadas ao emparelhamento de grupos de posicionamento:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Você criou um pool, e os grupos de posicionamento ainda não foram emparelhados.
     </para>
    </listitem>
    <listitem>
     <para>
      Os grupos de posicionamento estão se recuperando.
     </para>
    </listitem>
    <listitem>
     <para>
      Você adicionou ou removeu um OSD do cluster.
     </para>
    </listitem>
    <listitem>
     <para>
      Você modificou seu Mapa CRUSH, e seus grupos de posicionamento estão sendo migrados.
     </para>
    </listitem>
    <listitem>
     <para>
      Há dados inconsistentes em diferentes réplicas de um grupo de posicionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      O Ceph está depurando as réplicas de um grupo de posicionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      O Ceph não tem capacidade de armazenamento suficiente para concluir as operações de provisionamento.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Se uma das circunstâncias mencionadas acima fizer com que o Ceph retorne <literal>HEALTH WARN</literal>, não se preocupe. Em muitos casos, o cluster se recuperará por conta própria. Em alguns casos, pode ser necessário tomar medidas. Um aspecto importante do monitoramento dos grupos de posicionamento é garantir que, quando o cluster estiver em funcionamento, todos os grupos de posicionamento estejam "ativos" e, de preferência, no "estado limpo". Para ver o status de todos os grupos de posicionamento, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    O resultado deve indicar o número total de grupos de posicionamento (x), quantos grupos de posicionamento estão em determinado estado, como “ativo+limpo” (y) e a quantidade de dados armazenados (z).
   </para>
   <para>
    Além dos estados do grupo de posicionamento, o Ceph também retornará a quantidade de capacidade de armazenamento utilizada (aa), a quantidade de capacidade de armazenamento restante (bb) e a capacidade total de armazenamento do grupo de posicionamento. Esses números podem ser importantes em alguns casos:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Você está atingindo a <option>cota quase máxima</option> ou <option>cota máxima</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Seus dados não estão sendo distribuídos por todo o cluster por causa de um erro na configuração do CRUSH.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>IDs dos grupos de posicionamento</title>
    <para>
     Os IDs dos grupos de posicionamento consistem no número do pool (não no nome do pool) seguido de um ponto (.) e no ID do grupo de posicionamento: um número hexadecimal. Você pode ver os números de pool e os respectivos nomes na saída do comando <command>ceph osd lspools</command>. Por exemplo, o pool padrão <literal>rbd</literal> corresponde ao número do pool 0. Um ID de grupo de posicionamento totalmente qualificado tem o seguinte formato:
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     Normalmente, ele tem a seguinte aparência:
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    Para recuperar uma lista de grupos de posicionamento, execute o seguinte:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    Você também pode formatar a saída em JSON e gravá-la em um arquivo:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    Para consultar um determinado grupo de posicionamento, execute o seguinte:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    A lista a seguir descreve os estados comuns do grupo de posicionamento em detalhes.
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING</term>
     <listitem>
      <para>
       Quando você cria um pool, ele cria o número de grupos de posicionamento que você especificou. O Ceph retornará “creating” (criando) durante a criação de um ou mais grupos de posicionamento. Quando são criados, os OSDs que fazem parte do <emphasis>conjunto de atuação</emphasis> do grupo de posicionamento são emparelhados. Quando o emparelhamento é concluído, o status do grupo de posicionamento deve ser “ativo+limpo”, o que significa que um cliente Ceph pode começar a gravar no grupo de posicionamento.
      </para>
      <figure>
       <title>Status dos grupos de posicionamento</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING</term>
     <listitem>
      <para>
       Quando o Ceph está emparelhando um grupo de posicionamento, ele leva os OSDs que armazenam as réplicas do grupo de posicionamento a um acordo em relação ao estado dos objetos e metadados nesse grupo. Quando o Ceph conclui o emparelhamento, isso significa que os OSDs que armazenam o grupo de posicionamento concordam sobre o estado atual desse grupo. No entanto, a conclusão do processo de emparelhamento <emphasis role="bold">não</emphasis> significa que cada réplica tem o conteúdo mais recente.
      </para>
      <note>
       <title>Histórico de autorização</title>
       <para>
        O Ceph <emphasis role="bold">não</emphasis> confirmará uma operação de gravação em um cliente até que todos os OSDs do <emphasis>conjunto de atuação</emphasis> continuem a operação de gravação. Esta prática garante que pelo menos um membro do <emphasis>conjunto de atuação</emphasis> tenha um registro de cada operação de gravação confirmada desde a última operação de emparelhamento bem-sucedida.
       </para>
       <para>
        Com um registro preciso de cada operação de gravação confirmada, o Ceph pode construir e ampliar um novo histórico de autorização do grupo de posicionamento: um conjunto completo e totalmente organizado de operações que, se executadas, mantêm uma cópia do OSD de um grupo de posicionamento atualizada.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ACTIVE</term>
     <listitem>
      <para>
       Quando o Ceph conclui o processo de emparelhamento, um grupo de posicionamento pode se tornar <literal>ativo</literal>. O estado <literal>ativo</literal> significa que os dados no grupo de posicionamento geralmente estão disponíveis no grupo de posicionamento principal e nas réplicas para as operações de leitura e gravação.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN</term>
     <listitem>
      <para>
       Quando um grupo de posicionamento está no estado <literal>limpo</literal>, o OSD principal e os OSDs de réplica foram emparelhados com êxito e não há réplicas perdidas para o grupo de posicionamento. O Ceph replicou o número correto de vezes todos os objetos no grupo de posicionamento.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Quando um cliente grava um objeto no OSD principal, esse OSD é responsável por gravar as réplicas nos OSDs de réplica. Depois que o OSD principal gravar o objeto no armazenamento, o grupo de posicionamento permanecerá no estado "degradado" até que o OSD principal receba uma confirmação dos OSDs de réplica de que que Ceph criou os objetos de réplica com êxito.
      </para>
      <para>
       O motivo pelo qual um grupo de posicionamento pode ser “ativo+degradado” é que um OSD pode ser “ativo” mesmo que ainda não armazene todos os objetos. Se um OSD ficar inativo, o Ceph marcará cada grupo de posicionamento atribuído ao OSD como “degradado”. Os OSDs deverão ser emparelhados novamente quando o OSD voltar a ficar ativo. No entanto, um cliente ainda poderá gravar um novo objeto em um grupo de posicionamento degradado se ele for “ativo”.
      </para>
      <para>
       Se um OSD for “inativo” e a condição “degradado” permanecer, o Ceph poderá marcar o OSD inativo como “fora” do cluster e remapear os dados do OSD “inativo” para outro OSD. O tempo entre ser marcado como “inativo” e como “fora” é controlado pela opção <option>mon osd down out interval</option>, que por padrão está definida como 600 segundos.
      </para>
      <para>
       Um grupo de posicionamento também pode ser "degradado" porque o Ceph não encontra um ou mais objetos que deveriam estar no grupo de posicionamento. Embora você não possa ler ou gravar em objetos não encontrados, ainda pode acessar todos os outros objetos no grupo de posicionamento “degradado”.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING</term>
     <listitem>
      <para>
       O Ceph foi projetado para tolerância a falhas em escala, quando os problemas de hardware e software são constantes. Quando um OSD fica “inativo”, o conteúdo dele pode não acompanhar o estado atual das outras réplicas nos grupos de posicionamento. Quando o OSD volta a ficar “ativo”, o conteúdo dos grupos de posicionamento deve ser atualizado para refletir o estado atual. Durante esse período, o OSD pode refletir um estado de "recuperação".
      </para>
      <para>
       A recuperação nem sempre é comum, porque uma falha de hardware pode causar uma falha em cascata de vários OSDs. Por exemplo, em uma possível falha do switch de rede para um rack ou gabinete, os OSDs de várias máquinas host podem não acompanhar o estado atual do cluster. Cada um dos OSDs deverá se recuperar quando a falha for resolvida.
      </para>
      <para>
       O Ceph oferece uma série de configurações para equilibrar a contenção de recursos entre as novas solicitações de serviço e a necessidade de recuperar os objetos de dados e restaurar os grupos de posicionamento ao estado atual. A configuração <option>osd recovery delay start</option> permite que um OSD reinicie, emparelhe novamente e até processe algumas solicitações de reprodução antes do início do processo de recuperação. A <option>osd recovery thread timeout</option> define um tempo de espera do thread porque vários OSDs podem falhar, ser reiniciados e novamente emparelhados em fases. A configuração <option>osd recovery max active</option> limita o número de solicitações de recuperação que um OSD processará simultaneamente para impedir falha no processamento do OSD. A configuração <option>osd recovery max chunk</option> limita o tamanho dos blocos de dados recuperados para evitar congestionamento de rede.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING</term>
     <listitem>
      <para>
       Quando um novo OSD ingressa no cluster, o CRUSH reatribui os grupos de posicionamento dos OSDs no cluster para o OSD recém-adicionado. Forçar o novo OSD a aceitar os grupos de posicionamento reatribuídos imediatamente pode sobrecarregar o novo OSD. O provisionamento do OSD com os grupos de posicionamento permite que este processo seja iniciado em segundo plano. Quando o provisionamento for concluído, o novo OSD começará a processar solicitações quando estiver pronto.
      </para>
      <para>
       Durante as operações de provisionamento, você pode ver um dos vários estados: “backfill_wait” indica que uma operação de provisionamento está pendente, mas ainda não está em andamento; “backfill” indica que uma operação de provisionamento está em andamento; “backfill_too_full” indica que uma operação de provisionamento foi solicitada, mas não pôde ser concluída devido à capacidade de armazenamento insuficiente. Quando não é possível provisionar um grupo de posicionamento, ele pode ser considerado “incompleto”.
      </para>
      <para>
       O Ceph dispõe de várias configurações para gerenciar a carga associada à reatribuição de grupos de posicionamento para um OSD (especialmente um novo OSD). Por padrão, <option>osd max backfills</option> define o número máximo de provisionamentos simultâneos de/para um OSD como 10. A <option>cota máxima de provisionamento</option> permite que um OSD recuse uma solicitação de provisionamento se ele estiver próximo da sua cota máxima (90%, por padrão) e faça a modificação com o comando <command>ceph osd set-backfillfull-ratio</command>. Se um OSD recusar uma solicitação de provisionamento, o <option>osd backfill retry interval</option> permitirá que um OSD repita a solicitação (após 10 segundos, por padrão). O OSDs também podem definir <option>osd backfill scan min</option> e <option>osd backfill scan max</option> para gerenciar intervalos de exploração (64 e 512, por padrão).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED</term>
     <listitem>
      <para>
       Quando o <emphasis>conjunto de atuação</emphasis> que processa um grupo de posicionamento é modificado, os dados são migrados do <emphasis>conjunto de atuação</emphasis> antigo para o <emphasis>conjunto de atuação</emphasis> novo. Pode levar algum tempo para um novo OSD principal processar as solicitações. Por isso, talvez seja solicitado para o conjunto principal antigo continuar processando as solicitações até que a migração do grupo de posicionamento seja concluída. Quando a migração dos dados for concluída, o mapeamento usará o OSD principal do novo <emphasis>conjunto de atuação</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STALE</term>
     <listitem>
      <para>
       Enquanto o Ceph usa os heartbeats para garantir que os hosts e daemons estejam em execução, os daemons <literal>ceph-osd</literal> também podem entrar em um estado “travado” quando não estiverem relatando estatísticas em tempo hábil (por exemplo, uma falha temporária na rede). Por padrão, os daemons OSD relatam suas estatísticas de grupo de posicionamento, inicialização e falha a cada meio segundo (0,5), que é mais frequente do que os limites de heartbeat. Se o OSD principal do <emphasis>conjunto de atuação</emphasis> de um grupo de posicionamento não puder relatar para o monitor ou se outros OSDs relataram o OSD principal como “inativo”, os monitores marcarão o grupo de posicionamento como “obsoleto”.
      </para>
      <para>
       Quando você inicia o cluster, é comum ver o estado "obsoleto" até o processo de emparelhamento ser concluído. Depois que o cluster estiver funcionando por um tempo, ver grupos de posicionamento no estado "obsoleto" indicará que o OSD principal para esses grupos está inativo ou não está relatando as estatísticas de grupo de posicionamento para o monitor.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-pg-objectfinding">
   <title>Encontrando o local de um objeto</title>
   <para>
    Para armazenar dados de objetos no Armazenamento de Objetos do Ceph, um cliente Ceph precisa definir um nome de objeto e especificar um pool relacionado. O cliente Ceph recupera o mapa do cluster mais recente e o algoritmo CRUSH calcula como mapear o objeto para um grupo de posicionamento e, em seguida, calcula como atribuir o grupo de posicionamento a um OSD dinamicamente. Para encontrar o local do objeto, tudo o que você precisa é do nome do objeto e do nome do pool. Por exemplo:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>Localizando um objeto</title>
    <para>
     Como exemplo, vamos criar um objeto. Especifique o nome do objeto “test-object-1”, um caminho para o arquivo de exemplo “testfile.txt” com alguns dados do objeto e o nome do pool “data” usando o comando <command>rados put</command> na linha de comando:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     Para verificar se o Armazenamento de Objetos do Ceph armazenou o objeto, execute o seguinte:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados -p data ls
</screen>
    <para>
     Agora, identifique o local do objeto. O Ceph retornará o local do objeto:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     Para remover o objeto de exemplo, basta apagá-lo usando o comando <command>rados rm</command>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
</chapter>
