<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_core.xml" version="5.0" xml:id="deploy-core">
 <info>
  <title>Implantando os serviços principais restantes com o cephadm</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Após implantar o cluster básico do Ceph, implante os serviços principais em mais nós do cluster. Para tornar os dados do cluster acessíveis aos clientes, implante também mais serviços.
 </para>
 <para>
  Atualmente, oferecemos suporte à implantação de serviços do Ceph na linha de comando usando o orquestrador do Ceph (subcomandos <command>ceph orch</command>).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>O comando <command>ceph orch</command></title>

  <para>
   O comando do orquestrador do Ceph <command>ceph orch</command>, uma interface com o módulo cephadm, processará a listagem de componentes do cluster e a implantação dos serviços do Ceph em novos nós do cluster.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Exibindo o status do orquestrador</title>
   <para>
    O comando a seguir mostra o modo e o status atuais do orquestrador do Ceph.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Listando dispositivos, serviços e daemons</title>
   <para>
    Para listar todos os dispositivos de disco, execute o seguinte:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>Serviços e daemons</title>
    <para>
     <emphasis>Serviço</emphasis> é um termo geral para um serviço do Ceph de um tipo específico, por exemplo, Ceph Manager.
    </para>
    <para>
     <emphasis>Daemon</emphasis> é uma instância específica de um serviço, por exemplo, um processo <literal>mgr.ses-min1.gdlcik</literal> executado em um nó denominado <literal>ses-min1</literal>.
    </para>
   </tip>
   <para>
    Para listar todos os serviços conhecidos pelo cephadm, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     Você pode limitar a lista a serviços em um nó específico com o parâmetro opcional <option>--host</option> e a serviços de um determinado tipo com o parâmetro opcional <option>--service-type</option>. Os tipos aceitáveis são <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>, <literal>mds</literal> e <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    Para listar todos os daemons em execução implantados pelo cephadm, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     Para consultar o status de um daemon específico, use <option>--daemon_type</option> e <option>--daemon_id</option>. Para os OSDs, o ID é o ID numérico do OSD. Para o MDS, o ID é o nome do sistema de arquivos:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd --daemon_id 0
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Especificação de serviço e posicionamento</title>

  <para>
   A maneira recomendada de especificar a implantação dos serviços do Ceph é criar um arquivo no formato YAML com a especificação dos serviços que você pretende implantar.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Criando especificações de serviço</title>
   <para>
    Você pode criar um arquivo de especificação separado para cada tipo de serviço, por exemplo:
   </para>
<screen>
<prompt>root@master # </prompt>cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Se preferir, você poderá especificar vários (ou todos) tipos de serviço em um arquivo (por exemplo, <filename>cluster.yml</filename>), que descreve quais nós executarão serviços específicos. Lembre-se de separar os tipos de serviço individuais com três traços (<literal>---</literal>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    As propriedades mencionadas acima têm o seguinte significado:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       O tipo de serviço. Ele pode ser um serviço do Ceph (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>, <literal>crash</literal>, <literal>osd</literal> ou <literal>rbd-mirror</literal>), um gateway (<literal>nfs</literal> ou <literal>rgw</literal>) ou parte da pilha de monitoramento (<literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> ou <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       O nome do serviço. As especificações do tipo <literal>mon</literal>, <literal>mgr</literal>, <literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> e <literal>prometheus</literal> não exigem a propriedade <literal>service_id</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Especifica os nós que executarão o serviço. Consulte a <xref linkend="cephadm-placement-specs"/> para obter mais detalhes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Especificação adicional relevante para o tipo de serviço.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Aplicando serviços específicos</title>
    <para>
     Geralmente, os serviços de cluster do Ceph têm várias propriedades específicas. Para obter exemplos e detalhes da especificação de cada serviço, consulte a <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Criando a especificação de posicionamento</title>
   <para>
    Para implantar os serviços do Ceph, o cephadm precisa saber em quais nós implantá-los. Use a propriedade <literal>placement</literal> e liste os nomes abreviados de host dos nós aos quais o serviço se aplica:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Aplicando a especificação de cluster</title>
   <para>
    Após criar um arquivo <filename>cluster.yml</filename> completo com as especificações de todos os serviços e seu posicionamento, você poderá aplicar o cluster executando o seguinte comando:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yml</screen>
   <para>
    Para ver o status do cluster, execute o comando <command>ceph orch status</command>. Para ver mais detalhes, consulte a <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exportando a especificação de um cluster em execução</title>
   <para>
    Embora você tenha implantado serviços no cluster do Ceph usando os arquivos de especificação, conforme descrito na <xref linkend="cephadm-service-and-placement-specs"/>, a configuração do cluster pode divergir da especificação original durante sua operação. Além disso, você pode ter removido os arquivos de especificação por engano.
   </para>
   <para>
    Para recuperar uma especificação completa de um cluster em operação, execute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     Você pode anexar a opção <option>--format</option> para mudar o formato de saída padrão <literal>yaml</literal>. Você pode selecionar entre <literal>json</literal>, <literal>json-pretty</literal> ou <literal>yaml</literal>. Por exemplo:
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Implantar serviços do Ceph</title>

  <para>
   Depois que o cluster básico estiver em execução, você poderá implantar os serviços do Ceph nos outros nós.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Implantando Ceph Monitors e Ceph Managers</title>
   <para>
    O cluster do Ceph tem três ou cinco MONs implantados em nós diferentes. Se houver cinco ou mais nós no cluster, recomendamos a implantação de cinco MONs. Uma boa prática é implantar os MGRs nos mesmos nós que os MONs.
   </para>
   <important>
    <title>Incluir MON de boot</title>
    <para>
     Ao implantar MONs e MGRs, lembre-se de incluir o primeiro MON que você adicionou durante a configuração do cluster básico na <xref linkend="deploy-cephadm-configure-mon"/>.
    </para>
   </important>
   <para>
    Para implantar MONs, use a seguinte especificação:
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     Se você precisar adicionar outro nó, anexe o nome de host à mesma lista YAML. Por exemplo:
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    Da mesma forma, para implantar MGRs, use a seguinte especificação:
   </para>
   <important>
    <para>
     Verifique se a implantação tem pelo menos três Ceph Managers em cada implantação.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     Se os MONs ou MGRs <emphasis>não</emphasis> estiverem na mesma sub-rede, você precisará anexar os endereços das sub-redes. Por exemplo:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Implantando Ceph OSDs</title>
   <important>
    <title>Quando o dispositivo de armazenamento está disponível</title>
    <para>
     Um dispositivo de armazenamento será considerado <emphasis>disponível</emphasis> se todas as condições abaixo forem verdadeiras:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       O dispositivo não tem partições.
      </para>
     </listitem>
     <listitem>
      <para>
       O dispositivo não tem nenhum estado de LVM.
      </para>
     </listitem>
     <listitem>
      <para>
       O dispositivo não está montado.
      </para>
     </listitem>
     <listitem>
      <para>
       O dispositivo não contém um sistema de arquivos.
      </para>
     </listitem>
     <listitem>
      <para>
       O dispositivo não contém um OSD com BlueStore.
      </para>
     </listitem>
     <listitem>
      <para>
       O dispositivo é maior do que 5 GB.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Se as condições acima não forem verdadeiras, o Ceph se recusará a provisionar esses OSDs.
    </para>
   </important>
   <para>
    A implantação de OSDs pode ser feita de duas maneiras:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Instrua o Ceph a consumir todos os dispositivos de armazenamento disponíveis e não utilizados:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Use o DriveGroups (consulte o <xref linkend="drive-groups"/>) para criar uma especificação de OSD que descreva os dispositivos que serão implantados com base em suas propriedades, como tipo de dispositivo (SSD ou HDD), nomes de modelo de dispositivo e tamanho, ou os nós onde os dispositivos residem. Em seguida, aplique a especificação executando o seguinte comando:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Implantando servidores de metadados</title>
   <para>
    O CephFS requer um ou mais serviços do Servidor de Metadados (MDS, Metadata Server). Para criar um CephFS, primeiro crie os servidores MDS usando a seguinte especificação:
   </para>
   <note>
    <para>
     Verifique se você já criou pelo menos dois pools (um para os dados do CephFS e outro para os metadados do CephFS) antes de usar a especificação a seguir.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    Depois que os MDSs estiverem em operação, crie o CephFS:
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Implantando Gateways de Objetos</title>
   <para>
    O cephadm implanta um Gateway de Objetos como uma coleção de daemons que gerenciam um <emphasis>domínio</emphasis> e uma <emphasis>zona</emphasis> específicos.
   </para>
   <para>
    Você pode relacionar um serviço de Gateway de Objetos a um domínio e uma zona existentes (consulte o <xref linkend="ceph-rgw-fed"/> para obter mais detalhes) ou especificar um <replaceable>REALM_NAME</replaceable> e <replaceable>ZONE_NAME</replaceable> não existentes, e eles serão criados automaticamente depois que você aplicar a seguinte configuração:
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Usando o acesso SSL seguro</title>
    <para>
     Para usar uma conexão SSL segura com o Gateway de Objetos, você precisa de um par de arquivos de chave e certificado SSL válidos (consulte o <xref linkend="ceph-rgw-https"/> para obter mais detalhes). Você precisa habilitar o SSL e especificar um número de porta para conexões SSL e os arquivos de chave e certificado SSL.
    </para>
    <para>
     Para habilitar o SSL e especificar o número da porta, inclua o seguinte em sua especificação:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     Para especificar a chave e o certificado SSL, você pode colar o conteúdo deles diretamente no arquivo de especificação YAML. O sinal de barra vertical (<literal>|</literal>) no final da linha informa ao analisador para esperar um valor de string de várias linhas. Por exemplo:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      Em vez de colar o conteúdo dos arquivos de chave e certificado SSL, você pode omitir as palavras-chave <literal>rgw_frontend_ssl_certificate:</literal> e <literal>rgw_frontend_ssl_key:</literal> e fazer upload deles no banco de dados de configuração:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>Configurar o Gateway de Objetos para escutar nas portas 443 e 80</title>
     <para>
      Para configurar o Gateway de Objetos para escutar nas portas 443 (HTTPS) e 80 (HTTP), siga estas etapas:
     </para>
     <note>
      <para>
       Os comandos no procedimento usam domínio e zona <literal>default</literal>.
      </para>
     </note>
     <procedure>
      <step>
       <para>
        Forneça um arquivo de especificação para implantar o Gateway de Objetos. Consulte a <xref linkend="deploy-cephadm-day2-service-ogw"/> para obter mais detalhes sobre a especificação do Gateway de Objetos. Utilize o seguinte comando:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        Se os certificados SSL não forem fornecidos no arquivo de especificação, adicione-os usando o seguinte comando:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        Mude o valor padrão da opção <option>rgw_frontends</option>:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      
      <step>
       <para>
        Remova a configuração específica criada pelo cephadm. Identifique para qual destino a opção <option>rgw_frontends</option> foi configurada executando o comando:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config dump | grep rgw</screen>
       <para>
        Por exemplo, o destino é <literal>client.rgw.default.default.node4.yiewdu</literal>. Remova o valor específico atual de <option>rgw_frontends</option>:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         Em vez de remover um valor de <option>rgw_frontends</option>, você pode especificá-lo. Por exemplo:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      
      <step>
       <para>
        Reinicie os Gateways de Objetos:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Implantação com um subcluster</title>
    <para>
     Os <emphasis>subclusters</emphasis> ajudam a organizar os nós nos clusters para isolar as cargas de trabalho e facilitar a expansão elástica. Se a implantação for com um subcluster, use a seguinte configuração:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Implantando Gateways iSCSI</title>
   <para>
    O cephadm implanta um Gateway iSCSI, que é um protocolo SAN (Storage Area Network) que permite aos clientes (denominados iniciadores) enviar comandos SCSI para dispositivos de armazenamento SCSI (destinos) em servidores remotos.
   </para>
   <para>
    Use a seguinte configuração para a implantação. Verifique se <literal>trusted_ip_list</literal> contém os endereços IP de todos os nós do Gateway iSCSI e do Ceph Manager (consulte o exemplo de saída abaixo).
   </para>
   <note>
    <para>
     Verifique se o pool foi criado antes de aplicar a especificação a seguir.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Verifique se os IPs listados em <literal>trusted_ip_list</literal> <emphasis>não</emphasis> têm espaço após a separação por vírgula.
    </para>
   </note>
   <sect3>
    <title>Configuração SSL segura</title>
    <para>
     Para usar uma conexão SSL segura entre o Ceph Dashboard e a API de destino iSCSI, você precisa de um par de arquivos de chave e certificado SSL válidos. Eles podem ser emitidos por CA ou autoassinados (consulte o <xref linkend="self-sign-certificates"/>). Para habilitar o SSL, inclua a configuração <literal>api_secure: true</literal> no arquivo de especificação:
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     Para especificar a chave e o certificado SSL, você pode colar o conteúdo diretamente no arquivo de especificação YAML. O sinal de barra vertical (<literal>|</literal>) no final da linha informa ao analisador para esperar um valor de string de várias linhas. Por exemplo:
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Implantando o NFS Ganesha</title>
    
<important>
 <para>
  O NFS Ganesha suporta o NFS versão 4.1 e mais recente. Ele não suporta o NFS versão 3.
 </para>
</important>

    <para>
    O cephadm implanta o NFS Ganesha usando um pool RADOS predefinido e um namespace opcional. Para implantar o NFS Ganesha, use a seguinte especificação:
   </para>
   <note>
    <para>
     Você precisa ter um pool RADOS predefinido; do contrário, haverá falha na operação <command>ceph orch apply</command>. Para obter mais informações sobre como criar um pool, consulte o <xref linkend="ceph-pools-operate-add-pool"/>.
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NFS</replaceable> com uma string arbitrária que identifica a exportação do NFS.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_POOL</replaceable> com o nome do pool em que o objeto de configuração RADOS do NFS Ganesha será armazenado.
     </para>
    </listitem>
    <listitem>
     <para>
      <replaceable>EXAMPLE_NAMESPACE</replaceable> (opcional) com o namespace desejado do NFS do Gateway de Objetos (por exemplo, <literal>ganesha</literal>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Implantação <systemitem class="daemon">rbd-mirror</systemitem></title>
   <para>
    O serviço <systemitem class="daemon">rbd-mirror</systemitem> se encarrega de sincronizar as imagens do Dispositivo de Blocos RADOS entre dois clusters do Ceph (para obter mais detalhes, consulte o <xref linkend="ceph-rbd-mirror"/>). Para implantar o <systemitem class="daemon">rbd-mirror</systemitem>, use a seguinte especificação:
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Implantando a pilha de monitoramento</title>
   <para>
    A pilha de monitoramento consiste no Prometheus, nos exportadores do Prometheus, no Alertmanager do Prometheus e no Grafana. O Ceph Dashboard usa esses componentes para armazenar e visualizar as métricas detalhadas sobre o uso e o desempenho do cluster.
   </para>
   <tip>
    <para>
     Se a implantação exigir imagens de container personalizadas ou exibidas localmente dos serviços de pilha de monitoramento, consulte o <xref linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    Para implantar a pilha de monitoramento, siga estas etapas:
   </para>
   <procedure>
    <step>
     <para>
      Habilite o módulo <literal>prometheus</literal> no daemon do Ceph Manager. Esse procedimento expõe as métricas internas do Ceph para que o Prometheus possa lê-las:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Verifique se esse comando foi executado antes da implantação do Prometheus. Se o comando não foi executado antes da implantação, você deve reimplantar o Prometheus para atualizar a configuração do Prometheus:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Crie um arquivo de especificação (por exemplo, <filename>monitoramento.yaml</filename>) com um conteúdo semelhante ao seguinte:
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Aplique os serviços de monitoramento executando este comando:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i monitoring.yaml</screen>
     <para>
      Pode levar um ou dois minutos para que os serviços de monitoramento sejam implantados.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     O Prometheus, o Grafana e o Ceph Dashboard são todos configurados automaticamente para se comunicarem entre si, resultando em uma integração totalmente funcional do Grafana no Ceph Dashboard, quando implantados conforme descrito acima.
    </para>
    <para>
     A única exceção a essa regra é o monitoramento com imagens RBD. Consulte o <xref linkend="monitoring-rbd-image"/> para obter mais informações.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
