<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha-storage-about">
 <title>SES e Ceph</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sim</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  O SUSE Enterprise Storage é um sistema de armazenamento distribuído projetado para escalabilidade, confiabilidade e desempenho que usa a tecnologia Ceph. É possível executar um cluster do Ceph em servidores convencionais em uma rede comum, como Ethernet. O cluster tem capacidade de dimensionamento para milhares de servidores (posteriormente mencionados como nós) e dentro da faixa de petabytes. Ao contrário dos sistemas convencionais que têm tabelas de alocação para armazenar e buscar dados, o Ceph usa um algoritmo determinístico para alocar armazenamento de dados e não tem nenhuma estrutura centralizada de informações. Nos clusters de armazenamento, o Ceph considera a adição ou remoção de hardware a regra, e não a exceção. O cluster do Ceph automatiza as tarefas de gerenciamento, como distribuição e redistribuição de dados, replicação de dados, detecção de falhas e recuperação. O Ceph é autorreparável e autogerenciável, o que resulta na redução de overhead administrativo e orçamentário.
 </para>
 <para>
  Este capítulo apresenta uma visão geral resumida do SUSE Enterprise Storage 7 e descreve brevemente os componentes mais importantes.
 </para>
 <sect1 xml:id="storage-intro-features">
  <title>Recursos do Ceph</title>

  <para>
   O ambiente do Ceph tem os seguintes recursos:
  </para>

  <variablelist>
   <varlistentry>
    <term>Escalabilidade</term>
    <listitem>
     <para>
      O Ceph pode ser dimensionado para milhares de nós e pode gerenciar o armazenamento na faixa de petabytes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hardware convencional</term>
    <listitem>
     <para>
      Não há necessidade de hardware especial para executar um cluster do Ceph. Para obter os detalhes, consulte o <xref linkend="storage-bp-hwreq"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Autogerenciável</term>
    <listitem>
     <para>
      O cluster do Ceph é autogerenciável. Quando os nós são adicionados, removidos ou falham, o cluster redistribui os dados automaticamente. Ele também reconhece discos sobrecarregados.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Sem Ponto Único de Falha</term>
    <listitem>
     <para>
      Nenhum nó em um cluster armazena informações importantes separadamente. É possível configurar o número de redundâncias.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Software de código-fonte aberto</term>
    <listitem>
     <para>
      O Ceph é uma solução de software de código-fonte aberto e independente de hardware ou de fornecedores específicos.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-intro-core">
  <title>Componentes básicos do Ceph</title>

  <para>
   Para aproveitar todas as vantagens do Ceph, é necessário conhecer alguns dos componentes e conceitos básicos. Esta seção apresenta algumas partes do Ceph que são mencionadas com mais frequência em outros capítulos.
  </para>

  <sect2 xml:id="storage-intro-core-rados">
   <title>RADOS</title>
   <para>
    O componente básico de Ceph é denominado <emphasis>RADOS</emphasis>
    <emphasis> (Reliable Autonomic Distributed Object Store)</emphasis>. Ele é responsável por gerenciar os dados armazenados no cluster. Normalmente, os dados no Ceph são armazenados como objetos. Cada objeto consiste em um identificador e nos dados.
   </para>
   <para>
    O RADOS oferece os seguintes métodos de acesso aos objetos armazenados que envolvem diversos casos de uso:
   </para>
   <variablelist>
    <varlistentry>
     <term>Gateway de Objetos</term>
     <listitem>
      <para>
       O Gateway de Objetos é um gateway HTTP REST para armazenamento de objetos RADOS. Ele permite acesso direto aos objetos armazenados no cluster do Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Dispositivo de blocos RADOS</term>
     <listitem>
      <para>
       É possível acessar o Dispositivo de Blocos RADOS (RBD, RADOS Block Device) como qualquer outro dispositivo de blocos. Eles podem ser usados em combinação com o <systemitem class="library">libvirt</systemitem> para fins de virtualização, por exemplo.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       O Sistema de Arquivos Ceph é compatível com POSIX.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem></term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> é uma biblioteca que pode ser usada com várias linguagens de programação para criar um aplicativo capaz de interagir diretamente com o cluster de armazenamento.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    O Gateway de Objetos e o RBD usam essa <systemitem class="library">biblioteca</systemitem>, enquanto o CephFS estabelece interface direta com o RADOS. <xref linkend="storage-intro-core-rados-figure"/>
   </para>
   <figure xml:id="storage-intro-core-rados-figure">
    <title>Interfaces com o armazenamento de objetos do Ceph</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage-intro-core-crush">
   <title>CRUSH</title>
   <para>
    No centro de um cluster do Ceph está o algoritmo <emphasis>CRUSH</emphasis>. CRUSH é o acrônimo de <emphasis>Controlled Replication Under Scalable Hashing</emphasis>. Trata-se de uma função que processa a alocação de armazenamento e precisa de poucos parâmetros equivalentes. Isso significa que apenas uma pequena quantidade de informações é necessária para calcular a posição de armazenamento de um objeto. Os parâmetros são um mapa atual do cluster, incluindo o estado de saúde, algumas regras de posicionamento definidas pelo administrador e o nome do objeto que precisa ser armazenado ou recuperado. Com essas informações, todos os nós no cluster do Ceph são capazes de calcular onde um objeto e suas réplicas serão armazenados. Isso dificulta a gravação e a leitura de dados muito eficientes. O CRUSH tenta distribuir igualmente os dados a todos os nós do cluster.
   </para>
   <para>
    O <emphasis>Mapa CRUSH</emphasis> contém todos os nós de armazenamento e as regras de posicionamento definidas pelo administrador para armazenar objetos no cluster. Ele define uma estrutura hierárquica que costuma corresponder à estrutura física do cluster. Por exemplo, os discos com dados estão em hosts, os hosts estão em racks, os racks estão em linhas e as linhas estão em data centers. É possível usar essa estrutura para definir <emphasis>domínios de falha</emphasis>. Em seguida, o Ceph garante que as replicações sejam armazenadas em ramificações diferentes de um domínio de falha específico.
   </para>
   <para>
    Se o domínio de falha for definido como rack, as replicações dos objetos serão distribuídas para racks diferentes. Isso pode reduzir as interrupções causadas por um switch com falha em um rack. Se uma unidade de distribuição de energia fornece uma linha de racks, o domínio de falha pode ser definido como linha. Quando a unidade de distribuição de energia falha, os dados replicados ainda ficam disponíveis em outras linhas.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-core-nodes">
   <title>Nós e daemons do Ceph</title>
   <para>
    No Ceph, os nós são servidores que trabalham para o cluster. Eles podem executar vários tipos de daemons. Recomendamos executar apenas um tipo de daemon em cada nó, exceto os daemons do Ceph Manager, que podem ser combinados com Ceph Monitors. Cada cluster requer pelo menos os daemons do Ceph Monitor, Ceph Manager e Ceph OSD:
   </para>
   <variablelist>
    <varlistentry>
     <term>Nó de Admin</term>
     <listitem>
      <para>
       O <emphasis>Nó de Admin</emphasis> é um nó de cluster do Ceph do qual você executa comandos para gerenciar o cluster. O Nó de Admin é um ponto central do cluster do Ceph porque ele gerencia o restante dos nós do cluster, consultando e instruindo os serviços do Minion Salt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       Os nós do <emphasis>Ceph Monitor</emphasis> (geralmente abreviado como <emphasis>MON</emphasis>) mantêm informações sobre o estado de saúde do cluster, um mapa de todos os nós e as regras de distribuição de dados (consulte a <xref linkend="storage-intro-core-crush"/>).
      </para>
      <para>
       Em caso de falhas ou conflitos, os nós do Ceph Monitor no cluster decidem, por maioria, quais informações estão corretas. Para formar uma maioria qualificada, é recomendável ter um número ímpar de nós do Ceph Monitor e, pelo menos, três deles.
      </para>
      <para>
       Se for usado mais de um site, os nós do Ceph Monitor deverão ser distribuídos para um número ímpar de sites. O número de nós do Ceph Monitor por site deve ser suficiente para que mais do que 50% dos nós do Ceph Monitor continuem funcionando em caso de falha de um site.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       O Ceph Manager coleta as informações de estado do cluster inteiro. O daemon do Ceph Manager é executado com os daemons do Ceph Monitor. Ele fornece monitoramento adicional e estabelece interface com os sistemas externos de monitoramento e gerenciamento. Ele também inclui outros serviços. Por exemplo, a IU da Web do Ceph Dashboard é executada no mesmo nó que o Ceph Manager.
      </para>
      <para>
       O Ceph Manager não requer configuração adicional, além de garantir que esteja em execução.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       O <emphasis>Ceph OSD</emphasis> é um daemon que processa <emphasis>Dispositivos de Armazenamento de Objetos</emphasis>, que são unidades físicas ou lógicas de armazenamento (discos rígidos ou partições). Os Dispositivos de Armazenamento de Objetos podem ser discos/partições físicos ou volumes lógicos. O daemon também se encarrega da replicação e redistribuição de dados em caso de nós adicionados ou removidos.
      </para>
      <para>
       Os daemons Ceph OSD comunicam-se com os daemons do monitor e informam a eles o estado dos outros daemons OSD.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para usar o CephFS, o Gateway de Objetos, o NFS Ganesha ou o iSCSI Gateway, são necessários outros nós:
   </para>
   <variablelist>
    <varlistentry>
     <term>MDS (Metadata Server – Servidor de Metadados)</term>
     <listitem>
      <para>
       Os metadados do CephFS são armazenados em seu próprio pool RADOS (consulte a <xref linkend="storage-intro-structure-pool"/>). Os Servidores de Metadados atuam como uma camada de cache inteligente para os metadados e serializam o acesso quando necessário. Isso permite acesso simultâneo de vários clientes sem sincronização explícita.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Gateway de Objetos</term>
     <listitem>
      <para>
       O Gateway de Objetos é um gateway HTTP REST para armazenamento de objetos RADOS. Ele é compatível com o OpenStack Swift e o Amazon S3 e tem seu próprio gerenciamento de usuários.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       O NFS Ganesha concede acesso de NFS ao Gateway de Objetos ou CephFS. Ele é executado no espaço do usuário, em vez do kernel, e interage diretamente com o Gateway de Objetos ou o CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI Gateway</term>
     <listitem>
      <para>
       iSCSI é um protocolo de rede de armazenamento que permite aos clientes enviar comandos SCSI para dispositivos de armazenamento SCSI (destinos) em servidores remotos.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Gateway do Samba</term>
     <listitem>
      <para>
       O Gateway do Samba concede ao Samba acesso aos dados armazenados no CephFS.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-intro-structure">
  <title>Estrutura de armazenamento do Ceph</title>

  <sect2 xml:id="storage-intro-structure-pool">
   <title>Pools</title>
   <para>
    Os objetos armazenados em um cluster do Ceph são agrupados em <emphasis>pools</emphasis>. Os pools representam as partições lógicas do cluster para o mundo externo. É possível definir um conjunto de regras para cada pool. Por exemplo, o número necessário de replicações de cada objeto. A configuração padrão dos pools chama-se <emphasis>pool replicado</emphasis>.
   </para>
   <para>
    Normalmente, os pools contêm objetos, mas também podem ser configurados para agir como um RAID 5. Nessa configuração, os objetos são armazenados em pacotes com outros pacotes de codificação. Os pacotes de codificação contêm as informações redundantes. O número de dados e de pacotes de codificação pode ser definido pelo administrador. Nessa configuração, os pools são denominados <emphasis>pools codificados para eliminação</emphasis> ou <emphasis>pools EC</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-pg">
   <title>Grupos de posicionamento</title>
   <para>
    Os PGs (<emphasis>Placement Groups – Grupos de Posicionamento</emphasis>) são usados para distribuição de dados em um pool. Ao criar um pool, determinado número de grupos de posicionamento é definido. Os grupos de posicionamento são usados internamente para agrupar objetos e são um fator importante para o desempenho de um cluster do Ceph. O PG de um objeto é determinado pelo nome do objeto.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-example">
   <title>Exemplo</title>
   <para>
    Esta seção mostra um exemplo simplificado de como o Ceph gerencia os dados (consulte a <xref linkend="storage-intro-structure-example-figure"/>). Esse exemplo não representa uma configuração recomendada para um cluster do Ceph. A configuração de hardware consiste em três nós de armazenamento ou Ceph OSDs (<literal>Host 1</literal>, <literal>Host 2</literal>, <literal>Host 3</literal>). Cada nó tem três discos rígidos que são usados como OSDs (<literal>osd.1</literal> a <literal>osd.9</literal>). Os nós do Ceph Monitor são ignorados neste exemplo.
   </para>
   <note>
    <title>Diferença entre Ceph OSD e OSD</title>
    <para>
     Enquanto <emphasis>Ceph OSD</emphasis> ou <emphasis>daemon Ceph OSD</emphasis> refere-se a um daemon executado em um nó, a palavra <emphasis>OSD</emphasis> refere-se ao disco lógico com o qual o daemon interage.
    </para>
   </note>
   <para>
    O cluster tem dois pools: <literal>Pool A</literal> e <literal>Pool B</literal>. Enquanto o Pool A replica os objetos apenas duas vezes, a resiliência do Pool B é mais importante e efetua três replicações de cada objeto.
   </para>
   <para>
    Quando um aplicativo coloca um objeto em um pool (por exemplo, pela API REST), um Grupo de Posicionamento (<literal>PG1</literal> a <literal>PG4</literal>) é selecionado com base no nome do pool e do objeto. Em seguida, o algoritmo CRUSH calcula em quais OSDs o objeto é armazenado, com base no Grupo de Posicionamento que contém o objeto.
   </para>
   <para>
    Neste exemplo, o domínio de falha foi definido como host. Isso garante que as replicações dos objetos sejam armazenadas em hosts diferentes. Dependendo do nível de replicação definido para um pool, o objeto é armazenado em dois ou três OSDs, que são usados pelo Grupo de Posicionamento.
   </para>
   <para>
    Um aplicativo que grava um objeto interage apenas com um Ceph OSD: o principal. O Ceph OSD principal efetua a replicação e confirma a conclusão do processo de gravação depois que todos os outros OSDs armazenaram o objeto.
   </para>
   <para>
    Em caso de falha no <literal>osd.5</literal>, todos os objetos no <literal>PG1</literal> ainda estarão disponíveis no <literal>osd.1</literal>. Logo que o cluster reconhece a falha em um OSD, outro OSD entra em ação. Neste exemplo, o <literal>osd.4</literal> é usado como substituto do <literal>osd.5</literal>. Os objetos armazenados no <literal>osd.1</literal> são replicados para o <literal>osd.4</literal> para restaurar o nível de replicação.
   </para>
   <figure xml:id="storage-intro-structure-example-figure">
    <title>Exemplo de Ceph de pouco dimensionamento</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Se um novo nó com novos OSDs for adicionado ao cluster, o mapa do cluster será modificado. Na sequência, a função CRUSH retorna locais diferentes para os objetos. Os objetos que recebem os novos locais serão realocados. Esse processo resulta no uso equilibrado de todos os OSDs.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about-bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore é um novo back end de armazenamento padrão para Ceph a partir do SES 5. Ele apresenta melhor desempenho do que o FileStore, checksum completo de dados e compactação incorporada.
  </para>

  <para>
   O BlueStore gerencia um, dois ou três dispositivos de armazenamento. No caso mais simples, o BlueStore consome um único dispositivo de armazenamento principal. Normalmente, o dispositivo de armazenamento é particionado em duas partes:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Uma pequena partição denominada BlueFS que implementa funcionalidades do tipo do sistema de arquivos que o RocksDB exige.
    </para>
   </listitem>
   <listitem>
    <para>
     Normalmente, o restante do dispositivo é ocupado por uma partição grande. Ele é gerenciado diretamente pelo BlueStore e contém todos os dados reais. Normalmente, esse dispositivo principal é identificado por um link simbólico de bloco no diretório de dados.
    </para>
   </listitem>
  </orderedlist>

  <para>
   É possível também implantar o BlueStore em dois dispositivos adicionais:
  </para>

  <para>
   É possível usar o <emphasis>dispositivo WAL</emphasis> para o diário interno ou o registro write-ahead do BlueStore. Ele é identificado pelo link simbólico <literal>block.wal</literal> no diretório de dados. O uso de um dispositivo WAL separado será útil apenas se ele for mais rápido do que o dispositivo principal ou o dispositivo de BD. Por exemplo, quando:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     O dispositivo WAL é um NVMe, o dispositivo de BD é uma SSD e o dispositivo de dados é uma SSD ou HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Ambos os dispositivos WAL e de BD são SSDs separadas, e o dispositivo de dados é uma SSD ou HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   É possível usar um <emphasis>dispositivo de BD</emphasis> para armazenar metadados internos do BlueStore. O BlueStore (ou, em vez dele, o RocksDB incorporado) colocará o máximo de metadados que puder no dispositivo de BD para melhorar o desempenho. Mais uma vez, apenas será útil provisionar um dispositivo de BD compartilhado se ele for mais rápido do que o dispositivo principal.
  </para>

  <tip>
   <title>Planejar o tamanho do BD</title>
   <para>
    Planeje com cuidado para garantir o tamanho suficiente do dispositivo de BD. Se o dispositivo de BD ficar cheio, os metadados serão despejados no dispositivo principal, o que prejudica bastante o desempenho do OSD.
   </para>
   <para>
    Você pode verificar se uma partição WAL/BD está ficando lotada e sendo despejada ao executar o comando <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command>. O valor <option>slow_used_bytes</option> mostra a quantidade de dados que está sendo despejada:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage-moreinfo">
  <title>Informações adicionais</title>

  <itemizedlist>
   <listitem>
    <para>
     Como um projeto comunitário, o Ceph tem sua própria documentação online completa. Para os tópicos não encontrados neste manual, visite <link xlink:href="https://docs.ceph.com/en/octopus/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     A publicação original <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> por <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> apresenta informações úteis sobre o funcionamento interno do Ceph. Principalmente na implantação de clusters em grande escala, trata-se de uma leitura recomendada. Você encontra essa publicação em <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     É possível usar o SUSE Enterprise Storage com distribuições que não são do SUSE OpenStack. Os clientes Ceph precisam estar em um nível que seja compatível com o SUSE Enterprise Storage.
    </para>
    <note>
     <para>
      A SUSE suporta o componente de servidor da implantação do Ceph, e o cliente é suportado pelo fornecedor de distribuição do OpenStack.
     </para>
    </note>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
