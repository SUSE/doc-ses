<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>Dispositivo de blocos RADOS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Um bloco é uma sequência de bytes. Por exemplo, um bloco de dados de 4 MB. As interfaces de armazenamento com base em blocos são a maneira mais comum para armazenar dados com mídia rotativa, como discos rígidos, CDs e disquetes. A onipresença de interfaces de dispositivo de blocos faz do dispositivo de blocos virtual o candidato ideal para interagir com um sistema de armazenamento de dados em massa, como o Ceph.
 </para>
 <para>
  Os dispositivos de blocos do Ceph permitem o compartilhamento de recursos físicos e são redimensionáveis. Eles armazenam dados distribuídos por vários OSDs em um cluster do Ceph. Os dispositivos de blocos do Ceph aproveitam os recursos do RADOS, como criação de instantâneos, replicação e consistência. Os Dispositivo de Blocos RADOS (RBD) do Ceph interagem com os OSDs usando os módulos do kernel ou a biblioteca <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocolo RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Os dispositivos de blocos do Ceph oferecem alto desempenho com escalabilidade infinita aos módulos do kernel. Eles suportam soluções de virtualização, como QEMU, ou sistemas de computação com base em nuvem, como OpenStack, que utilizam a <systemitem class="library">libvirt</systemitem>. Você pode usar o mesmo cluster para operar o Gateway de Objetos, o CephFS e os Dispositivos de Blocos RADOS simultaneamente.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Comandos do dispositivo de blocos</title>

  <para>
   O comando <command>rbd</command> permite criar, listar, avaliar e remover imagens de dispositivo de blocos. Você também pode usá-lo, por exemplo, para clonar imagens, criar instantâneos, voltar uma imagem para um instantâneo ou ver um instantâneo.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Criando uma imagem de dispositivo de blocos em um pool replicado</title>
   <para>
    Antes que você possa adicionar um dispositivo de blocos a um cliente, precisa criar uma imagem relacionada em um pool existente (consulte o <xref linkend="ceph-pools"/>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Por exemplo, para criar uma imagem de 1 GB denominada “myimage” que armazena informações em um pool chamado “mypool”, execute o seguinte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>Unidades de tamanho de imagem</title>
    <para>
     Se você omitir um atalho de unidade de tamanho (“G” ou “T”), o tamanho da imagem será em megabytes. Use “G” ou “T” após o número do tamanho para especificar gigabytes ou terabytes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Criando uma imagem de dispositivo de blocos em um pool codificado para eliminação</title>
   <para>
    É possível armazenar dados de uma imagem de dispositivo de blocos diretamente em pools codificados para eliminação (EC, Erasure Coded). A imagem de um Dispositivo de Blocos RADOS consiste nas partes de <emphasis>dados</emphasis> e <emphasis>metadados</emphasis>. Você pode armazenar apenas a parte de dados de uma imagem de Dispositivo de Blocos RADOS em um pool EC. O pool precisa ter o flag <option>overwrite</option> definido como <emphasis>true</emphasis>, e isso apenas será possível se todos os OSDs em que o pool está armazenado usarem o BlueStore.
   </para>
   <para>
    Você não pode armazenar a parte de metadados da imagem em um pool EC. Você pode especificar o pool replicado para armazenar os metadados da imagem com a opção <option>--pool=</option> do comando <command>rbd create</command> ou especificar <option>pool/</option> como prefixo para o nome da imagem.
   </para>
   <para>
    Crie um pool EC:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Especifique o pool replicado para armazenar os metadados:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    Ou:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Listando imagens de dispositivo de blocos</title>
   <para>
    Para listar os dispositivos de blocos em um pool chamado “mypool”, execute o seguinte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Recuperando informações da imagem</title>
   <para>
    Para recuperar as informações de uma imagem “myimage” em um pool chamado “mypool”, execute o seguinte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Redimensionando uma imagem de dispositivo de blocos</title>
   <para>
    As imagens de Dispositivo de Blocos RADOS são aprovisionadas dinamicamente, elas não usam nenhum armazenamento físico até você começar a gravar dados nelas. No entanto, elas têm uma capacidade máxima que você define com a opção <option>--size</option>. Para aumentar (ou diminuir) o tamanho máximo da imagem, execute o seguinte:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Removendo uma imagem de dispositivo de blocos</title>
   <para>
    Para remover um dispositivo de blocos correspondente a uma imagem “myimage” no pool chamado “mypool”, execute o seguinte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montando e desmontando</title>

  <para>
   Após criar um Dispositivo de Blocos RADOS, você poderá usá-lo como qualquer outro dispositivo de disco: formatá-lo, montá-lo para poder trocar arquivos e desmontá-lo depois de concluído.
  </para>

  <para>
   O comando <command>rbd</command> usa como padrão o acesso ao cluster por meio da conta do usuário <literal>admin</literal> do Ceph. Essa conta tem acesso administrativo completo ao cluster. Esse comportamento cria o risco de danos acidentais, similar ao login em uma estação de trabalho Linux como <systemitem class="username">root</systemitem>. Portanto, é preferível criar contas dos usuários com menos privilégios e usá-las para acesso normal de leitura/gravação ao Dispositivo de Blocos RADOS.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Criando uma conta do usuário do Ceph</title>
   <para>
    Para criar uma nova conta do usuário com os recursos Ceph Manager, Ceph Monitor e Ceph OSD, use o comando <command>ceph</command> com o subcomando <command>auth get-or-create</command>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Por exemplo, para criar um usuário chamado <replaceable>qemu</replaceable> com acesso de leitura/gravação ao pool <replaceable>vms</replaceable> e acesso apenas leitura ao pool <replaceable>images</replaceable>, execute o seguinte:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    A saída do comando <command>ceph auth get-or-create</command> será o chaveiro do usuário especificado, que poderá gravar em <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     Ao usar o comando <command>rbd</command>, você pode especificar o ID de usuário inserindo o argumento <command>--id</command>
     <replaceable>ID</replaceable> opcional.
    </para>
   </note>
   <para>
    Para obter mais detalhes sobre o gerenciamento de contas dos usuários do Ceph, consulte o <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Autenticação de usuário</title>
   <para>
    Para especificar um nome de usuário, utilize <option>--id <replaceable>user-name</replaceable></option>. Se você usa a autenticação do <systemitem>cephx</systemitem>, também precisa especificar um segredo. Ele pode vir de um chaveiro ou de um arquivo que contém o segredo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    ou
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparando um Dispositivo de Blocos RADOS para uso</title>
   <procedure>
    <step>
     <para>
      Verifique se o cluster do Ceph inclui um pool com a imagem do disco que você deseja mapear. Considere o pool chamado <literal>mypool</literal> e a imagem <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Mapeie a imagem para um novo dispositivo de blocos:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Liste todos os dispositivos mapeados:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      O dispositivo no qual desejamos trabalhar é <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>Caminho do dispositivo RBD</title>
      <para>
       Em vez do <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>, você pode usar <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename> como o caminho de um dispositivo persistente. Por exemplo:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Crie um sistema de arquivos XFS no dispositivo <filename>/dev/rbd0:</filename>
     </para>
<screen><prompt role="root"># </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Substitua <filename>/mnt</filename> por seu ponto de montagem, monte o dispositivo e verifique se ele foi montado corretamente:
     </para>
<screen><prompt role="root"># </prompt>mount /dev/rbd0 /mnt
      <prompt role="root"># </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Agora, você pode mover os dados de e para o dispositivo como se ele fosse um diretório local.
     </para>
     <tip>
      <title>Aumentando o tamanho do dispositivo RBD</title>
      <para>
       Se você acha que o tamanho do dispositivo RBD não é mais suficiente, pode aumentá-lo com facilidade.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Aumente o tamanho da imagem RBD. Por exemplo, até 10 GB.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Expanda o sistema de arquivos para preencher o novo tamanho do dispositivo:
        </para>
<screen><prompt role="root"># </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Após terminar de acessar o dispositivo, você poderá anular o mapeamento e desmontá-lo.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root"># </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>Montagem e desmontagem manuais</title>
    <para>
     Um script <command>rbdmap</command> e a unidade <systemitem class="daemon">systemd</systemitem> são fornecidos para facilitar o processo de mapeamento e montagem de RBDs após a inicialização e de desmontagem deles antes do encerramento. Consulte a <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command> Mapear dispositivos RBD no momento da inicialização</title>
   <para>
    <command>rbdmap</command> é um script de shell que automatiza as operações <command>rbd map</command> e <command>rbd device unmap</command> em uma ou mais imagens RBD. Embora você possa executar o script manualmente a qualquer momento, a principal vantagem é mapear e montar automaticamente as imagens RBD no momento da inicialização (e desmontar e anular o mapeamento no encerramento), conforme acionado pelo sistema Init. Um arquivo da unidade <systemitem class="daemon">systemd</systemitem>, <filename>rbdmap.service</filename>, está incluído no pacote <systemitem>ceph-common</systemitem> para essa finalidade.
   </para>
   <para>
    O script aplica um único argumento, que pode ser <option>map</option> ou <option>unmap</option>. Em qualquer um dos casos, o script analisa um arquivo de configuração. Ele assume como padrão <filename>/etc/ceph/rbdmap</filename>, mas pode ser anulado por meio de uma variável de ambiente <literal>RBDMAPFILE</literal>. Cada linha do arquivo de configuração corresponde a uma imagem RBD que será mapeada ou que terá o mapeamento anulado.
   </para>
   <para>
    O arquivo de configuração tem o seguinte formato:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Caminho para uma imagem em um pool. Especifique como <replaceable>nome_do_pool</replaceable>/<replaceable>nome_da_imagem</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Uma lista opcional de parâmetros a serem passados para o comando <command>rbd device map</command> de base. Esses parâmetros e seus valores devem ser especificados como uma string separada por vírgula. Por exemplo:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       O exemplo faz com que o script <command>rbdmap</command> execute o seguinte comando:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       No exemplo a seguir, você pode ver como especificar um nome de usuário e um chaveiro com um segredo correspondente:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Quando executado como <command>rbdmap map</command>, o script analisa o arquivo de configuração e, para cada imagem RBD especificada, ele tenta primeiro mapear a imagem (usando o comando <command>rbd device map</command>) e, na sequência, montar a imagem.
   </para>
   <para>
    Quando executado como <command>rbdmap unmap</command>, as imagens listadas no arquivo de configuração serão desmontadas e o mapeamento delas será anulado.
   </para>
   <para>
    <command>rbdmap unmap-all</command> tenta desmontar e, na sequência, anular o mapeamento de todas as imagens RBD mapeadas, independentemente de estarem listadas no arquivo de configuração.
   </para>
   <para>
    Se bem-sucedida, a operação <command>rbd device map</command> mapeia a imagem para um dispositivo <filename>/dev/rbdX</filename> e, nesse ponto, uma regra udev é acionada para criar um link simbólico do nome do dispositivo amigável <filename>/dev/rbd/<replaceable>nome_do_pool</replaceable>/<replaceable>nome_da_imagem</replaceable></filename> apontando para o dispositivo real mapeado.
   </para>
   <para>
    Para que a montagem e a desmontagem sejam bem-sucedidas, o nome “amigável” do dispositivo precisa ter uma entrada correspondente em <filename>/etc/fstab</filename>. Ao gravar entradas <filename>/etc/fstab</filename> em imagens RBD, especifique a opção de montagem “noauto” (ou “nofail”). Isso impede que o sistema Init tente montar o dispositivo com muita antecedência, antes mesmo de ele existir, pois <filename>rbdmap.service</filename> é normalmente acionado mais adiante na sequência de boot.
   </para>
   <para>
    Para obter uma lista de opções <command>rbd</command>, consulte a página de manual do <command>rbd</command> (<command>man 8 rbd</command>).
   </para>
   <para>
    Para ver exemplos de uso do <command>rbdmap</command>, consulte a página de manual do <command>rbdmap</command> (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Aumentando o tamanho dos dispositivos RBD</title>
   <para>
    Se você acha que o tamanho do dispositivo RBD não é mais suficiente, pode aumentá-lo com facilidade.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Aumente o tamanho da imagem RBD. Por exemplo, até 10 GB.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Expanda o sistema de arquivos para preencher o novo tamanho do dispositivo.
     </para>
<screen><prompt role="root"># </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Instantâneos</title>

  <para>
   Um instantâneo RBD é aquele de uma imagem do Dispositivo de Blocos RADOS. Com os instantâneos, você pode manter o histórico de estado da imagem. O Ceph também suporta camadas de instantâneo, o que permite clonar imagens de VM de forma rápida e fácil. O Ceph suporta instantâneos de dispositivo de blocos usando o comando <command>rbd</command> e muitas interfaces de nível mais alto, incluindo QEMU, <systemitem>libvirt</systemitem>, OpenStack e CloudStack.
  </para>

  <note>
   <para>
    Pare as operações de entrada e saída e descarregue todas as gravações pendentes antes de criar o instantâneo de uma imagem. Se a imagem tiver um sistema de arquivos, o estado dele deverá ser consistente no momento da criação do instantâneo.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Habilitando e configurando o <systemitem>cephx</systemitem></title>
   <para>
    Quando o <systemitem>cephx</systemitem> está habilitado, você deve especificar um nome de usuário ou ID e um caminho para o chaveiro que contém a chave correspondente para o usuário. Consulte o <xref linkend="cha-storage-cephx"/> para obter mais detalhes. É possível também adicionar a variável de ambiente <systemitem>CEPH_ARGS</systemitem> para evitar uma nova entrada dos parâmetros a seguir.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Por exemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Adicione o usuário e o segredo à variável de ambiente <systemitem>CEPH_ARGS</systemitem> para que você não precise digitá-los toda vez.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Aspectos básicos do instantâneo</title>
   <para>
    Os procedimentos a seguir demonstram como criar, listar e remover instantâneos usando o comando <command>rbd</command> na linha de comando.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Criando instantâneos</title>
    <para>
     Para criar um instantâneo com <command>rbd</command>, especifique a opção <option>snap create</option>, o nome do pool e o nome da imagem.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Listando instantâneos</title>
    <para>
     Para listar os instantâneos de uma imagem, especifique o nome do pool e o nome da imagem.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Revertendo instantâneos</title>
    <para>
     Para voltar a um instantâneo com <command>rbd</command>, especifique a opção <option>snap rollback</option>, o nome do pool, o nome da imagem e o nome do instantâneo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Voltar uma imagem para um instantâneo significa sobregravar a versão atual da imagem com os dados de um instantâneo. O tempo necessário para executar um rollback aumenta de acordo com o tamanho da imagem. É <emphasis>mais rápido clonar</emphasis> de um instantâneo <emphasis>do que voltar</emphasis> uma imagem para um instantâneo, e é o método preferencial para reverter a um estado preexistente.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Apagando um instantâneo</title>
    <para>
     Para apagar um instantâneo com <command>rbd</command>, especifique a opção <option>snap rm</option>, o nome do pool, o nome da imagem e o nome de usuário.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Os Ceph OSDs apagam dados de forma assíncrona, portanto, apagar um instantâneo não libera o espaço em disco imediatamente.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Purgando instantâneos</title>
    <para>
     Para apagar todos os instantâneos de uma imagem com <command>rbd</command>, especifique a opção <option>snap purge</option> e o nome da imagem.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Camadas de instantâneo</title>
   <para>
    O Ceph permite criar vários clones de cópia em gravação (COW, Copy-On-Write) de um instantâneo de dispositivo de blocos. As camadas de instantâneo permitem que os clientes de dispositivo de blocos do Ceph criem imagens muito rapidamente. Por exemplo, você pode criar uma imagem de dispositivo de blocos com uma VM Linux gravada nela e, em seguida, capturar um instantâneo da imagem, proteger o instantâneo e criar quantos clones de cópia em gravação desejar. Um instantâneo é apenas leitura, portanto, sua clonagem simplifica a semântica, possibilitando criar clones rapidamente.
   </para>
   <note>
    <para>
     Os termos “pai” e “filho” mencionados nos exemplos de linha de comando a seguir indicam um instantâneo de dispositivo de blocos do Ceph (pai) e a imagem correspondente clonada do instantâneo (filho).
    </para>
   </note>
   <para>
    Cada imagem clonada (filho) armazena uma referência à imagem pai, o que permite que a imagem clonada abra o instantâneo pai e o leia.
   </para>
   <para>
    Um clone COW de um instantâneo funciona exatamente como qualquer outra imagem de dispositivo de blocos do Ceph. Você pode ler, gravar, clonar e redimensionar imagens clonadas. Não há nenhuma restrição especial em relação às imagens clonadas. No entanto, o clone de cópia em gravação de um instantâneo refere-se ao instantâneo. Sendo assim, você <emphasis>deve</emphasis> proteger o instantâneo antes de cloná-lo.
   </para>
   <note>
    <title><option>--image-format 1</option> não suportado</title>
    <para>
     Você não pode criar instantâneos de imagens criadas com a opção <command>rbd create --image-format 1</command> descontinuada. O Ceph suporta apenas a clonagem de imagens no <emphasis>formato 2</emphasis> padrão.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Introdução às camadas</title>
    <para>
     As camadas de dispositivo de blocos do Ceph são um processo simples. Você deve ter uma imagem. Você deve criar um instantâneo da imagem. Você deve proteger o instantâneo. Após executar essas etapas, você poderá iniciar a clonagem do instantâneo.
    </para>
    <para>
     A imagem clonada tem uma referência ao instantâneo pai e inclui os IDs do pool, da imagem e do instantâneo. A inclusão do ID do pool significa que você pode clonar instantâneos de um pool para imagens em outro pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Gabarito de Imagem</emphasis>: Um caso de uso comum para camadas de dispositivo de blocos é criar uma imagem master e um instantâneo que serve como gabarito para os clones. Por exemplo, um usuário pode criar uma imagem para uma distribuição Linux (por exemplo, SUSE Linux Enterprise Server) e criar um instantâneo para ela. Periodicamente, o usuário pode atualizar a imagem e criar um novo instantâneo (por exemplo, <command>zypper ref &amp;&amp; zypper patch</command> seguido por <command>rbd snap create</command>). Durante a maturação da imagem, o usuário pode clonar qualquer um dos instantâneos.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Gabarito Estendido</emphasis>: Um caso de uso mais avançado inclui estender a imagem de um gabarito que fornece mais informações do que uma imagem de base. Por exemplo, um usuário pode clonar uma imagem (um gabarito de VM) e instalar outro software (por exemplo, um banco de dados, um sistema de gerenciamento de conteúdo ou um sistema de análise) e, em seguida, capturar um instantâneo da imagem estendida que, por si só, pode ser atualizada da mesma forma que a imagem de base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Pool de Gabarito</emphasis>: Uma maneira de usar as camadas de dispositivo de blocos é criar um pool que contém imagens master que atuam como gabaritos e instantâneos desses gabaritos. Em seguida, você pode estender os privilégios apenas leitura aos usuários para que eles possam clonar os instantâneos sem a capacidade de gravação ou execução no pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migração/Recuperação de Imagens</emphasis>: Uma maneira de usar as camadas de dispositivo de blocos é migrar ou recuperar os dados de um pool para outro.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protegendo um instantâneo</title>
    <para>
     Os clones acessam os instantâneos pai. Todos os clones serão destruídos se um usuário apagar o instantâneo pai por engano. Para evitar a perda de dados, você precisa proteger o instantâneo antes de cloná-lo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Você não pode apagar um instantâneo protegido.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Clonando um instantâneo</title>
    <para>
     Para clonar um instantâneo, você precisa especificar o pool pai, a imagem, o instantâneo, o pool filho e o nome da imagem. Você precisa proteger o instantâneo antes de cloná-lo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Você pode clonar um instantâneo de um pool para uma imagem em outro pool. Por exemplo, você pode manter as imagens e os instantâneos apenas leitura como gabaritos em um pool e os clones graváveis em outro pool.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Anulando a proteção de um instantâneo</title>
    <para>
     Antes de apagar um instantâneo, você deve anular a proteção dele. Além disso, você <emphasis>não</emphasis> pode apagar instantâneos com referências de clones. Você precisa nivelar cada clone de um instantâneo antes de apagar o instantâneo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Listando os filhos de um instantâneo</title>
    <para>
     Para listar os filhos de um instantâneo, execute o seguinte:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Nivelando uma imagem clonada</title>
    <para>
     As imagens clonadas mantêm uma referência ao instantâneo pai. Ao remover a referência do clone filho para o instantâneo pai, você “nivela” com eficiência a imagem copiando as informações do instantâneo para o clone. O tempo necessário para nivelar um clone aumenta de acordo com o tamanho do instantâneo. Para apagar um instantâneo, você deve primeiro nivelar as imagens filho.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Como uma imagem nivelada contém todas as informações do instantâneo, ela ocupa mais espaço de armazenamento do que um clone em camadas.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Espelhos de imagens RBD</title>

  <para>
   É possível espelhar as imagens RBD de forma assíncrona entre dois clusters do Ceph. Esse recurso está disponível em dois modos:
  </para>

  <variablelist>
   <varlistentry>
    <term>Com base em diário</term>
    <listitem>
     <para>
      Esse modo usa o recurso de registro de imagens RBD em diário para garantir a replicação consistente com o ponto no tempo e a falha entre os clusters. Cada gravação na imagem RBD é registrada primeiro no diário associado antes de modificar a imagem real. O cluster <literal>remoto</literal> fará a leitura do diário e reproduzirá as atualizações em sua cópia local da imagem. Como cada gravação na imagem RBD resultará em duas gravações no cluster do Ceph, é esperado que as latências de gravação quase dobrem ao usar o recurso de registro de imagens RBD em diário.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Com base em instantâneo</term>
    <listitem>
     <para>
      Esse modo usa instantâneos de espelho de imagens RBD programados periodicamente ou criados manualmente para replicar imagens RBD consistentes com a falha entre os clusters. O cluster <literal>remoto</literal> determinará quaisquer atualizações de dados ou metadados entre dois instantâneos de espelho e copiará os deltas para sua cópia local da imagem. Com a ajuda do recurso de imagem RBD fast-diff, os blocos de dados atualizados podem ser rapidamente calculados sem a necessidade de explorar a imagem RBD completa. Como esse modo não é consistente com o ponto no tempo, o delta de instantâneo completo precisará ser sincronizado antes do uso durante um cenário de failover. Quaisquer deltas de instantâneo parcialmente aplicados voltarão ao último instantâneo totalmente sincronizado antes do uso.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   O espelhamento é configurado por pool nos clusters de peer. Ele pode ser configurado em um subconjunto específico de imagens no pool ou configurado para espelhar automaticamente todas as imagens em um pool ao usar apenas o espelhamento com base em diário. O espelhamento é configurado usando o comando <command>rbd</command>. O daemon <systemitem class="daemon">rbd-mirror</systemitem> é responsável por capturar as atualizações da imagem do cluster de peer <literal>remoto</literal> e aplicá-las à imagem no cluster <literal>local</literal>.
  </para>

  <para>
   Dependendo das necessidades de replicação desejadas, o espelhamento de RBD pode ser configurado para replicação unidirecional ou bidirecional:
  </para>

  <variablelist>
   <varlistentry>
    <term>Replicação unidirecional</term>
    <listitem>
     <para>
      Quando os dados são espelhados apenas de um cluster principal para um cluster secundário, o daemon <systemitem class="daemon">rbd-mirror</systemitem> é executado somente no cluster secundário.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replicação bidirecional</term>
    <listitem>
     <para>
      Quando os dados são espelhados de imagens principais em um cluster para imagens não principais em outro cluster (e vice-versa), o daemon <systemitem class="daemon">rbd-mirror</systemitem> é executado nos dois clusters.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Cada instância do daemon <systemitem class="daemon">rbd-mirror</systemitem> precisa ser capaz de se conectar aos clusters do Ceph <literal>local</literal> e <literal>remoto</literal> simultaneamente. Por exemplo, todos os hosts de monitor e OSD. Além disso, a rede precisa ter largura de banda suficiente entre os dois data centers para processar a carga de trabalho de espelhamento.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configuração do pool</title>
   <para>
    Os procedimentos a seguir demonstram como executar as tarefas administrativas básicas para configurar o espelhamento usando o comando <command>rbd</command>. O espelhamento é configurado por pool nos clusters do Ceph.
   </para>
   <para>
    Você precisa executar as etapas de configuração do pool em ambos os clusters peer. Estes procedimentos consideram a existência de dois clusters, chamados de <literal>local</literal> e <literal>remoto</literal>, acessíveis de um único host, por motivos de clareza.
   </para>
   <para>
    Consulte a página de manual do <command>rbd</command> (<command>man 8 rbd</command>) para obter mais detalhes sobre como se conectar a diferentes clusters do Ceph.
   </para>
   <tip>
    <title>Vários clusters</title>
    <para>
     O nome do cluster nos exemplos a seguir corresponde a um arquivo de configuração do Ceph com o mesmo nome <filename>/etc/ceph/remote.conf</filename> e ao arquivo de chaveiro do Ceph com o mesmo nome <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Permitir o espelhamento em um pool</title>
    <para>
     Para habilitar o espelhamento em um pool, especifique o subcomando <command>mirror pool enable</command>, o nome do pool e o modo de espelhamento. O modo de espelhamento pode ser pool ou image:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Todas as imagens no pool com o recurso de registro em diário habilitado são espelhadas.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        O espelhamento precisa ser habilitado explicitamente em cada imagem. Consulte a <xref linkend="rbd-mirror-enable-image-mirroring"/> para obter mais informações.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Desabilitar o espelhamento</title>
    <para>
     Para desabilitar o espelhamento em um pool, especifique o subcomando <command>mirror pool disable</command> e o nome do pool. Quando o espelhamento é desabilitado dessa maneira em um pool, ele também é desabilitado em todas as imagens (no pool) para as quais ele foi explicitamente habilitado.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Inicializando peers</title>
    <para>
     Para que o daemon <systemitem class="daemon">rbd-mirror</systemitem> descubra seu cluster de peer, o peer precisa ser registrado no pool, e uma conta do usuário precisa ser criada. Esse processo pode ser automatizado com o <command>rbd</command> e os comandos <command>mirror pool peer bootstrap create</command> e <command>mirror pool peer bootstrap import</command>.
    </para>
    <para>
     Para criar manualmente um novo token de boot com o <command>rbd</command>, especifique o comando <command>mirror pool peer bootstrap create</command>, o nome de um pool, junto com o nome amigável de um site, para descrever o cluster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     A saída do <command>mirror pool peer bootstrap create</command> será um token que deve ser inserido no comando <command>mirror pool peer bootstrap import</command>. Por exemplo, no cluster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Para importar manualmente o token de boot criado por outro cluster com o comando <command>rbd</command>, use a seguinte sintaxe:
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Onde:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        O nome amigável opcional de um site para descrever o cluster <literal>local</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Uma direção de espelhamento. Assume <literal>rx-tx</literal> como padrão para espelhamento bidirecional, mas também pode ser definido como <literal>rx-only</literal> para espelhamento unidirecional.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        O nome do pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        O caminho de arquivo para o token criado (ou <literal>-</literal> para lê-lo da entrada padrão).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por exemplo, no cluster <literal>remoto</literal>:
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Adicionando um peer do cluster manualmente</title>
    <para>
     Como alternativa à inicialização de peers, conforme descrito na <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>, você pode especificar os peers manualmente. O daemon remoto <systemitem class="daemon">rbd-mirror</systemitem>precisará de acesso ao cluster local para executar o espelhamento. Crie um novo usuário do Ceph local que o daemon <systemitem class="daemon">rbd-mirror</systemitem> remoto usará, por exemplo <literal>rbd-mirror-peer</literal>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Use a seguinte sintaxe para adicionar um cluster de peer de espelhamento do Ceph com o comando <command>rbd</command>:
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Por padrão, o daemon <systemitem class="daemon">rbd-mirror</systemitem> precisa ter acesso ao arquivo de configuração do Ceph localizado em <filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename>. Ele inclui os endereços IP dos MONs do cluster de peer e um chaveiro para um cliente denominado <replaceable>CLIENT_NAME</replaceable>, localizado nos caminhos padrão ou personalizado de pesquisa de chaveiro, por exemplo <filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>.
    </para>
    <para>
     Como alternativa, é possível gravar o MON do cluster de peer e/ou a chave do cliente com segurança no armazenamento de chave de configuração local do Ceph. Para especificar os atributos de conexão do cluster de peer ao adicionar um peer de espelhamento, use as opções <option>--remote-mon-host</option> e <option>--remote-key-file</option>. Por exemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Remover o peer de cluster</title>
    <para>
     Para remover um cluster peer de espelhamento, especifique o subcomando <command>mirror pool peer remove</command>, o nome do pool e o UUID do peer (disponível no comando <command>rbd mirror pool info</command>):
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Pools de dados</title>
    <para>
     Ao criar imagens no cluster de destino, o <systemitem class="daemon">rbd-mirror</systemitem> seleciona um pool de dados da seguinte maneira:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Se o cluster de destino tiver um pool de dados padrão configurado (com a opção de configuração <option>rbd_default_data_pool</option>), ele será usado.
      </para>
     </listitem>
     <listitem>
      <para>
       Do contrário, se a imagem de origem usar um pool de dados separado, e existir um pool com o mesmo nome no cluster de destino, esse pool será usado.
      </para>
     </listitem>
     <listitem>
      <para>
       Se nenhuma das opções acima for verdadeira, nenhum pool de dados será definido.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configuração de imagens RBD</title>
   <para>
    Diferentemente da configuração do pool, a configuração da imagem precisa apenas ser executada em um único cluster peer de espelhamento do Ceph.
   </para>
   <para>
    As imagens RBD espelhadas são designadas como <emphasis>principais</emphasis> ou <emphasis>não principais</emphasis>. Essa é uma propriedade da imagem, e não do pool. Não é possível modificar as imagens designadas como não principais.
   </para>
   <para>
    As imagens são automaticamente promovidas a principais quando o espelhamento é habilitado primeiro em uma imagem (seja implicitamente, se o modo de espelhamento do pool for “pool” e a imagem tiver o recurso de registro de imagens em diário habilitado, ou explicitamente (consulte a <xref linkend="rbd-mirror-enable-image-mirroring"/>) pelo comando <command>rbd</command>).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Habilitando o espelhamento de imagens</title>
    <para>
     Se o espelhamento for configurado no modo <literal>image</literal>, será necessário habilitar explicitamente o espelhamento para cada imagem no pool. Para habilitar o espelhamento de uma imagem específica com o <command>rbd</command>, especifique o subcomando <command>mirror image enable</command> junto com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     O modo de espelhamento da imagem pode ser <literal>journal</literal> ou <literal>snapshot</literal>:
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (padrão)</term>
      <listitem>
       <para>
        Quando configurado no modo <literal>journal</literal>, o espelhamento usará o recurso de registro de imagens RBD em diário para replicar o conteúdo da imagem. Se o recurso de registro de imagens RBD em diário ainda não estiver habilitado na imagem, ele será habilitado automaticamente.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        Quando configurado no modo <literal>snapshot</literal>, o espelhamento usará o instantâneo de espelho de imagens RBD para replicar o conteúdo da imagem. Uma vez habilitado, um instantâneo de espelho inicial será criado automaticamente. É possível criar mais instantâneos de espelho de imagens RBD com o comando <command>rbd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Habilitando o recurso de registro de imagens em diário</title>
    <para>
     O espelhamento de RBD usa o recurso de registro de RBD em diário para garantir que a imagem replicada permaneça sempre consistente com a falha. Ao usar o modo de espelhamento de <literal>imagem</literal>, o recurso de registro em diário será habilitado automaticamente se o espelhamento estiver habilitado na imagem. Ao usar o modo de espelhamento de <literal>pool</literal>, antes que uma imagem possa ser espelhada para um cluster de peer, o recurso de registro de imagens RBD em diário deve ser habilitado. O recurso pode ser habilitado no momento da criação da imagem inserindo a opção <option>--image-feature exclusive-lock,journaling</option> no comando <command>rbd</command>.
    </para>
    <para>
     Se preferir, o recurso de registro em diário pode ser habilitado dinamicamente nas imagens RBD preexistentes. Para habilitar o registro em diário, especifique o subcomando <command>feature enable</command>, o nome do pool e da imagem e o nome do recurso:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>Dependência da opção</title>
     <para>
      O recurso de <option>registro em diário</option> depende do recurso de <option>bloqueio exclusivo</option>. Se o recurso de <option>bloqueio exclusivo</option> ainda não estiver habilitado, você precisará habilitá-lo antes de habilitar o recurso de <option>registro em diário</option>.
     </para>
    </note>
    <tip>
     <para>
      É possível habilitar o registro em diário em todas as imagens novas por padrão, adicionando <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option> ao arquivo de configuração do Ceph.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Criando instantâneos de espelho de imagem</title>
    <para>
     Ao usar o espelhamento com base em instantâneo, os instantâneos de espelho precisarão ser criados sempre que você desejar espelhar o conteúdo modificado da imagem RBD. Para criar um instantâneo de espelho manualmente com o <command>rbd</command>, especifique o comando <command>mirror image snapshot</command>, junto com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Por padrão, apenas três instantâneos de espelho serão criados por imagem. O instantâneo de espelho mais recente será removido automaticamente se o limite for atingido. O limite poderá ser anulado por meio da opção de configuração <option>rbd_mirroring_max_mirroring_snapshots</option>, se necessário. Além disso, os instantâneos de espelho são automaticamente apagados quando a imagem é removida ou quando o espelhamento é desabilitado.
    </para>
    <para>
     Os instantâneos de espelho também poderão ser criados automaticamente de maneira periódica, se as programações de instantâneos de espelho forem definidas. O instantâneo de espelho pode ser programado nos níveis global, por pool ou por imagem. Várias programações de instantâneos de espelho podem ser definidas em qualquer nível, mas apenas as programações de instantâneo mais específicas correspondentes a uma imagem espelhada individual serão executadas.
    </para>
    <para>
     Para criar uma programação de instantâneo de espelho com o <command>rbd</command>, especifique o comando <command>mirror snapshot schedule add</command>, junto com o nome de um pool ou imagem opcional, o intervalo e o horário de início opcional.
    </para>
    <para>
     O intervalo pode ser especificado em dias, horas ou minutos usando os sufixos <option>d</option>, <option>h</option> ou <option>m</option>, respectivamente. O horário de início opcional pode ser especificado usando o formato de horário ISO 8601. Por exemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Para remover uma programação de instantâneo de espelho com o <command>rbd</command>, especifique o comando <command>mirror snapshot schedule remove</command> com as opções correspondentes ao comando de adição da programação.
    </para>
    <para>
     Para listar todas as programações de instantâneo para um nível específico (global, pool ou imagem) com o <command>rbd</command>, especifique o comando <command>mirror snapshot schedule ls</command>, junto com o nome de um pool ou imagem opcional. Além disso, a opção <option>--recursive</option> pode ser usada para listar todas as programações no nível especificado e abaixo dele. Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Para saber quando os próximos instantâneos serão criados para imagens RBD de espelhamento com base em instantâneo com o <command>rbd</command>, especifique o comando <command>mirror snapshot schedule status</command>, junto com o nome de um pool ou imagem opcional. Por exemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Desabilitando o espelhamento de imagens</title>
    <para>
     Para desabilitar o espelhamento em determinada imagem, especifique o subcomando <command>mirror image disable</command> juntamente com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Promovendo e retrocedendo imagens</title>
    <para>
     Em um cenário de failover em que a designação principal precisa ser movida para a imagem no cluster peer, você precisa interromper o acesso à imagem principal, rebaixar a imagem principal atual, promover a nova imagem principal e continuar o acesso à imagem no cluster alternativo.
    </para>
    <note>
     <title>Promoção forçada</title>
     <para>
      A promoção pode ser forçada usando a opção <option>--force</option>. A promoção forçada é necessária quando o rebaixamento não pode ser propagado para o cluster peer (por exemplo, em caso de falha do cluster ou interrupção da comunicação). Isso resultará em um cenário de split-brain (dupla personalidade) entre os dois peers, e a imagem não será mais sincronizada até que um subcomando <command>resync</command> seja emitido.
     </para>
    </note>
    <para>
     Para rebaixar determinada imagem para não principal, especifique o subcomando <command>mirror image demote</command> juntamente com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Para rebaixar todas as imagens principais em um pool para não principais, especifique o subcomando <command>mirror pool demote</command> juntamente com o nome do pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Para promover determinada imagem para principal, especifique o subcomando <command>mirror image promote</command> juntamente com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Para promover todas as imagens não principais em um pool para principais, especifique o subcomando <command>mirror pool promote</command> juntamente com o nome do pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>Dividir a carga de E/S</title>
     <para>
      Como o status principal ou não principal refere-se a cada imagem, é possível ter dois clusters que dividem a carga de E/S e o failover ou failback por fase.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Forçando a ressincronização da imagem</title>
    <para>
     Se um evento de split-brain for detectado pelo daemon <systemitem class="daemon">rbd-mirror</systemitem>, ele não tentará espelhar a imagem afetada até o problema ser corrigido. Para continuar o espelhamento de uma imagem, primeiro rebaixe a imagem que foi identificada como desatualizada e, em seguida, solicite uma ressincronização com a imagem principal. Para solicitar a ressincronização de uma imagem, especifique o subcomando <command>mirror image resync</command> juntamente com o nome do pool e da imagem:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Verificando o status do espelho</title>
   <para>
    O status de replicação do cluster peer é armazenado para cada imagem principal espelhada. Esse status pode ser recuperado usando os subcomandos <command>mirror image status</command> e <command>mirror pool status</command>:
   </para>
   <para>
    Para solicitar o status da imagem de espelho, especifique o subcomando <command>mirror image status</command> juntamente com o nome do pool e da imagem:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Para solicitar o status do resumo do pool de espelhos, especifique o subcomando <command>mirror pool status</command> juntamente com o nome do pool:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     A adição da opção <option>--verbose</option> ao subcomando <command>mirror pool status</command> resultará também nos detalhes de status para cada imagem de espelhamento no pool.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Configurações de cache</title>

  <para>
   A implementação do espaço de usuário do dispositivo de blocos do Ceph (<systemitem>librbd</systemitem>) não pode se beneficiar do cache de página do Linux. Portanto, ela inclui seu próprio cache na memória. O cache RBD tem comportamento semelhante ao cache de disco rígido. Quando o OS envia uma solicitação de barreira ou descarga, todos os dados &quot;modificados&quot; são gravados nos OSDs. Isso significa que o uso do cache de write-back é tão seguro quanto o uso de um disco rígido físico de bom funcionamento com uma VM que envia descarregamentos apropriadamente. O cache usa um algoritmo <emphasis>menos utilizado recentemente</emphasis> (LRU, Least Recently Used) e, no modo write-back, ele pode fundir solicitações adjacentes para um melhor throughput.
  </para>

  <para>
   O Ceph suporta cache de write-back para RBD. Para habilitá-lo, execute
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Por padrão, o <systemitem>librbd</systemitem> não executa armazenamento em cache. As gravações e leituras seguem diretamente para o cluster de armazenamento, e as gravações são retornadas apenas quando os dados estão no disco em todas as réplicas. Com o cache habilitado, as gravações são retornadas imediatamente, a menos que haja mais bytes descarregados do que o que foi definido na opção <option>rbd cache max dirty</option>. Nesse caso, a gravação aciona write-back e blocos até que sejam descarregados bytes suficientes.
  </para>

  <para>
   O Ceph suporta cache de write-through para RBD. Você pode definir o tamanho do cache, destinos e limites para alternar do cache de write-back para o cache de write-through. Para habilitar o modo write-through, execute
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Isso significa que as gravações são retornadas apenas quando os dados estão no disco em todas as réplicas, mas as leituras podem vir do cache. O cache está na memória do cliente, e cada imagem RBD tem seu próprio cache. Como o cache é local ao cliente, não fará sentido se houver outras pessoas acessando a imagem. A execução do GFS ou OCFS no RBD não funcionará com o cache habilitado.
  </para>

  <para>
   Os parâmetros a seguir afetam o comportamento dos Dispositivos de Blocos RADOS. Para defini-los, use a categoria <literal>client</literal>:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Habilitar o cache para Dispositivo de blocos RADOS (RBD, RADOS Block Device). O padrão é “true”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      O tamanho do cache RBD em bytes. O padrão é 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      O limite de &quot;modificação&quot; em bytes em que o cache aciona o write-back. <option>rbd cache max dirty</option> precisa ser inferior a <option>rbd cache size</option>. Se definido como 0, usa o cache de write-through. O padrão é 24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      O “destino de modificação” antes de o cache começar a gravar dados no armazenamento de dados. Não bloqueia as gravações para o cache. O padrão é 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Por quantos segundos os dados modificados permanecem no cache antes que o write-back seja iniciado. O padrão é 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Comece no modo write-through e alterne para write-back depois de receber a primeira solicitação de descarregamento. A habilitação dessa configuração é conservadora, porém segura, caso as máquinas virtuais executadas no <systemitem>rbd</systemitem> sejam muito antigas para enviar descarregamentos (por exemplo, o driver virtio no Linux antes do kernel 2.6.32). O padrão é “true”.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>Configurações de QdS</title>

  <para>
   Em geral, a Qualidade do Serviço (QdS) refere-se aos métodos de priorização de tráfego e reserva de recursos. Ela é importante principalmente para o transporte de tráfego com requisitos especiais.
  </para>

  <important>
   <title>Não suportadas pelo iSCSI</title>
   <para>
    As seguintes configurações de QdS são usadas apenas pela implementação RBD <systemitem class="daemon">librbd</systemitem> do espaço de usuário, e <emphasis>não</emphasis> são usadas pela implementação <systemitem>kRBD</systemitem>. Como o iSCSI usa o <systemitem>kRBD</systemitem>, ele não usa as configurações de QdS. No entanto, para o iSCSI, você pode configurar a QdS na camada do dispositivo de blocos do kernel usando os recursos padrão do kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      O limite desejado das operações de E/S por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      O limite desejado de bytes de E/S por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      O limite desejado das operações de leitura por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      O limite desejado das operações de gravação por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      O limite desejado de bytes de leitura por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      O limite desejado de bytes de gravação por segundo. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado das operações de E/S. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado de bytes de E/S. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado das operações de leitura. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado das operações de gravação. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado de bytes de leitura. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      O limite de burst desejado de bytes de gravação. O padrão é 0 (sem limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      O tique de horário mínimo (em milissegundos) para QdS. O padrão é 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Configurações de leitura com ajuda</title>

  <para>
   O Dispositivo de Blocos RADOS suporta leitura com ajuda/pré-busca para otimizar pequenas leituras sequenciais. Isso costuma ser processado pelo OS convidado no caso de uma máquina virtual, mas os carregadores de boot talvez não emitam leituras suficientes. A leitura com ajuda será automaticamente desabilitada se o cache for desabilitado.
  </para>

  <important>
   <title>Não suportadas pelo iSCSI</title>
   <para>
    As seguintes configurações de leitura com ajuda são usadas apenas pela implementação RBD <systemitem class="daemon">librbd</systemitem> do espaço de usuário, e <emphasis>não</emphasis> são usadas pela implementação <systemitem>kRBD</systemitem>. Como o iSCSI usa o <systemitem>kRBD</systemitem>, ele não usa as configurações de leitura com ajuda. No entanto, para o iSCSI, você pode configurar a leitura com ajuda na camada do dispositivo de blocos do kernel usando os recursos padrão do kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Número de solicitações de leitura sequenciais necessárias para acionar a leitura com ajuda. O padrão é 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Tamanho máximo de uma solicitação de leitura com ajuda. Se definido como 0, a leitura com ajuda será desabilitada. O padrão é 512 KB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Após a leitura dessa quantidade de bytes de uma imagem RBD, a leitura com ajuda será desabilitada para essa imagem até ser fechada. Isso permite que o OS convidado controle a leitura com ajuda quando é inicializado. Se definido como 0, a leitura com ajuda permanecerá habilitada. O padrão é 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Recursos avançados</title>

  <para>
   O Dispositivo de Blocos RADOS suporta recursos avançados que melhoram a funcionalidade das imagens RBD. Você pode especificar os recursos na linha de comando ao criar uma imagem RBD ou no arquivo de configuração do Ceph usando a opção <option>rbd_default_features</option>.
  </para>

  <para>
   Você pode especificar os valores da opção <option>rbd_default_features</option> de duas maneiras:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Como a soma dos valores internos dos recursos. Cada recurso tem seu próprio valor interno. Por exemplo, “layering” tem 1 e “fast-diff” tem 16. Portanto, para ativar esses dois recursos por padrão, inclua o seguinte:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     Como uma lista de recursos separada por vírgula. O exemplo anterior terá a seguinte aparência:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>Recursos não suportados pelo iSCSI</title>
   <para>
    As imagens RBD com os seguintes recursos não serão suportadas pelo iSCSI: <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option>, <option>striping</option>
   </para>
  </note>

  <para>
   Veja a seguir uma lista de recursos RBD avançados:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      As camadas (layering) permitem usar a clonagem.
     </para>
     <para>
      O valor interno é 1, o padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      A distribuição difunde os dados por vários objetos e ajuda com paralelismo para cargas de trabalho de leitura/gravação sequenciais. Ele evita gargalos de nó único para Dispositivo de Blocos RADOS grandes ou ocupados.
     </para>
     <para>
      O valor interno é 2, o padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Quando habilitado, ele requer que um cliente crie um bloqueio em um objeto antes de fazer uma gravação. Habilite o bloqueio exclusivo apenas quando um único cliente está acessando uma imagem ao mesmo tempo. O valor interno é 4. O padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      O suporte ao mapa de objetos depende do suporte ao bloqueio exclusivo. Os dispositivos de blocos são aprovisionados dinamicamente, o que significa que eles apenas armazenam dados que realmente existem. O suporte ao mapa de objetos ajuda a monitorar os objetos que realmente existem (com dados armazenados em uma unidade). A habilitação do suporte ao mapa de objetos acelera as operações de E/S para clonagem, importação e exportação de uma imagem pouco preenchida, além da exclusão.
     </para>
     <para>
      O valor interno é 8, o padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      O suporte à comparação rápida (fast-diff) depende do suporte ao mapa de objetos e do suporte ao bloqueio exclusivo. Ele adiciona outra propriedade ao mapa de objetos, o que o torna muito mais rápido para gerar comparações entre instantâneos de uma imagem e o uso real dos dados de um instantâneo.
     </para>
     <para>
      O valor interno é 16, o padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      O nivelamento profundo (deep-flatten) faz com que o <command>rbd flatten</command> (consulte a <xref linkend="rbd-flatten-cloned-image"/>) funcione em todos os instantâneos de uma imagem, além da própria imagem. Sem ele, os instantâneos de uma imagem ainda dependerão do pai, portanto, você não poderá apagar a imagem pai até os instantâneos serem apagados. O deep-flatten torna um pai independente de seus clones, mesmo que eles tenham instantâneos.
     </para>
     <para>
      O valor interno é 32, o padrão é “yes”.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      O suporte ao registro em diário depende do suporte ao bloqueio exclusivo. O diário registra todas as modificações em uma imagem na ordem em que elas ocorrem. O espelhamento RBD (consulte a <xref linkend="ceph-rbd-mirror"/>) usa o diário para replicar uma imagem consistente de falha para um cluster remoto.<literal></literal>
     </para>
     <para>
      O valor interno é 64, o padrão é “no”.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Mapeando o RBD por meio de clientes antigos do kernel</title>

  <para>
   Clientes antigos (por exemplo, SLE11 SP4) podem não conseguir mapear imagens RBD porque um cluster implantado com o SUSE Enterprise Storage 7.1 força alguns recursos (no nível tanto da imagem RBD quanto do RADOS) que esses clientes antigos não suportam. Quando isso acontece, os registros do OSD mostram mensagens semelhantes às seguintes:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>Mudança de tipos de compartimento de memória do Mapa CRUSH provoca rebalanceamento massivo</title>
   <para>
    Se você pretende alternar os tipos de compartimento de memória do Mapa CRUSH entre “straw” e “straw2”, faça um planejamento para isso. Espere um impacto significativo na carga do cluster, porque a mudança do tipo de compartimento de memória provoca uma redistribuição massiva do cluster.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Desabilite quaisquer recursos da imagem RBD que não sejam suportados. Por exemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Mude os tipos de compartimento de memória do Mapa CRUSH de “straw2” para “straw”:
    </para>
    <substeps>
     <step>
      <para>
       Grave o Mapa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Descompile o Mapa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Edite o Mapa CRUSH e substitua “straw2” por “straw”.
      </para>
     </step>
     <step>
      <para>
       Recompile o Mapa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Defina o novo Mapa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Habilitando dispositivos de blocos e Kubernetes</title>

  <para>
   Você pode usar o Ceph RBD com o Kubernetes 1.13 e versões mais recentes por meio do driver <literal>ceph-csi</literal>. Esse driver provisiona dinamicamente as imagens RBD para suportar os volumes do Kubernetes e mapeia essas imagens RBD como dispositivos de blocos (opcionalmente, montando um sistema de arquivos contido na imagem) em nós de trabalho que executam pods que fazem referência a um volume com suporte do RBD.
  </para>

  <para>
   Para usar dispositivos de blocos do Ceph com o Kubernetes, você deve instalar e configurar o <literal>ceph-csi</literal> no seu ambiente do Kubernetes.
  </para>

  <important>
   <para>
    O <literal>ceph-csi</literal> usa os módulos do kernel RBD por padrão, que podem não suportar todos os tunables do Ceph CRUSH ou os recursos de imagem RBD.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Por padrão, os dispositivos de blocos do Ceph usam o pool RBD. Crie um pool para armazenamento de volume do Kubernetes. Verifique se o cluster do Ceph está em execução e crie o pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Use a ferramenta RBD para inicializar o pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Crie um novo usuário para o Kubernetes e o <literal>ceph-csi</literal>. Execute o seguinte e registre a chave gerada:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     O <literal>ceph-csi</literal> requer um objeto ConfigMap armazenado no Kubernetes para definir os endereços do monitor do Ceph para o cluster do Ceph. Colete o FSID exclusivo do cluster do Ceph e os endereços do monitor:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Gere um arquivo <filename>csi-config-map.yaml</filename> semelhante ao exemplo abaixo, substituindo o FSID por <literal>clusterID</literal> e os endereços do monitor por <literal>monitors</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Quando gerado, armazene o novo objeto ConfigMap no Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     O <literal>ceph-csi</literal> requer as credenciais do cephx para se comunicar com o cluster do Ceph. Gere um arquivo <filename>csi-rbd-secret.yaml</filename> semelhante ao exemplo abaixo, usando o ID de usuário do Kubernetes recém-criado e a chave do cephx:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Quando gerado, armazene o novo objeto secreto no Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Crie os objetos do Kubernetes ServiceAccount e RBAC ClusterRole/ClusterRoleBinding necessários. Esses objetos não precisam ser necessariamente personalizados para seu ambiente do Kubernetes e, portanto, podem ser usados diretamente dos arquivos YAML de implantação do <literal>ceph-csi</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Crie o aprovisionador <literal>ceph-csi</literal> e os plug-ins de nó:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Por padrão, os arquivos YAML do aprovisionador e do plug-in de nó extrairão a versão de desenvolvimento do container <literal>ceph-csi</literal>. Os arquivos YAML devem ser atualizados para usar uma versão de lançamento.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Usando dispositivos de blocos do Ceph no Kubernetes</title>
   <para>
    A StorageClass do Kubernetes define uma classe de armazenamento. Vários objetos StorageClass podem ser criados para mapear para diferentes níveis e recursos de qualidade de serviço. Por exemplo, pools com base em NVMe em relação aos pools com base em HDD.
   </para>
   <para>
    Para criar uma StorageClass do <literal>ceph-csi</literal> que mapeie para o pool do Kubernetes criado acima, o seguinte arquivo YAML pode ser usado, depois de garantir que a propriedade <literal>clusterID</literal> corresponda ao FSID do cluster do Ceph:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    Um <literal>PersistentVolumeClaim</literal> é uma solicitação de recursos de armazenamento abstratos feita por um usuário. O <literal>PersistentVolumeClaim</literal> é associado a um recurso de pod para provisionar um <literal>PersistentVolume</literal>, que recebe suporte de uma imagem de bloco do Ceph. É possível incluir um <option>volumeMode</option> opcional para selecionar entre um sistema de arquivos montado (padrão) ou um volume bruto com base em dispositivo de blocos.
   </para>
   <para>
    Usando o <literal>ceph-csi</literal>, a especificação de <option>Filesystem</option> para <option>volumeMode</option> pode suportar ambos os requerimentos <literal>ReadWriteOnce</literal> e <literal>ReadOnlyMany accessMode</literal>, e a especificação de <option>Block</option> para <option>volumeMode</option> pode suportar os requerimentos <literal>ReadWriteOnce</literal>, <literal>ReadWriteMany</literal> e <literal>ReadOnlyMany accessMode</literal>.
   </para>
   <para>
    Por exemplo, para criar um <literal>PersistentVolumeClaim</literal> baseado em blocos que usa o <literal>ceph-csi-based StorageClass</literal> criado acima, o seguinte arquivo YAML pode ser usado para solicitar o armazenamento de blocos bruto de <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    Veja a seguir uma demonstração de como vincular o <literal>PersistentVolumeClaim</literal> acima a um recurso de pod como um dispositivo de blocos bruto:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Para criar um <literal>PersistentVolumeClaim</literal> baseado em sistema de arquivos que usa o <literal>ceph-csi-based StorageClass</literal> criado acima, o seguinte arquivo YAML pode ser usado para solicitar um sistema de arquivos montado (com suporte de uma imagem RBD) de <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    Veja a seguir uma demonstração de como vincular o <literal>PersistentVolumeClaim</literal> acima a um recurso de pod como um sistema de arquivos montado:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
