<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_caasp.xml" version="5.0" xml:id="deploy-rook">

 <title>Quick start</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SUSE Enterprise Storage is a distributed storage system designed for scalability,
  reliability, and performance, which is based on the Ceph technology. The
  traditional way to run a Ceph cluster is setting up a dedicated cluster to
  provide block, file, and object storage to a variety of clients.
 </para>
 <para>
  Rook manages Ceph as a containerized application on Kubernetes and allows a
  hyper-converged setup, in which a single Kubernetes cluster runs applications and
  storage together. The primary purpose of SUSE Enterprise Storage deployed with Rook
  is to provide storage to other applications running in the Kubernetes cluster.
  This can be block, file, or object storage.
 </para>
 <para>
  This chapter describes how to quickly deploy containerized SUSE Enterprise Storage
  7 on top of a SUSE CaaS Platform 4.5 Kubernetes cluster.
 </para>
 <sect1 xml:id="rook-deploy-hardware-specs">
  <title>Recommended hardware specifications</title>

  <para>
   For SUSE Enterprise Storage deployed with Rook, the minimal configuration is
   preliminary, we will update it based on real customer needs.
   <remark>
       LGHP 2020-10-07: Should we at least link to the CaaSP system requirements?
     </remark>
  </para>

  <para>
   For the purpose of this document, consider the following minimum
   configuration:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A highly available Kubernetes cluster with 3 master nodes
    </para>
   </listitem>
   <listitem>
    <para>
     Four physical Kubernetes worker nodes, each with two OSD disks and 5GB of RAM
     per OSD disk
    </para>
   </listitem>
   <listitem>
    <para>
     Allow additional 4GB of RAM per additional daemon deployed on a node
    </para>
   </listitem>
   <listitem>
    <para>
     Dual-10 Gb ethernet as bonded network
    </para>
   </listitem>
   <listitem>
    <para>
     If you are running a hyper-converged infrastructure (HCI), ensure you add
     any additional requirements for your workloads.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="rook-deploy-before-begin">
  <title>Prerequisites</title>

  <para>
   Ensure the following prerequisites are met before continuing with this
   quickstart guide:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Installation of SUSE CaaS Platform 4.5. See the SUSE CaaS Platform documentation for more
     details on how to install:
     <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure <literal>ceph-csi</literal> (and required sidecars) are running in
     your Kubernetes cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     Installation of the LVM2 package on the host where the OSDs are running.
    </para>
   </listitem>
   <listitem>
    <para>
     Ensure you have one of the following storage options to configure Ceph
     properly:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Raw devices (no partitions or formatted file systems)
      </para>
     </listitem>
     <listitem>
      <para>
       Raw partitions (no formatted file system)
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Ensure the SUSE CaaS Platform 4.5 repository is enabled for the installation of Helm
     3.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="getting-started-rook">
  <title>Getting started with Rook</title>

  <note>
   <para>
    The following instructions are designed for a quick start deployment only.
    For more information on installing Helm, see
    <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/"/>.
   </para>
  </note>

  <procedure>
   <step>
    <para>
     Install Helm v3:
    </para>
<screen><prompt role="root">root # </prompt>zypper in helm3</screen>
   </step>
   <step>
    <para>
     On a node with access to the Kubernetes cluster, execute the following:
    </para>
<screen><prompt>tux &gt; </prompt>export HELM_EXPERIMENTAL_OCI=1</screen>
   </step>
   <step>
    <para>
     Create a local copy of the Helm chart to your local registry:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 chart pull registry.suse.com/ses/7/charts/rook-ceph:latest</screen>
   </step>
   <step>
    <para>
     Export the Helm charts to a Rook-Ceph sub-directory under your current
     working directory:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 chart export registry.suse.com/ses/7/charts/rook-ceph:latest</screen>
   </step>
   <step>
    <para>
     Create a file named <filename>myvalues.yaml</filename> based off the
     <filename>rook-ceph/values.yaml file</filename>.
    </para>
   </step>
   <step>
    <para>
     Set local parameters in <filename>myvalues.yaml</filename>.
    </para>
   </step>
   <step>
    <para>
     Create the namespace:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl create namespace rook-ceph</screen>
   </step>
   <step>
    <para>
     Install the helm charts:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 install -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
   </step>
   <step>
    <para>
     Verify the <literal>rook-operator</literal> is running:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl -n rook-ceph get pod -l app=rook-ceph-operator</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-deploy-ceph">
  <title>Deploying Ceph with Rook</title>

  <procedure>
   <step>
    <para>
     You need to apply Labels to your Kubernetes nodes before deploying your Ceph
     cluster. The key <literal>node-rol.rook-ceph/cluster</literal> accepts one
     of the following values:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <literal>any</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon-mgr</literal>
      </para>
     </listitem>
     <listitem>
      <para>
       <literal>mon-mgr-osd</literal>
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Run the following the get the names of your cluster's nodes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl get nodes</screen>
   </step>
   <step>
    <para>
     On the node you want to Label, run the following:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl label nodes <replaceable>node-name</replaceable> <replaceable>label-key</replaceable>=<replaceable>label-value</replaceable></screen>
    <para>
     For example:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl label node <replaceable>k8s-worker-node-1</replaceable> <replaceable>node-role.rook-ceph/cluster</replaceable>=<replaceable>any</replaceable></screen>
   </step>
   <step>
    <para>
     Verify the application of the Label by re-running the following command:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl get nodes --show-labels</screen>
    <para>
     You can also use the <command>describe</command> command to get the full
     list of labels given to the node. For example:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl describe node <replaceable>node-name</replaceable></screen>
   </step>
   <step>
    <para>
     Next, you need to apply the <filename>cluster.yaml</filename> to your
     Kubernetes cluster. The default <filename>cluster.yaml</filename> can be
     applied as is without any additional services or requirements from a Helm
     chart.
    </para>
    <para>
     To apply the default Helm chart to your Kubernetes cluster, run the following
     command:
    </para>
<screen><prompt>tux &gt; </prompt>helm -n rook-ceph install rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rook-config-ceph">
  <title>Configuring the Ceph cluster</title>

  <para>
   You can have two types of integration with your SUSE Enterprise Storage intregrated
   cluster. These types are: CephFS or RADOS Block Device (RBD).
  </para>

  <para>
   Before you start the SUSE CaaS Platform and SUSE Enterprise Storage integration, ensure you have
   met the following prerequisites:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The SUSE CaaS Platform cluster must have <literal>ceph-common</literal> and
     <literal>xfsprogs</literal> installed on all nodes. You can check this by
     running the <command>rpm -q ceph-common</command> command or the
     <command>rpm -q xfsprogs</command> command.
    </para>
   </listitem>
   <listitem>
    <para>
     That the SUSE Enterprise Storage cluster has a pool with a RBD device or CephFS
     enabled.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="config-cephfs">
   <title>Configure CephFS</title>
   <para>
    For more information on configuring CephFS, see
    <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/"/>
    for steps and more information. This section will also provide the
    necessary procedure on attaching a pod to either an CephFS static or
    dynamic volume.
   </para>
  </sect2>

  <sect2 xml:id="config-persistent-volumes">
   <title>Configure RADOS block device</title>
   <para>
    For instructions on configuring the RADOS Block Device (RBD) in a pod,
    see
    <link xlink:href="https://documentation.suse.com/en-us/suse-caasp/4.5/"/>
    for more information. This section will also provide the necessary
    procedure on attaching a pod to either an RBD static or dynamic volume.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="updating-rook-images">
  <title>Updating local images</title>

  <procedure>
   <step>
    <para>
     To update your local image to the latest tag, apply the new parameters in
     <filename>myvalues.yaml</filename>:
    </para>
<screen>
image:
refix: rook
repository: registry.suse.com/ses/7/rook/ceph
tag: <replaceable>LATEST_TAG</replaceable>
pullPolicy: IfNotPresent
</screen>
   </step>
   <step>
    <para>
     Re-pull a new local copy of the Helm chart to your local registry:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 chart pull <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
   <step>
    <para>
     Export the Helm charts to a Rook-Ceph sub-directory under your current
     working directory:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 chart export <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
   <step>
    <para>
     Upgrade the Helm charts:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 upgrade -n rook-ceph rook-ceph ./rook-ceph/ -f myvalues.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="uninstalling-rook">
  <title>Uninstalling</title>

  <procedure>
   <step>
    <para>
     Delete any Kubernetes applications that are consuming Rook storage.
    </para>
   </step>
   <step>
    <para>
     Delete all object, file, and block storage artifacts.
    </para>
   </step>
   <step>
    <para>
     Remove the CephCluster:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>&gt;kubectl delete -f cluster.yaml</screen>
   </step>
   <step>
    <para>
     Uninstall the operator:
    </para>
<screen><prompt>tux &gt; </prompt>helm3 uninstall <replaceable>REGISTRY_URL</replaceable></screen>
   </step>
   <step>
    <para>
     Delete any data on the hosts:
    </para>
<screen><prompt>tux &gt; </prompt>rm -rf /var/lib/rook</screen>
   </step>
   <step>
    <para>
     Wipe the disks if necessary.
    </para>
   </step>
   <step>
    <para>
     Delete the namespace:
    </para>
<screen><prompt>tux &gt; </prompt>kubectl delete namespace rook-ceph</screen>
   </step>
  </procedure>
 </sect1>
</chapter>
