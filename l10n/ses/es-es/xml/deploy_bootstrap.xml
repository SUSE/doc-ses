<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_bootstrap.xml" version="5.0" xml:id="deploy-bootstrap">
 <info>
  <title>Distribución del clúster de bootstrap mediante <systemitem class="resource">ceph-salt</systemitem></title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Esta sección le guía por el proceso de distribuir un clúster de Ceph básico. Lea atentamente las subsecciones y ejecute los comandos incluidos en el orden indicado.
 </para>
 <sect1 xml:id="deploy-cephadm-cephsalt">
  <title>Instalación <systemitem class="resource">ceph-salt</systemitem></title>

  <para>
   <systemitem class="resource">ceph-salt</systemitem> proporciona herramientas para distribuir clústeres de Ceph gestionados por cephadm. <systemitem class="resource">ceph-salt</systemitem> utiliza la infraestructura de Salt para realizar la gestión del sistema operativo (por ejemplo, las actualizaciones de software o la sincronización horaria) y para definir funciones para los minions de Salt.
  </para>

  <para>
   En el master de Salt, instale el paquete <package>ceph-salt</package>:
  </para>

<screen><prompt>root@master # </prompt>zypper install ceph-salt</screen>

  <para>
   El comando anterior instaló <package>ceph-salt-formula</package> como una dependencia que modifica la configuración del master de Salt insertando archivos adicionales en el directorio <filename>/etc/salt/master.d</filename>. Para aplicar los cambios, reinicie <systemitem class="daemon">salt-master.service</systemitem> y sincronice los módulos de Salt:
  </para>

<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
 </sect1>
 <sect1 xml:id="deploy-cephadm-configure">
  <title>Configuración de las propiedades del clúster</title>

  <para>
   Utilice el comando <command>ceph-salt config</command> para configurar las propiedades básicas del clúster.
  </para>

  <important>
   <para>
    El archivo <filename>/etc/ceph/ceph.conf</filename> se gestiona mediante cephadm y los usuarios <emphasis>no deben</emphasis> editarlo. Los parámetros de configuración de Ceph deben definirse mediante el nuevo comando <command>ceph config</command>. Consulte el <xref linkend="cha-ceph-configuration-db"/> para obtener más información. 
   </para>
  </important>

  <sect2 xml:id="deploy-cephadm-configure-shell">
   <title>Uso de la shell <systemitem class="resource">ceph-salt</systemitem></title>
   <para>
    Si ejecuta <command> config</command> sin ninguna vía o subcomando, deberá introducir una shell <systemitem class="resource">ceph-salt</systemitem>ceph-salt interactiva. La shell es útil si necesita configurar varias propiedades en un lote y no desea escribir la sintaxis completa del comando.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config
<prompt>/&gt;</prompt> ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [no minions]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [no minions]
  |   o- bootstrap ........................................... [no minion]
  |   o- cephadm ............................................ [no minions]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .................................. [ no image path]
  | o- dashboard ................................................... [...]
  | | o- force_password_update ................................. [enabled]
  | | o- password ................................................ [admin]
  | | o- ssl_certificate ....................................... [not set]
  | | o- ssl_certificate_key ................................... [not set]
  | | o- username ................................................ [admin]
  | o- mon_ip .................................................. [not set]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh ............................................... [no key pair set]
  | o- private_key .................................. [no private key set]
  | o- public_key .................................... [no public key set]
  o- time_server ........................... [enabled, no server host set]
    o- external_servers .......................................... [empty]
    o- servers ................................................... [empty]
    o- subnet .................................................. [not set]
</screen>
   <para>
    Como puede ver en el resultado del comando <systemitem class="resource">ceph-salt</systemitem>ls<command> de </command>, la configuración del clúster está organizada en una estructura de árbol. Para configurar una propiedad específica del clúster en la shell <systemitem class="resource">ceph-salt</systemitem>, tiene dos opciones: 
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Ejecute el comando desde la posición actual e introduzca la vía absoluta a la propiedad como primer argumento:
     </para>
<screen>
<prompt>/&gt;</prompt> /cephadm_bootstrap/dashboard ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username .................................................... [admin]
<prompt>/&gt; /cephadm_bootstrap/dashboard/username set ceph-admin</prompt>
Value set.
</screen>
    </listitem>
    <listitem>
     <para>
      Cambie a la vía cuya propiedad necesite configurar y ejecute el comando:
     </para>
<screen>
<prompt>/&gt;</prompt> cd /cephadm_bootstrap/dashboard/
<prompt>/ceph_cluster/minions&gt;</prompt> ls
o- dashboard ....................................................... [...]
  o- force_password_update ..................................... [enabled]
  o- password .................................................... [admin]
  o- ssl_certificate ........................................... [not set]
  o- ssl_certificate_key ....................................... [not set]
  o- username ................................................[ceph-admin]
</screen>
    </listitem>
   </itemizedlist>
   <tip>
    <title>autocompletado de fragmentos de configuración</title>
    <para>
     En una shell <systemitem class="resource">ceph-salt</systemitem>, puede utilizar la función de autocompletado de forma similar a la de una shell de Linux normal (Bash). Completa vías de configuración, subcomandos o nombres de minions de Salt. Para autocompletar una vía de configuración, tiene dos opciones:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Para permitir que la shell termine una vía relativa a su posición actual, pulse la tecla TAB <keycap function="tab"/> dos veces.
      </para>
     </listitem>
     <listitem>
      <para>
       Para permitir que la shell termine una vía absoluta, introduzca <keycap>/</keycap> y pulse la tecla TAB <keycap function="tab"/> dos veces.
      </para>
     </listitem>
    </itemizedlist>
   </tip>
   <tip>
    <title>navegación con las teclas del cursor</title>
    <para>
     Si escribe <command>cd</command> desde la shell <systemitem class="resource">ceph-salt</systemitem> sin ninguna vía, el comando producirá una estructura de árbol de la configuración del clúster con la línea de la vía actual activa. Puede utilizar las teclas de cursor arriba y abajo para desplazarse por cada línea. Después de confirmar con <keycap function="enter"/>, la vía de configuración cambiará a la última activa.
    </para>
   </tip>
   <important>
    <title>convención</title>
    <para>
     Para mantener la coherencia de la documentación, se utilizará una sintaxis de comando única sin entrar en la shell <systemitem class="resource">ceph-salt</systemitem>. Por ejemplo, puede mostrar el árbol de configuración del clúster mediante el comando siguiente:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
   </important>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-minions">
   <title>Adición de minions de Salt</title>
   <para>
    Incluya todos los minions de Salt que se han distribuido y aceptado en la <xref linkend="deploy-salt"/>, o un subconjunto de ellos, en la configuración del clúster de Ceph. Puede especificar los minions de Salt por sus nombres completos o utilizar las expresiones globales &quot;*&quot; y &quot;?&quot; para incluir varios minions de Salt a la vez. Utilice el subcomando <command>add</command> en la vía <literal>/ceph_cluster/minions</literal>. El comando siguiente incluye todos los minions de Salt aceptados:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add '*'</screen>
   <para>
    Compruebe que se han añadido los minions de Salt especificados:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
  o- ses-master.example.com .................................. [no roles]
  o- ses-min1.example.com .................................... [no roles]
  o- ses-min2.example.com .................................... [no roles]
  o- ses-min3.example.com .................................... [no roles]
  o- ses-min4.example.com .................................... [no roles]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-cephadm">
   <title>Especificación de minions de Salt gestionados por cephadm</title>
   <para>
    Especifique qué nodos pertenecerán al clúster de Ceph y se gestionarán mediante cephadm. Incluya todos los nodos que ejecutarán servicios de Ceph, así como el nodo de administración:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add '*'</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-admin">
   <title>Especificación del nodo de administración</title>
   <para>
    El nodo de administración es el nodo en el que se instalan el archivo de configuración <filename>ceph.conf</filename> y el anillo de claves de administración de Ceph. Normalmente, los comandos relacionados con Ceph se ejecutan en el nodo de administración.
   </para>
   <tip>
    <title>master de Salt y nodo de administración en el mismo nodo</title>
    <para>
     En un entorno homogéneo en el que todos o la mayoría de los hosts pertenezcan a SUSE Enterprise Storage, se recomienda tener el nodo de administración en el mismo host que el master de Salt.
    </para>
    <para>
     En un entorno heterogéneo en el que una infraestructura de Salt aloje más de un clúster, por ejemplo, SUSE Enterprise Storage junto con SUSE Manager, <emphasis>no coloque</emphasis> el nodo de administración en el mismo host que el master de Salt.
    </para>
   </tip>
   <para>
    Para especificar el nodo de administración, ejecute el comando siguiente:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin add ses-master.example.com
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/admin ls
o- admin ................................................... [Minions: 1]
  o- ses-master.example.com ...................... [Other roles: cephadm]
</screen>
   <tip>
    <title>instalación de <filename>ceph.conf</filename> y el anillo de claves de administración en varios nodos</title>
    <para>
     Puede instalar el archivo de configuración de Ceph y el anillo de claves del administrador en varios nodos si la distribución lo requiere. Por motivos de seguridad, evite instalarlos en todos los nodos del clúster.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-mon">
   <title>Especificación del primer nodo de MON/MGR</title>
   <para>
    Debe especificar cuál de los minions de Salt del clúster arrancará el clúster. Este minion se convertirá en el primero en ejecutar los servicios de Ceph Monitor y Ceph Manager.
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap set ses-min1.example.com
Value set.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/bootstrap ls
o- bootstrap ..................................... [ses-min1.example.com]
</screen>
   <para>
    Además, debe especificar la dirección IP de carga del monitor en la red pública para asegurarse de que el parámetro <option>public_network</option> está definido correctamente, por ejemplo:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/mon_ip set 192.168.10.20
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-tuned-profiles">
   <title>Especificación de perfiles ajustados</title>
   <para>
    Debe especificar qué minions del clúster tienen perfiles ajustados de forma activa. Para ello, añada estas funciones explícitamente con los comandos siguientes:
   </para>
   <note>
    <para>
     Un minion no puede tener las funciones <literal>latency</literal> (latencia) y <literal>throughput</literal> (rendimiento).
    </para>
   </note>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency add ses-min1.example.com
Adding ses-min1.example.com...
1 minion added.
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput add ses-min2.example.com
Adding ses-min2.example.com...
1 minion added.
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ssh">
   <title>Generación de un par de claves SSH</title>
   <para>
    cephadm utiliza el protocolo SSH para comunicarse con los nodos del clúster. Se crea automáticamente una cuenta de usuario denominada <literal>cephadm</literal> que se utiliza para la comunicación SSH.
   </para>
   <para>
    Debe generar la parte privada y la pública del par de claves SSH:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ssh generate
Key pair generated.
<prompt>root@master # </prompt>ceph-salt config /ssh ls
o- ssh .................................................. [Key Pair set]
  o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-ntp">
   <title>Configuración del servidor de hora</title>
   <para>
    Todos los nodos del clúster deben tener su hora sincronizada con un origen horario fiable. Existen varios escenarios para abordar la sincronización horaria:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Si todos los nodos del clúster están ya configurados para sincronizar su hora mediante un servicio NTP de su elección, inhabilite la gestión del servidor horario:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server disable
</screen>
    </listitem>
    <listitem>
     <para>
      Si su sitio ya tiene una única fuente horaria, especifique el nombre de host de esta:
     </para>
<screen>
 <prompt>root@master # </prompt>ceph-salt config /time_server/servers add <replaceable>time-server.example.com</replaceable>
</screen>
    </listitem>
    <listitem>
     <para>
      Como alternativa, <systemitem class="resource">ceph-salt</systemitem> tiene la capacidad de configurar uno de los minions de Salt para que sirva como servidor horario para el resto del clúster. A veces se hace referencia a esto como un &quot;servidor horario interno&quot;. En este escenario, <systemitem class="resource">ceph-salt</systemitem> configurará el servidor horario interno (que debe ser uno de los minions de Salt) para sincronizar su hora con un servidor horario externo, como <literal>pool.ntp.org</literal>, y configurará todos los demás minions para obtener su hora del servidor horario interno. Esto se puede lograr de la siguiente manera:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/servers add ses-master.example.com
<prompt>root@master # </prompt>ceph-salt config /time_server/external_servers add pool.ntp.org
</screen>
     <para>
      La opción <option>/time_server/subnet</option> especifica la subred desde la que los clientes NTP pueden acceder al servidor NTP. Se define automáticamente al especificar <option>/time_server/servers</option>. Si necesita cambiarlo o especificarlo manualmente, ejecute:
     </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server/subnet set 10.20.6.0/24
</screen>
    </listitem>
   </itemizedlist>
   <para>
    Compruebe los ajustes del servidor horario:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /time_server ls
o- time_server ................................................ [enabled]
  o- external_servers ............................................... [1]
  | o- pool.ntp.org ............................................... [...]
  o- servers ........................................................ [1]
  | o- ses-master.example.com ..................................... [...]
  o- subnet .............................................. [10.20.6.0/24]
</screen>
   <para>
    Encontrará más información sobre cómo configurar la sincronización horaria en <link xlink:href="https://documentation.suse.com/sles/15-SP3/html/SLES-all/cha-ntp.html#sec-ntp-yast"/>.
   </para>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-dashboardlogin">
   <title>Configuración de las credenciales de entrada de Ceph Dashboard</title>
   <para>
    Ceph Dashboard estará disponible después de la distribución del clúster básico. Para acceder a la consola, debe definir un nombre de usuario y una contraseña válidos, por ejemplo:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/username set admin
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/password set <replaceable>PWD</replaceable>
</screen>
   <tip>
    <title>forzar la actualización de la contraseña</title>
    <para>
     Por defecto, el primer usuario de la consola se verá obligado a cambiar su contraseña la primera vez que entre a la consola. Para inhabilitar esta función, ejecute el comando siguiente:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/dashboard/force_password_update disable</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-registry">
   <title>Uso del registro del contenedor</title>
   <para>
    El clúster de Ceph debe tener acceso a un registro del contenedor para que pueda descargar y distribuir servicios de Ceph en contenedor. Existen dos formas de acceder al registro:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Si el clúster puede acceder al registro por defecto en <literal>registry.suse.com</literal> (directamente o mediante un proxy), puede hacer que <systemitem class="resource">ceph-salt</systemitem> apunte directamente a esta URL sin crear un registro local. Para continuar, siga los pasos de la <xref linkend="deploy-cephadm-configure-imagepath"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Si el clúster no puede acceder al registro por defecto, por ejemplo, para una distribución con espacio abierto, deberá configurar un registro de contenedor local. Después de crear y configurar el registro local, deberá hacer que <systemitem class="resource">ceph-salt</systemitem> apunte a él.
     </para>
    </listitem>
   </itemizedlist>
   <sect3 xml:id="updating-ceph-local-registry">
    <title>Creación y configuración del registro local (opcional)</title>
    <important>
     <para>
      Existen numerosos métodos para crear un registro local. Las instrucciones de esta sección son ejemplos para crear registros seguros y no seguros. Para obtener información general sobre cómo ejecutar un registro de imágenes de contenedor, consulte <link xlink:href="https://documentation.suse.com/sles/15-SP3/single-html/SLES-container/#sec-docker-registry-installation"/>.
     </para>
    </important>
    <tip>
     <title>colocación y uso de puertos</title>
     <para>
      Distribuya el registro en un equipo al que puedan acceder todos los nodos del clúster. Recomendamos el nodo de administración. Por defecto, el registro escucha en el puerto 5000.
     </para>
     <para>
      En el nodo de registro, utilice el siguiente comando para asegurarse de que el puerto está libre:
     </para>
<screen>ss -tulpn | grep :5000</screen>
     <para>
      Si otros procesos (como <literal>iscsi-tcmu</literal>) ya están escuchando en el puerto 5000, determine otro puerto libre que se pueda utilizar para asignar al puerto 5000 en el contenedor de registro.
     </para>
    </tip>
    <procedure>
     <title>Creación del registro local</title>
     <step>
      <para>
       Verifique que la extensión <package>Containers Module</package> esté habilitada:
      </para>
<screen>
<prompt>&gt; </prompt>SUSEConnect --list-extensions | grep -A2 "Containers Module"
Containers Module 15 SP3 x86_64 (Activated)
</screen>
     </step>
     <step>
      <para>
       Verifique que los siguientes paquetes están instalados: <package>apache2-utils</package> (si se habilita un registro seguro), <package>cni</package>, <package>cni-plugins</package>, <package>podman</package>, <package>podman-cni-config</package> y <package>skopeo</package>.
      </para>
     </step>
     <step>
      <para>
       Recopile la siguiente información:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Nombre de dominio completo del host de registro (<option>REG_HOST_FQDN</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Un número de puerto disponible utilizado para asignar al puerto del contenedor de registro 5000 (<option>REG_HOST_PORT</option>).
        </para>
       </listitem>
       <listitem>
        <para>
         Si el registro será seguro o no seguro (<option>insecure=[true|false]</option>).
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Para iniciar un registro no seguro (sin cifrado SSL), siga estos pasos:
      </para>
      <substeps>
       <step>
        <para>
         Configure <systemitem class="resource">ceph-salt</systemitem> para el registro no seguro:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf enable
<prompt>cephuser@adm &gt; </prompt>ceph-salt config containers/registries_conf/registries \
 add prefix=<option>REG_HOST_FQDN</option> insecure=true \
 location=<option>REG_HOST_PORT</option>:5000
</screen>
       </step>
       <step>
        <para>
         Inicie el registro no seguro creando el directorio necesario (por ejemplo, <filename>/var/lib/registry</filename>) e inicie el registro con el comando <command>podman</command>:
        </para>
<screen>
<prompt role="root"># </prompt>mkdir -p /var/lib/registry
<prompt role="root"># </prompt>podman run --privileged -d --name registry \
 -p <option>REG_HOST_PORT</option>:5000 -v /var/lib/registry:/var/lib/registry \
 --restart=always registry:2
</screen>
       </step>
       <step>
        <para>
         Para que el registro se inicie después de rearrancar, cree un archivo de unidad <systemitem class="daemon">systemd</systemitem> para él y habilítelo:
        </para>
<screen>
<prompt>&gt; </prompt><command>sudo</command> podman generate systemd --files --name registry
<prompt>&gt; </prompt><command>sudo</command> mv container-registry.service /etc/systemd/system/
<prompt>&gt; </prompt><command>sudo</command> systemctl enable container-registry.service
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Para iniciar un registro seguro, siga estos pasos:
      </para>
      <substeps>
       <step>
        <para>
         Cree los directorios necesarios:
        </para>
<screen><prompt role="root"># </prompt>mkdir -p /var/lib/registry/{auth,certs}</screen>
       </step>
       <step>
        <para>
         Genere un certificado SSL:
        </para>
<screen>
<prompt role="root"># </prompt>openssl req -newkey rsa:4096 -nodes -sha256 \
 -keyout /var/lib/registry/certs/domain.key -x509 -days 365 \
 -out /var/lib/registry/certs/domain.crt
</screen>
        <note>
         <para>
          Defina el valor <literal>CN=[valor]</literal> en el nombre de dominio completo del host (<option>[REG_HOST_FQDN]</option>).
         </para>
        </note>
       </step>
       <step>
        <para>
         Copie el certificado en todos los nodos del clúster y actualice el caché de certificados:
        </para>
<screen>
<prompt role="root"># </prompt>salt-cp '*' /var/lib/registry/certs/domain.crt \
 /etc/pki/trust/anchors/
<prompt role="root"># </prompt>salt '*' cmd.shell "update-ca-certificates"
</screen>
       </step>
       <step>
        <para>
         Genere una combinación de nombre de usuario y contraseña para la autenticación en el registro:
        </para>
<screen>
<prompt role="root"># </prompt>htpasswd2 -bBc /var/lib/registry/auth/htpasswd \
 <option>REG_USERNAME</option> <option>REG_PASSWORD</option>
</screen>
       </step>
       <step>
        <para>
         Inicie el registro seguro. Utilice el indicador <option>REGISTRY_STORAGE_DELETE_ENABLED=true</option> para poder suprimir las imágenes posteriormente con el comando <command>skopeo delete</command>.
        </para>
<screen>
podman run --name myregistry -p <option>REG_HOST_PORT</option>:5000 \
 -v /var/lib/registry:/var/lib/registry \
 -v /var/lib/registry/auth:/auth:z \
 -e "REGISTRY_AUTH=htpasswd" \
 -e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \
 -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd \
 -v /var/lib/registry/certs:/certs:z \
 -e "REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt" \
 -e "REGISTRY_HTTP_TLS_KEY=/certs/domain.key" \
 -e REGISTRY_STORAGE_DELETE_ENABLED=true \
 -e REGISTRY_COMPATIBILITY_SCHEMA1_ENABLED=true -d registry:2
</screen>
       </step>
       <step>
        <para>
         Pruebe el acceso seguro al registro:
        </para>
<screen>
<prompt>&gt; </prompt>curl https://<option>REG_HOST_FQDN</option>:<option>REG_HOST_PORT</option>/v2/_catalog \
 -u <option>REG_USERNAME</option>:<option>REG_PASSWORD</option>
</screen>
       </step>
      </substeps>
     </step>
     <step>
      <para>
       Después de crear el registro local, debe sincronizar las imágenes de contenedor desde el registro oficial de SUSE en <literal>registry.suse.com</literal> con el registro local. Puede utilizar el comando <command>skopeo sync</command> que se encuentra en el paquete <package>skopeo</package> para ese fin. Para obtener más información, consulte la página de manual (<command>man 1 skopeo-sync</command>). Considere los siguientes ejemplos:
      </para>
      <example>
       <title>Visualización de archivos de manifiesto</title>
<screen>
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/ceph | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/grafana | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 | jq .RepoTags
skopeo inspect docker://registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 | jq .RepoTags
</screen>
      </example>
      <example>
       <title>Sincronización con un directorio</title>
       <para>
        Sincroniza todas las imágenes de Ceph:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph /root/images/</screen>
       <para>
        Sincroniza solo las imágenes más recientes:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/ceph:latest /root/images/</screen>
      </example>
      <example>
       <title>Sincronización de imágenes de Grafana</title>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana /root/images/</screen>
       <para>
        Sincroniza solo las imágenes de Grafana más recientes:
       </para>
<screen>skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/grafana:latest /root/images/</screen>
      </example>
      <example>
       <title>Sincronización de las imágenes más recientes de Prometheus</title>
<screen>
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-server:2.32.1 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-node-exporter:1.1.2 /root/images/
skopeo sync --src docker --dest dir registry.suse.com/ses/7.1/ceph/prometheus-alertmanager:0.21.0 /root/images/
</screen>
      </example>
     </step>
    </procedure>
    <procedure>
     <title>Configuración del registro local y las credenciales de acceso</title>
     <step>
      <para>
       Configure la URL del registro local:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/registry set <replaceable>REG_HOST_URL</replaceable></screen>
     </step>
     <step>
      <para>
       Configure el nombre de usuario y la contraseña para acceder al registro local:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/username set <replaceable>REG_USERNAME</replaceable></screen>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /containers/registry_auth/password set <replaceable>REG_PASSWORD</replaceable></screen>
     </step>
    </procedure>
    <tip>
     <title>caché de registro</title>
     <para>
      Para evitar que se deba sincronizar de nuevo el registro local cuando aparezcan nuevos contenedores actualizados, puede configurar un <emphasis>caché de registro</emphasis>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="deploy-cephadm-configure-imagepath">
    <title>Configuración de la vía a las imágenes de contenedor</title>
    <important>
     <para>
      Esta sección ayuda a configurar la vía a las imágenes de contenedor del clúster de bootstrap (distribución del primer par de Ceph Monitor y Ceph Manager). La vía no se aplica a las imágenes de contenedor de servicios adicionales, por ejemplo, la pila de supervisión.
     </para>
    </important>
    <tip>
     <title>Configuración del proxy HTTPS</title>
     <para>
      Si necesita utilizar un servidor proxy para comunicarse con el servidor de registro del contenedor, realice los siguientes pasos de configuración en todos los nodos del clúster:
     </para>
     <procedure>
      <step>
       <para>
        Copie el archivo de configuración de los contenedores:
       </para>
<screen><prompt>&gt; </prompt><command>sudo</command> cp /usr/share/containers/containers.conf /etc/containers/containers.conf</screen>
      </step>
      <step>
       <para>
        Edite el archivo recién copiado y añada el ajuste <option>http_proxy</option> a su sección <literal>[engine]</literal>, por ejemplo:
       </para>
<screen><prompt>&gt; </prompt>cat /etc/containers/containers.conf
 [engine]
 http_proxy=proxy.example.com
 [...]
 </screen>
      </step>
     </procedure>
    </tip>
    <para>
     cephadm necesita conocer una vía de URI válida a las imágenes del contenedor. Verifique el ajuste por defecto ejecutando:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path ls</screen>
    <para>
     Si no necesita un registro alternativo o local, especifique el registro de contenedor de SUSE por defecto:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set registry.suse.com/ses/7.1/ceph/ceph</screen>
    <para>
     Si la distribución requiere una vía específica, por ejemplo, una vía a un registro local, configúrela de la siguiente manera:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_image_path set <replaceable>LOCAL_REGISTRY_PATH</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-inflight-encryption">
   <title>Habilitación del cifrado en curso de datos (msgr2)</title>
   <para>
    El protocolo Messenger v2 (MSGR2) es el protocolo con conexión de Ceph. Proporciona un modo de seguridad que cifra todos los datos que pasan por la red, encapsula las cargas útiles de autenticación y permite la integración futura de nuevos modos de autenticación (como Kerberos).
   </para>
   <important>
    <para>
     msgr2 no es compatible actualmente con los clientes de Ceph del kernel de Linux, como CephFS y el dispositivo de bloques RADOS.
    </para>
   </important>
   <para>
    Los daemons de Ceph se pueden asociar a varios puertos, lo que permite que tanto los clientes de Ceph legados como los nuevos clientes compatibles con v2 se conecten al mismo clúster. Por defecto, los MON se asocian ahora al nuevo puerto 3300 asignado por IANA (CE4h o 0xCE4) para el nuevo protocolo v2, y además se asocian al antiguo puerto por defecto 6789 para el protocolo legado v1.
   </para>
   <para>
    El protocolo v2 (MSGR2) admite dos modos de conexión:
   </para>
   <variablelist>
    <varlistentry>
     <term>crc mode</term>
     <listitem>
      <para>
       Una autenticación inicial sólida cuando se establece la conexión y una comprobación de integridad CRC32C.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>secure mode</term>
     <listitem>
      <para>
       Una autenticación inicial sólida cuando se establece la conexión y un cifrado completo de todo el tráfico posterior a la autenticación, incluida una comprobación de la integridad criptográfica.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para la mayoría de las conexiones, existen opciones que controlan los modos que se utilizan:
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_cluster_mode</term>
     <listitem>
      <para>
       El modo de conexión (o modos permitidos) utilizado para la comunicación dentro del clúster entre los daemons de Ceph. Si se muestran varios modos, se prefieren los que aparecen en primer lugar.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_service_mode</term>
     <listitem>
      <para>
       Una lista de los modos permitidos que pueden utilizar los clientes al conectarse al clúster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_client_mode</term>
     <listitem>
      <para>
       Una lista de modos de conexión, en orden de preferencia, que los clientes pueden utilizar (o permitir) al comunicarse con un clúster de Ceph.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Existe un conjunto paralelo de opciones que se aplican específicamente a los monitores, lo que permite a los administradores establecer distintos requisitos (normalmente más seguros) en la comunicación con esos monitores.
   </para>
   <variablelist>
    <varlistentry>
     <term>ms_mon_cluster_mode</term>
     <listitem>
      <para>
       El modo de conexión (o modos permitidos) que se utilizará entre monitores.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_service_mode</term>
     <listitem>
      <para>
       Una lista de modos permitidos para que los clientes u otros daemons de Ceph los utilicen al conectarse a los monitores.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ms_mon_client_mode</term>
     <listitem>
      <para>
       Una lista de modos de conexión, en orden de preferencia, para que los clientes o los daemons que no sean de monitor utilicen al conectarse a los monitores.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Para habilitar el modo de cifrado MSGR2 durante la distribución, debe añadir algunas opciones a la configuración de <systemitem class="resource">ceph-salt</systemitem> antes de ejecutar <command>ceph-salt apply</command>.
   </para>
   <para>
    Para utilizar el modo <literal>secure</literal>, ejecute los comandos siguientes.
   </para>
   <para>
    Añada la sección global a <filename>ceph_conf</filename> en la herramienta de configuración <systemitem class="resource">ceph-salt</systemitem>:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global</screen>
   <para>
    Defina las opciones siguientes:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode "secure crc"
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode "secure crc"
</screen>
   <note>
    <para>
     Asegúrese de que <literal>secure</literal> vaya delante de <literal>crc</literal>.
    </para>
   </note>
   <para>
    Para <emphasis>forzar </emphasis> <literal>el modo seguro</literal>, ejecute los comandos siguientes:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_cluster_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_service_mode secure
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set ms_client_mode secure
</screen>
   <tip xml:id="update-inflight-encryption-settings">
    <title>actualización de ajustes</title>
    <para>
     Si desea cambiar alguno de los ajustes anteriores, defina los cambios de configuración en el almacén de configuración del monitor. Esto se consigue mediante el comando <command>ceph config set</command>.
    </para>
<screen><prompt>root@master # </prompt>ceph config set global <replaceable>CONNECTION_OPTION</replaceable> <replaceable>CONNECTION_MODE</replaceable> [--force]</screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global ms_cluster_mode "secure crc"</screen>
    <para>
     Si desea comprobar el valor actual, incluido el valor por defecto, ejecute el comando siguiente:
    </para>
<screen><prompt>root@master # </prompt>ceph config get <replaceable>CEPH_COMPONENT</replaceable> <replaceable>CONNECTION_OPTION</replaceable></screen>
    <para>
     Por ejemplo, para obtener el modo <literal>ms_cluster_mode</literal> para los OSD, ejecute:
    </para>
<screen><prompt>root@master # </prompt>ceph config get osd ms_cluster_mode</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-enable-network">
   <title>Configuración de la red del clúster</title>
   <para>
    Opcionalmente, si ejecuta una red de clúster independiente, es posible que deba definir la dirección IP de la red del clúster seguida de la parte de la máscara de subred después de la barra inclinada, por ejemplo: <literal>192.168.10.22/24</literal>.
   </para>
   <para>
    Ejecute los comandos siguientes para habilitar <literal>cluster_network</literal>:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf add global
<prompt>root@master # </prompt>ceph-salt config /cephadm_bootstrap/ceph_conf/global set cluster_network <replaceable>NETWORK_ADDR</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-verify">
   <title>Verificación de la configuración del clúster</title>
   <para>
    La configuración mínima del clúster ha finalizado. Examínela en busca de errores obvios:
   </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config ls
o- / ............................................................... [...]
  o- ceph_cluster .................................................. [...]
  | o- minions .............................................. [Minions: 5]
  | | o- ses-master.example.com .................................. [admin]
  | | o- ses-min1.example.com ......................... [bootstrap, admin]
  | | o- ses-min2.example.com ................................. [no roles]
  | | o- ses-min3.example.com ................................. [no roles]
  | | o- ses-min4.example.com ................................. [no roles]
  | o- roles ....................................................... [...]
  |   o- admin .............................................. [Minions: 2]
  |   | o- ses-master.example.com ....................... [no other roles]
  |   | o- ses-min1.example.com ................. [other roles: bootstrap]
  |   o- bootstrap ................................ [ses-min1.example.com]
  |   o- cephadm ............................................ [Minions: 5]
  |   o- tuned ..................................................... [...]
  |     o- latency .......................................... [no minions]
  |     o- throughput ....................................... [no minions]
  o- cephadm_bootstrap ............................................. [...]
  | o- advanced .................................................... [...]
  | o- ceph_conf ................................................... [...]
  | o- ceph_image_path .............. [registry.suse.com/ses/7.1/ceph/ceph]
  | o- dashboard ................................................... [...]
  |   o- force_password_update ................................. [enabled]
  |   o- password ................................... [randomly generated]
  |   o- username ................................................ [admin]
  | o- mon_ip ............................................ [192.168.10.20]
  o- containers .................................................... [...]
  | o- registries_conf ......................................... [enabled]
  | | o- registries .............................................. [empty]
  | o- registry_auth ............................................... [...]
  |   o- password .............................................. [not set]
  |   o- registry .............................................. [not set]
  |   o- username .............................................. [not set]
  o- ssh .................................................. [Key Pair set]
  | o- private_key ..... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  | o- public_key ...... [53:b1:eb:65:d2:3a:ff:51:6c:e2:1b:ca:84:8e:0e:83]
  o- time_server ............................................... [enabled]
    o- external_servers .............................................. [1]
    | o- 0.pt.pool.ntp.org ......................................... [...]
    o- servers ....................................................... [1]
    | o- ses-master.example.com .................................... [...]
    o- subnet ............................................. [10.20.6.0/24]
</screen>
   <tip>
    <title>estado de la configuración del clúster</title>
    <para>
     Puede comprobar si la configuración del clúster es válida ejecutando el comando siguiente:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt status
cluster: 5 minions, 0 hosts managed by cephadm
config: OK
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-configure-export">
   <title>Exportación de configuraciones de clúster</title>
   <para>
    Después de haber configurado el clúster básico y comprobar que su configuración es válida, es recomendable exportar la configuración a un archivo:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt export &gt; cluster.json</screen>
   <warning>
    <para>
     El resultado de <command>ceph-salt export</command> incluye la clave privada SSH. Si le preocupan las implicaciones de seguridad, no ejecute este comando sin tomar las precauciones oportunas.
    </para>
   </warning>
   <para>
    En caso de que se interrumpa la configuración del clúster y necesite volver a un estado de copia de seguridad, ejecute:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt import cluster.json</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-deploy">
  <title>Actualización de nodos y clúster mínimo de arranque</title>

  <para>
   Antes de distribuir el clúster, actualice todos los paquetes de software en todos los nodos:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update</screen>

  <para>
   Si un nodo informa de que <literal>es necesario reiniciar</literal> durante la actualización, los paquetes del sistema operativo importantes, como el kernel, se actualizaron a una versión más reciente y es necesario reiniciar el nodo para aplicar los cambios.
  </para>

  <para>
   Para reiniciar todos los nodos que requieren reinicio, añada la opción <option>--reboot</option>
  </para>

<screen><prompt>root@master # </prompt>ceph-salt update --reboot</screen>

  <para>
   O bien, reinícielos en un paso independiente:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt reboot</screen>

  <important>
   <para>
    El master de Salt nunca se reinicia mediante los comandos <command>ceph-salt update --reboot</command> o <command>ceph-salt reboot</command>. Si es necesario reiniciar el master de Salt, deberá hacerlo manualmente.
   </para>
  </important>

  <para>
   Después de actualizar los nodos, arranque el clúster mínimo:
  </para>

<screen><prompt>root@master # </prompt>ceph-salt apply</screen>

  <note>
   <para>
    Cuando se complete el proceso, el clúster tendrá un monitor Ceph Monitor y una instancia de Ceph Manager.
   </para>
  </note>

  <para>
   El comando anterior abrirá una interfaz de usuario interactiva que muestra el progreso actual de cada minion.
  </para>

  <figure>
   <title>Distribución de un clúster mínimo</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="cephadm_deploy.png" width="75%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <tip>
   <title>modo no interactivo</title>
   <para>
    Si necesita aplicar la configuración desde un guion, también existe un modo de distribución no interactivo. Esto también resulta útil cuando se distribuye el clúster desde un equipo remoto, ya que la actualización constante de la información de progreso en la pantalla a través de la red puede resultar una distracción:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt apply --non-interactive</screen>
  </tip>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-final-steps">
  <title>Revisión de los pasos finales</title>

  <para>
   Después de que se haya completado el comando <command>ceph-salt apply</command>, debe tener un monitor de Ceph Monitor y una instancia de Ceph Manager. Debería poder ejecutar el comando <command>ceph status</command> correctamente en cualquiera de los minions a los que se les haya asignado la función de <literal>administrador</literal> como usuario <literal>root</literal> o como usuario <literal>cephadm</literal> mediante <literal>sudo</literal>.
  </para>

  <para>
   Los siguientes pasos implican el uso de cephadm para distribuir Ceph Monitor, Ceph Manager, los OSD, la pila de supervisión y pasarelas adicionales.
  </para>

  <para>
   Antes de continuar, revise la configuración de red del nuevo clúster. En este punto, el ajuste <literal>public_network</literal> se ha completado en función de lo que se haya introducido para <literal>/cephadm_bootstrap/mon_ip</literal> en la configuración de <literal>ceph-salt</literal>. Sin embargo, este ajuste solo se ha aplicado a Ceph Monitor. Puede revisar este ajuste con el comando siguiente:
  </para>

<screen><prompt>root@master # </prompt>ceph config get mon public_network</screen>

  <para>
   Esto es lo mínimo que Ceph requiere para funcionar, pero se recomienda convertir este ajuste <literal>public_network</literal> como <literal>global</literal>, lo que significa que se aplicará a todos los tipos de daemons de Ceph, y no solo a los MON:
  </para>

<screen><prompt>root@master # </prompt>ceph config set global public_network "$(ceph config get mon public_network)"</screen>

  <note>
   <para>
    Este paso no es obligatorio. Sin embargo, si no utiliza este ajuste, los OSD de Ceph y otros daemons (excepto Ceph Monitor) escucharán en <emphasis>todas las direcciones</emphasis>.
   </para>
   <para>
    Si desea que los OSD se comuniquen entre sí mediante una red completamente independiente, ejecute el comando siguiente:
   </para>
<screen><prompt>root@master # </prompt>ceph config set global cluster_network "<replaceable>cluster_network_in_cidr_notation</replaceable>"</screen>
   <para>
    La ejecución de este comando garantizará que los OSD creados en la distribución utilicen la red de clúster deseada desde el principio.
   </para>
  </note>

  <para>
   Si el clúster está configurado para tener nodos densos (más de 62 OSD por host), asegúrese de asignar suficientes puertos para los OSD de Ceph. El rango por defecto (6800-7300) no permite actualmente más de 62 OSD por host. Para un clúster con nodos densos, ajuste <literal>ms_bind_port_max</literal> con un valor adecuado. Cada OSD consumirá ocho puertos adicionales. Por ejemplo, si se configura un host para que ejecute 96 OSD, se necesitarán 768 puertos. <literal>ms_bind_port_max</literal> debe definirse al menos en 7568 ejecutando el comando siguiente:
  </para>

<screen><prompt>root@master # </prompt>ceph config set osd.* ms_bind_port_max 7568</screen>

  <para>
   Deberá ajustar la configuración del cortafuegos en consecuencia para que esto funcione. Consulte el <xref linkend="storage-bp-net-firewall"/> para obtener más información. 
  </para>
 </sect1>
 <sect1 xml:id="deploy-min-cluster-disable-insecure">
  <title>Inhabilitación de clientes no seguros</title>

  <para>
   Desde Pacific 15.2.11, se ha introducido una nueva advertencia de actividad que informa de que se permite a los clientes no seguros unirse al clúster. Esta advertencia está <emphasis>activada</emphasis> por defecto. Ceph Dashboard mostrará el clúster con el estado <literal>HEALTH_WARN</literal> y, al verificar el estado del clúster en la línea de comandos, obtendrá la siguiente información:
  </para>

<screen>
<prompt>cephuser@adm &gt; </prompt>ceph status
cluster:
  id:     3fe8b35a-689f-4970-819d-0e6b11f6707c
  health: HEALTH_WARN
  mons are allowing insecure global_id reclaim
[...]
</screen>

  <para>
   Esta advertencia significa que los monitores Ceph Monitor siguen permitiendo que los clientes antiguos sin parches aplicados se conecten al clúster. Esto garantiza que los clientes existentes puedan conectarse mientras se actualiza el clúster, pero le advierte de que hay un problema que debe solucionarse. Cuando el clúster y todos los clientes se actualicen a la versión más reciente de Ceph, deje de permitir el acceso a los clientes que no tengan los parches aplicados ejecutando el siguiente comando:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon auth_allow_insecure_global_id_reclaim false</screen>
 </sect1>
</chapter>
