<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deploy_core.xml" version="5.0" xml:id="deploy-core">
 <info>
  <title>Distribución de los servicios principales restantes mediante cephadm</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Después de distribuir el clúster de Ceph básico, distribuya los servicios principales a más nodos del clúster. Para que los clientes puedan acceder a los datos del clúster, distribuya también servicios adicionales.
 </para>
 <para>
  Actualmente, se admite la distribución de servicios de Ceph en la línea de comandos mediante Ceph Orchestrator (subcomandos de <command>ceph orch</command>).
 </para>
 <sect1 xml:id="deploy-cephadm-day2-orch">
  <title>El comando <command>ceph orch</command></title>

  <para>
   El comando <command>ceph orch</command> de Ceph Orchestrator, que es una interfaz para el módulo cephadm, se encarga de mostrar los componentes del clúster y de distribuir los servicios de Ceph en los nuevos nodos del clúster.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-orch-status">
   <title>Visualización del estado del orquestador</title>
   <para>
    El comando siguiente muestra el modo y el estado actuales de Ceph Orchestrator.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch status</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-orch-list">
   <title>Listado de dispositivos, servicios y daemons</title>
   <para>
    Para mostrar todos los dispositivos del disco, ejecute lo siguiente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
Hostname   Path      Type  Serial  Size   Health   Ident  Fault  Available
ses-master /dev/vdb  hdd   0d8a... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdc  hdd   8304... 10.7G  Unknown  N/A    N/A    No
ses-min1   /dev/vdd  hdd   7b81... 10.7G  Unknown  N/A    N/A    No
[...]
</screen>
   <tip>
    <title>servicios y daemons</title>
    <para>
     Un <emphasis>servicio</emphasis> es un término general que se usa para un servicio de Ceph de un tipo específico, por ejemplo, Ceph Manager.
    </para>
    <para>
     Un <emphasis>daemon</emphasis> es una instancia específica de un servicio, por ejemplo, un proceso <literal>mgr.ses-min1.gdlcik</literal> que se ejecuta en un nodo llamado <literal>ses-min1</literal>.
    </para>
   </tip>
   <para>
    Para mostrar todos los servicios conocidos por cephadm, ejecute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls
NAME  RUNNING  REFRESHED  AGE  PLACEMENT  IMAGE NAME                  IMAGE ID
mgr       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
mon       1/0  5m ago     -    &lt;no spec&gt;  registry.example.com/[...]  5bf12403d0bd
</screen>
   <tip>
    <para>
     Puede limitar la lista a los servicios de un nodo concreto con el parámetro opcional <option>‑‑host</option>, y los servicios de un tipo concreto con el parámetro opcional <option>--service-type</option>. Los tipos admitidos son <literal>mon</literal>, <literal>osd</literal>, <literal>mgr</literal>, <literal>mds</literal> y <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    Para mostrar todos los daemons en ejecución distribuidos por cephadm, ejecute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps
NAME            HOST     STATUS   REFRESHED AGE VERSION    IMAGE ID     CONTAINER ID
mgr.ses-min1.gd ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd b8104e09814c
mon.ses-min1    ses-min1 running) 8m ago    12d 15.2.0.108 5bf12403d0bd a719e0087369
</screen>
   <tip>
    <para>
     Para consultar el estado de un daemon concreto, utilice <option>--daemon_type</option> y <option>--daemon_id</option>. Para los OSD, el ID es el ID de OSD numérico. Para MDS, el ID es el nombre del de archivos:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd --daemon_id 0
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type mds --daemon_id my_cephfs
</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="cephadm-service-and-placement-specs">
  <title>Especificación del servicio y la colocación</title>

  <para>
   La forma recomendada de especificar la distribución de los servicios de Ceph es crear un archivo con formato YAML con la especificación de los servicios que desea distribuir.
  </para>

  <sect2 xml:id="cephadm-service-spec">
   <title>Creación de especificaciones de servicio</title>
   <para>
    Puede crear un archivo de especificación independiente para cada tipo de servicio, por ejemplo:
   </para>
<screen>
<prompt>root@master # </prompt>cat nfs.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <para>
    Como alternativa, puede especificar varios tipos de servicios, o todos ellos en un archivo (por ejemplo, <filename>cluster.yml</filename>) que describe qué nodos ejecutarán servicios específicos. Recuerde separar los tipos de servicios individuales con tres guiones (<literal>---</literal>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
---
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
---
[...]
</screen>
   <para>
    Las propiedades mencionadas anteriormente tienen el siguiente significado:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>service_type</literal></term>
     <listitem>
      <para>
       El tipo de servicio. Puede ser un servicio de Ceph (<literal>mon</literal>, <literal>mgr</literal>, <literal>mds</literal>, <literal>crash</literal>, <literal>osd</literal> o <literal>rbd-mirror</literal>), una pasarela (<literal>nfs</literal> o <literal>rgw</literal>) o parte de la pila de supervisión (<literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> o <literal>prometheus</literal>).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>service_id</literal></term>
     <listitem>
      <para>
       El nombre del servicio. Las especificaciones de tipo <literal>mon</literal>, <literal>mgr</literal>, <literal>alertmanager</literal>, <literal>grafana</literal>, <literal>node-exporter</literal> y <literal>prometheus</literal> no requieren la propiedad <literal>service_id</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>placement</literal></term>
     <listitem>
      <para>
       Especifica qué nodos ejecutarán el servicio. Consulte la <xref linkend="cephadm-placement-specs"/> para obtener más información.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>spec</literal></term>
     <listitem>
      <para>
       Especificación adicional relevante para el tipo de servicio.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>aplicación de servicios específicos</title>
    <para>
     Los servicios del clúster de Ceph suelen tener varias propiedades específicas. Para obtener ejemplos y detalles de cada especificación de servicios, consulte la <xref linkend="deploy-cephadm-day2-services"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="cephadm-placement-specs">
   <title>Creación de especificaciones de colocación</title>
   <para>
    Para distribuir los servicios de Ceph, cephadm necesita saber en qué nodos debe distribuirlos. Utilice la propiedad <literal>placement</literal> y muestre los nombres de host cortos de los nodos a los que se aplica el servicio:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cat cluster.yml
[...]
placement:
  hosts:
  - host1
  - host2
  - host3
[...]
</screen>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs">
   <title>Aplicación de la especificación del clúster</title>
   <para>
    Después de crear un archivo <filename>cluster.yml</filename> completo con las especificaciones de todos los servicios y su colocación, puede aplicar el clúster ejecutando el comando siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yml</screen>
   <para>
    Para ver el estado del clúster, ejecute el comando <command>ceph orch status</command>. Para obtener más información, consulte la <xref linkend="deploy-cephadm-day2-orch-status"/>.
   </para>
  </sect2>

  <sect2 xml:id="cephadm-apply-cluster-specs-">
   <title>Exportación de la especificación de un clúster en ejecución</title>
   <para>
    Aunque ha distribuido servicios al clúster de Ceph mediante los archivos de especificación como se describe en la <xref linkend="cephadm-service-and-placement-specs"/>, la configuración del clúster puede diferir de la original durante su funcionamiento. Además, es posible que haya eliminado los archivos de especificación accidentalmente.
   </para>
   <para>
    Para recuperar una especificación completa de un clúster en ejecución, ejecute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ls --export
placement:
  hosts:
  - hostname: ses-min1
    name: ''
    network: ''
service_id: my_cephfs
service_name: mds.my_cephfs
service_type: mds
---
placement:
  count: 2
service_name: mgr
service_type: mgr
---
[...]
</screen>
   <tip>
    <para>
     Puede añadir la opción <option>--format</option> para cambiar el formato de salida <literal>yaml</literal> por defecto. Puede seleccionar <literal>json</literal>, <literal>json-pretty</literal> o <literal>yaml</literal>. Por ejemplo:
    </para>
<screen>ceph orch ls --export --format json</screen>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-services">
  <title>Distribución de servicios de Ceph</title>

  <para>
   Cuando el clúster básico esté en ejecución, puede distribuir los servicios de Ceph a nodos adicionales.
  </para>

  <sect2 xml:id="deploy-cephadm-day2-service-mon">
   <title>Distribución de monitores Ceph Monitor y gestores Ceph Manager</title>
   <para>
    El clúster de Ceph tiene tres o cinco MON distribuidos en distintos nodos. Si hay cinco o más nodos en el clúster, se recomienda distribuir cinco MON. Una práctica recomendada consiste en distribuir los MGR en los mismos nodos que los MON.
   </para>
   <important>
    <title>inclusión de MON de arranque</title>
    <para>
     Al distribuir MON y MGR, recuerde incluir el primer MON que haya añadido al configurar el clúster básico en la <xref linkend="deploy-cephadm-configure-mon"/>.
    </para>
   </important>
   <para>
    Para distribuir MON, aplique la siguiente especificación:
   </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <note>
    <para>
     Si necesita añadir otro nodo, añada el nombre de host al final de la misma lista de YAML. Por ejemplo:
    </para>
<screen>
service_type: mon
placement:
 hosts:
 - ses-min1
 - ses-min2
 - ses-min3
 - ses-min4
</screen>
   </note>
   <para>
    Del mismo modo, para distribuir MGR, aplique la siguiente especificación:
   </para>
   <important>
    <para>
     Asegúrese de que la distribución tenga al menos tres Ceph Manager en cada distribución.
    </para>
   </important>
<screen>
service_type: mgr
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <tip>
    <para>
     Si los MON o MGR <emphasis>no</emphasis> se encuentran en la misma subred, deberá añadir las direcciones de subred al final. Por ejemplo:
    </para>
<screen>
service_type: mon
placement:
  hosts:
  - ses-min1:10.1.2.0/24
  - ses-min2:10.1.5.0/24
  - ses-min3:10.1.10.0/24
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-osd">
   <title>Distribución de los OSD de Ceph</title>
   <important>
    <title>cuando el dispositivo de almacenamiento está disponible</title>
    <para>
     Un dispositivo de almacenamiento se considera que está <emphasis>disponible</emphasis> si se cumplen todas las condiciones siguientes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       El dispositivo no tiene particiones.
      </para>
     </listitem>
     <listitem>
      <para>
       El dispositivo no tiene ningún estado de LVM.
      </para>
     </listitem>
     <listitem>
      <para>
       El dispositivo no está montado.
      </para>
     </listitem>
     <listitem>
      <para>
       El dispositivo no contiene un sistema de archivos.
      </para>
     </listitem>
     <listitem>
      <para>
       El dispositivo no contiene un OSD de BlueStore.
      </para>
     </listitem>
     <listitem>
      <para>
       El dispositivo tiene más de 5 GB.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Si no se cumplen las condiciones anteriores, Ceph se niega a aprovisionar dichos OSD.
    </para>
   </important>
   <para>
    Hay dos formas de distribuir los OSD:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Indique a Ceph que consuma todos los dispositivos de almacenamiento disponibles y no utilizados:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd --all-available-devices</screen>
    </listitem>
    <listitem>
     <para>
      Utilice DriveGroups (consulte el <xref linkend="drive-groups"/>) para crear una especificación de OSD que describa los dispositivos que se distribuirán en función de sus propiedades, como el tipo de dispositivo (SSD o HDD), el nombre de modelo del dispositivo, el tamaño o los nodos en los que existen los dispositivos. A continuación, aplique la especificación ejecutando el comando siguiente:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i drive_groups.yml</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-mds">
   <title>Distribución de servidores de metadatos</title>
   <para>
    CephFS requiere uno o varios servicios de servidor de metadatos (MDS). Para crear un CephFS, primero cree servidores MDS aplicando la siguiente especificación:
   </para>
   <note>
    <para>
     Asegúrese de tener al menos dos repositorios creados, uno para los datos de CephFS y otro para los metadatos de CephFS, antes de aplicar la siguiente especificación.
    </para>
   </note>
<screen>
service_type: mds
service_id: <replaceable>CEPHFS_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
   <para>
    Después de que los MDS sean funcionales, cree el CephFS:
   </para>
<screen>ceph fs new <replaceable>CEPHFS_NAME</replaceable> <replaceable>metadata_pool</replaceable> <replaceable>data_pool</replaceable></screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-ogw">
   <title>Distribución de pasarelas Object Gateways</title>
   <para>
    cephadm distribuye una pasarela Object Gateway como una colección de daemons que gestionan un <emphasis>dominio</emphasis> y una <emphasis>zona</emphasis> concretos.
   </para>
   <para>
    Puede relacionar un servicio de Object Gateway con un dominio y una zona ya existentes (consulte el <xref linkend="ceph-rgw-fed"/> para obtener más información), o puede especificar un <replaceable>REALM_NAME</replaceable> y <replaceable>ZONE_NAME</replaceable> que no existan y se crearán automáticamente después de aplicar la siguiente configuración:
   </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
</screen>
   <sect3 xml:id="cephadm-deploy-using-secure-ssl-access">
    <title>Uso del acceso SSL seguro</title>
    <para>
     Para utilizar una conexión SSL segura con Object Gateway, necesita un par de archivos de clave y certificado SSL válidos (consulte el <xref linkend="ceph-rgw-https"/> para obtener más información). Debe habilitar SSL, especificar un número de puerto para las conexiones SSL y especificar los archivos de certificado SSL y de clave.
    </para>
    <para>
     Para habilitar SSL y especificar el número de puerto, incluya lo siguiente en la especificación:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
</screen>
    <para>
     Para especificar el certificado SSL y la clave, puede pegar su contenido directamente en el archivo de especificación YAML. El signo de barra vertical (<literal>|</literal>) al final de la línea indica al analizador que debe esperar una cadena de varias líneas como valor. Por ejemplo:
    </para>
<screen>
spec:
  ssl: true
  rgw_frontend_port: 443
  rgw_frontend_ssl_certificate: |
   -----BEGIN CERTIFICATE-----
   MIIFmjCCA4KgAwIBAgIJAIZ2n35bmwXTMA0GCSqGSIb3DQEBCwUAMGIxCzAJBgNV
   BAYTAkFVMQwwCgYDVQQIDANOU1cxHTAbBgNVBAoMFEV4YW1wbGUgUkdXIFNTTCBp
   [...]
   -----END CERTIFICATE-----
   rgw_frontend_ssl_key: |
   -----BEGIN PRIVATE KEY-----
   MIIJRAIBADANBgkqhkiG9w0BAQEFAASCCS4wggkqAgEAAoICAQDLtFwg6LLl2j4Z
   BDV+iL4AO7VZ9KbmWIt37Ml2W6y2YeKX3Qwf+3eBz7TVHR1dm6iPpCpqpQjXUsT9
   [...]
   -----END PRIVATE KEY-----
</screen>
    <tip>
     <para>
      En lugar de pegar el contenido del certificado SSL y los archivos de clave, puede omitir las palabras clave <literal>rgw_frontend_ssl_certificate:</literal> y <literal>rgw_frontend_ssl_key:</literal> y cargarlos en la base de datos de configuración
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.crt \
 -i <replaceable>SSL_CERT_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/<replaceable>REALM_NAME</replaceable>/<replaceable>ZONE_NAME</replaceable>.key \
 -i <replaceable>SSL_KEY_FILE</replaceable>
</screen>
    </tip>
    <sect4 xml:id="cephadm-deploy-ogw-ports">
     <title>Configuración de Object Gateway para que escuche en los puertos 443 y 80</title>
     <para>
      Si desea configurar Object Gateway para que escuche en los puertos 443 (HTTPS) y 80 (HTTP), siga estos pasos:
     </para>
     <note>
      <para>
       Los comandos del procedimiento utilizan el dominio y la zona <literal>default</literal>.
      </para>
     </note>
     <procedure>
      <step>
       <para>
        Distribuya Object Gateway proporcionando un archivo de especificación. Consulte la <xref linkend="deploy-cephadm-day2-service-ogw"/> para obtener más información sobre la especificación de Object Gateway. Utilice el comando siguiente:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>SPEC_FILE</replaceable></screen>
      </step>
      <step>
       <para>
        Si no se proporcionan certificados SSL en el archivo de especificación, añádalos mediante el siguiente comando:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.crt -i certificate.pem
<prompt>cephuser@adm &gt; </prompt>ceph config-key set rgw/cert/default/default.key -i key.pem
</screen>
      </step>
      <step>
       <para>
        Cambie el valor por defecto de la opción <option>rgw_frontends</option>:
       </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default rgw_frontends \
 "beast port=80 ssl_port=443"
</screen>
      </step>
      
      <step>
       <para>
        Elimine la configuración específica creada por cephadm. Identifique para qué destino se ha configurado la opción <option>rgw_frontends</option> ejecutando el comando:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config dump | grep rgw</screen>
       <para>
        Por ejemplo, el destino es <literal>client.rgw.default.default.node4.yiewdu</literal>. Elimine el valor <option>rgw_frontends</option> específico actual:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config rm client.rgw.default.default.node4.yiewdu rgw_frontends</screen>
       <tip>
        <para>
         En lugar de eliminar un valor para <option>rgw_frontends</option>, puede especificarlo. Por ejemplo:
        </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph config set client.rgw.default.default.node4.yiewdu \
 rgw_frontends "beast port=80 ssl_port=443"
</screen>
       </tip>
      </step>
      
      <step>
       <para>
        Reinicie las instancias de Object Gateway:
       </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch restart rgw.default.default</screen>
      </step>
     </procedure>
    </sect4>
   </sect3>
   <sect3 xml:id="cephadm-deploy-with-subcluster">
    <title>Distribución con un subclúster</title>
    <para>
     Los <emphasis>subclústeres</emphasis> ayudan a organizar los nodos de los clústeres para aislar las cargas de trabajo y facilitar el escalado elástico. Si va a distribuir con un subclúster, aplique la siguiente configuración:
    </para>
<screen>
service_type: rgw
service_id: <replaceable>REALM_NAME</replaceable>.<replaceable>ZONE_NAME</replaceable>.<replaceable>SUBCLUSTER</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  rgw_realm: <replaceable>RGW_REALM</replaceable>
  rgw_zone: <replaceable>RGW_ZONE</replaceable>
  subcluster: <replaceable>SUBCLUSTER</replaceable>
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-igw">
   <title>Distribución de pasarelas iSCSI Gateway</title>
   <para>
    cephadm distribuye una instancia de iSCSI Gateway, que es un protocolo de red de área de almacenamiento (SAN) que permite a los clientes (denominados iniciadores) enviar comandos SCSI a dispositivos de almacenamiento SCSI (destinos) en servidores remotos.
   </para>
   <para>
    Aplique la siguiente configuración para la distribución. Asegúrese de que <literal>trusted_ip_list</literal> contiene las direcciones IP de todos los nodos de iSCSI Gateway y de Ceph Manager (consulte el resultado de ejemplo a continuación).
   </para>
   <note>
    <para>
     Asegúrese de crear el repositorio antes de aplicar la siguiente especificación.
    </para>
   </note>
<screen>
service_type: iscsi
service_id: <replaceable>EXAMPLE_ISCSI</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  api_user: <replaceable>EXAMPLE_USER</replaceable>
  api_password: <replaceable>EXAMPLE_PASSWORD</replaceable>
  trusted_ip_list: "<replaceable>IP_ADDRESS_1</replaceable>,<replaceable>IP_ADDRESS_2</replaceable>"
</screen>
   <note>
    <para>
     Asegúrese de que las direcciones IP mostradas para <literal>trusted_ip_list</literal> <emphasis>no tienen</emphasis> un espacio después de la separación por comas.
    </para>
   </note>
   <sect3>
    <title>Configuración SSL segura</title>
    <para>
     Para utilizar una conexión SSL segura entre Ceph Dashboard y la API de destino iSCSI, necesita un par de archivos de clave y certificado SSL válidos. Estos pueden ser emitidos por una autoridad certificadora o autofirmados (consulte el <xref linkend="self-sign-certificates"/>). Para habilitar SSL, incluya el ajuste <literal>api_secure: true</literal> en el archivo de especificación:
    </para>
<screen>
spec:
  api_secure: true
</screen>
    <para>
     Para especificar el certificado SSL y la clave, puede pegar el contenido directamente en el archivo de especificación YAML. El signo de barra vertical (<literal>|</literal>) al final de la línea indica al analizador que debe esperar una cadena de varias líneas como valor. Por ejemplo:
    </para>
<screen>
spec:
  pool: EXAMPLE_POOL
  api_user: EXAMPLE_USER
  api_password: EXAMPLE_PASSWORD
  trusted_ip_list: "IP_ADDRESS_1,IP_ADDRESS_2"
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-nfs">
   <title>Distribución de NFS Ganesha</title>
    
<important>
 <para>
  NFS Ganesha admite la versión 4.1 y posteriores de NFS. No es compatible con NFS versión 3.
 </para>
</important>

    <para>
    cephadm distribuye NFS Ganesha mediante un repositorio RADOS predefinido y un espacio de nombres opcional. Para distribuir NFS Ganesha, aplique la siguiente especificación:
   </para>
   <note>
    <para>
     Debe tener un repositorio RADOS predefinido; de lo contrario, la operación <command>ceph orch apply</command> fallará. Para obtener más información sobre cómo crear un repositorio, consulte el <xref linkend="ceph-pools-operate-add-pool"/>.
    </para>
   </note>
<screen>
service_type: nfs
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min1
  - ses-min2
spec:
  pool: <replaceable>EXAMPLE_POOL</replaceable>
  namespace: <replaceable>EXAMPLE_NAMESPACE</replaceable>
</screen>
   <itemizedlist>
    <listitem>
     <para>
      Sustituya <replaceable>EXAMPLE_NFS</replaceable> con una cadena arbitraria que identifique la exportación de NFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Sustituya <replaceable>EXAMPLE_POOL</replaceable> con el nombre del repositorio en el que se almacenará el objeto de configuración RADOS de NFS Ganesha.
     </para>
    </listitem>
    <listitem>
     <para>
      Sustituya <replaceable>EXAMPLE_NAMESPACE</replaceable> (opcional) con el espacio de nombres NFS de Object Gateway deseado (por ejemplo, <literal>ganesha</literal>)
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-rbdmirror">
   <title>Distribuyendo <systemitem class="daemon">rbd-mirror</systemitem></title>
   <para>
    El servicio <systemitem class="daemon">rbd-mirror</systemitem> se encarga de sincronizar las imágenes del dispositivo de bloques RADOS entre dos clústeres de Ceph (para obtener más información, consulte el <xref linkend="ceph-rbd-mirror"/>). Para distribuir <systemitem class="daemon">rbd-mirror</systemitem>, utilice la siguiente especificación:
   </para>
<screen>
service_type: rbd-mirror
service_id: <replaceable>EXAMPLE_RBD_MIRROR</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-service-monitoring">
   <title>Distribución de la pila de supervisión</title>
   <para>
    La pila de supervisión está formada por Prometheus, los exportadores de Prometheus, Prometheus Alertmanager y Grafana. Ceph Dashboard utiliza estos componentes para almacenar y visualizar métricas detalladas sobre el uso y el rendimiento del clúster.
   </para>
   <tip>
    <para>
     Si la distribución requiere imágenes de contenedor personalizadas o servidas localmente de los servicios de la pila de supervisión, consulte el <xref linkend="monitoring-custom-images"/>.
    </para>
   </tip>
   <para>
    Para distribuir la pila de supervisión, siga estos pasos:
   </para>
   <procedure>
    <step>
     <para>
      Habilite el módulo <literal>prometheus</literal> en el daemon de Ceph Manager. Esto expone las métricas internas de Ceph para que Prometheus pueda leerlas:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable prometheus</screen>
     <note>
      <para>
       Asegúrese de que este comando se ejecuta antes de distribuir Prometheus. Si el comando no se ejecutó antes de la distribución, debe volver a distribuir Prometheus para actualizar la configuración de Prometheus:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch redeploy prometheus</screen>
     </note>
    </step>
    <step>
     <para>
      Cree un archivo de especificación (por ejemplo, <filename>monitoring.yaml</filename>) con un contenido similar al siguiente:
     </para>
<screen>
service_type: prometheus
placement:
  hosts:
  - ses-min2
---
service_type: node-exporter
---
service_type: alertmanager
placement:
  hosts:
  - ses-min4
---
service_type: grafana
placement:
  hosts:
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Aplique los servicios de supervisión ejecutando:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i monitoring.yaml</screen>
     <para>
      La distribución de los servicios de supervisión puede tardar uno o dos minutos.
     </para>
    </step>
   </procedure>
   <important>
    <para>
     Prometheus, Grafana y Ceph Dashboard se configuran automáticamente para comunicarse entre sí, lo que da como resultado una integración totalmente funcional de Grafana en Ceph Dashboard cuando se distribuyen como se describe anteriormente.
    </para>
    <para>
     La única excepción a esta regla es la supervisión con imágenes RBD. Consulte el <xref linkend="monitoring-rbd-image"/> para obtener más información. 
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
