<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>Dispositivo de bloques RADOS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>yes</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Un bloque es una secuencia de bytes, por ejemplo, un bloque de 4 MB de datos. Las interfaces de almacenamiento basadas en bloques son la forma más habitual de almacenar los datos en soportes de uso rotativo, como discos duros, CD o disquetes. La ubicuidad de las interfaces de dispositivos de bloques hacen que un dispositivo de bloques virtual sea el candidato idóneo para interactuar con un sistema de almacenamiento masivo de datos como Ceph.
 </para>
 <para>
  Los dispositivos de bloques de Ceph permiten compartir recursos físicos y se puede modificar su tamaño. Los datos se almacenan repartidos en varios OSD en un clúster de Ceph. Los dispositivos de bloques de Ceph aprovechan las funciones de RADOS, como la realización de instantáneas, las réplicas y la comprobación de coherencia. Los dispositivos de bloques RADOS (RBD) de Ceph interactúan con los OSD mediante módulos del kernel o la biblioteca <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocolo RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Los dispositivos de bloques de Ceph ofrecen un alto rendimiento con infinitas posibilidades de escalabilidad para los módulos de kernel. Son compatibles con soluciones de virtualización como QEMU o sistemas informáticos basados en la nube que dependen de <systemitem class="library">libvirt</systemitem>, como OpenStack. Puede utilizar el mismo clúster para gestionar Object Gateway, CephFS y los dispositivos de bloques RADOS al mismo tiempo.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Comandos del dispositivo de bloques</title>

  <para>
   El comando <command>rbd</command> permite crear, enumerar, examinar y eliminar imágenes de los dispositivos de bloques. También se puede emplear, por ejemplo, para clonar imágenes, crear instantáneas, revertir una imagen al estado de una instantánea o ver una instantánea.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creación de una imagen del dispositivo de bloques en un repositorio replicado</title>
   <para>
    Antes de poder añadir un dispositivo de bloques a un cliente, debe crear una imagen relacionada en un repositorio existente (consulte el <xref linkend="ceph-pools"/>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Por ejemplo, para crear una imagen de 1 GB denominada &quot;myimage&quot; que almacena información en un repositorio llamado &quot;mypool&quot;, ejecute lo siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>unidades de tamaño de imagen</title>
    <para>
     Si omite un acceso directo de unidad de tamaño (&quot;G&quot; o &quot;T&quot;), el tamaño de la imagen se indicará en megabytes. Utilice &quot;G&quot; o &quot;T&quot; después del número de tamaño para especificar gigabytes o terabytes.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creación de una imagen del dispositivo de bloques en un repositorio codificado de borrado</title>
   <para>
    Es posible almacenar datos de una imagen de dispositivo de bloques directamente en repositorios codificados de borrado. Una imagen de dispositivo de bloques RADOS está formada por elementos de <emphasis>datos</emphasis> y <emphasis>metadatos</emphasis>. Solo se puede almacenar la parte de datos de una imagen de dispositivo de bloques RADOS en un repositorio codificado de borrado. El repositorio debe tener en el indicador <option>overwrite</option> el valor <emphasis>true</emphasis> definido, y eso solo es posible si todos los OSD donde se almacena el repositorio utilizan BlueStore.
   </para>
   <para>
    No es posible almacenar la parte de metadatos de la imagen en un repositorio codificado de borrado. Puede especificar el repositorio replicado para almacenar los metadatos de la imagen con la opción <option>--pool=</option> del comando <command>rbd create</command> o especificar <option>pool/</option> como prefijo del nombre de la imagen.
   </para>
   <para>
    Cree un repositorio codificado de borrado:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Especifique el repositorio replicado para almacenar los metadatos:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    O bien:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Listado de imágenes de dispositivos de bloques</title>
   <para>
    Para mostrar los dispositivos de bloques en un repositorio denominado &quot;mypool&quot;, ejecute lo siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Recuperación de información de la imagen</title>
   <para>
    Para recuperar información de una imagen denominada &quot;myimage&quot; dentro de un repositorio denominado &quot;mypool&quot;, ejecute lo siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Cambio de tamaño de la imagen de un dispositivo de bloques</title>
   <para>
    Las imágenes de los dispositivos de bloques RADOS emplean un sistema de provisión ligera, lo que significa que no emplean espacio físico real hasta que empieza a guardar datos en ellos. No obstante, tienen una capacidad máxima que se define mediante la opción <option>‑‑size</option>. Si desea aumentar (o reducir) el tamaño máximo de la imagen (o disminuir), ejecute lo siguiente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Eliminación de una imagen de dispositivo de bloques</title>
   <para>
    Para eliminar un dispositivo de bloques que se corresponde con una imagen llamada &quot;myimage&quot; en un repositorio denominado &quot;mypool&quot;, ejecute lo siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montaje y desmontaje</title>

  <para>
   Después de crear un dispositivo de bloques RADOS, puede usarlo como cualquier otro dispositivo de disco: formatearlo, montarlo para que permita el intercambio de archivos y desmontarlo cuando haya terminado.
  </para>

  <para>
   El comando <command>rbd</command> accede por defecto al clúster mediante la cuenta de usuario <literal>admin</literal> de Ceph. Esta cuenta tiene acceso administrativo completo al clúster. Esto aumenta el riesgo de causar daños accidentalmente, como ocurre cuando se entra en una estación de trabajo Linux como usuario <systemitem class="username">root</systemitem>. Por lo tanto, es preferible crear cuentas de usuario con menos privilegios y utilizar estas cuentas para el acceso normal de lectura/escritura a dispositivos de bloques RADOS.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Creación de una cuenta de usuario de Ceph</title>
   <para>
    Para crear una nueva cuenta de usuario con las capacidades de Ceph Manager, Ceph Monitor y Ceph OSD, utilice el comando <command>ceph</command> con el subcomando <command>auth get-or-create</command>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Por ejemplo, para crear un usuario llamado <replaceable>qemu</replaceable> con acceso de lectura y escritura al repositorio <replaceable>vms</replaceable> y acceso de solo lectura al repositorio <replaceable>images</replaceable>, ejecute lo siguiente:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    El resultado del comando <command>ceph auth get-or-create</command> será el anillo de claves del usuario especificado, que se puede escribir en <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     Al utilizar el comando <command>rbd</command>, puede especificar el ID de usuario proporcionando el argumento opcional <command>--id</command>
     <replaceable>ID</replaceable>.
    </para>
   </note>
   <para>
    Para obtener más información sobre la gestión de cuentas de usuario de Ceph, consulte el <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Autenticación de usuarios</title>
   <para>
    Para especificar un nombre de usuario, utilice <option>‑‑id <replaceable>nombre de usuario</replaceable></option>. Si utiliza la autenticación de <systemitem>cephx</systemitem>, también debe especificar un secreto. Puede proceder de un anillo de claves o de un archivo que contenga el secreto:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    o bien
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparación de un dispositivo de bloques RADOS para su uso</title>
   <procedure>
    <step>
     <para>
      Asegúrese de que el clúster de Ceph incluye un repositorio con la imagen de disco que desea asignar. Supongamos que el repositorio se denomina <literal>mypool</literal> y la imagen <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Asigne la imagen a un nuevo dispositivo de bloques:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Enumerar todos los dispositivos asignados:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      El dispositivo en el que queremos trabajar es <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>vía del dispositivo RBD</title>
      <para>
       En lugar de <filename>/dev/rbd<replaceable>NÚMERO_DISPOSITIVO</replaceable></filename>, puede utilizar <filename>/dev/rbd/<replaceable>NOMBRE_REPOSITORIO</replaceable>/<replaceable>NOMBRE_IMAGEN</replaceable></filename> como vía persistente del dispositivo. Por ejemplo:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Cree un sistema de archivos XFS en el dispositivo <filename>/dev/rbd0:</filename>
     </para>
<screen><prompt role="root"># </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Sustituyendo <filename>/mnt</filename> por el punto de montaje, monte el dispositivo y compruebe que está montado correctamente:
     </para>
<screen><prompt role="root"># </prompt>mount /dev/rbd0 /mnt
      <prompt role="root"># </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Ya puede mover datos al dispositivo y desde él como si fuese un directorio local.
     </para>
     <tip>
      <title>aumento del tamaño del dispositivo RBD</title>
      <para>
       Si descubre que el tamaño del dispositivo RBD es insuficiente, puede aumentarlo con facilidad.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Aumente el tamaño de la imagen RBD, por ejemplo, hasta 10 GB.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Aumente el sistema de archivos hasta llenar el nuevo tamaño del dispositivo:
        </para>
<screen><prompt role="root"># </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Cuando termine de acceder al dispositivo, puede desasignarlo y desmontarlo.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root"># </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>montaje y desmontaje manual</title>
    <para>
     Se proporciona un guion <command>rbdmap</command> y una unidad <systemitem class="daemon">systemd</systemitem> para facilitar el proceso de asignación y montaje de los RBD después del arranque, así como de desmontarlos antes del apagado. Consulte la <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command>: asignación de dispositivos RBD durante el arranque</title>
   <para>
    <command>rbdmap</command> es un guion de shell que automatiza las operaciones <command>rbd map</command> y <command>rbd device unmap</command> en una o varias imágenes RBD. Aunque es posible ejecutar el guion manualmente en cualquier momento, la ventaja principal consiste en asignar y montar automáticamente las imágenes RBD durante el arranque (y desmontarlas y desasignarlas al apagar el equipo), activándose mediante el sistema Init. Con este fin, el paquete <systemitem class="daemon">systemd</systemitem> incluye el archivo de unidad <filename> </filename>rbdmap.service<systemitem>ceph-common</systemitem>.
   </para>
   <para>
    El guion solo acepta un argumento, que puede ser <option>map</option> o <option>unmap</option>. En ambos casos, el guion analiza un archivo de configuración. Se utiliza <filename>/etc/ceph/rbdmap</filename> por defecto, pero se puede anular mediante una variable de entorno <literal>RBDMAPFILE</literal>. Cada línea del archivo de configuración se corresponde con una imagen RBD que se debe asignar o desasignar.
   </para>
   <para>
    El archivo de configuración tiene el formato siguiente:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Vía a una imagen dentro de un repositorio. Se debe especificar como <replaceable>nombre_repositorio</replaceable>/<replaceable>nombre_imagen</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Una lista opcional de parámetros que se puedan pasar al comando <command>rbd device map</command> subyacente. Estos parámetros y sus valores se deben especificar como una cadena separada por comas, por ejemplo:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       El ejemplo hace que el guion <command>rbdmap</command> ejecute el siguiente comando:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       En el ejemplo siguiente puede se explica cómo especificar un nombre de usuario y un anillo de claves con un secreto correspondiente:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Si se ejecuta como <command>rbdmap map</command>, el guion analiza el archivo de configuración y, para cada imagen RBD especificada, primero intenta asignar la imagen (mediante el comando <command>rbd device map</command>) y luego montarla.
   </para>
   <para>
    Si se ejecuta como <command>rbdmap unmap</command>, las imágenes enumeradas en el archivo de configuración se desmontarán y se desasignarán.
   </para>
   <para>
    <command>rbdmap unmap-all</command> intenta desmontar y posteriormente desasignar todas las imágenes RBD asignadas actualmente, independientemente de que estén enumeradas o no en el archivo de configuración.
   </para>
   <para>
    Si la operación se realiza correctamente, <command>rbd device map</command> asigna la imagen a un dispositivo <filename>/dev/rbdX</filename>, momento en el que se activa una regla udev para crear un enlace simbólico de nombre de dispositivo de confianza <filename>/dev/rbd/<replaceable>nombre_repositorio</replaceable>/<replaceable>nombre_imagen</replaceable></filename> que señala al dispositivo real asignado.
   </para>
   <para>
    Para que las operaciones de montaje y desmontaje se lleven a cabo correctamente, el nombre del dispositivo de confianza debe tener una entrada correspondiente en <filename>/etc/fstab</filename>. Al escribir entradas <filename>/etc/fstab</filename> para imágenes RBD, especifique la opción de montaje &quot;noauto&quot; (o &quot;nofail&quot;). Esto impide que el sistema Init intente montar el dispositivo demasiado pronto, antes de que el dispositivo en cuestión aún exista, ya que <filename>rbdmap.service</filename> normalmente se activa bastante tarde en la secuencia de arranque.
   </para>
   <para>
    Para una lista completa de opciones de <command>rbd</command>, consulte la página man de <command>rbd</command> (<command>man 8 rbd</command>).
   </para>
   <para>
    Para consultar ejemplos de uso de <command>rbdmap</command>, consulte la página man de <command>rbdmap</command> (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Aumento del tamaño de dispositivos RBD</title>
   <para>
    Si descubre que el tamaño del dispositivo RBD es insuficiente, puede aumentarlo con facilidad.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Aumente el tamaño de la imagen RBD, por ejemplo, hasta 10 GB.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Aumente el sistema de archivos hasta llenar el nuevo tamaño del dispositivo.
     </para>
<screen><prompt role="root"># </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Instantáneas</title>

  <para>
   Una instantánea RBD es una instantánea de una imagen de dispositivo de bloques RADOS. Las instantáneas permiten conservar un historial de los estados de la imagen. Ceph también es compatible con capas de instantáneas, lo que permite clonar imágenes de máquinas virtuales de forma rápida y sencilla. Ceph admite las instantáneas de dispositivos de bloques mediante el comando <command>rbd</command> y muchas interfaces de alto nivel, incluidas QEMU, <systemitem>libvirt</systemitem>, OpenStack y CloudStack.
  </para>

  <note>
   <para>
    Detenga las operaciones de entrada y salida y vacíe todas las escrituras pendientes antes de tomar una instantánea de una imagen. Si la imagen contiene un sistema de archivos, este debe tener un estado coherente en el momento de realizar la instantánea.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Habilitación y configuración de <systemitem>cephx</systemitem></title>
   <para>
    Si <systemitem>cephx</systemitem> está habilitado, debe especificar un nombre de usuario o ID y una vía al anillo de claves que contiene la clave correspondiente para el usuario. Consulte el <xref linkend="cha-storage-cephx"/> para obtener más información. También puede añadir la variable de entorno <systemitem>CEPH_ARGS</systemitem> para evitar la reintroducción de los siguientes parámetros.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Añada el usuario y el secreto a la variable de entorno <systemitem>CEPH_ARGS</systemitem> para no tener que introducir estos datos en cada ocasión.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Conceptos básicos sobre instantáneas</title>
   <para>
    Los procedimientos siguientes muestran cómo crear, enumerar y eliminar instantáneas mediante el comando <command>rbd</command> en la línea de comandos.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Creación de instantáneas</title>
    <para>
     Para crear una instantánea con <command>rbd</command>, especifique la opción <option>snap create</option>, el nombre del repositorio y el nombre de la imagen.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Listado de instantáneas</title>
    <para>
     Para enumerar las instantáneas de una imagen, especifique el nombre del repositorio y el de la imagen.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Reversión de instantáneas</title>
    <para>
     Para revertir una instantánea con <command>rbd</command>, especifique la opción <option>snap rollback</option>, el nombre del repositorio, el nombre de la imagen y el nombre de la instantánea.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Revertir una imagen a una instantánea significa sobrescribir la versión actual de la imagen con los datos de la instantánea. El tiempo necesario para ejecutar una reversión aumenta según el tamaño de la imagen. Es más rápido <emphasis>clonar</emphasis> una instantánea que <emphasis>revertir</emphasis> una imagen a una instantánea, por lo que es el método preferible para volver a un estado preexistente.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Supresión de una instantánea</title>
    <para>
     Para suprimir una instantánea con <command>rbd</command>, especifique la opción <option>snap rm</option>, el nombre del repositorio, el nombre de la imagen y el nombre de usuario.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Los OSD de Ceph suprimen los datos de forma asíncrona, por lo que al suprimir una instantánea, no se libera el espacio de disco inmediatamente.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Limpieza de instantáneas</title>
    <para>
     Para suprimir todas las instantáneas de una imagen con <command>rbd</command>, especifique la opción <option>snap purge</option> y el nombre de la imagen.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Capas de instantáneas</title>
   <para>
    Ceph admite la posibilidad de crear varios clones de copia de escritura (COW, por sus siglas en inglés) de una instantánea de dispositivo de bloques. Las capas de instantáneas permiten que los clientes de dispositivos de bloques de Ceph creen imágenes muy rápidamente. Por ejemplo, puede crear una imagen de dispositivo de bloques con una máquina virtual Linux escrita en él y, a continuación, realizar una instantánea de la imagen, protegerla y crear tantos clones de copia de escritura como desee. Las instantáneas son de solo lectura, por lo que clonar una instantánea simplifica la semántica y permite crear clones rápidamente.
   </para>
   <note>
    <para>
     Los términos &quot;parent&quot; (padre) y &quot;child&quot; (hijo) mencionados en los siguientes ejemplos de la línea de comandos hacen referencia a una instantánea de un dispositivo de bloques de Ceph (padre) y la imagen correspondiente clonada de la instantánea (hijo).
    </para>
   </note>
   <para>
    Cada imagen clonada (hijo) almacena una referencia a su imagen padre que permite que la imagen clonada abra la instantánea padre y la lea.
   </para>
   <para>
    Un clon COW de una instantánea se comporta exactamente igual que cualquier otra imagen de dispositivo de bloques de Ceph. Puede leer las imágenes clonadas, escribir en ellas, clonarlas y redimensionarlas. No existen restricciones especiales que afecten a las imágenes clonadas. No obstante, la clonación de copia de escritura de una instantánea hace referencia a la instantánea, por lo que <emphasis>debe</emphasis> proteger la instantánea antes de clonarla.
   </para>
   <note>
    <title><option>‑‑image-format 1</option> no se admite</title>
    <para>
     No es posible crear instantáneas de imágenes creadas con la opción obsoleta <command>rbd create ‑‑image-format 1</command>. Ceph solo admite la clonación de las imágenes <emphasis>format 2</emphasis> por defecto.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Procedimientos iniciales con las capas</title>
    <para>
     La aplicación de capas a un dispositivo de bloques de Ceph es un proceso sencillo. Debe disponer de una imagen. Debe crear una instantánea de la imagen. Debe proteger la instantánea. Una vez realizados estos pasos, puede empezar a clonar la instantánea.
    </para>
    <para>
     La imagen clonada tendrá una referencia a la instantánea padre e incluirá el ID del repositorio, el ID de la imagen y el ID de la instantánea. La inclusión del ID del repositorio significa que se pueden clonar instantáneas de un repositorio como imágenes en un repositorio distinto.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Plantilla de imagen:</emphasis> un ejemplo de uso habitual para crear capas de dispositivos de bloques es crear una imagen principal y una instantánea que sirva como plantilla para los clones. Por ejemplo, un usuario puede crear una imagen de una distribución de Linux (como SUSE Linux Enterprise Server) y crear una instantánea para ella. El usuario puede actualizar periódicamente la imagen y crear una instantánea nueva (por ejemplo, <command>zypper ref &amp;&amp; zypper patch</command>, seguido de <command>rbd snap create</command>). A medida que la imagen crezca, el usuario puede clonar cualquiera de las instantáneas.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Plantilla extendida:</emphasis> un ejemplo de uso más avanzado implica la posibilidad de extender una imagen de plantilla para que proporcione más información que una imagen base. Por ejemplo, un usuario puede clonar una imagen (una plantilla de máquina virtual) e instalar otro software (por ejemplo, una base de datos, un sistema de gestión de contenido o un sistema de análisis) y, a continuación, realizar una instantánea de la imagen extendida, que a su vez se puede actualizar de la misma manera que la imagen base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Repositorio de plantillas:</emphasis> una manera de utilizar capas de dispositivos de bloques es crear un repositorio que contenga imágenes principales que actúen como plantillas e instantáneas de dichas plantillas. A continuación, puede ampliar los privilegios de solo lectura a los usuarios, de modo que tengan la posibilidad de clonar las instantáneas sin la capacidad de escribir o ejecutar dentro del repositorio.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migración y recuperación de imágenes:</emphasis> una manera de utilizar capas de dispositivos de bloques consiste en migrar o recuperar datos de un repositorio a otro.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protección de una instantánea</title>
    <para>
     Los clones acceden a las instantáneas padre. Todos los clones dejarán de funcionar si un usuario suprime por error la instantánea padre. Para evitar la pérdida de datos, debe proteger la instantánea antes de clonarla.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      No es posible suprimir una instantánea protegida.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Clonación de una instantánea</title>
    <para>
     Para clonar una instantánea, debe especificar el repositorio padre, la imagen, la instantánea, el repositorio hijo y el nombre de la imagen. Debe proteger la instantánea para poder clonarla.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Puede clonar una instantánea de un repositorio como una imagen de otro repositorio. Por ejemplo, puede mantener imágenes e instantáneas de solo lectura como plantillas en un repositorio y clones con acceso de escritura en otro.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Desprotección de una instantánea</title>
    <para>
     Para suprimir una instantánea, primero debe desprotegerla. Además, <emphasis>no puede</emphasis> suprimir instantáneas que tengan referencias de clones. Debe aplanar todos los clones de una instantánea para poder eliminarla.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Listado de los hijos de una instantánea</title>
    <para>
     Para enumerar los hijos de una instantánea, ejecute lo siguiente:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Aplanamiento de una imagen clonada</title>
    <para>
     Las imágenes clonadas retienen una referencia a la instantánea padre. Si elimina la referencia del clon a la instantánea padre, la imagen se &quot;aplana&quot; copiando la información de la instantánea en el clon. El tiempo que se tarda en aplanar un clon aumenta según el tamaño de la instantánea. Para suprimir una instantánea, primero debe aplanar las imágenes hijo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Dado que una imagen plana contiene toda la información de la instantánea, ocupará más espacio de almacenamiento que un clon con capas.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Duplicados de imagen RBD</title>

  <para>
   Las imágenes RBD se pueden duplicar de forma asincrónica entre dos clústeres de Ceph. Esta capacidad está disponible en dos modos:
  </para>

  <variablelist>
   <varlistentry>
    <term>Basada en registro</term>
    <listitem>
     <para>
      Este modo utiliza las imágenes RBD transaccionales para garantizar la réplica protegida contra bloqueos de un momento concreto entre los clústeres. Cada escritura en la imagen RBD se guarda primero en el registro asociado antes de modificar la imagen real. El clúster <literal>remoto</literal> leerá el registro y reproducirá las actualizaciones en su copia local de la imagen. Dado que cada escritura en la imagen RBD dará como resultado dos escrituras en el clúster de Ceph, se espera que la latencia de escritura casi se duplique cuando se utiliza la función de imagen RBD transaccional.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Basada en instantáneas</term>
    <listitem>
     <para>
      Este modo utiliza instantáneas de duplicación de imágenes RBD creadas de forma programada periódicamente o manualmente para replicar imágenes de RBD protegidas contra bloqueos entre clústeres. El clúster <literal>remoto</literal> determinará las actualizaciones de datos o metadatos entre dos instantáneas de duplicación y copiará los deltas en su copia local de la imagen. Con la ayuda de la función de imagen fast-diff de RBD, los bloques de datos actualizados se pueden calcular rápidamente sin necesidad de explorar la imagen RBD completa. Dado que este modo no es coherente en un momento determinado, será necesario sincronizar el delta completo de la instantánea antes de utilizarlo durante una situación de failover. Cualquier delta de instantánea aplicado parcialmente se revertirá a la última instantánea totalmente sincronizada antes de su uso.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   La duplicación se configura de forma independiente para cada repositorio dentro de los clústeres conectores. Esto se puede configurar en un subconjunto específico de imágenes dentro del repositorio, o para que refleje automáticamente todas las imágenes de un repositorio si se usa únicamente la duplicación transaccional. La duplicación se configura mediante el comando <command>rbd</command>. El daemon <systemitem class="daemon">rbd-mirror</systemitem> es el responsable de la extracción de actualizaciones de imágenes del clúster conector <literal>remoto</literal> y de aplicarlos a la imagen en el clúster <literal>local</literal>.
  </para>

  <para>
   Dependiendo de las necesidades de réplica, la duplicación del RBD se puede configurar para la réplica unidireccional o bidireccional:
  </para>

  <variablelist>
   <varlistentry>
    <term>Réplica unidireccional</term>
    <listitem>
     <para>
      Cuando los datos solo se duplican desde un clúster primario a un clúster secundario, el daemon <systemitem class="daemon">rbd-mirror</systemitem> se ejecuta solo en el clúster secundario.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Réplica bidireccional</term>
    <listitem>
     <para>
      Cuando los datos se duplican desde imágenes primarias de un clúster a imágenes no primarias de otro clúster (y viceversa), el daemon <systemitem class="daemon">rbd-mirror</systemitem> se ejecuta en ambos clústeres.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Cada instancia del daemon <systemitem class="daemon">rbd-mirror</systemitem> debe poder conectarse a los clústeres de Ceph <literal>locales</literal> y <literal>remotos</literal> simultáneamente. Por ejemplo, todos los hosts de monitor y OSD. Además, la red debe tener suficiente ancho de banda entre los dos centros de datos para poder gestionar la carga de trabajo de duplicación.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configuración del repositorio</title>
   <para>
    Los siguientes procedimientos demuestran cómo llevar a cabo las tareas administrativas básicas para configurar la duplicación mediante el comando <command>rbd</command>. La duplicación se configura de forma independiente para cada repositorio dentro de los clústeres de Ceph.
   </para>
   <para>
    Debe llevar a cabo los pasos de configuración del repositorio en ambos clústeres conectores. Para simplificar los ejemplos, en estos procedimientos se presupone que hay dos clústeres, denominados <literal>local</literal> y <literal>remote</literal> a los que se puede acceder desde un único host.
   </para>
   <para>
    Consulte la página man de <command>rbd</command> (<command>man 8 rbd</command>) para obtener información adicional acerca de cómo conectarse a clústeres de Ceph diferentes.
   </para>
   <tip>
    <title>varios clústeres</title>
    <para>
     En los siguientes ejemplos, el nombre del clúster corresponde a un archivo de configuración de Ceph con el mismo nombre <filename>/etc/ceph/remote.conf</filename> y a un archivo de anillo de claves de Ceph con el mismo nombre <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Habilitación de la duplicación en un repositorio</title>
    <para>
     Para habilitar la duplicación en un repositorio, especifique el subcomando <command>mirror pool enable</command>, el nombre del repositorio y el modo de duplicación. El modo de duplicación puede ser de repositorio o de imagen:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Se duplican todas las imágenes del repositorio con el registro transaccional habilitado.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        La duplicación se debe habilitar explícitamente en cada imagen. Consulte el <xref linkend="rbd-mirror-enable-image-mirroring"/> para obtener más información. 
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Inhabilitación de la duplicación</title>
    <para>
     Para inhabilitar la duplicación en un repositorio, especifique el subcomando <command>mirror pool disable</command> y el nombre del repositorio. Cuando se inhabilita la duplicación en un repositorio de esta forma, la duplicación también se inhabilitará en cualquier imagen (dentro del repositorio) para la que se haya habilitado la duplicación explícitamente.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Conectores de carga</title>
    <para>
     Para que el daemon <systemitem class="daemon">rbd-mirror</systemitem> descubra su clúster conector, el conector debe estar registrado en el repositorio y se debe crear una cuenta de usuario. Este proceso se puede automatizar con <command>rbd</command> y con los comandos <command>mirror pool peer bootstrap create</command> y <command>mirror pool peer bootstrap import</command>.
    </para>
    <para>
     Para crear manualmente un nuevo testigo de arnque con <command>rbd</command>, especifique el comando <command>mirror pool peer bootstrap create</command>, un nombre de repositorio y un nombre de sitio descriptivo opcional para describir el clúster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     El resultado del comando <command>mirror pool peer bootstrap create</command> será un testigo que se debe proporcionar al comando <command>mirror pool peer bootstrap import</command>. Por ejemplo, en el clúster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5I \
joiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Para importar manualmente el testigo de arranque creado por otro clúster con el comando <command>rbd</command>, utilice la siguiente sintaxis:
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Dónde:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        Un nombre de sitio descriptivo opcional para describir el clúster <literal>local</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Una dirección de duplicación. El valor por defecto es <literal>rx-tx</literal> para la duplicación bidireccional, pero también se puede definir como <literal>rx-only</literal> para la duplicación unidireccional.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        El nombre del repositorio.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        Una vía de archivo al testigo creado (o <literal>-</literal> para leerlo desde la entrada estándar).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por ejemplo, en el clúster <literal>remote</literal>:
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1vbl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Adición manual de un conector de clúster</title>
    <para>
     Como alternativa al procedimiento descrito en la <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>, puede especificar conectores de forma manual. El daemon remoto <systemitem class="daemon">rbd-mirror</systemitem> necesitará acceso al clúster local para realizar la duplicación. Cree un nuevo usuario local de Ceph, que utilizará el daemon remoto <systemitem class="daemon">rbd-mirror</systemitem>; por ejemplo, <literal>rbd-mirror-peer</literal>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Utilice la siguiente sintaxis para añadir un clúster de Ceph conector duplicado con el comando <command>rbd</command>:
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Por defecto, el daemon <systemitem class="daemon">rbd-mirror</systemitem> debe tener acceso al archivo de configuración de Ceph ubicado en <filename>/etc/ceph/.<replaceable>NOMBRE_CLÚSTER</replaceable>.conf</filename>. Proporciona las direcciones IP de los MON del clúster conector y un anillo de claves para un cliente denominado <replaceable>NOMBRE_CLIENTE</replaceable> ubicado en las vías de búsqueda por defecto o personalizadas del anillo de claves; por ejemplo <filename>/etc/ceph/<replaceable>NOMBRE_CLUSTER</replaceable>.<replaceable>NOMBRE_CLIENTE</replaceable>.keyring</filename>.
    </para>
    <para>
     Como alternativa, el MON del clúster conector o la clave de cliente se pueden almacenar de forma segura en el almacén de claves de configuración de Ceph local. Para especificar los atributos de conexión del clúster conector al añadir un conector de duplicación, utilice las opciones <option>--remote-mon-host</option> y <option>--remote-key-file</option>. Por ejemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Eliminación de un clúster conector</title>
    <para>
     Para eliminar un clúster conector duplicado, especifique el subcomando <command>mirror pool peer remove</command>, el nombre del repositorio y el UUID del conector (disponible mediante el comando <command>rbd mirror pool info</command>):
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Repositorios de datos</title>
    <para>
     Al crear imágenes en el clúster de destino, <systemitem class="daemon">rbd-mirror</systemitem> selecciona un repositorio de datos de la siguiente manera:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Si el clúster de destino tiene un repositorio de datos por defecto configurado (con la opción de configuración <option>rbd_default_data_pool</option>), se utilizará.
      </para>
     </listitem>
     <listitem>
      <para>
       De lo contrario, si la imagen de origen utiliza un repositorio de datos independiente y existe un repositorio con el mismo nombre en el clúster de destino, se utilizará dicho repositorio.
      </para>
     </listitem>
     <listitem>
      <para>
       Si no se cumple ninguna de estas condiciones, no se definirá ningún repositorio de datos.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configuración de la imagen RBD</title>
   <para>
    A diferencia de la configuración del repositorio, la configuración de la imagen solo se debe realizar en un único clúster conector duplicado de Ceph.
   </para>
   <para>
    Las imágenes RBD duplicadas se designan como <emphasis>primary</emphasis> (principal) o <emphasis>non-primary</emphasis> (no principal). Se trata de una propiedad de la imagen, no del repositorio. Las imágenes designadas como no principales no se pueden modificar.
   </para>
   <para>
    Las imágenes suben de nivel a principal automáticamente cuando la duplicación se habilita por primera vez en una imagen (ya sea implícita, si el modo de duplicación del repositorio es &quot;pool&quot; [repositorio] y la imagen cuenta con la función de imagen transaccional habilitada, o explícita [consulte la <xref linkend="rbd-mirror-enable-image-mirroring"/>], mediante el comando <command>rbd</command>).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Habilitación de la duplicación de imágenes</title>
    <para>
     Si la duplicación está configurada en el modo <literal>image</literal>, es necesario habilitar explícitamente la duplicación para cada imagen del repositorio. Para habilitar la duplicación de una imagen concreta con <command>rbd</command>, especifique el subcomando <command>mirror image enable</command> junto con el nombre del repositorio y el de la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     El modo de imagen duplicada puede ser <literal>journal</literal> (registro) o <literal>snapshot</literal> (instantánea):
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (modo por defecto)</term>
      <listitem>
       <para>
        Cuando se configura en el modo <literal>journal</literal>, la duplicación utilizará la función de imagen RBD transaccional para replicar el contenido de la imagen. Si esta función aún no está habilitada en la imagen, se habilitará automáticamente.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        Cuando se configura en el modo de <literal>snapshot</literal>, la duplicación utilizará instantáneas de duplicación de la imagen RBD para replicar el contenido de la imagen. Una vez habilitada, se creará automáticamente una instantánea de duplicación inicial. Se pueden crear instantáneas adicionales mediante el comando <command>rbd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Habilitación de la función transaccional de imágenes</title>
    <para>
     La duplicación RBD utiliza las imágenes RBD transaccionales para garantizar que la imagen replicada siempre esté protegida contra fallos. Cuando se utiliza el modo de duplicación <literal>image</literal>, la función transaccional se habilita automáticamente si la duplicación está habilitada en la imagen. Cuando se utiliza el modo de duplicación <literal>pool</literal>, antes de que una imagen se pueda duplicar en un clúster conector, la función de imagen RBD transaccional debe estar habilitada. Se puede habilitar en el momento de crear la imagen proporcionando la opción <option>‑‑image-feature exclusive-lock,journaling</option> al comando <command>rbd</command>.
    </para>
    <para>
     Como alternativa, la función de registro transaccional se puede habilitar de forma dinámica en las imágenes RBD preexistentes. Para habilitar el registro transaccional, especifique el subcomando <command>feature enable</command>, el nombre del repositorio y la imagen y el nombre de la función:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>dependencia de opciones</title>
     <para>
      La función <option>journaling</option> (registro transaccional) depende de la función <option>exclusive-lock</option> (bloqueo exclusivo). Si la función <option>exclusive-lock</option> no está habilitada, deberá habilitarla antes que la función <option>journaling</option>.
     </para>
    </note>
    <tip>
     <para>
      Puede habilitar la función transaccional en todas las imágenes nuevas por defecto añadiendo <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option> al archivo de configuración de Ceph.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Creación de instantáneas de duplicación de la imagen</title>
    <para>
     Si se utiliza la duplicación basada en instantáneas, será necesario crear instantáneas de duplicación siempre que se desee duplicar el contenido modificado de la imagen RBD. Para crear una instantánea de duplicación manualmente con <command>rbd</command>, especifique el comando <command>mirror image snapshot</command> junto con el nombre del repositorio y la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Por defecto, solo se crearán tres instantáneas de duplicación por imagen. La instantánea de duplicación más reciente se elimina automáticamente si se alcanza el límite. El límite se puede anular mediante la opción de configuración <option>rbd_mirroring_max_mirroring_snapshots</option> si es necesario. Además, las instantáneas de duplicación se suprimen automáticamente cuando se elimina la imagen o cuando se inhabilita la duplicación.
    </para>
    <para>
     Las instantáneas de duplicación también se pueden crear automáticamente de forma periódica si se programa. La instantánea de duplicación se puede programar a nivel global, por repositorio o por imagen. Se pueden definir varias programaciones de instantáneas de duplicación en cualquier nivel, pero solo se ejecutarán las programaciones más específicas que coincidan con una imagen duplicada individual.
    </para>
    <para>
     Para crear una programación de instantáneas de duplicación con <command>rbd</command>, especifique el comando <command>mirror snapshot schedule add</command> junto con un nombre de repositorio o imagen opcional, un intervalo y una hora de inicio opcional.
    </para>
    <para>
     El intervalo se puede especificar en días, horas o minutos mediante los sufijos <option>d</option>, <option>h</option> o <option>m</option>, respectivamente. La hora de inicio opcional se puede especificar mediante el formato de hora ISO 8601. Por ejemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Para eliminar una programación de instantáneas de duplicación con <command>rbd</command>, especifique el comando <command>mirror snapshot schedule remove</command> con opciones que coincidan con el comando correspondiente para añadir una programación.
    </para>
    <para>
     Para  todas las programaciones de instantáneas para un nivel específico (global, repositorio o imagen) con <command>rbd</command>, especifique el comando <command>mirror snapshot schedule ls</command> junto con un nombre de repositorio o imagen opcional. Además, se puede especificar la opción <option>--recursive</option> para mostrar todas las programaciones en el nivel especificado e inferior. Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Para saber cuándo se crearán las siguientes instantáneas para las imágenes RBD de duplicación basadas en instantáneas con <command>rbd</command>, especifique el comando <command>mirror snapshot schedule status</command> junto con un nombre de repositorio o imagen opcional. Por ejemplo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Inhabilitación de la duplicación de imágenes</title>
    <para>
     Para habilitar la duplicación de una imagen concreta, especifique el subcomando <command>mirror image disable</command> junto con el nombre del repositorio y el de la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Subida y bajada de nivel de imágenes</title>
    <para>
     En una situación de failover en la que sea necesario mover la designación primara a la imagen del clúster conector, debe detener el acceso a la imagen principal, bajarla de nivel, subir el nivel de la nueva imagen principal y reanudar el acceso a la imagen en el clúster alternativo.
    </para>
    <note>
     <title>subida de nivel forzosa</title>
     <para>
      Puede forzar la subida de nivel mediante la opción <option>‑‑force</option>. Es necesario forzar la subida de nivel cuando la bajada de nivel no se puede propagar al clúster conector (por ejemplo, en caso de fallo del clúster o interrupción de la comunicación). Esto provocará una situación de división entre ambos conectores y la imagen dejará de sincronizarse hasta que se emita un subcomando <command>resync</command>.
     </para>
    </note>
    <para>
     Para bajar de nivel una imagen y convertirla en no principal, especifique el subcomando <command>mirror image demote</command> junto con el nombre del repositorio y el de la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Para bajar de nivel todas las imágenes principales de un repositorio a no principales, especifique el subcomando <command>mirror pool demote</command> junto con el nombre del repositorio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Para subir de nivel una imagen y convertirla en principal, especifique el subcomando <command>mirror image promote</command> junto con el nombre del repositorio y el de la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Para subir de nivel todas las imágenes de un repositorio a principales, especifique el subcomando <command>mirror pool promote</command> junto con el nombre del repositorio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>división de la carga de E/S</title>
     <para>
      Puesto que el estado primario o no primario se establece para cada imagen, es posible que haya dos clústeres que dividan la carga de E/S y el failover o failback por fases.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Forzado de la resincronización de la imagen</title>
    <para>
     Si el daemon <systemitem class="daemon">rbd-mirror</systemitem> detecta un evento dividido, no intentará duplicar la imagen afectada hasta que se corrija. Para reanudar la duplicación de una imagen, primero baje de nivel la imagen considerada obsoleta y, a continuación, realice una petición de resincronización de la imagen principal. Para pedir una resincronización de la imagen, especifique el subcomando <command>mirror image resync</command> junto con el nombre del repositorio y el de la imagen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Comprobación del estado de la duplicación</title>
   <para>
    El estado de replicación de clúster conector se almacena para cada imagen principal duplicada. Este estado se puede recuperar mediante los subcomandos <command>mirror image status</command> y <command>mirror pool status</command>:
   </para>
   <para>
    Para pedir el estado de la imagen duplicada, especifique el subcomando <command>mirror image status</command> junto con el nombre del repositorio y el de la imagen:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Para pedir el estado resumido del repositorio de duplicación, especifique el subcomando <command>mirror pool status</command> junto con el nombre del repositorio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Si se añade la opción <option>‑‑verbose</option> al subcomando <command>mirror pool status</command>, también se mostrará la información de estado de todas las imágenes duplicadas del repositorio.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Ajustes de caché</title>

  <para>
   La implementación del espacio de usuario del dispositivo de bloques Ceph (<systemitem>librbd</systemitem>) no puede hacer uso del caché de paginación de Linux. Por lo tanto, incluye su propio almacenamiento en caché en memoria. El almacenamiento en caché de RBD tiene un comportamiento similar al almacenamiento en caché del disco duro. Cuando el sistema operativo envía una barrera o una petición de vaciado, todos los datos &quot;sucios&quot; se escriben en los OSD. Esto significa que el uso del almacenamiento en caché en modo write-back es tan seguro como usar un disco duro físico de buen comportamiento con una máquina virtual que envía correctamente los vaciados. La memoria caché utiliza un algoritmo <emphasis>LRU</emphasis> (del inglés Least Recently Used, menos usadas recientemente) y, en el modo write-back, puede combinar peticiones adyacentes para obtener un mejor rendimiento.
  </para>

  <para>
   Ceph admite el modo write-back de almacenamiento en caché para los RBD. Para habilitarlo, ejecute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Por defecto, <systemitem>librbd</systemitem> no realiza ningún almacenamiento en caché. Las escrituras y lecturas van directamente al clúster de almacenamiento. Las escrituras solo se devuelven cuando los datos están en el disco en todas las réplicas. Con el almacenamiento en caché habilitado, las escrituras se devuelven de inmediato, a menos que haya más bytes sin vaciar de los definidos en la opción <option>rbd cache max dirty</option>. En tal caso, la escritura activa la escritura diferida y se bloquea hasta que se vacían suficientes bytes.
  </para>

  <para>
   Ceph admite el modo write-through de almacenamiento en caché para los RBD. Es posible definir el tamaño de la memoria caché y establecer destinos y límites para cambiar del modo write-back al modo write-through de almacenamiento en caché. Para habilitar el modo write-through, ejecute:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Esto significa que las escrituras solo se devuelven cuando los datos están en el disco en todas las réplicas, pero las lecturas pueden provenir de la memoria caché. La memoria caché está en memoria en el cliente y cada imagen RBD tiene su propia memoria caché. Puesto que la memoria caché es local para el cliente, si hay otros usuarios que acceden a la imagen, la coherencia se pierde. Ejecutar GFS u OCFS sobre el RBD no funcionará si el almacenamiento en caché está habilitado.
  </para>

  <para>
   Los siguientes parámetros afectan al comportamiento de los dispositivos de bloques RADOS. Para definirlos, utilice la categoría <literal>client</literal>:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Habilita el almacenamiento en caché para el dispositivo de bloques RADOS (RBD). El valor por defecto es &quot;true&quot; (verdadero).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      El tamaño de la memoria caché del RBD en bytes. El valor por defecto es 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      El límite de bytes &quot;sucios&quot; en el que la memoria caché activa el modo write-back. El valor de <option>rbd cache max dirty</option> debe ser menor que el de <option>rbd cache size</option>. Si se define en 0, se utiliza el almacenamiento en caché en modo write-through. El valor por defecto es 24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      El &quot;destino sucio&quot; antes de que la memoria caché comience a escribir datos en el almacenamiento de datos. No bloquea las escrituras en la memoria caché. El valor por defecto es 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      El número de segundos que los datos sucios están en la memoria caché antes de que se inicie la escritura diferida. El valor por defecto es 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Comienza en modo write-through y cambia al modo write-back después de que se reciba la primera petición de vaciado. Habilitar esta opción es una medida conservadora pero segura en caso de que las máquinas virtuales que se ejecutan en <systemitem>rbd</systemitem> sean demasiado antiguas para enviar vaciados (por ejemplo, el controlador virtio de Linux antes del kernel 2.6.32). El valor por defecto es &quot;true&quot; (verdadero).
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>Ajuste de QoS</title>

  <para>
   Por norma general, la calidad de servicio (QoS) hace referencia a los métodos de priorización del tráfico y de reserva de recursos. Es particularmente importante para el transporte del tráfico con requisitos especiales.
  </para>

  <important>
   <title>no es compatible con iSCSI</title>
   <para>
    Los ajustes de QoS siguientes solo se utilizan en la implementación <systemitem class="daemon">librbd</systemitem> del RBD de espacio de nombres, pero <emphasis>no</emphasis> por la implementación <systemitem>kRBD</systemitem>. Dado que iSCSI utiliza <systemitem>kRBD</systemitem>, no utiliza los valores de QoS. Sin embargo, para iSCSI puede configurar QoS en la capa de dispositivo de bloques del kernel mediante instalaciones del kernel estándar.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      El límite deseado de operaciones de E/S por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      El límite deseado de bytes de E/S por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      El límite deseado de operaciones de lectura por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      El límite deseado de operaciones de escritura por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      El límite deseado de bytes de lectura por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      El límite deseado de bytes de escritura por segundo. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de operaciones de E/S. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de bytes de E/S. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de operaciones de lectura. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de operaciones de escritura. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de bytes de lectura. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      El límite de ráfaga deseado de bytes de escritura. El valor por defecto es 0 (sin límite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      La pulsación de programación mínima (en milisegundos) para QoS. El valor por defecto es 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Ajustes de lectura anticipada</title>

  <para>
   El dispositivo de bloques RADOS admite la lectura anticipada y la captura previa para optimizar las lecturas pequeñas y secuenciales. Normalmente, el sistema operativo invitado se encargaría de gestionar esta función en una máquina virtual, pero es posible que los cargadores de arranque no emitan lecturas eficientes. La lectura anticipada se inhabilita automáticamente si el almacenamiento en caché está inhabilitado.
  </para>

  <important>
   <title>no es compatible con iSCSI</title>
   <para>
    Los ajustes de lectura anticipada siguientes solo se utilizan en la implementación <systemitem class="daemon">librbd</systemitem> del RBD de espacio de nombres, pero <emphasis>no</emphasis> por la implementación <systemitem>kRBD</systemitem>. Dado que iSCSI utiliza <systemitem>kRBD</systemitem>, no utiliza los ajustes de lectura anticipada. Sin embargo, para iSCSI puede configurar la lectura anticipada en la capa de dispositivo de bloques del kernel mediante instalaciones del kernel estándar.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      El número de peticiones de lectura secuenciales necesarias para activar la lectura anticipada. El valor por defecto es 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      El tamaño máximo de una petición de lectura anticipada. Si se define en 0, la lectura anticipada estará inhabilitada. El valor por defecto es 512 kB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Después de que se hayan leído este número de bytes de una imagen RBD, la lectura anticipada se inhabilita para esa imagen hasta que se cierra. Esto permite que el sistema operativo invitado recupere la función de lectura anticipada cuando se arranca. Si se define en 0, la lectura anticipada sigue habilitada. El valor por defecto es 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Funciones avanzadas</title>

  <para>
   El dispositivo de bloques RADOS admite funciones avanzadas que mejoran la funcionalidad de las imágenes RBD. Puede especificar las funciones en la línea de comandos cuando se crea una imagen RBD o en el archivo de configuración de Ceph mediante la opción <option>rbd_default_features</option>.
  </para>

  <para>
   Es posible especificar los valores de la opción <option>rbd_default_features</option> de dos maneras:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Como una suma de los valores internos de las funciones. Cada función tiene su propio valor interno; por ejemplo, la &quot;layering&quot; tiene 1 y &quot;fast-diff&quot; tiene 16. Por lo tanto, para activar estas dos funciones por defecto, incluya lo siguiente:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     Como una lista separada por comas de funciones. El ejemplo anterior tendrá el siguiente aspecto:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>funciones no compatibles con iSCSI</title>
   <para>
    Las imágenes RBD con las siguientes funciones no son compatibles con iSCSI: <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option> y <option>striping</option>
   </para>
  </note>

  <para>
   A continuación se muestra una lista de funciones avanzadas de RBD:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      La disposición en capas permite utilizar la clonación.
     </para>
     <para>
      El valor interno es 1, el valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      La repartición striping distribuye los datos entre varios objetos y ayuda a las cargas de trabajo de lectura y escritura secuenciales mediante paralelismo. Evita los cuellos de botella en un solo nodo en dispositivos de bloques RADOS de gran volumen u ocupación.
     </para>
     <para>
      El valor interno es 2, el valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Si está habilitado, requiere que un cliente obtenga un bloqueo en un objeto antes de realizar una escritura. Habilita el bloqueo exclusivo solo si un único cliente accede a una imagen al mismo tiempo. El valor interno es 4. El valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      La compatibilidad con el mapa de objetos depende de la compatibilidad con el bloqueo exclusivo. Los dispositivos de bloque son de provisión ligera, lo que significa que solo almacenan datos que realmente existen. La compatibilidad con el mapa de objetos ayuda a realizar un seguimiento de los objetos que realmente existen (que tienen datos almacenados en una unidad). Al habilitar la compatibilidad con el mapa de objetos, se aceleran las operaciones de E/S para clonar, importar y exportar una imagen escasamente poblada y suprimirla.
     </para>
     <para>
      El valor interno es 8, el valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      La compatibilidad con diff rápida depende de la compatibilidad con el mapa de objetos y con el bloqueo exclusivo. Añade otra propiedad al mapa de objetos, lo que hace que sea mucho más rápido generar diffs entre las instantáneas de una imagen y el uso de datos real de una instantánea.
     </para>
     <para>
      El valor interno es 16, el valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      El aplanamiento profundo hace que <command>rbd flatten</command> (consulte la <xref linkend="rbd-flatten-cloned-image"/>) funcione en todas las instantáneas de una imagen, además de en la propia imagen. Sin él, las instantáneas de una imagen seguirían dependiendo de la imagen padre y, por lo tanto, no se podría suprimir la imagen padre hasta que se eliminaran las instantáneas. El aplanamiento profundo provoca que una imagen padre sea independiente de sus clones, incluso si tienen instantáneas.
     </para>
     <para>
      El valor interno es 32, el valor por defecto es &quot;sí&quot;.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      La compatibilidad con el registro transaccional depende de la compatibilidad con el bloqueo exclusivo. El registro transaccional guarda todas las modificaciones en una imagen en el orden en el que se producen. La duplicación del RBD (consulte la <xref linkend="ceph-rbd-mirror"/>) utiliza el diario para replicar una imagen coherente con la detención por fallo en un clúster remoto.<literal></literal>
     </para>
     <para>
      El valor interno es 64 y el valor por defecto es &quot;no&quot;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Asignación del RBD utilizando clientes de kernel antiguos</title>

  <para>
   Es posible que los clientes antiguos (por ejemplo, SLE11 SP4) no puedan asignar imágenes RBD porque un clúster distribuido con SUSE Enterprise Storage 7.1 aplica algunas funciones (tanto funciones de nivel de imagen RBD como funciones de nivel de RADOS) que estos clientes antiguos no admiten. Si esto sucede, los registros del OSD mostrarán mensajes similares a los siguientes:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>el cambio de tipos de depósito del mapa de CRUSH provoca un reequilibrio masivo</title>
   <para>
    Si tiene previsto cambiar los tipos de depósito del mapa de CRUSH entre &quot;straw&quot; y &quot;straw2&quot;, hágalo de una manera planificada. Tenga previsto que el impacto en la carga del clúster será significativo, ya que provocará un reequilibrio masivo del clúster.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Inhabilite las funciones de la imagen RBD que no sean compatibles. Por ejemplo:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Cambie los tipos de depósito del mapa de CRUSH de &quot;straw2&quot; a &quot;straw&quot;:
    </para>
    <substeps>
     <step>
      <para>
       Guarde el mapa de CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Descompile el mapa de CRUSH.
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Edite el mapa de CRUSH y sustituya &quot;straw2&quot; por &quot;straw&quot;.
      </para>
     </step>
     <step>
      <para>
       Vuelva a compilar el mapa de CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Defina el nuevo mapa de CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Habilitación de dispositivos de bloques y Kubernetes</title>

  <para>
   Puede usar RBD de Ceph con Kubernetes 1.13 y versiones posteriores mediante el controlador <literal>ceph-csi</literal>. Este controlador proporciona imágenes RBD de forma dinámica para respaldar los volúmenes de Kubernetes y asigna estas imágenes RBD como dispositivos de bloques (montando opcionalmente un sistema de archivos incluido en la imagen) en nodos de trabajo que ejecutan pods que hacen referencia a un volumen respaldado por RBD.
  </para>

  <para>
   Para utilizar dispositivos de bloques de Ceph con Kubernetes, debe instalar y configurar <literal>ceph-csi</literal> en el entorno de Kubernetes.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> utiliza los módulos del kernel de RBD por defecto, que quizás no admitan todos los elementos ajustables de Ceph CRUSH o las funciones de imagen RBD.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Por defecto, los dispositivos de bloques de Ceph utilizan el repositorio de RBD. Cree un repositorio para el almacenamiento de volúmenes de Kubernetes. Asegúrese de que el clúster de Ceph se está ejecutando y, a continuación, cree el repositorio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Utilice la herramienta RBD para inicializar el repositorio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Cree un nuevo usuario para Kubernetes y <literal>ceph-csi</literal>. Ejecute lo siguiente y guarde la clave generada:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> requiere un objeto ConfigMap almacenado en Kubernetes para definir las direcciones del monitor de Ceph para el clúster de Ceph. Recopile tanto el fsid exclusivo del clúster de Ceph como las direcciones del monitor:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Genere un archivo <filename>csi-config-map.yaml</filename> similar al ejemplo siguiente, sustituyendo el FSID por <literal>clusterID</literal> y las direcciones de monitor por <literal>monitors</literal>.
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Cuando se genere, almacene el nuevo objeto ConfigMap en Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> requiere las credenciales de cephx para comunicarse con el clúster de Ceph. Genere un archivo <filename>csi-rbd-secret.yaml</filename> similar al ejemplo siguiente, utilizando el ID de usuario de Kubernetes y la clave de cephx recién creados:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Cuando se genere, guarde el nuevo objeto de secreto en Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Cree los objetos ServiceAccount y RBAC ClusterRole/ClusterRoleBinding necesarios de Kubernetes. No es obligatorio personalizar estos objetos para el entorno de Kubernetes y, por lo tanto, se pueden utilizar directamente desde los archivos YAML de distribución de <literal>ceph-csi</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Cree el aprovisionador <literal>ceph-csi</literal> y los complementos de nodo:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Por defecto, los archivos YAML del aprovisionador y del complemento de nodo extraerán la versión de desarrollo del contenedor de <literal>ceph-csi</literal>. Los archivos YAML deben actualizarse para utilizar una versión de lanzamiento.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Uso de dispositivos de bloques de Ceph en Kubernetes</title>
   <para>
    Kubernetes StorageClass define una clase de almacenamiento. Se pueden crear varios objetos StorageClass para asignarlos a diferentes niveles y funciones de calidad de servicio. Por ejemplo, NVMe frente a los repositorios basados en discos duros.
   </para>
   <para>
    Para crear un objeto StorageClass de <literal>ceph-csi</literal> que se asigne al repositorio de Kubernetes creado anteriormente, se puede usar el siguiente archivo YAML, después de asegurarse de que la propiedad <literal>clusterID</literal> coincide con el FSID del clúster de Ceph:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    Una <literal>PersistentVolumeClaim</literal> es una petición de recursos de almacenamiento abstractos por parte de un usuario. La <literal>PersistentVolumeClaim</literal> se asociará a un recurso de pod para aprovisionar un <literal>PersistentVolume</literal>, que estaría respaldado por una imagen de bloque de Ceph. Se puede incluir un <option>volumeMode</option> opcional para seleccionar entre un sistema de archivos montado (por defecto) o un volumen basado en dispositivos de bloques en bruto.
   </para>
   <para>
    Con <literal>ceph-csi</literal>, especificando <option>Filesystem</option> para <option>volumeMode</option> puede admitir peticiones tanto <literal>ReadWriteOnce</literal> como <literal>ReadOnlyMany accessMode</literal>; y especificando <option>Block</option> para <option>volumeMode</option> puede admitir peticiones <literal>ReadWriteOnce</literal>, <literal>ReadWriteMany</literal> y <literal>ReadOnlyMany accessMode</literal>.
   </para>
   <para>
    Por ejemplo, para crear una petición <literal>PersistentVolumeClaim</literal> basada en bloques que utilice la <literal>ceph-csi-based StorageClass</literal> creada anteriormente, se puede utilizar el siguiente archivo YAML para pedir almacenamiento de bloques en bruto desde <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    A continuación, se muestra un ejemplo de asociación de la <literal>PersistentVolumeClaim</literal> anterior a un recurso de pod como dispositivo de bloques en bruto:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Para crear una <literal>PersistentVolumeClaim</literal> basad en un sistema de archivos que utilice la clase c<literal>ceph-csi-based StorageClass</literal> creada anteriormente, se puede utilizar el siguiente archivo YAML para pedir un sistema de archivos montado (respaldado por una imagen RBD) desde la clase <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    A continuación, se muestra un ejemplo de asociación de la <literal>PersistentVolumeClaim</literal> anterior a un recurso de pod como sistema de archivos montado:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
