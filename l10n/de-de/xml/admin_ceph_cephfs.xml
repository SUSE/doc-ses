<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Cluster-Dateisystem</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In diesem Kapitel werden Verwaltungsaufgaben beschrieben, die normalerweise nach der Einrichtung des Clusters und dem Export des CephFS ausgeführt werden. Weitere Informationen zum Einrichten des CephFS finden Sie im <xref linkend="deploy-cephadm-day2-service-mds"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Einhängen von CephFS</title>

  <para>
   Wenn das Dateisystem erstellt und der MDS aktiv ist, sind Sie bereit, das Dateisystem von einem Client-Host aus einzuhängen.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Vorbereiten des Clients</title>
   <para>
    Wenn auf dem Client-Host SUSE Linux Enterprise 12 SP2 oder höher ausgeführt wird, ist das System bereit, das CephFS ohne weitere Anpassung einzuhängen.
   </para>
   <para>
    Wenn auf dem Client-Host SUSE Linux Enterprise 12 SP1 ausgeführt wird, müssen Sie alle neuesten Patches anwenden, bevor Sie das CephFS einhängen.
   </para>
   <para>
    In jedem Fall ist in SUSE Linux Enterprise alles enthalten, was zum Einhängen von CephFS benötigt wird. SUSE Enterprise Storage 7 wird nicht benötigt.
   </para>
   <para>
    Zur Unterstützung der vollständigen <command>mount</command>-Syntax sollte das Paket
    <package>ceph-common</package> (das im Lieferumfang von SUSE Linux Enterprise enthalten ist) installiert werden, bevor Sie versuchen, CephFS einzuhängen.
   </para>
   <important>
    <para>
     Ohne das Paket <package>ceph-common</package> (und somit ohne <command>mount.ceph</command>-Helper) müssen die IPs der Monitors anstelle ihrer Namen verwendet werden. Dies liegt daran, dass der Kernel-Client keine Namensauflösung durchführen kann.
    </para>
    <para>
     Die grundlegende Einhängesyntax lautet:
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Erstellen einer geheimen Datei</title>
   <para>
    Der Ceph-Cluster wird mit standardmäßig eingeschalteter Authentifizierung ausgeführt. Sie sollten eine Datei erstellen, in der Ihr geheimer Schlüssel (nicht der Schlüsselbund selbst) gespeichert wird. Gehen Sie folgendermaßen vor, um den geheimen Schlüssel für einen bestimmten Benutzer abzurufen und dann die Datei zu erstellen:
   </para>
   <procedure>
    <title>Erstellen eines geheimen Schlüssels</title>
    <step>
     <para>
      Sehen Sie sich den Schlüssel für den bestimmten Benutzer in einer Schlüsselbunddatei an:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Kopieren Sie den Schlüssel des Benutzers, der das eingehängte Ceph FS-Dateisystem verwenden wird. Normalerweise sieht der Schlüssel in etwa so aus:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine Datei mit dem Benutzernamen als Teil des Dateinamens, wie zum Beispiel <filename>/etc/ceph/admin.secret</filename> für den Benutzer <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Fügen Sie den Schlüsselwert in der im vorigen Schritt erstellten Datei ein.
     </para>
    </step>
    <step>
     <para>
      Legen Sie die ordnungsgemäßen Zugriffsrechte für die Datei fest. Ausschließlich der Benutzer sollte die Datei lesen können. Andere dürfen keine Zugriffsrechte dafür erhalten.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Einhängen von CephFS</title>
   <para>
    CephFS wird mit dem Befehl <command>mount</command> eingehängt. Sie müssen den Hostnamen oder die IP-Adresse des Monitors angeben. Da in SUSE Enterprise Storage standardmäßig die <systemitem>cephx</systemitem>-Authentifizierung aktiviert ist, müssen Sie einen Benutzernamen und das entsprechende Geheimnis angeben:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Da das vorige Kommando im Shell-Verlauf bleibt, ist es sicherer, das Geheimnis aus einer Datei zu lesen:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Beachten Sie, dass die Geheimnisdatei nur das tatsächliche Schlüsselbundgeheimnis enthalten sollte. In unserem Beispiel enthält die Datei dann nur die folgende Zeile:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>Geben Sie mehrere Monitore an</title>
    <para>
     Es ist sinnvoll, in der <command>mount</command>-Kommandozeile mehrere Monitors durch Komma getrennt anzugeben, für den Fall, dass zum Zeitpunkt des Einhängens ein Monitor zufällig inaktiv ist. Jede Monitoradresse hat die Form <literal>host[:port]</literal>. Wenn der Port nicht angegeben wird, ist es standardmäßig Port 6789.
    </para>
   </tip>
   <para>
    Erstellen sie den Einhängepunkt am lokalen Host:
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Hängen Sie das CephFS ein:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Ein Unterverzeichnis <filename>subdir</filename> kann angegeben werden, wenn eine Teilmenge des Dateisystems eingehängt werden soll:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Sie können mehr als einen Monitor-Host im Kommando <command>mount</command> angeben:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>Lesezugriff auf das Stammverzeichnis</title>
    <para>
     Wenn Clients mit Pfadbeschränkungen verwendet werden, muss der MDS Lesezugriff auf das Stammverzeichnis enthalten. Ein Schlüsselbund sieht beispielsweise in etwa wie folgt aus:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     Der Abschnitt <literal>allow r path=/</literal> bedeutet, dass pfadbeschränkte Clients das Root-Volume sehen können, jedoch nicht darauf schreiben dürfen. In Anwendungsfällen, in denen die vollständige Isolierung vorausgesetzt wird, könnte dies ein Problem darstellen.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Aushängen von CephFS</title>

  <para>
   CephFS wird mit dem Kommando <command>umount</command> ausgehängt:
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>Einhängen von CephFS in <filename>/etc/fstab</filename></title>

  <para>
   Fügen Sie zum automatischen Einhängen von CephFS bei Client-Start die entsprechende Zeile in die Tabelle <filename>/etc/fstab</filename> seines Dateisystems ein:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Mehrere aktive MDS-Daemons (Aktiv/Aktiv-MDS)</title>

  <para>
   CephFS wird standardmäßig für einen einzelnen aktiven MDS-Daemon konfiguriert. Zum Skalieren der Metadatenleistung für große Systeme können Sie mehrere aktive MDS-Daemons aktivieren. Dadurch wird der Metadaten-Workload untereinander aufgeteilt.
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>Verwenden von Aktiv/Aktiv-MDS</title>
   <para>
    Erwägen Sie, mehrere aktive MDS-Daemons zu verwenden, falls Ihre Metadatenleistung bei einem standardmäßigen einzelnen MDS einen Engpass erfahren würde.
   </para>
   <para>
    Das Hinzufügen weiterer Daemons erhöht nicht die Leistung bei allen Workload-Typen. Beispielsweise profitiert eine Einzelanwendung, die auf einem einzelnen Client ausgeführt wird, nicht von einer höheren Anzahl von MDS-Daemons, es sei denn, die Anwendung führt sehr viele Metadaten-Operationen gleichzeitig aus.
   </para>
   <para>
    Workloads, die normalerweise von einer höheren Anzahl aktiver MDS-Daemons profitieren, sind Workloads mit vielen Clients, die eventuell in vielen verschiedenen Verzeichnissen arbeiten.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Vergrößern des aktiven MDS-Clusters</title>
   <para>
    Jedes CephFS-Dateisystem verfügt über eine <option>max_mds</option>-Einstellung, die steuert, wie viele Rangstufen erstellt werden. Die tatsächliche Anzahl der Rangstufen im Dateisystem wird nur dann erhöht, wenn ein Ersatz-Daemon verfügbar ist, der die neue Rangstufe übernehmen kann. Wenn beispielsweise nur ein MDS-Daemon ausgeführt wird und <option>max_mds</option> auf „2“ festgelegt ist, wird keine zweite Rangstufe erstellt.
   </para>
   <para>
    Im folgenden Beispiel legen wir die Option <option>max_mds</option> auf „2“ fest, um eine neue Rangstufe abgesehen von der standardmäßigen Rangstufe zu erstellen. Führen Sie zum Anzeigen der Änderungen <command>ceph status</command> aus bevor und nachdem Sie <option>max_mds</option> festlegen und beobachten Sie die Zeile, die <literal>fsmap</literal> enthält:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    Die neu erstellte Rangstufe (1) durchläuft den Zustand „wird erstellt“ und wird dann in den Zustand „aktiv“ versetzt.
   </para>
   <important>
    <title>Standby-Daemons</title>
    <para>
     Auch bei mehreren aktiven Daemons benötigt ein hochverfügbares System weiterhin Standby-Daemons, die übernehmen, wenn einer der Server, auf dem ein aktiver Daemon ausgeführt wird, ausfällt.
    </para>
    <para>
     Folglich ist die praktische maximale Anzahl von <option>max_mds</option> bei hochverfügbaren Systemen ein Server weniger als die Gesamtanzahl der MDS-Server in Ihrem System. Um im Fall mehrerer Serverausfälle verfügbar zu bleiben, müssen Sie die Anzahl der Standby-Daemons im System entsprechend der Anzahl der Serverausfälle anpassen, die sie kompensieren müssen.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Reduzieren der Anzahl von Rangstufen</title>
   <para>
    Alle Rangstufen, einschließlich der zu entfernenden Rangstufen, müssen zunächst aktiv sein. Dies bedeutet, dass mindestens <option>max_mds</option> MDS-Daemons verfügbar sein müssen.
   </para>
   <para>
    Legen Sie zunächst <option>max_mds</option> auf eine kleinere Anzahl fest. Gehen Sie beispielsweise zu einem einzelnen aktiven MDS zurück:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Manuelles Anheften von Verzeichnisbäumen an eine Rangstufe</title>
   <para>
    Bei Konfigurationen mit mehreren aktiven Metadaten-Servern wird ein Ausgleichsprogramm ausgeführt, das die Metadatenlast gleichmäßig im Cluster verteilt. Dies funktioniert normalerweise bei den meisten Benutzern ganz gut, doch manchmal ist es wünschenswert, das dynamische Ausgleichsprogramm durch explizite Zuordnungen von Metadaten zu bestimmten Rangstufen außer Kraft zu setzen. Dadurch erhalten der Administrator oder die Benutzer die Möglichkeit, die Anwendungslast gleichmäßig zu verteilen oder die Auswirkungen auf die Metadatenanforderungen der Benutzer auf den gesamten Cluster einzuschränken.
   </para>
   <para>
    Der zu diesem Zweck bereitgestellte Mechanismus wird „Export-Pin“ genannt. Es handelt sich um ein erweitertes Attribut von Verzeichnissen. Der Name dieses erweiterten Attributs ist <literal>ceph.dir.pin</literal>. Benutzer können dieses Attribut mit Standardkommandos festlegen:
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    Der Wert (<option>-v</option>) des erweiterten Attributs ist die Rangstufe, zu dem der Verzeichnisunterbaum zugewiesen wird. Ein Standardwert von -1 gibt an, dass das Verzeichnis nicht angeheftet wird.
   </para>
   <para>
    Ein Verzeichnisexport-Pin wird vom nächstgelegenen übergeordneten Verzeichnis mit einem festgelegten Export-Pin übernommen. Daher betrifft die Festlegung eines Export-Pins auch alle untergeordneten Verzeichnisse. Der Pin des übergeordneten Verzeichnisses kann jedoch durch Festlegen des Verzeichnisexport-Pins des untergeordneten Verzeichnisses außer Kraft gesetzt werden. Beispiel:
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Failover-Verwaltung</title>

  <para>
   Wenn ein MDS-Daemon die Kommunikation mit dem Monitor stoppt, dann wartet der Monitor <option>mds_beacon_grace</option> Sekunden (standardmäßig 15 Sekunden), bevor er den Daemon als <emphasis>laggy</emphasis> (langsam) kennzeichnet. Sie können einen oder mehrere „standby“-Daemons konfigurieren, die bei einem MDS-Daemon-Failover übernehmen.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Konfigurieren von Standby-Replay</title>
   <para>
    Jedes CephFS-Dateisystem kann so konfiguriert werden, dass es Standby-Replay-Daemons hinzufügt. Diese Standby-Daemons folgen dem Metadatenjournal des aktiven MDS, um die Failover-Zeit zu reduzieren, falls der aktive MDS nicht mehr verfügbar ist. Jeder aktive MDS darf nur einen Standby-Replay-Daemon haben, der ihm folgt.
   </para>
   <para>
    Konfigurieren Sie Standby-Replay auf einem Dateisystem mit folgendem Kommando:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    Wenn diese Option aktiviert ist, weisen die Monitore verfügbare Standby-Daemons zu, die den aktiven MDS in diesem Dateisystem folgen.
   </para>
   <para>
    Wenn ein Daemon den Status Standby-Replay aufweist, wird er nur als Standby für die Rangstufe verwendet, der er folgt. Wenn eine andere Rangstufe ausfällt, wird dieser Standby-Replay-Daemon nicht als Ersatz verwendet, auch wenn keine anderen Standbys verfügbar sind. Aus diesem Grund ist es ratsam, dass jeder aktive MDS über einen Standby-Replay-Daemon verfügt, wenn Standby-Replay verwendet wird.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Festlegen von CephFS-Kontingenten</title>

  <para>
   Sie können Kontingente für beliebige Unterverzeichnisse des Ceph-Dateisystems anlegen. Das Kontingent beschränkt entweder die Anzahl der <emphasis role="bold">Byte</emphasis> oder die Anzahl der <emphasis role="bold">Dateien</emphasis>, die unterhalb des angegebenen Punkts in der Verzeichnishierarchie gespeichert werden können.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Einschränkungen von CephFS-Kontingenten</title>
   <para>
    Kontingente in CephFS unterliegen folgenden Einschränkungen:
   </para>
   <variablelist>
    <varlistentry>
     <term>Kontingente sind kooperativ und konkurrieren nicht untereinander.</term>
     <listitem>
      <para>
       Ceph-Kontingente sind darauf angewiesen, dass der Client, der das Dateisystem einhängt, nicht mehr darin schreibt, sobald ein bestimmter Grenzwert erreicht ist. Der Server-Teil kann nicht verhindern, dass ein böswilliger Client beliebig viele Daten schreibt. Es ist nicht zulässig, das Füllen des Dateisystems in Umgebungen, in denen die Clients als überhaupt nicht verbürgt gelten, mithilfe von Kontingenten zu verhindern.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Kontingente sind nicht absolut genau.</term>
     <listitem>
      <para>
       Prozesse, die in das Dateisystem schreiben, werden kurz nach Erreichen des Kontingentgrenzwerts angehalten. Es ist nicht zu verhindern, dass eine gewisse Datenmenge geschrieben wird, die den konfigurierten Grenzwert übersteigt. Client-Schreibvorgänge werden innerhalb weniger Zehntelsekunden nach dem Überschreiten des konfigurierten Grenzwerts angehalten.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Kontingente werden ab Version 4.17 im Kernel-Client implementiert.</term>
     <listitem>
      <para>
       Kontingente werden durch den Benutzerbereichs-Client (libcephfs, ceph-fuse) unterstützt. Die Linux-Kernel-Clients 4.17 und höher unterstützen CephFS-Kontingente auf Clustern mit SUSE Enterprise Storage 7. Kernel-Clients (selbst die neueren Versionen) können Kontingente nicht auf älteren Clustern verarbeiten, auch wenn sie die erweiterten Attribute der Kontingente festlegen können. Kernel ab SLE12-SP3 enthalten bereits die erforderlichen Backports zur Verarbeitung von Kontingenten.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Konfigurieren Sie die Kontingente nur mit Vorsicht, wenn pfadbasierte Einhängeeinschränkungen gelten.</term>
     <listitem>
      <para>
       Der Client kann die Kontingente nur dann durchsetzen, wenn er den Zugriff auf den Verzeichnis-Inode besitzt, auf dem die Kontingente konfiguriert sind. Wenn der Zugriff des Clients auf einen bestimmten Pfad (z. B. <filename>/home/user</filename>) gemäß der MDS-Capability eingeschränkt ist und Sie ein Kontingent für ein übergeordnetes Verzeichnis konfigurieren, auf das der Client nicht zugreifen kann (<filename>/home</filename>), kann der Client das Kontingent nicht erzwingen. Wenn Sie pfadbasierte Zugriffsbeschränkungen verwenden, stellen Sie sicher, dass Sie die Kontingente für das Verzeichnis konfigurieren, auf das der Client zugreifen kann (z. B. <filename>/home/user</filename> oder <filename>/home/user/quota_dir</filename>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Konfigurieren von CephFS-Kontingenten</title>
   <para>
    CephFS-Kontingente werden mit virtuellen erweiterten Attributen konfiguriert:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Konfiguriert einen Grenzwert für die Anzahl der <emphasis>Dateien</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Konfiguriert einen Grenzwert für die Anzahl der <emphasis>Bytes</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Wenn Attribute für einen Verzeichnis-Inode angezeigt werden, ist dort ein Kontingent konfiguriert. Fehlen die Attribute, wurde kein Kontingent für das Verzeichnis festgelegt (eventuell jedoch für ein übergeordnetes Verzeichnis).
   </para>
   <para>
    Mit folgendem Kommando legen Sie ein Kontingent von 100 MB fest:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Mit folgendem Kommando legen Sie ein Kontingent von 10.000 Dateien fest:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Mit folgendem Kommando rufen Sie die Kontingenteinstellung ab:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>Kein Kontingent festgelegt</title>
    <para>
     Wenn das erweiterte Attribut den Wert „0“ aufweist, ist kein Kontingent festgelegt.
    </para>
   </note>
   <para>
    Mit folgendem Kommando entfernen Sie ein Kontingent:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Verwalten von CephFS-Snapshots</title>

  <para>
   Ein CephFS-Snapshot ist eine schreibgeschützte Ansicht des Dateisystems zu dem Zeitpunkt, zu dem der Snapshot erstellt wurde. Sie können Snapshots in beliebigen Verzeichnissen erstellen. Der Snapshot deckt alle Daten im Dateisystem unter dem angegebenen Verzeichnis ab. Nach dem Erstellen des Snapshots werden die zwischengespeicherten Daten asynchron von verschiedenen Clients verschoben. Die Snapshot-Erstellung ist daher sehr schnell.
  </para>

  <important>
   <title>Mehrere Dateisysteme</title>
   <para>
    Greifen mehrere CephFS-Dateisysteme auf einen einzelnen Pool zu (über Namespaces), prallen ihre Snapshots aufeinander. Wenn Sie dann einen einzelnen Snapshot löschen, fehlen die entsprechenden Daten in anderen Snapshots, die denselben Pool nutzen.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Erstellen von Snapshots</title>
   <para>
    Die CephFS-Snapshot-Funktion ist auf neuen Dateisystemen standardmäßig aktiviert. Mit folgendem Kommando aktivieren Sie die Funktion auf vorhandenen Dateisystemen:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Sobald Sie die Snapshots aktiviert haben, enthalten alle Verzeichnisse im CephFS das besondere Unterverzeichnis <filename>.snap</filename>.
   </para>
   <note>
    <para>
     Dies ist ein <emphasis>virtuelles</emphasis> Unterverzeichnis. Es erscheint nicht in der Verzeichnisliste des übergeordneten Verzeichnisses, aber der Name <filename>.snap</filename> kann nicht als Datei- oder Verzeichnisname verwendet werden. Der Zugriff auf das Verzeichnis <filename>.snap</filename> muss explizit erfolgen. Beispiel:
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>Einschränkung von Kernel-Clients</title>
    <para>
     Für CephFS-Kernel-Clients gilt eine Einschränkung: Sie können maximal 400 Snapshots in einem Dateisystem verarbeiten. Halten Sie die Anzahl der Snapshots stets unter diesem Grenzwert, unabhängig vom tatsächlichen Client. Wenn Sie mit älteren CephFS-Clients arbeiten (z. B. mit SLE12-SP3), bedenken Sie, dass eine Anzahl von mehr als 400 Snapshots zum Absturz des Clients führt und damit die Abläufe erheblich beeinträchtigt.
    </para>
   </important>
   <tip>
    <title>Benutzerdefinierter Name für das Snapshot-Unterverzeichnis</title>
    <para>
     Mit der Einstellung <option>client snapdir</option> können Sie einen anderen Namen für das Snapshot-Unterverzeichnis konfigurieren.
    </para>
   </tip>
   <para>
    Zum Erstellen eines Snapshots legen Sie ein Unterverzeichnis mit einem benutzerdefinierten Namen unter dem Verzeichnis <filename>.snap</filename> an. Mit folgendem Kommando erstellen Sie beispielsweise einen Snapshot des Verzeichnisses <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Löschen von Snapshots</title>
   <para>
    Soll ein Snapshot gelöscht werden, entfernen Sie das zugehörige Unterverzeichnis aus dem Verzeichnis <filename>.snap</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
