<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>Verwalten von Speicher-Pools</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph speichert Daten in Pools. Pools sind logische Gruppen für Speicherobjekte. Wenn Sie zunächst einen Cluster bereitstellen, ohne einen Pool zu erstellen, verwendet Ceph die Standard-Pools zum Speichern von Daten. Für Ceph-Pools gelten die folgenden wichtigen Punkte:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Stabilität</emphasis>: Mit Ceph-Pools wird Stabilität erreicht, da die darin enthaltenen Daten reproduziert oder verschlüsselt werden. Jeder Pool kann auf <literal>reproduziert</literal> oder <literal>mit Löschcodierung</literal> festgelegt werden. Bei reproduzierten Pools legen Sie außerdem die Anzahl der Reproduktionen oder Kopien für jedes Datenobjekt innerhalb des Pools fest. Die Anzahl der Kopien (OSDs, CRUSH-Buckets/Blätter), die verloren gehen können, ist eins weniger als die Anzahl der Reproduktionen. Bei der Löschcodierung legen Sie die Werte von <option>k</option> und <option>m</option> fest, wobei <option>k</option> die Anzahl der Datenblöcke und <option>m</option> die Anzahl der Datenblöcke für die Codierung darstellt. Bei Pools mit Löschcodierung bestimmt die Anzahl der Datenblöcke für die Codierung, wie viele OSDs (CRUSH-Buckets/Blätter) ohne Datenverlust verloren gehen können.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Platzierungsgruppen</emphasis>: Sie können die Anzahl der Platzierungsgruppen für den Pool festlegen. Bei einer normalen Konfiguration sind etwa 100 Placement Groups pro OSD vorhanden. Dadurch wird ein optimaler Ausgleich ermöglicht, ohne zu viel Rechnerressourcen zu verbrauchen. Achten Sie bei der Einrichtung von mehreren Pools sorgfältig darauf, dass Sie eine vernünftige Anzahl von Placement Groups sowohl für den Pool als auch den Cluster insgesamt festlegen.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>CRUSH-Regeln</emphasis>: Wenn Sie Daten in einem Pool speichern, werden die Objekte und deren Reproduktionen (bzw. Blöcke bei Pools mit Löschcodierung) gemäß dem CRUSH-Regelsatz platziert, der dem Pool zugeordnet ist. Es ist auch möglich, eine benutzerdefinierte CRUSH-Regel für Ihren Pool zu erstellen.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshots</emphasis>: Wenn Sie einen Snapshot mit <command>ceph osd pool mksnap</command> erstellen, machen Sie im Grunde einen Snapshot von einem bestimmten Pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Zum Strukturieren der Daten in Pools können Sie Pools auflisten, erstellen und entfernen. Es ist auch möglich, die Auslastungsstatistik für jeden Pool anzuzeigen.
 </para>
 <sect1 xml:id="ceph-pools-operate-add-pool">
  <title>Erstellen eines Pools</title>

  <para>
   Ein Pool kann entweder als <literal>reproduziert</literal> erstellt werden, um eine Wiederherstellung verlorener OSDs durch Beibehaltung mehrerer Kopien von Objekten zu ermöglichen, oder als <literal>Löschcodierung</literal>, um eine Art von genereller RAID5/6-Funktion zu erhalten. Reproduzierte Pools belegen mehr Rohspeicher, Pools mit Löschcodierung dagegen weniger Rohspeicher. Die Standardeinstellung lautet <literal>reproduziert</literal>. Weitere Informationen zu Pools mit Löschcodierung finden Sie in <xref linkend="cha-ceph-erasure"/>.
  </para>

  <para>
   Führen Sie zum Erstellen eines reproduzierten Pools Folgendes aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable></screen>

  <note>
   <para>
    Die Autoskalierung verarbeitet die restlichen optionalen Argumente. Weitere Informationen finden Sie <xref linkend="op-pgs-autoscaler"/>.
   </para>
  </note>

  <para>
   Führen Sie zum Erstellen eines Erasure Coded Pools Folgendes aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>POOL_NAME</replaceable> erasure <replaceable>CRUSH_RULESET_NAME</replaceable> \
<replaceable>EXPECTED_NUM_OBJECTS</replaceable></screen>

  <para>
   Das Kommando <command>ceph osd pool create</command> wird möglicherweise nicht ausgeführt, wenn Sie die zulässige Anzahl der Platzierungsgruppen pro OSD überschreiten. Der Grenzwert wird mit der Option <option>mon_max_pg_per_osd</option> festgelegt.
  </para>

  <variablelist>
   <varlistentry>
    <term>POOL_NAME</term>
    <listitem>
     <para>
      Der Name des Pools. Er muss eindeutig sein. Diese Option muss aktiviert sein.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_TYPE</term>
    <listitem>
     <para>
      Der Pool-Typ, entweder <literal>replicated</literal> (reproduziert), um eine Wiederherstellung verlorener OSDs durch Beibehaltung mehrerer Kopien von Objekten zu ermöglich, oder <literal>erasure</literal>, um eine Art von genereller RAID5-Funktion zu erhalten. Reproduzierte Pools benötigen zwar mehr Basisspeicher, implementieren jedoch alle Ceph-Operationen. Erasure Pools benötigen weniger Basisspeicher, implementieren jedoch nur eine Teilmenge der verfügbaren Operationen. Die Standardeinstellung für <literal>POOL_TYPE</literal> lautet <literal>reproduziert</literal>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CRUSH_RULESET_NAME</term>
    <listitem>
     <para>
      Der Name des CRUSH-Regelsatzes für diesen Pool. Wenn der angegebene Regelsatz nicht vorhanden ist, werden die reproduzierten Pools nicht erstellt und -ENOENT wird ausgegeben. Bei reproduzierten Pools ist es der Regelsatz, der in der Konfigurationsvariablen <varname>osd pool default CRUSH replicated ruleset</varname> angegeben ist. Dieser Regelsatz muss vorhanden sein. Bei Erasure Pools ist dies „erasure-code“, wenn das standardmäßige Löschcode-Profil verwendet, wird, ansonsten <replaceable>POOL_NAME</replaceable>. Falls dieser Regelsatz nicht bereits vorhanden ist, wird er implizit erstellt.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>erasure_code_profile=profile</term>
    <listitem>
     <para>
      Nur für Erasure Coded Pools. Verwenden Sie das Erasure Code Profil. Es muss ein vorhandenes Profil sein, das durch <command>osd erasure-code-profile set</command> definiert wurde.
     </para>
     <note>
      <para>
       Wenn aus irgendeinem Grund die Autoskalierung in einem Pool deaktiviert wurde (<literal>pg_autoscale_mode</literal> auf „off“ festgelegt), können Sie die PG-Nummern manuell berechnen und festlegen. Unter <xref linkend="op-pgs"/> finden Sie detaillierte Informationen zur Berechnung einer angemessenen Anzahl von Placement Groups für Ihren Pool.
      </para>
     </note>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>EXPECTED_NUM_OBJECTS</term>
    <listitem>
     <para>
      Die erwartete Anzahl von Objekten für diesen Pool. Wenn Sie diesen Wert festlegen (zusammen mit einem negativen Wert für <option>filestore merge threshold</option>), wird der PG-Ordner zum Zeitpunkt der Pool-Erstellung aufgeteilt. Dadurch werden die negativen Auswirkungen der Latenz vermieden, die bei einer Aufteilung bei Laufzeit auftritt.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-listing-pools">
  <title>Auflisten der Pools</title>

  <para>
   Führen Sie zum Auflisten der Pools Ihres Clusters Folgendes aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool ls</screen>
 </sect1>
 <sect1 xml:id="ceph-renaming-pool">
  <title>Umbenennen eines Pools</title>

  <para>
   Führen Sie zum Umbenennen eines Pools folgendes Kommando aus:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rename <replaceable>CURRENT_POOL_NAME</replaceable> <replaceable>NEW_POOL_NAME</replaceable></screen>

  <para>
   Wenn Sie einen Pool umbenennen und Capabilities pro Pool für einen authentifizierten Benutzer festgelegt haben, müssen Sie die Capabilities des Benutzer mit dem neuen Pool-Namen aktualisieren.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pools-operate-del-pool">
  <title>Löschen eines Pools</title>

  <warning>
   <title>Die Löschung eines Pools lässt sich nicht rückgängig machen</title>
   <para>
    Pools enthalten möglicherweise wichtige Daten. Durch Löschen eines Pools gehen alle Daten im Pool verloren und es gibt keine Möglichkeit, sie wiederherzustellen.
   </para>
  </warning>

  <para>
   Da die unbeabsichtigte Löschung von Pools eine echte Gefahr darstellt, implementiert Ceph zwei Mechanismen, durch die eine Löschung verhindert wird. Beide Mechanismen müssen deaktiviert werden, bevor ein Pool gelöscht werden kann.
  </para>

  <para>
   Der erste Mechanismus ist das Flag <literal>NODELETE</literal>. Diese Flagge ist bei jedem Pool gesetzt und der Standardwert ist „false“. Führen Sie folgendes Kommando aus, um den Wert dieser Flagge an einem Pool festzustellen:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>

  <para>
   Wenn das Kommando <literal>nodelete: true</literal> ausgibt, ist es erst möglich, den Pool zu löschen, wenn Sie das Flag mit folgendem Kommando ändern:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>

  <para>
   Der zweite Mechanismus ist der Cluster-weite Konfigurationsparameter <option>mon allow pool delete</option>, der standardmäßig auf „false“ festgelegt ist. Dies bedeutet, dass es standardmäßig nicht möglich ist, einen Pool zu löschen. Die angezeigte Fehlermeldung sieht folgendermaßen aus:
  </para>

<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>

  <para>
   Zum Löschen des Pools trotz der Sicherheitseinstellung legen Sie vorübergehend <option>mon allow pool delete</option> auf „true“ fest, löschen den Pool und legen den Parameter dann wieder auf „false“ fest:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>cephuser@adm &gt; </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>

  <para>
   Das Kommando <command>injectargs</command> zeigt die folgende Meldung an:
  </para>

<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>

  <para>
   Dadurch wird einfach nur bestätigt, dass das Kommando erfolgreich ausgeführt wurde. Dies ist kein Fehler.
  </para>

  <para>
   Wenn Sie für einen erstellten Pool eigene Regelsätze und Regeln festgelegt haben, sollten Sie diese entfernen, wenn Sie den Pool nicht länger benötigen.
  </para>
 </sect1>
 <sect1 xml:id="ceph-pool-other-operations">
  <title>Sonstige Vorgänge</title>

  <sect2 xml:id="ceph-pools-associate">
   <title>Verknüpfen von Pools mit einer Anwendung</title>
   <para>
    Bevor Sie Pools verwenden, müssen Sie sie mit einer Anwendung verknüpfen. Mit dem CephFS verwendete Pools oder automatisch von Object Gateway erstellte Pools werden automatisch verknüpft.
   </para>
   <para>
    In anderen Fällen können Sie manuell einen frei formulierten Anwendungsnamen mit einem Pool verknüpfen:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application enable <replaceable>POOL_NAME</replaceable> <replaceable>APPLICATION_NAME</replaceable></screen>
   <tip>
    <title>Standardanwendungsnamen</title>
    <para>
     CephFS verwendet den Anwendungsnamen <literal>cephfs</literal>, RADOS Block Device verwendet <literal>rbd</literal> und Object Gateway verwendet <literal>rgw</literal>.
    </para>
   </tip>
   <para>
    Ein Pool kann mit mehreren Anwendungen verknüpft werden und jede Anwendung kann eigene Metadaten enthalten. Listen Sie die mit einem Pool verknüpften Anwendungen mit folgendem Kommando auf:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
  </sect2>

  <sect2 xml:id="ceph-set-pool-quotas">
   <title>Festlegen von Pool-Kontingenten</title>
   <para>
    Pool-Kontingente können für die maximale Anzahl von Byte und/oder die maximale Anzahl von Objekten pro Pool festgelegt werden.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota <replaceable>POOL_NAME</replaceable> <replaceable>MAX_OBJECTS</replaceable> <replaceable>OBJ_COUNT</replaceable> <replaceable>MAX_BYTES</replaceable> <replaceable>BYTES</replaceable></screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Zum Entfernen eines Kontingents legen Sie den entsprechenden Wert auf 0 fest.
   </para>
  </sect2>

  <sect2 xml:id="ceph-showing-pool-statistics">
   <title>Anzeigen von Pool-Statistiken</title>
   <para>
    Führen Sie zum Anzeigen der Auslastungsstatistik eines Pools folgendes Kommando aus:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados df
 POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
 .rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
 cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
 cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
 default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
 default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
 default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
 default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
 example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
 iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
 mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
 pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
 pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
 pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
 </screen>
   <para>
    Beschreibung der einzelnen Spalten:
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       Speicherplatz (in Byte), der durch den Pool belegt wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       Anzahl der im Pool gespeicherten Objekte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       Anzahl der im Pool gespeicherten Klone. Wenn Sie einen Snapshot erstellen und in ein Objekt schreiben, wird nicht das ursprüngliche Objekt geändert, sondern es wird ein Klon erstellt, sodass der ursprünglich im Snapshot festgehaltene Objektinhalt nicht geändert wird. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>KOPIEN</term>
     <listitem>
      <para>
       Anzahl der Objektreproduktionen. Wenn beispielsweise ein reproduzierter Pool mit dem Reproduktionsfaktor 3 „x“ Objekte umfasst, enthält er in der Regel 3 * x Exemplare.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       Anzahl von Objekten im eingeschränkt leistungsfähigen Status (nicht alle Exemplare vorhanden), wobei das Exemplar auf dem primären OSD fehlt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       Anzahl der nicht gefundenen Objekte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Anzahl der eingeschränkt leistungsfähigen Objekte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       Gesamtanzahl der für diesen Pool angeforderten Leseoperationen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       Datenmenge (in Byte), die aus diesem Pool ausgelesen wurde.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       Gesamtanzahl der für diesen Pool angeforderten Schreiboperationen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       Datenmenge (in Byte), die in diesen Pool geschrieben wurde. Dieser Wert ist nicht mit der Auslastung des Pools identisch, da Sie mehrfach in ein einzelnes Objekt schreiben können. Die Pool-Auslastung bleibt dabei unverändert, die in den Pool geschriebene Datenmenge (in Byte) steigt jedoch an.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       Speicherplatz (in Byte), der für komprimierte Daten zugewiesen ist.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       Speicherplatz (in Byte), den die komprimierten Daten belegen, wenn sie nicht komprimiert sind.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-getting-pool-values">
   <title>Abrufen von Pool-Werten</title>
   <para>
    Rufen Sie einen Wert von einem Pool mit folgendem <command>get</command>-Kommando ab:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable></screen>
   <para>
    Sie können die Werte für Schlüssel abrufen, die in <xref linkend="ceph-pools-values"/> aufgelistet sind, und zudem folgende Schlüssel:
   </para>
   <variablelist>
    <varlistentry>
     <term>PG_NUM</term>
     <listitem>
      <para>
       Die Anzahl der Placement Groups für den Pool. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PGP_NUM</term>
     <listitem>
      <para>
       Die effektive Anzahl von Placement Groups zum Berechnen der Datenplatzierung. Der gültige Bereich ist gleich oder kleiner <option>PG_NUM</option>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>Alle Werte eines Pools</title>
    <para>
     Mit folgendem Kommando rufen Sie alle Werte für einen bestimmten Pool ab:
    </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> all
 </screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>Festlegen von Pool-Werten</title>
   <para>
    Führen Sie zum Festlegen des Werts für einen Pool folgendes Kommando aus:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> <replaceable>KEY</replaceable> <replaceable>VALUE</replaceable></screen>
   <para>
    Die folgende Liste zeigt die Pool-Werte sortiert nach Pool-Typ:
   </para>
   <variablelist>
    <title>Allgemeine Pool-Werte</title>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       Der Zeitraum in Sekunden, für den Clients bestätigte, aber noch nicht zugewiesene Anforderungen erneut wiedergeben können.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Die Anzahl der Placement Groups für den Pool. Wenn Sie neue OSDs in den Cluster aufnehmen, prüfen Sie den Wert für die Platzierungsgruppen in allen Pools, die für die neuen OSDs vorgesehen sind.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Die effektive Anzahl von Placement Groups zum Berechnen der Datenplatzierung.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       Der zu verwendende Regelsatz für die Zuordnung der Objektplatzierung im Cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Setzen Sie das Flag HASHPSPOOL an einem angegebenen Pool (1) oder entfernen Sie sie (0). Durch Aktivieren dieses Flags wird der Algorithmus geändert, um PGs besser auf OSDs zu verteilen. Nach Aktivierung dieses Flags für einen Pool, dessen HASHPSPOOL-Flag auf den Standardwert 0 festgelegt wurde, beginnt der Cluster einen Abgleich, um erneut eine korrekte Platzierung aller PGs zu erreichen. Dies kann zu einer erheblichen E/A-Belastung auf einem Cluster führen. Aktivieren Sie diese Flagge daher nicht in stark ausgelasteten Produktions-Clustern (Umstellung von 0 auf 1).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Verhindert, dass der Pool entfernt wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Verhindert, dass die Werte <option>pg_num</option> und <option>pgp_num</option> des Pools geändert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Deaktiviert (Deep-)Scrubbing der Daten für den entsprechenden Pool, um eine vorübergehend hohe E/A-Last zu vermeiden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Legt das Flag <literal>WRITE_FADVISE_DONTNEED</literal> bei Lese-/Schreibanforderungen eines bestimmten Pools fest oder entfernt sie, um das Ablegen von Daten in den Cache zu umgehen. Die Standardeinstellung ist <literal>false</literal>. Gilt sowohl für reproduzierte als auch für EC-Pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       Das Mindestintervall in Sekunden für ein Pool Scrubbing, wenn die Cluster-Last gering ist. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_scrub_min_interval</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       Das maximale Intervall in Sekunden für ein Pool Scrubbing, unabhängig von der Cluster-Last. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_scrub_max_interval</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       Das Intervall in Sekunden für ein <emphasis>deep</emphasis> Scrubbing des Pools. Der Standardwert <literal>0</literal> bedeutet, dass der Wert <option>osd_deep_scrub</option> aus der Ceph-Konfigurationsdatei verwendet wird.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>Reproduzierte Pool-Werte</title>
    <varlistentry>
     <term>Größe</term>
     <listitem>
      <para>
       Legt die Anzahl der Reproduktionen für Objekte im Pool fest. Weitere Informationen finden Sie in <xref linkend="ceph-pools-options-num-of-replicas"/>. Nur reproduzierte Pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Legt die Mindestanzahl von Reproduktionen fest, die für E/A benötigt werden. Weitere Informationen finden Sie in <xref linkend="ceph-pools-options-num-of-replicas"/>. Nur reproduzierte Pools.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Verhindert, dass die Größe des Pools geändert wird. Beim Erstellen eines Pools wird der Standardwert aus dem Wert des Parameters <option>osd_pool_default_flag_nosizechange</option> übernommen, der standardmäßig <literal>false</literal> lautet. Gilt nur für reproduzierte Pools, da Sie die Größe für EC-Pools nicht ändern können.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Aktiviert die Treffersatz-Verfolgung für Cache Pools. Weitere Informationen finden Sie unter <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloomfilter</link>. Für diese Option sind die folgenden Werte möglich: <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. Der Standardwert ist <literal>bloom</literal>, die anderen Werte sind nur für Testzwecke vorgesehen.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       Die Anzahl der Treffersätze, die für Cache Pools gespeichert werden sollen. Je höher die Anzahl, desto mehr RAM wird vom <systemitem>ceph-osd</systemitem>-Daemon belegt. Der Standardwert ist <literal>0</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       Die Dauer einer Treffersatz-Periode in Sekunden für Cache Pools. Je höher die Anzahl, desto mehr RAM wird vom <systemitem>ceph-osd</systemitem>-Daemon belegt. Beim Erstellen eines Pools wird der Standardwert aus dem Wert des Parameters <option>osd_tier_default_cache_hit_set_period</option> übernommen, der standardmäßig <literal>1200</literal> lautet. Gilt nur für reproduzierte Pools, da EC-Pools nicht als Cache-Ebene verwendet werden können.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       Die falsch positive Wahrscheinlichkeit für den Bloom-Treffersatz-Typ. Weitere Informationen finden Sie unter <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloomfilter</link>. Der gültige Bereich ist 0,0 bis 1,0. Der Standardwert ist <literal>0,05</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Erzwingen Sie bei Erstellung eines Treffersatzes für das Cache Tiering, dass OSDs die MGZ-Zeitstempel (mittlere Greenwich-Zeit) verwenden. Dadurch wird sichergestellt, dass Knoten in verschiedenen Zeitzonen dasselbe Ergebnis zurückgeben. Der Standardwert ist <literal>1</literal>. Dieser Wert darf nicht geändert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der bearbeiteten (fehlerhaften) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte in den Hintergrundspeicher-Pool verschiebt. Der Standardwert ist <literal>0.4</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der bearbeiteten (fehlerhaften) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte mit höherer Geschwindigkeit in den Hintergrundspeicher-Pool verschiebt. Der Standardwert ist <literal>0.6</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       Der Prozentsatz der unbearbeiteten (intakten) Objekte im Cache Pool, der erreicht sein muss, bevor der Cache Tiering Agent diese Objekte aus dem Cache Pool entfernt. Der Standardwert ist <literal>0.8</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph beginnt mit dem Verschieben oder Entfernen von Objekten, wenn der Grenzwert <option>max_bytes</option> ausgelöst wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph beginnt mit dem Verschieben oder Entfernen von Objekten, wenn der Grenzwert <option>max_objects</option> ausgelöst wird.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Temperaturzerfallsrate zwischen zwei aufeinanderfolgenden <literal>hit_set</literal>. Der Standardwert ist <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Anzahl der meisten <literal>N</literal>-Vorkommen in <literal>hit_set</literal>s für die Temperaturberechnung. Der Standardwert ist <literal>1</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       Die Zeit (in Sekunden), bevor der Cache Tiering Agent ein Objekt vom Cache Pool in den Speicher-Pool verschiebt.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       Die Zeit (in Sekunden), bevor der Cache Tiering Agent ein Objekt aus dem Cache Pool entfernt.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist xml:id="pool-values-ec">
    <title>Werte der Pools mit Löschcodierung</title>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Wenn dieses Flag bei Erasure Coding Pools aktiviert wird, stellt die Leseanforderung Teil-Lesevorgänge an alle Shards aus und wartet, bis sie genügend Shards zum Decodieren erhält, die für den Client verarbeitet werden. Wenn im Fall von <emphasis>jerasure</emphasis>- und <emphasis>isa</emphasis>-Erasure-Plugins die ersten <literal>K</literal>-Antworten zurückgegeben werden, dann wird die Anforderung des Clients sofort anhand der von diesen Antworten decodierten Daten verarbeitet. Diese Vorgehensweise erhöht die CPU-Auslastung und senkt die Datenträger-/Netzwerkauslastung. Diese Flagge wird aktuell nur für Erasure Coded Pools unterstützt. Der Standardwert ist <literal>0</literal>. 
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>Festlegen der Anzahl der Objektreproduktionen</title>
   <para>
    Führen Sie folgendes Kommando aus, um die Anzahl der Objektreproduktionen in einem reproduzierten Pool festzulegen:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    In <replaceable>num-replicas</replaceable> ist das Objekt selbst enthalten. Geben Sie 3 an, wenn Sie beispielsweise das Objekt und zwei Kopien des Objekts für insgesamt drei Instanzen des Objekts wünschen.
   </para>
   <warning>
    <title>Legen Sie nicht weniger als 3 Reproduktionen fest</title>
    <para>
     Wenn Sie <replaceable>num-replicas</replaceable> auf 2 festlegen, wird nur <emphasis>eine</emphasis> Kopie Ihrer Daten erstellt. Wenn Sie eine Objektinstanz verlieren, müssen Sie sich darauf verlassen, dass die andere Kopie seit dem letzten Scrubbing bei der Wiederherstellung nicht beschädigt wurde (ausführliche Informationen siehe <xref linkend="scrubbing-pgs"/>).
    </para>
    <para>
     Wenn der Pool auf eine Reproduktion festgelegt wird, bedeutet dies, dass genau <emphasis>eine</emphasis> Instanz des Datenobjekts im Pool vorhanden ist. Wenn der OSD ausfällt, verlieren Sie die Daten. Ein Pool mit einer Reproduktion wird normalerweise verwendet, wenn temporäre Daten für kurze Zeit gespeichert werden.
    </para>
   </warning>
   <tip>
    <title>Festlegen von mehr als 3 Reproduktionen</title>
    <para>
     Wenn Sie 4 Reproduktionen für einen Pool festlegen, erhöht dies die Zuverlässigkeit um 25 %.
    </para>
    <para>
     Bei zwei Rechenzentren müssen Sie mindestens 4 Reproduktionen für einen Pool festlegen, damit sich je zwei Exemplare in jedem Rechenzentrum befinden. Sollte dann ein Rechenzentrum ausfallen, sind weiterhin zwei Exemplare vorhanden, sodass noch ein weiterer Datenträger verloren gehen kann, ohne dass Datenverlust eintritt.
    </para>
   </tip>
   <note>
    <para>
     Ein Objekt akzeptiert möglicherweise E/As im eingeschränkt leistungsfähigen Modus mit weniger als <literal>pool size</literal> Reproduktionen. Sie sollten die Einstellung <literal>min_size</literal> verwenden, um eine Mindestanzahl erforderlicher Reproduktionen für E/A festzulegen. Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Dadurch wird sichergestellt, dass kein Objekt im Daten-Pool E/A mit weniger Reproduktionen als <literal>min_size</literal> erhält.
    </para>
   </note>
   <tip>
    <title>Rufen Sie die Anzahl der Objektreproduktionen ab</title>
    <para>
     Führen Sie folgendes Kommando aus, um die Anzahl der Objektreproduktionen abzurufen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd dump | grep 'replicated size'</screen>
    <para>
     Ceph listet die Pools mit hervorgehobenem Attribut <literal>replicated size</literal> auf. Standardmäßig erstellt Ceph zwei Reproduktionen eines Objekts (insgesamt drei Kopien oder eine Größe von 3).
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>Pool-Migration</title>

  <para>
   Beim Erstellen eines Pools (Informationen hierzu finden Sie in <xref linkend="ceph-pools-operate-add-pool"/>) müssen Sie die ersten Parameter wie den Pool-Typ oder die Anzahl der Placement Groups angeben. Wenn Sie sich später entscheiden, diese Parameter zu ändern (z. B. einen reproduzierten Pool in einen Pool mit Löschcodierung umwandeln oder die Anzahl der Platzierungsgruppen verringern), müssen Sie die Pool-Daten zu einem anderen Pool migrieren, dessen Parameter zur gewünschten Implementierung passen.
  </para>

  <para>
   In diesem Abschnitt werden zwei Migrationsmethoden beschrieben: eine <emphasis>Cache-Ebene</emphasis>-Methode für die allgemeine Pool-Datenmigration und eine Methode, die <command>rbd migrate</command>-Unterkommandos verwendet, um RBD-Images in einen neuen Pool zu migrieren. Jede Methode hat ihre Eigenheiten und Grenzen.
  </para>

  <sect2 xml:id="pool-migrate-limits">
   <title>Nutzungsbeschränkungen</title>
   <itemizedlist>
    <listitem>
     <para>
      Mit der <emphasis>Cache-Ebene</emphasis>-Methode können Sie einen reproduzierten Pool entweder zu einem Pool mit Löschcodierung oder zu einem anderen reproduzierten Pool migrieren. Die Migration von einem Pool mit Löschcodierung wird nicht unterstützt.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie können keine RBD-Images und CephFS-Exporte aus einem reproduzierten Pool zu einem EC-Pool migrieren. Der Grund ist, dass EC-Pools <literal>omap</literal> nicht unterstützen, während RBD und CephFS <literal>omap</literal> zum Speichern ihrer Metadaten verwenden. Beispielsweise kann das Header-Objekt des RBD nicht geleert werden. Sie können jedoch Daten in den EC-Pool migrieren und die Metadaten im reproduzierten Pool belassen.
     </para>
    </listitem>
    <listitem>
     <para>
      Mit der <command>rbd migration</command>-Methode können Images mit minimaler Client-Ausfallzeit migriert werden. Sie müssen den Client lediglich vor dem <option>prepare</option>-Schritt anhalten und danach wieder starten. Hierbei ist zu beachten, dass nur ein <systemitem>librbd</systemitem>-Client, der diese Funktion unterstützt (Ceph Nautilus oder höher), das Image direkt nach dem <option>prepare</option>-Schritt öffnen kann, ältere <systemitem>librbd</systemitem>-Clients oder die <systemitem>krbd</systemitem>-Clients dagegen erst nach dem <option>commit</option>-Schritt.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>Migrieren mit der Cache-Ebene</title>
   <para>
    Das Prinzip ist einfach: Stellen Sie den Pool, der migriert werden soll, in umgekehrter Reihenfolge in eine Cache-Schicht. Das folgende Beispiel zeigt die Migration des reproduzierten Pools „testpool“ zu einem Pool mit Löschcodierung:
   </para>
   <procedure>
    <title>Migrieren eines reproduzierten Pools zu einem Pool mit Löschcodierung</title>
    <step>
     <para>
      Erstellen Sie einen neuen Pool mit Löschcodierung mit der Bezeichnung „newpool“. Eine ausführliche Erläuterung der Parameter für die Pool-Erstellung finden Sie in <xref linkend="ceph-pools-operate-add-pool"/>.
     </para>
<screen>
 <prompt>cephuser@adm &gt; </prompt>ceph osd pool create newpool erasure default
</screen>
     <para>
      Prüfen Sie, ob der verwendete Client-Schlüsselbund für „newpool“ mindestens dieselben Funktionen bietet wie für „testpool“.
     </para>
     <para>
      Nun verfügen Sie über zwei Pools: den ursprünglichen reproduzierten „testpool“ mit Daten und den neuen leeren Erasure Coded „newpool“:
     </para>
     <figure>
      <title>Pools vor der Migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Richten Sie die Cache-Schicht ein und konfigurieren Sie den reproduzierten „testpool“ als Cache Pool. Mit der Option <option>-force-nonempty</option> können Sie selbst dann eine Cache-Schicht hinzufügen, wenn der Pool bereits Daten enthält:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<prompt>cephuser@adm &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>cephuser@adm &gt; </prompt>ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>Einrichtung der Cache-Ebene</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Erzwingen Sie, dass der Cache Pool alle Objekte in den neuen Pool verschiebt:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Leeren von Daten</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Solange nicht alle Daten geleert und zum neuen Erasure Coded Pool verschoben sind, müssen Sie eine Überlagerung angeben, sodass Objekte noch im alten Pool gesucht werden:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Bei einer Überlagerung werden alle Operationen an den alten reproduzierten „testpool“ weitergeleitet:
     </para>
     <figure>
      <title>Festlegen der Überlagerung</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Nun können Sie alle Clients auf den Zugriff von Objekten im neuen Pool umstellen.
     </para>
    </step>
    <step>
     <para>
      Wenn alle Daten zum Erasure Coded „newpool“ migriert sind, entfernen Sie die Überlagerung und den alten Cache Pool „testpool“:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>cephuser@adm &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration abgeschlossen</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Führen Sie folgendes Kommando aus:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>Migrieren von RBD-Images</title>
   <para>
    Für die Migration von RBD-Images aus einem reproduzierten Pool zu einem anderen reproduzierten Pool wird folgende Vorgehensweise empfohlen:
   </para>
   <procedure>
    <step>
     <para>
      Beenden Sie den Zugriff von Clients (z. B. von einer virtuellen Maschine) auf das RBD-Image.
     </para>
    </step>
    <step>
     <para>
      Erstellen Sie ein neues Image im Ziel-Pool und legen Sie das Quell-Image als übergeordnetes Image fest:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>Migrieren Sie nur Daten zu einem Pool mit Löschcodierung</title>
      <para>
       Sollen lediglich die Image-Daten zu einem neuen EC-Pool migriert werden und die Metadaten im ursprünglichen reproduzierten Pool verbleiben, führen Sie stattdessen folgendes Kommando aus:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
    </step>
    <step>
     <para>
      Geben Sie den Clients den Zugriff auf das Image im Ziel-Pool.
     </para>
    </step>
    <step>
     <para>
      Migrieren Sie die Daten zum Ziel-Pool:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      Entfernen Sie das bisherige Image:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>Pool Snapshots</title>

  <para>
   Pool Snapshots sind Snapshots vom Zustand des gesamten Ceph Pools. Mit Pool Snapshots behalten Sie den Verlauf des Pool-Zustands bei. Die Erstellung von Pool Snapshots belegt Speicherplatz proportional zur Pool-Größe. Prüfen Sie immer zunächst, ob im betreffenden Speicher genügend Festplattenspeicherplatz vorhanden ist, bevor Sie einen Snapshot eines Pools erstellen.
  </para>

  <sect2 xml:id="ceph-make-snapshot-pool">
   <title>Erstellen eines Pool Snapshots</title>
   <para>
    Mit folgendem Kommando erstellen Sie einen Snapshot eines Pools:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    Beispiel:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2 xml:id="ceph-listing-snapshots-pool">
   <title>Auflisten der Snapshots eines Pools</title>
   <para>
    Mit folgendem Kommando rufen Sie die vorhandenen Snapshots eines Pools ab:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    Beispiel:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2 xml:id="ceph-removing-snapshot-pool">
   <title>Entfernen eines Snapshots eines Pools</title>
   <para>
    Mit folgendem Kommando entfernen Sie einen Snapshot eines Pools:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>Datenkomprimierung</title>

  <para>
   BlueStore (siehe <xref linkend="about-bluestore"/>) bietet eine direkte Datenkomprimierung, mit der Sie Speicherplatz sparen. Das Komprimierungsverhältnis ist abhängig von den im System gespeicherten Daten. Beachten Sie, dass für die Komprimierung/die Dekomprimierung zusätzliche CPU-Leistung erforderlich ist.
  </para>

  <para>
   Sie können die Datenkomprimierung global konfigurieren (siehe <xref linkend="sec-ceph-pool-bluestore-compression-options"/>) und dann bestimmte Komprimierungseinstellungen für die einzelnen Pools überschreiben.
  </para>

  <para>
   Sie können die Komprimierung der Pool-Daten aktivieren oder deaktivieren und auch jederzeit den Komprimierungsalgorithmus und -modus ändern, unabhängig davon, ob der Pool Daten enthält oder nicht.
  </para>

  <para>
   Vorhandene Daten werden nach Aktivierung der Pool-Komprimierung nicht komprimiert.
  </para>

  <para>
   Wenn Sie die Komprimierung für einen Pool deaktivieren, werden dessen Daten alle dekomprimiert.
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>Aktivieren der Komprimierung</title>
   <para>
    Mit folgendem Kommando aktivieren Sie die Datenkomprimierung für den Pool <replaceable>POOL_NAME</replaceable>:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>Deaktivieren der Pool-Komprimierung</title>
    <para>
     Zum Deaktivieren der Datenkomprimierung für einen Pool geben Sie „none“ als Komprimierungsalgorithmus an:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>Optionen der Pool-Komprimierung</title>
   <para>
    Eine vollständige Liste der Komprimierungseinstellungen:
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Zulässige Werte sind <literal>none</literal>, <literal>zstd</literal> und <literal>snappy</literal>. Die Standardeinstellung lautet <literal>snappy</literal>.
      </para>
      <para>
       Der zu verwendende Komprimierungsalgorithmus hängt vom einzelnen Anwendungsfall ab. Einige Empfehlungen:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Behalten Sie die Standardeinstellung <literal>snappy</literal> bei, sofern es keinen guten Grund gibt, diese Einstellung zu ändern.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal> bietet ein gutes Komprimierungsverhältnis, verursacht jedoch beim Komprimieren kleiner Datenmengen einen hohen CPU-Overhead.
        </para>
       </listitem>
       <listitem>
        <para>
         Vergleichen Sie diese Algorithmen anhand eines Beispiels Ihrer realen Daten und beobachten Sie dabei die CPU- und Arbeitsspeicherauslastung Ihres Clusters.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       Zulässige Werte sind <literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal> und <literal>force</literal>. Die Standardeinstellung lautet <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: Niemals komprimieren
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: Komprimieren bei Hinweis auf <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: Komprimieren, falls kein Hinweis auf <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: Immer komprimieren
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Wert: Gleitkomma „double“, Grad = SIZE_COMPRESSED / SIZE_ORIGINAL. Der Standardwert lautet <literal>0,875</literal>; wenn die Komprimierung also den belegten Speicherplatz um mindestens 12,5 % verringert, wird das Objekt nicht komprimiert.
      </para>
      <para>
       Objekte mit einem höheren Grad werden nicht komprimiert gespeichert, weil der Nutzen insgesamt sehr gering ist.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Maximale Größe der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>Globale Komprimierungsoptionen</title>
   <para>
    Die folgenden Konfigurationsoptionen können in der Ceph-Konfiguration festgelegt werden und gelten für alle OSDs und nicht nur für einen einzelnen Pool. Die in <xref linkend="sec-ceph-pool-compression-options"/> aufgelistete für den Pool spezifische Konfiguration hat Vorrang.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Siehe <xref linkend="compr-algorithm"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Siehe <xref linkend="compr-mode"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Siehe <xref linkend="compr-ratio"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert werden. Die Einstellung wird standardmäßig zugunsten von <option>bluestore_compression_min_blob_size_hdd</option> und <option>bluestore_compression_min_blob_size_ssd</option> ignoriert. Wenn ein Wert ungleich null festgelegt ist, erhält sie Vorrang.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>0</literal>
      </para>
      <para>
       Maximale Größe von komprimierten Objekten, bevor sie in kleinere Blöcke aufgeteilt werden. Die Einstellung wird standardmäßig zugunsten von <option>bluestore_compression_max_blob_size_hdd</option> und <option>bluestore_compression_max_blob_size_ssd</option> ignoriert. Wenn ein Wert ungleich null festgelegt ist, erhält sie Vorrang.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>8K</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert und auf Festkörperlaufwerk gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>64K</literal>
      </para>
      <para>
       Maximale Größe von komprimierten, auf einem Solid-State-Laufwerk gespeicherten Objekten, bevor sie in kleinere Blöcke aufgeteilt werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>128K</literal>
      </para>
      <para>
       Mindestgröße der Objekte, die komprimiert und auf Festplatten gespeichert werden.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Wert: Ganzzahl ohne Vorzeichen, Größe in Byte. Standardeinstellung: <literal>512K</literal>
      </para>
      <para>
       Maximale Größe von komprimierten, auf einer Festplatte gespeicherten Objekten, bevor sie in kleinere Blöcke aufgeteilt werden.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
