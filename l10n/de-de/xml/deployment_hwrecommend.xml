<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>Hardwareanforderungen und Empfehlungen</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Die Hardwareanforderungen für Ceph hängen stark vom E/A-Workload ab. Die folgenden Hardwareanforderungen und Empfehlungen sollten als Ausgangspunkt für die detaillierte Planung betrachtet werden.
 </para>
 <para>
  Im Allgemeinen sind die Empfehlungen in diesem Abschnitt auf jeweils einen Prozess ausgelegt. Wenn auf dem selben Rechner mehrere Prozesse ablaufen, müssen die CPU-, RAM-, Festplatten- und Netzwerkanforderungen entsprechend erhöht werden.
 </para>
 <sect1 xml:id="network-overview">
  <title>Netzwerke im Überblick</title>

  <para>
   Ceph verfügt über einige logische Netzwerke:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Ein Frontend-Netzwerk namens <literal>öffentliches Netzwerk</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     Ein vertrauenswürdiges internes Netzwerk, das Back-End-Netzwerk, namens <literal>Clusternetzwerk</literal>. Diese Einstellung ist optional.
    </para>
   </listitem>
   <listitem>
    <para>
     Ein oder mehrere Client-Netzwerke für Gateways. Dies ist optional und würde den Rahmen dieses Kapitels sprengen.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Das öffentliche Netzwerk ist das Netzwerk, über das Ceph-Daemons untereinander und mit ihren Clients kommunizieren. Das bedeutet, dass der gesamte Datenverkehr des Ceph-Clusters über dieses Netzwerk läuft, es sei denn, es ist ein Clusternetzwerk konfiguriert.
  </para>

  <para>
   Das Clusternetzwerk ist das Back-End-Netzwerk zwischen den OSD-Knoten für Reproduktion, Ausgleich und Wiederherstellung. Ist dieses optionale Netzwerk konfiguriert, würde es im Idealfall die doppelte Bandbreite des öffentlichen Netzwerks mit standardmäßiger Drei-Wege-Reproduktion bieten, da das primäre OSD zwei Kopien an andere OSDs über dieses Netzwerk sendet. Das öffentliche Netzwerk befindet sich zwischen Clients und Gateways auf der einen Seite für die Kommunikation mit Monitoren, Managern, MDS-Knoten und OSD-Knoten. Monitore, Manager und MDS-Knoten nutzen es auch für die Kommunikation mit OSD-Knoten.
  </para>

  <figure xml:id="network-overview-figure">
   <title>Netzwerke im Überblick</title>
   <mediaobject>
    <imageobject role="html">
     <imagedata fileref="network-overview-diagram.png" width="70%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="ceph-install-ceph-deploy-network">
   <title>Netzwerkempfehlungen</title>
   <para>
    Wir empfehlen ein einzelnes fehlertolerantes Netzwerk mit ausreichender Bandbreite für Ihre Anforderungen. Für die öffentliche Ceph-Netzwerkumgebung empfehlen wir zwei verbundene 25-GbE-Netzwerkschnittstellen (oder schneller), die über 802.3ad (LACP) verbunden sind. Dies wird als die Mindesteinrichtung für Ceph betrachtet. Wenn Sie zusätzlich ein Clusternetzwerk verwenden, empfehlen wir vier verbundene 25-GbE-Netzwerkschnittstellen. Durch Verbinden von zwei oder mehr Netzwerkschnittstellen verbessern sich der Durchsatz durch Link-Aggregation und, bei redundanten Links und Switches, die Fehlertoleranz und Wartbarkeit.
   </para>
   <para>
    Sie können auch VLANs erstellen, um verschiedene Arten von Datenverkehr über eine Verbindung zu isolieren. Sie können beispielsweise eine Verbindung erstellen, um zwei VLAN-Schnittstellen bereitzustellen, eine für das öffentliche Netzwerk und die zweite für das Clusternetzwerk. Beim Einrichten des Ceph-Netzwerks ist dies jedoch <emphasis>nicht</emphasis> erforderlich. Details zum Verbinden von Schnittstellen finden Sie unter <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-network.html#sec-network-iface-bonding"/>.
   </para>
   <para>
    Die Fehlertoleranz kann durch Isolierung der Komponenten in Fehlerdomänen erhöht werden. Zur Erhöhung der Fehlertoleranz des Netzwerks wird durch Verbinden einer Schnittstelle von zwei separaten Netzwerkschnittstellenkarten (NIC) Schutz vor dem Ausfall einer einzelnen NIC erreicht. In ähnlicher Weise schützt das Erstellen einer Verbindung zwischen zwei Switches vor dem Ausfall eines Switches. Wir empfehlen, den Hersteller der Netzwerkgeräte zu konsultieren, um den erforderlichen Grad an Fehlertoleranz zu ermitteln.
   </para>
   <important>
    <title>Verwaltungsnetzwerk</title>
    <para>
     Die Einrichtung eines zusätzlichen Verwaltungsnetzwerks, das zum Beispiel die Trennung von SSH-, Salt- oder DNS-Netzwerken ermöglicht, wurde weder getestet, noch wird sie unterstützt.
    </para>
   </important>
   <tip>
    <title>Über DHCP konfigurierte Knoten</title>
    <para>
     Wenn Ihre Speicherknoten über DHCP konfiguriert werden, reichen die standardmäßigen Zeitüberschreitungen möglicherweise nicht für eine korrekte Konfiguration des Netzwerks aus, bevor die Ceph Daemons starten. Wenn dies geschieht, werden die Ceph-MONs und -OSDs nicht korrekt gestartet (das Ausführen von <command>systemctl status ceph\*</command> führt zur Fehlermeldung „unable to bind“ (Verbinden nicht möglich)). Zur Vermeidung dieses Problems empfehlen wir, die DHCP-Client-Zeitüberschreitung auf jedem Knoten in Ihrem Speichercluster auf mindestens 30 Sekunden zu erhöhen. Dies wird erreicht durch Ändern der folgenden Einstellungen in jedem Knoten:
    </para>
    <para>
     Legen Sie in <filename>/etc/sysconfig/network/dhcp</filename> Folgendes fest:
    </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
    <para>
     Legen Sie in <filename>/etc/sysconfig/network/config</filename> Folgendes fest:
    </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
   </tip>
   <sect3 xml:id="storage-bp-net-private">
    <title>Hinzufügen eines privaten Netzwerks zu einem aktiven Cluster</title>
    <para>
     Wenn Sie bei der Ceph-Bereitstellung kein Cluster-Netzwerk angeben, dann wird eine einzelne öffentliche Netzwerkumgebung angenommen. Auch wenn Ceph in einem öffentlichen Netzwerk gut funktioniert, wird seine Leistung und Sicherheit durch Festlegen eines zweiten privaten Cluster-Netzwerks erhöht. Zur Unterstützung von zwei Netzwerken muss jeder Ceph-Knoten über mindestens zwei Netzwerkkarten verfügen.
    </para>
    <para>
     Sie müssen auf jeden Ceph-Knoten die folgenden Änderungen anwenden. Bei einem kleinen Cluster ist dies relativ schnell erledigt, doch bei einem Cluster mit Hunderten oder Tausenden Knoten kann dieser Vorgang sehr zeitaufwändig sein.
    </para>
    <procedure>
     <step>
      <para>
       Legen Sie das Clusternetzwerk mit folgendem Kommando fest:
      </para>
<screen><prompt role="root">root # </prompt>ceph config set global cluster_network <replaceable>MY_NETWORK</replaceable></screen>
      <para>
       Starten Sie die OSDs neu, um das angegebene Clusternetzwerk zu binden:
      </para>
<screen><prompt role="root">root # </prompt>systemctl restart ceph-*@osd.*.service</screen>
     </step>
     <step>
      <para>
       Überprüfen Sie, ob das private Cluster-Netzwerk auf Betriebssystemebene wie erwartet funktioniert.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="storage-bp-net-subnets">
    <title>Überwachungsknoten in verschiedenen Teilnetzen</title>
    <para>
     Wenn sich die Überwachungsknoten in mehreren Teilnetzen befinden, beispielsweise in verschiedenen Räumen und gesteuert durch verschiedene Schalter, dann müssen Sie ihre öffentliche Netzwerkadresse in der CIDR-Schreibweise entsprechend anpassen:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon public_network "<replaceable>MON_NETWORK_1</replaceable>, <replaceable>MON_NETWORK_2</replaceable>, <replaceable>MON_NETWORK_N</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config set mon public_network "192.168.1.0/24, 10.10.0.0/16"</screen>
    <warning>
     <para>
      Wenn Sie, wie in diesem Abschnitt beschrieben, mehr als ein Netzwerksegment für das öffentliche Netzwerk (oder für das Clusternetzwerk) angeben, muss jedes dieser Teilnetze zur Weiterleitung zu den anderen Teilnetzen in der Lage sein. Andernfalls können die MONs und andere Ceph-Daemons auf verschiedenen Netzwerksegmenten nicht miteinander kommunizieren, was zu einem geteilten Cluster führen würde. Wenn Sie eine Firewall verwenden, stellen Sie außerdem sicher, dass Sie jede IP-Adresse oder jedes Teilnetz in Ihre iptables einbeziehen und Ports für sie auf allen Knoten öffnen, falls erforderlich.
     </para>
    </warning>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="multi-architecture">
  <title>Konfigurationen mit mehreren Architekturen</title>

  <para>
   SUSE Enterprise Storage unterstützt sowohl x86- als auch Arm-Architekturen. Bei der Erwägung der einzelnen Architekturen ist zu beachten, dass es bei den Cores pro OSD, der Frequenz und dem RAM keine gravierenden Unterschiede zwischen den CPU-Architekturen gibt, die sich auf die Größenfestlegung auswirken würden.
  </para>

  <para>
   Wie bei kleineren x86-Prozessoren (keine Server) bieten weniger leistungsstarke Arm-basierte Cores unter Umständen keine optimalen Arbeitsbedingungen, insbesondere wenn sie für Pools mit Löschcodierung eingesetzt werden.
  </para>

  <note>
   <para>
    In der gesamten Dokumentation wird <replaceable>SYSTEM-ARCH</replaceable> anstelle von x86 oder Arm verwendet.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="ses-hardware-config">
  <title>Hardwarekonfiguration</title>

  <para>
   Für eine optimale Produkterfahrung empfehlen wir, mit der empfohlenen Cluster-Konfiguration zu beginnen. Für einen Testcluster oder einen Cluster mit geringeren Leistungsanforderungen dokumentieren wir eine minimal unterstützte Cluster-Konfiguration.
  </para>

  <sect2 xml:id="ses-bp-minimum-cluster">
   <title>Mindestkonfiguration für den Cluster</title>
   <para>
    Eine Mindestkonfiguration für den Produktcluster besteht aus:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Mindestens vier physikalischen Knoten (OSD-Knoten) mit gemeinsamem Speicherort der Services
     </para>
    </listitem>
    <listitem>
     <para>
      Dual-10-Gbit-Ethernet als verbundenes Netzwerk
     </para>
    </listitem>
    <listitem>
     <para>
      Einem separaten Admin-Knoten (kann auf einem externen Knoten virtualisiert werden)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Eine detaillierte Konfiguration besteht aus folgenden Komponenten:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Separater Verwaltungsknoten mit 4 GB RAM, vier Cores, 1 TB Kapazität. Dies ist in der Regel der Salt-Master-Knoten. Ceph-Services und -Gateways wie Ceph Monitor, Metadata Server, Ceph OSD, Object Gateway oder NFS Ganesha werden auf dem Admin-Knoten nicht unterstützt, da er die Clusteraktualisierungs- und Upgrade-Prozesse eigenständig orchestrieren muss.
     </para>
    </listitem>
    <listitem>
     <para>
      Mindestens vier physische OSD-Knoten mit jeweils acht OSD-Datenträgern. Die Anforderungen finden Sie in <xref linkend="sysreq-osd"/>.
     </para>
     <para>
      Die Gesamtkapazität des Clusters sollte so bemessen sein, dass auch bei Ausfall eines Knotens die gesamte genutzte Kapazität (einschließlich Redundanz) 80 % nicht überschreitet.
     </para>
    </listitem>
    <listitem>
     <para>
      Drei Ceph Monitor-Instanzen. Monitore müssen aus Latenzgründen von SSD/NVMe-Speicher, nicht von HDDs, betrieben werden.
     </para>
    </listitem>
    <listitem>
     <para>
      Monitore, Metadatenserver und Gateways können gemeinsam auf den OSD-Knoten platziert werden. Informationen über die gemeinsame Platzierung finden Sie in <xref linkend="ses-bp-diskshare"/>. Wenn Sie Services gemeinsam platzieren, müssen die Speicher- und CPU-Anforderungen addiert werden.
     </para>
    </listitem>
    <listitem>
     <para>
      iSCSI Gateway, Object Gateway und Metadata Server benötigen mindestens inkrementelle 4 GB RAM und vier Cores.
     </para>
    </listitem>
    <listitem>
     <para>
      Wenn Sie CephFS, S3/Swift, iSCSI verwenden, sind für Redundanz und Verfügbarkeit mindestens zwei Instanzen der jeweiligen Rollen (Metadata Server, Object Gateway, iSCSI) erforderlich.
     </para>
    </listitem>
    <listitem>
     <para>
      Die Knoten müssen für SUSE Enterprise Storage bestimmt sein und dürfen nicht für andere physische, containerisierte oder virtualisierte Workloads verwendet werden.
     </para>
    </listitem>
    <listitem>
     <para>
      Wenn eines der Gateways (iSCSI, Object Gateway, NFS Ganesha, Metadata Server usw.) innerhalb von VMs bereitgestellt wird, dürfen diese VMs nicht auf den physischen Rechnern gehostet werden, die andere Clusterrollen bedienen. (Dies ist nicht notwendig, da sie als verbundene Services unterstützt werden.)
     </para>
    </listitem>
    <listitem>
     <para>
      Bei der Bereitstellung von Services als VMs auf Hypervisoren außerhalb des physischen Core-Clusters müssen Ausfalldomänen beachtet werden, um Redundanz zu gewährleisten.
     </para>
     <para>
      Stellen Sie zum Beispiel nicht mehrere Rollen desselben Typs auf demselben Hypervisor bereit, beispielsweise mehrere MON- oder MDS-Instanzen.
     </para>
    </listitem>
    <listitem>
     <para>
      Bei der Bereitstellung innerhalb von VMs ist es besonders wichtig, dass die Knoten über eine starke Netzwerkanbindung und eine gut funktionierende Zeitsynchronisierung verfügen.
     </para>
    </listitem>
    <listitem>
     <para>
      Die Hypervisorknoten müssen ausreichend dimensioniert sein, um Störungen durch andere Workloads zu vermeiden, die CPU-, RAM-, Netzwerk- und Speicherressourcen verbrauchen.
     </para>
    </listitem>
   </itemizedlist>
   <figure>
    <title>Mindestkonfiguration für den Cluster</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="minimal-ses.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="ses-bp-production-cluster">
   <title>Empfohlene Konfiguration für Produktionscluster</title>
   <para>
    Sobald Sie Ihren Cluster vergrößern, empfehlen wir, Ceph Monitors, Metadatenserver und Gateways auf separate Knoten zu verlagern, um eine bessere Fehlertoleranz zu erreichen.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Sieben Objektspeicher-Knoten
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Die einzelnen Knoten dürfen nicht mehr als ca. 15 % des Gesamtspeichers ausmachen.
       </para>
      </listitem>
      <listitem>
       <para>
        Die Gesamtkapazität des Clusters sollte so bemessen sein, dass auch bei Ausfall eines Knotens die gesamte genutzte Kapazität (einschließlich Redundanz) 80 % nicht überschreitet.
       </para>
      </listitem>
      <listitem>
       <para>
        25 Gb Ethernet oder besser, jeweils verbunden für internes Clusternetzwerk und externes öffentliches Netzwerk.
       </para>
      </listitem>
      <listitem>
       <para>
        Mindestens 56 OSDs pro Speicher-Cluster.
       </para>
      </listitem>
      <listitem>
       <para>
        In <xref linkend="sysreq-osd"/> finden Sie weitere Empfehlungen.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Dedizierte physische Infrastruktur-Knoten.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Drei Ceph-Monitor-Knoten: 4 GB RAM, 4-Core-Prozessor, RAID 1-SSDs als Festplatte.
       </para>
       <para>
        In <xref linkend="sysreq-mon"/> finden Sie weitere Empfehlungen.
       </para>
      </listitem>
      <listitem>
       <para>
        Object Gateway Knoten: 32 GB RAM, 8-Core-Prozessor, RAID 1-SSDs als Festplatte.
       </para>
       <para>
        In <xref linkend="sysreq-rgw"/> finden Sie weitere Empfehlungen.
       </para>
      </listitem>
      <listitem>
       <para>
        iSCSI Gateway Knoten: 16 GB RAM, 8-Core-Prozessor, RAID 1-SSDs als Festplatte.
       </para>
       <para>
        In <xref linkend="sysreq-iscsi"/> finden Sie weitere Empfehlungen.
       </para>
      </listitem>
      <listitem>
       <para>
        Metadata Server Knoten (einer aktiv, einer unmittelbar betriebsbereit im Standby-Modus): 32 GB RAM, 8-Core-Prozessor, RAID 1-SSDs als Festplatte.
       </para>
       <para>
        In <xref linkend="sysreq-mds"/> finden Sie weitere Empfehlungen.
       </para>
      </listitem>
      <listitem>
       <para>
        Ein SES-Admin-Knoten: 4 GB RAM, 4-Core-Prozessor, RAID 1-SSDs als Datenträger.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="deployment-hw-multipath">
   <title>Multipfadkonfiguration</title>
   <para>
    Wenn Sie Multipfadhardware verwenden möchten, stellen Sie sicher, dass LVM in der Konfigurationsdatei unter dem Abschnitt <literal>Geräte</literal> die Einstellung <literal>multipath_component_detection = 1</literal> sieht. Dies wird mit dem Kommando <command>lvm config</command> geprüft.
   </para>
   <para>
    Stellen Sie alternativ sicher, dass LVM die Multipfadkomponenten eines Geräts über die LVM-Filterkonfiguration filtert. Das ist hostspezifisch zu verstehen.
   </para>
   <note>
    <para>
     Es wird nicht empfohlen und sollte immer nur dann in Betracht gezogen werden, wenn <literal>multipath_component_detection = 1</literal> nicht festgelegt werden kann.
    </para>
   </note>
   <para>
    Weitere Informationen zur Multipfadkonfiguration finden Sie unter <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-multipath.html#sec-multipath-lvm"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>Objektspeicherknoten</title>

  <sect2 xml:id="sysreq-osd">
   <title>Mindestanforderungen</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Die folgenden CPU-Empfehlungen berücksichtigen Geräte unabhängig von der Nutzung durch Ceph:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Ein 2-GHz-CPU-Thread pro rotierendem Datenträger.
       </para>
      </listitem>
      <listitem>
       <para>
        Zwei 2-GHz-CPU-Threads pro SSD.
       </para>
      </listitem>
      <listitem>
       <para>
        Vier 2-GHz-CPU-Threads pro NVMe.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Separate 10-GbE-Netzwerke (öffentlich, Client und intern), erforderlich 4 × 10 GbE, empfohlen 2 × 25 GbE.
     </para>
    </listitem>
    <listitem>
     <para>
      Insgesamt benötigtes RAM = Anzahl der OSDs × (1 GB + <option>osd_memory_target</option>) + 16 GB
     </para>
     <para>
      Weitere Informationen zu <option>osd_memory_target</option> finden Sie in <xref linkend="config-auto-cache-sizing"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD-Datenträger in JBOD-Konfigurationen oder individuelle RAID-0-Konfigurationen.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD-Journal darf sich auf der OSD-Festplatte befinden.
     </para>
    </listitem>
    <listitem>
     <para>
      OSD-Festplatten sollten exklusiv von SUSE Enterprise Storage verwendet werden.
     </para>
    </listitem>
    <listitem>
     <para>
      Dedizierter Datenträger und SSD für das Betriebssystem, vorzugsweise in einer RAID 1-Konfiguration.
     </para>
    </listitem>
    <listitem>
     <para>
      Weisen Sie mindestens 4 GB zusätzlichen Arbeitsspeicher zu, wenn dieser OSD-Host einen Teil eines Cache-Pools hostet, der für Cache-Tiering verwendet wird.
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph Monitors, Gateway und Metadata Server dürfen in Objektspeicher-Knoten vorhanden sein.
     </para>
    </listitem>
    <listitem>
     <para>
      Aus Gründen der Festplattenleistung handelt es sich bei OSD-Knoten um Bare-Metal-Knoten. Auf einem OSD-Knoten sollten keine anderen Workloads ausgeführt werden, es sei denn, es handelt sich um eine Mindesteinrichtung von Ceph Monitors und Ceph Managers.
     </para>
    </listitem>
    <listitem>
     <para>
      SSDs für Journal im Verhältnis 6:1 von SSD-Journal zu OSD.
     </para>
    </listitem>
   </itemizedlist>
     <note>
       <para>
         Stellen Sie sicher, dass den OSD-Knoten keine vernetzten Blockgeräte zugeordnet sind, wie zum Beispiel iSCSI- oder RADOS-Blockgeräte-Images.
       </para>
     </note>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Mindestgröße für Datenträger</title>
   <para>
    Zwei Arten von Datenträgerspeicherplatz werden zur Ausführung auf OSD benötigt: der Speicherplatz für das WAL/DB-Gerät sowie der primäre Speicherplatz für die gespeicherten Daten. Der Mindestwert (und Standardwert) für das WAL/DB beträgt 6 GB. Der Mindestspeicherplatz für Daten beträgt 5 GB, da Partitionen, die kleiner als 5 GB sind, das Gewicht 0 zugewiesen wird.
   </para>
   <para>
    Auch wenn nun der Mindestspeicherplatz für ein OSD 11 GB beträgt, empfehlen wir mindestens 20 GB pro Festplatte, sogar für Testzwecke.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Empfohlene Größe für das WAL- und DB-Gerät von BlueStore</title>
   <tip>
    <title>Weitere Informationen</title>
    <para>
     Weitere Informationen zu BlueStore finden Sie in <xref linkend="about-bluestore"/>.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      Es wird empfohlen, 4 GB für das WAL-Gerät zu reservieren. Die empfohlene Größe für DB liegt bei den meisten Workloads bei 64 GB.
     </para>
     <important>
      <para>
       Wir empfehlen größere DB-Volumes für Bereitstellungen mit hoher Auslastung, insbesondere bei hoher RGW- oder CephFS-Nutzung. Reservieren Sie etwas Kapazität (Slots), um bei Bedarf mehr Hardware für mehr DB-Speicherplatz zu installieren.
      </para>
     </important>
    </listitem>
    <listitem>
     <para>
      Falls Sie beabsichtigen, das WAL- und DB-Gerät auf dieselbe Festplatte zu stellen, dann empfehlen wir eine einzelne Partition für beide Geräte statt eine eigene Partition pro Gerät. Dadurch kann Ceph das DB-Gerät auch für WAL-Operationen verwenden. Die Verwaltung des Festplattenspeicherplatzes ist daher effizienter, weil Ceph die DB-Partition für WAL nur dann verwendet, wenn es unbedingt erforderlich ist. Ein weiterer Vorteil besteht darin, dass eine volle Auslastung der WAL-Partition sehr unwahrscheinlich ist und kein Speicherplatz verschwendet wird, da er gegebenenfalls auch für DB-Operationen verwendet werden kann.
     </para>
     <para>
      Um das DB-Gerät für WAL freizugeben, geben Sie <emphasis>nicht</emphasis> das WAL-Gerät an, sondern nur das DB-Gerät.
     </para>
     <para>
      Weitere Informationen zum Festlegen eines OSD-Layouts finden Sie im <xref linkend="drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>SSD für WAL/DB-Partitionen</title>
   <para>
    Solid-State- oder Festkörperlaufwerke (SSD) haben keine beweglichen Teile. Dadurch wird die Zeit für den zufälligen Zugriff und die Leselatenz reduziert und der Datendurchsatz beschleunigt. Da der Preis pro 1 MB für SSDs erheblich höher ist als der Preis für sich drehende Festplatten, eignen sich SSDs nur für kleinere Speicher.
   </para>
   <para>
    Die Leistung von OSDs wird möglicherweise erheblich verbessert, wenn Sie deren WAL/ DB auf einem SSD speichern und die Objektdaten auf einer separaten Festplatte.
   </para>
   <tip>
    <title>Freigabe einer SSD für mehrere WAL/DB-Partitionen</title>
    <para>
     Da WAL/DB-Partitionen relativ wenig Speicherplatz belegen, können Sie einen SSD-Datenträger mit mehreren WAL/DB-Partitionen freigeben. Bedenken Sie dabei jedoch, dass sich mit jeder WAL/DB-Partition die Leistung des SSD-Datenträgers verschlechtert. Wir empfehlen, maximal sechs WAL/DB-Partitionen pro SSD-Datenträger zu speichern und zwölf pro NVMe-Datenträger.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Maximale empfohlene Anzahl von Datenträgern</title>
   <para>
    Jeder Server kann so viele Festplatten enthalten wie für ihn zulässig sind. Bei der Planung der Anzahl von Festplatten pro Server gibt es einiges zu bedenken:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Netzwerk-Bandbreite.</emphasis> Je mehr Festplatten ein Server enthält, desto mehr Daten müssen für die Schreiboperationen der Festplatte über die Netzwerkkarte(n) übertragen werden.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Arbeitsspeicher.</emphasis> RAM ab 2 GB wird für den BlueStore-Cache herangezogen. Mit dem Standardwert von 4 GB für <option>osd_memory_target</option> erhält das System eine angemessene Cache-Anfangsgröße für rotierende Datenträger. Bei SSD oder NVME sollten Sie die Cache-Größe und die RAM-Zuordnung pro OSD erhöhen, damit die höchstmögliche Leistung erzielt wird.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Fehlertoleranz.</emphasis> Wenn der Server komplett ausfällt, verliert der Cluster temporär so viele OSDs wie er Festplatten hat. Darüberhinaus müssen Sie alle Daten des ausgefallenen Servers auf die anderen Knoten im Cluster kopieren, damit die Reproduktionsregeln weiterhin ausgeführt werden.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Monitorknoten</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Mindestens drei MON-Knoten sind erforderlich. Die Anzahl der Monitore sollte immer ungerade sein (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Prozessor mit vier logischen Cores.
    </para>
   </listitem>
   <listitem>
    <para>
     Ein SSD oder ein anderer ausreichend schneller Speichertyp ist für Monitors sehr zu empfehlen, insbesondere für den Pfad <filename>/var/lib/ceph</filename> in jedem Monitor-Knoten, da das Quorum bei hohen Festplattenlatenzen möglicherweise instabil ist. Zwei Festplatten in der RAID 1-Konfiguration werden aus Redundanzgründen empfohlen. Es wird empfohlen, dass separate Festplatten oder mindestens separate Festplattenpartitionen für die Überwachungsprozesse zur Verfügung stehen, um den verfügbaren Festplattenspeicherplatz des Monitors vor Ereignissen wie schleichender Protokolldateiausweitung zu schützen.
    </para>
   </listitem>
   <listitem>
    <para>
     Pro Knoten darf nur ein Überwachungsprozess vorhanden sein.
    </para>
   </listitem>
   <listitem>
    <para>
     Die Kombination von OSD-, MON- oder Object-Gateway-Knoten wird nur unterstützt, wenn ausreichend Hardwareressourcen verfügbar sind. Dies bedeutet, dass die Anforderungen für alle Services aufsummiert werden müssen.
    </para>
   </listitem>
   <listitem>
    <para>
     Zwei Netzwerkschnittstellen verbunden mit mehreren Schaltern.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Object-Gateway-Knoten</title>

  <para>
   Object-Gateway-Knoten sollten über mindestens sechs CPU-Cores und 32 GB RAM verfügen. Wenn sich noch andere Prozesse auf demselben Rechner befinden, müssen deren Anforderungen aufsummiert werden.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Metadata Server-Knoten</title>

  <para>
   Die richtige Größe der Metadata Server Knoten hängt vom spezifischen Anwendungsfall ab. Generell gilt, je mehr offene Dateien der Metadata Server verarbeiten muss, desto mehr CPU und RAM benötigt er. Nachfolgend finden Sie die Mindestanforderungen an die 
  </para>

  <itemizedlist>
   <listitem>
    <para>
     4 GB RAM für jeden Metadatenserver-Daemon.
    </para>
   </listitem>
   <listitem>
    <para>
     Gebundene Netzwerkschnittstelle.
    </para>
   </listitem>
   <listitem>
    <para>
     2,5 GHz CPU mit mindestens 2 Cores.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-admin-node">
  <title>Admin-Knoten</title>

  <para>
   Mindestens 4 GB RAM und ein CPU mit vier Cores sind erforderlich. Dies umfasst die Ausführung des Salt Masters auf dem Admin-Knoten Für große Cluster mit Hunderten von Knoten werden 6 GB RAM vorgeschlagen.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>iSCSI-Gateway-Knoten</title>

  <para>
   iSCSI-Gateway-Knoten sollten über mindestens sechs CPU-Cores und 16 GB RAM verfügen.
  </para>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SES und andere SUSE-Produkte</title>

  <para>
   Dieser Abschnitt enthält wichtige Informationen zur Integration von SES in andere SUSE-Produkte.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager und SUSE Enterprise Storage sind nicht integriert. Daher kann SUSE Manager aktuell keinen SES-Cluster verwalten.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Namensbegrenzungen</title>

  <para>
   Ceph unterstützt nicht generell Nicht-ASCII-Zeichen in Konfigurationsdateien, Pool-Namen, Benutzernamen und so weiter. Wir empfehlen, beim Konfigurieren eines Ceph-Clusters in allen Ceph-Objekt- bzw. Konfigurationsnamen nur einfache alphanumerische Zeichen (A-Z, a-z, 0-9) und wenige Satzzeichen  ('.', '-', '_') zu verwenden.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>Ein einziger Server für OSD und Monitor</title>

  <para>
   Obwohl es technisch möglich ist, OSDs und MONs in Testumgebungen auf demselben Server auszuführen, empfehlen wir dringend, einen separaten Server für jeden Monitorknoten in der Produktionsumgebung einzurichten. Der hauptsächliche Grund dafür besteht in der Leistung. Je mehr OSDs der Cluster enthält, desto mehr E/A-Vorgänge müssen die MON-Knoten durchführen. Wenn ein Server von einem MON-Knoten und OSD(s) gemeinsam genutzt wird, stellen die E/A-Vorgänge des OSD eine Beschränkung für den Monitorknoten dar.
  </para>

  <para>
   Es ist weiterhin zu überlegen, ob Festplatten von einem OSD, einem MON-Knoten und dem Betriebssystem auf dem Server gemeinsam genutzt werden sollen. Die Antwort ist einfach: wenn möglich, stellen Sie eine separate Festplatte für den OSD bereit und einen separaten Server für einen Monitor-Knoten.
  </para>

  <para>
   Obwohl Ceph verzeichnisbasierte OSDs unterstützt, sollte für einen OSD immer eine eigene Festplatte vorhanden sein und nicht die des Betriebssystems dafür genutzt werden.
  </para>

  <tip>
   <para>
    Wenn es <emphasis>wirklich</emphasis> erforderlich ist, OSD- und MON-Knoten auf demselben Server auszuführen, führen Sie MON auf einem separaten Datenträger aus. Hängen Sie dazu den Datenträger im Verzeichnis <filename>/var/lib/ceph/mon</filename> ein, um die Leistung etwas zu verbessern.
   </para>
  </tip>
 </sect1>
</chapter>
