<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Upgrade von einer früheren Version</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>Ja</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In diesem Kapitel finden Sie die Schritte zum Upgrade von SUSE Enterprise Storage 6 auf Version 7.
 </para>
 <para>
  Das Upgrade umfasst die folgenden Aufgaben:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Upgrade von Ceph Nautilus zu Octopus.
   </para>
  </listitem>
  <listitem>
   <para>
    Umstellung von der Installation und Ausführung von Ceph über RPM-Pakete auf die Ausführung in Containern.
   </para>
  </listitem>
  <listitem>
   <para>
    Vollständiges Entfernen von DeepSea und Ersetzen durch <systemitem class="resource">ceph-salt</systemitem> und cephadm.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   Die Upgrade-Informationen in diesem Kapitel gelten <emphasis>nur</emphasis> für Upgrades von DeepSea auf cephadm. Versuchen Sie nicht, diese Anweisungen zu befolgen, wenn Sie SUSE Enterprise Storage auf der SUSE CaaS-Plattform implementieren möchten.
  </para>
 </warning>
 <important>
  <para>
   Das Upgrade von SUSE Enterprise Storage-Versionen vor 6 wird nicht unterstützt. Sie müssen zunächst ein Upgrade auf die aktuelle Version von SUSE Enterprise Storage 6 durchführen und dann die Schritte in diesem Kapitel ausführen.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Vor dem Aufrüsten</title>

  <para>
   Die folgenden Aufgaben <emphasis>müssen</emphasis> abgeschlossen sein, bevor Sie das Upgrade starten. Dies kann jederzeit während der Laufzeit von SUSE Enterprise Storage 6 erfolgen.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Die OSD-Migration von FileStore zu BlueStore <emphasis>muss</emphasis> vor dem Upgrade erfolgen, da FileStore in SUSE Enterprise Storage 7 nicht unterstützt wird. Weitere Details zu BlueStore und zur Migration von FileStore finden Sie unter <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Wenn Sie einen älteren Cluster ausführen, der noch <literal>ceph-disk</literal>-OSDs verwendet, <emphasis>müssen</emphasis> Sie vor dem Upgrade auf <literal>ceph-volume</literal> umstellen. Weitere Informationen finden Sie in <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Zu berücksichtigende Aspekte</title>
   <para>
    Lesen Sie vor dem Upgrade unbedingt die folgenden Abschnitte durch, um sicherzustellen, dass Sie alle auszuführenden Aufgaben verstehen.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>In den Versionshinweisen</emphasis> finden Sie zusätzliche Informationen zu den Änderungen, die seit der vorigen Version von SUSE Enterprise Storage vorgenommen wurden. Informieren Sie sich in den Versionshinweisen über Folgendes:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Sind bei der Hardware besondere Überlegungen zu beachten?
       </para>
      </listitem>
      <listitem>
       <para>
        Wurden erhebliche Änderungen an den verwendeten Softwarepaketen vorgenommen?
       </para>
      </listitem>
      <listitem>
       <para>
        Gelten besondere Vorsichtsmaßnahmen für die vorliegende Installation?
       </para>
      </listitem>
     </itemizedlist>
     <para>
      In den Versionshinweisen finden Sie auch Informationen, die erst nach der Fertigstellung des Handbuchs bekannt wurden. Auch bekannte Probleme werden beschrieben.
     </para>
     <para>
      Die Versionshinweise zu SES 7 finden Sie online unter <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Nach Installation des Pakets
      <package>release-notes-ses</package> aus dem SES 7-Repository finden Sie die Versionshinweise zudem lokal im Verzeichnis <filename>/usr/share/doc/release-notes</filename> oder online unter <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Lesen Sie <xref linkend="deploy-cephadm"/>, um sich mit <systemitem class="resource">ceph-salt</systemitem> und dem Ceph-Orchestrator vertraut zu machen, insbesondere die Informationen zu Servicespezifikationen.
     </para>
    </listitem>
    <listitem>
     <para>
      Das Cluster-Upgrade kann lange dauern, nämlich in etwa so lange, wie es dauert, ein Upgrade eines Computers multipliziert mit der Anzahl der Cluster-Knoten durchzuführen.
     </para>
    </listitem>
    <listitem>
     <para>
      Sie müssen zuerst ein Upgrade des Salt Master durchführen und danach DeepSea durch <systemitem class="resource">ceph-salt</systemitem> und cephadm ersetzen. Sie können das cephadm-Orchestrator-Modul <emphasis>erst dann</emphasis> verwenden, wenn mindestens für alle Ceph-Manager ein Upgrade durchgeführt wurde.
     </para>
    </listitem>
    <listitem>
     <para>
      Das Upgrade von Nautilus-RPMs auf Octopus-Container muss in einem Schritt erfolgen. Das bedeutet, dass Sie für einen ganzen Knoten auf einmal ein Upgrade durchführen, nicht nur für einen Daemon auf einmal.
     </para>
    </listitem>
    <listitem>
     <para>
      Das Upgrade der wichtigsten Services (MON, MGR, OSD) erfolgt nacheinander. Jeder Service ist während des Upgrades verfügbar. Die Gateway-Services (Metadata Server, Object Gateway, NFS Ganesha, iSCSI Gateway) müssen nach dem Upgrade der wichtigsten Services neu bereitgestellt werden. Für jeden der folgenden Services ist eine gewisse Ausfallzeit zu berücksichtigen:
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         Metadata Server und Object Gateways sind ab dem Zeitpunkt, an dem für die Knoten von SUSE Linux Enterprise Server 15 SP1 ein Upgrade auf SUSE Linux Enterprise Server 15 SP2 durchgeführt wurde, bis zur erneuten Bereitstellung der Services am Ende des Upgrade-Vorgangs außer Betrieb. Dies ist insbesondere dann zu beachten, wenn diese Services zusammen mit MONs, MGRs oder OSDs untergebracht sind, da sie in diesem Fall für die gesamte Dauer des Cluster-Upgrades ausfallen können. Sollte dies ein Problem sein, ziehen Sie in Erwägung, diese Services vor dem Upgrade separat auf zusätzlichen Knoten bereitzustellen, damit sie so kurz wie möglich ausfallen. Hierbei handelt es sich um die Dauer des Upgrades der Gateway-Knoten, nicht die Dauer des Upgrades des gesamten Clusters.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha und iSCSI Gateways sind nur für die Dauer des Neustarts der Knoten während des Upgrades von SUSE Linux Enterprise Server 15 SP1 auf SUSE Linux Enterprise Server 15 SP2 außer Betrieb und erneut für kurze Zeit, wenn die einzelnen Services im containerisierten Modus neu bereitgestellt werden.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Sichern der Cluster-Konfiguration und -Daten</title>
   <para>
    Wir empfehlen dringend, vor Beginn des Upgrades auf SUSE Enterprise Storage 7 alle Cluster-Konfigurationen und -Daten zu sichern. Eine Anleitung zur Sicherung aller Daten finden Sie unter <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Überprüfen der Schritte des vorherigen Upgrades</title>
   <para>
    Falls Sie zuvor ein Upgrade von Version 5 durchgeführt haben, prüfen Sie, ob das Upgrade auf Version 6 erfolgreich abgeschlossen wurde:
   </para>
   <para>
    Prüfen Sie, ob die Datei <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename> vorhanden ist.
   </para>
   <para>
    Diese Datei wird durch den Importvorgang während des Upgrades von SUSE Enterprise Storage 5 auf 6 erstellt. Die Option <option>configuration_init: default-import</option> wird in <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> festgelegt.
   </para>
   <para>
    Wenn <option>configuration_init</option> noch auf <option>default-import</option> festgelegt ist, greift der Cluster auf <filename>ceph.conf.import</filename> als Konfigurationsdatei zurück, nicht auf die Standarddatei <filename>ceph.conf</filename> von DeepSea, die aus Dateien in <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename> kompiliert wird.
   </para>
   <para>
    Sie müssen daher <filename>ceph.conf.import</filename> auf benutzerdefinierte Konfigurationen prüfen und diese Konfigurationen ggf. in eine der Dateien in <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename> verschieben.
   </para>
   <para>
    Entfernen Sie dann die Zeile <option>configuration_init: default-import</option> aus <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Aktualisieren von Cluster-Knoten und Überprüfen des Zustands des Clusters</title>
   <para>
    Überprüfen Sie, ob alle aktuellen Aktualisierungen von SUSE Linux Enterprise Server 15 SP1 und SUSE Enterprise Storage 6 auf alle Cluster-Knoten angewendet werden:
   </para>
<screen><prompt role="root">root # </prompt>zypper refresh &amp;&amp; zypper patch</screen>
   <para>
    Überprüfen Sie nach der Anwendung der Aktualisierungen den Zustand des Clusters:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Überprüfen des Zugriffs auf Software-Repositorys und Container-Images</title>
   <para>
    Stellen Sie sicher, dass alle Cluster-Knoten Zugriff auf die Software-Repositorys von SUSE Linux Enterprise Server 15 SP2 und SUSE Enterprise Storage 7 sowie auf die Registrierung von Container-Images haben.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Software-Repositorys</title>
    <para>
     Wenn alle Knoten bei SCC registriert sind, können Sie das Kommando <command>zypper migration</command> für das Upgrade verwenden. Weitere Informationen finden Sie in <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/>.
    </para>
    <para>
     Wenn Knoten <emphasis role="bold">nicht</emphasis> bei SCC registriert sind, deaktivieren Sie alle vorhandenen Software-Repositorys und fügen Sie sowohl das <literal>Pool</literal>- als auch das <literal>Updates</literal>-Repository für jede der folgenden Erweiterungen hinzu:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Container-Images</title>
    <para>
     Alle Cluster-Knoten benötigen Zugriff auf die Container-Image-Registrierung. In den meisten Fällen verwenden Sie die öffentliche SUSE-Registrierung unter <literal>registry.suse.com</literal>. Dafür benötigen Sie die folgenden Images:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Alternativ (z. B. für Air-Gap-Bereitstellungen) können Sie eine lokale Registrierung konfigurieren und überprüfen, ob Sie den richtigen Satz von Container-Images zur Verfügung haben. Weitere Details zum Konfigurieren einer lokalen Container-Image-Registrierung finden Sie in <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Upgrade des Salt Masters</title>

  <para>
   Das folgende Verfahren beschreibt den Prozess des Upgrades des Salt Masters:
  </para>

  <procedure>
   <step>
    <para>
     Führen Sie ein Upgrade des zugrunde liegenden Betriebssystems auf SUSE Linux Enterprise Server 15 SP2 durch:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Führen Sie für Cluster, deren Knoten bei SCC registriert sind, <command>zypper migration</command> aus.
      </para>
     </listitem>
     <listitem>
      <para>
       Führen Sie für Cluster, deren Knoten Software-Repositorys manuell zugewiesen wurden, <command>zypper dup</command> und dann <command>reboot</command> aus.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Deaktivieren Sie die DeepSea-Phasen, um eine versehentliche Verwendung zu vermeiden. Fügen Sie folgenden Inhalt zu <filename>/srv/pillar/ceph/stack/global.yml</filename> hinzu:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Speichern Sie die Datei und übernehmen Sie die Änderungen:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     Wenn Sie <emphasis role="bold">keine</emphasis> Container-Images von <literal>registry.suse.com</literal> verwenden, sondern die lokal konfigurierte Registrierung, bearbeiten Sie <filename>/srv/pillar/ceph/stack/global.yml</filename>, um DeepSea mitzuteilen, welches Ceph-Container-Image und welche Registrierung verwendet werden sollen. Beispiel: Fügen Sie die folgenden Zeilen ein, um <literal>192.168.121.1:5000/my/ceph/image</literal> zu verwenden:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Speichern Sie die Datei und übernehmen Sie die Änderungen:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Übernehmen Sie die bestehende Konfiguration:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Prüfen Sie den Upgrade-Status. Je nach Cluster-Konfiguration kann Ihre Ausgabe abweichen:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Upgrade der MON-, MGR- und OSD-Knoten</title>

  <para>
   Führen Sie die Upgrades für Ceph-Monitor-, Ceph-Manager- und OSD-Knoten nacheinander durch. Führen Sie für jeden Service die folgenden Schritte aus:
  </para>

  <procedure>
   <step>
    <para>
     Wenn es sich bei dem Knoten, für den Sie ein Upgrade durchführen, um einen OSD-Knoten handelt, verhindern Sie, dass der OSD-Knoten während des Upgrade-Vorgangs als <literal>out</literal> markiert wird. Führen Sie hierzu folgendes Kommando aus:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Ersetzen Sie <replaceable>SHORT_NODE_NAME</replaceable> durch den Kurznamen des Knotens, wie er in der Ausgabe des Kommandos <command>ceph osd tree</command> erscheint. In der folgenden Eingabe lauten die Kurznamen des Hosts <literal>ses-min1</literal> und <literal>ses-min2</literal>.
    </para>
<screen>
<prompt>root@master # </prompt>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Führen Sie ein Upgrade des zugrunde liegenden Betriebssystems auf SUSE Linux Enterprise Server 15 SP2 durch:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Wenn alle Knoten des Clusters bei SCC registriert sind, führen Sie <command>zypper migration</command> aus.
      </para>
     </listitem>
     <listitem>
      <para>
       Wenn den Knoten des Clusters Software-Repositorys manuell zugewiesen wurden, führen Sie <command>zypper dup</command> und dann <command>reboot</command> aus.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Containerisieren Sie nach dem Neustart des Knotens alle vorhandenen MON-, MGR- und OSD-Daemons auf diesem Knoten. Führen Sie hierzu folgendes Kommando auf dem Salt Master aus:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Ersetzen Sie <replaceable>MINION_ID</replaceable> durch die ID des Minion, den Sie aufrüsten möchten. Die Liste der Minion-IDs erhalten Sie durch Ausführen des Kommandos <command>salt-key -L</command> auf dem Salt Master.
    </para>
    <tip>
     <para>
      Den Status und den Fortschritt der <emphasis>Übernahme</emphasis> finden Sie im Ceph-Dashboard oder durch Ausführen eines der folgenden Kommandos auf dem Salt Master:
     </para>
<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     Entfernen Sie nach der erfolgreichen Übernahme das Flag <literal>noout</literal>, wenn der Knoten, den Sie aufrüsten, ein OSD-Knoten ist:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Upgrade von Gateway-Knoten</title>
  <para>
   Führen Sie als Nächstes ein Upgrade der separaten Gateway-Knoten (Metadata Server, Object Gateway, NFS Ganesha oder iSCSI Gateway) durch. Führen Sie für jeden Knoten ein Upgrade des zugrunde liegenden Betriebssystems auf SUSE Linux Enterprise Server 15 SP2 auf:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Wenn alle Knoten des Clusters beim SUSE Customer Center registriert sind, führen Sie das Kommando <command>zypper migration</command> aus.
    </para>
   </listitem>
   <listitem>
    <para>
     Wenn den Knoten des Clusters Software-Repositorys manuell zugewiesen wurden, führen Sie das Kommando <command>zypper dup</command> und dann das Kommando <command>reboot</command> aus.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Dieser Schritt gilt auch für alle Knoten, die Teil des Clusters sind, denen aber noch keine Rollen zugewiesen wurden (überprüfen Sie im Zweifelsfall die Liste der Hosts auf dem Salt Master, die durch das Kommando <command>salt-key -L</command> erstellt wird, und vergleichen Sie sie mit der Ausgabe des Kommandos <command>salt-run upgrade.status</command>).
  </para>
  <para>
   Sobald für das Betriebssystem auf allen Knoten im Cluster ein Upgrade durchgeführt wurde, wird im nächsten Schritt das Paket <package>ceph-salt</package> installiert und die Cluster-Konfiguration angewendet. Die eigentlichen Gateway-Services werden am Ende des Upgrade-Vorgangs in einem containerisierten Modus neu bereitgestellt.
  </para>
  <note>
   <para>
    Metadata-Server- und Object-Gateway-Services sind ab dem Zeitpunkt des Upgrades auf SUSE Linux Enterprise Server 15 SP2 nicht mehr verfügbar. Sie müssen am Ende des Upgrade-Vorgangs erneut bereitgestellt werden.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Installieren von <systemitem class="resource">ceph-salt</systemitem> und Anwenden der Cluster-Konfiguration</title>

  <para>
   Bevor Sie damit beginnen, <systemitem class="resource">ceph-salt</systemitem> zu installieren und die Cluster-Konfiguration anzuwenden, müssen Sie den Cluster- und Upgrade-Status überprüfen. Führen Sie dazu folgende Kommandos aus:
  </para>

<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Entfernen Sie die von DeepSea erstellten Crons <literal>rbd_exporter</literal> und <literal>rgw_exporter</literal>. Führen Sie auf dem Salt Master als <systemitem class="username">root</systemitem> das Kommando <command>crontab -e</command> aus, um crontab zu bearbeiten. Löschen Sie die folgenden Elemente, falls vorhanden:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null
</screen>
   </step>
   <step>
    <para>
     Führen Sie zum Exportieren der Cluster-Konfiguration aus DeepSea folgende Kommandos aus:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<prompt>root@master # </prompt>salt-run upgrade.generate_service_specs &gt; specs.yaml
</screen>
   </step>
   <step>
    <para>
     Deinstallieren Sie DeepSea und installieren Sie <systemitem class="resource">ceph-salt</systemitem> auf dem Salt Master:
    </para>
<screen>
<prompt>root@master # </prompt>zypper remove 'deepsea*'
<prompt>root@master # </prompt>zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Starten Sie den Salt Master neu und synchronisieren Sie die Salt-Module:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Importieren Sie die Cluster-Konfiguration von DeepSea in <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Generieren Sie SSH-Schlüssel für die Cluster-Knoten-Kommunikation:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Überprüfen Sie, ob die Cluster-Konfiguration aus DeepSea importiert wurde, und geben Sie eventuell fehlende Optionen an:
     </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
     <para>
      Eine vollständige Beschreibung der Cluster-Konfiguration finden Sie in <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Wenden Sie die Konfiguration an und aktivieren Sie cephadm:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </step>
   <step>
    <para>
     Wenn Sie die URL und die Zugangsdaten für die lokale Container-Registrierung angeben müssen, führen Sie die in <xref linkend="deploy-cephadm-configure-registry"/> beschriebenen Schritte aus.
    </para>
   </step>
   <step>
    <para>
     Wenn Sie <emphasis role="bold">keine</emphasis> Container-Images von <literal>registry.suse.com</literal> verwenden, sondern die lokal konfigurierte Registrierung, teilen Sie Ceph mit, welches Container-Image verwendet werden soll. Führen Sie dazu folgendes Kommando aus:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Beispiel:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Stoppen und deaktivieren Sie die <systemitem class="daemon">ceph-crash</systemitem>-Daemons von SUSE Enterprise Storage 6. Neue containerisierte Formen dieser Daemons werden später automatisch gestartet.
    </para>
<screen>
<prompt>root@master # </prompt>salt '*' service.stop ceph-crash
<prompt>root@master # </prompt>salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Upgrade und Übernahme des Überwachungs-Stacks</title>

  <para>
   Durch folgendes Verfahren werden alle Komponenten des Überwachungs-Stacks übernommen (weitere Details finden Sie im <xref linkend="monitoring-alerting"/>).
  </para>

  <procedure>
   <step>
    <para>
     Halten Sie den Orchestrator an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch pause</screen>
   </step>
   <step>
    <para>
     Führen Sie auf dem Knoten, auf dem Prometheus, Grafana und Alertmanager ausgeführt werden (standardmäßig der Salt Master), die folgenden Kommandos aus:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      Wenn Sie <emphasis role="bold">nicht</emphasis> das Standard-Container-Image <literal>registry.suse.com</literal> ausführen, müssen Sie das zu verwendende Image angeben, zum Beispiel:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-server:2.18.0 \
  adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-alertmanager:0.16.2 \
  adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.0.3 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      Weitere Details zur Verwendung benutzerdefinierter oder lokaler Container-Images finden Sie im <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Entfernen Sie den Node Exporter. Er muss nicht migriert werden und wird als Container neu installiert, wenn die Datei <filename>specs.yaml</filename> angewendet wird.
    </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> zypper rm golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Wenden Sie die zuvor aus DeepSea exportierten Servicespezifikationen an:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i specs.yaml</screen>
   </step>
   <step>
    <para>
     Setzen Sie den Orchestrator fort:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Erneute Bereitstellung des Gateway-Service</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Upgrade des Object Gateways</title>
   <para>
    In SUSE Enterprise Storage 7 werden die Object Gateways immer mit einem Bereich konfiguriert, was in Zukunft mehrere Standorte ermöglicht (weitere Details finden Sie im <xref linkend="ceph-rgw-fed"/>). Wenn Sie in SUSE Enterprise Storage 6 eine Object-Gateway-Konfiguration für einen einzigen Standort verwendet haben, gehen Sie folgendermaßen vor, um einen Bereich hinzuzufügen. Wenn Sie nicht vorhaben, die Funktionalität für mehrere Standorte tatsächlich zu nutzen, ist es in Ordnung, die <literal>Standardwerte</literal> für die Namen des Bereichs, der Zonengruppe und der Zone zu verwenden.
   </para>
   <procedure>
    <step>
     <para>
      Erstellen Sie einen neuen Bereich:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Optional können Sie die standardmäßige Zone und Zonengruppe umbenennen.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Konfigurieren Sie die Master-Zonengruppe:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Konfigurieren Sie die Master-Zone. Dazu benötigen Sie den ACCESS_KEY und SECRET_KEY eines Object-Gateway-Benutzers, bei dem das Flag <option>system</option> aktiviert ist. Hierbei handelt es sich normalerweise um den <literal>admin</literal>-Benutzer. Führen Sie zum Abrufen des ACCESS_KEY und SECRET_KEY das Kommando <command>radosgw-admin user info --uid admin</command> aus.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Bestätigen Sie die aktualisierte Konfiguration:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    Um den Object-Gateway-Service zu containerisieren, erstellen Sie dessen Spezifikationsdatei wie in <xref linkend="deploy-cephadm-day2-service-ogw"/> beschrieben, und wenden Sie sie an.
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Upgrade von NFS Ganesha</title>
   <para>
    Im Folgenden wird demonstriert, wie ein bestehender NFS-Ganesha-Service unter Ceph Nautilus in einen NFS-Ganesha-Container unter Ceph Octopus migriert wird.
   </para>
   <warning>
    <para>
     Die folgende Dokumentation setzt voraus, dass Sie für die wichtigsten Ceph-Services bereits erfolgreich ein Upgrade durchgeführt haben.
    </para>
   </warning>
   <para>
    NFS Ganesha speichert zusätzlich die Konfiguration pro Daemon und exportiert die Konfiguration in einen RADOS-Pool. Den konfigurierten RADOS-Pool finden Sie in der Datei <filename>ganesha.conf</filename> im Block <literal>RADOS_URLS</literal> in der Zeile <literal>watch_url</literal>. Standardmäßig wird dieser Pool <literal>ganesha_config</literal> genannt.
   </para>
   <para>
    Bevor Sie eine Migration durchführen, empfehlen wir dringend, eine Kopie der Export- und der Daemon-Konfigurationsobjekte zu erstellen, die sich im RADOS-Pool befinden. Führen Sie das folgende Kommando aus, um den konfigurierten RADOS-Pool zu finden:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    So listen Sie den Inhalt des RADOS-Pools auf:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    So kopieren Sie die RADOS-Objekte:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>OBJS=$(rados $RADOS_ARGS ls)
<prompt>cephuser@adm &gt; </prompt>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    Pro Knoten muss ein bestehender NFS-Ganesha-Service gestoppt und anschließend durch einen von cephadm verwalteten Container ersetzt werden.
   </para>
   <procedure>
    <step>
     <para>
      Stoppen und deaktivieren Sie den vorhandenen NFS-Ganesha-Service:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>systemctl stop nfs-ganesha
<prompt>cephuser@adm &gt; </prompt>systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      Nachdem der bestehende NFS-Ganesha-Service gestoppt wurde, kann mit cephadm ein neuer Service in einem Container eingerichtet werden. Dazu müssen Sie eine Service-Spezifikation erstellen, die eine <literal>service_id</literal> enthält, die zur Identifizierung dieses neuen NFS-Clusters verwendet wird, den Hostnamen des Knotens, den wir migrieren und der in der Platzierungsspezifikation als Host aufgeführt ist, sowie den RADOS-Pool und -Namespace, der die konfigurierten NFS-Exportobjekte enthält. Beispiel:
     </para>
     <screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      Weitere Informationen zum Erstellen einer Platzierungsspezifikation finden Sie in <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Wenden Sie die Platzierungsspezifikation an:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Bestätigen Sie, dass der NFS-Ganesha-Daemon auf dem Host ausgeführt wird:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Wiederholen Sie diese Schritte für jeden NFS-Ganesha-Knoten. Sie müssen nicht für jeden Knoten eine eigene Servicespezifikation erstellen. Es reicht aus, den Hostnamen jedes Knotens zur bestehenden NFS-Servicespezifikation hinzuzufügen und sie erneut anzuwenden.
     </para>
    </step>
   </procedure>
   <para>
    Die vorhandenen Exporte können auf zwei verschiedene Arten migriert werden:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      manuell neu erstellt oder neu zugewiesen über das Ceph Dashboard.
     </para>
    </listitem>
    <listitem>
     <para>
      Kopieren Sie den Inhalt jedes RADOS-Objekts pro Daemon manuell in die neu erstellte gemeinsame NFS-Ganesha-Konfiguration.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Manuelles Kopieren von Exporten in die gemeinsame Konfigurationsdatei von NFS Ganesha</title>
    <step>
     <para>
      Ermitteln Sie die Liste der RADOS-Objekte pro Daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine Kopie der RADOS-Objekte pro Daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Sortieren Sie sie und führen Sie sie in einer einzigen Liste von Exporten zusammen:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat conf-* | sort -u &gt; conf-nfs.<replaceable>SERVICE_ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Schreiben Sie die neue gemeinsame Konfigurationsdatei für NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Benachrichtigen Sie den NFS-Ganesha-Daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       Diese Aktion veranlasst den Daemon, die Konfiguration neu zu laden.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    Nach der erfolgreichen Migration des Service kann der Nautilus-basierte NFS-Ganesha-Service entfernt werden.
   </para>
   <procedure>
    <step>
     <para>
      Entfernen Sie NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Entfernen Sie die veralteten Cluster-Einstellungen aus dem Ceph Dashboard:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Upgrade des Metadata Servers</title>
   <para>
    Im Gegensatz zu MONs, MGRs und OSDs kann der Metadata Server nicht an Ort und Stelle übernommen werden. Stattdessen müssen Sie sie mit dem Ceph-Orchestrator neu in Containern bereitstellen.
   </para>
   <procedure>
    <step>
     <para>
      Führen Sie das Kommando <command>ceph fs ls</command> aus, um beispielsweise den Namen Ihres Dateisystems zu erhalten:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine neue Servicespezifikationsdatei <filename>mds.yml</filename> wie in <xref linkend="deploy-cephadm-day2-service-mds"/> beschrieben. Verwenden Sie dazu den Dateisystemnamen als <option>service_id</option> und geben Sie die Hosts an, auf denen die MDS-Daemons ausgeführt werden. Beispiel:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Führen Sie das Kommando <command>ceph orch apply -i mds.yml</command> aus, um die Servicespezifikation anzuwenden und die MDS-Daemons zu starten.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Upgrade des iSCSI Gateways</title>
   <para>
    Für ein Upgrade des iSCSI Gateways müssen Sie es mithilfe des Ceph-Orchestrators neu in Containern bereitstellen. Wenn Sie über mehrere iSCSI Gateways verfügen, müssen Sie diese nacheinander neu bereitstellen, um die Ausfallzeit des Service zu reduzieren.
   </para>
   <procedure>
    <step>
     <para>
      Stoppen und deaktivieren Sie die vorhandenen iSCSI-Daemons auf jedem iSCSI Gateway-Knoten:
     </para>
<screen>
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-api
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Erstellen Sie eine Servicespezifikation für das iSCSI Gateway, wie in <xref linkend="deploy-cephadm-day2-service-igw"/> beschrieben. Dazu benötigen Sie die Einstellungen <option>pool</option>, <option>trusted_ip_list</option> und <option>api_*</option> aus der bestehenden Datei <filename>/etc/ceph/iscsi-gateway.cfg</filename>. Wenn Sie SSL-Unterstützung aktiviert haben (<literal>api_secure = true</literal>), benötigen Sie auch das SSL-Zertifikat (<filename>/etc/ceph/iscsi-gateway.crt</filename>) und den Schlüssel (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      Beispiel: Wenn <filename>/etc/ceph/iscsi-gateway.cfg</filename> Folgendes enthält:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Dann müssen Sie die folgende Servicespezifikationsdatei <filename>iscsi.yml</filename> erstellen:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       Die Einstellungen <option>pool</option>, <option>trusted_ip_list</option>, <option>api_port</option>, <option>api_user</option>, <option>api_password</option>, <option>api_secure</option> sind identisch mit den Einstellungen aus Datei <filename>/etc/ceph/iscsi-gateway.cfg</filename>. Die Werte <option>ssl_cert</option> und <option>ssl_key</option> können aus dem bestehenden SSL-Zertifikat und den Schlüsseldateien kopiert und eingefügt werden. Vergewissern Sie sich, dass sie richtig eingerückt sind und das <emphasis>pipe</emphasis> Zeichen <literal>|</literal> am Ende der Zeilen <literal>ssl_cert:</literal> und <literal>ssl_key:</literal>  erscheint (den Inhalt der Datei <filename>iscsi.yml</filename> finden Sie oben).
      </para>
     </note>
    </step>
    <step>
     <para>
      Führen Sie das Kommando <command>ceph orch apply -i iscsi.yml</command> aus, um die Servicespezifikation anzuwenden und die iSCSI Gateway-Daemons zu starten.
     </para>
    </step>
    <step>
     <para>
      Entfernen Sie das bisherige <package>ceph-iscsi</package> -Paket aus allen bestehenden iSCSI Gateway-Knoten:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Bereinigung nach dem Upgrade</title>

  <para>
   Führen Sie nach dem Upgrade die folgenden Schritte zur Bereinigung aus:
  </para>

  <procedure>
   <step>
    <para>
     Überprüfen Sie die aktuelle Ceph-Version, um sich zu vergewissern, dass für den Cluster erfolgreich ein Upgrade durchgeführt wurde:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph versions</screen>
   </step>
   <step>
    <para>
     Stellen Sie sicher, dass keine alten OSDs im Cluster aufgenommen werden:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd require-osd-release octopus</screen>
   </step>
   <step>
    <para>
     Aktivieren Sie das Autoscaler-Modul:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable pg_autoscaler</screen>
    <important>
     <para>
      Bei Pools in SUSE Enterprise Storage 6 war der Modus <option>pg_autoscale_mode</option> standardmäßig auf <option>warn</option> festgelegt. Dies führte zwar zu einer Warnmeldung bei einer suboptimalen Anzahl von PGs, doch eine automatische Skalierung fand nicht statt. In der Standardeinstellung von SUSE Enterprise Storage 7 ist die Option <option>pg_autoscale_mode</option> für neue Pools auf <option>on</option> festgelegt, und PGs werden tatsächlich automatisch skaliert. Bei dem Upgrade wird der <option>pg_autoscale_mode</option> von bestehenden Pools nicht automatisch geändert. Wenn Sie die Einstellung zu <option>on</option> ändern möchten, um die Vorteile des Autoscalers voll auszuschöpfen, lesen Sie die Anweisungen im <xref linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Weitere Informationen finden Sie in <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Verhindern Sie Clients aus Versionen vor Luminous:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Aktivieren Sie das Ausgleichsprogramm-Modul:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph balancer mode upmap
<prompt>cephuser@adm &gt; </prompt>ceph balancer on
</screen>
    <para>
     Weitere Informationen finden Sie im <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     Aktivieren Sie optional das Telemetriemodul:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph mgr module enable telemetry
<prompt>cephuser@adm &gt; </prompt>ceph telemetry on
 </screen>
    <para>
     Weitere Informationen finden Sie im <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
