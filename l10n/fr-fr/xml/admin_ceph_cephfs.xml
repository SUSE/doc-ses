<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Système de fichiers en grappe</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>oui</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ce chapitre décrit les tâches d'administration normalement effectuées après la configuration de la grappe et l'exportation de CephFS. Pour plus d'informations sur la configuration de CephFS, reportez-vous au <xref linkend="deploy-cephadm-day2-service-mds"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montage de CephFS</title>

  <para>
   Lorsque le système de fichiers est créé et que MDS est actif, vous êtes prêt à monter le système de fichiers à partir d'un hôte client.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Préparation du client</title>
   <para>
    Si l'hôte client exécute SUSE Linux Enterprise 12 SP2 ou une version ultérieure, le système est prêt à monter CephFS dans sa version « prête à l'emploi ».
   </para>
   <para>
    Si l'hôte client exécute SUSE Linux Enterprise 12 SP1, vous devez appliquer tous les correctifs les plus récents avant de monter CephFS.
   </para>
   <para>
    Dans tous les cas, SUSE Linux Enterprise inclut tout ce qui est nécessaire au montage de CephFS. Le produit SUSE Enterprise Storage 7 n'est pas nécessaire.
   </para>
   <para>
    Pour prendre en charge la syntaxe complète de <command>mount</command>, il est nécessaire d'installer le paquetage
    <package>ceph-common</package> (fourni avec SUSE Linux Enterprise) pour pouvoir monter CephFS.
   </para>
   <important>
    <para>
     Sans le paquetage <package>ceph-common</package> (et donc sans le programme auxiliaire <command>mount.ceph</command>), les adresses IP des moniteurs doivent être utilisées plutôt que leurs noms. Cela est dû au fait que le client de kernel ne pourra pas effectuer la résolution de nom.
    </para>
    <para>
     La syntaxe de montage de base est la suivante :
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Création d'un fichier de secret</title>
   <para>
    L'authentification est activée par défaut pour la grappe Ceph active. Vous devez créer un fichier qui stocke votre clé secrète (et non pas le trousseau de clés lui-même). Pour obtenir la clé secrète d'un utilisateur particulier et créer ensuite le fichier, procédez comme suit :
   </para>
   <procedure>
    <title>Création d'une clé secrète</title>
    <step>
     <para>
      Affichez la clé d'un utilisateur particulier dans un fichier de trousseau de clés :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copiez la clé de l'utilisateur qui emploiera le système de fichiers Ceph FS monté. La clé ressemble généralement à ceci :
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Créez un fichier en indiquant le nom de l'utilisateur dans le nom de fichier, par exemple <filename>/etc/ceph/admin.secret</filename> pour l'utilisateur <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Collez la valeur de la clé dans le fichier créé à l'étape précédente.
     </para>
    </step>
    <step>
     <para>
      Définissez les droits d'accès appropriés au fichier. L'utilisateur doit être le seul à pouvoir lire le fichier, les autres n'ont aucun droit d'accès. 
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montage de CephFS</title>
   <para>
    La commande <command>mount</command> permet de monter CephFS. Vous devez indiquer le nom d'hôte ou l'adresse IP du moniteur. Comme l'authentification <systemitem>cephx</systemitem> est activée par défaut dans SUSE Enterprise Storage, vous devez également spécifier un nom d'utilisateur et le secret qui lui est associé :
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Étant donné que la commande précédente reste dans l'historique du shell, une approche plus sécurisée consiste à lire le secret d'un fichier :
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Notez que le fichier de secret ne doit contenir que le secret du trousseau de clés. Dans notre exemple, le fichier contient uniquement la ligne suivante :
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>spécification de plusieurs moniteurs</title>
    <para>
     Il est judicieux d'indiquer plusieurs moniteurs séparés par des virgules sur la ligne de commande <command>mount</command> dans le cas où un moniteur est arrêté au moment du montage. Chaque adresse de moniteur figure sous la forme <literal>hôte[:port]</literal>. Si le port n'est pas indiqué, le port par défaut est le port 6789.
    </para>
   </tip>
   <para>
    Créez le point de montage sur l'hôte local :
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Montez le système de fichiers CephFS :
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Un sous-répertoire <filename>subdir</filename> peut être indiqué si un sous-ensemble du système de fichiers doit être monté :
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Vous pouvez spécifier plusieurs hôtes de moniteur dans la commande <command>mount</command> :
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>accès en lecture au répertoire racine</title>
    <para>
     Si des clients avec restriction de chemin d'accès sont utilisés, les fonctionnalités MDS doivent inclure un accès en lecture au répertoire racine. Par exemple, un trousseau de clés peut ressembler à ceci :
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     La partie <literal>allow r path=/</literal> signifie que les clients dont le chemin est restreint peuvent voir le volume racine sans être autorisés à y écrire des données. Cela peut être un problème dans les cas où une isolation complète est requise.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Démontage de CephFS</title>

  <para>
   Pour démonter le système de fichiers CephFS, utilisez la commande <command>umount</command> :
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>Montage de CephFS dans <filename>/etc/fstab</filename></title>

  <para>
   Pour monter automatiquement CephFS au démarrage du client, insérez la ligne correspondante dans sa table des systèmes de fichiers <filename>/etc/fstab</filename> :
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Daemons MDS actifs multiples (MDS actif-actif)</title>

  <para>
   Par défaut, CephFS est configuré pour un seul daemon MDS actif. Pour mettre à l'échelle les performances des métadonnées pour les systèmes vastes, vous pouvez activer plusieurs daemons MDS actifs, qui partageront entre eux la charge de travail des métadonnées.
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>Utilisation de MDS actif-actif</title>
   <para>
    Vous pouvez envisager d'utiliser plusieurs daemons MDS actifs lorsque les performances de vos métadonnées sont bloquées sur le MDS unique par défaut.
   </para>
   <para>
    L'ajout de plusieurs daemons n'améliore pas les performances pour tous les types de charge de travail. Par exemple, une seule application s'exécutant sur un seul client ne bénéficie pas d'un nombre accru de daemons MDS à moins d'effectuer de nombreuses opérations de métadonnées en parallèle.
   </para>
   <para>
    Les charges de travail qui bénéficient généralement d'un nombre supérieur de daemons MDS actifs sont celles qui possèdent de nombreux clients, travaillant, le cas échéant, sur plusieurs répertoires distincts.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Augmentation de la taille de la grappe active MDS</title>
   <para>
    Chaque système de fichiers CephFS possède un paramètre <option>max_mds</option> qui détermine le nombre de rangs à créer. Le nombre réel de rangs dans le système de fichiers n'augmentera que si un daemon supplémentaire est en mesure de prendre le nouveau rang créé. Par exemple, si un seul daemon MDS s'exécute et que la valeur <option>max_mds</option> est définie sur 2, aucun second rang n'est créé.
   </para>
   <para>
    Dans l'exemple suivant, nous définissons <option>max_mds</option> sur 2 pour créer un rang en dehors du rang par défaut. Pour voir les changements, lancez <command>ceph status</command> avant et après avoir défini <option>max_mds</option> et reportez-vous à la ligne contenant <literal>fsmap</literal> :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    Le rang nouvellement créé (1) passe à l'état « creating » (création en cours), puis à l'état« active » (actif).
   </para>
   <important>
    <title>daemons de secours</title>
    <para>
     Même avec plusieurs daemons MDS actifs, un système hautement disponible nécessite toujours des daemons de secours qui prennent le relais si l'un des serveurs exécutant un daemon actif tombe en panne.
    </para>
    <para>
     Par conséquent, la valeur maximale pratique de <option>max_mds</option> pour les systèmes hautement disponibles est égale au nombre total de serveurs MDS de votre système moins 1. Pour rester disponible en cas de défaillance de plusieurs serveurs, augmentez le nombre de daemons de secours du système pour qu'ils correspondent au nombre de pannes de serveur que vous devez pouvoir surmonter. 
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Diminution du nombre de rangs</title>
   <para>
    Tous les rangs, y compris les rangs à supprimer, doivent d'abord être actifs. Cela signifie que vous devez disposer d'au moins <option>max_mds</option> daemons MDS disponibles.
   </para>
   <para>
    Commencez par définir <option>max_mds</option> sur un nombre inférieur. Par exemple, définissez un seul MDS actif :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Épinglage manuel d'arborescences de répertoires à un rang</title>
   <para>
    Dans plusieurs configurations de serveur de métadonnées actives, l'équilibreur permet de répartir la charge de métadonnées uniformément dans la grappe. Cela fonctionne généralement assez bien pour la plupart des utilisateurs, mais il est parfois souhaitable de remplacer l'équilibreur dynamique par des assignations explicites de métadonnées à des rangs particuliers. L'administrateur ou les utilisateurs peuvent ainsi répartir uniformément la charge d'applications ou limiter l'impact des demandes de métadonnées des utilisateurs sur l'ensemble de la grappe.
   </para>
   <para>
    Le mécanisme prévu à cet effet s'appelle une « épingle d'exportation ». Il s'agit d'un attribut étendu des répertoires. Cet attribut étendu s'appelle <literal>ceph.dir.pin</literal>. Les utilisateurs peuvent définir cet attribut à l'aide de commandes standard :
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    La valeur (<option>-v</option>) de l'attribut étendu correspond au rang à attribuer à la sous-arborescence de répertoires. La valeur par défaut (-1) indique que le répertoire n'est pas épinglé.
   </para>
   <para>
    Une épingle d'exportation de répertoire est héritée de son parent le plus proche ayant une épingle d'exportation définie. Par conséquent, la définition de l'épingle d'exportation sur un répertoire affecte tous ses enfants. Cependant, il est possible d'annuler l'épingle du parent en définissant l'épingle d'exportation du répertoire enfant. Par exemple :
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Gestion du basculement</title>

  <para>
   Si un daemon MDS cesse de communiquer avec le moniteur, le moniteur attend <option>mds_beacon_grace</option> secondes (par défaut, 15 secondes) avant de marquer le daemon comme <emphasis>laggy</emphasis> (lent à réagir). Vous pouvez configurer un ou plusieurs daemons de secours qui prendront le relais lors du basculement du daemon MDS.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configuration de daemons de secours avec relecture</title>
   <para>
    Chaque système de fichiers CephFS peut être configuré pour ajouter des daemons de secours avec relecture. Ces daemons de secours suivent le journal de métadonnées du MDS actif pour réduire le temps de basculement en cas d'indisponibilité du MDS actif. Chaque MDS actif ne peut avoir qu'un seul daemon de secours avec relecture.
   </para>
   <para>
    Configurez le daemon de secours avec relecture sur un système de fichiers à l'aide de la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    Une fois les daemons de secours avec relecture définis, les moniteurs les assigneront pour suivre les MDS actifs dans ce système de fichiers.
   </para>
   <para>
    Lorsqu'un MDS passe à l'état de daemon de secours avec relecture, il ne peut être utilisé que comme daemon de secours pour le rang auquel il est associé. En cas d'échec d'un autre rang, ce daemon de secours avec relecture ne peut pas jouer le rôle de remplaçant même si tous les autres daemons de secours sont indisponibles. Pour cette raison, il est conseillé d'utiliser un daemon de secours avec relecture pour chaque MDS actif.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Définition des quotas CephFS</title>

  <para>
   Vous pouvez définir des quotas sur n'importe quel sous-répertoire du système de fichiers Ceph. Le quota limite le nombre d'<emphasis role="bold">octets</emphasis> ou de <emphasis role="bold">fichiers</emphasis> stockés sous le point spécifié dans la hiérarchie de répertoires.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limites des quotas CephFS</title>
   <para>
    L'utilisation de quotas avec CephFS présente les limites suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>Les quotas sont coopératifs et non concurrentiels.</term>
     <listitem>
      <para>
       Les quotas Ceph s'appuient sur le client qui monte le système de fichiers pour arrêter d'écrire sur ce dernier lorsqu'une limite est atteinte. La partie serveur ne peut pas empêcher un client malveillant d'écrire autant de données qu'il en a besoin. N'utilisez pas de quotas pour empêcher la saturation du système de fichiers dans des environnements où les clients ne sont pas pleinement approuvés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Les quotas sont imprécis.</term>
     <listitem>
      <para>
       Les processus qui écrivent sur le système de fichiers seront arrêtés peu de temps après que la limite de quota aura été atteinte. Ils seront inévitablement autorisés à écrire une certaine quantité de données au-delà de la limite configurée. Les systèmes d'écriture clients seront arrêtés quelques dixièmes de seconde après avoir franchi la limite configurée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Les quotas sont implémentés dans le client du kernel à partir de la version 4.17.</term>
     <listitem>
      <para>
       Les quotas sont pris en charge par le client de l'espace utilisateur (libcephfs, ceph-fuse). Les clients de kernel Linux des versions 4.17 et ultérieures prennent en charge les quotas CephFS sur les grappes SUSE Enterprise Storage 7. Les clients de kernel (y compris les versions récentes) ne sont pas en mesure de gérer les quotas sur des grappes plus anciennes, même s'ils parviennent à définir les attributs étendus des quotas. Les kernels SLE12-SP3 (et versions ultérieures) incluent déjà les rétroports requis pour gérer les quotas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configurez les quotas avec prudence lorsqu'ils sont utilisés avec des restrictions de montage basées sur le chemin.</term>
     <listitem>
      <para>
       Le client doit avoir accès à l'inode de répertoire sur lequel les quotas sont configurés afin de les appliquer. Si le client dispose d'un accès restreint à un chemin spécifique (par exemple <filename>/home/user</filename>) sur la base de la fonction MDS et qu'un quota est configuré sur un répertoire ancêtre auquel il n'a pas accès (<filename>/home</filename>), le client n'appliquera pas ce quota. Lorsque vous utilisez des restrictions d'accès basées sur les chemins, veillez à configurer le quota sur le répertoire auquel le client peut accéder (par exemple <filename>/home/user</filename> ou <filename>/home/user/quota_dir</filename>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configuration des quotas CephFS</title>
   <para>
    Vous pouvez configurer les quotas CephFS à l'aide d'attributs étendus virtuels :
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configure une limite de <emphasis>fichiers</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configure une limite d'<emphasis>octets</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Si les attributs apparaissent sur un inode de répertoire, un quota y est configuré. S'ils ne sont pas présents, aucun quota n'est défini sur ce répertoire (mais il est encore possible d'en configurer un sur un répertoire parent).
   </para>
   <para>
    Pour définir un quota de 100 Mo, exécutez :
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Pour définir un quota de 10 000 fichiers, exécutez :
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Pour afficher la configuration de quota, exécutez :
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>quota non défini</title>
    <para>
     Si la valeur de l'attribut étendu est « 0 », le quota n'est pas défini.
    </para>
   </note>
   <para>
    Pour supprimer un quota, exécutez :
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Gestion des instantanés CephFS</title>

  <para>
   Les instantanés CephFS créent une vue en lecture seule du système de fichiers au moment où ils sont réalisés. Vous pouvez créer un instantané dans n'importe quel répertoire. L'instantané couvrira toutes les données du système de fichiers sous le répertoire spécifié. Après avoir créé un instantané, les données mises en mémoire tampon sont vidées de façon asynchrone à partir de divers clients. La création d'un instantané est dès lors très rapide.
  </para>

  <important>
   <title>systèmes de fichiers multiples</title>
   <para>
    Si vous avez plusieurs systèmes de fichiers CephFS partageant une réserve unique (via des espaces de noms), leurs instantanés entreront en collision, et la suppression d'un instantané entraînera des données de fichier manquantes pour d'autres instantanés partageant la même réserve.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Création d'instantanés</title>
   <para>
    La fonction d'instantané CephFS est activée par défaut sur les nouveaux systèmes de fichiers. Pour l'activer sur les systèmes de fichiers existants, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Une fois que vous activez des instantanés, tous les répertoires de CephFS auront un sous-répertoire spécial <filename>.snap</filename>.
   </para>
   <note>
    <para>
     Il s'agit d'un sous-répertoire <emphasis>virtuel</emphasis>. Il n'apparaît pas dans la liste des répertoires du répertoire parent, mais le nom <filename>.snap</filename> ne peut pas être utilisé comme nom de fichier ou de répertoire. Pour accéder au répertoire <filename>.snap</filename>, vous devez y accéder explicitement, par exemple :
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>limite des clients de kernel</title>
    <para>
     Les clients de kernel CephFS ont une limite : ils ne peuvent pas gérer plus de 400 instantanés dans un système de fichiers. Le nombre d'instantanés doit toujours être maintenu en dessous de cette limite, quel que soit le client que vous utilisez. Si vous utilisez des clients CephFS plus anciens, tels que SLE12-SP3, gardez à l'esprit que dépasser la limite de 400 instantanés est préjudiciable pour les opérations, car le client va se bloquer.
    </para>
   </important>
   <tip>
    <title>nom personnalisé pour le sous-répertoire d'instantanés</title>
    <para>
     Vous pouvez configurer un nom différent pour le sous-répertoire d'instantanés en définissant le paramètre <option>client snapdir</option>.
    </para>
   </tip>
   <para>
    Pour créer un instantané, créez un sous-répertoire sous le répertoire <filename>.snap</filename> avec un nom personnalisé. Par exemple, pour créer un instantané du répertoire <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, exécutez la commande suivante :
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Suppression d'instantanés</title>
   <para>
    Pour supprimer un instantané, supprimez son sous-répertoire au sein du répertoire <filename>.snap</filename> :
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
