<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>Périphérique de bloc RADOS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>oui</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Un bloc est une séquence d'octets, par exemple un bloc de 4 Mo de données. Les interfaces de stockage basées sur des blocs constituent le moyen le plus courant de stocker des données sur des supports rotatifs, tels que des disques durs, des CD ou des disquettes. L'omniprésence des interfaces de périphériques de bloc fait d'un périphérique de bloc virtuel un candidat idéal pour interagir avec un système de stockage de données de masse, tel que Ceph.
 </para>
 <para>
  Les périphériques de bloc Ceph permettent le partage de ressources physiques et sont redimensionnables. Ils stockent les données réparties sur plusieurs OSD dans une grappe Ceph. Les périphériques de bloc Ceph exploitent les fonctionnalités RADOS, telles que les instantanés, la réplication et la cohérence. Les périphériques de bloc RADOS (RADOS Block Devices, RBD) Ceph interagissent avec les OSD utilisant des modules de kernel ou la bibliothèque <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocole RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  Les périphériques de bloc Ceph offrent des performances exceptionnelles ainsi qu'une évolutivité infinie des modules de kernel. Ils prennent en charge des solutions de virtualisation, telles que QEMU, ou des systèmes basés sur le cloud, tels qu'OpenStack, qui reposent sur <systemitem class="library">libvirt</systemitem>. Vous pouvez utiliser la même grappe pour faire fonctionner Object Gateway, CephFS et les périphériques de bloc RADOS simultanément.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Commandes de périphériques de bloc</title>

  <para>
   La commande <command>rbd</command> permet de créer, de répertorier, d'explorer et de supprimer des images de périphérique de bloc. Vous pouvez également l'utiliser, par exemple, pour cloner des images, créer des instantanés, restaurer une image dans un instantané ou afficher un instantané.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Création d'une image de périphérique de bloc dans une réserve répliquée</title>
   <para>
    Avant de pouvoir ajouter un périphérique de bloc à un client, vous devez créer une image associée dans une réserve existante (voir <xref linkend="ceph-pools"/>) :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Par exemple, pour créer une image de 1 Go nommée « myimage » qui stocke des informations dans une réserve nommée « mypool », exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>unités de taille d'image</title>
    <para>
     Si vous n'indiquez pas de raccourci d'unité de taille (« G » ou « T »), la taille de l'image est en mégaoctets. Indiquez « G » ou « T » après le chiffre de la taille pour spécifier des gigaoctets ou des téraoctets.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Création d'une image de périphérique de bloc dans une réserve codée à effacement</title>
   <para>
    Il est possible de stocker les données d'une image de périphérique de bloc directement dans des réserves codées à effacement (Erasure Coded, EC). Une image de périphérique de bloc RADOS se compose de <emphasis>données</emphasis> et de <emphasis>métadonnées</emphasis>. Seules les données d'une image de périphérique de bloc peuvent être stockées dans une réserve EC. Le drapeau <option>overwrite</option> (écraser) de la réserve doit être défini sur <emphasis>true</emphasis> (vrai), ce qui est possible uniquement si tous les OSD sur lesquels la réserve est stockée utilisent BlueStore.
   </para>
   <para>
    La partie « métadonnées » de l'image ne peut pas être stockée dans une réserve EC. Vous pouvez spécifier la réserve répliquée pour stocker les métadonnées de l'image à l'aide de l'option <option>--pool=</option> de la commande <command>rbd create</command> ou spécifier <option>pool/</option> comme préfixe du nom de l'image.
   </para>
   <para>
    Créez une réserve EC :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Spécifiez la réserve répliquée dans laquelle stocker les métadonnées :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    Ou :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Liste des images de périphériques de bloc</title>
   <para>
    Pour lister les périphériques de bloc dans une réserve nommée « mypool », exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Récupération d'informations sur l'image</title>
   <para>
    Pour récupérer des informations à partir d'une image « myimage » dans une réserve nommée « mypool », exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Redimensionnement d'une image de périphérique de bloc</title>
   <para>
    Les images de périphérique de bloc RADOS sont provisionnées dynamiquement : en effet, elles n'utilisent aucun stockage physique tant que vous n'y avez pas enregistré des données. Cependant, elles possèdent une capacité maximale que vous définissez à l'aide de l'option <option>--size</option>. Si vous souhaitez augmenter (ou diminuer) la taille maximale de l'image, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Suppression d'une image de périphérique de bloc</title>
   <para>
    Pour supprimer un périphérique de bloc qui correspond à une image « myimage » dans une réserve nommée « mypool », exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montage et démontage</title>

  <para>
   Après avoir créé un périphérique de bloc RADOS, vous pouvez l'utiliser comme n'importe quel autre périphérique de disque et le formater, le monter pour pouvoir échanger des fichiers et le démonter une fois que vous avez terminé.
  </para>

  <para>
   La commande <command>rbd</command> accède par défaut à la grappe à l'aide du compte utilisateur Ceph <literal>admin</literal>. Ce compte dispose d'un accès administratif complet à la grappe. Il existe un risque de causer accidentellement des dommages, comme lorsque vous vous connectez à un poste de travail Linux en tant que <systemitem class="username">root</systemitem>. Par conséquent, il est préférable de créer des comptes utilisateur avec moins de privilèges et d'utiliser ces comptes pour un accès normal en lecture/écriture aux périphériques de bloc RADOS.
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Création d'un compte utilisateur Ceph</title>
   <para>
    Pour créer un nouveau compte utilisateur avec les fonctionnalités Ceph Manager, Ceph Monitor et Ceph OSD, utilisez la commande <command>ceph</command> avec la sous-commande <command>auth get-or-create</command> :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Par exemple, pour créer un utilisateur appelé <replaceable>qemu</replaceable> avec un accès en lecture-écriture aux <replaceable>vms</replaceable> de la réserve et un accès en lecture seule aux <replaceable>images</replaceable> de la réserve, exécutez la commande suivante :
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    La sortie de la commande <command>ceph auth get-or-create</command> sera le trousseau de clés de l'utilisateur spécifié, qui peut être écrit dans <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     Lorsque vous utilisez la commande <command>rbd</command>, vous pouvez spécifier l'ID utilisateur en fournissant l'argument facultatif <command>--id</command>
     <replaceable>ID</replaceable>.
    </para>
   </note>
   <para>
    Pour plus d'informations sur la gestion des comptes utilisateur Ceph, reportez-vous au <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Authentification des utilisateurs</title>
   <para>
    Pour indiquer un nom d'utilisateur, utilisez <option>--id <replaceable>nom-utilisateur</replaceable></option>. Si vous utilisez l'authentification <systemitem>cephx</systemitem>, vous devez également indiquer un secret. Il peut provenir d'un trousseau de clés ou d'un fichier contenant le secret :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    ou
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Préparation du périphérique de bloc RADOS à utiliser</title>
   <procedure>
    <step>
     <para>
      Assurez-vous que votre grappe Ceph inclut une réserve avec l'image disque que vous souhaitez assigner. Supposons que la réserve soit appelée <literal>mypool</literal> et l'image <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Assignez l'image à un nouveau périphérique de bloc:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Dressez la liste de tous les périphériques assignés :
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      Le périphérique sur lequel nous voulons travailler est <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>chemin du périphérique RBD</title>
      <para>
       Au lieu de <filename>/dev/rbd<replaceable>NUMÉRO_PÉRIPHÉRIQUE</replaceable></filename>, vous pouvez utiliser <filename>/dev/rbd/<replaceable>NOM_RÉSERVE</replaceable>/<replaceable>NOM_IMAGE</replaceable></filename> comme chemin de périphérique persistant. Par exemple :
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Créez un système de fichiers XFS sur le périphérique <filename>/dev/rbd0:</filename>
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      En remplaçant <filename>/mnt</filename> par votre point de montage, montez le périphérique et vérifiez qu'il est correctement monté :
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Vous pouvez maintenant déplacer des données vers et depuis le périphérique comme s'il s'agissait d'un répertoire local.
     </para>
     <tip>
      <title>augmentation de la taille du périphérique RBD</title>
      <para>
       Si vous trouvez que la taille du périphérique RBD n'est plus suffisante, vous pouvez facilement l'augmenter.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Augmentez la taille de l'image RBD, par exemple jusqu'à 10 Go.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Développez le système de fichiers à la nouvelle taille du périphérique:
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Après avoir accédé au périphérique, vous pouvez annuler son assignation et le démonter.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>montage et démontage manuels</title>
    <para>
     Un script <command>rbdmap</command> et une unité <systemitem class="daemon">systemd</systemitem> sont fournis pour faciliter le processus d'assignation et de montage des RBD après le démarrage et de leur démontage avant l'arrêt. Reportez-vous à la <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command> : assignation de périphériques RBD au moment du démarrage</title>
   <para>
    <command>rbdmap</command> est un script shell qui automatise les opérations <command>rbd map</command> et <command>rbd unmap</command> sur une ou plusieurs images RBD. Bien que vous puissiez exécuter le script manuellement à tout moment, les principaux avantages sont l'assignation et le montage automatiques des images RBD au démarrage (ainsi que le démontage et la désassignation à l'arrêt) déclenchés par le système Init. À cet effet, un fichier unité <systemitem class="daemon">systemd</systemitem>, <filename>rbdmap.service</filename>, est fourni avec le paquetage <systemitem>ceph-common</systemitem>.
   </para>
   <para>
    Le script accepte un argument unique, qui peut être <option>map</option> ou <option>unmap</option>. Dans les deux cas, le script analyse un fichier de configuration. Il pointe vers <filename>/etc/ceph/rbdmap</filename> par défaut, mais peut être remplacé par le biais d'une variable d'environnement <literal>RBDMAPFILE</literal>. Chaque ligne du fichier de configuration correspond à une image RBD qui doit être assignée ou dont l'assignation doit être annulée.
   </para>
   <para>
    Le fichier de configuration possède le format suivant :
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Chemin d'accès à une image dans une réserve. Indiquez-le en tant que <replaceable>nom_réserve</replaceable>/<replaceable>nom_image</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Liste facultative de paramètres à transmettre à la commande <command>rbd device map</command> sous-jacente. Ces paramètres et leurs valeurs doivent être indiqués en tant que chaîne séparée par des virgules, par exemple :
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       Dans cet exemple suivant, le script <command>rbdmap</command> exécute la commande suivante :
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       L'exemple suivant illustre comment spécifier un nom d'utilisateur et un trousseau de clés avec un secret correspondant :
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Lorsqu'il est exécuté en tant que <command>rdbmap map</command>, le script analyse le fichier de configuration et, pour chaque image RBD indiquée, tente d'abord d'assigner l'image (à l'aide de la commande <command>rbd device map</command>), puis de la monter.
   </para>
   <para>
    Lorsqu'elles sont exécutées en tant que <command>rbdmap unmap</command>, les images répertoriées dans le fichier de configuration seront démontées et désassignées.
   </para>
   <para>
    <command>rbdmap unmap-all</command> tente de démonter puis de désassigner toutes les images RBD actuellement assignées, qu'elles soient ou non répertoriées dans le fichier de configuration.
   </para>
   <para>
    En cas de réussite, l'opération <command>rbd device map</command> assigne l'image à un périphérique <filename>/dev/rbdX</filename> ; une règle udev est alors déclenchée afin de créer un lien symbolique de nom de périphérique convivial <filename>/dev/rbd/<replaceable>nom_réserve</replaceable>/<replaceable>nom_image</replaceable></filename> pointant vers le périphérique réellement assigné.
   </para>
   <para>
    Pour que le montage et le démontage réussissent, le nom de périphérique « convivial » doit être répertorié dans le fichier <filename>/etc/fstab</filename>. Lors de l'écriture d'entrées <filename>/etc/fstab</filename> pour les images RBD, indiquez l'option de montage « noauto » (ou « nofail »). Cela empêche le système Init d'essayer de monter le périphérique trop tôt, avant même que le périphérique en question existe, car <filename>rbdmap.service</filename> est généralement déclenché assez tard dans la séquence de démarrage.
   </para>
   <para>
    Pour obtenir la liste complète des options de <command>rbd</command>, reportez-vous à la page de manuel <command>rbd</command> (<command>man 8 rbd</command>).
   </para>
   <para>
    Pour obtenir des exemples de l'utilisation de <command>rbd</command>, reportez-vous à la page de manuel de <command>rbd</command> (<command>man 8 rbd</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Augmentation de la taille des périphériques RBD</title>
   <para>
    Si vous trouvez que la taille du périphérique RBD n'est plus suffisante, vous pouvez facilement l'augmenter.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Augmentez la taille de l'image RBD, par exemple jusqu'à 10 Go.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Développez le système de fichiers à la nouvelle taille du périphérique.
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Images instantanées</title>

  <para>
   Un instantané RBD est un instantané d'une image de périphérique de bloc RADOS. Avec les instantanés, vous conservez l'historique de l'état de l'image. Ceph prend également en charge la superposition d'instantanés, ce qui vous permet de cloner des images de machine virtuelle rapidement et facilement. Ceph prend en charge les instantanés de périphériques de bloc en utilisant la commande <command>rbd</command> et de nombreuses interfaces de niveau supérieur, notamment QEMU, <systemitem>libvirt</systemitem>, OpenStack et CloudStack.
  </para>

  <note>
   <para>
    Arrêtez les opérations d'entrée et de sortie et videz toutes les écritures en attente avant de créer l'instantané d'une image. Si l'image contient un système de fichiers, celui-ci doit être cohérent lors de la création de l'instantané.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Activation et configuration de <systemitem>cephx</systemitem></title>
   <para>
    Quand <systemitem>cephx</systemitem> est activé, vous devez spécifier un nom ou un ID d'utilisateur et un chemin d'accès au trousseau de clés contenant la clé correspondante pour l'utilisateur. Pour plus d'informations, reportez-vous au <xref linkend="cha-storage-cephx"/>. Vous pouvez également ajouter la variable d'environnement <systemitem>CEPH_ARGS</systemitem> pour ne pas avoir à saisir à nouveau les paramètres suivants.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Ajoutez l'utilisateur et le secret à la variable d'environnement <systemitem>CEPH_ARGS</systemitem> afin de ne pas avoir à les saisir à chaque fois.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Notions de base sur les instantanés</title>
   <para>
    Les procédures suivantes montrent comment créer, répertorier et supprimer des instantanés à l'aide de la commande <command>rbd</command> sur la ligne de commande.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Création d'instantanés</title>
    <para>
     Pour créer un instantané avec <command>rbd</command>, indiquez l'option <option>snap create</option>, le nom de la réserve et le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Liste des instantanés</title>
    <para>
     Pour répertorier les instantanés d'une image, spécifiez le nom de la réserve et le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Restauration de l'état initial des instantanés</title>
    <para>
     Pour rétablir l'état initial d'un instantané avec <command>rbd</command>, indiquez l'option <option>snap rollback</option>, le nom de la réserve, le nom de l'image et le nom de l'instantané.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Le retour à l'état initial d'une image dans un instantané revient à écraser la version actuelle de l'image avec les données d'un instantané. Le temps nécessaire à l'exécution d'un retour à l'état initial augmente avec la taille de l'image. Il est <emphasis>plus rapide de cloner</emphasis> à partir d'un instantané <emphasis>que de rétablir</emphasis> une image vers un instantané, cette méthode étant recommandée pour revenir à un état préexistant.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Suppression d'un instantané</title>
    <para>
     Pour supprimer un instantané avec <command>rbd</command>, indiquez l'option <option>snap rm</option>, le nom de la réserve, le nom de l'image et le nom de l'utilisateur.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Les instances Ceph OSD suppriment les données de manière asynchrone de sorte que la suppression d'un instantané ne libère pas immédiatement l'espace disque.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Purge des instantanés</title>
    <para>
     Pour supprimer tous les instantanés d'une image avec <command>rbd</command>, indiquez l'option <option>snap purge</option> et le nom de l'image.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Superposition d'instantanés</title>
   <para>
    Ceph prend en charge la possibilité de créer plusieurs clones de copie à l'écriture (COW) d'un instantané de périphérique de bloc. La superposition d'instantanés donne aux clients de périphériques de bloc Ceph les moyens de créer des images très rapidement. Par exemple, vous pouvez créer une image de périphérique de bloc avec une machine virtuelle Linux écrite, puis capturer l'image, protéger l'instantané et créer autant de clones COW que vous le souhaitez. Un instantané étant en lecture seule, le clonage d'un instantané simplifie la sémantique et permet de créer rapidement des clones.
   </para>
   <note>
    <para>
     Dans les exemples de ligne de commande ci-dessous, les termes « parent » et « child » (enfant) désignent un instantané de périphérique de bloc Ceph (parent) et l'image correspondante clonée à partir de l'instantané (enfant).
    </para>
   </note>
   <para>
    Chaque image clonée (enfant) stocke une référence à son image parent, ce qui permet à l'image clonée d'ouvrir l'instantané parent et de le lire.
   </para>
   <para>
    Un clone COW d'un instantané se comporte exactement comme n'importe quelle autre image de périphérique Ceph. Vous pouvez lire, écrire, cloner et redimensionner des images clonées. Il n'y a pas de restrictions spéciales avec les images clonées. Cependant, le clone copy-on-write d'un instantané fait référence à l'instantané, donc vous <emphasis>devez</emphasis> protéger l'instantané avant de le cloner.
   </para>
   <note>
    <title><option>--image-format 1</option> non pris en charge</title>
    <para>
     Vous ne pouvez pas créer d'instantanés d'images créés avec l'option <command>rbd create ‑‑image-format 1</command> obsolète. Ceph ne prend en charge que le clonage des images <emphasis>format 2</emphasis> par défaut.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Démarrage de la superposition</title>
    <para>
     La superposition de périphériques de bloc Ceph est un processus simple. Vous devez disposer d'une image. Vous devez créer un instantané de l'image. Vous devez protéger l'instantané. Après avoir effectué ces étapes, vous pouvez commencer le clonage de l'instantané.
    </para>
    <para>
     L'image clonée contient une référence à l'instantané parent et inclut l'ID de la réserve, l'ID de l'image et l'ID de l'instantané. L'inclusion de l'ID de réserve signifie que vous pouvez cloner des instantanés d'une réserve vers des images d'une autre réserve.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Modèle d'image</emphasis> : un cas d'utilisation courant de la superposition de périphériques de bloc consiste à créer une image principale et un instantané servant de modèle aux clones. Par exemple, un utilisateur peut créer une image pour une distribution Linux (par exemple, SUSE Linux Enterprise Server (SLES)) et créer un instantané correspondant. Périodiquement, l'utilisateur peut mettre à jour l'image et créer un instantané (par exemple, <command>zypper ref &amp;&amp; zypper patch</command> suivi de <command>rbd snap create</command>). Au fur et à mesure que l'image mûrit, l'utilisateur peut cloner l'un des instantanés.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Modèle étendu</emphasis> : un cas d'utilisation plus avancée inclut l'extension d'une image modèle fournissant plus d'informations qu'une image de base. Par exemple, un utilisateur peut cloner une image (un modèle de machine virtuelle) et installer d'autres logiciels (par exemple, une base de données, un système de gestion de contenu ou un système d'analyse), puis prendre un instantané de l'image agrandie, qui peut elle-même être mise à jour de la même manière que l'image de base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Réserve de modèles</emphasis> : une façon d'utiliser la superposition de périphériques de bloc consiste à créer une réserve contenant des images principales agissant comme des modèles et des instantanés de ces modèles. Vous pouvez ensuite étendre les privilèges de lecture seule aux utilisateurs afin qu'ils puissent cloner les instantanés sans possibilité d'écriture ou d'exécution dans la réserve.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migration/récupération d'image</emphasis> : une façon d'utiliser la superposition de périphériques de bloc consiste à migrer ou récupérer des données d'une réserve vers une autre.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protection d'un instantané</title>
    <para>
     Les clones accèdent aux instantanés parents. Tous les clones seraient endommagés si un utilisateur supprimait par inadvertance l'instantané parent. Pour éviter toute perte de données, vous devez protéger l'instantané avant de pouvoir le cloner.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Vous ne pouvez pas supprimer un instantané protégé.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Clonage d'un instantané</title>
    <para>
     Pour cloner un instantané, vous devez spécifier la réserve parent, l'image, l'instantané, la réserve enfant et le nom de l'image. Vous devez protéger l'instantané avant de pouvoir le cloner.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Vous pouvez cloner un instantané d'une réserve vers une image d'une autre réserve. Par exemple, vous pouvez gérer des images en lecture seule et des instantanés en tant que modèles dans une réserve, d'une part, et des clones inscriptibles dans une autre réserve, d'autre part.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Suppression de la protection d'un instantané</title>
    <para>
     Avant de pouvoir supprimer un instantané, vous devez d'abord le déprotéger. En outre, vous pouvez <emphasis>ne pas</emphasis> supprimer des instantanés contenant des références de clones. Vous devez fusionner chaque clone d'un instantané avant de pouvoir supprimer celui-ci.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Liste des enfants d'un instantané</title>
    <para>
     Pour dresser la liste des enfants d'un instantané, exécutez :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Mise à plat d'une image clonée</title>
    <para>
     Les images clonées conservent une référence à l'instantané parent. Lorsque vous supprimez la référence du clone enfant dans l'instantané parent, vous « aplatissez » (fusionnez) l'image en copiant les informations de l'instantané sur le clone. Le temps nécessaire à la fusion d'un clone augmente avec la taille de l'instantané. Pour supprimer un instantané, vous devez d'abord fusionner les images enfant.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Comme une image fusionnée contient toutes les informations de l'instantané, elle occupe plus d'espace de stockage qu'un clone en couches.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Miroirs d'image RBD</title>

  <para>
   Les images RBD peuvent être mises en miroir de manière asynchrone entre deux grappes Ceph. Cette fonctionnalité est disponible en deux modes :
  </para>

  <variablelist>
   <varlistentry>
    <term>Mode basé sur un journal</term>
    <listitem>
     <para>
      Ce mode utilise la fonctionnalité de journalisation de l'image RBD afin de garantir une réplication ponctuelle, cohérente entre les grappes en cas de panne. Chaque écriture dans l'image RBD est d'abord enregistrée dans le journal associé avant de réellement modifier l'image. La grappe <literal>remote</literal> lira le journal et relira les mises à jour de sa copie locale de l'image. Étant donné que chaque écriture dans l'image RBD entraîne deux écritures dans la grappe Ceph, attendez-vous à ce que les temps de latence en écriture soient pratiquement multipliés par deux lorsque vous utilisez la fonctionnalité de journalisation de l'image RBD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Mode basé sur des instantanés</term>
    <listitem>
     <para>
      Ce mode utilise des instantanés en miroir d'image RBD planifiés ou créés manuellement pour répliquer des images RBD cohérentes entre les grappes en cas de panne. La grappe <literal>remote</literal> détermine toutes les mises à jour de données ou de métadonnées entre deux instantanés-miroir et copie les différences dans sa copie locale de l'image. La fonctionnalité d'image RBD fast-diff permet de calculer rapidement les blocs de données mis à jour sans devoir analyser toute l'image RBD. Étant donné que ce mode n'est pas cohérent à un moment donné, la différence de l'instantané complet devra être synchronisée avant d'être utilisée pendant un scénario de basculement. Toutes les différences d'instantanés partiellement appliquées sont restaurées vers l'état du dernier instantané entièrement synchronisé avant utilisation.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   La mise en miroir est configurée réserve par réserve au sein des grappes homologues. Elle peut être configurée sur un sous-ensemble spécifique d'images dans la réserve ou configurée pour mettre en miroir automatiquement toutes les images d'une réserve lorsque vous utilisez la mise en miroir basée sur le journal uniquement. La mise en miroir est configurée à l'aide de la commande <command>rbd</command>. Le daemon <systemitem class="daemon">rbd-mirror</systemitem> est chargé d'extraire les mises à jour de l'image de la grappe homologue <literal>remote</literal> (distante) et de les appliquer à l'image dans la grappe <literal>local</literal> (locale).
  </para>

  <para>
   Selon les besoins de réplication souhaités, la mise en miroir RBD peut être configurée pour une réplication unidirectionnelle ou bidirectionnelle :
  </para>

  <variablelist>
   <varlistentry>
    <term>Réplication unidirectionnelle</term>
    <listitem>
     <para>
      Lorsque les données sont mises en miroir uniquement à partir d'une grappe primaire vers une grappe secondaire, le daemon <systemitem class="daemon">rbd-mirror</systemitem> s'exécute uniquement sur la grappe secondaire.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Réplication bidirectionnelle</term>
    <listitem>
     <para>
      Lorsque les données sont mises en miroir à partir des images primaires sur une grappe vers des images non primaires sur une autre grappe (et inversement), le daemon <systemitem class="daemon">rbd-mirror</systemitem> s'exécute sur les deux grappes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Chaque instance du daemon <systemitem class="daemon">rbd-mirror</systemitem> doit pouvoir se connecter simultanément aux grappes Ceph locales (<literal>local</literal>) et distantes (<literal>remote</literal>). Par exemple, tous les hôtes du moniteur et OSD. En outre, le réseau doit disposer de suffisamment de bande passante entre les deux centres de données pour gérer le workload en miroir.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configuration de la réserve</title>
   <para>
    Les procédures suivantes montrent comment effectuer les tâches d'administration de base pour configurer la mise en miroir à l'aide de la commande <command>rbd</command>. La mise en miroir est configurée réserve par réserve au sein des grappes Ceph.
   </para>
   <para>
    Vous devez effectuer les étapes de configuration de la réserve sur les deux grappes homologues. Ces procédures supposent que deux grappes, nommées <literal>local</literal> et <literal>remote</literal>, sont accessibles depuis un seul hôte pour plus de clarté.
   </para>
   <para>
    Reportez-vous à la page de manuel <command>rbd</command> (<command>man 8 rbd</command>) pour plus de détails sur la procédure de connexion à des grappes Ceph différentes.
   </para>
   <tip>
    <title>grappes multiples</title>
    <para>
     Le nom de la grappe dans les exemples suivants correspond à un fichier de configuration Ceph du même nom <filename>/etc/ceph/remote.conf</filename> et à un fichier de trousseau de clés Ceph du même nom <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Activation de la mise en miroir sur une réserve</title>
    <para>
     Pour activer la mise en miroir sur une grappe, indiquez la sous-commande <command>mirror pool enable</command>, le nom de la réserve et le mode de mise en miroir. Le mode de mise en miroir peut être pool ou image :
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Toutes les images de la réserve dont la fonctionnalité de journalisation est activée sont mises en miroir.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>image</term>
      <listitem>
       <para>
        La mise en miroir doit être explicitement activée sur chaque image. Pour plus d'informations, reportez-vous à la <xref linkend="rbd-mirror-enable-image-mirroring"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Désactivation de la mise en miroir</title>
    <para>
     Pour désactiver la mise en miroir sur une grappe, indiquez la sous-commande <command>mirror pool disable</command> et le nom de la réserve. Lorsque la mise en miroir est désactivée sur une réserve de cette manière, la mise en miroir est également désactivée sur toutes les images (dans la réserve) pour lesquelles la mise en miroir a été explicitement activée.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Démarrage des homologues</title>
    <para>
     Pour que le daemon <systemitem class="daemon">rbd-mirror</systemitem> découvre sa grappe homologue, l'homologue doit être enregistré dans la réserve et un compte utilisateur doit être créé. Ce processus peut être automatisé avec <command>rbd</command> et les commandes <command>mirror pool peer bootstrap create</command> ainsi que<command> mirror pool peer bootstrap import</command>.
    </para>
    <para>
     Pour créer manuellement un nouveau jeton de démarrage avec <command>rbd</command>, spécifiez la commande <command>mirror pool peer bootstrap create</command>, un nom de réserve, ainsi qu'un nom de site convivial facultatif pour décrire la grappe <literal>local</literal> :
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     La sortie de la commande <command>mirror pool peer bootstrap create</command> sera un jeton qui doit être fourni à la commande <command>mirror pool peer bootstrap import</command>. Par exemple, sur la grappe <literal>local</literal> :
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Pour importer manuellement le jeton de démarrage créé par une autre grappe avec la commande <command>rbd</command>, utilisez la syntaxe suivante :
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Où :
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        Nom facultatif convivial du site pour décrire la grappe <literal>local</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Direction de la mise en miroir. La valeur par défaut est <literal>rx-tx</literal> pour la mise en miroir bidirectionnelle, mais peut également être définie sur <literal>rx-only</literal> pour la mise en miroir unidirectionnelle.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        Nom de la réserve.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        Chemin du fichier pour accéder au jeton créé (ou <literal>-</literal> pour le lire à partir de l'entrée standard).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple, sur la grappe <literal>remote</literal> :
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Ajout manuel d'un homologue de grappe</title>
    <para>
     Au lieu de démarrer des homologues comme décrit à la <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>, vous pouvez spécifier des homologues manuellement. Le daemon <systemitem class="daemon">rbd-mirror</systemitem> distant doit accéder à la grappe locale pour effectuer la mise en miroir. Créez un nouvel utilisateur Ceph local que le daemon <systemitem class="daemon">rbd-mirror</systemitem> distant utilisera, par exemple, <literal>rbd-mirror-peer</literal> :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Utilisez la syntaxe suivante pour ajouter une grappe Ceph homologue en miroir avec la commande <command>rbd</command> :
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Par défaut, le daemon <systemitem class="daemon">rbd-mirror</systemitem> doit avoir accès au fichier de configuration Ceph situé à l'emplacement <filename>/etc/ceph/.<replaceable>NOM_GRAPPE</replaceable>.conf</filename>. Il fournit les adresses IP des instances MON de la grappe homologue et un trousseau de clés pour un client nommé <replaceable>NOM_CLIENT</replaceable> situé dans les chemins de recherche par défaut ou personnalisés du trousseau de clés, par exemple <filename>/etc/ceph/<replaceable>NOM_GRAPPE</replaceable>.<replaceable>NOM_CLIENT</replaceable>.keyring</filename>.
    </para>
    <para>
     Sinon, l'instance MON et/ou la clé du client de la grappe homologue peut être stockée en toute sécurité dans la zone de stockage locale config-key de Ceph. Pour spécifier les attributs de connexion de la grappe homologue lors de l'ajout d'un homologue en miroir, utilisez les options <option>--remote-mon-host</option> et <option>--remote-key-file</option>. Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Suppression d'un homologue de grappe</title>
    <para>
     Pour supprimer une grappe homologue de mise en miroir, indiquez la sous-commande <command>mirror pool peer remove</command>, le nom de la réserve et l'UUID de l'homologue (disponible dans le résultat de la commande <command>rbd mirror pool info</command>) :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Réserves de données</title>
    <para>
     Lors de la création d'images dans la grappe cible, <systemitem class="daemon">rbd-mirror</systemitem> sélectionne une réserve de données comme suit :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Si une réserve de données par défaut est configurée pour la grappe cible (avec l'option de configuration <option>rbd_default_data_pool</option>), cette réserve sera utilisée.
      </para>
     </listitem>
     <listitem>
      <para>
       Dans le cas contraire, si l'image source utilise une réserve de données distincte et qu'une réserve portant le même nom existe sur la grappe cible, cette réserve est utilisée.
      </para>
     </listitem>
     <listitem>
      <para>
       Si aucune des conditions ci-dessus n'est vraie, aucune réserve de données n'est configurée.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configuration de l'image RBD</title>
   <para>
    Contrairement à la configuration de réserve, la configuration d'image ne doit être effectuée que par rapport à une seule grappe Ceph homologue de mise en miroir.
   </para>
   <para>
    Les images RBD en miroir sont désignées comme étant soit <emphasis>primaires</emphasis>, soit <emphasis>non primaires</emphasis>. Il s'agit d'une propriété de l'image et non pas de la réserve. Les images qui sont désignées comme non primaires ne sont pas modifiables.
   </para>
   <para>
    Les images sont automatiquement promues au rang d'images primaires lorsque la mise en miroir est activée pour la première fois sur une image (soit implicitement si le mode de mise en miroir de la réserve était « pool » et que la fonctionnalité de journalisation de l'image a été activée, soit explicitement – reportez-vous à la <xref linkend="rbd-mirror-enable-image-mirroring"/> – à l'aide de la commande <command>rbd</command>).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Activation de la mise en miroir d'images</title>
    <para>
     Si la mise en miroir est configurée en mode <literal>image</literal>, il est nécessaire d'activer explicitement la mise en miroir pour chaque image de la réserve. Pour activer la mise en miroir d'une image en particulier avec la commande <command>rbd</command>, indiquez la sous-commande <command>mirror image enable</command> ainsi que le nom de la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Le mode d'image en miroir peut être <literal>journal</literal> ou <literal>snapshot</literal> :
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (valeur par défaut)</term>
      <listitem>
       <para>
        Lorsqu'elle est configurée en mode <literal>journal</literal>, la mise en miroir utilise la fonctionnalité de journalisation de l'image RBD pour répliquer le contenu de l'image. Si la fonction de journalisation de l'image RBD n'est pas encore activée sur l'image, elle sera activée automatiquement.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>snapshot</term>
      <listitem>
       <para>
        Lorsqu'elle est configurée en mode <literal>snapshot</literal> (instantané), la mise en miroir utilise des instantanés-miroir de l'image RBD pour répliquer le contenu de l'image. Une fois activé, un instantané-miroir initial est automatiquement créé. Des instantanés-miroir de l'image RBD supplémentaires peuvent être créés à l'aide de la commande <command>rbd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Activation de la fonctionnalité de journalisation des images</title>
    <para>
     La mise en miroir RBD utilise la fonctionnalité de journalisation RBD pour garantir que l'image répliquée est préservée en cas de panne. Lorsque vous utilisez le mode de mise en miroir <literal>image</literal>, la fonctionnalité de journalisation est automatiquement activée si la mise en miroir est activée sur l'image. Lorsque vous utilisez le mode de mise en miroir <literal>pool</literal>, avant qu'une image puisse être mise en miroir sur une grappe homologue, la fonction de journalisation d'image RBD doit être activée. La fonctionnalité peut être activée au moment de la création de l'image en indiquant l'option <option>--image-feature exclusive-lock,journaling</option> dans la commande <command>rbd</command>.
    </para>
    <para>
     Le cas échéant, la fonction de journalisation peut être dynamiquement activée sur des images RBD préexistantes. Pour activer la journalisation, indiquez la sous-commande <command>feature enable</command>, le nom de la réserve et de l'image, et le nom de l'entité :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>dépendance des options</title>
     <para>
      La fonctionnalité <option>journaling</option> dépend de la fonctionnalité <option>exclusive-lock</option>. Si la fonctionnalité <option>exclusive-lock</option> n'est pas encore activée, vous devez l'activer avant la fonctionnalité <option>journaling</option>.
     </para>
    </note>
    <tip>
     <para>
      Vous pouvez activer la journalisation sur toutes les nouvelles images par défaut en ajoutant <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option> à votre fichier de configuration Ceph.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Création d'instantanés-miroir d'images</title>
    <para>
     Lors de l'utilisation de la mise en miroir basée sur des instantanés, des instantanés-miroir doivent être créés chaque fois que vous souhaitez mettre en miroir le contenu modifié de l'image RBD. Pour créer manuellement un instantané-miroir à l'aide de la commande <command>rbd</command>, spécifiez la commande <command>mirror image snapshot</command> ainsi que le nom de la réserve et de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Par défaut, seuls trois instantanés-miroir sont créés par image. L'instantané-miroir le plus récent est automatiquement nettoyé si la limite est atteinte. La limite peut être remplacée par l'option de configuration <option>rbd_mirroring_max_mirroring_snapshots</option> si nécessaire. En outre, les instantanés-miroir sont automatiquement supprimés lors du retrait de l'image ou de la désactivation de la mise en miroir.
    </para>
    <para>
     Des instantanés-miroir peuvent également être créés automatiquement à intervalles réguliers si des planifications d'instantanés-miroir sont définies. L'instantané-miroir peut être planifié de manière globale, par réserve ou par image. Plusieurs planifications d'instantanés-miroir peuvent être définies à n'importe quel niveau, mais seules les planifications d'instantanés les plus spécifiques qui correspondent à une image en miroir individuelle seront exécutées.
    </para>
    <para>
     Pour créer une planification d'instantanés-miroir avec <command>rbd</command>, spécifiez la commande <command>mirror snapshot schedule add</command> ainsi qu'un nom de réserve ou d'image, un intervalle et une heure de début facultative.
    </para>
    <para>
     L'intervalle peut être spécifié en jours, heures ou minutes respectivement à l'aide des suffixes <option>d</option>, <option>h</option> ou <option>m</option>. L'heure de début facultative peut être spécifiée à l'aide du format d'heure ISO 8601. Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Pour supprimer une planification d'instantané-miroir avec <command>rbd</command>, spécifiez la commande <command>mirror snapshot schedule remove</command> avec des options qui correspondent à la commande d'ajout de planification correspondante.
    </para>
    <para>
     Pour répertorier toutes les planifications d'instantanés d'un niveau spécifique (global, réserve ou image) avec la commande <command>rbd</command>, spécifiez la commande <command>mirror snapshot schedule ls</command> avec un nom facultatif de réserve ou d'image. En outre, l'option <option>--recursive</option> peut être spécifiée pour répertorier toutes les planifications du niveau spécifié et des niveaux inférieurs. Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Pour savoir quand les prochains instantanés seront créés pour les images RBD en miroir basées sur des instantanés avec <command>rbd</command>, spécifiez la commande <command>mirror snapshot schedule status</command> ainsi qu'un nom de réserve ou d'image facultatif. Par exemple :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Désactivation de la mise en miroir d'images</title>
    <para>
     Pour désactiver la mise en miroir d'une image en particulier, indiquez la sous-commande <command>mirror image disable</command> avec le nom de la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Promotion et rétrogradation d'images</title>
    <para>
     Dans un scénario de basculement où la désignation principale doit être déplacée sur l'image dans la grappe homologue, vous devez arrêter l'accès à l'image primaire, rétrograder l'image primaire actuelle, promouvoir la nouvelle image primaire et reprendre l'accès à l'image sur la grappe alternative.
    </para>
    <note>
     <title>promotion forcée</title>
     <para>
      La promotion peut être forcée à l'aide de l'option <option>--force</option>. La promotion forcée est nécessaire lorsque la rétrogradation ne peut pas être propagée à la grappe homologue (par exemple, en cas d'échec de la grappe ou de panne de communication). Cela se traduira par un scénario de divergence entre les deux homologues, et l'image ne sera plus synchronisée jusqu'à l'émission de la sous-commande <command>resync</command>.
     </para>
    </note>
    <para>
     Pour rétrograder une image non primaire spécifique, indiquez la sous-commande <command>mirror image demote</command> ainsi que le nom de la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Pour rétrograder toutes les images primaires, indiquez la sous-commande <command>mirror image demote</command> ainsi que le nom de la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Pour promouvoir une image spécifique au rang d'image primaire, indiquez la sous-commande <command>mirror image promote</command> ainsi que le nom de la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Pour promouvoir toutes les images non primaires d'une réserve au rang d'images primaires, indiquez la sous-commande <command>mirror image promote</command> ainsi que le nom de la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>division de la charge d'E/S</title>
     <para>
      Comme l'état primaire ou non primaire s'applique au niveau de l'image, il est possible que deux grappes divisent le chargement des E/S et le basculement ou la restauration par phases.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Resynchronisation forcée de l'image</title>
    <para>
     Si le daemon <systemitem class="daemon">rbd-mirror</systemitem> détecte un événement de divergence, il n'y aura pas de tentative de mettre en miroir l'image concernée jusqu'à ce que celle-ci soit corrigée. Pour reprendre la mise en miroir d'une image, commencez par rétrograder l'image jugée obsolète, puis demandez une resynchronisation avec l'image principale. Pour demander une resynchronisation de l'image, indiquez la sous-commande <command>mirror image resync</command> avec le nom de la réserve et le nom de l'image :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Vérification de l'état du miroir</title>
   <para>
    L'état de réplication de la grappe homologue est stocké pour chaque image en miroir principale. Cet état peut être récupéré à l'aide des sous-commandes <command>mirror image status</command> et <command>mirror pool status</command> :
   </para>
   <para>
    Pour demander l'état de l'image miroir, indiquez la sous-commande <command>mirror image status</command> avec le nom de la réserve et le nom de l'image :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Pour demander l'état du résumé de la réserve miroir, indiquez la sous-commande <command>mirror pool status</command> avec le nom de la réserve :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     L'option <option>--verbose</option> de la sous-commande <command>mirror pool status</command> permet d'afficher des informations détaillées sur l'état de chaque image de mise en miroir présente dans la réserve.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Paramètres de cache</title>

  <para>
   L'implémentation de l'espace utilisateur du périphérique de bloc Ceph (<systemitem>librbd</systemitem>) ne peut pas profiter du cache de page Linux. Il comprend donc son propre caching en mémoire. Le caching RBD se comporte comme le caching de disque dur. Lorsque le système d'exploitation envoie une demande de barrière ou de vidage, toutes les données altérées (« dirty ») sont écrites sur l'OSD. Cela signifie que l'utilisation du caching à écriture différée est tout aussi sûre que celle d'un disque dur physique correct avec une machine virtuelle qui envoie correctement des demandes de vidage. Le cache utilise un algorithme <emphasis>Moins récemment utilisée</emphasis> (LRU) et peut, en mode d'écriture différée, fusionner les demandes adjacentes pour un meilleur débit.
  </para>

  <para>
   Ceph prend en charge le caching à écriture différée pour RBD. Pour l'activer, exécutez
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Par défaut, <systemitem>librbd</systemitem> n'effectue aucun caching. Les écritures et les lectures sont envoyées directement à la grappe de stockage, et les écritures ne reviennent que lorsque les données sont sur disque sur toutes les répliques. Lorsque le caching est activé, les écritures reviennent immédiatement sauf si le volume d'octets non vidés est supérieur à celui défini par l'option <option>rbd cache max dirty</option>. Dans un tel cas, l'écriture déclenche l'écriture différée et les blocs jusqu'à ce que suffisamment d'octets soient vidés.
  </para>

  <para>
   Ceph prend en charge le caching à écriture immédiate pour RBD. Vous pouvez définir la taille du cache ainsi que des objectifs et des limites pour passer du caching à écriture différée au caching à écriture immédiate. Pour activer le mode d'écriture immédiate, exécutez
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Cela signifie que les écritures ne reviennent que lorsque les données sont sur disque sur toutes les répliques, mais que les lectures peuvent provenir du cache. Le cache est en mémoire sur le client, et chaque image RBD a son propre cache. Étant donné que le cache est en local sur le client, il n'y a pas de cohérence s'il y a d'autres accès à l'image. L'exécution de GFS ou d'OCFS sur RBD ne fonctionnera pas avec le caching activé.
  </para>

  <para>
   Les paramètres suivants affectent le comportement des périphériques de bloc RADOS. Pour les définir, utilisez la catégorie <literal>client</literal> :
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Permet d'activer le caching pour le périphérique de bloc RADOS (RBD). La valeur par défaut est « true ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      Taille du cache RBD en octets. La valeur par défaut est 32 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      Limite « dirty » en octets à laquelle le cache déclenche l'écriture différée. <option>rbd cache max dirty</option> doit être inférieur à <option>rbd cache size</option>. Si la valeur est définie sur 0, le caching à écriture immédiate est utilisé. La valeur par défaut est 24 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      Valeur « dirty target » avant que le cache commence à écrire des données sur le stockage de données. Ne bloque pas les écritures dans le cache. La valeur par défaut est 16 Mo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Temps en secondes pendant lequel les données altérées sont dans le cache avant le début de l'écriture différée. La valeur par défaut est 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Indique de commencer en mode d'écriture immédiate et de passer à l'écriture différée après la réception de la première demande de vidage. Cette configuration classique est judicieuse lorsque les machines virtuelles qui s'exécutent sur <systemitem>rbd</systemitem> sont trop anciennes pour envoyer des vidages (par exemple, le pilote virtio dans Linux avant le kernel 2.6.32). La valeur par défaut est « true ».
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>Paramètres QoS</title>

  <para>
   En règle générale, la qualité de service (QoS) fait référence aux méthodes de priorisation du trafic et de réservation des ressources. Elle est particulièrement importante pour le transport du trafic avec des exigences spéciales.
  </para>

  <important>
   <title>non pris en charge par iSCSI</title>
   <para>
    Les paramètres QoS suivants sont utilisés uniquement par l'implémentation RBD de l'espace utilisateur <systemitem class="daemon">librbd</systemitem> et <emphasis>non</emphasis> par l'implémentation <systemitem>kRBD</systemitem>. Étant donné qu'iSCSI utilise <systemitem>kRBD</systemitem>, il n'emploie pas les paramètres QoS. Toutefois, pour iSCSI, vous pouvez configurer la qualité de service sur la couche des périphériques de bloc du kernel à l'aide des fonctionnalités standard du kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations d'E/S par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée d'octets en E/S par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations de lecture par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des opérations d'écriture par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des octets en lecture par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      Limite souhaitée des octets en écriture par seconde. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations d'E/S. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en E/S. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations de lecture. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des opérations d'écriture. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en lecture. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      Limite de rafales souhaitée des octets en écriture. La valeur par défaut est 0 (pas de limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      Cycle d'horloge de planification minimal (en millisecondes) pour la qualité de service. La valeur par défaut est 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Paramètres de la lecture anticipée</title>

  <para>
   Le périphérique de bloc RADOS prend en charge la lecture anticipée/la prérécupération pour optimiser les petites lectures séquentielles. Ces opérations devraient normalement être gérées par le système d'exploitation invité dans le cas d'une machine virtuelle, mais les chargeurs de démarrage peuvent ne pas émettre des lectures efficaces. La lecture anticipée est automatiquement désactivée si le caching est désactivé.
  </para>

  <important>
   <title>non pris en charge par iSCSI</title>
   <para>
    Les paramètres de lecture anticipée suivants sont utilisés uniquement par l'implémentation RBD de l'espace utilisateur <systemitem class="daemon">librbd</systemitem> et <emphasis>non</emphasis> par l'implémentation <systemitem>kRBD</systemitem>. Étant donné qu'iSCSI utilise <systemitem>kRBD</systemitem>, il n'emploie pas les paramètres de lecture anticipée. Toutefois, pour iSCSI, vous pouvez configurer la lecture anticipée sur la couche des périphériques de bloc du kernel à l'aide des fonctionnalités standard du kernel.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Nombre de demandes de lecture séquentielle nécessaires pour déclencher la lecture anticipée. La valeur par défaut est 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Taille maximale d'une demande de lecture anticipée. Lorsque la valeur est 0, la lecture anticipée est désactivée. La valeur par défaut est 512 Ko.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      Après la lecture de tous ces octets à partir d'une image RBD, la lecture anticipée est désactivée pour cette image jusqu'à ce qu'elle soit fermée. Cela permet à l'OS invité de prendre en charge la lecture anticipée quand il est démarré. Lorsque la valeur est 0, la lecture anticipée reste activée. La valeur par défaut est 50 Mo.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Fonctions avancées</title>

  <para>
   Le périphérique de bloc RADOS prend en charge les fonctions avancées qui améliorent la fonctionnalité des images RBD. Vous pouvez spécifier les fonctions sur la ligne de commande lors de la création d'une image RBD ou dans le fichier de configuration Ceph à l'aide de l'option <option>rbd_default_features</option>.
  </para>

  <para>
   Vous pouvez spécifier les valeurs de l'option <option>rbd_default_features</option> de deux façons :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Comme une somme de valeurs internes des fonctions. Chaque fonction a sa propre valeur interne, par exemple 1 pour « layering » et 16 pour « fast-diff ». Par conséquent, pour activer ces deux fonctions par défaut, incluez la ligne suivante :
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     Comme une liste de fonctions séparées par des virgules. L'exemple précédent se présentera comme suit :
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>fonctions non prises en charge par iSCSI</title>
   <para>
    Les images RBD avec les fonctions suivantes ne seront pas prises en charge par iSCSI : <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option> et <option>striping</option>.
   </para>
  </note>

  <para>
   Voici une liste de fonctions RBD avancées :
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      La création de couches, ou superposition (layering), permet d'utiliser le clonage.
     </para>
     <para>
      La valeur interne est 1, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      La segmentation (striping) propage les données sur plusieurs objets et contribue au parallélisme pour les workloads séquentiels de lecture/écriture. Elle empêche les goulots d'étranglement de noeud unique pour les périphériques de bloc RADOS volumineux ou fort occupés.
     </para>
     <para>
      La valeur interne est 2, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Lorsque cette fonction est activée, il faut qu'un client obtienne un verrouillage sur un objet avant d'effectuer une écriture. Activez le verrouillage exclusif uniquement lorsqu'un seul client accède à une image en même temps. La valeur interne est 4. La valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      La prise en charge de l'assignation d'objet dépend de la prise en charge du verrouillage exclusif. Les périphériques de bloc sont provisionnés dynamiquement, ce qui signifie qu'ils ne stockent que les données qui existent réellement. La prise en charge de l'assignation d'objet permet de suivre quels objets existent réellement (ont des données stockées sur un disque). L'activation de la prise en charge de l'assignation d'objet permet d'accélérer les opérations d'E/S pour le clonage, l'importation et l'exportation d'une image peu peuplée, et pour la suppression.
     </para>
     <para>
      La valeur interne est 8, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      La prise en charge de la fonction fast-diff dépend de la prise en charge de l'assignation d'objet et du verrouillage exclusif. Elle ajoute une propriété à l'assignation d'objet, ce qui la rend beaucoup plus rapide pour générer des différentiels entre les instantanés d'une image et l'utilisation réelle des données d'un instantané.
     </para>
     <para>
      La valeur interne est 16, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      La fonction deep-flatten rend <command>rbd flatten</command> (voir la <xref linkend="rbd-flatten-cloned-image"/>) opérationnel sur tous les instantanés d'une image, en plus de l'image elle-même. Sans elle, les instantanés d'une image s'appuieront toujours sur le parent, et vous ne pourrez pas supprimer l'image parent avant que les instantanés soient supprimés. La fonction deep-flatten rend un parent indépendant de ses clones, même s'ils ont des instantanés.
     </para>
     <para>
      La valeur interne est 32, la valeur par défaut est « yes ».
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      La prise en charge de la fonction de journalisation (journaling) dépend de la prise en charge du verrouillage exclusif. La journalisation enregistre toutes les modifications d'une image dans l'ordre où elles se produisent. La mise en miroir RBD (voir la <xref linkend="ceph-rbd-mirror"/>) utilise le journal pour répliquer une image cohérente sur une grappe distante en cas de panne.<literal/>
     </para>
     <para>
      La valeur interne est 64, la valeur par défaut est « no ».
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Assignation RBD à l'aide d'anciens clients de kernel</title>

  <para>
   Les anciens clients (par exemple, SLE 11 SP4) peuvent ne pas être en mesure d'assigner les images RBD parce qu'une grappe déployée avec SUSE Enterprise Storage 7 force certaines fonctions (à la fois les fonctions de niveau image RBD et celles de niveau RADOS) que ces anciens clients ne prennent pas en charge. Dans ce cas, les journaux OSD afficheront des messages semblables à ce qui suit :
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>la modification des types de compartiment de carte CRUSH provoque un rééquilibrage massif</title>
   <para>
    Si vous avez l'intention de commuter les types de compartiment de carte CRUSH « straw » et « straw2 », procédez de manière méthodique. Attendez-vous à un impact significatif sur la charge de la grappe, car un tel changement provoque un rééquilibrage massif des grappes.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Désactivez toutes les fonctions d'image RBD qui ne sont pas prises en charge. Par exemple :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Remplacez les types de compartiment de carte CRUSH « straw2 » par « straw » :
    </para>
    <substeps>
     <step>
      <para>
       Enregistrez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Décompilez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Modifiez la carte CRUSH et remplacez « straw2 » par « straw ».
      </para>
     </step>
     <step>
      <para>
       Recompilez la carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Définissez la nouvelle carte CRUSH :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Activation des périphériques de bloc et de Kubernetes</title>

  <para>
   Vous pouvez utiliser Ceph RBD avec Kubernetes v1.13 et versions ultérieures via le pilote <literal>ceph-csi</literal>. Ce pilote provisionne dynamiquement des images RBD pour soutenir les volumes Kubernetes et assigne ces images RBD en tant que périphériques de bloc (éventuellement en montant un système de fichiers contenu dans l'image) sur des noeuds de travail exécutant des pods faisant référence à un volume soutenu par RBD.
  </para>

  <para>
   Pour utiliser des périphériques de bloc Ceph avec Kubernetes, vous devez installer et configurer <literal>ceph-csi</literal> dans votre environnement Kubernetes.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> utilise les modules de kernel RBD par défaut qui peuvent ne pas prendre en charge tous les paramètres Ceph CRUSH ou les fonctions d'image RBD.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Par défaut, les périphériques de bloc Ceph utilisent la réserve RBD. Créez une réserve pour stocker les volumes Kubernetes. Assurez-vous que votre grappe Ceph est en cours d'exécution, puis créez la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Utilisez l'outil RBD pour initialiser la réserve :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Créez un nouvel utilisateur pour Kubernetes et <literal>ceph-csi</literal>. Exécutez la commande suivante et enregistrez la clé générée :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> nécessite un objet ConfigMap stocké dans Kubernetes pour définir les adresses de moniteur Ceph pour la grappe Ceph. Collectez le fsid unique de la grappe Ceph et les adresses de moniteur :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Générez un fichier <filename>csi-config-map.yaml</filename> similaire à l'exemple ci-dessous, en remplaçant le FSID par <literal>clusterID</literal> et les adresses de moniteur pour <literal>monitors</literal> :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Une fois généré, stockez le nouvel objet ConfigMap dans Kubernetes :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     <literal>ceph-csi</literal> a besoin des informations d'identification cephx pour communiquer avec la grappe Ceph. Générez un fichier <filename>csi-rbd-secret.yaml</filename> similaire à l'exemple ci-dessous, en utilisant l'ID utilisateur Kubernetes et la clé cephx que vous venez de créer :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Une fois généré, stockez le nouvel objet Secret dans Kubernetes :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Créez les objets ServiceAccount et RBAC ClusterRole/ClusterRoleBinding Kubernetes requis. Ces objets ne doivent pas nécessairement être personnalisés pour votre environnement Kubernetes et peuvent donc être utilisés directement à partir des fichiers YAML de déploiement <literal>ceph-csi</literal> :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Créez l'outil de déploiement <literal>ceph-csi</literal> et les plug-ins de noeud :
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Par défaut, les fichiers YAML de l'outil de déploiement et du plug-in de noeud récupèrent la version de développement du conteneur <literal>ceph-csi</literal>. Les fichiers YAML doivent être mis à jour pour utiliser une version commerciale.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Utilisation de périphériques de bloc Ceph dans Kubernetes</title>
   <para>
    Kubernetes StorageClass définit une classe de stockage. Plusieurs objets StorageClass peuvent être créés pour être assignés à différents niveaux et fonctionnalités de qualité de service. Par exemple, NVMe par rapport aux réserves sur disque dur.
   </para>
   <para>
    Pour créer une classe de stockage <literal>ceph-csi</literal> assignée à la réserve Kubernetes créée ci-dessus, le fichier YAML suivant peut être utilisé, après avoir vérifié que la propriété <literal>clusterID</literal> correspond au FSID de votre grappe Ceph :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    <literal>PersistentVolumeClaim</literal> est une requête de ressources de stockage abstrait émise par un utilisateur. Le paramètre <literal>PersistentVolumeClaim</literal> serait alors associé à une ressource de pod pour provisionner un volume <literal>PersistentVolume</literal>, qui serait soutenu par une image de bloc Ceph. Un mode de volume <option>volumeMode</option> facultatif peut être inclus pour choisir entre un système de fichiers monté (par défaut) ou un volume basé sur un périphérique de bloc brut.
   </para>
   <para>
    À l'aide de <literal>ceph-csi</literal>, la spécification de <option>Filesystem</option> pour <option>volumeMode</option> peut prendre en charge les réclamations <literal>ReadWriteOnce</literal> et <literal>ReadOnlyMode accessMode</literal> et la spécification de <option>Block</option> pour <option>volumeMode</option> peut prendre en charge les réclamations <literal>ReadWriteOnce</literal>, <literal>ReadWriteMany</literal> et <literal>ReadOnlyMany accessMode</literal>.
   </para>
   <para>
    Par exemple, pour créer une réclamation <literal>PersistentVolumeClaim</literal> basée sur des blocs qui utilise la classe <literal>ceph-csi-based StorageClass</literal> créée ci-dessus, le fichier YAML suivant peut être utilisé pour demander un stockage de bloc brut à partir de la classe de stockage <literal>csi-rbd-sc StorageClass</literal> :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    L'exemple suivant illustre la liaison d'une réclamation <literal>PersistentVolumeClaim</literal> à une ressource de pod en tant que périphérique de bloc brut :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Pour créer une réclamation <literal>PersistentVolumeClaim</literal> basée sur le système de fichiers qui utilise la classe <literal>ceph-csi-based StorageClass</literal> créée ci-dessus, le fichier YAML suivant peut être utilisé pour demander un système de fichiers monté (soutenu par une image RBD) à partir de la classe de stockage <literal>csi-rbd-sc StorageClass</literal> :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    L'exemple suivant illustre la liaison d'une réclamation <literal>PersistentVolumeClaim</literal> à une ressource de pod en tant que système de fichiers monté :
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
