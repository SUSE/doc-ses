<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Tâches opérationnelles</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>oui</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modification de la configuration d'une grappe</title>

  <para>
   Pour modifier la configuration d'une grappe Ceph existante, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Exportez la configuration actuelle de la grappe dans un fichier :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Modifiez le fichier de configuration et mettez à jour les lignes appropriées. Vous trouverez des exemples de spécification dans le <xref linkend="deploy-cephadm-day2"/> et à la <xref linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Appliquez la nouvelle configuration :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Ajout de noeuds</title>

  <para>
   Pour ajouter un noeud à une grappe Ceph, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Installez SUSE Linux Enterprise Server et SUSE Enterprise Storage sur le nouvel hôte. Reportez-vous au <xref linkend="deploy-sles"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d'informations.
    </para>
   </step>
   <step>
    <para>
     Configurez l'hôte en tant que minion Salt d'un Salt Master préexistant. Reportez-vous au <xref linkend="deploy-salt"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d'informations.
    </para>
   </step>
   <step>
    <para>
     Ajoutez le nouvel hôte à <systemitem class="resource">ceph-salt</systemitem> et informez-en cephadm, par exemple :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Reportez-vous au <xref linkend="deploy-cephadm-configure-minions"/> (Guide de sécurité, Chapitre 17 « Masquage et pare-feu ») pour plus d'informations.
    </para>
   </step>
   <step>
    <para>
     Vérifiez que le noeud a bien été ajouté à <systemitem class="resource">ceph-salt</systemitem> :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Appliquez la configuration au nouvel hôte de la grappe :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Vérifiez que l'hôte qui vient d'être ajouté appartient désormais à l'environnement cephadm :
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Suppression de noeuds</title>

  <tip>
   <title>suppression des OSD</title>
   <para>
    Si le noeud que vous souhaitez supprimer exécute des OSD, commencez par supprimer ces derniers et vérifiez qu'aucun OSD ne s'exécute sur ce noeud. Pour plus d'informations sur la suppression des OSD, reportez-vous à la <xref linkend="removing-node-osds"/>.
   </para>
  </tip>

  <para>
   Pour supprimer un noeud d'une grappe, procédez comme suit :
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Pour tous les types de service Ceph, à l'exception de <literal>node-exporter</literal> et <literal>crash</literal>, supprimez le nom d'hôte du noeud du fichier de spécification de placement de la grappe (par exemple, <filename>cluster.yml</filename>). Pour plus d'informations, reportez-vous au <xref linkend="cephadm-service-and-placement-specs"/>. Par exemple, si vous souhaitez supprimer l'hôte nommé <literal>ses-min2</literal>, supprimez toutes les occurrences de <literal>- ses-min2</literal> de toutes les sections <literal>placement:</literal> :
    </para>
    <para>
     Remplacez
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     par
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Appliquez vos modifications au fichier de configuration :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Supprimez le noeud de l'environnement de cephadm :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Si le noeud exécute les services <literal>crash.osd.1</literal> et <literal>crash.osd.2</literal>, supprimez-les en exécutant la commande suivante sur l'hôte :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Par exemple :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Supprimez tous les rôles du minion à supprimer :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Si le minion que vous souhaitez supprimer est le minion Bootstrap, vous devez également supprimer le rôle Bootstrap :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     Après avoir supprimé tous les OSD sur un hôte unique, supprimez ce dernier de la carte CRUSH :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      Le nom du compartiment doit être identique au nom d'hôte.
     </para>
    </note>
   </step>
   <step>
    <para>
     Vous pouvez à présent supprimer le minion de la grappe :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    En cas d'échec et si le minion que vous essayez de supprimer se trouve dans un état de désactivation permanente, vous devrez supprimer le noeud du Salt Master :
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    Supprimez ensuite manuellement le noeud du fichier <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>. Celui-ci se trouve généralement dans <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>Gestion des OSD</title>

  <para>
   Cette section explique comment ajouter, effacer ou supprimer des OSD dans une grappe Ceph.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Liste des périphériques de disque</title>
   <para>
    Pour identifier les périphériques de disque utilisés et inutilisés sur tous les noeuds de la grappe, listez-les en exécutant la commande suivante :
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Effacement de périphériques de disque</title>
   <para>
    Pour réutiliser un périphérique de disque, vous devez d'abord l'effacer <emphasis/>:
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Si vous avez déjà déployé des OSD à l'aide de DriveGroups ou de l'option <option>--all-available-devices</option> alors que l'indicateur <literal>unmanaged</literal> n'était pas défini, cephadm déploiera automatiquement ces OSD une fois que vous les aurez effacés.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Ajout d'OSD à l'aide de la spécification DriveGroups</title>
   <para>
    Les <emphasis>groupes d'unités</emphasis> (« DriveGroups ») spécifient les dispositions des OSD dans la grappe Ceph. Ils sont définis dans un fichier YAML unique. Dans cette section, nous utiliserons le fichier <filename>drive_groups.yml</filename> comme exemple.
   </para>
   <para>
    Un administrateur doit spécifier manuellement un groupe d'OSD interdépendants (OSD hybrides déployés sur un mélange de disques durs et SSD) ou qui partagent des options de déploiement identiques (par exemple, même magasin d'objets, même option de chiffrement, OSD autonomes). Pour éviter de lister explicitement les périphériques, les groupes d'unités utilisent une liste d'éléments de filtre qui correspondent à quelques champs sélectionnés de rapports d'inventaire de <command>ceph-volume</command>. cephadm fournit le code qui traduit ces DriveGroups en listes de périphériques réelles pour inspection par l'utilisateur.
   </para>
   <para>
    La commande permettant d'appliquer la spécification d'OSD à la grappe est la suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Pour afficher un aperçu des opérations et tester votre application, vous pouvez utiliser l'option <option>--dry-run</option> en combinaison avec la commande <command>ceph orch apply osd</command>. Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Si la sortie de l'option <option>--dry-run</option> correspond à vos attentes, réexécutez simplement la commande sans l'option <option>--dry-run</option>.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>OSD non gérés</title>
    <para>
     Tous les périphériques de disque propres disponibles correspondant à la spécification DriveGroups sont automatiquement utilisés comme OSD une fois que vous les avez ajoutés à la grappe. Ce comportement est appelé mode <emphasis>managed</emphasis> (géré).
    </para>
    <para>
     Pour désactiver le mode <emphasis>managed</emphasis>, ajoutez la ligne <literal>unmanaged: true</literal> aux spécifications appropriées, par exemple :
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Pour faire passer des OSD déjà déployés du mode <emphasis>managed</emphasis> au mode <emphasis>unmanaged</emphasis>, ajoutez les lignes <literal>unmanaged: true</literal>, le cas échéant, au cours de la procédure décrite à la <xref linkend="modifying-cluster-configuration"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Spécification DriveGroups</title>
    <para>
     Voici un exemple de fichier de spécification DriveGroups :
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         L'option précédemment appelée « encryption » (chiffrement) dans DeepSea a été renommée « encrypted » (chiffré). Lorsque vous appliquez des DriveGroups dans SUSE Enterprise Storage 7, veillez à utiliser cette nouvelle terminologie dans votre spécification de services, sinon l'opération <command>ceph orch apply</command> échouera.
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Périphériques de disque correspondants</title>
    <para>
     Vous pouvez décrire les spécifications à l'aide des filtres suivants :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Par modèle de disque :
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Par fournisseur de disque :
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Saisissez toujours <replaceable>DISK_VENDOR_STRING</replaceable> en minuscules.
       </para>
      </tip>
      <para>
       Pour obtenir des informations sur le modèle et le fournisseur du disque, examinez la sortie de la commande suivante :
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Selon qu'un disque est rotatif ou non. Les disques SSD et NVMe ne sont pas rotatifs.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Déployez un noeud à l'aide de <emphasis>tous</emphasis> les disques disponibles pour les OSD :
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       En outre, vous pouvez limiter le nombre de disques correspondants :
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Filtrage des périphériques par taille</title>
    <para>
     Vous pouvez filtrer les périphériques de disque par leur taille, soit en fonction d'une taille précise, soit selon une plage de tailles. Le paramètre <option>size:</option> (taille :) accepte les arguments sous la forme suivante :
    </para>
    <itemizedlist>
     <listitem>
      <para>
       « 10G » : inclut les disques d'une taille exacte.
      </para>
     </listitem>
     <listitem>
      <para>
       « 10G:40G » : inclut les disques dont la taille est dans la plage.
      </para>
     </listitem>
     <listitem>
      <para>
       « :10G » : inclut les disques dont la taille est inférieure ou égale à 10 Go.
      </para>
     </listitem>
     <listitem>
      <para>
       « 40G: » : inclut les disques dont la taille est égale ou supérieure à 40 Go.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Correspondance par taille de disque</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>guillemets requis</title>
     <para>
      Lorsque vous utilisez le délimiteur « : », vous devez entourer la taille par des guillemets simples, faute de quoi le signe deux-points est interprété comme un nouveau hachage de configuration.
     </para>
    </note>
    <tip>
     <title>abréviations des unités</title>
     <para>
      Au lieu d'indiquer les tailles en gigaoctets (G), vous pouvez les spécifier en mégaoctets (M) ou téraoctets (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Exemples de DriveGroups</title>
    <para>
     Cette section comprend des exemples de différentes configurations OSD.
    </para>
    <example>
     <title>Configuration simple</title>
     <para>
      Cet exemple décrit deux noeuds avec la même configuration :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Le fichier <filename>drive_groups.yml</filename> correspondant se présentera comme suit :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Une telle configuration est simple et valide. Le problème est qu'un administrateur peut ajouter des disques de fournisseurs différents par la suite et ceux-ci ne seront pas inclus. Vous pouvez améliorer cela en limitant les filtres aux propriétés de base des unités :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      Dans l'exemple précédent, nous imposons de déclarer tous les périphériques rotatifs comme « périphériques de données » et tous les périphériques non rotatifs seront utilisés comme « périphériques partagés » (wal, db).
     </para>
     <para>
      Si vous savez que les unités de plus de 2 To seront toujours les périphériques de données plus lents, vous pouvez filtrer par taille :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Configuration avancée</title>
     <para>
      Cet exemple décrit deux configurations distinctes : 20 HDD devraient partager 2 SSD, tandis que 10 SSD devraient partager 2 NVMe.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Une telle configuration peut être définie avec deux dispositions comme suit :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Configuration avancée avec des noeuds non uniformes</title>
     <para>
      Les exemples précédents supposaient que tous les noeuds avaient les mêmes unités. Cependant, ce n'est pas toujours le cas :
     </para>
     <para>
      Noeuds 1 à 5 :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Noeuds 6 à 10 :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Vous pouvez utiliser la clé « target » dans la disposition pour cibler des noeuds spécifiques. La notation de cible Salt aide à garder les choses simples :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      suivi de
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Configuration experte</title>
     <para>
      Tous les cas précédents supposaient que les WAL et les DB utilisaient le même périphérique. Il est cependant possible également de déployer le WAL sur un périphérique dédié :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Configuration complexe (et peu probable)</title>
     <para>
      Dans la configuration suivante, nous essayons de définir :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD soutenus par 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDD soutenus par 1 SSD (db) et 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSD soutenus par 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSD autonomes (chiffrés)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD est de rechange et ne doit pas être déployé
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Le résumé des unités utilisées est le suivant :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 4 To
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 512 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Fournisseur : Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modèle : NVME-QQQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Taille : 256 Go
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      La définition des groupes d'unités sera la suivante :
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Il reste un HDD dans la mesure où le fichier est en cours d'analyse du haut vers le bas.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Suppression des OSD</title>
   <para>
    Avant de supprimer un noeud OSD de la grappe, vérifiez que cette dernière dispose de plus d'espace disque disponible que le disque OSD que vous allez supprimer. Gardez à l'esprit que la suppression d'un OSD entraîne un rééquilibrage de l'ensemble de la grappe.
   </para>
   <procedure>
    <step>
     <para>
      Identifiez l'OSD à supprimer en obtenant son ID :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Supprimez un ou plusieurs OSD de la grappe :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Par exemple :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      Vous pouvez exécuter une requête pour connaître l'état de l'opération de suppression :
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Arrêt de la suppression d'un OSD</title>
    <para>
     Après avoir planifié la suppression d'un OSD, vous pouvez interrompre l'opération si nécessaire. La commande suivante permet de rétablir l'état initial de l'OSD et de le supprimer de la file d'attente :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Remplacement d'OSD</title>
   <para>
    Pour remplacer un OSD tout en conservant son ID, exécutez la commande suivante :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    Le remplacement d'un OSD est identique à la suppression d'un OSD (pour plus de détails, reportez-vous à la <xref linkend="removing-node-osds"/>) à la différence près que l'OSD n'est pas supprimé de façon permanente de la hiérarchie CRUSH et se voit assigner un indicateur <literal>destroyed</literal> (détruit).
   </para>
   <para>
    L'indicateur <literal>destroyed</literal> (détruit) sert à déterminer les ID d'OSD qui seront réutilisés lors du prochain déploiement d'OSD. Les nouveaux disques ajoutés qui correspondent à la spécification DriveGroups (pour plus de détails, reportez-vous à la <xref linkend="drive-groups"/>) se verront assigner les ID d'OSD de leur homologue remplacé.
   </para>
   <tip>
    <para>
     L'ajout de l'option <option>--dry-run</option> ne permet pas d'exécuter le remplacement réel, mais d'afficher un aperçu des étapes qui se produiraient normalement.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Déplacement du Salt Master vers un nouveau noeud</title>

  <para>
   Si vous devez remplacer l'hôte Salt Master par un autre, procédez comme suit :
  </para>

  <procedure>
   <step>
    <para>
     Exportez la configuration de la grappe et sauvegardez le fichier JSON exporté. Pour plus de détails, reportez-vous au <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     Si l'ancien Salt Master est également le seul noeud d'administration de la grappe, déplacez manuellement les fichiers <filename>/etc/ceph/ceph.client.admin.keyring</filename> et <filename>/etc/ceph/ceph.conf</filename> vers le nouveau Salt Master.
    </para>
   </step>
   <step>
    <para>
     Arrêtez et désactivez le service <systemitem class="daemon">systemd</systemitem> Salt Master sur l'ancien noeud Salt Master :
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Si l'ancien noeud Salt Master ne se trouve plus dans la grappe, arrêtez et désactivez également le service <systemitem class="daemon">systemd</systemitem> du minion Salt :
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      N'arrêtez ou ne désactivez pas <literal>salt-minion.service</literal> si des daemons Ceph (MON, MGR, OSD, MDS, passerelle, surveillance) s'exécutent sur l'ancien noeud Salt Master.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Installez SUSE Linux Enterprise Server 15 SP2 sur le nouveau Salt Master en suivant la procédure décrite dans le <xref linkend="deploy-sles"/>.
    </para>
    <tip>
     <title>transition des minions Salt</title>
     <para>
      Pour simplifier la transition des minions Salt vers le nouveau Salt Master, retirez la clé publique Salt Master d'origine de chacun d'eux :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Installez le paquetage <package>salt-master</package> et, le cas échéant, le
     <package>salt minion,</package> sur le nouveau Salt Master.
    </para>
   </step>
   <step>
    <para>
     Installez <systemitem class="resource">ceph-salt</systemitem> sur le nouveau noeud Salt Master :
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Veillez à exécuter les trois commandes avant de continuer. Les commandes sont idempotentes ; peu importe si elles sont exécutées à plusieurs reprises.
     </para>
    </important>
   </step>
   <step>
    <para>
     Incluez le nouveau Salt Master dans la grappe comme décrit dans les <xref linkend="deploy-cephadm-cephsalt"/>, <xref linkend="deploy-cephadm-configure-minions"/> et <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Importez la configuration de grappe sauvegardée et appliquez-la :
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Renommez le <literal>minion id</literal> du Salt Master dans le fichier <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> exporté avant de l'importer.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Mise à jour des noeuds de grappe</title>

  <para>
   Gardez les noeuds de grappe Ceph à jour en appliquant régulièrement des mises à jour progressives.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Dépôts logiciels</title>
   <para>
    Avant d'appliquer des correctifs à la grappe avec les paquetages les plus récents, vérifiez que tous les noeuds de la grappe ont accès aux dépôts pertinents. Pour obtenir la liste complète des dépôts requis, reportez-vous au <xref linkend="verify-previous-upgrade-patch-repos-repos"/>.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Préparation du dépôt</title>
   <para>
    Si vous utilisez un outil de préparation (SUSE Manager, Repository Management Tool ou RMT, par exemple) qui met à disposition des dépôts logiciels pour les noeuds de la grappe, vérifiez que les phases pour les dépôts de mise à jour de SUSE Linux Enterprise Server et de SUSE Enterprise Storage sont créées au même moment.
   </para>
   <para>
    Il est vivement recommandé d'utiliser un outil de préparation pour appliquer des correctifs de niveau <literal>frozen</literal> ou <literal>staged</literal>. Cela garantit le même niveau de correctif aux noeuds qui rejoignent la grappe et à ceux qui y sont déjà en cours d'exécution. Vous évitez ainsi de devoir appliquer les correctifs les plus récents à tous les noeuds de la grappe avant que de nouveaux noeuds puissent la rejoindre.
   </para>
  </sect2>

  <sect2>
   <title>Temps d'indisponibilité des services Ceph</title>
   <para>
    Selon la configuration, les noeuds de grappe peuvent être redémarrés pendant la mise à jour. S'il existe un point d'échec unique pour des services tels qu'Object Gateway, Samba Gateway, NFS Ganesha ou iSCSI, les machines clientes peuvent être temporairement déconnectées des services dont les noeuds sont redémarrés.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Exécution de la mise à jour</title>
   <para>
    Pour mettre à jour les paquetages logiciels sur tous les noeuds de grappe vers la dernière version, exécutez la commande suivante :
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Mise à jour de Ceph</title>

  <para>
   Vous pouvez demander à cephadm de mettre à jour Ceph d'une version de correctifs vers une autre. La mise à jour automatique des services Ceph respecte l'ordre recommandé : elle commence par les instances Ceph Manager, Ceph Monitor, puis continue avec d'autres services tels que les OSD Ceph et les instances Metadata Server et Object Gateway. Chaque daemon est redémarré uniquement après que Ceph indique que la grappe restera disponible.
  </para>

  <note>
   <para>
    La procédure de mise à jour ci-dessous utilise la commande <command>ceph orch upgrade</command>. Gardez à l'esprit que les instructions suivantes expliquent comment mettre à jour votre grappe Ceph avec une version de produit (par exemple, une mise à jour de maintenance), et <emphasis>non</emphasis> comment mettre à niveau votre grappe d'une version de produit à une autre.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Démarrage de la mise à jour</title>
   <para>
    Avant de démarrer la mise à jour, vérifiez que tous les noeuds sont en ligne et que votre grappe est saine :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    Pour effectuer une mise à jour vers une version spécifique de Ceph :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    Mettez à niveau les paquetages sur les hôtes :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Surveillance de la mise à jour</title>
   <para>
    Exécutez la commande suivante pour déterminer si une mise à jour est en cours :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Pendant l'exécution de la mise à jour, vous verrez une barre de progression dans la sortie d'état de Ceph :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    Vous pouvez également consulter le journal cephadm :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Annulation d'une mise à jour</title>
   <para>
    Vous pouvez arrêter le processus de mise à jour à tout moment :
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Arrêt ou redémarrage de la grappe</title>

  <para>
   Dans certains cas, il faudra peut-être arrêter ou redémarrer l'ensemble de la grappe. Nous vous recommandons de contrôler soigneusement les dépendances des services en cours d'exécution. Les étapes suivantes fournissent un aperçu de l'arrêt et du démarrage de la grappe :
  </para>

  <procedure>
   <step>
    <para>
     Ordonnez à la grappe Ceph de ne pas marquer les OSD comme étant hors service :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Arrêtez les daemons et les noeuds dans l'ordre suivant :
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clients de stockage
      </para>
     </listitem>
     <listitem>
      <para>
       Passerelles, par exemple NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Serveur de métadonnées
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Si nécessaire, effectuez des tâches de maintenance.
    </para>
   </step>
   <step>
    <para>
     Démarrez les noeuds et les serveurs dans l'ordre inverse du processus d'arrêt :
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Serveur de métadonnées
      </para>
     </listitem>
     <listitem>
      <para>
       Passerelles, par exemple NFS Ganesha ou Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Clients de stockage
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Supprimez l'indicateur noout :
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Suppression d'une grappe Ceph entière</title>

  <para>
   La commande <command>ceph-salt purge</command> permet de supprimer l'intégralité de la grappe Ceph. Si d'autres grappes Ceph sont déployées, celle spécifiée par <command>ceph -s</command> est purgée. De cette façon, vous pouvez nettoyer l'environnement de grappe lors du test de différentes configurations.
  </para>

  <para>
   Pour éviter toute suppression accidentelle, l'orchestration vérifie si la sécurité est désengagée. Vous pouvez désengager les mesures de sécurité et supprimer la grappe Ceph en exécutant les commandes suivantes :
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
