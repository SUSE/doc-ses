<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Sistema de archivos en clúster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sí</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  En este capítulo se describen las tareas de administración que normalmente se llevan a cabo después de configurar el clúster y exportar CephFS. Si necesita más información sobre cómo configurar CephFS, consulte <xref linkend="deploy-cephadm-day2-service-mds"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montaje de CephFS</title>

  <para>
   Cuando se crea el sistema de archivos y el servidor de metadatos está activo, estará preparado para montar el sistema de archivos desde un host de cliente.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Preparación del cliente</title>
   <para>
    Si el host del cliente ejecuta SUSE Linux Enterprise 12 SP2 o una versión posterior, el sistema ya está preparado para montar CephFS sin preparativos adicionales.
   </para>
   <para>
    Si el host de cliente ejecuta SUSE Linux Enterprise 12 SP1, debe aplicar todos los parches más recientes antes de montar CephFS.
   </para>
   <para>
    En cualquier caso, todo lo necesario para montar CephFS se incluye en SUSE Linux Enterprise. El producto SUSE Enterprise Storage 7 no es necesario.
   </para>
   <para>
    Para admitir la sintaxis completa de <command>mount</command>, el paquete
    <package>ceph-common</package> (que se incluye con SUSE Linux Enterprise) debe estar instalado antes de intentar montar CephFS.
   </para>
   <important>
    <para>
     Sin el paquete <package>ceph-common</package> (y, por tanto, sin el ayudante <command>mount.ceph</command>), será necesario utilizar las direcciones IP de los monitores en lugar de sus nombres. Esto se debe a que el cliente del kernel no podrá resolver los nombres.
    </para>
    <para>
     La sintaxis básica de montaje es:
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Creación de un archivo secreto</title>
   <para>
    El clúster de Ceph se ejecuta con la autenticación activada por defecto. Debe crear un archivo donde se almacene la clave de secreto (no el anillo de claves en sí). Para obtener la clave de secreto para un usuario concreto y, a continuación, crear el archivo, haga lo siguiente:
   </para>
   <procedure>
    <title>Creación de una clave secreta</title>
    <step>
     <para>
      Muestre la clave para el usuario concreto en un archivo de anillo de claves:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copie la clave del usuario que va a utilizar el sistema de archivos CephFS montado. Normalmente, la clave tiene un aspecto similar al siguiente:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Cree un archivo con el nombre de usuario como parte del nombre de archivo; por ejemplo, <filename>/etc/ceph/admin.secret</filename> para el usuario <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Pegue el valor de la clave en el archivo creado en el paso anterior.
     </para>
    </step>
    <step>
     <para>
      Defina los derechos de acceso adecuados en el archivo. El usuario debe ser el único que pueda leer el archivo: los demás usuarios no deben tener ningún derecho de acceso.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montaje de CephFS</title>
   <para>
    Puede montar CephFS con el comando <command>mount</command>. Es preciso especificar el nombre de host o la dirección IP del monitor. Dado que la autenticación <systemitem>cephx</systemitem> está habilitada por defecto en SUSE Enterprise Storage, debe especificar también un nombre de usuario y su secreto relacionado:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Como el comando anterior permanece en el historial de shell, un enfoque más seguro es leer el secreto desde un archivo:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Tenga en cuenta que el archivo de secreto solo debe contener el secreto del anillo de claves actual. En nuestro ejemplo, el archivo solo contendrá la línea siguiente:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>especificación de varios monitores</title>
    <para>
     Resulta recomendable especificar varios monitores separados por comas en la línea del comando <command>mount</command> por si un monitor se desactiva en algún momento del montaje. Las direcciones de los monitores tiene el formato <literal>host[:puerto]</literal>. Si no se especifica el puerto, se utiliza por defecto el 6789.
    </para>
   </tip>
   <para>
    Cree el punto de montaje en el host local:
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Monte CephFS:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Es posible especificar un subdirectorio <filename>subdir</filename> si se va a montar un subconjunto del sistema de archivos:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Puede especificar más de un host de monitor en el comando <command>mount</command>:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>acceso de lectura al directorio raíz</title>
    <para>
     Si se usan clientes con restricciones de vía, es preciso incluir acceso de lectura al directorio raíz en los permisos del servidor de metadatos. Por ejemplo, un anillo de claves puede tener este aspecto:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     La parte <literal>allow r path=/</literal> significa que los clientes que tienen vías restringidas podrán ver el volumen raíz, pero no escribir en él. Esto puede ser un problema para los casos en los que se requiera aislamiento completo.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Desmontaje de CephFS</title>

  <para>
   Para desmontar CephFS, utilice el comando <command>umount</command>:
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>Montaje de CephFS en <filename>/etc/fstab</filename></title>

  <para>
   Para montar CephFS automáticamente durante el inicio del cliente, inserte la línea correspondiente en la tabla de su sistema de archivos <filename>/etc/fstab</filename>:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Varios daemons de servidor de metadatos activos (MDS activo-activo)</title>

  <para>
   CephFS está configurado por defecto para un solo daemon de servidor de metadatos activo. Para ampliar el rendimiento de los metadatos en sistemas a gran escala, puede habilitar varios daemons de servidor de metadatos activos: compartirán la carga de trabajo de los metadatos entre sí.
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>Uso de servidor de metadatos activo-activo</title>
   <para>
    Puede ser útil usar varios daemons de servidor de metadatos activos si el rendimiento de los metadatos sufre un cuello de botella en el servidor de metadatos único por defecto.
   </para>
   <para>
    Añadir más daemons no mejora el rendimiento de todos los tipos de cargas de trabajo. Por ejemplo, una única aplicación que se ejecute en un único cliente no se beneficiará de un número mayor de daemons de servidor de metadatos; a menos que la aplicación lleve a cabo muchas operaciones de metadatos en paralelo.
   </para>
   <para>
    Las cargas de trabajo que suelen beneficiarse de que haya un número mayor de daemons de servidor de metadatos activos son las que tienen muchos clientes, quizás funcionando en muchos directorios diferentes.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Aumento del tamaño del clúster activo del servidor de metadatos</title>
   <para>
    Cada sistema de archivos CephFS tiene un valor <option>max_mds</option> que controla el número de rangos que se van a crear. El número real de rangos en el sistema de archivos solo aumentará si hay un daemon de repuesto disponible para hacerse cargo del nuevo rango. Por ejemplo, si hay solo un daemon de servidor de metadatos en ejecución y en <option>max_mds</option> se define el valor 2, no se creará un segundo rango.
   </para>
   <para>
    En el ejemplo siguiente, la opción <option>max_mds</option> se define con el valor 2 para crear un nuevo rango aparte del rango por defecto. Para ver los cambios, ejecute <command>ceph status</command> antes y, después de definir <option>max_mds</option>, observe la línea que contiene <literal>fsmap</literal>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    El rango recién creado (1) pasa por el estado de "creación" y entra en su estado "activo".
   </para>
   <important>
    <title>daemons de reserva</title>
    <para>
     Incluso con varios daemons de servidor de metadatos activos, un sistema de alta disponibilidad sigue necesitando daemons en espera para que entren en acción si falla alguno de los servidores en los que se ejecuta un daemon activo.
    </para>
    <para>
     En consecuencia, el número máximo práctico de <option>max_mds</option> para sistemas de alta disponibilidad es uno menos que el número total de servidores de metadatos del sistema. Para seguir estando disponible en caso de que varios servidores fallen, aumente el número de daemons en espera del sistema para que sea igual al número de servidores que se pueda admitir que fallen.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Reducción del número de rangos</title>
   <para>
    Todos los rangos, incluidos los que se van a eliminar, deben estar activos en primer lugar. Esto significa que debe tener al menos el número de <option>max_mds</option> de daemons de servidor de metadatos disponibles.
   </para>
   <para>
    En primer lugar, defina en <option>max_mds</option> un número inferior. Por ejemplo, vuelva a tener un único servidor de metadatos activo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Fijación manual de árboles de directorio a un rango</title>
   <para>
    En las configuraciones con varios servidores de metadatos activos, se ejecuta un mecanismo de equilibrio que funciona para distribuir la carga de los metadatos de forma uniforme en el clúster. Esto suele ser suficientemente para la mayoría de los usuarios pero, en ocasiones, es aconsejable sustituir el mecanismo de equilibrio dinámico por asignaciones explícitas de metadatos para rangos concreto. De esta forma, se permite que el administrador o los usuarios puedan distribuir uniformemente la carga de la aplicación o limitar el impacto de las peticiones de metadatos de los usuarios en todo el clúster.
   </para>
   <para>
    El mecanismo proporcionado para este propósito se denomina "fijación de exportación". Se trata de un atributo extendido de los directorios. El nombre de este atributo extendido es <literal>ceph.dir.pin</literal>. Los usuarios pueden definir este atributo mediante comandos estándar:
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    El valor (<option>- v</option>) del atributo extendido es el rango al que se debe asignar el subárbol de directorio. Un valor por defecto de -1 indica que el directorio no está fijado.
   </para>
   <para>
    La fijación de exportación de directorio se hereda de su padre más próximo que cuente con una fijación de exportación definido. Por lo tanto, definir la fijación de exportación en un directorio, afecta a todos sus hijos. Sin embargo, la fijación padre se puede sustituir definiendo fijaciones de exportación en el directorio hijo. Por ejemplo:
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Gestión del failover</title>

  <para>
   Si un daemon de servidor de metadatos deja de comunicarse con el monitor, este espera el número de segundos definido en <option>mds_beacon_grace</option> (por defecto, 15 segundos) antes de marcar el daemon como <emphasis>retrasado</emphasis>. Es posible configurar uno o varios daemons en espera que se harán cargo del trabajo durante un failover del daemon de servidor de metadatos.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configuración reserva-respuesta</title>
   <para>
    Cada sistema de archivos CephFS puede configurarse para añadir daemons de reserva-respuesta. Estos daemons de reserva siguen el registro de metadatos del MDS activo para reducir el tiempo de failover en caso de que el MDS activo deje de estar disponible. Cada MDS activo puede tener solo un daemon de reserva-respuesta siguiéndolo.
   </para>
   <para>
    Para configurar un daemon de reserva-respuesta en un sistema de archivos, ejecute el comando siguiente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    Cuando se definen los monitores, se asignan daemons de reserva-respuesta disponibles para seguir los MDS activos en ese sistema de archivos.
   </para>
   <para>
    Si un MDS ha entrado en estado de reserva-respuesta, solo se utilizará como daemon de reserva para el rango al que sigue. Si otro rango falla, este daemon de reserva-respuesta no se utilizará como sustituto, aunque no haya ningún otro daemon de reserva disponible. Por este motivo, se recomienda que si se utiliza este sistema, todos los MDS activos deben tener un daemon de reserva-respuesta.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Configuración de cuotas de CephFS</title>

  <para>
   Puede establecer cuotas en cualquier subdirectorio del sistema de archivos de Ceph. La cuota restringe el número de <emphasis role="bold">bytes</emphasis> o de <emphasis role="bold">archivos</emphasis> almacenados debajo del punto especificado en la jerarquía de directorios.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limitaciones de cuotas de CephFS</title>
   <para>
    El uso de cuotas con CephFS tiene las siguientes limitaciones:
   </para>
   <variablelist>
    <varlistentry>
     <term>Las cuotas son cooperativas y no compiten.</term>
     <listitem>
      <para>
       Las cuotas de Ceph dependen de que el cliente que está montando el sistema de archivos deje de escribir en él cuando se alcance un límite. La parte del servidor no puede impedir que un cliente malintencionado escriba tantos datos como necesite. No utilice cuotas para evitar que se rellene el sistema de archivos en entornos en los que los clientes no sean de total confianza.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Las cuotas son imprecisas.</term>
     <listitem>
      <para>
       Los procesos que se escriben en el sistema de archivos se detienen poco después de que se alcance el límite de cuota. Inevitablemente se les permitirá escribir cierta cantidad de datos por encima del límite configurado. Los escritores del cliente se detendrán décimas de segundos después de superar el límite configurado.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Las cuotas se implementan en el cliente del kernel a partir de la versión 4.17.</term>
     <listitem>
      <para>
       Las cuotas son compatibles con el cliente de espacio de usuario (libcephfs, ceph-fuse). Los clientes del kernel de Linux de la versión 4.17 y superiores admiten cuotas de CephFS en clústeres de SUSE Enterprise Storage 7. Los clientes del kernel (incluso de las versiones recientes) no podrán controlar las cuotas en clústeres más antiguos, incluso si son capaces de establecer los atributos extendidos de las cuotas. Los kernel SLE12-SP3 (y versiones posteriores) ya incluyen las adaptaciones necesarias para gestionar cuotas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configure las cuotas cuidadosamente cuando se utilicen con restricciones de montaje basadas en vías.</term>
     <listitem>
      <para>
       El cliente necesita tener acceso al inodo del directorio en el cual se configuran las cuotas para aplicarlas. Si el cliente tiene acceso restringido a una vía específica (por ejemplo <filename>/home/user</filename>) según la capacidad del MDS y se configura una cuota en un directorio antecesor al que no tienen acceso (<filename>/home</filename>), el cliente no la aplicará. Si utiliza restricciones de acceso basadas en vías, asegúrese de configurar la cuota en el directorio al que puede acceder el cliente (por ejemplo, <filename>/home/user</filename> o <filename>/home/user/quota_dir</filename>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configuración de cuotas de CephFS</title>
   <para>
    Puede configurar cuotas de CephFS mediante atributos extendidos virtuales:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configura un límite de <emphasis>archivos</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configura un límite de<emphasis>bytes</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Si los atributos aparecen en un inodo de directorio, allí se configura una cuota. Si no están presentes, no se establece ninguna cuota en ese directorio (aunque se podría configurar una en un directorio padre).
   </para>
   <para>
    Para establecer una cuota de 100 MB, ejecute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Para establecer una cuota de 10.000 archivos, ejecute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Para ver la configuración de cuota, ejecute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>cuota no configurada</title>
    <para>
     Si el valor del atributo extendido es "0", no se establece la cuota.
    </para>
   </note>
   <para>
    Para eliminar una cuota, ejecute:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Gestión de instantáneas de CephFS</title>

  <para>
   Las instantáneas de CephFS crean una vista de solo lectura del sistema de archivos del momento en que se toman. Puede crear una instantánea en cualquier directorio. La instantánea cubrirá todos los datos del sistema de archivos del directorio especificado. Después de crear una instantánea, los datos almacenados en búfer se vacían de forma asincrónica desde varios clientes. Como resultado, crear una instantánea es muy rápido.
  </para>

  <important>
   <title>varios sistemas de archivos</title>
   <para>
    Si tiene varios sistemas de archivos CephFS que comparten un único repositorio (mediante espacios de nombres), sus instantáneas tendrán un conflicto y si se suprime una instantánea, faltarán datos de archivo en otras instantáneas que compartan el mismo repositorio.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Creación de instantáneas</title>
   <para>
    La función de instantáneas de CephFS está habilitada por defecto en los nuevos sistemas de archivos. Para habilitarla en sistemas de archivos existentes, ejecute:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Después de habilitar las instantáneas, todos los directorios de CephFS tendrán un subdirectorio <filename>.snap</filename> especial.
   </para>
   <note>
    <para>
     Este es un subdirectorio <emphasis>virtual.</emphasis> No aparece en la lista de directorios del directorio padre, pero el nombre <filename>.snap</filename> no se puede utilizar como nombre de archivo ni directorio. Para acceder al directorio <filename>.snap</filename> es necesario acceder a él explícitamente, por ejemplo:
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>limitación de los clientes del kernel</title>
    <para>
     Los clientes del kernel de CephFS tienen una limitación: no pueden gestionar más de 400 instantáneas en un sistema de archivos. El número de instantáneas siempre debe mantenerse por debajo de este límite, independientemente del cliente que se esté utilizando. Si se utilizan clientes de CephFS más antiguos, como SLE12-SP3, tenga en cuenta que tener más 400 instantáneas es perjudicial para las operaciones, ya que el cliente se detendrá por fallo.
    </para>
   </important>
   <tip>
    <title>nombre del subdirectorio de instantáneas personalizado</title>
    <para>
     Puede configurar un nombre diferente para el subdirectorio de instantáneas definiendo el valor <option>client snapdir</option>.
    </para>
   </tip>
   <para>
    Para crear una instantánea, cree un subdirectorio en el directorio <filename>.snap</filename> con un nombre personalizado. Por ejemplo, para crear una instantánea del directorio <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, ejecute:
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Supresión de instantáneas</title>
   <para>
    Para suprimir una instantánea, elimine su subdirectorio dentro del directorio <filename>.snap</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
