<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Tareas operativas</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sí</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modificación de la configuración del clúster</title>

  <para>
   Para modificar la configuración de un clúster de Ceph existente, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Exporte la configuración actual del clúster a un archivo:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Edite el archivo con la configuración y actualice las líneas pertinentes. Encontrará ejemplos de especificaciones en el <xref linkend="deploy-cephadm-day2"/> y en la <xref linkend="drive-groups"/>.
    </para>
   </step>
   <step>
    <para>
     Aplique la nueva configuración:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Adición de nodos</title>

  <para>
   Para añadir un nodo nuevo a un clúster de Ceph, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Instale SUSE Linux Enterprise Server y SUSE Enterprise Storage en el nuevo host. Consulte el <xref linkend="deploy-os"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     Configure el host como un minion de Salt de un master de Salt ya existente. Consulte el <xref linkend="deploy-salt"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     Añada el nuevo host a <systemitem class="resource">ceph-salt</systemitem> y haga que cephadm lo reconozca, por ejemplo:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Consulte el <xref linkend="deploy-cephadm-configure-minions"/> para obtener más información.
    </para>
   </step>
   <step>
    <para>
     Verifique que el nodo se ha añadido a <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Aplique la configuración al nuevo host del clúster:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verifique que el host recién añadido pertenece ahora al entorno de cephadm:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Eliminación de nodos</title>

  <tip>
   <title>eliminación de OSD</title>
   <para>
    Si el nodo que va a eliminar ejecuta OSD, elimínelos primero y compruebe que no hay ningún OSD en ejecución en ese nodo. Consulte la <xref linkend="removing-node-osds"/> para obtener más información sobre la eliminación de OSD.
   </para>
  </tip>

  <para>
   Para eliminar un nodo de un clúster, haga lo siguiente:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Para todos los tipos de servicio de Ceph, excepto <literal>node-exporter</literal> y <literal>crash</literal>, elimine el nombre de host del nodo del archivo de especificación de colocación del clúster (por ejemplo, <filename>cluster.yml</filename>). Consulte <xref linkend="cephadm-service-and-placement-specs"/> para obtener más información. Por ejemplo, si va a eliminar el host denominado <literal>ses-min2</literal>, elimine todas las apariciones de <literal>- ses-min2</literal> de todas las secciones <literal>placement:</literal>.
    </para>
    <para>
     Actualice
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     a
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Aplique los cambios al archivo de configuración:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Elimine el nodo del entorno de cephadm:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Si el nodo ejecuta los servicios <literal>crash.osd.1</literal> y <literal>crash.osd.2</literal>, elimínelos ejecutando el siguiente comando en el host:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Por ejemplo:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Elimine todas las funciones del minion que desee suprimir:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Si el minion que desea suprimir es el minion de arranque, también debe eliminar la función bootstrap:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     Después de eliminar todos los OSD de un solo host, elimine el host del mapa de CRUSH:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      El nombre del depósito debe ser el mismo que el nombre del host.
     </para>
    </note>
   </step>
   <step>
    <para>
     Ahora puede eliminar el minion del clúster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    En caso de que se produzca un fallo y el minion que está intentando eliminar se encuentre apagado permanentemente, deberá eliminar el nodo del master de Salt:
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    A continuación, elimine manualmente el nodo de <filename><replaceable>raíz_pilar</replaceable>/ceph-salt.sls</filename>. Normalmente se encuentra en <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>Gestión de OSD</title>

  <para>
   En esta sección se describe cómo añadir, borrar o eliminar OSD en un clúster de Ceph.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Listado de dispositivos de disco</title>
   <para>
    Para identificar los dispositivos de disco usados y sin usar en todos los nodos del clúster, muéstrelos ejecutando el comando siguiente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Borrado de dispositivos de disco</title>
   <para>
    Para reutilizar un dispositivo de disco, primero debe borrarlo (comando <emphasis>zap</emphasis>):
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Si ha distribuido previamente los OSD mediante DriveGroups o con la opción <option>--all-available-devices</option> sin que se haya definido el indicador <literal>unmanaged</literal>, cephadm distribuirá estos OSD automáticamente después de borrarlos.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Adición de OSD mediante la especificación DriveGroups</title>
   <para>
    <emphasis>DriveGroups</emphasis> especifica los diseños de los OSD del clúster de Ceph. Se definen en un único archivo YAML. En esta sección, utilizaremos <filename>drive_groups.yml</filename> como ejemplo.
   </para>
   <para>
    Un administrador debe especificar manualmente un grupo de OSD que estén interrelacionados (OSD híbridos que se distribuyen en una mezcla de unidades HDD y SDD) o que compartan opciones de distribución idénticas (por ejemplo, el mismo almacén de objetos, la misma opción de cifrado y varios OSD independientes). Para no tener que mostrar explícitamente los dispositivos, DriveGroups utiliza una lista de elementos de filtro que corresponden a algunos campos seleccionados de los informes de inventario de <command>ceph-volume</command>. cephadm proporciona código que traduce esta especificación DriveGroups en listas de dispositivos reales para que el usuario las pueda inspeccionar.
   </para>
   <para>
    El comando para aplicar la especificación de OSD al clúster es:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Para ver una vista previa de las acciones y probar la aplicación, puede utilizar la opción <option>--dry-run</option> junto con el comando <command>ceph orch apply osd</command>. Por ejemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Si el resultado de <option>--dry-run</option> cumple sus expectativas, solo tiene que volver a ejecutar el comando sin la opción <option>--dry-run</option>.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>OSD no gestionados</title>
    <para>
     Todos los dispositivos de disco limpios disponibles que coincidan con la especificación DriveGroups se utilizarán automáticamente como OSD después de añadirlos al clúster. Este comportamiento se denomina modo <emphasis>gestionado.</emphasis>
    </para>
    <para>
     Para inhabilitar el modo <emphasis>gestionado,</emphasis> añada la línea <literal>unmanaged: true</literal> a las especificaciones relevantes, por ejemplo:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Para cambiar los OSD ya distribuidos del modo <emphasis>gestionado</emphasis> al modo <emphasis>no gestionado,</emphasis> añada las líneas <literal>unmanaged: true</literal> cuando corresponda durante el procedimiento descrito en la <xref linkend="modifying-cluster-configuration"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Especificación DriveGroups</title>
    <para>
     A continuación se muestra un ejemplo de archivo de especificación DriveGroups:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         La opción denominada anteriormente "encryption" en DeepSea ahora se llama "encrypted". Al aplicar DriveGroups en SUSE Enterprise Storage 7, asegúrese de utilizar esta nueva terminología en la especificación del servicio; de lo contrario, la operación <command>ceph orch apply</command> fallará.
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Filtrado de dispositivos de disco</title>
    <para>
     Puede describir la especificación utilizando los filtros siguientes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Por modelo del disco:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       Por proveedor del disco:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Escriba siempre <replaceable>DISK_VENDOR_STRING</replaceable> en minúsculas.
       </para>
      </tip>
      <para>
       Para obtener detalles sobre el modelo de disco y el proveedor, examine el resultado del siguiente comando:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Indica si un disco es giratorio o no. Las unidades SSD y NVMe no son giratorias.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Distribuya un nodo utilizando <emphasis>todas</emphasis> las unidades disponibles para los OSD:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       También puede limitar el número de discos que coinciden:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Filtrado de dispositivos por tamaño</title>
    <para>
     Es posible filtrar los dispositivos de disco por su tamaño, ya sea por un tamaño exacto o por un intervalo de tamaños. El parámetro <option>size:</option> acepta argumentos con el formato siguiente:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G': incluye discos de un tamaño exacto.
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G': incluye discos cuyo tamaño está dentro del intervalo indicado.
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G': incluye discos de 10 GB o menos.
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:': incluye discos de 40 GB o más.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Filtrado por tamaño de disco</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>comillas obligatorias</title>
     <para>
      Si se usa el delimitador ":", debe incluir el tamaño entre comillas simples, de lo contrario el signo ":" se interpretará como un nuevo hash de configuración.
     </para>
    </note>
    <tip>
     <title>accesos directos a unidades</title>
     <para>
      En lugar de gigabytes (G), puede especificar el tamaño en megabytes (M) o en terabytes (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Ejemplos de DriveGroups</title>
    <para>
     Esta sección incluye ejemplos de diferentes configuraciones de OSD.
    </para>
    <example>
     <title>configuración sencilla</title>
     <para>
      Este ejemplo describe dos nodos con la misma configuración:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 discos duros
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      El archivo <filename>drive_groups.yml</filename> correspondiente será el siguiente:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Esta configuración es sencilla y válida. El problema es que un administrador puede añadir discos de diferentes proveedores en el futuro, y estos no se incluirán. Puede mejorarla reduciendo los filtros en las propiedades principales de las unidades:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      En el ejemplo anterior, estamos imponiendo que se declaren todos los dispositivos giratorios como "dispositivos de datos" y todos los dispositivos no giratorios se utilizarán como "dispositivos compartidos" (wal, db).
     </para>
     <para>
      Si sabe que las unidades de más de 2 TB siempre serán los dispositivos de datos más lentos, puede filtrar por tamaño:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>configuración avanzada</title>
     <para>
      En este ejemplo se describen dos configuraciones distintas: 20 discos duros deben compartir 2 unidades SSD, mientras que 10 unidades SSD deben compartir 2 NVMe.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 discos duros
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Esta configuración se puede definir con dos diseños de la siguiente manera:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>configuración avanzada con nodos no uniformes</title>
     <para>
      En los ejemplos anteriores se suponía que todos los nodos tienen las mismas unidades. Sin embargo, eso no siempre es así:
     </para>
     <para>
      Nodos 1-5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 discos duros
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodos 6-10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Puede utilizar la clave "target" en el diseño para asignar nodos específicos a un destino. La notación del destino de Salt ayuda a simplificar las cosas:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      seguido de
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>configuración para expertos</title>
     <para>
      En todos los casos anteriores se presuponía que los WAL y DB usaban el mismo dispositivo. Sin embargo, también es posible distribuir el WAL en un dispositivo dedicado:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 discos duros
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>configuración compleja (e improbable)</title>
     <para>
      En la siguiente configuración, tratamos de definir lo siguiente:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 discos duros respaldados por 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 discos duros respaldados por 1 SSD (db) y 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 discos SSD respaldados por 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 discos SSD independientes (cifrados)
       </para>
      </listitem>
      <listitem>
       <para>
        1 disco duro es de repuesto y no debe distribuirse
       </para>
      </listitem>
     </itemizedlist>
     <para>
      El resumen de las unidades usadas es el siguiente:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 discos duros
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 discos SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Proveedor: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modelo: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Tamaño: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      La definición de DriveGroups será la siguiente:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Se conservará un disco duro porque el archivo se está analizando de arriba abajo.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Eliminación de OSD</title>
   <para>
    Antes de eliminar un nodo de OSD del clúster, verifique que el clúster tiene más espacio libre en disco que el disco del OSD que va a eliminar. Tenga en cuenta que, si se elimina un OSD, todo el clúster se reequilibra.
   </para>
   <procedure>
    <step>
     <para>
      Identifique qué OSD desea eliminar obteniendo su ID:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Elimine uno o varios OSD del clúster:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Por ejemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      Puede consultar el estado de la operación de eliminación:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Detención de la eliminación de OSD</title>
    <para>
     Después de programar la eliminación de un OSD, puede detener la eliminación si es necesario. El siguiente comando restablecerá el estado inicial del OSD y lo eliminará de la cola:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Sustitución de OSD</title>
   <para>
    Para sustituir un OSD conservando su ID, ejecute:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    Sustituir un OSD es igual que eliminar un OSD (consulte la <xref linkend="removing-node-osds"/> para obtener más información), con la excepción de que el OSD no se elimina permanentemente de la jerarquía de CRUSH y se le asigna un indicador <literal>destroyed</literal>.
   </para>
   <para>
    El indicador <literal>destroyed</literal> se utiliza para determinar los ID de OSD que se reutilizarán durante la siguiente distribución de OSD. A los discos recién añadidos que coincidan con la especificación DriveGroups (consulte la <xref linkend="drive-groups"/> para obtener más información) se les asignarán los ID de OSD de su equivalente sustituido.
   </para>
   <tip>
    <para>
     Al añadir la opción <option>--dry-run</option> no se ejecutará la sustitución real, pero se mostrarán los pasos que se llevarían a cabo normalmente.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Traslado del master de Salt a un nodo nuevo</title>

  <para>
   Si necesita sustituir el host del master de Salt por uno nuevo, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Exporte la configuración del clúster y realice una copia de seguridad del archivo JSON exportado. Más detalles en el <xref linkend="deploy-cephadm-configure-export"/>
    </para>
   </step>
   <step>
    <para>
     Si el master de Salt antiguo es también el único nodo de administración del clúster, traslade manualmente <filename>/etc/ceph/ceph.client.admin.keyring</filename> y <filename>/etc/ceph/ceph.conf</filename> al nuevo master de Salt.
    </para>
   </step>
   <step>
    <para>
     Detenga e inhabilite el servicio <systemitem class="daemon">systemd</systemitem> del master de Salt en el nodo del master de Salt antiguo:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Si el nodo del master de Salt antiguo ya no está en el clúster, detenga e inhabilite también el servicio <systemitem class="daemon">systemd</systemitem> del minion de Salt:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      no detenga ni inhabilite <literal>salt-minion.service</literal> si el nodo del master de Salt antiguo tiene algún daemon de Ceph (MON, MGR, OSD, MDS, pasarela, supervisión) en ejecución.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Instale SUSE Linux Enterprise Server 15 SP2 en el nuevo master de Salt siguiendo el procedimiento descrito en el <xref linkend="deploy-os"/>.
    </para>
    <tip>
     <title>transición de los minions de Salt</title>
     <para>
      Para simplificar la transición de los minions de Salt al nuevo master de Salt, elimine la clave pública del master de Salt original de cada minion:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Instale el paquete <package>salt-master</package> y, si procede, el paquete
     <package>salt-minion</package> en el nuevo master de Salt.
    </para>
   </step>
   <step>
    <para>
     Instale <systemitem class="resource">ceph-salt</systemitem> en el nuevo nodo del master de Salt:
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Asegúrese de ejecutar los tres comandos antes de continuar. Los comandos son idempotentes; no importa si se repiten.
     </para>
    </important>
   </step>
   <step>
    <para>
     Incluya el nuevo master de Salt en el clúster, como se describe en el <xref linkend="deploy-cephadm-cephsalt"/>, el <xref linkend="deploy-cephadm-configure-minions"/> y el <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Importe la copia de seguridad de la configuración del clúster y aplíquela:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Cambie el nombre del minion del master de Salt <literal>minion id</literal> del archivo <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> exportado antes de importarlo.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Actualización de los nodos del clúster</title>

  <para>
   Para mantener actualizados los nodos del clúster de Ceph, aplique actualizaciones periódicas.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Repositorios de software</title>
   <para>
    Antes de aplicar al clúster los paquetes de software más recientes como parches, compruebe que todos los nodos del clúster tienen acceso a los repositorios relevantes. Consulte el <xref linkend="verify-previous-upgrade-patch-repos-repos"/> para obtener una lista completa de los repositorios necesarios.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>División en etapas del repositorio</title>
   <para>
    Si utiliza una herramienta de división en etapas (por ejemplo, SUSE Manager, o la herramienta de gestión de repositorios o RMT) que provea repositorios de software a los nodos del clúster, verifique que las etapas de los repositorios "de actualizaciones" de SUSE Linux Enterprise Server y SUSE Enterprise Storage se crean en el mismo momento.
   </para>
   <para>
    Se recomienda encarecidamente utilizar una herramienta de división en etapas para aplicar parches con los niveles de parches <literal>inmovilizados</literal> o <literal>preconfigurados</literal>. Esto garantiza que los nuevos nodos que se unen al clúster tengan el mismo nivel de parche que los nodos que ya se ejecutan en el clúster. De ese modo, no será necesario aplicar los parches más recientes a todos los nodos del clúster antes de que los nuevos nodos se puedan unir al clúster.
   </para>
  </sect2>

  <sect2>
   <title>Tiempo de inactividad de servicios de Ceph</title>
   <para>
    Dependiendo de la configuración, los nodos del clúster podrían reiniciarse durante la actualización. Si hay un punto único de error para servicios como Object Gateway, Samba Gateway, NFS Ganesha o iSCSI, los equipos cliente podrían desconectarse temporalmente de los servicios cuyos nodos se van a reiniciar.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Ejecución de la actualización</title>
   <para>
    Para actualizar los paquetes de software de todos los nodos del clúster a la versión más reciente, ejecute el comando siguiente:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Actualización de Ceph</title>

  <para>
   Puede indicar a cephadm que actualice Ceph a partir de una versión de corrección de errores a otra. La actualización automatizada de los servicios de Ceph respeta el orden recomendado: comienza con Ceph Managers, después Ceph Monitors y continúa con otros servicios como Ceph OSD, servidores de metadatos y Object Gateways. Cada daemon solo se reinicia después de que Ceph indique que el clúster seguirá estando disponible.
  </para>

  <note>
   <para>
    El siguiente procedimiento de actualización utiliza el comando <command>ceph orch upgrade</command>. Tenga en cuenta que las instrucciones siguientes detallan cómo actualizar el clúster de Ceph con una versión de producto (por ejemplo, una actualización de mantenimiento), pero <emphasis>no</emphasis> proporcionan instrucciones sobre cómo actualizar el clúster de una versión de producto a otra.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Inicio de la actualización</title>
   <para>
    Antes de iniciar la actualización, compruebe que todos los nodos están en línea y que el clúster funciona correctamente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    Para actualizar a una versión de Ceph específica:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Por ejemplo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    Actualice los paquetes en los hosts:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Supervisión de la actualización</title>
   <para>
    Ejecute el comando siguiente para determinar si hay una actualización en curso:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Mientras la actualización está en curso, verá una barra de progreso en el estado de Ceph:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    También puede ver el registro de cephadm:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Cancelación de una actualización</title>
   <para>
    Puede detener el proceso de actualización en cualquier momento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Detención o reinicio del clúster</title>

  <para>
   En algunos casos, puede ser necesario detener o reiniciar el clúster completo. Recomendamos comprobar atentamente las dependencias de los servicios en ejecución. Los siguientes pasos se pueden usar como esquema de inicio y detención del clúster:
  </para>

  <procedure>
   <step>
    <para>
     Indique al clúster de Ceph que no marque los OSD como "out":
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Detenga los daemons y los nodos en el siguiente orden:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Clientes de almacenamiento
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, por ejemplo, NFS Ganesha u Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de metadatos
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Si es necesario, realice las tareas de mantenimiento.
    </para>
   </step>
   <step>
    <para>
     Inicie los nodos y los servidores en el orden inverso al del proceso de apagado:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Servidor de metadatos
      </para>
     </listitem>
     <listitem>
      <para>
       Gateways, por ejemplo, NFS Ganesha u Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Clientes de almacenamiento
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Elimine el indicador de noout:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Eliminación de un clúster de Ceph completo</title>

  <para>
   El comando <command>ceph-salt purge</command> elimina todo el clúster de Ceph. Si hay más clústeres de Ceph distribuidos, se limpia el notificado por <command>ceph -s</command>. De esta manera, puede limpiar el entorno del clúster cuando pruebe diferentes configuraciones.
  </para>

  <para>
   Para evitar la supresión accidental, la orquestación comprueba si la seguridad está desactivada. Puede desactivar las medidas de seguridad y eliminar el clúster de Ceph ejecutando:
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
