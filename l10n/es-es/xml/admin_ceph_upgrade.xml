<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>Actualización desde una versión anterior</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sí</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  En este capítulo se presentan los pasos necesarios para actualizar SUSE Enterprise Storage 6 a la versión 7.
 </para>
 <para>
  La actualización incluye las siguientes tareas:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Actualización de Ceph Nautilus a Octopus.
   </para>
  </listitem>
  <listitem>
   <para>
    Cambio de la instalación y ejecución de Ceph a través de paquetes RPM a la ejecución en contenedores.
   </para>
  </listitem>
  <listitem>
   <para>
    Eliminación completa de DeepSea y sustitución por <systemitem class="resource">ceph-salt</systemitem> y cephadm.
   </para>
  </listitem>
 </itemizedlist>
 <warning>
  <para>
   La información sobre actualización de este capítulo <emphasis>solo</emphasis> se aplica a las actualizaciones de DeepSea a cephadm. No intente seguir estas instrucciones si desea distribuir SUSE Enterprise Storage en la plataforma SUSE CaaS.
  </para>
 </warning>
 <important>
  <para>
   No se admite la actualización desde versiones de SUSE Enterprise Storage anteriores a la 6. En primer lugar, debe actualizar a la versión más reciente de SUSE Enterprise Storage 6 y, a continuación, siga los pasos de este capítulo.
  </para>
 </important>
 <sect1 xml:id="before-upgrade">
  <title>Antes de actualizar</title>

  <para>
   Las siguientes tareas <emphasis>deben</emphasis> completarse antes de iniciar la actualización. Puede hacerlo en cualquier momento durante la vida útil de SUSE Enterprise Storage 6.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     La migración del OSD de FileStore a BlueStore <emphasis>debe</emphasis> realizarse antes de la actualización, ya que FileStore no es compatible con SUSE Enterprise Storage 7. Encontrará más información sobre BlueStore y sobre cómo migrar desde FileStore en <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#filestore2bluestore"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Si está ejecutando un clúster antiguo que aún utiliza los OSD de <literal>ceph-disk</literal>, <emphasis>debe</emphasis> cambiar a <literal>ceph-volume</literal> antes de la actualización. Más detalles en <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-deployment/#upgrade-osd-deployment"/>
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="upgrade-consider-points">
   <title>Puntos que se deben tener en cuenta</title>
   <para>
    Antes de actualizar, lea atentamente las secciones siguientes para asegurarse de que comprende todas las tareas que deben ejecutarse.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Lea las notas de la versión.</emphasis> En ellas encontrará información adicional sobre los cambios realizados desde la versión previa de SUSE Enterprise Storage. Consulte las notas de versión para comprobar lo siguiente:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Si el hardware necesita consideraciones especiales.
       </para>
      </listitem>
      <listitem>
       <para>
        Si los paquetes de software usados han cambiado de forma significativa.
       </para>
      </listitem>
      <listitem>
       <para>
        Si es necesario tomar precauciones especiales para la instalación.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Las notas de la versión también proporcionan información que no pudo publicarse en el manual a tiempo y notas acerca de problemas conocidos.
     </para>
     <para>
      Encontrará las notas de la versión de SES 7 en línea en <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
     <para>
      Asimismo, después de instalar el paquete
      <package>release-notes-ses</package> desde el repositorio de SES 7, encontrará las notas de la versión en el directorio <filename>/usr/share/doc/release-notes</filename> o en línea en <link xlink:href="https://www.suse.com/releasenotes/"/>.
     </para>
    </listitem>
    <listitem>
     <para>
      Lea el <xref linkend="deploy-cephadm"/> para familiarizarse con <systemitem class="resource">ceph-salt</systemitem> y Ceph Orchestrator y, en particular, con la información sobre las especificaciones de servicio.
     </para>
    </listitem>
    <listitem>
     <para>
      La actualización del clúster puede tardar mucho tiempo, aproximadamente el que se tarda en actualizar un equipo multiplicado por el número de nodos del clúster.
     </para>
    </listitem>
    <listitem>
     <para>
      Primero debe actualizar el master de Salt, y después sustituir DeepSea con <systemitem class="resource">ceph-salt</systemitem> y cephadm. <emphasis>No podrá</emphasis> empezar a utilizar el módulo de orcephadm orchestrator al menos hasta que se hayan actualizado todos los de Ceph Manager.
     </para>
    </listitem>
    <listitem>
     <para>
      La actualización de usar RPM de Nautilus a contenedores de Octopus debe realizarse en un solo paso. Esto significa que se debe actualizar un nodo completo a la vez, no de daemon en daemon.
     </para>
    </listitem>
    <listitem>
     <para>
      La actualización de los servicios principales (MON, MGR, OSD) se produce de forma ordenada. Cada servicio está disponible durante la actualización. Los servicios de pasarela (servidor de metadatos, Object Gateway, NFS Ganesha e iSCSI Gateway) deben redistribuirse después de actualizar los servicios principales. Para los siguientes servicios se produce un cierto tiempo de inactividad:
     </para>
     <itemizedlist>
      <listitem>
       <important>
        <para>
         Los servidores de metadatos y las pasarelas Object Gateway están inactivos desde el momento en que los nodos se actualizan de SUSE Linux Enterprise Server 15 SP1 a SUSE Linux Enterprise Server 15 SP2 hasta que los servicios se vuelven a distribuir al final del proceso de actualización. Es especialmente importante tener esto en cuenta si estos servicios se colocan junto a servicios MON, MGR u OSD, ya que en tal caso, pueden estar inactivos durante toda la actualización del clúster. Si se prevé que esto sea un problema, considere la posibilidad de distribuir estos servicios por separado en nodos adicionales antes de la actualización, de forma que permanezcan inactivos durante el menor tiempo posible; que será lo que tarden en actualizarse los nodos de pasarela, no lo que tarde todo el clúster en actualizarse.
        </para>
       </important>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha y las pasarelas iSCSI Gateway solo están inactivos mientras los nodos se reinician durante la actualización de SUSE Linux Enterprise Server 15 SP1 a SUSE Linux Enterprise Server 15 SP2 y, de nuevo, brevemente, cuando cada servicio se vuelve a distribuir en modo de contenedores.
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-backup-config-data">
   <title>Copia de seguridad de la configuración y los datos del clúster</title>
   <para>
    Se recomienda encarecidamente realizar una copia de seguridad de toda la configuración y los datos del clúster antes de iniciar la actualización a SUSE Enterprise Storage 7. Para obtener instrucciones sobre cómo realizar copias de seguridad de todos los datos, consulte <link xlink:href="https://documentation.suse.com/ses/6/single-html/ses-admin/#cha-deployment-backup"/>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade">
   <title>Verificación de los pasos de la actualización anterior</title>
   <para>
    En caso de que haya actualizado previamente desde la versión 5, compruebe que la actualización a la versión 6 se haya completado correctamente:
   </para>
   <para>
    Compruebe si el archivo <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename> existe.
   </para>
   <para>
    Este archivo se crea en el proceso de importación durante la actualización de SUSE Enterprise Storage 5 a la versión 6. La opción <option>configuration_init: default-import</option> se define como <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
   <para>
    Si <option>configuration_init</option> todavía está definido como <option>default-import</option>, el clúster está utilizando <filename>ceph.conf.import</filename> como su archivo de configuración, y no el archivo <filename>ceph.conf</filename> por defecto de DeepSea, que se compila a partir de los archivos de <filename>:/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    Por lo tanto, debe inspeccionar si hay alguna configuración personalizada en <filename>ceph.conf.import</filename> y, posiblemente, trasladar la configuración a uno de los archivos de: <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/</filename>.
   </para>
   <para>
    A continuación, elimine la línea <option>configuration_init: default-import</option> de <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename>.
   </para>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch">
   <title>Actualización de los nodos de clúster y verificación del estado del clúster</title>
   <para>
    Verifique que todas las actualizaciones más recientes de SUSE Linux Enterprise Server 15 SP1 y SUSE Enterprise Storage 6 se han aplicado a todos los nodos de clúster:
   </para>
<screen><prompt role="root">root # </prompt>zypper refresh &amp;&amp; zypper patch</screen>
   <para>
    Después de aplicar las actualizaciones, compruebe el estado del clúster:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s</screen>
  </sect2>

  <sect2 xml:id="verify-previous-upgrade-patch-repos">
   <title>Verificación del acceso a repositorios de software e imágenes de contenedor</title>
   <para>
    Verifique que cada nodo del clúster tenga acceso a los repositorios de software de SUSE Linux Enterprise Server 15 SP2 y SUSE Enterprise Storage 7, así como al registro de imágenes de contenedor.
   </para>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-repos">
    <title>Repositorios de software</title>
    <para>
     Si todos los nodos están registrados con SCC, podrá utilizar el comando <command>zypper migration</command> para actualizar. Consulte <link xlink:href="https://documentation.suse.com/sles/15-SP2/html/SLES-all/cha-upgrade-online.html#sec-upgrade-online-zypper"/> para obtener más información.
    </para>
    <para>
     Si los nodos <emphasis role="bold">no</emphasis> están registrados con SCC, inhabilite todos los repositorios de software existentes y añada los repositorios <literal>Repositorio</literal> y <literal>Actualizaciones</literal> para cada una de las extensiones siguientes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Product-SLES/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Basesystem/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Server-Applications/15-SP2
      </para>
     </listitem>
     <listitem>
      <para>
       SUSE-Enterprise-Storage-7
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="verify-previous-upgrade-patch-repos-images">
    <title>Imágenes de contenedor</title>
    <para>
     Todos los nodos de clúster necesitan acceso al registro de imágenes de contenedor. En la mayoría de los casos, utilizará el registro público de SUSE situado en <literal>registration.suse.com</literal>. Necesitará las siguientes imágenes:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/ceph
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/ses/7/ceph/grafana
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-server
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-node-exporter
      </para>
     </listitem>
     <listitem>
      <para>
       registry.suse.com/caasp/v4.5/prometheus-alertmanager
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Como alternativa, por ejemplo para las distribuciones en entornos aislados, configure un registro local y verifique que tiene el conjunto correcto de imágenes de contenedor. Consulte la <xref linkend="deploy-cephadm-configure-registry"/> para obtener más información sobre cómo configurar un registro de imagen de contenedor local.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-salt-master">
  <title>Actualización del master de Salt</title>

  <para>
   El procedimiento siguiente describe el proceso de actualización del master de Salt:
  </para>

  <procedure>
   <step>
    <para>
     Actualice el sistema operativo subyacente a SUSE Linux Enterprise Server 15 SP2:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       En el caso de un clúster que tenga todos los nodos registrados con SCC, ejecute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Para un clúster cuyos nodos tienen repositorios de software asignados manualmente, ejecute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Inhabilite las fases de DeepSea para evitar un uso accidental. Añada el contenido siguiente a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
    </para>
<screen>
stage_prep: disabled
stage_discovery: disabled
stage_configure: disabled
stage_deploy: disabled
stage_services: disabled
stage_remove: disabled
</screen>
    <para>
     Guarde el archivo y aplique los cambios:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.pillar_refresh</screen>
   </step>
   <step>
    <para>
     Si <emphasis role="bold">no</emphasis> utiliza imágenes de contenedor de <literal>registration.suse.com</literal> sino el registro configurado localmente, edite <filename>/srv/pillar/ceph/stack/global.yml</filename> para informar a DeepSea sobre qué imagen de contenedor de Ceph y qué registro de Ceph debe utilizar. Por ejemplo, para utilizar <literal>192.168.121.1:5000/my/ceph/image</literal>, añada las siguientes líneas:
    </para>
<screen>
ses7_container_image: 192.168.121.1:5000/my/ceph/image
ses7_container_registries:
  - location: 192.168.121.1:5000
</screen>
    <para>
     Guarde el archivo y aplique los cambios:
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar</screen>
   </step>
   <step>
    <para>
     Asimile la configuración existente:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph config assimilate-conf -i /etc/ceph/ceph.conf</screen>
   </step>
   <step>
    <para>
     Verifique el estado de la actualización. El resultado puede variar según la configuración del clúster:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
 ceph: ceph version 15.2.2-60-gf5864377ab (f5864377abb5549f843784c93577980aa264b9bc) octopus (stable)
 os: SUSE Linux Enterprise Server 15 SP2
Nodes running these software versions:
 admin.ceph (assigned roles: master, prometheus, grafana)
Nodes running older software versions must be upgraded in the following order:
 1: mon1.ceph (assigned roles: admin, mon, mgr)
 2: mon2.ceph (assigned roles: admin, mon, mgr)
 3: mon3.ceph (assigned roles: admin, mon, mgr)
 4: data4.ceph (assigned roles: storage, mds)
 5: data1.ceph (assigned roles: storage)
 6: data2.ceph (assigned roles: storage)
 7: data3.ceph (assigned roles: storage)
 8: data5.ceph (assigned roles: storage, rgw)
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-mon-mgr-nodes">
  <title>Actualización de los nodos de MON, MGR y OSD</title>

  <para>
   Actualice los nodos de Ceph Monitor, Ceph Manager y OSD de uno en uno. Para cada servicio, siga estos pasos:
  </para>

  <procedure>
   <step>
    <para>
     Si el nodo que está actualizando es un nodo de OSD, procure que no esté marcado como <literal>out</literal> durante la actualización ejecutando el comando siguiente:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd add-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
    <para>
     Sustituya <replaceable>SHORT_NODE_NAME</replaceable> por el nombre corto del nodo tal y como aparece en la salida del <command>comando ceph osd</command>. En la siguiente entrada, los nombres de host cortos son <literal>ses-min1</literal> y <literal>ses-min2</literal>.
    </para>
<screen>
<prompt>root@master # </prompt>ceph osd tree
ID   CLASS  WEIGHT   TYPE NAME       STATUS  REWEIGHT  PRI-AFF
 -1         0.60405  root default
-11         0.11691      host ses-min1
  4    hdd  0.01949          osd.4       up   1.00000  1.00000
  9    hdd  0.01949          osd.9       up   1.00000  1.00000
 13    hdd  0.01949          osd.13      up   1.00000  1.00000
[...]
 -5         0.11691      host ses-min2
  2    hdd  0.01949          osd.2       up   1.00000  1.00000
  5    hdd  0.01949          osd.5       up   1.00000  1.00000
[...]
</screen>
   </step>
   <step>
    <para>
     Actualice el sistema operativo subyacente a SUSE Linux Enterprise Server 15 SP2:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Si todos los nodos de clúster están registrados con SCC, ejecute <command>zypper migration</command>.
      </para>
     </listitem>
     <listitem>
      <para>
       Si los nodos de clúster tienen repositorios de software asignados manualmente, ejecute <command>zypper dup</command> seguido de <command>reboot</command>.
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     Después de reiniciar el nodo, convierta en contenedores todos los daemons de MON, MGR y OSD existentes en ese nodo ejecutando el comando siguiente en el master de Salt:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>MINION_ID</replaceable> state.apply ceph.upgrade.ses7.adopt</screen>
    <para>
     Sustituya <replaceable>MINION_ID</replaceable> por el ID del minion que está actualizando. Puede obtener la lista de ID de minion ejecutando el comando <command>salt-key -L</command> en el master de Salt.
    </para>
    <tip>
     <para>
      Para ver el estado y el progreso de la <emphasis>adopción</emphasis>, compruebe Ceph Dashboard o ejecute uno de los comandos siguientes en el master de Salt:
     </para>
<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>
    </tip>
   </step>
   <step>
    <para>
     Después de que la adopción haya finalizado correctamente, desmarque el indicador <literal>noout</literal> si el nodo que está actualizando es un nodo de OSD:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd rm-noout <replaceable>SHORT_NODE_NAME</replaceable></screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateway-nodes">
  <title>Actualización de nodos de pasarela</title>
  <para>
   A continuación, actualice los nodos de pasarela independientes (servidor de metadatos, Object Gateway, NFS Ganesha o iSCSI Gateway). Actualice el sistema operativo subyacente a SUSE Linux Enterprise Server 15 SP2 para cada nodo:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     Si todos los nodos de clúster están registrados en el Centro de servicios al cliente de SUSE, ejecute el comando <command>zypper migration</command>.
    </para>
   </listitem>
   <listitem>
    <para>
     Si los nodos de clúster tienen repositorios de software asignados manualmente, ejecute <command>zypper dup</command> seguido de <command>reboot</command>.
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Este paso también se aplica a los nodos que forman parte del clúster, pero que aún no tienen ninguna función asignada (en caso de duda, compruebe la lista de hosts en el master de Salt proporcionada por el comando <command>salt-key -L</command> y compárela con el resultado del comando <command>salt-run upgrade.status</command>).
  </para>
  <para>
   Una vez que se actualiza el sistema operativo en todos los nodos de clúster, el siguiente paso es instalar el paquete <package>ceph-salt</package> y aplicar la configuración del clúster. Los servicios de pasarela reales se redistribuyen en un modo de contenedores al final del proceso de actualización.
  </para>
  <note>
   <para>
    Los servicios de servidor de metadatos y Object Gateway no están disponibles desde el momento de la actualización a SUSE Linux Enterprise Server 15 SP2 hasta que se redistribuyen al final del proceso de actualización.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt">
  <title>Instalación de <systemitem class="resource">ceph-salt</systemitem> y aplicación de la configuración del clúster</title>

  <para>
   Antes de iniciar el proceso de instalación de <systemitem class="resource">ceph-salt</systemitem> y aplicar la configuración del clúster, compruebe el estado del clúster y de la actualización ejecutando los comandos siguientes:
  </para>

<screen>
<prompt>root@master # </prompt>ceph status
<prompt>root@master # </prompt>ceph versions
<prompt>root@master # </prompt>salt-run upgrade.status
</screen>

  <procedure>
   <step>
    <para>
     Elimine los trabajos cron <literal>rbd_exporter</literal> y <literal>rgw_exporter</literal> creados por DeepSea. En el master de Salt, como usuario <systemitem class="username">root</systemitem>, ejecute el comando <command>crontab -e</command> para editar crontab. Suprima los siguientes elementos, si están presentes:
    </para>
<screen>
# SALT_CRON_IDENTIFIER:deepsea rbd_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/rbd.sh &gt; \
 /var/lib/prometheus/node-exporter/rbd.prom 2&gt; /dev/null
# SALT_CRON_IDENTIFIER:Prometheus rgw_exporter cron job
*/5 * * * * /var/lib/prometheus/node-exporter/ceph_rgw.py &gt; \
 /var/lib/prometheus/node-exporter/ceph_rgw.prom 2&gt; /dev/null
</screen>
   </step>
   <step>
    <para>
     Exporte la configuración del clúster desde DeepSea ejecutando los comandos siguientes:
    </para>
<screen>
<prompt>root@master # </prompt>salt-run upgrade.ceph_salt_config &gt; ceph-salt-config.json
<prompt>root@master # </prompt>salt-run upgrade.generate_service_specs &gt; specs.yaml
</screen>
   </step>
   <step>
    <para>
     Desinstale DeepSea e instale <systemitem class="resource">ceph-salt</systemitem> en el master de Salt:
    </para>
<screen>
<prompt>root@master # </prompt>zypper remove 'deepsea*'
<prompt>root@master # </prompt>zypper install ceph-salt
</screen>
   </step>
   <step>
    <para>
     Reinicie el master de Salt y sincronice los módulos de Salt:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt \* saltutil.sync_all
</screen>
   </step>
   <step>
    <para>
     Importe la configuración del clúster de DeepSea a <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt import ceph-salt-config.json</screen>
   </step>
   <step>
    <para>
     Genere las claves SSH para la comunicación del nodo de clúster:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt config /ssh generate</screen>
    <tip>
     <para>
      Verifique que la configuración del clúster se ha importado desde DeepSea y especifique las opciones que pudieran faltar:
     </para>
<screen><prompt>root@master # </prompt>ceph-salt config ls</screen>
     <para>
      Para obtener una descripción completa de la configuración del clúster, consulte la <xref linkend="deploy-cephadm-configure"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Aplique la configuración y habilite cephadm:
    </para>
<screen><prompt>root@master # </prompt>ceph-salt apply</screen>
   </step>
   <step>
    <para>
     Si necesita proporcionar la URL del registro del contenedor local y las credenciales de acceso, siga los pasos descritos en la <xref linkend="deploy-cephadm-configure-registry"/>.
    </para>
   </step>
   <step>
    <para>
     Si <emphasis role="bold">no</emphasis> está utilizando imágenes de contenedor de <literal>registration.suse.com</literal>, sino el registro configurado localmente, informe a Ceph sobre qué imagen de contenedor debe utilizar ejecutando:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image <replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Por ejemplo:
    </para>
<screen><prompt>root@master # </prompt>ceph config set global container_image 192.168.121.1:5000/my/ceph/image</screen>
   </step>
   <step>
    <para>
     Detenga e inhabilite los daemons <systemitem class="daemon">ceph-crash</systemitem> de SUSE Enterprise Storage 6. Los nuevos formularios en contenedores de estos daemons se inician más tarde automáticamente.
    </para>
<screen>
<prompt>root@master # </prompt>salt '*' service.stop ceph-crash
<prompt>root@master # </prompt>salt '*' service.disable ceph-crash
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-cephsalt-monitoring">
  <title>Actualización y adopción de la pila de supervisión</title>

  <para>
   El siguiente procedimiento adopta todos los componentes de la pila de supervisión (consulte el <xref linkend="monitoring-alerting"/> para obtener más información).
  </para>

  <procedure>
   <step>
    <para>
     Ponga en pausa el orquestador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch pause</screen>
   </step>
   <step>
    <para>
     En cualquier nodo en el que se ejecute Prometheus, Grafana y Alertmanager (el master de Salt por defecto), ejecute los comandos siguientes:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm adopt --style=legacy --name grafana.$(hostname)
</screen>
    <tip>
     <para>
      Si <emphasis role="bold">no</emphasis> está ejecutando el registro de imágenes de contenedor por defecto, <literal>registration.suse.com</literal>, debe especificar la imagen que desea utilizar, por ejemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-server:2.18.0 \
  adopt --style=legacy --name prometheus.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/caasp/v4.5/prometheus-alertmanager:0.16.2 \
  adopt --style=legacy --name alertmanager.$(hostname)
<prompt>cephuser@adm &gt; </prompt>cephadm --image 192.168.121.1:5000/ses/7/ceph/grafana:7.0.3 \
 adopt --style=legacy --name grafana.$(hostname)
</screen>
     <para>
      Para obtener más información sobre el uso de imágenes de contenedor personalizadas o locales, consulte el <xref linkend="monitoring-custom-images"/>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     Elimine Node-Exporter. No es necesario migrarlo y se volverá a instalar como contenedor cuando se aplique el archivo <filename>specs.yaml</filename>.
    </para>
<screen><prompt>tux &gt; </prompt><command>sudo</command> zypper rm golang-github-prometheus-node_exporter</screen>
   </step>
   <step>
    <para>
     Aplique las especificaciones de servicio que exportó anteriormente desde DeepSea:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i specs.yaml</screen>
   </step>
   <step>
    <para>
     Reanude el orquestador:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch resume</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-gateways">
  <title>Redistribución del servicio de pasarela</title>

  <sect2 xml:id="upgrade-ogw">
   <title>Actualización de Object Gateway</title>
   <para>
    En SUSE Enterprise Storage 7, las pasarelas Object Gateway siempre se configuran con un dominio, lo que permite utilizar varios sitios en el futuro (consulte el <xref linkend="ceph-rgw-fed"/> para obtener más información). Si ha utilizado una configuración de Object Gateway de un solo sitio en SUSE Enterprise Storage 6, siga estos pasos para añadir un dominio. Si no tiene previsto utilizar la funcionalidad de varios sitios, puede utilizar el valor <literal>por defecto</literal> para los nombres de dominio, grupo de zonas y zona.
   </para>
   <procedure>
    <step>
     <para>
      Cree un dominio nuevo:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin realm create --rgw-realm=<replaceable>REALM_NAME</replaceable> --default</screen>
    </step>
    <step>
     <para>
      Opcionalmente, cambie el nombre de la zona y el grupo de zonas por defecto.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup rename \
 --rgw-zonegroup default \
 --zonegroup-new-name=<replaceable>ZONEGROUP_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone rename \
 --rgw-zone default \
 --zone-new-name <replaceable>ZONE_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable>
</screen>
    </step>
    <step>
     <para>
      Configure el grupo de zona principal:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zonegroup modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Configure la zona principal. Para ello, necesitará los valores de ACCESS_KEY y SECRET_KEY de un usuario de Object Gateway con el indicador <option>system</option> habilitado. Suele ser el usuario <literal>admin</literal>. Para obtener los valores de ACCESS_KEY y SECRET_KEY, ejecute <command>radosgw-admin user info --uid admin</command>.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>radosgw-admin zone modify \
 --rgw-realm=<replaceable>REALM_NAME</replaceable> \
 --rgw-zonegroup=<replaceable>ZONEGROUP_NAME</replaceable> \
 --rgw-zone=<replaceable>ZONE_NAME</replaceable> \
 --endpoints http://<replaceable>RGW.EXAMPLE.COM</replaceable>:80 \
 --access-key=<replaceable>ACCESS_KEY</replaceable> \
 --secret=<replaceable>SECRET_KEY</replaceable> \
 --master --default
</screen>
    </step>
    <step>
     <para>
      Asigne la configuración actualizada:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>radosgw-admin period update --commit</screen>
    </step>
   </procedure>
   <para>
    Para que el servicio Object Gateway se convierta en contenedores, cree su archivo de especificación como se describe en la <xref linkend="deploy-cephadm-day2-service-ogw"/> y aplíquelo.
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>RGW</replaceable>.yml
</screen>
  </sect2>

  <sect2 xml:id="upgrade-ganesha">
   <title>Actualización de NFS Ganesha</title>
   <para>
    A continuación se muestra cómo migrar un servicio existente de NFS Ganesha con Ceph Nautilus a un contenedor de NFS Ganesha con Ceph Octopus.
   </para>
   <warning>
    <para>
     Para los pasos siguientes es preciso que haya actualizado correctamente los servicios principales de Ceph.
    </para>
   </warning>
   <para>
    NFS Ganesha almacena configuraciones adicionales por daemon y exporta la configuración en un repositorio RADOS. El repositorio RADOS configurado se puede encontrar en la línea <literal>watch_url</literal> del bloque <literal>RADOS_URLS</literal> en el archivo <filename>ganesha.conf</filename>. Por defecto, este repositorio se llamará <literal>ganesha_config</literal>.
   </para>
   <para>
    Antes de iniciar cualquier migración, se recomienda encarecidamente realizar una copia de los objetos de exportación y de configuración del daemon ubicados en el repositorio RADOS. Para localizar el repositorio RADOS configurado, ejecute el comando siguiente:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>grep -A5 RADOS_URLS /etc/ganesha/ganesha.conf</screen>
   <para>
    Para mostrar el contenido del repositorio RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados --pool ganesha_config --namespace ganesha ls | sort
  conf-node3
  export-1
  export-2
  export-3
  export-4</screen>
   <para>
    Para copiar los objetos RADOS:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>OBJS=$(rados $RADOS_ARGS ls)
<prompt>cephuser@adm &gt; </prompt>for obj in $OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 40K
drwxr-xr-x 2 root root 4.0K Sep 8 03:30 .
drwx------ 9 root root 4.0K Sep 8 03:23 ..
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 03:30 conf-node3
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-1
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-2
-rw-r--r-- 1 root root 350 Sep 8 03:30 export-3
-rw-r--r-- 1 root root 358 Sep 8 03:30 export-4</screen>
   <para>
    Es necesario detener el servicio de NFS Ganesha existente en cada nodo de uno en uno y, a continuación, sustituirlo por un contenedor gestionado por cephadm.
   </para>
   <procedure>
    <step>
     <para>
      Detenga e inhabilite el servicio de NFS Ganesha existente:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>systemctl stop nfs-ganesha
<prompt>cephuser@adm &gt; </prompt>systemctl disable nfs-ganesha
</screen>
    </step>
    <step>
     <para>
      Después de detener el servicio de NFS Ganesha existente, se puede distribuir uno nuevo en un contenedor mediante cephadm. Para ello, debe crear una especificación de servicio que contenga un <literal>service_id</literal> que se utilizará para identificar este nuevo clúster de NFS, el nombre de host del nodo que estamos migrando mostrado como host en la especificación de colocación, y el repositorio RADOS y el espacio de nombres que contiene los objetos de exportación NFS configurados. Por ejemplo:
     </para>
     <screen>service_type: nfs
service_id: <replaceable>SERVICE_ID</replaceable>
placement:
  hosts:
  - node2
  pool: ganesha_config
  namespace: ganesha
</screen>
     <para>
      Para obtener más información sobre cómo crear una especificación de colocación, consulte la <xref linkend="cephadm-service-and-placement-specs"/>.
     </para>
    </step>
    <step>
     <para>
      Aplique la especificación de colocación:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>FILENAME</replaceable>.yaml</screen>
    </step>
    <step>
     <para>
      Confirme que el daemon de NFS Ganesha se está ejecutando en el host:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type nfs
NAME           HOST   STATUS         REFRESHED  AGE  VERSION  IMAGE NAME                                IMAGE ID      CONTAINER ID
nfs.foo.node2  node2  running (26m)  8m ago     27m  3.3      registry.suse.com/ses/7/ceph/ceph:latest  8b4be7c42abd  c8b75d7c8f0d</screen>
    </step>
    <step>
     <para>
      Repita estos pasos para cada nodo de NFS Ganesha. No es necesario crear una especificación de servicio independiente para cada nodo. Basta con añadir el nombre de host de cada nodo a la especificación del servicio NFS existente y volver a aplicarlo.
     </para>
    </step>
   </procedure>
   <para>
    Las exportaciones existentes se pueden migrar de dos formas distintas:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Volviéndolas a crear o a asignar manualmente mediante Ceph Dashboard.
     </para>
    </listitem>
    <listitem>
     <para>
      Copiando manualmente el contenido de cada objeto RADOS por daemon en la configuración común de NFS Ganesha recién creada.
     </para>
    </listitem>
   </itemizedlist>
   <procedure>
    <title>Copia manual de las exportaciones al archivo de configuración común de NFS Ganesha</title>
    <step>
     <para>
      Determine la lista de objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>RADOS_ARGS="--pool ganesha_config --namespace ganesha"
<prompt>cephuser@adm &gt; </prompt>DAEMON_OBJS=$(rados $RADOS_ARGS ls | grep 'conf-')</screen>
    </step>
    <step>
     <para>
      Realice una copia de los objetos RADOS por daemon:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>for obj in $DAEMON_OBJS; do rados $RADOS_ARGS get $obj $obj; done
<prompt>cephuser@adm &gt; </prompt>ls -lah
total 20K
drwxr-xr-x 2 root root 4.0K Sep 8 16:51 .
drwxr-xr-x 3 root root 4.0K Sep 8 16:47 ..
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-nfs.<replaceable>SERVICE_ID</replaceable>
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node2
-rw-r--r-- 1 root root 90 Sep 8 16:51 conf-node3</screen>
    </step>
    <step>
     <para>
      Ordene y fusione en una sola lista de exportaciones:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat conf-* | sort -u &gt; conf-nfs.<replaceable>SERVICE_ID</replaceable>
<prompt>cephuser@adm &gt; </prompt>cat conf-nfs.foo
%url "rados://ganesha_config/ganesha/export-1"
%url "rados://ganesha_config/ganesha/export-2"
%url "rados://ganesha_config/ganesha/export-3"
%url "rados://ganesha_config/ganesha/export-4"</screen>
    </step>
    <step>
     <para>
      Escriba el nuevo archivo de configuración común de NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS put conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
    </step>
    <step>
     <para>
      Notifíqueselo al daemon de NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rados $RADOS_ARGS notify conf-nfs.<replaceable>SERVICE_ID</replaceable> conf-nfs.<replaceable>SERVICE_ID</replaceable></screen>
     <note>
      <para>
       Esta acción hará que el daemon vuelva a cargar la configuración.
      </para>
     </note>
    </step>
   </procedure>
   <para>
    Después de que el servicio se haya migrado correctamente, se puede eliminar el servicio de NFS Ganesha basado en Nautilus.
   </para>
   <procedure>
    <step>
     <para>
      Elimine NFS Ganesha:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm nfs-ganesha
Reading installed packages...
Resolving package dependencies...
The following 5 packages are going to be REMOVED:
  nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls nfs-ganesha-rgw
5 packages to remove.
After the operation, 308.9 KiB will be freed.
Continue? [y/n/v/...? shows all options] (y): y
(1/5) Removing nfs-ganesha-ceph-2.8.3+git0.d504d374e-3.3.1.x86_64 .................................................................................................................................................................................................................................................................................................[done]
(2/5) Removing nfs-ganesha-rgw-2.8.3+git0.d504d374e-3.3.1.x86_64 ..................................................................................................................................................................................................................................................................................................[done]
(3/5) Removing nfs-ganesha-rados-urls-2.8.3+git0.d504d374e-3.3.1.x86_64 ...........................................................................................................................................................................................................................................................................................[done]
(4/5) Removing nfs-ganesha-rados-grace-2.8.3+git0.d504d374e-3.3.1.x86_64 ..........................................................................................................................................................................................................................................................................................[done]
(5/5) Removing nfs-ganesha-2.8.3+git0.d504d374e-3.3.1.x86_64 ......................................................................................................................................................................................................................................................................................................[done]
Additional rpm output:
warning: /etc/ganesha/ganesha.conf saved as /etc/ganesha/ganesha.conf.rpmsave</screen>
    </step>
    <step>
     <para>
      Elimine la configuración del clúster heredada de Ceph Dashboard:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph dashboard reset-ganesha-clusters-rados-pool-namespace
</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-mds">
   <title>Actualización del servidor de metadatos</title>
   <para>
    A diferencia de los servicios MON, MGR y OSD, el servidor de metadatos no se puede adoptar in situ. En su lugar, debe volver a distribuirlo en contenedores mediante Ceph Orchestrator.
   </para>
   <procedure>
    <step>
     <para>
      Ejecute el comando <command>ceph fs ls</command> para obtener el nombre del sistema de archivos, por ejemplo:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs ls
name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]
</screen>
    </step>
    <step>
     <para>
      Cree un nuevo archivo de especificación de servicio <filename>mds.yml</filename> tal como se describe en la <xref linkend="deploy-cephadm-day2-service-mds"/> utilizando el nombre del sistema de archivos como <option>service_id</option> y especificando los hosts que ejecutarán los daemons de MDS. Por ejemplo:
     </para>
<screen>
service_type: mds
service_id: cephfs
placement:
  hosts:
  - ses-min1
  - ses-min2
  - ses-min3
</screen>
    </step>
    <step>
     <para>
      Ejecute el comando <command>ceph orch apply -i mds.yml</command> para aplicar la especificación de servicio e iniciar los daemons de MDS.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-igw">
   <title>Actualización de iSCSI Gateway</title>
   <para>
    Para actualizar iSCSI Gateway, debe volver a distribuirlo en contenedores mediante Ceph Orchestrator. Si dispone de varias pasarelas iSCSI Gateway, deberá volver a distribuirlas una a una para reducir el tiempo de inactividad del servicio.
   </para>
   <procedure>
    <step>
     <para>
      Detenga e inhabilite los daemons iSCSI existentes en cada nodo iSCSI Gateway:
     </para>
<screen>
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-gw
<prompt>tux &gt; </prompt><command>sudo</command> systemctl stop rbd-target-api
<prompt>tux &gt; </prompt><command>sudo</command> systemctl disable rbd-target-api
</screen>
    </step>
    <step>
     <para>
      Cree una especificación de servicio para iSCSI Gateway como se describe en la <xref linkend="deploy-cephadm-day2-service-igw"/>. Para ello, necesita los ajustes <option>pool</option>, <option>trusted_ip_list</option> y <option>api_*</option> del archivo <filename>/etc/ceph/iscsi-gateway.cfg</filename> existente. Si tiene habilitada la compatibilidad con SSL (<literal>api_secure = true</literal>), también necesitará el certificado SSL (<filename>/etc/ceph/iscsi-gateway.crt</filename>) y la clave (<filename>/etc/ceph/iscsi-gateway.key</filename>).
     </para>
     <para>
      Por ejemplo, si <filename>/etc/ceph/iscsi-gateway.cfg</filename> contiene lo siguiente:
     </para>
<screen>
[config]
cluster_client_name = client.igw.ses-min5
pool = iscsi-images
trusted_ip_list = 10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202
api_port = 5000
api_user = admin
api_password = admin
api_secure = true
</screen>
     <para>
      Debe crear el siguiente archivo de especificación de servicio <filename>iscsi.yml</filename>:
     </para>
<screen>
service_type: iscsi
service_id: igw
placement:
  hosts:
  - ses-min5
spec:
  pool: iscsi-images
  trusted_ip_list: "10.20.179.203,10.20.179.201,10.20.179.205,10.20.179.202"
  api_port: 5000
  api_user: admin
  api_password: admin
  api_secure: true
  ssl_cert: |
    -----BEGIN CERTIFICATE-----
    MIIDtTCCAp2gAwIBAgIYMC4xNzc1NDQxNjEzMzc2MjMyXzxvQ7EcMA0GCSqGSIb3
    DQEBCwUAMG0xCzAJBgNVBAYTAlVTMQ0wCwYDVQQIDARVdGFoMRcwFQYDVQQHDA5T
    [...]
    -----END CERTIFICATE-----
  ssl_key: |
    -----BEGIN PRIVATE KEY-----
    MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQC5jdYbjtNTAKW4
    /CwQr/7wOiLGzVxChn3mmCIF3DwbL/qvTFTX2d8bDf6LjGwLYloXHscRfxszX/4h
    [...]
    -----END PRIVATE KEY-----
</screen>
     <note>
      <para>
       Los ajustes <option>pool</option>, <option>trusted_ip_list</option>, <option>api_port</option>, <option>api_user</option>, <option>api_password</option> y <option>api_secure</option> son idénticos a los del archivo <filename>/etc/ceph/iscsi-gateway.cfg</filename>. Los ajustes <option>ssl_cert</option> y <option>ssl_key</option> se pueden copiar desde el certificado SSL existente y los archivos de clave. Compruebe que están sangrados correctamente y que el carácter de <emphasis>barra vertical</emphasis> (<literal>|</literal>) aparece al final de las líneas <literal>ssl_cert:</literal> y <literal>ssl_key:</literal> (consulte el contenido del archivo <filename>iscsi.yml</filename> más arriba).
      </para>
     </note>
    </step>
    <step>
     <para>
      Ejecute el comando <command>ceph orch apply -i iscsi.yml</command> para aplicar la especificación del servicio e iniciar los daemons de iSCSI Gateway.
     </para>
    </step>
    <step>
     <para>
      Elimine el paquete <package>ceph-iscsi</package> antiguo de todos los nodos de iSCSI Gateway existentes:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>zypper rm -u ceph-iscsi</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-post-cleanup">
  <title>Limpieza posterior a la actualización</title>

  <para>
   Después de la actualización, realice los siguientes pasos de limpieza:
  </para>

  <procedure>
   <step>
    <para>
     Verifique que el clúster se haya actualizado correctamente comprobando la versión actual de Ceph:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph versions</screen>
   </step>
   <step>
    <para>
     Asegúrese de que ningún OSD antiguo se va a unir al clúster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd require-osd-release octopus</screen>
   </step>
   <step>
    <para>
     Habilite el módulo de escalador automático:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mgr module enable pg_autoscaler</screen>
    <important>
     <para>
      Los repositorios de SUSE Enterprise Storage 6 tenían el valor <option>pg_autoscale_mode</option> definido por defecto como <option>warn</option>. Esto daba como resultado un mensaje de advertencia en caso de que hubiera un número de grupos de colocación que no fuera óptimo, pero el ajuste automático de escala no se producía. El ajuste por defecto en SUSE Enterprise Storage 7 es que la opción <option>pg_autoscale_mode</option> está en <option>on</option> para los nuevos repositorios, y los grupos de colocación se autoescalan. El proceso de actualización no cambia automáticamente el valor de <option>pg_autoscale_mode</option> de los repositorios existentes. Si desea cambiarlo a <option>on</option> para obtener todas las ventajas del escalador automático, consulte las instrucciones del <xref linkend="op-pgs-autoscaler"/>.
     </para>
    </important>
    <para>
     Más detalles en el <xref linkend="op-pgs-autoscaler"/>.
    </para>
   </step>
   <step>
    <para>
     Impida los clientes anteriores a Luminous:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd set-require-min-compat-client luminous</screen>
   </step>
   <step>
    <para>
     Habilite el módulo de equilibrador:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph balancer mode upmap
<prompt>cephuser@adm &gt; </prompt>ceph balancer on
</screen>
    <para>
     Más detalles en el <xref linkend="mgr-modules-balancer"/>.
    </para>
   </step>
   <step>
    <para>
     También puede habilitar el módulo de telemetría:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph mgr module enable telemetry
<prompt>cephuser@adm &gt; </prompt>ceph telemetry on
 </screen>
    <para>
     Más detalles en el <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
