<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>操作任务</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>是</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>修改集群配置</title>

  <para>
   要修改现有 Ceph 集群的配置，请执行以下步骤：
  </para>

  <procedure>
   <step>
    <para>
     将集群的当前配置导出到文件：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     编辑包含配置的文件并更新相关行。可在<xref linkend="deploy-cephadm-day2"/>和<xref linkend="drive-groups"/>中找到规范示例。
    </para>
   </step>
   <step>
    <para>
     应用新配置：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>添加节点</title>

  <para>
   要向 Ceph 集群添加新节点，请执行以下步骤：
  </para>

  <procedure>
   <step>
    <para>
     在新主机上安装 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage。有关更多信息，请参考<xref linkend="deploy-os"/>。
    </para>
   </step>
   <step>
    <para>
     将主机配置为已存在 Salt Master 的 Salt Minion。有关更多信息，请参考<xref linkend="deploy-salt"/>。
    </para>
   </step>
   <step>
    <para>
     将新主机添加到 <systemitem class="resource">ceph-salt</systemitem> 并让 cephadm 能识别它，例如：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     有关更多信息，请参考<xref linkend="deploy-cephadm-configure-minions"/>。
    </para>
   </step>
   <step>
    <para>
     确认节点是否已添加到 <systemitem class="resource">ceph-salt</systemitem>：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     将配置应用于新集群主机：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     确认新添加的主机现在是否属于 cephadm 环境：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>删除节点</title>

  <tip>
   <title>删除 OSD</title>
   <para>
    如果您要删除的节点运行有 OSD，请先从中删除 OSD，然后检查该节点上是否未运行任何 OSD。有关删除 OSD 的更多详细信息，请参见<xref linkend="removing-node-osds"/>。
   </para>
  </tip>

  <para>
   要从集群中删除节点，请执行以下操作：
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     对于除 <literal>node-exporter</literal> 和 <literal>crash</literal> 之外的所有 Ceph 服务类型，请从集群归置规范文件（例如 <filename>cluster.yml</filename>）中删除节点的主机名。有关更多详细信息，请参见<xref linkend="cephadm-service-and-placement-specs"/>。例如，如果您要删除名为 <literal>ses-min2</literal> 的主机，请从所有 <literal>placement:</literal> 部分中删除所有出现的 <literal>- ses-min2</literal>：
    </para>
    <para>
     更新
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     更改为
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     将您的更改应用于配置文件：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     从 cephadm 的环境中删除节点：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     如果节点正在运行 <literal>crash.osd.1</literal> 和 <literal>crash.osd.2</literal> 服务，请通过在主机上运行以下命令将其删除：
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     从您要删除的 Minion 中删除所有角色：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     如果您要删除的 Minion 是引导 Minion，则还需要删除引导角色：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     删除单个主机上的所有 OSD 后，从 CRUSH 索引中删除该主机：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      存储桶名称应该与主机名相同。
     </para>
    </note>
   </step>
   <step>
    <para>
     现在您便可以从集群中删除 Minion：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    如果发生故障，并且您尝试删除的 Minion 处于永久断电状态，则需要从 Salt Master 中删除节点：
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    然后，手动将节点从 <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename> 中删除。它通常位于 <filename>/srv/pillar/ceph-salt.sls</filename> 中。
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSD 管理</title>

  <para>
   本节介绍如何在 Ceph 集群中添加、擦除或删除 OSD。
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>列出磁盘设备</title>
   <para>
    要识别所有集群节点上已使用和未使用的磁盘设备，请通过运行以下命令列出这些设备：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>擦除磁盘设备</title>
   <para>
    要重新使用磁盘设备，需要先将其擦除（或<emphasis>清除</emphasis>）：
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     如果您之前在未设置 <literal>unmanaged</literal> 标志的情况下使用 DriveGroups 或 <option>--all-available-devices</option> 选项部署 OSD，cephadm 将在您擦除这些 OSD 后再自动部署它们。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>使用 DriveGroups 规范添加 OSD。</title>
   <para>
    <emphasis>DriveGroups</emphasis> 用于指定 Ceph 集群中 OSD 的布局。它们在单个 YAML 文件中定义。在本节中，我们将使用 <filename>drive_groups.yml</filename> 作为示例。
   </para>
   <para>
    管理员应手动指定一组相关的 OSD（部署在 HDD 和 SDD 组合上的混合 OSD）或一组使用相同部署选项的 OSD（例如，对象存储、加密选项相同的各独立 OSD）。为了避免明确列出设备，DriveGroups 会使用与 <command>ceph-volume</command> 库存报告中的几个所选字段对应的过滤项列表。cephadm 将提供一些代码，用于将这些 DriveGroups 转换为供用户检查的实际设备列表。
   </para>
   <para>
    将 OSD 规范应用于集群的命令是：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    要查看操作预览并测试您的应用，可以将 <option>--dry-run</option> 选项与 <command>ceph orch apply osd</command> 命令一起使用。例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    如果 <option>--dry-run</option> 输出符合您的预期，则只需重新运行命令而无需使用 <option>--dry-run</option> 选项。
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>非受管 OSD</title>
    <para>
     将符合 DriveGroups 规范的所有可用正常磁盘设备添加到集群后，它们将自动用作 OSD。此行为称为<emphasis>受管</emphasis>模式。
    </para>
    <para>
     要禁用<emphasis>受管</emphasis>模式，请将 <literal>unmanaged: true</literal> 行添加到相关规范，例如：
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      要将已部署的 OSD 从<emphasis>受管</emphasis>模式更改为<emphasis>非受管</emphasis>模式，请于<xref linkend="modifying-cluster-configuration"/>中所述的过程期间在适当位置添加 <literal>unmanaged: true</literal> 行。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>DriveGroups 规范</title>
    <para>
     下面是 DriveGroups 规范文件的示例：
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         之前在 DeepSea 中名为“加密”的选项已被重命名为“已加密”。在 SUSE Enterprise Storage 7 中应用 DriveGroups 时，请确保在服务规范中使用此新术语，否则 <command>ceph orch apply</command> 操作将失败。
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>匹配磁盘设备</title>
    <para>
     您可以使用以下过滤器描述规格：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       按磁盘型号：
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       按磁盘供应商：
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        始终以小写形式输入 <replaceable>DISK_VENDOR_STRING</replaceable>。
       </para>
      </tip>
      <para>
       要获取有关磁盘型号和供应商的详细信息，请检查以下命令的输出：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       磁盘是否为旋转硬盘。SSD 和 NVMe 驱动器不属于旋转硬盘。
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       部署将<emphasis>所有</emphasis>可用驱动器都用于 OSD 的节点：
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       还可通过限制匹配磁盘的数量来过滤：
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>按大小过滤设备</title>
    <para>
     您可以按磁盘设备大小对其过滤，可以按确切大小也可以按大小范围来过滤。<option>size:</option> 参数接受使用下列格式的自变量：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - 包括大小为该值的磁盘。
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - 包括大小在该范围内的磁盘。
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - 包括大小小于或等于 10 GB 的磁盘。
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - 包括大小等于或大于 40 GB 的磁盘。
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>按磁盘大小匹配</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>需要引号</title>
     <para>
      如果使用“:”分隔符，您需要用引号括住大小，否则“:”符号将被解释为新的配置哈希。
     </para>
    </note>
    <tip>
     <title>单位缩写</title>
     <para>
      您可以指定以兆字节 (M) 或太字节 (T) 为单位的大小，而不能指定以千兆字节 (G) 为单位的大小。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>DriveGroups 示例</title>
    <para>
     本节包含其他 OSD 设置的示例。
    </para>
    <example>
     <title>简单设置</title>
     <para>
      下面的示例说明了使用相同设置的两个节点：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 个 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      对应的 <filename>drive_groups.yml</filename> 文件如下所示：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      这样的配置简单有效。但问题是管理员将来可能要添加来自不同供应商的磁盘，而这样的磁盘不在可添加范围内。您可以通过减少针对驱动器核心属性的过滤器来予以改进。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      在上面的示例中，我们强制将所有旋转设备声明为“data devices”，所有非旋转设备将被用作“shared devices”（wal、db）。
     </para>
     <para>
      如果您知道大小超过 2 TB 的驱动器将始终充当较慢的数据设备，则可以按大小过滤：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>高级设置</title>
     <para>
      下面的示例说明了两种不同的设置：20 个 HDD 将共享 2 个 SSD，而 10 个 SSD 将共享 2 个 NVMe。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 个 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 个 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型号：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      这样的设置可使用如下两个布局定义：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>包含非一致节点的高级设置</title>
     <para>
      上述示例假设所有节点的驱动器都相同，但情况并非总是如此：
     </para>
     <para>
      节点 1-5：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 个 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      节点 6-10：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 个 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      您可以在布局中使用“target”键来定位特定节点。Salt 定位标记可让事情变得简单：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      后接
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>专家设置</title>
     <para>
      上述所有案例都假设 WAL 和 DB 使用相同设备，但也有可能会在专用设备上部署 WAL：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 个 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 个 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型号：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>复杂（和不太可能的）设置</title>
     <para>
      在以下设置中，我们尝试定义：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        由 1 个 NVMe 支持 20 个 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 个 SSD (db) 和 1 个 NVMe (wal) 支持 2 个 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 个 NVMe 支持 8 个 SSD
       </para>
      </listitem>
      <listitem>
       <para>
        2 个独立 SSD（加密）
       </para>
      </listitem>
      <listitem>
       <para>
        1 个 HDD 作为备用，不应部署
       </para>
      </listitem>
     </itemizedlist>
     <para>
      共使用如下数量的驱动器：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 个 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型号：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 个 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型号：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 个 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          供应商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型号：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      DriveGroups 定义如下所示：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      在文件的整个分析过程中，将始终保留一个 HDD。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>删除 OSD</title>
   <para>
    在从集群中删除 OSD 节点之前，请确认该集群中的可用磁盘空间是否比您要删除的 OSD 磁盘大。请注意，删除 OSD 会导致整个集群进行重新平衡。
   </para>
   <procedure>
    <step>
     <para>
      通过获取其 ID 来识别要删除的 OSD：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      从集群中删除一个或多个 OSD：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      例如：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      您可以查询删除操作的状态：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>停止 OSD 删除</title>
    <para>
     安排 OSD 删除后，您可以视需要停止删除。以下命令将重置 OSD 的初始状态并将其从队列中删除：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>替换 OSD</title>
   <para>
    要在保留其 ID 的情况下替换 OSD，请运行：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    替换 OSD 与删除 OSD 基本相同（有关更多详细信息，请参见<xref linkend="removing-node-osds"/>），只不过 OSD 不是从 CRUSH 层次结构中永久删除，而是被指定了一个 <literal>destroyed</literal> 标志。
   </para>
   <para>
    <literal>destroyed</literal> 标志用于确定将在下一次 OSD 部署期间重复使用的 OSD ID。新添加的符合 DriveGroups 规范的磁盘（有关更多详细信息，请参见<xref linkend="drive-groups"/>）将被指定其所替换磁盘的 OSD ID。
   </para>
   <tip>
    <para>
     追加 <option>--dry-run</option> 选项不会执行实际替换，而会预览通常会发生的步骤。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>将 Salt Master 移至新节点</title>

  <para>
   如果需要用新 Salt Master 主机替换 Salt Master 主机，请执行以下步骤：
  </para>

  <procedure>
   <step>
    <para>
     导出集群配置并备份导出的 JSON 文件。有关详细信息，请参见<xref linkend="deploy-cephadm-configure-export"/>。
    </para>
   </step>
   <step>
    <para>
     如果旧的 Salt Master 也是集群中的唯一管理节点，则需要将 <filename>/etc/ceph/ceph.client.admin.keyring</filename> 和 <filename>/etc/ceph/ceph.conf</filename> 手动移至新的 Salt Master。
    </para>
   </step>
   <step>
    <para>
     停止并禁用旧的 Salt Master 节点上的 Salt Master <systemitem class="daemon">systemd</systemitem> 服务：
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     如果旧的 Salt Master 节点不再位于集群中，还要停止并禁用 Salt Minion <systemitem class="daemon">systemd</systemitem> 服务：
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      如果旧的 Salt Master 节点上有任何正在运行的 Ceph 守护进程（MON、MGR、OSD、MDS、网关、监控），请不要停止或禁用 <literal>salt-minion.service</literal>。
     </para>
    </warning>
   </step>
   <step>
    <para>
     按照<xref linkend="deploy-os"/>中所述的过程在新的 Salt Master 上安装 SUSE Linux Enterprise Server 15 SP2。
    </para>
    <tip>
     <title>转换 Salt Minion</title>
     <para>
      为便于将 Salt Minion 转换为新的 Salt Master，请从每个 Salt Minion 中删除原来的 Salt Master 的公共密钥：
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     在新的 Salt Master 上安装 <package>salt-master</package> 包和
     <package>salt-minion</package> 包（如适用）。
    </para>
   </step>
   <step>
    <para>
     在新的 Salt Master 节点上安装 <systemitem class="resource">ceph-salt</systemitem>：
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      确保在继续之前运行所有三个命令。这些命令是幂等的；是否重复并不重要。
     </para>
    </important>
   </step>
   <step>
    <para>
     在集群中包含新的 Salt Master，如<xref linkend="deploy-cephadm-cephsalt"/>、<xref linkend="deploy-cephadm-configure-minions"/>和<xref linkend="deploy-cephadm-configure-admin"/>中所述。
    </para>
   </step>
   <step>
    <para>
     导入备份的集群配置并应用该配置：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      导入之前，请在导出的 <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> 文件中重命名 Salt Master 的 <literal>minion id</literal>。
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>更新集群节点</title>

  <para>
   定期应用滚动更新，以使 Ceph 集群节点保持最新。
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>软件储存库</title>
   <para>
    使用最新的软件包增补集群之前，请确认集群的所有节点均可访问相关的储存库。有关所需储存库的完整列表，请参见<xref linkend="verify-previous-upgrade-patch-repos-repos"/>。
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>储存库暂存</title>
   <para>
    如果您使用向集群节点提供软件储存库的暂存工具（例如 SUSE Manager、储存库管理工具或 RMT），请确认 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage 的“Updates”储存库的阶段都是在同一时刻创建的。
   </para>
   <para>
    强烈建议您使用暂存工具来应用增补程序级别为 <literal>frozen</literal> 或 <literal>staged</literal> 的增补程序。这样可确保加入集群的新节点具有与已在集群中运行的节点相同的增补程序级别。通过这种方法，您无需向集群的所有节点都应用最新增补程序，新节点也能加入集群。
   </para>
  </sect2>

  <sect2>
   <title>Ceph 服务停机时间</title>
   <para>
    集群节点可能会在更新期间重引导，具体视配置而定。如果对象网关、Samba 网关、NFS Ganesha 或 iSCSI 等服务存在单一故障点，客户端计算机可能会暂时与相应节点正在重引导的服务断开连接。
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>运行更新</title>
   <para>
    要将所有集群节点上的软件包更新到最新版本，请运行以下命令：
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>更新 Ceph</title>

  <para>
   您可以指示 cephadm 将 Ceph 从一个 Bug 修复版本更新到另一个版本。Ceph 服务的自动更新遵循建议的顺序进行，即从 Ceph Manager、Ceph Monitor 开始更新，然后继续更新 Ceph OSD、元数据服务器和对象网关等其他服务。只有当 Ceph 指示集群将仍然可用之后，才会重启动每个守护进程。
  </para>

  <note>
   <para>
    以下更新过程使用 <command>ceph orch upgrade</command> 命令。请注意，以下说明详细介绍了如何使用某个产品版本（例如维护更新）更新您的 Ceph 集群，但<emphasis>未</emphasis>提供如何将集群从一个产品版本升级到另一个产品版本的说明。
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>启动更新</title>
   <para>
    开始更新之前，请确认所有节点当前都处于联机状态，并且集群运行状况良好：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    更新到某个特定的 Ceph 版本：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    升级主机上的包：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>监控更新</title>
   <para>
    运行以下命令可确定是否正在进行更新：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    如果正在进行更新，您将在 Ceph 状态输出中看到一个进度条：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    您还可以查看 cephadm 日志：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>取消更新</title>
   <para>
    您可以随时停止更新过程：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>停止或重引导集群</title>

  <para>
   在某些情况下，可能需要停止或重引导整个集群。建议您仔细检查运行中服务的依赖项。下列步骤概要说明如何停止和启动集群：
  </para>

  <procedure>
   <step>
    <para>
     告知 Ceph 集群不要将 OSD 标记为 out：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     按下面的顺序停止守护进程和节点：
    </para>
    <orderedlist>
     <listitem>
      <para>
       存储客户端
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或对象网关
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     根据需要执行维护任务。
    </para>
   </step>
   <step>
    <para>
     以与关闭过程相反的顺序启动节点和服务器：
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或对象网关
      </para>
     </listitem>
     <listitem>
      <para>
       存储客户端
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     删除 noout 标志：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>删除整个 Ceph 集群</title>

  <para>
   <command>ceph-salt purge</command> 命令可删除整个 Ceph 集群。如果部署了更多的 Ceph 集群，则将清除 <command>ceph -s</command> 报告的集群。这样，您便可以在测试不同的设置时清理集群环境。
  </para>

  <para>
   为防止意外删除，编制流程会检查是否解除了安全措施。您可以通过运行以下命令来解除安全措施并删除 Ceph 集群：
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
