<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>操作任務</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>是</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>修改叢集組態</title>

  <para>
   若要修改現有 Ceph 叢集的組態，請執行以下步驟：
  </para>

  <procedure>
   <step>
    <para>
     將叢集的目前組態輸出至檔案：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     編輯包含組態的檔案並更新相關行。可在<xref linkend="deploy-cephadm-day2"/>和<xref linkend="drive-groups"/>中找到規格範例。
    </para>
   </step>
   <step>
    <para>
     套用新組態：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>新增節點</title>

  <para>
   若要向 Ceph 叢集新增新的節點，請執行以下步驟：
  </para>

  <procedure>
   <step>
    <para>
     在新主機上安裝 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage。如需相關資訊，請參閱<xref linkend="deploy-os"/>。
    </para>
   </step>
   <step>
    <para>
     將主機設定為已存在 Salt Master 的 Salt Minion。如需相關資訊，請參閱<xref linkend="deploy-salt"/>。
    </para>
   </step>
   <step>
    <para>
     將新主機新增至 <systemitem class="resource">ceph-salt</systemitem> 並讓 cephadm 能識別它，例如：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     如需相關資訊，請參閱<xref linkend="deploy-cephadm-configure-minions"/>。
    </para>
   </step>
   <step>
    <para>
     確認節點是否已新增至 <systemitem class="resource">ceph-salt</systemitem>：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     將組態套用於新叢集主機：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     確認新增的主機現在是否屬於 cephadm 環境：
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>移除節點</title>

  <tip>
   <title>移除 OSD</title>
   <para>
    如果您要移除的節點執行有 OSD，請先從中移除 OSD，然後檢查該節點上是否未執行任何 OSD。如需移除 OSD 的更多詳細資料，請參閱<xref linkend="removing-node-osds"/>。
   </para>
  </tip>

  <para>
   若要從叢集移除節點，請執行以下操作：
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     對於除 <literal>node-exporter</literal> 和 <literal>crash</literal> 以外的所有 Ceph 服務類型，請從叢集放置規格檔案 (例如 <filename>cluster.yml</filename>) 中移除節點的主機名稱。如需更多詳細資料，請參閱<xref linkend="cephadm-service-and-placement-specs"/>。例如，如果您要移除名為 <literal>ses-min2</literal> 的主機，請從所有 <literal>placement:</literal> 區段中移除所有出現的 <literal>- ses-min2</literal>：
    </para>
    <para>
     更新
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     針對
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     將您的變更套用於組態檔案：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     從 cephadm 的環境中移除節點：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     如果節點正在執行 <literal>crash.osd.1</literal> 和 <literal>crash.osd.2</literal> 服務，請透過在主機上執行以下指令將其移除：
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     從您要刪除的 Minion 中移除所有角色：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     如果您要移除的 Minion 是開機 Minion，則還需要移除開機角色：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     移除單部主機上的所有 OSD 後，從 CRUSH 地圖中移除該主機：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      桶名稱應該與主機名稱相同。
     </para>
    </note>
   </step>
   <step>
    <para>
     現在您便可以從叢集中移除 Minion：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    如果發生故障，並且您嘗試移除的 Minion 處於永久關閉電源狀態，則需要從 Salt Master 中移除節點：
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    然後，手動從 <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename> 中移除節點。它通常位於 <filename>/srv/pillar/ceph-salt.sls</filename> 中。
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>OSD 管理</title>

  <para>
   本節介紹如何在 Ceph 叢集中新增、去除或移除 OSD。
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>列出磁碟裝置</title>
   <para>
    若要識別所有叢集節點上已使用和未使用的磁碟裝置，請透過執行以下指令列出這些裝置：
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>去除磁碟裝置</title>
   <para>
    若要重新使用磁碟裝置，需要先將其去除 (或<emphasis>清除</emphasis>)：
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     如果您先前在未設定 <literal>unmanaged</literal> 旗標的情況下使用 DriveGroups 或 <option>--all-available-devices</option> 選項部署了 OSD，cephadm 將在您去除這些 OSD 後再自動部署它們。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>使用 DriveGroups 規格新增 OSD。</title>
   <para>
    <emphasis>DriveGroups</emphasis> 用於指定 Ceph 叢集中 OSD 的配置。它們在單個 YAML 檔案中定義。在本節中，我們將使用 <filename>drive_groups.yml</filename> 做為範例。
   </para>
   <para>
    管理員應手動指定一組相關的 OSD (部署在 HDD 和 SDD 組合上的混合 OSD)，或一組使用相同部署選項的 OSD (例如，物件儲存、加密選項相同的各獨立 OSD)。為了避免明確列出裝置，DriveGroups 會使用與 <command>ceph-volume</command> 庫存報告中的幾個所選欄位對應的過濾項目清單。cephadm 將提供一些代碼，用於將這些 DriveGroups 轉換為供使用者檢查的實際裝置清單。
   </para>
   <para>
    將 OSD 規格套用於叢集的指令是：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    若要查看動作預覽並測試您的應用程式，您可以將 <option>--dry-run</option> 選項與 <command>ceph orch apply osd</command> 指令結合使用。例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    如果 <option>--dry-run</option> 輸出符合您的預期，則只需重新執行指令而無需使用 <option>--dry-run</option> 選項。
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>未受管理 OSD</title>
    <para>
     將符合 DriveGroups 規格的所有可用的乾淨磁碟裝置新增至叢集後，它們將自動做為 OSD 使用。此行為稱為<emphasis>受管理</emphasis>模式。
    </para>
    <para>
     若要停用<emphasis>受管理</emphasis>模式，請將 <literal>unmanaged: true</literal> 行新增至相關規格，例如：
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      若要將已部署的 OSD 從<emphasis>受管理</emphasis>模式變更為<emphasis>未受管理</emphasis>模式，請於執行<xref linkend="modifying-cluster-configuration"/>中所述的程序期間在適當位置新增 <literal>unmanaged: true</literal> 行。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>DriveGroups 規格</title>
    <para>
     下面是 DriveGroups 規格檔案的範例：
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         先前在 DeepSea 中名為「加密」的選項已被重新命名為「已加密」。在 SUSE Enterprise Storage 7 中套用 DriveGroups 時，請確定在服務規格中使用此新術語，否則 <command>ceph orch apply</command> 操作將失敗。
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>相符磁碟裝置</title>
    <para>
     您可以使用以下過濾器描述規格：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       依磁碟型號：
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       依磁碟廠商：
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        永遠以小寫形式輸入 <replaceable>DISK_VENDOR_STRING</replaceable>。
       </para>
      </tip>
      <para>
       若要獲取有關磁碟型號和廠商的詳細資料，請檢查以下指令的輸出：
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       磁碟是否為旋轉式硬碟。SSD 和 NVMe 磁碟機不屬於旋轉式硬碟。
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       部署將<emphasis>全部</emphasis>可用磁碟機皆用於 OSD 的節點：
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       此外，還可透過限制相符磁碟的數量來過濾：
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>依大小過濾裝置</title>
    <para>
     您可以依磁碟裝置大小對其過濾，可以依確切大小也可以依大小範圍來過濾。<option>size:</option> 參數接受使用下列格式的引數：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - 包括大小為該值的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - 包括大小在該範圍內的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - 包括大小小於或等於 10 GB 的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - 包括大小等於或大於 40 GB 的磁碟。
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>依磁碟大小比對</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>需要引號</title>
     <para>
      如果使用「:」分隔符，您需要使用引號括住大小，否則系統會將「:」符號解譯為新組態雜湊。
     </para>
    </note>
    <tip>
     <title>單位捷徑</title>
     <para>
      您可以指定以百萬位元組 (M) 或兆位元組 (T) 計的大小，而不能指定以十億位元組 (G) 計的大小。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>DriveGroups 範例</title>
    <para>
     本節包含其他 OSD 設定的範例。
    </para>
    <example>
     <title>簡單設定</title>
     <para>
      下面的範例描述了使用相同設定的兩個節點：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      對應的 <filename>drive_groups.yml</filename> 檔案如下所示：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      這樣的組態簡單有效。但問題是管理員將來可能要新增來自不同廠商的磁碟，而這樣的磁碟不在可新增範圍內。您可以透過減少針對磁碟機核心內容的過濾器來予以改進。
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      在上面的範例中，我們強制將所有旋轉式裝置宣告為「data devices」，將所有非旋轉式裝置當成「shared devices」(wal、db) 使用。
     </para>
     <para>
      如果您知道大小超過 2 TB 的磁碟機將一律做為較慢的資料裝置，則可以依大小過濾：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>進階設定</title>
     <para>
      下面的範例描述了兩種不同的設定：20 個 HDD 將共用 2 個 SSD，而 10 個 SSD 將共用 2 個 NVMe。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      這樣的設定可使用如下兩個配置定義：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>包含非統一節點的進階設定</title>
     <para>
      上述範例假設所有節點的磁碟機都相同，但情況並非總是如此：
     </para>
     <para>
      節點 1-5：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      節點 6-10：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      您可以在配置中使用「target」鍵來定位特定節點。Salt 定位標記可讓事情變得簡單：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      後接
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>專家設定</title>
     <para>
      上述所有案例都假設 WAL 和 DB 使用相同裝置，但也有可能會在專屬裝置上部署 WAL：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>複雜 (和不太可能的) 設定</title>
     <para>
      在以下設定中，我們嘗試定義：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        由 1 個 NVMe 支援 20 個 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 個 SSD (db) 和 1 個 NVMe (wal) 支援 2 個 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 個 NVMe 支援 8 個 SSD
       </para>
      </listitem>
      <listitem>
       <para>
        2 個獨立 SSD (加密)
       </para>
      </listitem>
      <listitem>
       <para>
        1 個 HDD 做為備用，不應部署
       </para>
      </listitem>
     </itemizedlist>
     <para>
      共使用如下數量的磁碟機：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      DriveGroups 定義如下所示：
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      在檔案的整個剖析過程中，一直會保留一個 HDD。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>移除 OSD</title>
   <para>
    在從叢集中移除 OSD 節點之前，請確認該叢集中的可用磁碟空間是否比您要移除的 OSD 磁碟大。請注意，移除 OSD 會導致整個叢集進行重新平衡。
   </para>
   <procedure>
    <step>
     <para>
      透過獲取其 ID 來識別要移除的 OSD：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      從叢集中移除一或多個 OSD：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      例如：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      您可以查詢移除操作的狀態：
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>停止 OSD 移除</title>
    <para>
     排程 OSD 移除後，您可以視需要停止移除。以下指令將重設 OSD 的初始狀態並將其從佇列中移除：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>取代 OSD</title>
   <para>
    若要在保留其 ID 的情況下取代 OSD，請執行：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    取代 OSD 與移除 OSD 基本相同 (如需更多詳細資料，請參閱<xref linkend="removing-node-osds"/>)，只不過 OSD 不是從 CRUSH 階層中永久移除，而是被指定了一個 <literal>destroyed</literal> 旗標。
   </para>
   <para>
    <literal>destroyed</literal> 旗標用於確定將在下一次 OSD 部署期間重複使用的 OSD ID。新增的符合 DriveGroups 規格的磁碟 (如需更多詳細資料，請參閱<xref linkend="drive-groups"/>) 將被指定其所取代之磁碟的 OSD ID。
   </para>
   <tip>
    <para>
     附加 <option>--dry-run</option> 選項不會執行實際取代，而會預覽通常會發生的步驟。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>將 Salt Master 移至新節點</title>

  <para>
   如果需要以新 Salt Master 主機取代 Salt Master 主機，請執行以下步驟：
  </para>

  <procedure>
   <step>
    <para>
     輸出叢集組態並備份輸出的 JSON 檔案。如需更多詳細資料，請參閱<xref linkend="deploy-cephadm-configure-export"/>。
    </para>
   </step>
   <step>
    <para>
     如果舊的 Salt Master 也是叢集中的唯一管理節點，則需要將 <filename>/etc/ceph/ceph.client.admin.keyring</filename> 和 <filename>/etc/ceph/ceph.conf</filename> 手動移至新 Salt Master。
    </para>
   </step>
   <step>
    <para>
     停止並停用舊 Salt Master 節點上的 Salt Master <systemitem class="daemon">systemd</systemitem> 服務：
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     如果舊 Salt Master 節點不再位於叢集中，還要停止並停用 Salt Minion <systemitem class="daemon">systemd</systemitem> 服務：
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      如果舊 Salt Master 節點上有任何執行中 Ceph 精靈 (MON、MGR、OSD、MDS、閘道、監控)，請不要停止或停用 <literal>salt-minion.service</literal>。
     </para>
    </warning>
   </step>
   <step>
    <para>
     依據<xref linkend="deploy-os"/>中所述的程序，在新的 Salt Master 上安裝 SUSE Linux Enterprise Server 15 SP2。
    </para>
    <tip>
     <title>轉換 Salt Minion</title>
     <para>
      為便於將 Salt Minion 轉換為新的 Salt Master，請從每個 Salt Minion 中移除原始 Salt Master 的公用金鑰：
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     將 <package>salt-master</package> 套件和
     <package>salt-minion</package> 套件 (如適用) 安裝到新的 Salt Master。
    </para>
   </step>
   <step>
    <para>
     在新的 Salt Master 節點上安裝 <systemitem class="resource">ceph-salt</systemitem>：
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      確定在繼續之前執行所有三個指令。這些指令是等冪的；是否重複並不重要。
     </para>
    </important>
   </step>
   <step>
    <para>
     在叢集中包含新的 Salt Master，如<xref linkend="deploy-cephadm-cephsalt"/>、<xref linkend="deploy-cephadm-configure-minions"/>和<xref linkend="deploy-cephadm-configure-admin"/>中所述。
    </para>
   </step>
   <step>
    <para>
     輸入備份的叢集組態並套用該組態：
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      輸入之前，請在輸出的 <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> 檔案中重新命名 Salt Master 的 <literal>minion id</literal>。
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>更新叢集節點</title>

  <para>
   定期套用滾存更新，以使 Ceph 叢集節點保持最新。
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>軟體儲存庫</title>
   <para>
    使用最新的套裝軟體修補叢集之前，請確認叢集的所有節點均可存取相關的儲存庫。如需所需儲存庫的完整清單，請參閱<xref linkend="verify-previous-upgrade-patch-repos-repos"/>。
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>儲存庫暫存</title>
   <para>
    如果您使用向叢集節點提供軟體儲存庫的暫存工具 (例如 SUSE Manager、儲存庫管理工具或 RMT)，請確認 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage 的「Updates」儲存庫的階段都是在同一時刻建立的。
   </para>
   <para>
    強烈建議您使用暫存工具來套用修補程式層級為 <literal>frozen</literal> 或 <literal>staged</literal> 的修補程式。這樣可確保加入叢集的新節點具有與已在叢集中執行的節點相同的修補程式層級。透過這種方法，您無需向叢集的所有節點都套用最新修補程式，新節點也能加入叢集。
   </para>
  </sect2>

  <sect2>
   <title>Ceph 服務停機時間</title>
   <para>
    叢集節點可能會在更新期間重新開機，具體視組態而定。如果物件閘道、Samba 閘道、NFS Ganesha 或 iSCSI 等服務存在單一故障點，用戶端機器可能會暫時解除與相應節點正在重新開機的服務的連接。
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>執行更新</title>
   <para>
    若要將所有叢集節點上的軟體套件更新至最新版本，請執行以下指令：
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>更新 Ceph</title>

  <para>
   您可以指示 cephadm 將 Ceph 從一個錯誤修復版本更新至另一個版本。Ceph 服務的自動更新遵循建議的順序進行，即從 Ceph 管理員、Ceph 監控程式開始更新，然後繼續更新 Ceph OSD、中繼資料伺服器和物件閘道等其他服務。只有當 Ceph 指示叢集將仍然可用之後，才會重新啟動每個精靈。
  </para>

  <note>
   <para>
    以下更新程序使用 <command>ceph orch upgrade</command> 指令。請注意，以下說明詳細介紹了如何使用某個產品版本 (例如維護更新) 更新您的 Ceph 叢集，但<emphasis>未</emphasis>提供如何將叢集從一個產品版本升級至另一個產品版本的說明。
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>啟動更新</title>
   <para>
    開始更新之前，請確認所有節點目前都處於線上狀態，並且叢集狀況良好：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    更新至某個特定的 Ceph 版本：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    升級主機上的套件：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>監控更新</title>
   <para>
    執行以下指令可確定是否正在進行更新：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    如果正在進行更新，您將在 Ceph 狀態輸出中看到一個進度列：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    您還可以查看 cephadm 記錄：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>取消更新</title>
   <para>
    您可以隨時停止更新程序：
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>將叢集停止或重新開機</title>

  <para>
   在某些情況下，可能需要將整個叢集停止或重新開機。建議您仔細檢查執行中服務的相依項。下列步驟簡要說明如何停止和啟動叢集：
  </para>

  <procedure>
   <step>
    <para>
     告知 Ceph 叢集不要將 OSD 標示為 out：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     依下面的順序停止精靈和節點：
    </para>
    <orderedlist>
     <listitem>
      <para>
       儲存用戶端
      </para>
     </listitem>
     <listitem>
      <para>
       閘道，例如 NFS Ganesha 或物件閘道
      </para>
     </listitem>
     <listitem>
      <para>
       中繼資料伺服器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 管理員
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 監控程式
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     視需要執行維護任務。
    </para>
   </step>
   <step>
    <para>
     依與關閉過程相反的順序啟動節點和伺服器：
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph 監控程式
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 管理員
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       中繼資料伺服器
      </para>
     </listitem>
     <listitem>
      <para>
       閘道，例如 NFS Ganesha 或物件閘道
      </para>
     </listitem>
     <listitem>
      <para>
       儲存用戶端
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     移除 noout 旗標：
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>移除整個 Ceph 叢集</title>

  <para>
   <command>ceph-salt purge</command> 指令可移除整個 Ceph 叢集。如果部署了更多的 Ceph 叢集，則將清除 <command>ceph -s</command> 報告的叢集。這樣，您便可以在測試不同的設定時清理叢集環境。
  </para>

  <para>
   為防止意外刪除，協調程序會檢查是否解除了安全措施。您可以透過執行以下指令來解除安全措施並移除 Ceph 叢集：
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
