<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Task operativi</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sì</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <sect1 xml:id="modifying-cluster-configuration">
  <title>Modifica della configurazione del cluster</title>

  <para>
   Per modificare la configurazione di un cluster Ceph esistente, seguire la procedura indicata di seguito:
  </para>

  <procedure>
   <step>
    <para>
     Esportare la configurazione attuale del cluster in un file:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch ls --export --format yaml &gt; cluster.yaml</screen>
   </step>
   <step>
    <para>
     Modificare il file con la configurazione e aggiornare le righe pertinenti. In <xref linkend="deploy-cephadm-day2"/> e nella <xref linkend="drive-groups"/> sono disponibili esempi sulle specifiche.
    </para>
   </step>
   <step>
    <para>
     Applicare la nuova configurazione:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i cluster.yaml</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="adding-node">
  <title>Aggiunta di nodi</title>

  <para>
   Per aggiungere un nuovo nodo a un cluster Ceph, seguire la procedura indicata di seguito:
  </para>

  <procedure>
   <step>
    <para>
     Installare SUSE Linux Enterprise Server e SUSE Enterprise Storage sul nuovo host. Per ulteriori informazioni consultare <xref linkend="deploy-os"/>.
    </para>
   </step>
   <step>
    <para>
     Configurare l'host come Salt Minion di un Salt Master già esistente. Per ulteriori informazioni consultare <xref linkend="deploy-salt"/>.
    </para>
   </step>
   <step>
    <para>
     Aggiungere il nuovo host a <systemitem class="resource">ceph-salt</systemitem> e renderlo riconoscibile da cephadm, ad esempio:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions add ses-min5.example.com
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/roles/cephadm add ses-min5.example.com
</screen>
    <para>
     Per ulteriori informazioni consultare <xref linkend="deploy-cephadm-configure-minions"/>.
    </para>
   </step>
   <step>
    <para>
     Verificare che il nodo sia stato aggiunto a <systemitem class="resource">ceph-salt</systemitem>:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt config /ceph_cluster/minions ls
o- minions ................................................. [Minions: 5]
[...]
  o- ses-min5.example.com .................................... [no roles]
</screen>
   </step>
   <step>
    <para>
     Applicare la configurazione al nuovo host del cluster:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt apply ses-min5.example.com
</screen>
   </step>
   <step>
    <para>
     Verificare che l'host appena aggiunto appartenga all'ambiente cephadm:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch host ls
HOST                   ADDR                    LABELS   STATUS
[...]
ses-min5.example.com   ses-min5.example.com
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>Rimozione di nodi</title>

  <tip>
   <title>rimozione degli OSD</title>
   <para>
    Se sul nodo da rimuovere sono in esecuzione degli OSD, rimuovere innanzitutto questi ultimi dal nodo e verificare che non ne siano rimasti altri in esecuzione. Per ulteriori dettagli sulla rimozione degli OSD, fare riferimento alla <xref linkend="removing-node-osds"/>.
   </para>
  </tip>

  <para>
   Per rimuovere un nodo da un cluster, procedere come segue:
  </para>

  <procedure xml:id="removing-node">
   <step>
    <para>
     Per tutti i tipi di servizio Ceph, ad eccezione di <literal>node-exporter</literal> e <literal>crash</literal>, rimuovere il nome host del nodo dal file della specifica del posizionamento del cluster (ad esempio <filename>cluster.yml</filename>). Per ulteriori dettagli, fare riferimento al <xref linkend="cephadm-service-and-placement-specs"/>. Ad esempio, se si sta rimuovendo l'host denominato <literal>ses-min2</literal>, rimuovere tutte le occorrenze di <literal>ses-min-2</literal> da tutte le sezioni <literal>placement:</literal>:
    </para>
    <para>
     Aggiornare
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min2
  - ses-min3
</screen>
    <para>
     in
    </para>
<screen>
service_type: rgw
service_id: <replaceable>EXAMPLE_NFS</replaceable>
placement:
  hosts:
  - ses-min3
</screen>
    <para>
     Applicare le modifiche al file di configurazione:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply -i <replaceable>rgw-example.yaml</replaceable></screen>
   </step>
   <step>
    <para>
     Rimuovere il nodo dall'ambiente cephadm:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch host rm ses-min2</screen>
   </step>
   <step>
    <para>
     Se sul nodo sono in esecuzione i servizi <literal>crash.osd.1</literal> e <literal>crash.osd.2</literal>, rimuoverli eseguendo il comando seguente sull'host:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid <replaceable>CLUSTER_ID</replaceable> --name <replaceable>SERVICE_NAME</replaceable>
</screen>
    <para>
     Esempio:
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.1
<prompt>root@minion &gt; </prompt>cephadm rm-daemon --fsid b4b30c6e... --name crash.osd.2
</screen>
   </step>
   <step>
    <para>
     Rimuovere tutti i ruoli dal minion che si desidera eliminare:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/throughput remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/tuned/latency remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/cephadm remove ses-min2
<prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/admin remove ses-min2</screen>
    <para>
     Se il minion che si desidera rimuovere è il minion di bootstrap, è necessario rimuovere anche il ruolo di bootstrap:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/roles/bootstrap reset</screen>
   </step>
   <step>
    <para>
     In seguito alla rimozione di tutti gli OSD su un singolo host, rimuovere l'host dalla mappa CRUSH:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd crush remove <replaceable>bucket-name</replaceable></screen>
    <note>
     <para>
      Il nome del compartimento deve coincidere con il nome host.
     </para>
    </note>
   </step>
   <step>
    <para>
     Adesso, è possibile rimuovere il minion dal cluster:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt config /ceph_cluster/minions remove ses-min2</screen>
   </step>
  </procedure>

  <important>
   <para>
    In caso di errore e se il minion che si sta tentando di rimuovere è nello stato di disattivazione permanente, sarà necessario rimuovere il nodo dal Salt Master:
   </para>
<screen><prompt>root@master # </prompt>salt-key -d <replaceable>minion_id</replaceable></screen>
   <para>
    Quindi, rimuovere manualmente il nodo da <filename><replaceable>pillar_root</replaceable>/ceph-salt.sls</filename>. In genere, questo si trova in <filename>/srv/pillar/ceph-salt.sls</filename>.
   </para>
  </important>
 </sect1>
 <sect1 xml:id="osd-management">
  <title>Gestione degli OSD</title>

  <para>
   Questa sezione descrive come aggiungere, cancellare o rimuovere gli OSD in un cluster Ceph.
  </para>

  <sect2 xml:id="osd-management-listing">
   <title>Elenco dei dispositivi disco</title>
   <para>
    Per identificare i dispositivi disco utilizzati e non utilizzati su tutti i nodi del cluster, elencarli eseguendo il comando seguente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST       PATH      TYPE SIZE  DEVICE  AVAIL REJECT REASONS
ses-master /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vda  hdd  42.0G         False locked
ses-min1   /dev/vdb  hdd  8192M  387836 False locked, LVM detected, Insufficient space (&lt;5GB) on vgs
ses-min2   /dev/vdc  hdd  8192M  450575 True
</screen>
  </sect2>

  <sect2 xml:id="osd-management-erasing">
   <title>Cancellazione dei dispositivi disco</title>
   <para>
    Per riutilizzare un dispositivo disco, è necessario innanzitutto cancellarlo (o <emphasis>rimuoverlo con zap</emphasis>):
   </para>
<screen>ceph orch device zap <replaceable>HOST_NAME</replaceable> <replaceable>DISK_DEVICE</replaceable></screen>
   <para>
    Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch device zap ses-min2 /dev/vdc</screen>
   <note>
    <para>
     Se in precedenza l'utente ha distribuito gli OSD tramite i DriveGroups o l'opzione <option>--all-available-devices</option> mentre il flag <literal>unmanaged</literal> non era impostato, cephadm distribuirà automaticamente questi OSD in seguito alla loro cancellazione.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="drive-groups">
   <title>Aggiunta di OSD tramite la specifica dei DriveGroups</title>
   <para>
    I <emphasis>DriveGroups</emphasis> specificano i layout degli OSD nel cluster Ceph. Questi ultimi vengono definiti in un singolo file YAML. In questa sezione, verrà utilizzato <filename>drive_groups.yml</filename> come esempio.
   </para>
   <para>
    L'amministratore deve specificare manualmente un gruppo di OSD correlati tra di loro (OSD ibridi distribuiti su unità HDD e SDD) o condividere le stesse opzioni di distribuzione (ad esempio lo stesso archivio dati, la stessa opzione di cifratura, gli stessi OSD stand-alone). Per evitare di elencare esplicitamente i dispositivi, i DriveGroups utilizzano un elenco di elementi di filtro che corrispondono ad alcuni campi selezionati dei rapporti di archivio di <command>ceph-volume</command>. In cephadm è fornito un codice che traduce tali DriveGroups in elenchi di dispositivi effettivi che l'utente potrà esaminare.
   </para>
   <para>
    Il comando da eseguire per applicare la specifica OSD al cluster è:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename></screen>
   <para>
    Per visualizzare un'anteprima delle azioni e testare l'applicazione, è possibile utilizzare l'opzione <option>--dry-run</option> insieme al comando <command>ceph orch apply osd</command>. Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch apply osd -i <filename>drive_groups.yml</filename> --dry-run
...
+---------+------+------+----------+----+-----+
|SERVICE  |NAME  |HOST  |DATA      |DB  |WAL  |
+---------+------+------+----------+----+-----+
|osd      |test  |mgr0  |/dev/sda  |-   |-    |
|osd      |test  |mgr0  |/dev/sdb  |-   |-    |
+---------+------+------+----------+----+-----+</screen>
   <para>
    Se l'output di <option>--dry-run</option> soddisfa le aspettative, eseguire nuovamente il comando senza l'opzione <option>--dry-run</option>.
   </para>
   <sect3 xml:id="unmanaged-osds">
    <title>OSD non gestiti</title>
    <para>
     Tutti i dispositivi disco puliti disponibili corrispondenti alla specifica dei DriveGroups verranno utilizzati automaticamente come OSD dopo essere stati aggiunti al cluster. Per descrivere questo comportamento si parla di modalità <emphasis>gestita</emphasis>.
    </para>
    <para>
     Per disabilitare la modalità <emphasis>gestita</emphasis>, aggiungere la riga <literal>unmanaged: true</literal> alle specifiche pertinenti, ad esempio:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
 hosts:
 - ses-min2
 - ses-min3
encrypted: true
unmanaged: true
</screen>
    <tip>
     <para>
      Per modificare gli OSD già distribuiti dalla modalità <emphasis>gestita</emphasis> a quella <emphasis>non gestita</emphasis>, aggiungere le righe <literal>unmanaged: true</literal> dove applicabile durante la procedura descritta nella <xref linkend="modifying-cluster-configuration"/>.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="drive-groups-specs">
    <title>Specifica dei DriveGroups</title>
    <para>
     Di seguito è riportato un file della specifica dei DriveGroups di esempio:
    </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
db_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
wal_devices:
  drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
block_wal_size: '5G'  # (optional, unit suffixes permitted)
block_db_size: '5G'   # (optional, unit suffixes permitted)
encrypted: true       # 'True' or 'False' (defaults to 'False')
</screen>
     <note>
       <para>
         L'opzione precedentemente chiamata "encryption" in DeepSea adesso si chiama "encrypted". Quando si applicano i DriveGroups in SUSE Enterprise Storage 7, assicurarsi di utilizzare questa nuova terminologia nella specifica del servizio, altrimenti <command>ceph orch apply</command> genererà un errore.
       </para>
     </note>
   </sect3>
   <sect3 xml:id="matching-disk-devices">
    <title>Creazione di corrispondenze dei dispositivi disco</title>
    <para>
     È possibile descrivere la specifica tramite i filtri seguenti:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In base al modello di disco:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       In base al produttore del disco:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <para>
        Utilizzare sempre caratteri minuscoli quando si immette il valore <replaceable>DISK_VENDOR_STRING</replaceable>.
       </para>
      </tip>
      <para>
       Per ottenere dettagli sul produttore e il modello del disco, esaminare l'output del comando seguente:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch device ls
HOST     PATH     TYPE  SIZE DEVICE_ID                  MODEL            VENDOR
ses-min1 /dev/sdb ssd  29.8G SATA_SSD_AF34075704240015  SATA SSD         ATA
ses-min2 /dev/sda ssd   223G Micron_5200_MTFDDAK240TDN  Micron_5200_MTFD ATA
[...]
</screen>
     </listitem>
     <listitem>
      <para>
       Per indicare se si tratta di un disco rotativo o meno. Le unità SSD e NVME non sono rotative.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Per distribuire un nodo utilizzando <emphasis>tutte</emphasis> le unità disponibili per gli OSD:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Inoltre, tramite la limitazione del numero di dischi corrispondenti:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="filtering-devices-size">
    <title>Aggiunta di filtri ai dispositivi in base alle dimensioni</title>
    <para>
     È possibile filtrare i dispositivi disco in base alle dimensioni (valore esatto o intervallo di dimensioni). Il parametro <option>size:</option> accetta gli argomenti nel formato seguente:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       "10G" - include i dischi di dimensioni esatte.
      </para>
     </listitem>
     <listitem>
      <para>
       "10G:40G" - include i dischi di dimensioni comprese nell'intervallo.
      </para>
     </listitem>
     <listitem>
      <para>
       ":10G" - include i dischi di dimensioni inferiori o uguali a 10 GB.
      </para>
     </listitem>
     <listitem>
      <para>
       "40G" - include i dischi di dimensioni uguali o superiori a 40 GB.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Corrispondenza in base alle dimensioni del disco</title>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '40TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <note>
     <title>virgolette obbligatorie</title>
     <para>
      Quando si utilizza il delimitatore ":", occorre racchiudere le dimensioni tra virgolette altrimenti il simbolo ":" verrà interpretato come un nuovo hash di configurazione.
     </para>
    </note>
    <tip>
     <title>scorciatoie di unità</title>
     <para>
      Al posto di Gigabyte (G), è possibile specificare le dimensioni in Megabyte (M) o Terabyte (T).
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Esempi di DriveGroups</title>
    <para>
     Questa sezione include esempi di diverse configurazioni OSD.
    </para>
    <example>
     <title>Configurazione semplice</title>
     <para>
      Questo esempio descrive due nodi con la stessa configurazione:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Il file <filename>drive_groups.yml</filename> corrispondente avrà l'aspetto seguente:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: MC-55-44-XZ
</screen>
     <para>
      Tale configurazione è semplice e valida. Tuttavia, in futuro un amministratore può aggiungere dischi di altri produttori che non verranno inclusi. È possibile ovviare a questo problema riducendo i filtri sulle proprietà di base delle unità:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      Nell'esempio precedente, viene forzata la dichiarazione di tutti i dispositivi a rotazione come "dispositivi di dati" e tutti i dispositivi non a rotazione verranno utilizzati come "dispositivi condivisi" (wal, db).
     </para>
     <para>
      Presupponendo che le unità di più di 2 TB saranno sempre i dispositivi di dati più lenti, è possibile applicare dei filtri in base alle dimensioni:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  size: '2TB:'
db_devices:
  size: ':2TB'
</screen>
    </example>
    <example>
     <title>Configurazione avanzata</title>
     <para>
      Questo esempio descrive due configurazioni diverse: 20 HDD devono condividere 2 SSD, mentre 10 SSD devono condividere 2 NVMe.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      È possibile definire tale configurazione con due layout come segue:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_name2
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  vendor: samsung
  size: 256GB
</screen>
    </example>
    <example>
     <title>Configurazione avanzata con nodi non uniformi</title>
     <para>
      Gli esempi precedenti sono basati sul presupposto che tutti i nodi dispongano delle stesse unità. Tuttavia, non è sempre questo il caso:
     </para>
     <para>
      Nodi da 1 a 5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodi da 6 a 10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      È possibile utilizzare la chiave "target" nel layout per indirizzare nodi specifici. La notazione della destinazione Salt consente di non complicare la configurazione:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_one2five
placement:
  host_pattern: 'node[1-5]'
data_devices:
  rotational: 1
db_devices:
  rotational: 0
</screen>
     <para>
      seguito da
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_rest
placement:
  host_pattern: 'node[6-10]'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Configurazione esperta</title>
     <para>
      In tutti i casi descritti in precedenza si presupponeva che i WAL e i DB utilizzassero lo stesso dispositivo. È tuttavia possibile anche distribuire i WAL su un dispositivo dedicato:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
service_type: osd
service_id: example_drvgrp_name
placement:
  host_pattern: '*'
data_devices:
  model: MC-55-44-XZ
db_devices:
  model: SSD-123-foo
wal_devices:
  model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Configurazione complessa (e improbabile)</title>
     <para>
      Nella configurazione seguente, si tenterà di definire:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD supportati da 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDD supportati da 1 SSD (db) e 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSD supportati da 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSD stand-alone (cifrati)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD è di riserva e non deve essere distribuito.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Di seguito è riportato il riepilogo delle unità utilizzate:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      La definizione dei DriveGroups sarà la seguente:
     </para>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_hdd_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  rotational: 0
db_devices:
  model: MC-55-44-XZ
wal_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_ssd_nvme
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
db_devices:
  model: NVME-QQQQ-987
</screen>
<screen>
service_type: osd
service_id: example_drvgrp_standalone_encrypted
placement:
  host_pattern: '*'
data_devices:
  model: SSD-123-foo
encrypted: True
</screen>
     <para>
      Rimarrà un'unità HDD, poiché il file viene analizzato dall'alto verso il basso.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds">
   <title>Rimozione degli OSD</title>
   <para>
    Prima di rimuovere un nodo OSD dal cluster, verificare che lo spazio su disco disponibile sul cluster sia maggiore del disco OSD che verrà rimosso. Tenere presente che la rimozione di un OSD comporta il ribilanciamento dell'intero cluster.
   </para>
   <procedure>
    <step>
     <para>
      Identificare l'OSD da rimuovere recuperandone l'ID:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch ps --daemon_type osd
NAME   HOST            STATUS        REFRESHED  AGE  VERSION
osd.0  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.1  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.2  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
osd.3  target-ses-090  running (3h)  7m ago     3h   15.2.7.689 ...
</screen>
    </step>
    <step>
     <para>
      Rimuovere uno o più OSD dal cluster:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD1_ID</replaceable> <replaceable>OSD2_ID</replaceable> ...
</screen>
     <para>
      Esempio:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 1 2
</screen>
    </step>
    <step>
     <para>
      È possibile interrogare lo stato dell'operazione di rimozione:
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph orch osd rm status
OSD_ID  HOST         STATE                    PG_COUNT  REPLACE  FORCE  STARTED_AT
2       cephadm-dev  done, waiting for purge  0         True     False  2020-07-17 13:01:43.147684
3       cephadm-dev  draining                 17        False    True   2020-07-17 13:01:45.162158
4       cephadm-dev  started                  42        False    True   2020-07-17 13:01:45.162158
</screen>
    </step>
   </procedure>
   <sect3 xml:id="removing-node-osds-stop">
    <title>Interruzione della rimozione dell'OSD</title>
    <para>
     Se necessario, è possibile interrompere la rimozione di OSD dopo averla pianificata. Il comando seguente reimposterà lo stato iniziale dell'OSD e lo rimuoverà dalla coda:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm stop <replaceable>OSD_SERVICE_ID</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="removing-node-osds-replace">
   <title>Sostituzione degli OSD</title>
   <para>
    Per sostituire un OSD conservandone l'ID, eseguire:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm <replaceable>OSD_SERVICE_ID</replaceable> --replace</screen>
   <para>
    Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch osd rm 4 --replace</screen>
   <para>
    La procedura per la sostituzione di un OSD è identica a quella per la rimozione (vedere la <xref linkend="removing-node-osds"/> per ulteriori dettagli) con l'eccezione che l'OSD non viene rimosso definitivamente dalla gerarchia CRUSH e gli viene invece assegnato un flag <literal>destroyed</literal>.
   </para>
   <para>
    Il flag <literal>destroyed</literal> è utilizzato per determinare gli ID degli OSD che verranno riutilizzati durante la successiva distribuzione degli OSD. Ai dischi appena aggiunti corrispondenti alla specifica dei DriveGroups (vedere la <xref linkend="drive-groups"/> per ulteriori dettagli) verranno assegnati gli ID degli OSD della controparte corrispondente sostituita.
   </para>
   <tip>
    <para>
     Aggiungendo l'opzione <option>--dry-run</option>, non verrà eseguita la sostituzione effettiva, ma verrà visualizzata in anteprima la procedura prevista.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>Trasferimento del Salt Master a un nuovo nodo</title>

  <para>
   Se è necessario sostituire l'host del Salt Master con uno nuovo, seguire la procedura indicata di seguito:
  </para>

  <procedure>
   <step>
    <para>
     Esportare la configurazione del cluster ed eseguire il backup del file JSON esportato. Ulteriori dettagli sono disponibili nel <xref linkend="deploy-cephadm-configure-export"/>.
    </para>
   </step>
   <step>
    <para>
     Se il Salt Master precedente è anche l'unico nodo di amministrazione nel cluster, spostare manualmente <filename>/etc/ceph/ceph.client.admin.keyring</filename> e <filename>/etc/ceph/ceph.conf</filename> nel nuovo Salt Master.
    </para>
   </step>
   <step>
    <para>
     Interrompere e disabilitare il servizio <systemitem class="daemon">systemd</systemitem> del Salt Master sul nodo del Salt Master precedente:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl disable salt-master.service
</screen>
   </step>
   <step>
    <para>
     Se il nodo del Salt Master precedente non è più presente nel cluster, interrompere e disabilitare anche il servizio <systemitem class="daemon">systemd</systemitem> del Salt Minion:
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-minion.service
<prompt>root@master # </prompt>systemctl disable salt-minion.service
</screen>
    <warning>
     <para>
      Non interrompere o disabilitare <literal>salt-minion.service</literal> se sul nodo del Salt Master precedente sono presenti daemon Ceph (MON, MGR, OSD, MDS, gateway, monitoraggio) in esecuzione.
     </para>
    </warning>
   </step>
   <step>
    <para>
     Installare SUSE Linux Enterprise Server 15 SP2 sul nuovo Salt Master seguendo la procedura descritta in <xref linkend="deploy-os"/>.
    </para>
    <tip>
     <title>transizione del Salt Minion</title>
     <para>
      Per semplificare la transizione dei Salt Minion sul nuovo Salt Master, rimuovere la chiave pubblica originale del Salt Master da ciascuno di questi:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     Installare il pacchetto <package>salt-master</package> e, se applicabile, il pacchetto
     <package>salt-minion</package> sul nuovo Salt Master.
    </para>
   </step>
   <step>
    <para>
     Installare <systemitem class="resource">ceph-salt</systemitem> sul nuovo nodo del Salt Master:
    </para>
<screen>
<prompt>root@master # </prompt>zypper install ceph-salt
<prompt>root@master # </prompt>systemctl restart salt-master.service
<prompt>root@master # </prompt>salt '*' saltutil.sync_all
</screen>
    <important>
     <para>
      Assicurarsi di eseguire tutti i tre comandi prima di continuare. I comandi sono idempotenti; non importa se vengono ripetuti.
     </para>
    </important>
   </step>
   <step>
    <para>
     Includere il nuovo Salt Master nel cluster come descritto in <xref linkend="deploy-cephadm-cephsalt"/>, <xref linkend="deploy-cephadm-configure-minions"/> e <xref linkend="deploy-cephadm-configure-admin"/>.
    </para>
   </step>
   <step>
    <para>
     Importare la configurazione del cluster sottoposta a backup e applicarla:
    </para>
<screen>
<prompt>root@master # </prompt>ceph-salt import <replaceable>CLUSTER_CONFIG</replaceable>.json
<prompt>root@master # </prompt>ceph-salt apply
</screen>
    <important>
     <para>
      Rinominare il <literal>minion id</literal> del Salt Master nel file <filename><replaceable>CLUSTER_CONFIG</replaceable>.json</filename> esportato prima di importarlo.
     </para>
    </important>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cephadm-rolling-updates">
  <title>Aggiornamento dei nodi del cluster</title>

  <para>
   Mantenere i nodi del cluster Ceph aggiornati applicando regolarmente gli aggiornamenti in sequenza.
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>Archivi software</title>
   <para>
    Prima di applicare le patch al cluster con i pacchetti software più recenti, verificare che tutti i nodi del cluster dispongano dell'accesso agli archivi pertinenti. Per un elenco completo degli archivi obbligatori, consultare questo riferimento: <xref linkend="verify-previous-upgrade-patch-repos-repos"/>.
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>Gestione provvisoria dell'archivio</title>
   <para>
    Se si utilizza uno strumento di gestione provvisoria, ad esempio SUSE Manager, Repository Management Tool o RMT, tramite cui implementare gli archivi software sui nodi del cluster, verificare che le fasi degli archivi "Updates" di SUSE Linux Enterprise Server e SUSE Enterprise Storage vengano create nello stesso momento.
   </para>
   <para>
    Si consiglia di utilizzare uno strumento di gestione temporanea per l'applicazione di patch con livelli di patch <literal>frozen</literal> o <literal>staged</literal>. In questo modo, sarà possibile assicurarsi che i nuovi nodi uniti al cluster dispongano dello stesso livello di patch dei nodi già in esecuzione al suo interno. Questo evita di dover applicare le patch più recenti a tutti i nodi del cluster prima che i nuovi nodi possano unirsi a quest'ultimo.
   </para>
  </sect2>

  <sect2>
   <title>Tempo di fermo dei servizi Ceph</title>
   <para>
    A seconda della configurazione, i nodi del cluster potrebbero essere riavviati durante l'aggiornamento. Se è presente un single point of failure per servizi come Object Gateway, gateway Samba, NFS Ganesha o iSCSI, i computer del client potrebbero essere temporaneamente disconnessi dai servizi i cui nodi sono in corso di riavvio.
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>Esecuzione dell'aggiornamento</title>
   <para>
    Per aggiornare i pacchetti software su tutti i nodi del cluster alla versione più recente, eseguire il comando seguente:
   </para>
<screen><prompt>root@master # </prompt>ceph-salt update</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="deploy-cephadm-day2-cephupdate">
  <title>Aggiornamento di Ceph</title>

  <para>
   È possibile inviare a cephadm l'istruzione di aggiornare Ceph da una release di correzione di bug all'altra. L'aggiornamento automatico dei servizi Ceph rispetta l'ordine consigliato: inizia con i Ceph Manager, i Ceph Monitor e quindi continua sugli altri servizi, come Ceph OSD, Metadata Server e Object Gateway. Ogni daemon viene riavviato solo dopo che Ceph indica che il cluster resterà disponibile.
  </para>

  <note>
   <para>
    Nel processo di aggiornamento riportato di seguito è utilizzato il comando <command>ceph orch upgrade</command>. Tenere presente che le istruzioni seguenti illustrano come aggiornare il cluster Ceph con una versione del prodotto (ad esempio, un aggiornamento di manutenzione) e <emphasis>non</emphasis> spiegano come eseguire l'upgrade del cluster da una versione del prodotto all'altra.
   </para>
  </note>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-start">
   <title>Avvio dell'aggiornamento</title>
   <para>
    Prima di avviare l'aggiornamento, verificare che tutti i nodi siano online e che il cluster sia integro:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>cephadm shell -- ceph -s</screen>
   <para>
    Per eseguire l'aggiornamento a una release Ceph specifica:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image <replaceable>REGISTRY_URL</replaceable></screen>
   <para>
    Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade start --image registry.suse.com/ses/7/ceph/ceph:latest</screen>
   <para>
    Eseguire l'upgrade dei pacchetti sugli host:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph-salt update</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-monitor">
   <title>Monitoraggio dell'aggiornamento</title>
   <para>
    Eseguire il comando seguente per determinare se è in corso un aggiornamento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade status</screen>
   <para>
    Mentre l'aggiornamento è in corso, nell'output dello stato di Ceph verrà visualizzata una barra di avanzamento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -s
[...]
  progress:
    Upgrade to registry.suse.com/ses/7/ceph/ceph:latest (00h 20m 12s)
      [=======.....................] (time remaining: 01h 43m 31s)</screen>
   <para>
    È inoltre possibile visualizzare il log cephadm:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph -W cephadm</screen>
  </sect2>

  <sect2 xml:id="deploy-cephadm-day2-cephupdate-stop">
   <title>Annullamento dell'aggiornamento</title>
   <para>
    È possibile interrompere il processo di aggiornamento in qualsiasi momento:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph orch upgrade stop</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>Interruzione o riavvio del cluster</title>

  <para>
   In alcuni casi può essere necessario interrompere o riavviare l'intero cluster. Si consiglia di verificare attentamente le dipendenze dei servizi in esecuzione. Nei passaggi successivi è descritto come interrompere e avviare il cluster:
  </para>

  <procedure>
   <step>
    <para>
     Indicare al cluster Ceph di impostare il flag noout:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     Interrompere daemon e nodi nel seguente ordine:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Client di memorizzazione
      </para>
     </listitem>
     <listitem>
      <para>
       Gateway, ad esempio NFS Ganesha o Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Se necessario, eseguire task di manutenzione.
    </para>
   </step>
   <step>
    <para>
     Avviare nodi e server in ordine inverso rispetto al processo di spegnimento:
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Metadata Server
      </para>
     </listitem>
     <listitem>
      <para>
       Gateway, ad esempio NFS Ganesha o Object Gateway
      </para>
     </listitem>
     <listitem>
      <para>
       Client di memorizzazione
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     Rimuovere il flag noout:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph-cluster-purge">
  <title>Rimozione dell'intero cluster Ceph</title>

  <para>
   Il comando <command>ceph-salt purge</command> consente di rimuovere l'intero cluster Ceph. Se sono stati distribuiti altri cluster Ceph, viene eliminato definitivamente quello segnalato da <command>ceph -s</command>. In questo modo è possibile pulire l'ambiente del cluster durante il test di diverse configurazioni.
  </para>

  <para>
   Per evitare eliminazioni involontarie, lo strumento di coordinamento controlla se le misure di sicurezza sono disattivate. È possibile disattivare le misure di sicurezza e rimuovere il cluster Ceph eseguendo:
  </para>

<screen>
<prompt>root@master # </prompt>ceph-salt disengage-safety
<prompt>root@master # </prompt>ceph-salt purge
</screen>
 </sect1>
</chapter>
