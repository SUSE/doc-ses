<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_rbd.xml" version="5.0" xml:id="ceph-rbd">
 <title>Dispositivo di blocco RADOS (RADOS Block Device, RBD)</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sì</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Un blocco è una sequenza di byte, ad esempio un blocco di dati da 4 MB. Le interfacce di memorizzazione basate su blocchi rappresentano il modo più comune per memorizzare i dati con supporti rotanti, come dischi rigidi, CD, dischi floppy. L'ubiquità delle interfacce dei dispositivi di blocco rende un dispositivo di blocco virtuale un candidato ideale per interagire con un sistema di memorizzazione di massa come Ceph.
 </para>
 <para>
  I dispositivi di blocco Ceph consentono la condivisione di risorse fisiche e sono ridimensionabili. Questi dispositivi memorizzano i dati suddivisi in più OSD in un cluster Ceph. I dispositivi di blocco Ceph sfruttano le funzionalità RADOS come la creazione di snapshot, replica e coerenza. I RADOS Block Device (RBD, dispositivi di blocco RADOS) di Ceph interagiscono con gli OSD utilizzando i moduli del kernel o la libreria <systemitem>librbd</systemitem>.
 </para>
 <figure>
  <title>Protocollo RADOS</title>
  <mediaobject>
   <imageobject role="fo">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
   <imageobject role="html">
    <imagedata fileref="ceph_rbd_schema.png" width="70%" format="PNG"/>
   </imageobject>
  </mediaobject>
 </figure>
 <para>
  I dispositivi di blocco Ceph offrono prestazioni elevate con scalabilità infinita ai moduli del kernel. Supportano soluzioni di virtualizzazione come QEMU o sistemi di calcolo basati su cloud come OpenStack che fanno affidamento su <systemitem class="library">libvirt</systemitem>. È possibile utilizzare lo stesso cluster per attivare Object Gateway, CephFS e RADOS Block Device (dispositivi di blocco RADOS) simultaneamente.
 </para>
 <sect1 xml:id="ceph-rbd-commands">
  <title>Comandi dei dispositivi di blocco</title>

  <para>
   Il comando <command>rbd</command> consente di creare, elencare, analizzare e rimuovere immagini dei dispositivi di blocco. È inoltre possibile utilizzarlo per clonare immagini, creare snapshot, eseguire il rollback di un'immagine in uno snapshot o visualizzare uno snapshot.
  </para>

  <sect2 xml:id="ceph-rbd-cmds-create">
   <title>Creazione di un'immagine del dispositivo di blocco in un pool replicato</title>
   <para>
    Prima che sia possibile aggiungere un dispositivo di blocco a un client, è necessario creare un'immagine correlata in un pool esistente (vedere il <xref linkend="ceph-pools"/>):
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create --size <replaceable>MEGABYTES</replaceable> <replaceable>POOL-NAME</replaceable>/<replaceable>IMAGE-NAME</replaceable>
</screen>
   <para>
    Ad esempio, per creare un'immagine da 1 GB denominata "myimage" le cui informazioni vengono memorizzate in un pool denominato "mypool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd create --size 1024 mypool/myimage</screen>
   <tip>
    <title>unità di dimensioni immagine</title>
    <para>
     Se si omette la scorciatoia dell'unità di dimensioni ("G" o "T"), le dimensioni dell'immagine vengono indicate in megabyte. Utilizzare "G" o "T" dopo le dimensioni per specificare gigabyte o terabyte.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-create-ec">
   <title>Creazione di un'immagine del dispositivo di blocco in un pool con codice di cancellazione</title>
   <para>
    È possibile memorizzare i dati dell'immagine di un dispositivo di blocco direttamente nei pool con codice di cancellazione (EC, Erasure Coded). L'immagine di un dispositivo di blocco RADOS è costituita da due parti, <emphasis>dati</emphasis> e <emphasis>metadati</emphasis>. È possibile memorizzare solo la parte dati dell'immagine di un dispositivo di blocco RADOS (RADOS Block Device, RBD) in un pool EC. Il flag <option>overwrite</option> del pool deve essere impostato su <emphasis>true</emphasis> e ciò è possibile soltanto se tutti gli OSD in cui è memorizzato il pool utilizzano BlueStore.
   </para>
   <para>
    Non è possibile memorizzare la parte metadati dell'immagine in un pool EC. È necessario specificare il pool replicato per memorizzare i metadati dell'immagine con l'opzione <option>--pool=</option> del comando <command>rbd create</command> oppure aggiungendo <option>pool/</option> come prefisso del nome dell'immagine.
   </para>
   <para>
    Creare un pool EC:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create <replaceable>EC_POOL</replaceable> 12 12 erasure
<prompt>cephuser@adm &gt; </prompt>ceph osd pool set <replaceable>EC_POOL</replaceable> allow_ec_overwrites true</screen>
   <para>
    Specificare il pool replicato per la memorizzazione dei metadati:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>IMAGE_NAME</replaceable> --size=1G --data-pool <replaceable>EC_POOL</replaceable> --pool=<replaceable>POOL</replaceable>
</screen>
   <para>
    Oppure:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd create <replaceable>POOL/IMAGE_NAME</replaceable> --size=1G --data-pool EC_POOL
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-list">
   <title>Elenco delle immagini dei dispositivi di blocco</title>
   <para>
    Per visualizzare un elenco dei dispositivi di blocco in un pool denominato "mypool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd ls mypool</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-info">
   <title>Recupero delle informazioni sull'immagine</title>
   <para>
    Per recuperare le informazioni da un'immagine "myimage" in un pool denominato "mypool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd info mypool/myimage</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-resize">
   <title>Ridimensionamento di un'immagine del dispositivo di blocco</title>
   <para>
    Le immagini del RADOS Block Device (dispositivo di blocco RADOS) sono sottoposte a thin provisioning, non utilizzano effettivamente alcuno spazio di memorizzazione fisico fino a quando non si inizia a salvare i dati in esse. Dispongono tuttavia di una capacità massima che è possibile impostare con l'opzione <option>--size</option>. Se si desidera aumentare (o diminuire) le dimensioni massime di un'immagine, eseguire quanto riportato di seguito:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> # to increase
<prompt>cephuser@adm &gt; </prompt>rbd resize --size 2048 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --allow-shrink # to decrease
</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-cmds-rm">
   <title>Rimozione di un'immagine del dispositivo di blocco</title>
   <para>
    Per rimuovere un dispositivo di blocco che corrisponde a un'immagine "myimage" in un pool denominato "mypool", eseguire quanto riportato di seguito:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd rm mypool/myimage</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-bp-integration-mount-rbd">
  <title>Montaggio e smontaggio</title>

  <para>
   Dopo aver creato un dispositivo di blocco RADOS, è possibile utilizzarlo come qualsiasi altro dispositivo disco: formattarlo, montarlo in modo che sia in grado di scambiare file e smontarlo al termine dell'operazione.
  </para>

  <para>
   L'impostazione di default del comando <command>rbd</command> è configurata sull'accesso al cluster tramite l'account utente <literal>admin</literal> di Ceph, che dispone dell'accesso amministrativo completo al cluster. Con questa impostazione vi è il rischio che gli utenti possano causare involontariamente danni, come quando si esegue il login come <systemitem class="username">root</systemitem> a una workstation Linux. Pertanto, è preferibile creare account utente con meno privilegi e utilizzare tali account per il normale accesso in lettura/scrittura al dispositivo di blocco RADOS (RADOS Block Device, RBD).
  </para>

  <sect2 xml:id="ceph-rbd-creatuser">
   <title>Creazione di un account utente Ceph</title>
   <para>
    Per creare un nuovo account utente con le funzionalità Ceph Manager, Ceph Monitor e Ceph OSD, utilizzare il comando <command>ceph</command> con il sottocomando <command>auth get-or-create</command>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.<replaceable>ID</replaceable> mon 'profile rbd' osd 'profile <replaceable>profile name</replaceable> \
  [pool=<replaceable>pool-name</replaceable>] [, profile ...]' mgr 'profile rbd [pool=<replaceable>pool-name</replaceable>]'</screen>
   <para>
    Ad esempio, per creare un utente denominato <replaceable>qemu</replaceable> con accesso in lettura-scrittura al pool <replaceable>vms</replaceable> e accesso in sola lettura al pool <replaceable>images</replaceable>, eseguire quanto riportato di seguito:
   </para>
<screen>ceph auth get-or-create client.<replaceable>qemu</replaceable> mon 'profile rbd' osd 'profile rbd pool=<replaceable>vms</replaceable>, profile rbd-read-only pool=<replaceable>images</replaceable>' \
  mgr 'profile rbd pool=<replaceable>images</replaceable>'</screen>
   <para>
    L'output del comando <command>ceph auth get-or-create</command> sarà il portachiavi dell'utente specificato ed è possibile scriverlo in <filename>/etc/ceph/ceph.client.<replaceable>ID</replaceable>.keyring</filename>.
   </para>
   <note>
    <para>
     Quando si utilizza il comando <command>rbd</command>, è possibile specificare l'ID utente fornendo l'argomento facoltativo <command>--id</command>
     <replaceable>ID</replaceable>.
    </para>
   </note>
   <para>
    Per ulteriori dettagli sulla gestione degli account utente Ceph, fare riferimento al <xref linkend="cha-storage-cephx"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph-rbd-auth">
   <title>Autenticazione utente</title>
   <para>
    Per specificare un nome utente, utilizzare <option>--id <replaceable>user-name</replaceable></option>. Se si utilizza l'autenticazione <systemitem>cephx</systemitem>, è necessario specificare anche un segreto. Quest'ultimo potrebbe essere ricavato da un portachiavi o da un file contenente il segreto:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyring /path/to/keyring</screen>
   <para>
    oppure
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool rbd myimage --id admin --keyfile /path/to/file</screen>
  </sect2>

  <sect2 xml:id="ceph-rbd-prep">
   <title>Preparazione di un dispositivo di blocco RADOS (RADOS Block Device, RBD) per l'uso</title>
   <procedure>
    <step>
     <para>
      Accertarsi che nel cluster Ceph sia incluso un pool con l'immagine disco che si desidera mappare. Presupporre che il pool sia denominato <literal>mypool</literal> e l'immagine <literal>myimage</literal>.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd list mypool</screen>
    </step>
    <step>
     <para>
      Mappare l'immagine nel nuovo dispositivo di blocco:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map --pool mypool myimage</screen>
    </step>
    <step>
     <para>
      Elencare tutti i dispositivi mappati:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device list
id pool   image   snap device
0  mypool myimage -    /dev/rbd0</screen>
     <para>
      Il dispositivo che si desidera utilizzare è <filename>/dev/rbd0</filename>.
     </para>
     <tip>
      <title>percorso di dispositivo RBD</title>
      <para>
       Al posto di <filename>/dev/rbd<replaceable>DEVICE_NUMBER</replaceable></filename>, è possibile utilizzare <filename>/dev/rbd/<replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></filename> come percorso di dispositivo permanente. Esempio:
      </para>
<screen>
       /dev/rbd/mypool/myimage
      </screen>
     </tip>
    </step>
    <step>
     <para>
      Creare un file system XFS sul dispositivo <filename>/dev/rbd0:</filename>
     </para>
<screen><prompt role="root">root # </prompt>mkfs.xfs /dev/rbd0
      log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
      log stripe unit adjusted to 32KiB
      meta-data=/dev/rbd0              isize=256    agcount=9, agsize=261120 blks
      =                       sectsz=512   attr=2, projid32bit=1
      =                       crc=0        finobt=0
      data     =                       bsize=4096   blocks=2097152, imaxpct=25
      =                       sunit=1024   swidth=1024 blks
      naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
      log      =internal log           bsize=4096   blocks=2560, version=2
      =                       sectsz=512   sunit=8 blks, lazy-count=1
      realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
    </step>
    <step>
     <para>
      Sostituire <filename>/mnt</filename> con il proprio punto di montaggio, montare il dispositivo e verificare che sia montato correttamente:
     </para>
<screen><prompt role="root">root # </prompt>mount /dev/rbd0 /mnt
      <prompt role="root">root # </prompt>mount | grep rbd0
      /dev/rbd0 on /mnt type xfs (rw,relatime,attr2,inode64,sunit=8192,...</screen>
     <para>
      Adesso è possibile spostare i dati nel e dal dispositivo come se questo fosse una directory locale.
     </para>
     <tip>
      <title>aumento delle dimensioni del dispositivo RBD</title>
      <para>
       Se le dimensioni del dispositivo RBD non sono più sufficienti, è possibile aumentarle.
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Aumentare le dimensioni dell'immagine RBD, ad esempio fino a 10 GB.
        </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
         Resizing image: 100% complete...done.</screen>
       </listitem>
       <listitem>
        <para>
         Accrescere il file system in modo da riempire le nuove dimensioni del dispositivo:
        </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
[...]
data blocks changed from 2097152 to 2560000</screen>
       </listitem>
      </orderedlist>
     </tip>
    </step>
    <step>
     <para>
      Una volta terminato l'accesso al dispositivo, è possibile annullare la mappatura e smontarlo.
     </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd device unmap /dev/rbd0
<prompt role="root">root # </prompt>unmount /mnt
</screen>
    </step>
   </procedure>
   <tip>
    <title>montaggio e smontaggio manuali</title>
    <para>
     Per semplificare i processi di mappatura e montaggio degli RBD dopo l'avvio e di smontaggio dopo l'arresto, vengono forniti uno script <command>rbdmap</command> e un'unità <systemitem class="daemon">systemd</systemitem>. Vedere <xref linkend="ceph-rbd-rbdmap"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-rbd-rbdmap">
   <title><command>rbdmap</command>: mappatura dei dispositivi RBD all'avvio</title>
   <para>
    <command>rbdmap</command> è uno script della shell che consente di automatizzare le operazioni <command>rbd map</command> e <command>rbd device unmap</command> in una o più immagini RBD. Sebbene sia possibile eseguire manualmente lo script in qualsiasi momento, i principali vantaggi sono la mappatura automatica e il montaggio di immagini RBD all'avvio (e lo smontaggio e l'annullamento della mappatura all'arresto), attivati dal sistema Init. A tal fine è incluso un file di unità <systemitem class="daemon">systemd</systemitem>, <filename>rbdmap.service</filename> con il pacchetto <systemitem>ceph-common</systemitem>.
   </para>
   <para>
    Lo script impiega un singolo argomento, che può essere <option>map</option> o <option>unmap</option>. In entrambi i casi lo script analizza sintatticamente un file di configurazione. Il valore di default è <filename>/etc/ceph/rbdmap</filename>, ma è possibile ignorarlo tramite una variabile di ambiente <literal>RBDMAPFILE</literal>. Ciascuna riga del file di configurazione corrisponde a un'immagine RBD che deve anche essere mappata o non mappata.
   </para>
   <para>
    Il file di configurazione presenta il seguente formato:
   </para>
<screen>image_specification rbd_options</screen>
   <variablelist>
    <varlistentry>
     <term><option>image_specification</option></term>
     <listitem>
      <para>
       Percorso di un'immagine in un pool. Specificare come <replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>rbd_options</option></term>
     <listitem>
      <para>
       Elenco di parametri facoltativo da passare al comando <command>rbd device map</command> sottostante. Questi parametri e i rispettivi valori devono essere specificati come stringa separata da virgola, ad esempio:
      </para>
<screen>PARAM1=VAL1,PARAM2=VAL2,...</screen>
      <para>
       Nell'esempio con lo script <command>rbdmap</command> viene eseguito il seguente comando:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd device map <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> --PARAM1 VAL1 --PARAM2 VAL2</screen>
      <para>
       Nell'esempio seguente è possibile vedere come specificare un nome utente e un portachiavi con un segreto corrispondente:
      </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbdmap device map mypool/myimage id=<replaceable>rbd_user</replaceable>,keyring=/etc/ceph/ceph.client.rbd.keyring</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Quando viene eseguito come <command>rbdmap map</command>, lo script analizza sintatticamente il file di configurazione e, per ogni immagine RBD specificata, prima tenta di mappare l'immagine (con il comando <command>rbd device map</command>), quindi ne esegue il montaggio.
   </para>
   <para>
    Quando eseguito come <command>rbdmap unmap</command>, le immagini elencate nel file di configurazione il montaggio e la mappatura verranno annullati.
   </para>
   <para>
    <command>rbdmap unmap-all</command> tenta di smontare e successivamente di annullare la mappatura di tutte le immagini RBD attualmente mappate, indipendentemente dalla loro presenza nell'elenco del file di configurazione.
   </para>
   <para>
    Se ha esito positivo, l'operazione <command>rbd device map</command> mappa l'immagine a un dispositivo <filename>/dev/rbdX</filename>, quindi viene attivata una regola udev per creare un collegamento simbolico del nome del dispositivo <filename>/dev/rbd/<replaceable>pool_name</replaceable>/<replaceable>image_name</replaceable></filename> che punta al dispositivo realmente mappato.
   </para>
   <para>
    Affinché il montaggio e lo smontaggio abbiano esito positivo, il nome del dispositivo "intuitivo" deve avere una voce corrispondente in <filename>/etc/fstab</filename>. Quando si scrivono voci <filename>/etc/fstab</filename> per le immagini RBD, specificare l'opzione di montaggio "noauto" (o "nofail"). In tal modo si impedisce al sistema Init di tentare di montare il dispositivo troppo presto, perfino prima dell'esistenza del dispositivo in questione, poiché di norma <filename>rbdmap.service</filename> viene attivato piuttosto tardi nella sequenza di avvio.
   </para>
   <para>
    Per un elenco completo di opzioni <command>rbd</command>, vedere la documentazione relativa a <command>rbd</command> (<command>man 8 rbd</command>).
   </para>
   <para>
    Per alcuni esempi di utilizzo di <command>rbdmap</command>, vedere la documentazione relativa a <command>rbdmap</command> (<command>man 8 rbdmap</command>).
   </para>
  </sect2>

  <sect2 xml:id="increasing-size-rbd-device">
   <title>Aumento delle dimensioni dei dispositivi RBD</title>
   <para>
    Se le dimensioni del dispositivo RBD non sono più sufficienti, è possibile aumentarle.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Aumentare le dimensioni dell'immagine RBD, ad esempio fino a 10 GB.
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd resize --size 10000 mypool/myimage
 Resizing image: 100% complete...done.</screen>
    </listitem>
    <listitem>
     <para>
      Accrescere il file system in modo da riempire le nuove dimensioni del dispositivo.
     </para>
<screen><prompt role="root">root # </prompt>xfs_growfs /mnt
 [...]
 data blocks changed from 2097152 to 2560000</screen>
    </listitem>
   </orderedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-rbd">
  <title>Snapshot</title>

  <para>
   Uno snapshot RBD è uno snapshot di un'immagine del RADOS Block Device (dispositivo di blocco RADOS). Gli snapshot consentono di conservare la cronologia dello stato dell'immagine. Ceph supporta anche il layering di snapshot, che consente di clonare rapidamente e facilmente immagini VM. Ceph supporta gli snapshot dei dispositivi di blocco mediante l'uso del comando <command>rbd</command> e molte interfacce di livello superiore, tra cui QEMU, <systemitem>libvirt</systemitem>, OpenStack e CloudStack.
  </para>

  <note>
   <para>
    Interrompere le operazioni di input e output e svuotare tutte le operazioni di scrittura in sospeso prima di creare una snapshot di un'immagine. Se l'immagine contiene un file system, questo deve presentare uno stato coerente al momento della creazione della snapshot.
   </para>
  </note>

  <sect2 xml:id="rbd-enable-configure-cephx">
   <title>Abilitazione e configurazione di <systemitem>cephx</systemitem></title>
   <para>
    Quando <systemitem>cephx</systemitem> è abilitato, è necessario specificare un nome o un ID utente e un percorso del portachiavi contenente la chiave corrispondente dell'utente. Per ulteriori informazioni, vedere il <xref linkend="cha-storage-cephx"/>. È inoltre possibile aggiungere la variabile di ambiente <systemitem>CEPH_ARGS</systemitem> per evitare di immettere di nuovo i seguenti parametri.
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id <replaceable>user-ID</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name <replaceable>username</replaceable> --keyring=/path/to/secret <replaceable>commands</replaceable></screen>
   <para>
    Esempio:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --id admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --name client.admin --keyring=/etc/ceph/ceph.keyring <replaceable>commands</replaceable></screen>
   <tip>
    <para>
     Aggiungere l'utente e il segreto nella variabile di ambiente <systemitem>CEPH_ARGS</systemitem> in modo che non sia necessario immetterli ogni volta.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="rbd-snapshot-basics">
   <title>Nozioni di base sugli snapshot</title>
   <para>
    Nelle procedure seguenti è dimostrato come creare, elencare e rimuovere snapshot mediante l'uso del comando <command>rbd</command> sulla riga di comando.
   </para>
   <sect3 xml:id="rbd-creating-snapshots">
    <title>Creazione di snapshot</title>
    <para>
     Per creare uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap create</option>, il nome pool e il nome immagine.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap create --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap create <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap create --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap create rbd/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-listing-snapshots">
    <title>Elenco di snapshot</title>
    <para>
     Per elencare gli snapshot di un'immagine, specificare il nome pool e il nome immagine.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap ls <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap ls <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool rbd snap ls image1
<prompt>cephuser@adm &gt; </prompt>rbd snap ls rbd/image1</screen>
   </sect3>
   <sect3 xml:id="rbd-rollback-snapshots">
    <title>Rollback di snapshot</title>
    <para>
     Per eseguire il rollback a uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap rollback</option>, il nome pool, il nome immagine e il nome snapshot.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rollback --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rollback --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rollback pool1/image1@snapshot1</screen>
    <note>
     <para>
      Eseguire il rollback di un'immagine a uno snapshot significa sovrascrivere la versione attuale dell'immagine con i dati provenienti da uno snapshot. La durata di esecuzione di un rollback aumenta proporzionalmente alle dimensioni dell'immagine. È <emphasis>più rapido eseguire la clonazione</emphasis> da uno snapshot <emphasis>piuttosto che eseguire il rollback</emphasis> di un'immagine a uno snapshot; questo è inoltre il metodo preferito per tornare a uno stato preesistente.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-deleting-snapshots">
    <title>Eliminazione di uno snapshot</title>
    <para>
     Per eliminare uno snapshot con <command>rbd</command>, specificare l'opzione <option>snap rm</option>, il nome pool, il nome immagine e il nome utente.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap rm --snap <replaceable>snap-name</replaceable> <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap rm <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snap-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap rm --snap snapshot1 image1
<prompt>cephuser@adm &gt; </prompt>rbd snap rm pool1/image1@snapshot1</screen>
    <note>
     <para>
      Nei Ceph OSD i dati vengono eliminati in modo asincrono, quindi con l'eliminazione di uno snapshot non si libera immediatamente spazio su disco.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-purging-snapshots">
    <title>Eliminazione definitiva di snapshot</title>
    <para>
     Per eliminare tutti gli snapshot di un'immagine con <command>rbd</command>, specificare l'opzione <option>snap purge</option> e il nome immagine.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap purge <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap purge <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap purge image1
<prompt>cephuser@adm &gt; </prompt>rbd snap purge pool1/image1</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-snapshoti-layering">
   <title>Layering degli snapshot</title>
   <para>
    Ceph supporta la creazione di più cloni copia su scrittura (copy-on-write, COW) di uno snapshot del dispositivo di blocco. Il layering degli snapshot consente ai client dei dispositivi di blocco Ceph di creare immagini molto rapidamente. Ad esempio, si può creare un'immagine del dispositivo di blocco con una Linux VM scritta al suo interno, quindi eseguire lo snapshot dell'immagine, proteggere lo snapshot e creare tutti i cloni copia su scrittura desiderati. Poiché gli snapshot sono di sola lettura, la clonazione di uno di essi semplifica la semantica e consente quindi di creare i cloni rapidamente.
   </para>
   <note>
    <para>
     I termini "parent" e "child" menzionati negli esempi di riga di comando riportati sotto significano uno snapshot del dispositivo di blocco Ceph (parent) e l'immagine corrispondente clonata dallo snapshot (child).
    </para>
   </note>
   <para>
    In ciascuna immagine clonata (child) è memorizzato il rifermento alla rispettiva immagine superiore, che consente all'immagine clonata di aprire e leggere lo snapshot superiore.
   </para>
   <para>
    Un clone COW di uno snapshot si comporta esattamente come qualsiasi altra immagine del dispositivo di blocco Ceph. Nelle immagini clonate è possibile eseguire operazioni di lettura e scrittura ed è possibile clonarle e ridimensionarle. Con le immagini clonate non esistono restrizioni speciali. Il clone copia su scrittura di uno snapshot si riferisce tuttavia allo snapshot, quindi è <emphasis>necessario</emphasis> proteggere quest'ultimo prima di clonarlo.
   </para>
   <note>
    <title>opzione <option>--image-format 1</option> non supportata</title>
    <para>
     Non è possibile creare snapshot di immagini create con l'opzione <command>rbd create --image-format 1</command> obsoleta. Ceph supporta soltanto la clonazione delle immagini <emphasis>format 2</emphasis> di default.
    </para>
   </note>
   <sect3 xml:id="rbd-start-layering">
    <title>Introduzione al layering</title>
    <para>
     Il layering dei dispositivi di blocco Ceph è un processo semplice. È necessario disporre di un'immagine, creare uno snapshot dell'immagine, proteggere lo snapshot. Dopo aver eseguito questi passaggi, è possibile iniziare la clonazione dello snapshot.
    </para>
    <para>
     L'immagine clonata fa riferimento allo snapshot superiore e include l'ID pool, l'ID immagine e l'ID snapshot. L'inclusione dell'ID pool significa che è possibile clonare snapshot da un pool nelle immagini in un altro pool.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       <emphasis>Modello di immagine</emphasis>: un caso comune di layering dei dispositivi di blocco consiste nel creare un'immagine master e uno snapshot che funge da modello per i cloni. Ad esempio, un utente può creare un'immagine per una distribuzione Linux (ad esempio, SUSE Linux Enterprise Server) e creare uno snapshot corrispondente. Periodicamente, l'utente può aggiornare l'immagine e creare un nuovo snapshot (ad esempio, <command>zypper ref &amp;&amp; zypper patch</command> seguito da <command>rbd snap create</command>). Ma mano che l'immagine matura, l'utente può clonare qualsiasi snapshot.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Modello esteso</emphasis>: un caso più avanzato consiste nell'estensione di un'immagine modello che fornisce ulteriori informazioni rispetto all'immagine di base. Ad esempio, un utente può clonare un'immagine (un modello VM) e installare un software diverso (ad esempio un database, un sistema di gestione di contenuti o un sistema di analisi) ed eseguire quindi lo snapshot dell'immagine estesa, che a sua volta è possibile aggiornare allo stesso modo dell'immagine di base.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Pool di modelli</emphasis>: un metodo per utilizzare il layering dei dispositivi di blocco consiste nel creare un pool contenente immagini master che fungono da modelli e snapshot di tali modelli. È quindi possibile estendere i privilegi di sola lettura agli utenti in modo che possano clonare gli snapshot senza doverli scrivere o eseguire nel pool.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>Migrazione/recupero dell'immagine</emphasis>: un metodo per utilizzare il layering dei dispositivi di blocco consiste nell'eseguire la migrazione o il recupero dei dati da un pool in un altro.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3 xml:id="rbd-protecting-snapshot">
    <title>Protezione di uno snapshot</title>
    <para>
     I cloni accedono agli snapshot superiori. Tutti i cloni verrebbero interrotti se un utente eliminasse inavvertitamente lo snapshot superiore. Per impedire la perdita di dati, è necessario proteggere lo snapshot prima di poterlo clonare.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap protect \
 --image <replaceable>image-name</replaceable> --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap protect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap protect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap protect pool1/image1@snapshot1</screen>
    <note>
     <para>
      Non è possibile eliminare uno snapshot protetto.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-cloning-snapshots">
    <title>Clonazione di uno snapshot</title>
    <para>
     Per clonare uno snapshot, è necessario specificare il pool superiore, l'immagine e lo snapshot, il pool secondario e il nome immagine. È necessario proteggere lo snapshot prima di poterlo clonare.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone --pool <replaceable>pool-name</replaceable> --image <replaceable>parent-image</replaceable> \
 --snap <replaceable>snap-name</replaceable> --dest-pool <replaceable>pool-name</replaceable> \
 --dest <replaceable>child-image</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd clone <replaceable>pool-name</replaceable>/<replaceable>parent-image</replaceable>@<replaceable>snap-name</replaceable> \
<replaceable>pool-name</replaceable>/<replaceable>child-image-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd clone pool1/image1@snapshot1 pool1/image2</screen>
    <note>
     <para>
      Si può clonare uno snapshot da un pool in un'immagine in un altro pool. Ad esempio, si possono mantenere immagini e snapshot di sola lettura come modelli in un pool e cloni scrivibili in un altro pool.
     </para>
    </note>
   </sect3>
   <sect3 xml:id="rbd-unprotecting-snapshots">
    <title>Annullamento della protezione di uno snapshot</title>
    <para>
     Prima di poter eliminare uno snapshot, è necessario annullarne la protezione. Inoltre, <emphasis>non</emphasis> è possibile eliminare snapshot con riferimenti dai cloni. È necessario appiattire ciascun clone di uno snapshot prima di poter eliminare quest'ultimo.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> snap unprotect --image <replaceable>image-name</replaceable> \
 --snap <replaceable>snapshot-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 snap unprotect --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd snap unprotect pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-list-children-snapshots">
    <title>Elenco degli elementi secondari di uno snapshot</title>
    <para>
     Per elencare gli elementi secondari di uno snapshot, eseguire quanto riportato di seguito:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> children --image <replaceable>image-name</replaceable> --snap <replaceable>snap-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd children <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable>@<replaceable>snapshot-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 children --image image1 --snap snapshot1
<prompt>cephuser@adm &gt; </prompt>rbd children pool1/image1@snapshot1</screen>
   </sect3>
   <sect3 xml:id="rbd-flatten-cloned-image">
    <title>Appiattimento di un'immagine clonata</title>
    <para>
     Le immagini clonate mantengono un riferimento allo snapshot superiore. Quando si rimuove il riferimento dal clone secondario nello parent superiore, di fatto si "appiattisce" l'immagine copiando le informazioni dallo snapshot al clone. La durata di appiattimento di un clone aumenta proporzionalmente alle dimensioni dello snapshot. Per eliminare uno snapshot, prima è necessario appiattire le immagini secondarie.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool <replaceable>pool-name</replaceable> flatten --image <replaceable>image-name</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd flatten <replaceable>pool-name</replaceable>/<replaceable>image-name</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --pool pool1 flatten --image image1
<prompt>cephuser@adm &gt; </prompt>rbd flatten pool1/image1</screen>
    <note>
     <para>
      Poiché un'immagine appiattita contiene tutte le informazioni provenienti dallo snapshot, questa occuperà uno spazio di memorizzazione maggiore rispetto a un clone su più strati.
     </para>
    </note>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-rbd-mirror">
  <title>Copie speculari delle immagini RBD</title>

  <para>
   È possibile eseguire la copia speculare delle immagini RBD in modo asincrono tra due cluster Ceph. Questa funzionalità è disponibile in due modalità:
  </para>

  <variablelist>
   <varlistentry>
    <term>Basata sul journal</term>
    <listitem>
     <para>
      Questa modalità utilizza la funzione di journaling dell'immagine RBD per assicurare la replica temporizzata e con coerenza per arresto anomalo tra cluster. Prima di modificare l'immagine effettiva, ogni operazione di scrittura sull'immagine RBD viene innanzitutto registrata sul journal associato. Il cluster <literal>remote</literal> leggerà dal journal e riprodurrà gli aggiornamenti nella relativa copia locale dell'immagine. Dal momento che ogni operazione di scrittura sull'immagine RBD risulta in due operazioni di scrittura sul cluster Ceph, prevedere un raddoppiamento delle latenze di scrittura quando si utilizza la funzione di journaling dell'immagine RBD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Basata su snapshot</term>
    <listitem>
     <para>
      Questa modalità utilizza snapshot di copia speculare dell'immagine RBD pianificati periodicamente o creati manualmente per eseguire la replica delle immagini RBD con coerenza per arresto anomalo tra cluster. Il cluster <literal>remote</literal> determinerà la presenza di eventuali aggiornamenti dei dati o dei metadati tra due snapshot di copia speculare e copierà i valori differenziali nella relativa copia locale dell'immagine. Con la funzione fast-diff dell'immagine RBD, è possibile calcolare rapidamente i blocchi di dati aggiornati senza dover effettuare la scansione dell'intera immagine RBD. Dal momento che questa modalità non garantisce coerenza temporizzata, sarà necessario sincronizzare il valore differenziale completo dello snapshot prima dell'uso in uno scenario di failover. Gli eventuali valori differenziali dello snapshot applicati parzialmente verranno sottoposti a rollback sull'ultimo snapshot di cui è stata completata la sincronizzazione.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   La copia speculare è configurata per ogni singolo pool nei cluster peer e può essere configurata su un sottoinsieme specifico di immagini all'interno del pool o in modo che venga eseguita automaticamente la copia speculare di tutte le immagini di un pool quando è in uso soltanto la copia speculare basata sul journal. Per la configurazione della copia speculare si utilizza il comando <command>rbd</command>. Il daemon <systemitem class="daemon">rbd-mirror</systemitem> è responsabile del pull degli aggiornamenti delle immagini dal cluster peer <literal>remote</literal> e della loro applicazione all'immagine nel cluster <literal>local</literal>.
  </para>

  <para>
   A seconda delle necessità di replica, è possibile configurare la copia speculare RBD sulla replica a una o a due vie:
  </para>

  <variablelist>
   <varlistentry>
    <term>Replica a una via</term>
    <listitem>
     <para>
      Quando la copia speculare dei dati avviene soltanto da un cluster primario a uno secondario, il daemon <systemitem class="daemon">rbd-mirror</systemitem> vene eseguito solo sul cluster secondario.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Replica a due vie</term>
    <listitem>
     <para>
      Quando la copia speculare dei dati avviene dalle immagini primarie su un cluster alle immagini non primarie su un altro cluster (e viceversa), il daemon <systemitem class="daemon">rbd-mirror</systemitem> viene eseguito su entrambi i cluster.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <important>
   <para>
    Ogni istanza del daemon <systemitem class="daemon">rbd-mirror</systemitem> deve essere in grado di connettersi contemporaneamente a entrambi i cluster <literal>local</literal> e <literal>remote</literal> Ceph. Ad esempio, a tutti i monitor e agli host OSD. Inoltre, la larghezza di banda della rete tra i due data center deve essere sufficiente per gestire il workload della copia speculare.
   </para>
  </important>

  <sect2 xml:id="ceph-rbd-mirror-poolconfig">
   <title>Configurazione del pool</title>
   <para>
    Nelle procedure seguenti è illustrato come eseguire task amministrativi di base per configurare la copia speculare tramite il comando <command>rbd</command>. La copia speculare è configurata per ogni singolo pool nei cluster Ceph.
   </para>
   <para>
    È necessario eseguire i passaggi della configurazione del pool su entrambi i cluster peer. Per maggior chiarezza, in queste procedure si presuppone che due cluster, denominati <literal>local</literal> e <literal>remote</literal>, siano accessibili da un singolo host.
   </para>
   <para>
    Vedere la documentazione relativa a <command>rbd</command> (<command>man 8 rbd</command>) per ulteriori dettagli su come connettersi a cluster Ceph diversi.
   </para>
   <tip>
    <title>cluster multipli</title>
    <para>
     Il nome del cluster negli esempi seguenti corrisponde a un file di configurazione Ceph omonimo <filename>/etc/ceph/remote.conf</filename> e al file del portachiavi Ceph omonimo <filename>/etc/ceph/remote.client.admin.keyring</filename>.
    </para>
   </tip>
   <sect3 xml:id="rbd-enable-mirroring-pool">
    <title>Abilitazione della copia speculare su un pool</title>
    <para>
     Per abilitare la copia speculare su un pool, specificare il sottocomando <command>mirror pool enable</command>, il nome pool e la modalità di esecuzione di copia speculare. La modalità di esecuzione di copia speculare può essere "pool" oppure "image":
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        Tutte le immagini nel pool in cui è abilitata la funzione di journaling vengono sottoposte a copia speculare.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>immagine</term>
      <listitem>
       <para>
        La copia speculare deve essere abilitata esplicitamente su ciascuna immagine. Consultare <xref linkend="rbd-mirror-enable-image-mirroring"/> per maggiori informazioni.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool enable <replaceable>POOL_NAME</replaceable> pool
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool enable <replaceable>POOL_NAME</replaceable> pool</screen>
   </sect3>
   <sect3 xml:id="rbd-disable-mirroring-pool">
    <title>Disabilitazione della copia speculare</title>
    <para>
     Per disabilitare la copia speculare su un pool, specificare il sottocomando <command>mirror pool disable</command> e il nome pool. Quando si disabilita la copia speculare su un pool in questo modo, questa verrà disabilitata anche su qualsiasi immagine (nel pool) per la quale è stata abilitata esplicitamente la copia speculare.
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool disable <replaceable>POOL_NAME</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool disable <replaceable>POOL_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-bootstrap-peer">
    <title>Esecuzione del bootstrap dei peer</title>
    <para>
     Affinché il daemon <systemitem class="daemon">rbd-mirror</systemitem> rilevi il rispettivo cluster peer, è necessario registrare il peer nel pool e creare un account utente. Questo processo può essere automatizzato con <command>rbd</command> e i comandi <command>mirror pool peer bootstrap create</command> e <command>mirror pool peer bootstrap import</command>.
    </para>
    <para>
     Per creare manualmente un nuovo token di bootstrap con <command>rbd</command>, specificare il comando <command>mirror pool peer bootstrap create</command>, il nome del pool e un nome descrittivo facoltativo del sito per la descrizione del cluster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd mirror pool peer bootstrap create \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] <replaceable>POOL_NAME</replaceable></screen>
    <para>
     L'output di <command>mirror pool peer bootstrap create</command> sarà un token che dovrà essere fornito al comando <command>mirror pool peer bootstrap import</command>. Ad esempio, sul cluster <literal>local</literal>:
    </para>
<screen><prompt>cephuser@local &gt; </prompt>rbd --cluster local mirror pool peer bootstrap create --site-name local image-pool
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==</screen>
    <para>
     Per importare manualmente il token di bootstrap creato da un altro cluster con il comando <command>rbd</command>, utilizzare la sintassi seguente:
    </para>
<screen>
rbd mirror pool peer bootstrap import \
 [--site-name <replaceable>LOCAL_SITE_NAME</replaceable>] \
 [--direction <replaceable>DIRECTION</replaceable> \
 <replaceable>POOL_NAME</replaceable> <replaceable>TOKEN_PATH</replaceable>
</screen>
    <para>
     Dove:
    </para>
    <variablelist>
     <varlistentry>
      <term><replaceable>LOCAL_SITE_NAME</replaceable></term>
      <listitem>
       <para>
        Nome descrittivo facoltativo del sito per la descrizione del cluster <literal>local</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>DIRECTION</replaceable></term>
      <listitem>
       <para>
        Direzione della copia speculare. L'impostazione di default è <literal>rx-tx</literal> per la copia speculare bidirezionale, ma è possibile configurare questa impostazione anche su <literal>rx-only</literal> per la copia speculare unidirezionale.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>POOL_NAME</replaceable></term>
      <listitem>
       <para>
        Nome del pool.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term><replaceable>TOKEN_PATH</replaceable></term>
      <listitem>
       <para>
        Percorso del file al token creato (o <literal>-</literal> per leggerlo dall'input standard).
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Ad esempio, sul cluster <literal>remote</literal>:
    </para>
<screen><prompt>cephuser@remote &gt; </prompt>cat &lt;&lt;EOF &gt; token
eyJmc2lkIjoiOWY1MjgyZGItYjg5OS00NTk2LTgwOTgtMzIwYzFmYzM5NmYzIiwiY2xpZW50X2lkIjoicmJkLW \
1pcnJvci1wZWVyIiwia2V5IjoiQVFBUnczOWQwdkhvQmhBQVlMM1I4RmR5dHNJQU50bkFTZ0lOTVE9PSIsIm1v \
bl9ob3N0IjoiW3YyOjE5Mi4xNjguMS4zOjY4MjAsdjE6MTkyLjE2OC4xLjM6NjgyMV0ifQ==
EOF</screen>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer bootstrap import \
 --site-name remote image-pool token</screen>
   </sect3>
   <sect3 xml:id="ceph-rbd-mirror-add-peer">
    <title>Aggiunta manuale di un peer del cluster</title>
    <para>
     Come alternativa all'esecuzione del bootstrap dei peer descritta nella <xref linkend="ceph-rbd-mirror-bootstrap-peer"/>, è possibile specificare i peer manualmente. Per eseguire la copia speculare, il daemon <systemitem class="daemon">rbd-mirror</systemitem> remoto necessita dell'accesso al cluster locale. Creare un nuovo utente Ceph locale che verrà utilizzato dal daemon <systemitem class="daemon">rbd-mirror</systemitem> remoto, ad esempio <literal>rbd-mirror-peer</literal>:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.rbd-mirror-peer \
 mon 'profile rbd' osd 'profile rbd'
</screen>
    <para>
     Utilizzare la sintassi seguente per aggiungere un cluster Ceph peer in copia speculare con il comando <command>rbd</command>:
    </para>
<screen>rbd mirror pool peer add <replaceable>POOL_NAME</replaceable> <replaceable>CLIENT_NAME</replaceable>@<replaceable>CLUSTER_NAME</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool client.rbd-mirror-peer@site-b
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-b mirror pool peer add image-pool client.rbd-mirror-peer@site-a
</screen>
    <para>
     Per default, il daemon <systemitem class="daemon">rbd-mirror</systemitem> deve disporre dell'accesso al file di configurazione Ceph ubicato in <filename>/etc/ceph/.<replaceable>CLUSTER_NAME</replaceable>.conf</filename>, in cui sono specificati gli indirizzi IP dei MON del cluster peer e il portachiavi relativo a un client denominato <replaceable>CLIENT_NAME</replaceable> e ubicato nel percorso di ricerca del portachiavi di default o predefinito, ad esempio <filename>/etc/ceph/<replaceable>CLUSTER_NAME</replaceable>.<replaceable>CLIENT_NAME</replaceable>.keyring</filename>.
    </para>
    <para>
     In alternativa, è possibile memorizzare in modo sicuro la chiave client e/o il MON del cluster peer nell'archivio config-key Ceph. Per specificare gli attributi di connessione del cluster peer durante l'aggiunta di un peer in copia speculare, utilizzare le opzioni <option>--remote-mon-host</option> e <option>--remote-key-file</option>. Esempio:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool peer add image-pool \
 client.rbd-mirror-peer@site-b --remote-mon-host 192.168.1.1,192.168.1.2 \
 --remote-key-file <replaceable>/PATH/TO/KEY_FILE</replaceable>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster site-a mirror pool info image-pool --all
Mode: pool
Peers:
  UUID        NAME   CLIENT                 MON_HOST                KEY
  587b08db... site-b client.rbd-mirror-peer 192.168.1.1,192.168.1.2 AQAeuZdb...
</screen>
   </sect3>
   <sect3 xml:id="rbd-remove-cluster-peer">
    <title>Rimozione di un peer del cluster</title>
    <para>
     Per rimuovere un cluster peer in copia speculare, specificare il sottocomando <command>mirror pool peer remove</command>, il nome pool e l'UUID peer (reso disponibile dal comando <command>rbd mirror pool info</command>):
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 55672766-c02b-4729-8567-f13a66893445
<prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror pool peer remove <replaceable>POOL_NAME</replaceable> \
 60c0e299-b38f-4234-91f6-eed0a367be08
</screen>
   </sect3>
   <sect3 xml:id="rbd-data-pools">
    <title>Pool di dati</title>
    <para>
     Durante la creazione di immagini nel cluster di destinazione, <systemitem class="daemon">rbd-mirror</systemitem> seleziona un pool di dati come segue:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Se presente, verrà utilizzato il pool di dati di default configurato (con l'opzione di configurazione <option>rbd_default_data_pool</option>) del cluster di destinazione.
      </para>
     </listitem>
     <listitem>
      <para>
       In caso contrario, se l'immagine di origine utilizza un pool di dati separato e sul cluster di destinazione esiste già un pool con lo stesso nome, verrà utilizzato tale pool.
      </para>
     </listitem>
     <listitem>
      <para>
       Se nessuna delle due condizioni descritte sopra è applicabile, non verrà impostato alcun pool di dati.
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-imageconfig">
   <title>Configurazione dell'immagine RBD</title>
   <para>
    Diversamente dalla configurazione del pool, la configurazione dell'immagine deve essere eseguita solo a fronte di un singolo cluster Ceph peer in copia speculare.
   </para>
   <para>
    Le immagini RBD sottoposte a copia speculare sono designate come <emphasis>primarie</emphasis> o <emphasis>non primarie</emphasis>. Questa è una proprietà dell'immagine e non del pool. Non è possibile modificare le immagini designate come non primarie.
   </para>
   <para>
    Le immagini vengono promosse automaticamente a primarie quando la copia speculare viene prima abilitata su un'immagine (implicitamente, se la modalità di copia speculare del pool è "pool" e la funzione di journaling dell'immagine è abilitata, oppure esplicitamente (vedere <xref linkend="rbd-mirror-enable-image-mirroring"/>) mediante il comando <command>rbd</command>).
   </para>
   <sect3 xml:id="rbd-mirror-enable-image-mirroring">
    <title>Abilitazione della copia speculare dell'immagine</title>
    <para>
     Se la copia speculare è configurata in modalità <literal>image</literal>, è necessario abilitarla esplicitamente per ciascuna immagine nel pool. Per abilitare la copia speculare per un'immagine specifica con <command>rbd</command>, specificare il sottocomando <command>mirror image enable</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable \
 <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     È possibile eseguire la copia speculare dell'immagine in modalità <literal>journal</literal> o <literal>snapshot</literal>:
    </para>
    <variablelist>
     <varlistentry>
      <term>journal (default)</term>
      <listitem>
       <para>
        Se configurata nella modalità <literal>journal</literal>, la copia speculare utilizzerà la funzione di journaling dell'immagine RBD per replicarne il contenuto. Tale funzione verrà abilitata automaticamente se non è stata ancora abilitata sull'immagine.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>istantanea</term>
      <listitem>
       <para>
        Se configurata nella modalità <literal>snapshot</literal>, la copia speculare utilizzerà gli snapshot di copia speculare dell'immagine RBD per replicarne il contenuto. In seguito all'abilitazione, verrà automaticamente creato un primo snapshot di copia speculare dell'immagine RBD e sarà possibile crearne altri con il comando <command>rbd</command>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-1 snapshot
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image enable image-pool/image-2 journal</screen>
   </sect3>
   <sect3 xml:id="rbd-enable-image-jouranling">
    <title>Abilitazione della funzione di journaling dell'immagine</title>
    <para>
     Nella copia speculare RBD viene utilizzata la funzione di journaling RBD per assicurare che l'immagine replicata mantenga sempre con coerenza per arresto anomalo. Se si utilizza la modalità di copia speculare <literal>image</literal>, la funzione di journaling sarà abilitata automaticamente se sull'immagine è abilitata la copia speculare. Se si utilizza la modalità di copia speculare <literal>pool</literal>, prima che sia possibile sottoporre a copia speculare un'immagine su un cluster peer, occorre abilitare la funzione di journaling dell'immagine RBD. È possibile abilitare tale funzione al momento della creazione dell'immagine specificando l'opzione <option>--image-feature exclusive-lock,journaling</option> nel comando <command>rbd</command>.
    </para>
    <para>
     In alternativa, è possibile abilitare dinamicamente la funzione di journaling sulle immagini RBD preesistenti. Per abilitare il journaling, specificare il sottocomando <command>feature enable</command>, il nome del pool e dell'immagine e il nome della funzione:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> exclusive-lock
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local feature enable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable> journaling</screen>
    <note>
     <title>dipendenza dell'opzione</title>
     <para>
      La funzione <option>journaling</option> dipende dalla funzione <option>exclusive-lock</option>. Se la funzione <option>exclusive-lock</option> non è già stata abilitata, è necessario farlo prima di abilitare la funzione <option>journaling</option>.
     </para>
    </note>
    <tip>
     <para>
      È possibile abilitare per default il journaling su tutte le nuove immagini aggiungendo <option>rbd default features = layering,exclusive-lock,object-map,deep-flatten,journaling</option> al file di configurazione Ceph.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-create-image-mirror-snapshots">
    <title>Creazione di snapshot di copia speculare dell'immagine</title>
    <para>
     Se si utilizza la copia speculare basata su snapshot, sarà necessario creare snapshot di copia speculare ogni volta che si desidera eseguire la copia speculare dei contenuti modificati dell'immagine RBD. Per creare manualmente uno snapshot di copia speculare con <command>rbd</command>, specificare il comando <command>mirror image snapshot</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image snapshot <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image snapshot image-pool/image-1</screen>
    <para>
     Per default, vengono creati solo tre snapshot di copia speculare per immagine. Se questo limite viene raggiunto, lo snapshot di copia speculare più recente viene eliminato automaticamente. Se necessario, è possibile ignorare questo limite con l'opzione di configurazione <option>rbd_mirroring_max_mirroring_snapshots</option>. Inoltre, gli snapshot di copia speculare vengono eliminati automaticamente quando l'immagine viene rimossa o quando la copia speculare viene disabilitata.
    </para>
    <para>
     È inoltre possibile definire una pianificazione per la creazione automatica e periodica degli snapshot di copia speculare. Questi ultimi possono essere pianificati a livello globale, di singolo pool o di singola immagine. È possibile definire più pianificazioni di snapshot di copia speculare su qualsiasi livello, ma verranno eseguite soltanto le pianificazioni più specifiche corrispondenti a una singola immagine in copia speculare.
    </para>
    <para>
     Per creare una pianificazione di snapshot di copia speculare con <command>rbd</command>, specificare il comando <command>mirror snapshot schedule add</command> insieme a un nome facoltativo del pool o dell'immagine, a un intervallo e a un'ora di inizio facoltativa.
    </para>
    <para>
     L'intervallo può essere espresso in giorni, ore o minuti utilizzando rispettivamente i suffissi <option>d</option>, <option>h</option> o <option>m</option>. L'ora di inizio facoltativa può essere specificata utilizzando il formato di ora ISO 8601. Esempio:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool 24h 14:00:00-05:00
<prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror snapshot schedule add --pool image-pool --image image1 6h
</screen>
    <para>
     Per rimuovere una pianificazione di snapshot di copia speculare con <command>rbd</command>, specificare il comando <command>mirror snapshot schedule remove</command> con le opzioni corrispondenti al comando per l'aggiunta della pianificazione equivalente.
    </para>
    <para>
     Per elencare tutte le pianificazioni di snapshot per un livello specifico (globale, pool o immagine) con <command>rbd</command>, immettere il comando <command>mirror snapshot schedule ls</command> insieme a un nome facoltativo del pool o dell'immagine. Inoltre, tramite l'opzione <option>--recursive</option>, è possibile elencare tutte le pianificazioni del livello specificato e dei livelli sottostanti. Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule ls --pool image-pool --recursive
POOL        NAMESPACE IMAGE  SCHEDULE
image-pool  -         -      every 1d starting at 14:00:00-05:00
image-pool            image1 every 6h
</screen>
    <para>
     Per verificare con <command>rbd</command> quando saranno creati i successivi snapshot per le immagini RBD con copia speculare basata su snapshot, specificare il comando <command>mirror snapshot schedule status</command> insieme a un nome facoltativo del pool o dell'immagine. Esempio:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror schedule status
SCHEDULE TIME       IMAGE
2020-02-26 18:00:00 image-pool/image1
</screen>
   </sect3>
   <sect3 xml:id="rbd-disenable-image-mirroring">
    <title>Disabilitazione della copia speculare dell'immagine</title>
    <para>
     Per disabilitare la copia speculare per un'immagine specifica, specificare il sottocomando <command>mirror image disable</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image disable <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
   <sect3 xml:id="rbd-image-promotion-demotion">
    <title>Promozione e abbassamento di livello delle immagini</title>
    <para>
     In uno scenario di failover in cui è necessario spostare la designazione primaria all'immagine nel cluster peer, è necessario interrompere l'accesso all'immagine primaria, abbassare di livello l'attuale immagine primaria, promuovere quella nuova e riprendere l'accesso all'immagine sul cluster alternativo.
    </para>
    <note>
     <title>promozione forzata</title>
     <para>
      È possibile forzare la promozione utilizzando l'opzione <option>--force</option>. La promozione forzata è necessaria quando è impossibile propagare l'abbassamento di livello al cluster peer (ad esempio, in caso di errore del cluster o di interruzione della comunicazione). Ne risulterà uno scenario split brain tra i due peer e l'immagine non viene più sincronizzata fino all'emissione di un sottocomando <command>resync</command>.
     </para>
    </note>
    <para>
     Per abbassare di livello un'immagine specifica a non primaria, specificare il sottocomando <command>mirror image demote</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror image demote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Per abbassare di livello tutte le immagini primarie in un pool a non primarie, specificare il sottocomando <command>mirror pool demote</command> insieme al nome del pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool demote <replaceable>POOL_NAME</replaceable></screen>
    <para>
     Per promuovere un'immagine specifica a primaria, specificare il sottocomando <command>mirror image promote</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster remote mirror image promote <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
    <para>
     Per promuovere tutte le immagini primarie in un pool a primarie, specificare il sottocomando <command>mirror pool promote</command> insieme al nome del pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd --cluster local mirror pool promote <replaceable>POOL_NAME</replaceable></screen>
    <tip>
     <title>suddivisione del carico I/O</title>
     <para>
      Poiché lo stato di primaria o non primaria si riferisce a un'immagine singola, è possibile fare in modo che il carico I/O e il failover o il failback della fase vengano suddivisi tra due cluster.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="rbd-force-image-resync">
    <title>Risincronizzazione forzata dell'immagine</title>
    <para>
     Se viene rilevato un evento split brain dal daemon <systemitem class="daemon">rbd-mirror</systemitem>, non verrà effettuato alcun tentativo di copia speculare dell'immagine interessata finché non viene corretto. Per riprendere la copia speculare di un'immagine, prima abbassare di livello l'immagine definita obsoleta, quindi richiedere una risincronizzazione all'immagine primaria. Per richiedere una risincronizzazione dell'immagine, specificare il sottocomando <command>mirror image resync</command> insieme al nome del pool e dell'immagine:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image resync <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   </sect3>
  </sect2>

  <sect2 xml:id="rbd-mirror-status">
   <title>Verifica dello stato della copia speculare</title>
   <para>
    Lo stato di replica del cluster peer viene memorizzato per ciascuna immagine primaria in copia speculare. È possibile recuperare tale stato mediante i sottocomandi <command>mirror image status</command> e <command>mirror pool status</command>:
   </para>
   <para>
    Per richiedere lo stato dell'immagine speculare, specificare il sottocomando <command>mirror image status</command> insieme al nome del pool e dell'immagine:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror image status <replaceable>POOL_NAME</replaceable>/<replaceable>IMAGE_NAME</replaceable></screen>
   <para>
    Per richiedere lo stato di riepilogo del pool speculare, specificare il sottocomando <command>mirror pool status</command> insieme al nome del pool:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd mirror pool status <replaceable>POOL_NAME</replaceable></screen>
   <tip>
    <title/>
    <para>
     Con l'aggiunta dell'opzione <option>--verbose</option> al sottocomando <command>mirror pool status</command> verranno generati ulteriori dettagli sullo stato di ciascuna immagine in copia speculare nel pool.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="rbd-cache-settings">
  <title>Impostazioni della cache</title>

  <para>
   L'implementazione dello spazio utente del dispositivo di blocco Ceph (<systemitem>librbd</systemitem>) non può servirsi della cache delle pagine Linux. Pertanto, dispone di memorizzazione nella cache integrata. Il comportamento della memorizzazione nella cache RBD è simile a quello della memorizzazione nella cache del disco rigido. Quando il sistema operativo invia una richiesta di sbarramento o di svuotamento, tutti i dati modificati vengono scritti sugli OSD. Ciò significa che la memorizzazione nella cache Write-back garantisce gli stessi livelli di sicurezza di un disco rigido fisico integro con una macchina virtuale che invia correttamente gli svuotamenti. La cache utilizza un algoritmo <emphasis>Least Recently Used</emphasis> (LRU) e, in modalità Write-back, è in grado di unire le richieste adiacenti per migliorare la velocità effettiva.
  </para>

  <para>
   Ceph supporta la memorizzazione nella cache Write-back per RBD. Per abilitarla, eseguire
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache true</screen>

  <para>
   Per default, <systemitem>librbd</systemitem> non esegue alcuna memorizzazione nella cache. Le operazioni di scrittura e lettura vanno direttamente al cluster di memorizzazione e le operazioni di scrittura vengono restituite solo quando i dati si trovano sul disco in tutte le repliche. Se la memorizzazione nella cache è abilitata, le operazioni di scrittura vengono subito restituite, a meno che il numero di byte non svuotati non sia superiore a quello impostato nell'opzione <option>rbd cache max dirty</option>. In questo caso, l'operazione di scrittura attiva il writeback e si blocca finché non viene svuotato un numero sufficiente di byte.
  </para>

  <para>
   Ceph supporta la memorizzazione nella cache Write-through per RBD. È possibile impostare le dimensioni della cache e le destinazioni e i limiti per passare dalla memorizzazione nella cache Write-back a quella Write-through. Per abilitare la modalità Write-through, eseguire
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client rbd_cache_max_dirty 0</screen>

  <para>
   Ciò vuol dire che le operazioni di scrittura vengono restituite solo quando i dati si trovano sul disco in tutte le repliche, ma che le operazioni di lettura possono provenire dalla cache. La cache si trova nella memoria del client e ogni immagine RBD dispone della propria cache. Dal momento che la cache si trova in locale sul client, se altri accedono all'immagine, non vi è alcuna coerenza. Se la memorizzazione nella cache è abilitata, non sarà possibile eseguire OCFS o GFS su RBD.
  </para>

  <para>
   I parametri seguenti influiscono sul comportamento dei dispositivi di blocco RADOS (RADOS Block Device, RBD). Per impostarli, utilizzare la categoria <literal>client</literal>:
  </para>

<screen><prompt>cephuser@adm &gt; </prompt>ceph config set client <replaceable>PARAMETER</replaceable> <replaceable>VALUE</replaceable></screen>

  <variablelist>
   <varlistentry>
    <term><option>rbd cache</option></term>
    <listitem>
     <para>
      Per abilitare la memorizzazione nella cache per il dispositivo di blocco RADOS (RBD). L'impostazione di default è "true".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache size</option></term>
    <listitem>
     <para>
      Dimensioni in byte della cache RBD. Il valore di default è 32 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty</option></term>
    <listitem>
     <para>
      Limite di dati modificati espresso in byte in corrispondenza del quale la cache attiva il Write-back. Il valore di <option>rbd cache max dirty</option> deve essere inferiore a <option>rbd cache size</option>. Se è impostato a 0, utilizza la memorizzazione nella cache Write-through. Il valore di default è 24 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache target dirty</option></term>
    <listitem>
     <para>
      Destinazione modificata prima che la cache inizi le operazioni di scrittura dei dati nel relativo storage. Non blocca le operazioni di scrittura nella cache. Il valore di default è 16 MB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache max dirty age</option></term>
    <listitem>
     <para>
      Numero di secondi in cui i dati modificati si trovano nella cache prima dell'avvio del writeback. Il valore di default è 1.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd cache writethrough until flush</option></term>
    <listitem>
     <para>
      Si avvia in modalità Write-through e passa alla modalità Write-back in seguito alla ricezione della prima richiesta di svuotamento. Si tratta di un'impostazione prudente ma sicura se le macchine virtuali in esecuzione su <systemitem>rbd</systemitem> sono troppo obsolete per inviare richieste di svuotamento (ad esempio, nel caso del driver virtio in Linux prima del kernel 2.6.32). L'impostazione di default è "true".
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-qos">
  <title>Impostazioni QoS</title>

  <para>
   In genere, la Qualità di servizio (QoS, Quality of Service) fa riferimento ai metodi di prioritizzazione del traffico e di prenotazione delle risorse. È particolarmente importante per il trasporto del traffico con requisiti speciali.
  </para>

  <important>
   <title>non supportate da iSCSI</title>
   <para>
    Le impostazioni QoS seguenti sono utilizzate solo dall'implementazione dello spazio utente RBD <systemitem class="daemon">librbd</systemitem> e <emphasis>non</emphasis> sono utilizzate dall'implementazione <systemitem>kRBD</systemitem>. Poiché iSCSI utilizza <systemitem>kRBD</systemitem>, non usa le impostazioni QoS. Tuttavia, per iSCSI è possibile configurare la QoS sul livello del dispositivo di blocco del kernel tramite le facility del kernel standard.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd qos iops limit</option></term>
    <listitem>
     <para>
      Limite desiderato delle operazioni I/O al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps limit</option></term>
    <listitem>
     <para>
      Limite desiderato dei byte I/O al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops limit</option></term>
    <listitem>
     <para>
      Il limite desiderato di operazioni di lettura al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops limit</option></term>
    <listitem>
     <para>
      Il limite desiderato di operazioni di scrittura al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps limit</option></term>
    <listitem>
     <para>
      Il limite desiderato di byte letti al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps limit</option></term>
    <listitem>
     <para>
      Il limite desiderato di byte scritti al secondo. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos iops burst</option></term>
    <listitem>
     <para>
      Limite di burst desiderato delle operazioni I/O. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos bps burst</option></term>
    <listitem>
     <para>
      Limite di burst desiderato dei byte I/O. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read iops burst</option></term>
    <listitem>
     <para>
      Il limite di burst desiderato delle operazioni di lettura. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write iops burst</option></term>
    <listitem>
     <para>
      Il limite di burst desiderato delle operazioni di scrittura. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos read bps burst</option></term>
    <listitem>
     <para>
      Il limite di burst desiderato dei byte letti. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos write bps burst</option></term>
    <listitem>
     <para>
      Il limite di burst desiderato dei byte scritti. Il valore di default è 0 (nessun limite).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd qos schedule tick min</option></term>
    <listitem>
     <para>
      Tick di pianificazione minimo (in millisecondi) per QoS. Il valore di default è 50.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-readahead-settings">
  <title>Impostazioni di lettura in avanti</title>

  <para>
   Il dispositivo di blocco RADOS supporta la lettura in avanti/prelettura per l'ottimizzazione delle operazioni di lettura piccole e sequenziali. Nel caso di una macchina virtuale, queste operazioni di lettura sono in genere gestite dal sistema operativo guest, anche se i boot loader potrebbero non generare letture efficaci. Se la memorizzazione nella cache è disabilitata, la lettura in avanti viene disabilitata automaticamente.
  </para>

  <important>
   <title>non supportate da iSCSI</title>
   <para>
    Le impostazioni di lettura in avanti seguenti sono utilizzate solo dall'implementazione dello spazio utente RBD <systemitem class="daemon">librbd</systemitem> e <emphasis>non</emphasis> sono utilizzate dall'implementazione <systemitem>kRBD</systemitem>. Poiché iSCSI utilizza <systemitem>kRBD</systemitem>, non usa le impostazioni di lettura in avanti. Tuttavia, per iSCSI è possibile configurare la lettura in avanti sullo strato del dispositivo di blocco del kernel tramite le facility del kernel standard.
   </para>
  </important>

  <variablelist>
   <varlistentry>
    <term><option>rbd readahead trigger requests</option></term>
    <listitem>
     <para>
      Numero di richieste di lettura sequenziali necessarie per attivare la lettura in avanti. Il valore di default è 10.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead max bytes</option></term>
    <listitem>
     <para>
      Dimensioni massime della richiesta di lettura in avanti. Se è impostata su 0, la lettura in avanti è disabilitata. Il valore di default è 512 kB.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>rbd readahead disable after bytes</option></term>
    <listitem>
     <para>
      In seguito alla lettura di questo numero di byte da un'immagine RBD, la lettura in avanti è disabilitata per tale immagine fino alla chiusura. In questo modo, il sistema operativo guest può subentrare alla lettura in avanti al momento dell'avvio. Se è impostata su 0, la lettura in avanti rimane abilitata. Il valore di default è 50 MB.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-features">
  <title>Funzioni avanzate</title>

  <para>
   Il dispositivo di blocco RADOS supporta le funzioni avanzate che consentono di migliorare la funzionalità delle immagini RBD. È possibile specificare le funzioni sulla riga di comando durante la creazione di un'immagine RBD oppure nel file di configurazione Ceph tramite l'opzione <option>rbd_default_features</option>.
  </para>

  <para>
   È possibile specificare i valori dell'opzione <option>rbd_default_features</option> in due modi:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Come somma dei valori interni delle funzioni. Ogni funzione dispone di un proprio valore interno; ad esempio il valore di "layering" è 1 e quello di "fast-diff" è 16. Di conseguenza, per attivare queste due funzioni per default, includere quanto segue:
    </para>
<screen>
rbd_default_features = 17
</screen>
   </listitem>
   <listitem>
    <para>
     Come elenco di funzioni separate da virgole. L'esempio precedente avrà l'aspetto seguente:
    </para>
<screen>
rbd_default_features = layering,fast-diff
</screen>
   </listitem>
  </itemizedlist>

  <note>
   <title>funzioni non supportate da iSCSI</title>
   <para>
    Le immagini RBD con le funzioni seguenti non saranno supportate da iSCSI: <option>deep-flatten</option>, <option>object-map</option>, <option>journaling</option>, <option>fast-diff</option>, <option>striping</option>
   </para>
  </note>

  <para>
   Di seguito è riportato un elenco delle funzioni RBD avanzate:
  </para>

  <variablelist>
   <varlistentry>
    <term><option>layering</option></term>
    <listitem>
     <para>
      Il layering consente di utilizzare la funzione di clonazione.
     </para>
     <para>
      Il valore interno è 1; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>striping</option></term>
    <listitem>
     <para>
      Lo striping consente di distribuire i dati tra più oggetti e agevola il parallelismo dei workload sequenziali di lettura/scrittura. Consente di evitare la formazione di colli di bottiglia sui singoli nodi per dispositivi di blocco RADOS di grandi dimensioni oppure occupati.
     </para>
     <para>
      Il valore interno è 2; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>exclusive-lock</option></term>
    <listitem>
     <para>
      Quando abilitata, necessita di un client per ottenere un blocco su un oggetto prima di eseguire un'operazione di scrittura. Abilitare il blocco esclusivo solo quando un client singolo sta effettuando l'accesso a un'immagine nello stesso momento. Il valore interno è 4. L'impostazione di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>object-map</option></term>
    <listitem>
     <para>
      Il supporto della mappa oggetti dipende dal supporto del blocco esclusivo. I dispositivi di blocco sono soggetti a thin provisioning e di conseguenza memorizzano soltanto i dati effettivamente esistenti. Il supporto della mappa oggetti consente di monitorare gli oggetti effettivamente esistenti (con dati memorizzati su un'unità). L'abilitazione del supporto della mappa oggetti consente di velocizzare le operazioni I/O per la clonazione, l'importazione e l'esportazione di un'immagine compilata in modo sparse e per l'eliminazione.
     </para>
     <para>
      Il valore interno è 8; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>fast-diff</option></term>
    <listitem>
     <para>
      Il supporto di fast-diff dipende dal supporto della mappa oggetti e del blocco esclusivo. Aggiunge un'altra proprietà alla mappa oggetti che la rende notevolmente più rapida nella creazione delle differenze tra le snapshot di un'immagine e l'utilizzo effettivo dei dati di una snapshot.
     </para>
     <para>
      Il valore interno è 16; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>deep-flatten</option></term>
    <listitem>
     <para>
      La funzione deep-flatten attiva l'opzione <command>rbd flatten</command> (vedere la <xref linkend="rbd-flatten-cloned-image"/>) su tutte le snapshot di un'immagine oltre che sull'immagine stessa. Senza di essa, le snapshot di un'immagine continueranno a dipendere da quella superiore e di conseguenza non sarà possibile eliminare tale immagine superiore finché non vengono eliminate anche le snapshot. Con l'impostazione deep-flatten, l'elemento superiore è reso indipendente dai cloni, anche se dispongono di snapshot.
     </para>
     <para>
      Il valore interno è 32; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><option>journaling</option></term>
    <listitem>
     <para>
      Il supporto del journaling dipende dal supporto del blocco esclusivo. La funzione di journaling consente di registrare tutte le modifiche apportate a un'immagine nell'ordine in cui si verificano. La copia speculare RBD (vedere la <xref linkend="ceph-rbd-mirror"/>) utilizza il journal per replicare un'immagine coerente con l'arresto anomalo in un cluster remoto.<literal/>
     </para>
     <para>
      Il valore interno è 64; quello di default è "yes".
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="rbd-old-clients-map">
  <title>Mappatura di RBD tramite i client kernel meno recenti</title>

  <para>
   I client meno recenti (ad esempio SLE11 SP4) potrebbero non essere in grado di mappare le immagini RBD poiché un cluster distribuito con SUSE Enterprise Storage 7 forza alcune funzioni (sia le funzioni a livello di immagine RBD che quelle a livello RADOS) non supportate dai client meno recenti. In questo caso, sui log OSD vengono visualizzati dei messaggi simili al seguente:
  </para>

<screen>2019-05-17 16:11:33.739133 7fcb83a2e700  0 -- 192.168.122.221:0/1006830 &gt;&gt; \
192.168.122.152:6789/0 pipe(0x65d4e0 sd=3 :57323 s=1 pgs=0 cs=0 l=1 c=0x65d770).connect \
protocol feature mismatch, my 2fffffffffff &lt; peer 4010ff8ffacffff missing 401000000000000
</screen>

  <warning>
   <title>la modifica dei tipi di compartimento della mappa CRUSH causa un significativo ribilanciamento</title>
   <para>
    Se si intende passare dal tipo di compartimento della mappa CRUSH "straw" a quello "straw2" e viceversa, procedere secondo una pianificazione. È prevedibile che questa operazione causi un impatto significativo sul carico del cluster, poiché la modifica del tipo di compartimento causerà un significativo ribilanciamento.
   </para>
  </warning>

  <procedure>
   <step>
    <para>
     Disabilitare le funzioni dell'immagine RBD non supportate. Esempio:
    </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 object-map
<prompt>cephuser@adm &gt; </prompt>rbd feature disable pool1/image1 exclusive-lock
</screen>
   </step>
   <step>
    <para>
     Modificare il tipo di compartimento della mappa CRUSH da "straw2" a "straw":
    </para>
    <substeps>
     <step>
      <para>
       Salvare la mappa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd getcrushmap -o crushmap.original
</screen>
     </step>
     <step>
      <para>
       Decompilare la mappa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -d crushmap.original -o crushmap.txt
</screen>
     </step>
     <step>
      <para>
       Modificare la mappa CRUSH e sostituire "straw2" con "straw".
      </para>
     </step>
     <step>
      <para>
       Ricompilare la mappa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>crushtool -c crushmap.txt -o crushmap.new
</screen>
     </step>
     <step>
      <para>
       Impostare la nuova mappa CRUSH:
      </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph osd setcrushmap -i crushmap.new
</screen>
     </step>
    </substeps>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="rbd-kubernetes">
  <title>Abilitazione dei dispositivi di blocco e di Kubernetes</title>

  <para>
   È possibile utilizzare i dispositivi di blocco RADOS Ceph con Kubernetes v1.13 e versioni successive tramite il driver <literal>ceph-csi</literal>, che esegue il provisioning dinamico delle immagini RBD sui volumi Kubernetes sottostanti e la mappatura di tali immagini sotto forma di dispositivi di blocco (facoltativamente montando un file system contenuto all'interno dell'immagine) ai nodi di lavoro su cui sono in esecuzione i pod che fanno riferimento a un volume con supporto RBD.
  </para>

  <para>
   Per utilizzare i dispositivi di blocco Ceph con Kubernetes, è necessario installare e configurare <literal>ceph-csi</literal> all'interno dell'ambiente Kubernetes.
  </para>

  <important>
   <para>
    <literal>ceph-csi</literal> utilizza per default i moduli del kernel RBD, che potrebbero non supportare tutti gli elementi ottimizzabili CRUSH di Ceph o le funzioni dell'immagine RBD.
   </para>
  </important>

  <procedure>
   <step>
    <para>
     Per default, i dispositivi di blocco Ceph utilizzano il pool RBD. Creare un pool per la memorizzazione del volume Kubernetes. Assicurarsi che il cluster Ceph sia in esecuzione e creare il pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph osd pool create kubernetes</screen>
   </step>
   <step>
    <para>
     Utilizzare lo strumento RBD per inizializzare il pool:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>rbd pool init kubernetes</screen>
   </step>
   <step>
    <para>
     Creare un nuovo utente per Kubernetes e <literal>ceph-csi</literal>. Eseguire quanto riportato di seguito e registrare la chiave generata:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph auth get-or-create client.kubernetes mon 'profile rbd' osd 'profile rbd pool=kubernetes' mgr 'profile rbd pool=kubernetes'
[client.kubernetes]
    key = AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==</screen>
   </step>
   <step>
    <para>
     Per definire gli indirizzi del Ceph Monitor per il cluster Ceph, <literal>ceph-csi</literal> necessita di un oggetto ConfigMap memorizzato in Kubernetes. Raccogliere l'FSID e gli indirizzi del monitor univoci del cluster Ceph:
    </para>
<screen><prompt>cephuser@adm &gt; </prompt>ceph mon dump
&lt;...&gt;
fsid b9127830-b0cc-4e34-aa47-9d1a2e9949a8
&lt;...&gt;
0: [v2:192.168.1.1:3300/0,v1:192.168.1.1:6789/0] mon.a
1: [v2:192.168.1.2:3300/0,v1:192.168.1.2:6789/0] mon.b
2: [v2:192.168.1.3:3300/0,v1:192.168.1.3:6789/0] mon.c</screen>
   </step>
   <step>
    <para>
     Generare un file <filename>csi-config-map.yaml</filename> simile all'esempio riportato di seguito, sostituendo l'FSID per <literal>clusterID</literal> e gli indirizzi dei monitor per <literal>monitors</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-config-map.yaml
---
apiVersion: v1
kind: ConfigMap
data:
  config.json: |-
    [
      {
        "clusterID": "b9127830-b0cc-4e34-aa47-9d1a2e9949a8",
        "monitors": [
          "192.168.1.1:6789",
          "192.168.1.2:6789",
          "192.168.1.3:6789"
        ]
      }
    ]
metadata:
  name: ceph-csi-config
EOF</screen>
   </step>
   <step>
    <para>
     Quindi, memorizzare il nuovo oggetto ConfigMap in Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-config-map.yaml</screen>
   </step>
   <step>
    <para>
     Per la comunicazione con il cluster Ceph tramite <literal>ceph-csi</literal>, sono necessarie le credenziali cephx. Generare un file <filename>csi-rbd-secret.yaml</filename> simile a quello nell'esempio riportato di seguito utilizzando l'ID utente Kubernetes e la chiave cephx appena creati:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: csi-rbd-secret
  namespace: default
stringData:
  userID: kubernetes
  userKey: AQD9o0Fd6hQRChAAt7fMaSZXduT3NWEqylNpmg==
EOF</screen>
   </step>
   <step>
    <para>
     Quindi, memorizzare il nuovo oggetto segreto in Kubernetes:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-secret.yaml</screen>
   </step>
   <step>
    <para>
     Creare il ServiceAccount e gli oggetti Kubernetes RBAC ClusterRole/ClusterRoleBinding richiesti. Questi oggetti non devono essere necessariamente personalizzati per il proprio ambiente Kubernetes e pertanto è possibile utilizzarli direttamente dai file YAML di distribuzione di <literal>ceph-csi</literal>.
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-provisioner-rbac.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-nodeplugin-rbac.yaml</screen>
   </step>
   <step>
    <para>
     Creare lo strumento di provisioning e i plug-in del nodo <literal>ceph-csi</literal>:
    </para>
<screen><prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin-provisioner.yaml
<prompt>kubectl@adm &gt; </prompt>wget https://raw.githubusercontent.com/ceph/ceph-csi/master/deploy/rbd/kubernetes/csi-rbdplugin.yaml
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbdplugin.yaml</screen>
    <important>
     <para>
      Per default, i file YAML dello strumento di provisioning e del plug-in del nodo eseguiranno il pull della release di sviluppo del container <literal>ceph-csi</literal>. È necessario aggiornare i file YAML per utilizzare la versione della release.
     </para>
    </important>
   </step>
  </procedure>

  <sect2 xml:id="using-rbd-kubernetes">
   <title>Uso dei dispositivi di blocco Ceph in Kubernetes</title>
   <para>
    La StorageClass Kubernetes definisce una classe di storage. È possibile creare più oggetti StorageClass per eseguire la mappatura a diversi livelli di qualità del servizio e funzioni. Si pensi ad esempio ai pool NVMe rispetto a quelli basati su HDD.
   </para>
   <para>
    Per creare una StorageClass <literal>ceph-csi</literal> che mappi al pool Kubernetes creato sopra, è possibile utilizzare il file YAML seguente, dopo aver verificato che la proprietà <literal>clusterID</literal> corrisponda all'FSID del cluster Ceph:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; csi-rbd-sc.yaml
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
   name: csi-rbd-sc
provisioner: rbd.csi.ceph.com
parameters:
   clusterID: b9127830-b0cc-4e34-aa47-9d1a2e9949a8
   pool: kubernetes
   csi.storage.k8s.io/provisioner-secret-name: csi-rbd-secret
   csi.storage.k8s.io/provisioner-secret-namespace: default
   csi.storage.k8s.io/node-stage-secret-name: csi-rbd-secret
   csi.storage.k8s.io/node-stage-secret-namespace: default
reclaimPolicy: Delete
mountOptions:
   - discard
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f csi-rbd-sc.yaml</screen>
   <para>
    Una <literal>PersistentVolumeClaim</literal> è una richiesta di risorse di memorizzazione astratte effettuata da un utente. La richiesta <literal>PersistentVolumeClaim</literal> viene quindi associata a una risorsa pod per il provisioning di un <literal>PersistentVolume</literal>, supportato da un'immagine del dispositivo di blocco Ceph. È possibile includere un valore <option>volumeMode</option> facoltativo per scegliere tra un file system montato (default) o un volume non elaborato basato sul dispositivo di blocco.
   </para>
   <para>
    Con <literal>ceph-csi</literal>, specificando <option>Filesystem</option> per <option>volumeMode</option>, verranno supportate sia le richieste <literal>ReadWriteOnce</literal> che quelle <literal>ReadOnlyMany accessMode</literal>, mentre specificando <option>Block</option> per <option>volumeMode</option>, verranno supportate le richieste <literal>ReadWriteOnce</literal>, <literal>ReadWriteMany</literal> e <literal>ReadOnlyMany accessMode</literal>.
   </para>
   <para>
    Ad esempio, per creare una richiesta <literal>PersistentVolumeClaim</literal> basata su blocchi in cui venga utilizzata la <literal>ceph-csi-based StorageClass</literal> creata sopra, è possibile usare il file YAML seguente per richiedere la memorizzazione del blocco non elaborato di <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: raw-block-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Block
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pvc.yaml</screen>
   <para>
    Di seguito è riportato un esempio di associazione della richiesta <literal>PersistentVolumeClaim</literal> descritta sopra a una risorsa pod sotto forma di dispositivo di blocco non elaborato:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; raw-block-pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-raw-block-volume
spec:
  containers:
    - name: fc-container
      image: fedora:26
      command: ["/bin/sh", "-c"]
      args: ["tail -f /dev/null"]
      volumeDevices:
        - name: data
          devicePath: /dev/xvda
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: raw-block-pvc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f raw-block-pod.yaml</screen>
   <para>
    Per creare una richiesta <literal>PersistentVolumeClaim</literal> basata su file system in cui venga utilizzata la <literal>ceph-csi-based StorageClass</literal> creata sopra, è possibile utilizzare il file YAML seguente per richiedere un file system montato (supportato da un'immagine RBD) di <literal>csi-rbd-sc StorageClass</literal>:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pvc.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: rbd-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: csi-rbd-sc
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pvc.yaml</screen>
   <para>
    Di seguito è riportato un esempio di associazione della richiesta <literal>PersistentVolumeClaim</literal> descritta sopra a una risorsa pod sotto forma di file system montato:
   </para>
<screen><prompt>kubectl@adm &gt; </prompt>cat &lt;&lt;EOF &gt; pod.yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: csi-rbd-demo-pod
spec:
  containers:
    - name: web-server
      image: nginx
      volumeMounts:
        - name: mypvc
          mountPath: /var/lib/www/html
  volumes:
    - name: mypvc
      persistentVolumeClaim:
        claimName: rbd-pvc
        readOnly: false
EOF
<prompt>kubectl@adm &gt; </prompt>kubectl apply -f pod.yaml</screen>
  </sect2>
 </sect1>
</chapter>
