<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>File system in cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sì</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  In questo capitolo sono descritti i task amministrativi che di norma vengono eseguiti dopo la configurazione del cluster e l'esportazione di CephFS. Per ulteriori informazioni sulla configurazione di CephFS, fare riferimento a <xref linkend="deploy-cephadm-day2-service-mds"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montaggio di CephFS</title>

  <para>
   Una volta creato il file system ed MDS è attivo, si è pronti per montare il file da un host client.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Preparazione del client</title>
   <para>
    Se sull'host client è in esecuzione SUSE Linux Enterprise 12 SP2 o versioni successive, il sistema è pronto per il montaggio di CephFS senza alcuna configurazione aggiuntiva.
   </para>
   <para>
    Se sull'host client è in esecuzione SUSE Linux Enterprise 12 SP1, è necessario applicare le patch più recenti prima di montare CephFS.
   </para>
   <para>
    In ogni caso, tutto il necessario per montare CephFS è incluso in SUSE Linux Enterprise. Il prodotto SUSE Enterprise Storage 7 non è necessario.
   </para>
   <para>
    Per supportare la sintassi <command>mount</command> completa, è necessario installare il pacchetto
    <package>ceph-common</package> (fornito in dotazione con SUSE Linux Enterprise) prima di tentare di montare CephFS.
   </para>
   <important>
    <para>
     Senza il pacchetto <package>ceph-common</package> (e pertanto senza l'helper <command>mount.ceph</command>), occorrerà utilizzare gli IP dei monitor al posto del loro nome, poiché il client kernel non sarà in grado di eseguire la risoluzione del nome.
    </para>
    <para>
     La sintassi mount di base è:
    </para>
<screen>
<prompt role="root">root # </prompt>mount -t ceph <replaceable>MON1_IP</replaceable>[:<replaceable>PORT</replaceable>],<replaceable>MON2_IP</replaceable>[:<replaceable>PORT</replaceable>],...:<replaceable>CEPHFS_MOUNT_TARGET</replaceable> \
<replaceable>MOUNT_POINT</replaceable> -o name=<replaceable>CEPHX_USER_NAME</replaceable>,secret=<replaceable>SECRET_STRING</replaceable>
</screen>
   </important>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Creazione di un file segreto</title>
   <para>
    Per default, quando il cluster Ceph è in esecuzione l'autenticazione è attivata. È necessario creare un file in cui archiviare la chiave segreta (non il portachiavi stesso). Per ottenere la chiave segreta per un determinato utente e creare il file, procedere come indicato di seguito:
   </para>
   <procedure>
    <title>Creazione di una chiave segreta</title>
    <step>
     <para>
      Visualizzare la chiave per l'utente specificato in un file portachiavi:
     </para>
<screen><prompt>cephuser@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copiare la chiave dell'utente che utilizzerà il file system Ceph FS montato. Di norma, la chiave è simile a quanto riportato di seguito:
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Creare un file in cui il nome utente costituisce parte del nome file, ad esempio <filename>/etc/ceph/admin.secret</filename> per l'utente <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Incollare il valore della chiave nel file creato al passaggio precedente.
     </para>
    </step>
    <step>
     <para>
      Impostare i diritti di accesso al file appropriati. L'utente deve essere il solo in grado di leggere il file, gli altri non possono avere alcun diritto di accesso.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montaggio di CephFS</title>
   <para>
    È possibile montare CephFS utilizzando il comando <command>mount</command>. È necessario specificare il nome host di monitoraggio o l'indirizzo IP. Poiché in SUSE Enterprise Storage l'autenticazione <systemitem>cephx</systemitem> è abilitata per default, è necessario specificare un nome utente e il rispettivo segreto:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Dato che il comando precedente rimane nella cronologia shell, la lettura del segreto da un file rappresenta un approccio più sicuro:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Il file segreto deve contenere solo il segreto del portachiavi effettivo. Nell'esempio, il file conterrà quindi solo la riga seguente:
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>specifica di più monitor</title>
    <para>
     Qualora uno dei monitoraggi dovesse essere inattivo al momento del montaggio, è buona prassi specificare più monitoraggi separati da virgole nella riga di comando <command>mount</command>. Ciascun indirizzo di monitoraggio si presenta nella forma <literal>host[:port]</literal>. Se non si specifica la porta, per default viene utilizzata la 6789.
    </para>
   </tip>
   <para>
    Creare il punto di montaggio nell'host locale:
   </para>
<screen><prompt role="root">root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Montaggio di CephFS:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Se è necessario montare un sottoinsieme del file system, è possibile specificare una sottodirectory <filename>subdir</filename>:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    È possibile specificare più host di monitoraggio nel comando <command>mount</command>:
   </para>
<screen><prompt role="root">root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>accesso in lettura alla directory radice</title>
    <para>
     Se si utilizzano client con restrizioni di percorso, tra le funzionalità MDS deve essere incluso l'accesso in lettura alla directory radice. Ad esempio, un portachiavi potrebbe presentare l'aspetto seguente:
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     la parte <literal>allow r path=/</literal> significa che i client con restrizioni di percorso sono in grado di leggere il volume radice, ma non di scriverlo. Nei casi di utilizzo in cui l'isolamento completo è un requisito, ciò può rappresentare un problema.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Smontaggio di CephFS</title>

  <para>
   Per smontare CephFS, utilizzare il comando <command>umount</command>:
  </para>

<screen><prompt role="root">root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>Montaggio di CephFS in <filename>/etc/fstab</filename></title>

  <para>
   Per montare CephFS automaticamente all'avvio del client, inserire la riga corrispondente nella rispettiva tabella dei file system <filename>/etc/fstab</filename>:
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Più daemon MDS attivi (MDS attivo-attivo)</title>

  <para>
   Per default CephFS è configurato per un unico daemon MDS attivo. Per adattare le prestazioni dei metadati a sistemi su larga scala, è possibile abilitare più daemon MDS attivi in modo che il workload dei metadati venga condiviso con un altro.
  </para>

  <sect2 xml:id="using-active-active-mds">
   <title>Utilizzo dell'MDS attivo-attivo</title>
   <para>
    Quando le prestazioni dei metadati sull'MDS singolo di default si bloccano, prendere in considerazione l'eventualità di utilizzare più daemon MDS attivi.
   </para>
   <para>
    L'aggiunta di ulteriori daemon non migliora le prestazioni su tutti i tipi di workload. Ad esempio, una singola applicazione in esecuzione su un unico client non sarà avvantaggiata da un maggior numero di daemon MDS, a meno che l'applicazione non esegua parallelamente molte operazioni di metadati.
   </para>
   <para>
    I workload che di norma traggono vantaggi da un maggior numero di daemon MDS attivi sono quelli provvisti di numerosi client, che magari funzionano su molte directory separate.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Aumento delle dimensioni del cluster attivo MDS</title>
   <para>
    Ciascun file system CephFS presenta un'impostazione <option>max_mds</option> che controlla il numero di livelli che saranno creati. Il numero di livelli effettivo nel file system aumenterà solo se è disponibile un daemon di riserva da inserire nel nuovo livello. Se ad esempio è in esecuzione un solo daemon MDS e <option>max_mds</option> è impostato su due, non verrà creato un secondo livello.
   </para>
   <para>
    Nell'esempio seguente, l'opzione <option>max_mds</option> viene impostata a 2 per creare un nuovo livello oltre a quello di default. Per visualizzare le modifiche, eseguire <command>ceph status</command> prima di impostare <option>max_mds</option>, quindi osservare la riga contenente <literal>fsmap</literal>:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 2
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    Il livello appena creato (1) passa dallo stato "creating" al rispettivo stato "active".
   </para>
   <important>
    <title>daemon in standby</title>
    <para>
     Anche con più daemon MDS attivi, in un sistema altamente disponibile è comunque richiesto che vengano impiegati daemon in standby se i server in esecuzione su un daemon attivo vengono meno.
    </para>
    <para>
     Di conseguenza, il numero massimo effettivo di <option>max_mds</option> per i sistemi altamente disponibili è uno in meno rispetto al numero totale di server MDS presenti nel sistema. Per garantire la disponibilità qualora si verificassero più errori di server, aumentare il numero di daemon in standby nel sistema in modo che corrisponda al numero di errori di server necessario per poter rimanere attivi.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Riduzione del numero di livelli</title>
   <para>
    Tutti i livelli, inclusi quelli da rimuovere, devono prima essere attivi. Vale a dire che è necessario disporre di almeno <option>max_mds</option> daemon MDS disponibili.
   </para>
   <para>
    In primo luogo, impostare <option>max_mds</option> a un numero inferiore. Ad esempio, tornare a un singolo MDS attivo:
   </para>
<screen><prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> fs set cephfs max_mds 1
<prompt>cephuser@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Aggiunta manuale di un albero della directory a un livello</title>
   <para>
    Per distribuire uniformemente il carico dei metadati nel cluster in configurazioni con vari server di metadati attivi, viene eseguito un bilanciamento. Di norma, questo metodo risulta sufficientemente efficace per la maggior parte degli utenti, ma talvolta è auspicabile sostituire il bilanciamento dinamico con mappature esplicite dei metadati in determinati livelli. In tal modo l'amministratore o gli utenti possono distribuire uniformemente il carico di applicazioni o limitare l'impatto delle richieste di metadati degli utenti sull'intero cluster.
   </para>
   <para>
    Il meccanismo fornito a tale scopo è denominato "pin di esportazione". Si tratta di un attributo esteso delle directory il cui nome è <literal>ceph.dir.pin</literal>. Gli utenti possono impostare questo attributo mediante l'uso di comandi standard:
   </para>
<screen><prompt role="root">root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    Il valore (<option>-v</option>) dell'attributo esteso corrisponde al livello cui assegnare il sottoalbero della directory. Un valore di default pari a -1 indica che la directory non è stata aggiunta.
   </para>
   <para>
    La directory superiore più vicina con pin di esportazione impostato, eredita il pin. Pertanto, l'impostazione del pin di esportazione in una directory influisce su tutte le rispettive directory secondarie. Il pin della directory superiore può essere sostituito impostando il pin di esportazione della directory secondaria. Esempio:
   </para>
<screen><prompt role="root">root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Gestione del failover</title>

  <para>
   Se un daemon MDS smette di comunicare con il monitoraggio, questo attenderà <option>mds_beacon_grace</option> secondi (valore di default 15 secondi) prima di contrassegnare il daemon come <emphasis>lento</emphasis>. È possibile configurare uno o più daemon "in standby" che verranno impiegati durante il failover del daemon MDS.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configurazione della riproduzione in standby</title>
   <para>
    È possibile configurare ciascun file system CephFS per aggiungere daemon standby-replay. Questi daemon standby-replay seguono il journal dei metadati dell'MDS attivo per ridurre il tempo di failover nel caso in cui quest'ultimo dovesse diventare non disponibile. Ogni MDS attivo può essere seguito da un solo daemon standby-replay.
   </para>
   <para>
    Configurare la riproduzione in standby su un file system con il comando seguente:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>FS-NAME</replaceable> allow_standby_replay <replaceable>BOOL</replaceable>
</screen>
   <para>
    Se impostati, i monitor assegneranno i daemon standby-replay disponibili per fare in modo che seguano gli MDS attivi su tale file system.
   </para>
   <para>
    Una volta che un MDS entra in stato di riproduzione in standby, verrà utilizzato solo come standby per il livello seguito. Se un altro livello genera un errore, il daemon standby-replay non verrà utilizzato come sostituto, anche se non ve ne sono altri in standby disponibili. Per questo motivo, se si utilizza la riproduzione in standby, è consigliabile che per ogni MDS attivo sia presente un daemon standby-replay.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Impostazione delle quote CephFS</title>

  <para>
   È possibile impostare le quote in qualsiasi sottodirectory del file system Ceph. La quota consente di limitare il numero di <emphasis role="bold">byte</emphasis> o <emphasis role="bold">file</emphasis> memorizzati al di sotto del punto specificato nella gerarchia della directory.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limitazioni delle quote CephFS</title>
   <para>
    L'uso delle quote con CephFS pone le limitazioni seguenti:
   </para>
   <variablelist>
    <varlistentry>
     <term>Le quote sono cooperative e non concorrenziali.</term>
     <listitem>
      <para>
       Le quote Ceph contano sul fatto che il client, al raggiungimento di uno specifico limite, interrompa la scrittura sul file system di cui esegue il montaggio. La parte server non può impedire a un client dannoso di scrivere quanti più dati possibile. Le quote non devono essere utilizzate per evitare il riempimento del file system negli ambienti con client completamente inattendibili.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Le quote non sono precise.</term>
     <listitem>
      <para>
       I processi che scrivono sul file system verranno interrotti poco dopo il raggiungimento del limite di quota. Verrà loro inevitabilmente concesso di scrivere alcuni dati oltre il limite configurato. I client scrittori verranno interrotti entro alcuni decimi di secondo dopo il superamento di tale limite.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Le quote sono implementate nel client kernel a partire dalla versione 4.17.</term>
     <listitem>
      <para>
       Le quote sono supportate dal client dello spazio utente (libcephfs, ceph-fuse). I client kernel Linux 4.17 e versioni successive supportano le quote CephFS sui cluster di SUSE Enterprise Storage 7. I client kernel (anche le versioni recenti) non sono in grado di gestire le quote sui cluster meno recenti, anche se possono impostarne gli attributi estesi. I kernel SLE12-SP3 (e versioni successive) includono già i backport necessari per la gestione delle quote.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>In presenza di restrizioni di montaggio basate sul percorso, prestare attenzione quando si configurano le quote.</term>
     <listitem>
      <para>
       Per applicare le quote, il client deve disporre dell'accesso all'inode della directory sulla quale sono configurate. Il client non applicherà la quota se dispone dell'accesso limitato a un percorso specifico (ad esempio <filename>home/user</filename>) in base alla funzionalità MDS e se la quota è configurata su una directory antenato a cui il client non ha accesso (<filename>/home</filename>). Se si applicano restrizioni di accesso basate sul percorso, assicurarsi di configurare la quota sulla directory a cui il client può accedere (ad esempio <filename>/home/user</filename> o <filename>/home/user/quota_dir</filename>).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configurazione delle quote CephFS</title>
   <para>
    È possibile configurare le quote CephFS tramite gli attributi estesi virtuali:
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configura un limite di <emphasis>file</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configura un limite di <emphasis>byte</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Se gli attributi vengono visualizzati su un inode della directory, la quota viene configurata in questa posizione. Se gli attributi non sono presenti, in questa directory non viene impostata nessuna quota (anche se potrebbe comunque esserne configurata una nella directory superiore).
   </para>
   <para>
    Per impostare una quota di 100 MB, eseguire:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Per impostare una quota di 10.000 file, eseguire:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Per visualizzare l'impostazione della quota, eseguire:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephuser@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>quota non impostata</title>
    <para>
     Se il valore dell'attributo esteso è "0", la quota non è impostata.
    </para>
   </note>
   <para>
    Per rimuovere una quota, eseguire:
   </para>
<screen>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephuser@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Gestione degli snapshot CephFS</title>

  <para>
   Le snapshot CephFS creano una vista di sola lettura del file system relativa al momento in cui vengono acquisite. È possibile creare una snapshot in qualsiasi directory. La snapshot includerà tutti i dati del file system nella directory specificata. In seguito alla creazione della snapshot, i dati memorizzati nel buffer vengono trasferiti in modo asincrono da diversi client. Di conseguenza, la creazione delle snapshot è un processo molto rapido.
  </para>

  <important>
   <title>file system multipli</title>
   <para>
    Se sono presenti più file system CephFS che condividono un singolo pool (tramite spazi dei nomi), le relative snapshot entreranno in conflitto e l'eliminazione di una snapshot comporterà l'assenza di dati file su altre snapshot che condividono lo stesso pool.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Creazione di snapshot</title>
   <para>
    La funzione delle snapshot CephFS è abilitata per default sui nuovi file system. Per abilitarla sui file system esistenti, eseguire:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    In seguito all'abilitazione delle snapshot, tutte le directory in CephFS avranno una sottodirectory<filename>.snap</filename> speciale.
   </para>
   <note>
    <para>
     Si tratta di una sottodirectory <emphasis>virtuale</emphasis>. Non è riportata nell'elenco di directory della directory superiore, ma il nome <filename>.snap</filename> non può essere utilizzato come nome di file o directory. Per accedere alla directory <filename>.snap</filename>, è necessario farlo esplicitamente, ad esempio:
    </para>
<screen>
<prompt>tux &gt; </prompt>ls -la /<replaceable>CEPHFS_MOUNT</replaceable>/.snap/
 </screen>
   </note>
   <important>
    <title>limitazioni dei client kernel</title>
    <para>
     I client kernel CephFS hanno una limitazione: non sono in grado di gestire più di 400 snapshot in un file system. Il numero di snapshot deve essere sempre tenuto al di sotto di questo limite, a prescindere dal client utilizzato. Se si utilizzano client CephFS meno recenti, come SLE12-SP3, tenere presente che superare le 400 snapshot è dannoso per le operazioni perché si verificherà un arresto anomalo del client.
    </para>
   </important>
   <tip>
    <title>nome personalizzato della sottodirectory degli snapshot</title>
    <para>
     Configurare l'impostazione <option>client snapdir</option> per assegnare un nome diverso alla sottodirectory delle snapshot.
    </para>
   </tip>
   <para>
    Per creare una snapshot, creare una sottodirectory nella directory <filename>.snap</filename> con un nome personalizzato. Ad esempio, per creare una snapshot della directory <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, eseguire:
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Eliminazione degli snapshot</title>
   <para>
    Per eliminare una snapshot, rimuovere la relativa sottodirectory all'interno della directory <filename>.snap</filename>:
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
