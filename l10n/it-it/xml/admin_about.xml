<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha-storage-about">
 <title>SES e Ceph</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sì</dm:translation>
   <dm:release>SES 7</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SUSE Enterprise Storage è un sistema di storage distribuito progettato per scalabilità, affidabilità e prestazioni basato sulla tecnologia Ceph. È possibile eseguire un cluster Ceph su server generici in una rete comune come Ethernet. Il cluster può scalare inoltre fino a migliaia di server (più avanti denominati nodi) e nel campo dei petabyte. Rispetto ai sistemi convenzionali che dispongono di tabelle di allocazione per memorizzare e recuperare i dati, Ceph utilizza un algoritmo deterministico per allocare lo storage per i dati e non dispone di alcuna struttura informativa centralizzata. Ceph suppone che nei cluster di storage l'aggiunta o la rimozione dell'hardware sia la normalità, non l'eccezione. Il cluster Ceph automatizza i task di gestione come distribuzione e ridistribuzione dei dati, replica dei dati, rilevamento e ripristino degli errori. Ceph sfrutta capacità di autoregolazione e autogestione, determinando la riduzione delle spese amministrative e sforamenti di bilancio.
 </para>
 <para>
  Questo capitolo contiene una panoramica di alto livello di SUSE Enterprise Storage 7 e una breve descrizione dei componenti più importanti.
 </para>
 <sect1 xml:id="storage-intro-features">
  <title>Funzioni di Ceph</title>

  <para>
   L'ambiente Ceph presenta le seguenti caratteristiche:
  </para>

  <variablelist>
   <varlistentry>
    <term>Scalabilità</term>
    <listitem>
     <para>
      Ceph è in grado di scalare a migliaia di nodi e gestire lo storage nel campo dei petabyte.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Hardware</term>
    <listitem>
     <para>
      Per eseguire il cluster Ceph, non sono richiesti hardware speciali. Per i dettagli, vedere <xref linkend="storage-bp-hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Autogestione</term>
    <listitem>
     <para>
      Il cluster Ceph è in grado di autogestirsi. Quando si aggiungono o rimuovono nodi o in caso di guasto, il cluster ridistribuisce automaticamente i dati. È inoltre in grado di riconoscere i dischi sovraccarichi.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Nessun Single point of failure</term>
    <listitem>
     <para>
      Nessun nodo in un cluster memorizza da solo dati importanti. È possibile configurare il numero di ridondanze.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Software open source</term>
    <listitem>
     <para>
      Ceph è una soluzione software open source e indipendente da hardware o fornitori specifici.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-intro-core">
  <title>Componenti di base di Ceph</title>

  <para>
   Per utilizzare pienamente la potenza di Ceph, è necessario comprendere alcuni dei componenti e concetti di base. Questa sezione presenta alcune parti di Ceph a cui si fa spesso riferimento in altri capitoli.
  </para>

  <sect2 xml:id="storage-intro-core-rados">
   <title>RADOS</title>
   <para>
    Il componente base di Ceph è denominato <emphasis>RADOS</emphasis>
    <emphasis> (Reliable Autonomic Distributed Object Store)</emphasis>, responsabile per la gestione dei dati memorizzati nel cluster. I dati in Ceph vengono in genere memorizzati come oggetti. Ciascun oggetto consiste di un identificativo e dai dati.
   </para>
   <para>
    RADOS fornisce i seguenti metodi di accesso agli oggetti memorizzati che riguardano molti casi d'uso:
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway è un gateway HTTP REST per lo store di oggetti RADOS che consente l'accesso diretto agli oggetti memorizzati nel cluster Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RADOS Block Device</term>
     <listitem>
      <para>
       È possibile accedere ai RADOS Block Device (RBD) seguendo la stessa procedura degli altri dispositivi di blocco e utilizzarli ad esempio insieme con <systemitem class="library">libvirt</systemitem> per scopi di virtualizzazione.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       Il file system di Ceph è di tipo POSIX-compatibile.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem></term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> è una libreria utilizzabile con molti linguaggi di programmazione per creare un'applicazione in grado di interagire direttamente con il cluster di storage.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> è utilizzata da Object Gateway e RBD mentre CephFS si interfaccia direttamente con RADOS <xref linkend="storage-intro-core-rados-figure"/>.
   </para>
   <figure xml:id="storage-intro-core-rados-figure">
    <title>Interfacce con l'archivio dati Ceph</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage-intro-core-crush">
   <title>CRUSH</title>
   <para>
    Il core del cluster Ceph è l'algoritmo <emphasis>CRUSH</emphasis>. CRUSH è l'acronimo di <emphasis>Controlled Replication Under Scalable Hashing</emphasis>. CRUSH è una funzione che gestisce l'allocazione dello storage e richiede comparabilmente pochi parametri. Ciò significa che è necessaria solo una piccola quantità di dati per calcolare la posizione di storage di un oggetto. I parametri sono una mappa corrente del cluster comprendente lo stato, alcune regole di posizionamento definite dall'amministratore e il nome dell'oggetto che deve essere memorizzato o recuperato. Con questi dati, tutti i nodi nel cluster Ceph sono in grado di calcolare dove si trova un oggetto e le sue repliche. Ciò rende la scrittura o la lettura dei dati molto efficace. CRUSH tenta di distribuire in modo uniforme i dati tra tutti i nodi nel cluster.
   </para>
   <para>
    La <emphasis>mappa CRUSH</emphasis> contiene tutti i nodi di storage e le regole di posizionamento definite dall'amministratore per la memorizzazione degli oggetti nel cluster e definisce una struttura gerarchica che di solito corrisponde alla struttura fisica del cluster. Ad esempio, i dischi contenenti i dati sono negli host, gli host sono nei rack, i rack in righe e le righe nei data center. È possibile utilizzare questa struttura per definire i <emphasis>domini di guasto</emphasis>. Ceph quindi garantisce che le repliche vengano memorizzate su diverse diramazioni di uno specifico dominio di guasto.
   </para>
   <para>
    Se il dominio di guasto è configurato su un rack, le repliche degli oggetti sono distribuite su diversi rack. Ciò può ridurre le perdite provocate da un errore di commutazione in un rack. Se un'unità di distribuzione alimentazione alimenta una riga di rack, il dominio di guasto può essere configurato su una riga. In caso di errore dell'unità di distribuzione di alimentazione, i dati replicati sono sempre disponibili su altre righe.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-core-nodes">
   <title>Daemon e nodi Ceph</title>
   <para>
    In Ceph, i nodi sono server che lavorano per il cluster e possono eseguire diversi tipi di daemon. Si consiglia di eseguire solo un tipo di daemon in ogni nodo, tranne per i daemon Ceph Manager che possono essere collocati con i Ceph Monitor. Per ogni cluster sono necessari almeno i daemon Ceph Monitor, Ceph Manager e Ceph OSD:
   </para>
   <variablelist>
    <varlistentry>
     <term>Nodo admin</term>
     <listitem>
      <para>
       Il <emphasis>nodo admin</emphasis> è un nodo del cluster Ceph da cui si eseguono i comandi per la gestione del cluster. Il nodo admin è un punto centrale del cluster Ceph perché gestisce i nodi residui del cluster tramite interrogazione e istruendo i relativi servizi Salt Minion.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       I nodi <emphasis>Ceph Monitor</emphasis> (spesso abbreviato con <emphasis>MON</emphasis>) mantengono le informazioni sullo stato del cluster, una mappa di tutti i nodi e regole di distribuzione dati (vedere <xref linkend="storage-intro-core-crush"/>).
      </para>
      <para>
       Se si verificano errori o conflitti, i nodi Ceph Monitor nel cluster decidono a maggioranza quali dati sono corretti. Per formare una maggioranza qualificata, si consiglia un numero dispari di nodi Ceph Monitor e almeno tre.
      </para>
      <para>
       Se si utilizzano più siti, i nodi Ceph Monitor devono essere distribuiti su un numero dispari di siti. Il numero di nodi Ceph Monitor per sito deve essere tale che oltre il 50% dei nodi Ceph Monitor resti funzionale in caso di errore di un sito.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       Ceph Manager raccoglie le informazioni di stato dall'intero cluster. Il daemon Ceph Manager viene eseguito insieme con i daemon Ceph Monitor, fornisce ulteriore monitoraggio e si interfaccia con i sistemi di gestione e monitoraggio esterni. Include anche altri servizi. Ad esempio, l'interfaccia utente Web del Ceph Dashboard viene eseguita sullo stesso nodo del Ceph Manager.
      </para>
      <para>
       Per Ceph Manager non è necessaria una configurazione aggiuntiva; è sufficiente verificare che sia in esecuzione.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       <emphasis>Ceph OSD</emphasis> è un daemon che gestisce gli <emphasis>Object Storage Device</emphasis> che sono unità di storage logiche o fisiche (dischi rigidi o partizioni). Gli Object Storage Device possono essere dischi fisici/partizioni o volumi logici. Il daemon si occupa inoltre della replica dei dati e del ribilanciamento in caso di nodi aggiunti o rimossi.
      </para>
      <para>
       I daemon Ceph OSD comunicano con i daemon monitor e forniscono loro lo stato degli altri daemon OSD.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Per utilizzare CephFS, Object Gateway, NFS Ganesha o iSCSI Gateway, sono richiesti nodi aggiuntivi:
   </para>
   <variablelist>
    <varlistentry>
     <term>Metadata Server (MDS)</term>
     <listitem>
      <para>
       I metadati di CephFS vengono archiviati sul relativo pool RADOS (vedere la <xref linkend="storage-intro-structure-pool"/>). I Metadata Server agiscono come strato di memorizzazione nella cache smart per i metadati e serializzano l'accesso quando necessario. In questo modo, è possibile consentire l'accesso simultaneo da diversi client senza sincronizzazione esplicita.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway è un gateway HTTP REST per l'archivio dati RADOS. È compatibile con OpenStack Swift e Amazon S3 e dispone di propria gestione utente.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       NFS Ganesha fornisce un accesso NFS all'Object Gateway o a CephFS. Viene eseguito nell'utente invece che nello spazio kernel e interagisce direttamente con l'Object Gateway o CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>iSCSI Gateway</term>
     <listitem>
      <para>
       iSCSI è un protocollo di rete di storage che consente ai client di inviare comandi SCSI ai dispositivi di storage SCSI (destinazioni) su server remoti.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Gateway Samba</term>
     <listitem>
      <para>
       Il gateway Samba fornisce accesso Samba ai dati memorizzati su CephFS.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-intro-structure">
  <title>Struttura di memorizzazione di Ceph</title>

  <sect2 xml:id="storage-intro-structure-pool">
   <title>Pool</title>
   <para>
    Gli oggetti memorizzati in un cluster Ceph vengono posti nei <emphasis>pool</emphasis>. I pool rappresentano le partizioni logiche del cluster per l'esterno. Per ogni pool è possibile definire un set di regole, ad esempio quante repliche di ogni oggetto devono esistere. La configurazione standard dei pool è denominata <emphasis>pool replicato</emphasis>.
   </para>
   <para>
    I pool in genere contengono oggetti, ma è possibile configurarli anche per funzionare come RAID 5. In questa configurazione, gli oggetti vengono memorizzati in chunk insieme con i chunk di codifica aggiuntivi. I chunk di codifica contengono i dati ridondanti. Il numero di dati e chunk di codifica può essere definito dall'amministratore. In questa configurazione, i pool sono denominati <emphasis>pool con codice di cancellazione</emphasis> o <emphasis>pool EC</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-pg">
   <title>Gruppi di posizionamento</title>
   <para>
    I <emphasis>gruppi di posizionamento</emphasis> (PG) sono utilizzati per la distribuzione dei dati in un pool. Quando si crea un pool, viene impostato un determinato numero di gruppi di posizionamento. I gruppi di posizionamento sono utilizzati internamente per raggruppare gli oggetti e sono un fattore importante per le prestazioni di un cluster Ceph. Il PG di un oggetto è determinato dal nome dell'oggetto.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-example">
   <title>Esempio</title>
   <para>
    Questa sezione fornisce un esempio semplificato della gestione dati di Ceph (vedere <xref linkend="storage-intro-structure-example-figure"/>). Questo esempio non rappresenta una configurazione consigliata per un cluster Ceph. La configurazione hardware consiste di tre nodi di storage Ceph OSD (<literal>Host 1</literal>, <literal>Host 2</literal>, <literal>Host 3</literal>). Ogni nodo ha tre dischi rigidi utilizzati come OSD (da <literal>osd.1</literal> a <literal>osd.9</literal>). In questo esempio, vengono tralasciati i nodi Ceph Monitor.
   </para>
   <note>
    <title>differenza tra Ceph OSD e OSD</title>
    <para>
     Mentre <emphasis>Ceph OSD</emphasis> o <emphasis>daemon Ceph OSD</emphasis> si riferisce a un daemon eseguito su un nodo, il termine <emphasis>OSD</emphasis> si riferisce al disco logico con cui interagisce il daemon.
    </para>
   </note>
   <para>
    Il cluster ha due pool, <literal>Pool A</literal> e <literal>Pool B</literal>. Mentre il Pool A replica gli oggetti solo due volte, la resilienza per Pool B è più importante e ha tre repliche per ogni oggetto.
   </para>
   <para>
    Quando un'applicazione mette un oggetto in un pool, ad esempio tramite la API REST, un Gruppo di posizionamento (da <literal>PG1</literal> a <literal>PG4</literal>) viene selezionato in base a pool e nome oggetto. L'algoritmo CRUSH calcola quindi su quali OSD viene memorizzato l'oggetto, in base al Gruppo di posizionamento contenente l'oggetto.
   </para>
   <para>
    In questo esempio, il dominio di guasto è impostato sull'host. Ciò garantisce che le repliche degli oggetti siano memorizzate su diversi host. In base al livello di replica impostato per un pool, l'oggetto viene memorizzato su due o tre OSD utilizzati dal Gruppo di posizionamento.
   </para>
   <para>
    Un'applicazione che scrive un oggetto interagisce solo con un Ceph OSD, il Ceph OSD primario. Il Ceph OSD primario si occupa della replica e conferma il completamento del processo di scrittura dopo che tutti gli altri OSD hanno memorizzato l'oggetto.
   </para>
   <para>
    In caso di errore di <literal>osd.5</literal>, tutti gli oggetti in <literal>PG1</literal> sono ancora disponibili su <literal>osd.1</literal>. Non appena il cluster riconosce l'errore di un OSD, un altro OSD lo sostituisce. In questo esempio <literal>osd.4</literal> viene utilizzato come sostituzione per <literal>osd.5</literal>. Gli oggetti memorizzati su <literal>osd.1</literal> vengono quindi replicati su <literal>osd.4</literal> per ripristinare il livello di replica.
   </para>
   <figure xml:id="storage-intro-structure-example-figure">
    <title>Esempio di Ceph su piccola scala</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Se si aggiunge al cluster un nuovo nodo con nuovi OSD, la mappa del cluster cambia. La funzione CRUSH restituisce quindi diverse ubicazioni per gli oggetti. Gli oggetti che ricevono nuove ubicazioni vengono riposizionati. Questo processo determina un uso bilanciato di tutti gli OSD.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about-bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore è un nuovo back-end di storage di default per Ceph di SES 5. Offre migliori prestazioni di FileStore, check-sum dei dati completo e compressione integrata.
  </para>

  <para>
   BlueStore gestisce uno, due o tre dispositivi di storage. Nel caso più semplice, BlueStore consuma un singolo dispositivo di storage primario. Il dispositivo di storage è di solito partizionato in due parti:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Una piccola partizione denominata BlueFS che implementa funzionalità di tipo file system richieste da RocksDB.
    </para>
   </listitem>
   <listitem>
    <para>
     Il resto del dispositivo è in genere una grande partizione che occupa tutto lo spazio residuo del dispositivo. È gestito direttamente da BlueStore e contiene tutti i dati effettivi. Tale dispositivo primario è in genere identificato da un collegamento simbolico di blocco nella directory dei dati.
    </para>
   </listitem>
  </orderedlist>

  <para>
   È inoltre possibile distribuire BlueStore su due dispositivi aggiuntivi:
  </para>

  <para>
   Un <emphasis>dispositivo WAL</emphasis> può essere utilizzato per il giornale di registrazione interno di BlueStore o il log di scrittura. È identificato dal collegamento simbolico <literal>block.wal</literal> nella directory dei dati e serve solo per utilizzare un dispositivo WAL separato se il dispositivo è più veloce del dispositivo primario o del dispositivo DB, ad esempio quando:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     il dispositivo WAL è un NVMe e il dispositivo DB è una SSD e il dispositivo dati è un'unità SSD o HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Entrambi i dispositivi WAL e DB sono SSD separate e il dispositivo dati è un'unità SSD o HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   È possibile utilizzare un <emphasis>dispositivo DB</emphasis> per memorizzare i metadati interni di BlueStore. BlueStore (o piuttosto il RocksDB integrato) trasferisce tutti i metadati possibili sul dispositivo DB per migliorare le prestazioni. Di nuovo, è utile solo per il provisioning di un dispositivo DB condiviso se è più veloce del dispositivo primario.
  </para>

  <tip>
   <title>pianificazione della dimensione del DB</title>
   <para>
    Pianificarla attentamente per assicurarsi che il dispositivo DB sia delle dimensioni sufficienti. Se il dispositivo DB è pieno, i metadati si riverseranno sul dispositivo primario che degraderà notevolmente le prestazioni dell'OSD.
   </para>
   <para>
    È possibile verificare se una partizione WAL/DB si sta riempiendo e trasferendo i dati con il comando <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command>. Il valore <option>slow_used_bytes</option> mostra la quantità di dati in uscita:
   </para>
<screen>
<prompt>cephuser@adm &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage-moreinfo">
  <title>Informazioni aggiuntive</title>

  <itemizedlist>
   <listitem>
    <para>
     Ceph è un progetto di community che dispone di una propria documentazione completa online. Per argomenti non trovati nel presente manuale, consultare <link xlink:href="https://docs.ceph.com/en/octopus/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     La pubblicazione originale <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> di <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> fornisce spunti utili nelle opere interne di Ceph. Se ne consiglia la lettura soprattutto per la distribuzione di cluster in grande scala. La pubblicazione è disponibile al seguente collegamento <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Enterprise Storage può essere utilizzato con le distribuzioni OpenStack non SUSE. I client Ceph devono trovarsi a un livello compatibile con SUSE Enterprise Storage.
    </para>
    <note>
     <para>
      SUSE supporta il componente server della distribuzione Ceph, mentre il client è supportato dal fornitore della distribuzione OpenStack.
     </para>
    </note>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
