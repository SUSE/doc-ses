<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_iscsi.xml" version="5.0" xml:id="cha.ceph.as.iscsi">

 <title>Installation de la passerelle iSCSI</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>modification</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>oui</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  iSCSI est un protocole SAN (sous-réseau de stockage) qui permet aux clients (appelés <emphasis>initiateurs</emphasis>) d'envoyer des commandes SCSI aux périphériques de stockage SCSI (<emphasis>cibles</emphasis>) sur des serveurs distants. SUSE Enterprise Storage comprend une interface qui ouvre la gestion du stockage Ceph aux clients hétérogènes, tels que Microsoft Windows* et VMware* vSphere, via le protocole iSCSI. L'accès iSCSI multipath assure la disponibilité et l'évolutivité à ces clients, et le protocole iSCSI standard fournit également une couche supplémentaire d'isolation de sécurité entre les clients et la grappe SUSE Enterprise Storage. La fonction de configuration est nommée <systemitem>lrbd</systemitem>. À l'aide de la fonction <systemitem>lrbd</systemitem>, les administrateurs du stockage Ceph peuvent définir des volumes à allocation dynamique, hautement disponibles et répliqués, prenant en charge les instantanés en lecture seule, les clones en lecture-écriture et le redimensionnement automatique avec le périphérique de traitement par bloc RADOS (RADOS Block Device, RBD) de Ceph. Les administrateurs peuvent ensuite exporter des volumes via un seul hôte de passerelle <systemitem>lrbd</systemitem> ou via plusieurs hôtes de passerelle prenant en charge le basculement multipath. Les hôtes Linux, Microsoft Windows et VMware peuvent se connecter aux volumes à l'aide du protocole iSCSI, ce qui les rend disponibles comme tout autre périphérique de traitement par blocs SCSI. Cela signifie que les clients de SUSE Enterprise Storage peuvent exécuter efficacement un sous-système d'infrastructure de stockage de blocs complet sur Ceph, qui offre toutes les fonctionnalités et avantages d'un SAN conventionnel, ce qui permet une croissance future.
 </para>
 <para>
  Ce chapitre présente des informations détaillées permettant de configurer une infrastructure de grappe Ceph avec une passerelle iSCSI afin que les hôtes clients puissent utiliser des données stockées à distance en tant que périphériques de stockage locaux à l'aide du protocole iSCSI.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>Stockage de blocs iSCSI</title>

  <para>
   iSCSI est une implémentation de la commande SCSI (Small Computer System Interface) définie en utilisant le protocole Internet (IP), spécifié dans la norme RFC 3720. iSCSI est implémenté comme un service dans lequel un client (initiateur) se connecte à un serveur (cible) via une session sur le port TCP 3260. L'adresse IP et le port d'une cible iSCSI sont appelés « portail iSCSI », où une cible peut être exposée via un ou plusieurs portails. La combinaison d'une cible et d'un ou de plusieurs portails est appelée le groupe de portails cible (TPG).
  </para>

  <para>
   Le protocole de couche de liaison de données sous-jacent pour iSCSI est généralement Ethernet. Plus spécifiquement, les infrastructures iSCSI modernes utilisent des réseaux 10 Gigabit Ethernet ou plus rapides pour un débit optimal. Une connectivité 10 Gigabit Ethernet entre la passerelle iSCSI et la grappe Ceph de l'interface dorsale est fortement recommandée.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>Cible iSCSI du kernel Linux</title>
   <para>
    La cible iSCSI du kernel Linux s'appelait à l'origine LIO pour linux-iscsi.org, le domaine et le site Web d'origine du projet. Pendant un certain temps, pas moins de quatre implémentations de cibles iSCSI concurrentes étaient disponibles pour la plate-forme Linux, mais LIO a finalement prévalu en tant que cible de référence iSCSI unique. Le code du kernel principal de LIO utilise le nom simple, mais quelque peu ambigu « target », en faisant la distinction entre « target core » et une variété de modules cibles d'interface client et d'interface dorsale.
   </para>
   <para>
    Le module d'interface client le plus couramment utilisé est sans doute iSCSI. Toutefois, LIO prend également en charge Fibre Channel (FC), Fibre Channel sur Ethernet (FCoE) et plusieurs autres protocoles d'interface client. Pour l'instant, seul le protocole iSCSI est pris en charge par SUSE Enterprise Storage.
   </para>
   <para>
    Le module d'interface dorsale cible le plus fréquemment utilisé est celui qui est capable de réexporter simplement n'importe quel périphérique de traitement par blocs disponible sur l'hôte cible. Ce module est nommé iblock. Cependant, LIO dispose également d'un module d'interface dorsale spécifique à RBD prenant en charge l'accès E/S multipath parallélisé aux images RBD.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>Initiateurs iSCSI</title>
   <para>
    Cette section présente des informations succinctes sur les initiateurs iSCSI utilisés sur les plates-formes Linux, Microsoft Windows et VMware.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     L'initiateur standard de la plate-forme Linux est <systemitem>open-iscsi</systemitem>. <systemitem>open-iscsi</systemitem> lance le daemon <systemitem>iscsid</systemitem>, que l'utilisateur peut ensuite utiliser pour découvrir des cibles iSCSI sur n'importe quel portail donné, se connecter à des cibles et assigner des volumes iSCSI. <systemitem>iscsid</systemitem> communique avec la mi couche SCSI pour créer des périphériques de bloc du kernel que ce dernier peut ensuite traiter comme n'importe quel autre périphérique de bloc SCSI du système. L'initiateur <systemitem>open-iscsi</systemitem> peut être déployé en association avec l'outil Device Mapper Multipath (<systemitem>dm-multipath</systemitem>) capable de fournir un périphérique de bloc iSCSI hautement disponible.
    </para>
   </sect3>
   <sect3>
    <title>Microsoft Windows et Hyper-V</title>
    <para>
     L'initiateur iSCSI par défaut pour le système d'exploitation Microsoft Windows est l'initiateur iSCSI de Microsoft. Le service iSCSI peut être configuré via une interface graphique (GUI) et prend en charge les E/S multipath pour une haute disponibilité.
    </para>
   </sect3>
   <sect3>
    <title>VMware</title>
    <para>
     L'initiateur iSCSI par défaut pour VMware vSphere et ESX est l'initiateur iSCSI du logiciel VMware ESX, <systemitem>vmkiscsi</systemitem>. Lorsque qu'il est activé, il peut être configuré à partir du client vSphere ou à l'aide de la commande <command>vmkiscsi-tool</command>. Vous pouvez ensuite formater les volumes de stockage connectés via l'adaptateur de disque vSphere iSCSI avec VMFS et les utiliser comme tout autre périphérique de stockage VM. L'initiateur VMware prend également en charge les E/S multipath pour une haute disponibilité.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrbd">
  <title>Informations générales à propos de la fonction lrbd</title>

  <para>
   La fonction <systemitem>lrbd</systemitem> associe les avantages des périphériques de bloc RADOS à la polyvalence omniprésente du protocole iSCSI. En utilisant <systemitem>lrbd</systemitem> sur un hôte de la cible iSCSI (connu sous le nom de passerelle <systemitem>lrbd</systemitem>), toute application qui a besoin d'utiliser le stockage de blocs peut bénéficier de Ceph, même si elle n'utilise pas un protocole de client Ceph. Au lieu de cela, les utilisateurs peuvent utiliser le protocole iSCSI ou tout autre protocole d'interface client cible pour se connecter à une cible LIO, ce qui traduit toutes les E/S cibles en opérations de stockage RBD.
  </para>

  <figure>
   <title>grappe Ceph avec une passerelle iSCSI unique</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   <systemitem>lrbd</systemitem> est intrinsèquement hautement disponible et prend en charge les opérations multipath. Ainsi, les hôtes initiateurs en aval peuvent utiliser plusieurs passerelles iSCSI pour la haute disponibilité et l'évolutivité. Lors de la communication avec une configuration iSCSI avec plus d'une passerelle, les initiateurs peuvent équilibrer la charge des requêtes iSCSI sur plusieurs passerelles. En cas d'échec d'une passerelle, d'inaccessibilité temporaire ou de désactivation pour maintenance, les E/S continuent de manière transparente via une autre passerelle.
  </para>

  <figure>
   <title>grappe Ceph avec plusieurs passerelles iSCSI</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Considérations sur le déploiement</title>

  <para>
   Une configuration minimale de SUSE Enterprise Storage avec <systemitem>lrbd</systemitem> comprend les composants suivants :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Une grappe de stockage Ceph. La grappe Ceph consiste en un minimum de quatre serveurs physiques hébergeant au moins huit daemons de stockage des objets (Object Storage Daemon, OSD) chacun. Dans une telle configuration, les trois noeuds OSD doublent également en tant qu'hôte Monitor (MON).
    </para>
   </listitem>
   <listitem>
    <para>
     Un serveur cible iSCSI exécutant la cible iSCSI LIO, configurée via <systemitem>lrbd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     Un hôte initiateur iSCSI, exécutant <systemitem>open-iscsi</systemitem> (Linux), l'initiateur Microsoft iSCSI (Microsoft Windows) ou toute autre implémentation d'initiateur iSCSI compatible.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   La configuration recommandée en production pour SUSE Enterprise Storage avec <systemitem>lrbd</systemitem> est constituée des éléments suivants :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Une grappe de stockage Ceph. Une grappe Ceph de production est constituée de n'importe quel nombre de noeuds OSD (généralement plus de 10), chacun exécutant généralement 10 à 12 OSD, avec pas moins de trois hôtes MON dédiés.
    </para>
   </listitem>
   <listitem>
    <para>
     Plusieurs serveurs cible iSCSI exécutant la cible iSCSI LIO, configurés via <systemitem>lrbd</systemitem>. Pour une reprise après échec et un équilibrage de charge iSCSI, ces serveurs doivent exécuter un kernel prenant en charge le module <systemitem>target_core_rbd</systemitem>. Des paquetages de mise à jour sont disponibles à partir du canal de maintenance SUSE Linux Enterprise Server.
    </para>
   </listitem>
   <listitem>
    <para>
     N'importe quel nombre d'hôtes initiateurs iSCSI exécutant <systemitem>open-iscsi</systemitem> (Linux), l'initiateur Microsoft iSCSI (Microsoft Windows) ou toute autre implémentation d'initiateur iSCSI compatible.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation et configuration</title>

  <para>
   Cette section décrit les étapes d'installation et de configuration d'une passerelle iSCSI en plus de SUSE Enterprise Storage.
  </para>

  <sect2>
   <title>Déploiement de la passerelle iSCSI sur une grappe Ceph</title>
   <para>
    Vous pouvez déployer la passerelle iSCSI au cours du processus de déploiement de la grappe Ceph ou l'ajouter à une grappe existante à l'aide de DeepSea.
   </para>
   <para>
    Pour inclure la passerelle iSCSI au cours du processus de déploiement de la grappe, reportez-vous à la <xref linkend="policy.role.assignment"/>.
   </para>
   <para>
    Pour ajouter la passerelle iSCSI à une grappe existante, reportez-vous au <xref linkend="salt.adding.services"/>.
   </para>
  </sect2>

  <sect2>
   <title>Création d'images RBD</title>
   <para>
    Les images RBD sont créées dans la zone de stockage Ceph et ensuite exportées vers iSCSI. Il est recommandé d'utiliser une réserve RADOS dédiée à cette fin. Vous pouvez créer un volume à partir de n'importe quel hôte pouvant se connecter à votre grappe de stockage à l'aide de l'utilitaire de ligne de commande Ceph <command>rbd</command>. Pour cela, le client doit avoir au moins un fichier de configuration ceph.conf minimal et les informations d'identification d'authentification CephX appropriées.
   </para>
   <para>
    Pour créer un volume pour l'exportation ultérieure via iSCSI, utilisez la commande <command>rbd create</command> en spécifiant la taille du volume en mégaoctets. Par exemple, pour créer un volume de 100 Go nommé <literal>testvol</literal> dans la réserve nommée <literal>iscsi</literal>, exécutez :
   </para>
<screen><prompt>root # </prompt>rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    La commande ci-dessus crée un volume RBD au format par défaut 2.
   </para>
   <note>
    <para>
     À compter de SUSE Enterprise Storage 3, le format de volume par défaut est 2, et le format 1 est obsolète. Toutefois, vous pouvez toujours créer des volumes au format 1 obsolète avec l'option <option>--image-format 1</option>.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.export">
   <title>Exportation d'images RBD via iSCSI</title>
   <para>
    Pour exporter des images RBD via iSCSI, utilisez l'utilitaire <systemitem>lrbd</systemitem>. L'utilitaire <systemitem>lrbd</systemitem> permet de créer, de vérifier et de modifier la configuration de la cible iSCSI, qui utilise un format JSON.
   </para>
   <tip>
    <title>importation des modifications dans openATTIC</title>
    <para>
     Les modifications apportées à la configuration de la passerelle iSCSI à l'aide de la commande <command>lrbd</command> ne sont pas visibles dans DeepSea et openATTIC. Pour importer vos modifications manuelles, vous devez exporter la configuration de la passerelle iSCSI vers un fichier :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o /tmp/lrbd.conf
</screen>
    <para>
     Ensuite, copiez-le sur Salt Master pour que DeepSea et openATTIC puissent le voir :
    </para>
<screen>
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf ses5master:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
    <para>
     Enfin, modifiez <filename>/srv/pillar/ceph/stack/global.yml</filename> et définissez :
    </para>
<screen>
igw_config: default-ui
</screen>
   </tip>
   <para>
    Pour modifier la configuration, utilisez <command>lrbd -e</command> ou <command>lrbd --edit</command>. Cette commande appelle l'éditeur par défaut, comme défini par la variable d'environnement <literal>EDITOR</literal>. Vous pouvez remplacer ce comportement en définissant l'option <option>-E</option> en plus de l'option <option>-e</option>.
   </para>
   <para>
    Vous trouverez ci-dessous un exemple de configuration pour
   </para>
   <itemizedlist>
    <listitem>
     <para>
      deux hôtes de passerelle iSCSI nommés <literal>iscsi1.example.com</literal> et <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      définissant une cible iSCSI unique avec un nom complet iSCSI (IQN) <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      avec une seule unité logique (LU) iSCSI,
     </para>
    </listitem>
    <listitem>
     <para>
      soutenue par une image RBD nommée <literal>testvol</literal> dans la réserve RADOS <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      et exportant la cible par le biais de deux portails nommés « east » et « west » :
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "authentication": "none"
        }
    ],
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
            "hosts": [
                {
                    "host": "iscsi1.example.com",
                    "portal": "east"
                },
                {
                    "host": "iscsi2.example.com",
                    "portal": "west"
                }
            ]
        }
    ],
    "portals": [
        {
            "name": "east",
            "addresses": [
                "192.168.124.104"
            ]
        },
        {
            "name": "west",
            "addresses": [
                "192.168.124.105"
            ]
        }
    ],
    "pools": [
        {
            "pool": "rbd",
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol",
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</screen>
   <para>
    Notez que chaque fois que vous faites référence à un nom d'hôte dans la configuration, ce nom d'hôte doit correspondre à la sortie de la commande <command>uname -n</command> de la passerelle iSCSI.
   </para>
   <para>
    Le JSON édité est stocké dans les attributs étendus (xattrs) d'un seul objet RADOS par réserve. Cet objet est disponible pour les hôtes de la passerelle pour lesquels le JSON est édité, ainsi que pour tous les hôtes de la passerelle connectés à la même grappe Ceph. Aucune information de configuration n'est stockée localement sur la passerelle <systemitem>lrbd</systemitem>.
   </para>
   <para>
    Pour activer la configuration, stockez-la dans la grappe Ceph et effectuez l'une des opérations suivantes (en tant que <systemitem class="username">root</systemitem>) :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Exécutez la commande <command>lrbd</command> (sans les options supplémentaires) à partir de la ligne de commande,
     </para>
    </listitem>
   </itemizedlist>
   <para>
    ou
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Redémarrez le service <systemitem>lrbd</systemitem> avec <command>service lrbd restart</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Le « service » <systemitem>lrbd</systemitem> n'exécute aucun daemon en arrière-plan. Au lieu de cela, il appelle simplement la commande <command>lrbd</command>. Ce type de service est connu comme un service de type « à utilisation unique ».
   </para>
   <para>
    Vous devez également activer <systemitem>lrbd</systemitem> pour la configuration automatique au démarrage du système. Pour ce faire, exécutez la commande <command>systemctl enable lrbd</command>.
   </para>
   <para>
    La configuration ci-dessus reflète une configuration simple, avec une seule passerelle. La configuration <systemitem>lrbd</systemitem> peut être beaucoup plus complexe et puissante. Le paquetage RPM <systemitem>lrbd</systemitem> contient un grand nombre d'exemples de configuration auxquels vous pouvez vous référer en consultant le contenu du répertoire <filename>/usr/share/doc/packages/lrbd/samples</filename> après l'installation. Les exemples sont également disponibles sur le site <link xlink:href="https://github.com/SUSE/lrbd/tree/master/samples"/>.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.optional">
   <title>Paramètres facultatifs</title>
   <para>
    Les paramètres suivants peuvent être utiles pour certains environnements. Pour les images, il existe les attributs <option>uuid</option>, <option>lun</option>, <option>retries</option>, <option>sleep</option> et <option>retry_errors</option>. Les deux premiers, <option>uuid</option> et <option>lun</option>, permettent de coder en dur « uuid » ou « lun » pour une image spécifique. Vous pouvez spécifier l'un d'entre eux pour une image. Les paramètres <option>retries</option>, <option>sleep</option> et <option>retry_errors</option> affectent les tentatives d'assignation d'une image rbd.
   </para>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "uuid": "12345678-abcd-9012-efab-345678901234",
                        "lun": "2",
                        "retries": "3",
                        "sleep": "4",
                        "retry_errors": [ 95 ],
                        [...]
                    }
                ]
            }
        ]
    }
]</screen>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.advanced">
   <title>Paramètres avancés</title>
   <para>
    <systemitem>lrbd</systemitem> peut être configuré avec des paramètres avancés qui sont ensuite transmis à la cible des E/S LIO. Les paramètres sont divisés en composants iSCSI et de zone de stockage, qui peuvent ensuite être spécifiés dans les sections « targets » et « tpg », respectivement, de la configuration <systemitem>lrbd</systemitem>.
   </para>
   <warning>
    <para>
     La modification de ces paramètres par rapport au paramètre par défaut n'est pas recommandée.
    </para>
   </warning>
<screen>"targets": [
    {
        [...]
        "tpg_default_cmdsn_depth": "64",
        "tpg_default_erl": "0",
        "tpg_login_timeout": "10",
        "tpg_netif_timeout": "2",
        "tpg_prod_mode_write_protect": "0",
    }
]</screen>
   <para>
    Vous trouverez ci-dessous une description des options :
   </para>
   <variablelist>
    <varlistentry>
     <term>tpg_default_cmdsn_depth</term>
     <listitem>
      <para>
       Profondeur de CmdSN (numéro de séquence de commande) par défaut. Limite le nombre de requêtes qu'un initiateur iSCSI peut avoir en attente à tout moment.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_default_erl</term>
     <listitem>
      <para>
       Niveau de la récupération d'erreur par défaut.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_login_timeout</term>
     <listitem>
      <para>
       Valeur de timeout de connexion en secondes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_netif_timeout</term>
     <listitem>
      <para>
       Timeout d'échec de la carte d'interface réseau en secondes.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>tpg_prod_mode_write_protect</term>
     <listitem>
      <para>
       Si définie sur 1, empêche les écritures sur les LUN (numéros d'unité logique).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
<screen>"pools": [
    {
        "pool": "rbd",
        "gateways": [
        {
        "host": "igw1",
        "tpg": [
                    {
                        "image": "archive",
                        "backstore_block_size": "512",
                        "backstore_emulate_3pc": "1",
                        "backstore_emulate_caw": "1",
                        "backstore_emulate_dpo": "0",
                        "backstore_emulate_fua_read": "0",
                        "backstore_emulate_fua_write": "1",
                        "backstore_emulate_model_alias": "0",
                        "backstore_emulate_rest_reord": "0",
                        "backstore_emulate_tas": "1",
                        "backstore_emulate_tpu": "0",
                        "backstore_emulate_tpws": "0",
                        "backstore_emulate_ua_intlck_ctrl": "0",
                        "backstore_emulate_write_cache": "0",
                        "backstore_enforce_pr_isids": "1",
                        "backstore_fabric_max_sectors": "8192",
                        "backstore_hw_block_size": "512",
                        "backstore_hw_max_sectors": "8192",
                        "backstore_hw_pi_prot_type": "0",
                        "backstore_hw_queue_depth": "128",
                        "backstore_is_nonrot": "1",
                        "backstore_max_unmap_block_desc_count": "1",
                        "backstore_max_unmap_lba_count": "8192",
                        "backstore_max_write_same_len": "65535",
                        "backstore_optimal_sectors": "8192",
                        "backstore_pi_prot_format": "0",
                        "backstore_pi_prot_type": "0",
                        "backstore_queue_depth": "128",
                        "backstore_unmap_granularity": "8192",
                        "backstore_unmap_granularity_alignment": "4194304"
                    }
                ]
            }
        ]
    }
]</screen>
   <para>
    Vous trouverez ci-dessous une description des options :
   </para>
   <variablelist>
    <varlistentry>
     <term>backstore_block_size</term>
     <listitem>
      <para>
       Taille du bloc du périphérique sous-jacent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_3pc</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Third Party Copy (Copie tierce).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_caw</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Compare and Write (Comparer et écrire).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_dpo</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Disable Page Out (Désactiver la sortie de page).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_read</term>
     <listitem>
      <para>
       Si définie sur 1, active la lecture pour l'option Force Unit Access (Forcer l'accès aux unités).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_fua_write</term>
     <listitem>
      <para>
       Si définie sur 1, active l'écriture pour l'option Force Unit Access (Forcer l'accès aux unités).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_model_alias</term>
     <listitem>
      <para>
       Si définie sur 1, utilise le nom du périphérique d'interface dorsale pour l'alias du modèle.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_rest_reord</term>
     <listitem>
      <para>
       Si définie sur 0, le modificateur d'algorithme de file d'attente comporte une réorganisation restreinte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tas</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Task Aborted Status (État Tâche abandonnée).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpu</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Thin Provisioning Unmap (Annulation d'assignation du provisioning léger).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_tpws</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Thin Provisioning Write Same (Écriture identique provisioning léger).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_ua_intlck_ctrl</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Unit Attention Interlock (Interverrouillage attention unité).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_emulate_write_cache</term>
     <listitem>
      <para>
       Si définie sur 1, active l'option Write Cache Enable (Écriture sur le cache activée).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_enforce_pr_isids</term>
     <listitem>
      <para>
       Si définie sur 1, applique les ISID de réservation persistante.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_fabric_max_sectors</term>
     <listitem>
      <para>
       Nombre maximal de secteurs que la structure peut transférer à la fois.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_block_size</term>
     <listitem>
      <para>
       Taille des blocs matériels en octets.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_max_sectors</term>
     <listitem>
      <para>
       Nombre maximal de secteurs que le matériel peut transférer à la fois.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_pi_prot_type</term>
     <listitem>
      <para>
       Si différent de zéro, une protection DIF est activée sur le matériel sous-jacent.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_hw_queue_depth</term>
     <listitem>
      <para>
       Profondeur de la file d'attente du matériel.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_is_nonrot</term>
     <listitem>
      <para>
       Si définie sur 1, le backstore est un périphérique non rotationnel.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_block_desc_count</term>
     <listitem>
      <para>
       Nombre maximal de descripteurs de bloc pour UNMAP (Annuler l'assignation).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_unmap_lba_count:</term>
     <listitem>
      <para>
       Nombre maximal de LBA pour UNMAP (Annuler l'assignation).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_max_write_same_len</term>
     <listitem>
      <para>
       Longueur maximale pour WRITE_SAME (Écriture identique).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_optimal_sectors</term>
     <listitem>
      <para>
       Taille de la requête optimale dans les secteurs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_format</term>
     <listitem>
      <para>
       Format de protection DIF.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_pi_prot_type</term>
     <listitem>
      <para>
       Type de protection DIF.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_queue_depth</term>
     <listitem>
      <para>
       Profondeur de la file d'attente.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity</term>
     <listitem>
      <para>
       Granularité UNMAP (Annuler l'assignation).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>backstore_unmap_granularity_alignment</term>
     <listitem>
      <para>
       Alignement de la granularité UNMAP (Annuler l'assignation).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Pour les cibles, les attributs <option>tpg</option> permettent d'ajuster les paramètres du kernel. À utiliser avec prudence.
   </para>
<screen>"targets": [
{
    "host": "igw1",
    "target": "iqn.2003-01.org.linux-iscsi.generic.x86:sn.abcdefghijk",
    "tpg_default_cmdsn_depth": "64",
    "tpg_default_erl": "0",
    "tpg_login_timeout": "10",
    "tpg_netif_timeout": "2",
    "tpg_prod_mode_write_protect": "0",
    "tpg_t10_pi": "0"
}</screen>
   <tip>
    <para>
     Si un site a besoin d'unités logiques (LUN) assignées de manière statique, assignez des numéros à chaque LUN.
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="iscsi.tcmu">
  <title>Exportation d'images de périphérique de bloc RADOS à l'aide de <systemitem>tcmu-runner</systemitem></title>

  <para>
   Depuis la version 5, SUSE Enterprise Storage fournit une interface dorsale RBD d'espace utilisateur pour <systemitem>tcmu-runner</systemitem> (reportez-vous à <command>man 8 tcmu-runner</command> pour plus de détails).
  </para>

  <warning>
   <title>Aperçu de la technologie</title>
   <para>
    Les déploiements de passerelles iSCSI basés sur <systemitem>tcmu-runner</systemitem> représentent actuellement un aperçu de la technologie. Reportez-vous au <xref linkend="cha.ceph.as.iscsi"/> pour obtenir des instructions sur le déploiement de passerelles iSCSI basé sur un kernel avec <systemitem>lrbd</systemitem>.
   </para>
  </warning>

  <para>
   Contrairement aux déploiements de passerelles iSCSI <systemitem>lrbd</systemitem> basés sur le kernel, les passerelles iSCSI basées sur <systemitem>tcmu-runner</systemitem> ne prennent pas en charge les E/S multipath ou les réservations persistantes SCSI.
  </para>

  <para>
   Comme DeepSea et openATTIC ne prennent actuellement pas en charge les déploiements <systemitem>tcmu-runner</systemitem>, vous devez gérer l'installation, le déploiement et la surveillance manuellement.
  </para>

  <sect2 xml:id="iscsi.tcmu.install">
   <title>Installation</title>
   <para>
    Sur le noeud de votre passerelle iSCSI, installez le paquetage <systemitem>tcmu-runner-handler-rbd</systemitem> à partir du média de SUSE Enterprise Storage 5, conjointement avec les dépendances de paquetages <systemitem>libtcmu1</systemitem> et <systemitem>tcmu-runner</systemitem>. Installez le paquetage <systemitem>targetcli-fb</systemitem> à des fins de configuration. Notez que le paquetage <systemitem>targetcli-fb</systemitem> n'est pas compatible avec la version « non-fb » du paquetage <systemitem>targetcli</systemitem>.
   </para>
   <para>
    Confirmez que le service <systemitem>tcmu-runner</systemitem> <systemitem class="daemon">systemd</systemitem> est en cours d'exécution :
   </para>
<screen><prompt>root # </prompt>systemctl enable tcmu-runner
tcmu-gw:~ # systemctl status tcmu-runner
● tcmu-runner.service - LIO Userspace-passthrough daemon
  Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; static; vendor
  preset: disabled)
    Active: active (running) since ...</screen>
  </sect2>

  <sect2 xml:id="iscsi.tcmu.depl">
   <title>Configuration et déploiement</title>
   <para>
    Créez une image de périphérique de bloc RADOS sur votre grappe Ceph existante. Dans l'exemple suivant, nous allons utiliser une image 10G appelée « tcmu-lu » située dans la réserve « rbd ».
   </para>
   <para>
    Après la création de l'image de périphérique de bloc RADOS, exécutez <command>targetcli</command> et assurez-vous que le gestionnaire RBD tcmu-runner (plug-in) est disponible :
   </para>
<screen><prompt>root # </prompt>targetcli
targetcli shell version 2.1.fb46
Copyright 2011-2013 by Datera, Inc and others.
For help on commands, type 'help'.

/&gt; ls
o- / ................................... [...]
  o- backstores ........................ [...]
...
  | o- user:rbd ......... [Storage Objects: 0]</screen>
   <para>
    Créez une entrée de configuration backstore pour l'image RBD :
   </para>
<screen>/&gt; cd backstores/user:rbd
/backstores/user:rbd&gt; create tcmu-lu 10G /rbd/tcmu-lu
Created user-backed storage object tcmu-lu size 10737418240.</screen>
   <para>
    Créez une entrée de configuration du transport iSCSI. Dans l'exemple suivant, le nom complet (IQN) cible « iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a » est automatiquement généré par <command>targetcli</command> pour une utilisation comme un identificateur unique de la cible iSCSI :
   </para>
<screen>/backstores/user:rbd&gt; cd /iscsi
/iscsi&gt; create
Created target iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a.
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.</screen>
   <para>
    Créez une entrée ACL pour les initiateurs iSCSI que vous souhaitez connecter à la cible. Dans l'exemple suivant, un IQN d'initiateur « iqn.1998-01.com.vmware:esxi-872c4888 » est utilisé :
   </para>
<screen>/iscsi&gt; cd
iqn.2003-01.org.linux-iscsi.tcmu-gw.x8664:sn.cb3d2a3a/tpg1/acls/
/iscsi/iqn.20...a3a/tpg1/acls&gt; create iqn.1998-01.com.vmware:esxi-872c4888</screen>
   <para>
    Enfin, liez la configuration du backstore RBD précédemment créée à la cible iSCSI :
   </para>
<screen>/iscsi/iqn.20...a3a/tpg1/acls&gt; cd ../luns
/iscsi/iqn.20...a3a/tpg1/luns&gt; create /backstores/user:rbd/tcmu-lu
Created LUN 0.
Created LUN 0-&gt;0 mapping in node ACL iqn.1998-01.com.vmware:esxi-872c4888</screen>
   <para>
    Quittez le Shell pour enregistrer la configuration existante :
   </para>
<screen>/iscsi/iqn.20...a3a/tpg1/luns&gt; exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup.
Configuration saved to /etc/target/saveconfig.json</screen>
  </sect2>

  <sect2 xml:id="iscsi.tcmu.use">
   <title>Syntaxe</title>
   <para>
    À partir de votre noeud d'initiateur (client) iSCSI, connectez-vous à votre nouvelle cible iSCSI en utilisant l'IQN et le nom d'hôte configurés ci-dessus.
   </para>
  </sect2>
 </sect1>
</chapter>
