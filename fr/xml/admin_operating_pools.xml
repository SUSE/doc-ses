<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph-pools">
 <title>Gestion des réserves de stockage</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>oui</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph stocke les données dans des réserves. Les réserves sont des groupes logiques pour le stockage des objets. Lorsque vous déployez une grappe pour la première fois sans créer de réserve, Ceph utilise les réserves par défaut pour stocker les données. Les points importants suivants concernent les réserves Ceph :
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Solidité</emphasis> : vous pouvez définir le nombre d'OSD, de compartiments et de feuilles pouvant échouer sans perte de données. Pour les réserves répliquées, il s'agit du nombre souhaité de copies/répliques d'un objet. Les nouvelles réserves sont créées avec un nombre par défaut de répliques défini sur 3. Pour les réserves codées à effacement, il s'agit du nombre de tranches de codage (c'est-à-dire <emphasis>m=2</emphasis> dans le profil de code à effacement).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Groupes de placement</emphasis> : il s'agit de structures de données internes pour le stockage des données entre OSD d'une réserve. La façon dont Ceph stocke les données dans les groupes de placement est définie dans une carte CRUSH. Vous pouvez définir le nombre de groupes de placement pour une réserve à sa création. Une configuration type utilise environ 100 groupes de placement par OSD pour fournir un équilibrage optimal sans nécessiter trop de ressources informatiques. Lors de la configuration de plusieurs grappes, veillez à définir un nombre raisonnable de groupes de placement pour la réserve et la grappe dans leur ensemble. Un <link xlink:href="https://ceph.com/pgcalc/">calculateur de groupes de placement Ceph par réserve</link> peut vous aider.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Règles CRUSH</emphasis> : lorsque vous stockez des données dans une réserve, les objets et leurs répliques (ou tranches en cas de réserves codées à effacement) sont placés selon l'ensemble de règles CRUSH assignées à la réserve. Vous pouvez créer une règle CRUSH personnalisée pour votre réserve.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Instantanés</emphasis> : lorsque vous créez des instantanés avec <command>ceph osd pool mksnap</command>, vous prenez effectivement un instantané d'une réserve en particulier.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Pour organiser les données en réserves, vous pouvez répertorier, créer et supprimer des réserves. Vous pouvez également afficher les statistiques d'utilisation pour chaque réserve.
 </para>
 <sect1 xml:id="ceph-pools-associate">
  <title>Association de réserves à une application</title>

  <para>
   Pour pouvoir utiliser les réserves, vous devez les associer à une application. Les réserves qui seront utilisées avec CephFS ou les réserves créées automatiquement par Object Gateway sont automatiquement associées.
  </para>

  <para>
   Pour les autres cas, vous pouvez associer manuellement un nom de l'application de format libre à une réserve :
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>noms d'application par défaut</title>
   <para>
    CephFS utilise le nom de l'application <literal>cephfs</literal>, le périphérique de bloc RADOS emploie <literal>rbd</literal> et la passerelle Object Gateway fait appel à <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   Une réserve peut être associée à plusieurs applications et chaque application peut avoir ses propres métadonnées. Vous pouvez afficher les métadonnées de l'application pour une réserve donnée à l'aide de la commande suivante :
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph-pools-operate">
  <title>Exploitation de réserves</title>

  <para>
   Cette section présente des informations pratiques pour effectuer des tâches de base avec les réserves. Vous pouvez découvrir comment répertorier, créer et supprimer des réserves, afficher des statistiques de réserve ou gérer des instantanés d'une réserve.
  </para>

  <sect2>
   <title>Liste de réserves</title>
   <para>
    Pour afficher la liste des réserves de la grappe, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool ls</screen>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-add-pool">
   <title>Création d'une réserve</title>
   <para>
    Une réserve peut être de type « replicated » (répliquée) pour récupérer des OSD perdus en conservant plusieurs copies des objets, ou de type « erasure » (à effacement) pour obtenir une sorte de fonctionnalité RAID5/6 généralisée. Les réserves répliquées nécessitent plus de stockage brut, tandis que les réserves codées à effacement en exigent moins. La valeur par défaut est « replicated ».
   </para>
   <para>
    Pour créer une réserve répliquée, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Pour créer une réserve codée à effacement, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    La commande <command>ceph osd pool create</command> peut échouer si vous dépassez la limite de groupes de placement par OSD. La limite est définie avec l'option <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       Nom de la réserve. Il doit être unique. Cette option est obligatoire. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre total de groupes de placement pour la réserve. Cette option est obligatoire. La valeur par défaut est 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre total de groupes de placement à des fins de placement. Il doit être égal au nombre total de groupes de placement, à l'exception des scénarios de fractionnement des groupes de placement. Cette option est obligatoire. La valeur par défaut est 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       Nom de l'ensemble de règles CRUSH de cette réserve. Si l'ensemble de règles indiqué n'existe pas, la création de réserves répliquées échoue avec -ENOENT. Pour les réserves répliquées, il s'agit de l'ensemble de règles spécifié par la variable de configuration <varname>osd pool default crush replicated ruleset</varname>. Cet ensemble de règles doit exister. Pour les réserves à effacement, il s'agit de « erasure-code » si le profil de code à effacement par défaut est utilisé, sinon de <replaceable>NOM_RÉSERVE</replaceable>. Cet ensemble de règles sera créé implicitement s'il n'existe pas déjà.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       Pour les réserves codées à effacement uniquement. Utilisez le profil de code d'effacement. Il doit s'agir d'un profil existant tel que défini par <command>osd erasure-code-profile set</command>.
      </para>
      <para>
       Lorsque vous créez une réserve, définissez le nombre de groupes de placement sur une valeur raisonnable. Considérez également le nombre total de groupes de placement par OSD. Les groupes de placement sont coûteux en termes de calcul de sorte que les performances se dégradent lorsque vous avez un grand nombre de réserves avec un grand nombre de groupes de placement (par exemple, 50 réserves dont chacune possède 100 groupes de placement).
      </para>
      <para>
       Reportez-vous à la <xref linkend="op-pgs"/> pour plus d'informations sur le calcul du nombre de groupes de placement approprié pour votre réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       Nombre d'objets attendu pour cette réserve. En définissant cette valeur (avec un seuil <option>filestore merge threshold</option> négatif), le fractionnement du dossier de groupes de placement se produit au moment de la création de la réserve. Cela évite l'impact de latence lié au fractionnement du dossier à l'exécution.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Définition de quotas de réserve</title>
   <para>
    Vous pouvez définir des quotas de réserve pour le nombre maximal d'octets et/ou le nombre maximal d'objets par réserve.
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Pour supprimer un quota, définissez sa valeur sur 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph-pools-operate-del-pool">
   <title>Suppression d'une réserve</title>
   <warning>
    <title>la suppression d'une réserve est irréversible</title>
    <para>
     Les réserves peuvent contenir des données importantes. La suppression d'une réserve entraîne la disparition de toutes les données qu'elle contient et l'impossibilité de la récupérer.
    </para>
   </warning>
   <para>
    Comme la suppression accidentelle d'une réserve constitue un réel danger, Ceph implémente deux mécanismes qui empêchent cette suppression. Ces deux mécanismes doivent être désactivés avant la suppression d'une réserve.
   </para>
   <para>
    Le premier mécanisme consiste à utiliser l'indicateur <literal>NODELETE</literal>. Chaque réserve possède cet indicateur dont la valeur par défaut est « false ». Pour connaître la valeur de cet indicateur sur une réserve, exécutez la commande suivante :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Si elle génère <literal>nodelete: true</literal>, il n'est pas possible de supprimer la réserve tant que vous ne modifiez pas l'indicateur à l'aide de la commande suivante :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    Le second mécanisme est le paramètre de configuration de la grappe <option>mon allow pool delete</option>, qui est « false » par défaut. Cela signifie que, par défaut, il n'est pas possible de supprimer une réserve. Le message d'erreur affiché est le suivant :
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Pour supprimer la réserve malgré ce paramètre de sécurité, vous pouvez définir temporairement <option>mon allow pool delete</option> sur « true », supprimer la réserve, puis renvoyer le paramètre avec « false » :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>cephadm@adm &gt; </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    La commande <command>injectargs</command> affiche le message suivant :
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Cela confirme simplement que la commande a été exécutée avec succès. Il ne s'agit pas d'une erreur.
   </para>
   <para>
    Si vous avez défini vos propres ensembles de règles et règles pour une réserve que vous avez créée, vous devez envisager de les supprimer lorsque vous n'avez plus besoin de la réserve.
   </para>
  </sect2>

  <sect2>
   <title>Modification du nom d'une réserve</title>
   <para>
    Pour renommer une réserve, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Si vous renommez une réserve et que vous disposez de fonctions de réserve pour un utilisateur authentifié, vous devez mettre à jour les fonctions de l'utilisateur avec le nouveau nom de réserve.
   </para>
  </sect2>

  <sect2>
   <title>Affichage des statistiques de la réserve</title>
   <para>
    Pour afficher les statistiques d'utilisation d'une réserve, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>rados df
POOL_NAME                    USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED  RD_OPS      RD  WR_OPS      WR USED COMPR UNDER COMPR
.rgw.root                 768 KiB       4      0     12                  0       0        0      44  44 KiB       4   4 KiB        0 B         0 B
cephfs_data               960 KiB       5      0     15                  0       0        0    5502 2.1 MiB      14  11 KiB        0 B         0 B
cephfs_metadata           1.5 MiB      22      0     66                  0       0        0      26  78 KiB     176 147 KiB        0 B         0 B
default.rgw.buckets.index     0 B       1      0      3                  0       0        0       4   4 KiB       1     0 B        0 B         0 B
default.rgw.control           0 B       8      0     24                  0       0        0       0     0 B       0     0 B        0 B         0 B
default.rgw.log               0 B     207      0    621                  0       0        0 5372132 5.1 GiB 3579618     0 B        0 B         0 B
default.rgw.meta          961 KiB       6      0     18                  0       0        0     155 140 KiB      14   7 KiB        0 B         0 B
example_rbd_pool          2.1 MiB      18      0     54                  0       0        0 3350841 2.7 GiB     118  98 KiB        0 B         0 B
iscsi-images              769 KiB       8      0     24                  0       0        0 1559261 1.3 GiB      61  42 KiB        0 B         0 B
mirrored-pool             1.1 MiB      10      0     30                  0       0        0  475724 395 MiB      54  48 KiB        0 B         0 B
pool2                         0 B       0      0      0                  0       0        0       0     0 B       0     0 B        0 B         0 B
pool3                     333 MiB      37      0    111                  0       0        0 3169308 2.5 GiB   14847 118 MiB        0 B         0 B
pool4                     1.1 MiB      13      0     39                  0       0        0 1379568 1.1 GiB   16840  16 MiB        0 B         0 B
</screen>
   <para>
    Une description de chaque colonne suit :
   </para>
   <variablelist>
    <varlistentry>
     <term>USED</term>
     <listitem>
      <para>
       Nombre d'octets utilisés par la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>OBJECTS</term>
     <listitem>
      <para>
       Nombre d'objets stockés dans la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLONES</term>
     <listitem>
      <para>
       Nombre de clones stockés dans la réserve. Lorsqu'un instantané est créé et que l'on écrit dans un objet, au lieu de modifier l'objet d'origine, son clone est créé de sorte que le contenu de l'objet instantané d'origine n'est pas modifié.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>COPIES</term>
     <listitem>
      <para>
       Nombre de répliques d'objets. Par exemple, si une réserve répliquée avec le facteur de réplication 3 a « x » objets, elle aura normalement 3 * x copies.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>MISSING_ON_PRIMARY</term>
     <listitem>
      <para>
       Nombre d'objets dans l'état altéré (toutes les copies n'existent pas) alors que la copie est manquante sur l'OSD primaire.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNFOUND</term>
     <listitem>
      <para>
       Nombre d'objets introuvables.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Nombre d'objets altérés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD_OPS</term>
     <listitem>
      <para>
       Nombre total d'opérations de lecture demandées pour cette réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RD</term>
     <listitem>
      <para>
       Nombre total d'octets lus à partir de cette réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR_OPS</term>
     <listitem>
      <para>
       Nombre total d'opérations d'écriture demandées pour cette réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>WR</term>
     <listitem>
      <para>
       Nombre total d'octets écrits dans la réserve. Notez que cela n'est pas la même chose que l'utilisation de la réserve, car vous pouvez écrire plusieurs fois sur le même objet. Au final, l'utilisation de la réserve restera la même, mais le nombre d'octets qui y sont écrits augmentera.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>USED COMPR</term>
     <listitem>
      <para>
       Nombre d'octets alloués aux données compressées.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>UNDER COMPR</term>
     <listitem>
      <para>
       Nombre d'octets occupés par les données compressées lorsqu'elles ne sont pas compressées.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Obtention des valeurs d'une réserve</title>
   <para>
    Pour obtenir une valeur d'une réserve, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    Vous pouvez obtenir des valeurs pour les clés répertoriées à la <xref linkend="ceph-pools-values"/> ainsi que les clés suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre de groupes de placement pour la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre effectif de groupes de placement à utiliser lors du calcul du placement des données. La plage valide est égale ou inférieure à <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <tip>
    <title>toutes les valeurs d'une réserve</title>
    <para>
     Pour répertorier toutes les valeurs associées à une réserve spécifique, exécutez :
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool get <replaceable>POOL_NAME</replaceable> all
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-pools-values">
   <title>Définition des valeurs d'une réserve</title>
   <para>
    Pour définir une valeur d'une réserve, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    Vous pouvez définir des valeurs pour les clés suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Définit le nombre de répliques d'objets dans la réserve. Pour plus d'informations, reportez-vous à la <xref linkend="ceph-pools-options-num-of-replicas"/>. Réserves répliquées uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Définit le nombre minimum de répliques requises pour les E/S. Reportez-vous à la <xref linkend="ceph-pools-options-num-of-replicas"/> pour plus de détails. Réserves répliquées uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       Nombre de secondes pendant lesquelles les clients peuvent relire les requêtes acquittées mais non validées.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre de groupes de placement pour la réserve. Si vous ajoutez des OSD à la grappe, vérifiez la valeur des groupes de placement sur toutes les réserves ciblées pour les nouveaux OSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre effectif de groupes de placement à utiliser lors du calcul du placement des données.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       Ensemble de règles à utiliser pour l'assignation de placement d'objets dans la grappe.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Définissez (1) ou désélectionnez (0) l'indicateur HASHPSPOOL sur une réserve donnée. L'activation de cet indicateur modifie l'algorithme pour mieux répartir les PG sur les OSD. Après avoir activé ce drapeau sur une réserve dont le drapeau HASHPSPOOL avait été défini par défaut sur 0, la grappe commence à effectuer des renvois afin de rétablir le placement correct de tous les groupes de placement. Cette activation pouvant créer une charge d'E/S assez importante sur une grappe, ne passez pas le drapeau de 0 à 1 sur les grappes de production très chargées.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Empêche la suppression de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Empêche la modification des options <option>pg_num</option> et <option>pgp_num</option> de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Empêche la modification de la taille de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Définissez/désélectionnez l'indicateur <literal>WRITE_FADVISE_DONTNEED</literal> sur une réserve donnée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub, nodeep-scrub</term>
     <listitem>
      <para>
       Désactive le nettoyage (en profondeur) des données pour la réserve en particulier afin de résoudre une charge d'E/S élevée temporaire.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Active le suivi des jeux d'accès pour les réserves de cache. Reportez-vous à l'article <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtre de Bloom</link> pour obtenir des informations complémentaires. Cette option accepte l'une des valeurs suivantes : <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. La valeur par défaut est <literal>bloom</literal>, les autres valeurs sont utilisées à des fins de test uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       Nombre de jeux d'accès à stocker dans les réserves de cache. Plus le nombre est élevé, plus le daemon <systemitem>ceph-osd</systemitem> consomme une quantité importante de mémoire vive. La valeur par défaut est <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       Durée d'une période de jeu d'accès définie en secondes pour les réserves de cache. Plus le nombre est élevé, plus le daemon <systemitem>ceph-osd</systemitem> consomme une quantité importante de mémoire vive.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       Probabilité de faux positifs pour le type de jeu d'accès de filtre de Bloom. Reportez-vous à l'article <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtre de Bloom</link> pour obtenir des informations complémentaires. La plage valide est comprise entre 0.0 et 1.0. La valeur par défaut est <literal>0.05</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Forcez les OSD à utiliser les horodatages GMT (Greenwich Mean Time) lors de la création d'un jeu d'accès pour la hiérarchisation du cache. Cela garantit que les noeuds situés dans des fuseaux horaires différents retournent le même résultat. La valeur par défaut est <literal>1</literal>. Cette valeur ne doit pas être modifiée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       Pourcentage de la réserve de cache contenant des objets modifiés (altérés) avant que l'agent de hiérarchisation du cache les transfère à la réserve de stockage de sauvegarde. La valeur par défaut est <literal>0.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       Vous pouvez indiquer l'âge minimal d'un objet récemment modifié (altéré) avant que l'agent de hiérarchisation du cache le transfère à la réserve de stockage de sauvegarde à une vitesse supérieure. La valeur par défaut est <literal>0.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       Pourcentage de la réserve de cache contenant des objets non modifiés (propres) avant que l'agent de hiérarchisation du cache les élimine de la réserve de cache. La valeur par défaut est <literal>0.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph commence à vider ou à éliminer des objets lorsque le seuil <option>max_bytes</option> est déclenché.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph commence à vider ou à éliminer des objets lorsque le seuil <option>max_objects</option> est déclenché.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Taux de baisse de la température entre deux <literal>hit_set</literal> successifs. Valeur par défaut : <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Comptez au plus <literal>N</literal> apparitions dans les valeurs de <literal>hit_set</literal> pour le calcul de la température. La valeur par défaut est <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       Durée (en secondes) avant que l'agent de hiérarchisation du cache vide un objet de la réserve de cache vers la réserve de stockage.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       Durée (en secondes) avant que l'agent de hiérarchisation du cache élimine un objet de la réserve de cache.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Si cet indicateur est activé sur les réserves de codage à effacement, la demande de lecture émet des sous-lectures sur toutes les partitions et attend de recevoir un nombre suffisant de fragments à décoder afin de servir le client. Dans le cas des plug-ins <emphasis>jerasure</emphasis> et <emphasis>isa</emphasis>, lorsque les premières réponses <literal>K</literal> sont retournées, la requête du client est servie immédiatement avec les données décodées issues de ces réponses. Cette approche augmente la charge de processeur et diminue la charge de disque/réseau. Pour le moment, cet indicateur est pris en charge uniquement pour les réserves de codage à effacement. La valeur par défaut est <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       Intervalle minimal en secondes pour le nettoyage des réserves lorsque la charge de la grappe est faible. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_scrub_min_interval</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       Intervalle maximal en secondes pour le nettoyage des réserves, quelle que soit la charge de la grappe. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_scrub_max_interval</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       Intervalle en secondes pour le nettoyage <emphasis>en profondeur</emphasis> de la grappe. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_deep_scrub</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-pools-options-num-of-replicas">
   <title>Définition du nombre de répliques d'objets</title>
   <para>
    Pour définir le nombre de répliques d'objets sur un réserve répliquée, exécutez la commande suivante :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable> inclut l'objet lui-même. Si vous souhaitez, par exemple, l'objet et deux copies de l'objet pour obtenir au total trois instances de l'objet, indiquez 3.
   </para>
   <warning>
    <title>ne définissez pas moins de 3 répliques</title>
    <para>
     Si vous définissez <replaceable>num-replicas</replaceable> sur 2, <emphasis>une seule</emphasis> copie de vos données est disponible. Si vous perdez une instance d'objet, vous devez être sûr que l'autre copie n'a pas été endommagée, par exemple depuis le dernier nettoyage pendant la récupération (pour plus d'informations, reportez-vous à la <xref linkend="scrubbing"/>).
    </para>
    <para>
     La définition d'une réserve à réplique unique implique qu'il existe exactement <emphasis>une</emphasis> instance de l'objet de données dans la réserve. Si l'OSD échoue, vous perdez les données. Une utilisation possible d'une réserve avec une réplique consiste à stocker des données temporaires pendant une courte période.
    </para>
   </warning>
   <tip>
    <title>définition de plus de 3 répliques</title>
    <para>
     La définition de 4 répliques pour une réserve augmente la fiabilité de 25 %.
    </para>
    <para>
     Dans le cas de deux centres de données, vous devez définir au moins 4 répliques pour une réserve de sorte à avoir deux copies dans chaque centre de données. De cette façon, en cas de perte d'un centre de données, il existe toujours deux copies et vous pouvez perdre un disque sans perdre de données.
    </para>
   </tip>
   <note>
    <para>
     Un objet peut accepter des E/S en mode dégradé avec moins de <literal>pool size</literal> répliques. Pour définir un nombre minimum de répliques requis pour les E/S, vous devez utiliser le paramètre <literal>min_size</literal>. Par exemple :
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Cela garantit qu'aucun objet de la réserve de données ne recevra d'E/S avec moins de <literal>min_size</literal> répliques.
    </para>
   </note>
   <tip>
    <title>obtention du nombre de répliques d'objets</title>
    <para>
     Pour obtenir le nombre de répliques d'objet, exécutez la commande suivante :
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump | grep 'replicated size'</screen>
    <para>
     Ceph dresse la liste des réserves en mettant en surbrillance l'attribut <literal>replicated size</literal>. Par défaut, Ceph crée deux répliques d'un objet (soit un total de trois copies ou une taille de 3).
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="pools-migration">
  <title>Migration d'une réserve</title>

  <para>
   Lors de la création d'une réserve (voir <xref linkend="ceph-pools-operate-add-pool"/>), vous devez indiquer ses paramètres initiaux, tels que le type de réserve ou le nombre de groupes de placement. Si vous décidez ultérieurement de modifier l'un de ces paramètres, par exemple lors de la conversion d'une réserve répliquée en réserve codée à effacement ou de la diminution du nombre de groupes de placement, vous devez migrer les données de réserve vers une autre réserve dont les paramètres conviennent à votre déploiement.
  </para>

  <para>
   Il existe plusieurs méthodes de migration de réserve. Nous vous recommandons d'utiliser le <emphasis>niveau de cache</emphasis>, car cette méthode est transparente, réduit les temps d'arrêt de la grappe et évite la duplication de l'ensemble des données de la réserve.
  </para>

  <sect2 xml:id="pool-migrate-cache-tier">
   <title>Migration à l'aide d'un niveau de cache</title>
   <tip>
    <title>migration de réserve répliquée uniquement</title>
    <para>
     Vous pouvez utiliser la méthode de niveau de cache pour migrer une réserve répliquée vers une réserve codée à effacement ou vers une autre réserve répliquée. La migration à partir d'une réserve codée à effacement n'est pas prise en charge.
    </para>
   </tip>
   <para>
    Le principe est simple : incluez la réserve dont vous avez besoin pour migrer dans un niveau de cache dans l'ordre inverse. Le <xref linkend="cha-ceph-tiered"/> fournit des informations détaillées sur les niveaux de cache. L'exemple suivant illustre la migration d'une réserve répliquée appelée « testpool » vers une réserve codée à effacement :
   </para>
   <procedure>
    <title>Migration d'une réserve répliquée vers une réserve codée à effacement</title>
    <step>
     <para>
      Créez une réserve codée à effacement nommée « newpool ». Pour plus d'informations sur les paramètres de création d'une réserve, reportez-vous à la <xref linkend="ceph-pools-operate-add-pool"/>.
     </para>
<screen>
 <prompt>cephadm@adm &gt; </prompt>ceph osd pool create newpool <replaceable>PG_NUM</replaceable> <replaceable>PGP_NUM</replaceable> erasure default
</screen>
     <para>
      Vérifiez que le trousseau de clés client utilisé fournit au moins les mêmes fonctionnalités pour « newpool » que pour « testpool ».
     </para>
     <para>
      Vous avez maintenant deux réserves : la réserve répliquée initiale « testpool » contenant des données et la nouvelle réserve codée à effacement « newpool » :
     </para>
     <figure>
      <title>Réserves avant migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Configurez le niveau de cache et la réserve répliquée « testpool » en tant que réserve de cache. L'option <option>-force-nonempty</option> permet d'ajouter un niveau de cache même si la réserve a déjà des données :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=1'
<prompt>cephadm@adm &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode testpool proxy
</screen>
     <figure>
      <title>Configuration du niveau de cache</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Forcez la réserve de cache à déplacer tous les objets vers la nouvelle réserve :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Vidage des données</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Tant que toutes les données n'ont pas été vidées vers la nouvelle réserve codée à effacement, vous devez indiquer une superposition afin que les recherches d'objets s'effectuent dans l'ancienne réserve :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Avec la superposition, toutes les opérations sont réacheminées vers l'ancienne réserve « testpool » répliquée :
     </para>
     <figure>
      <title>Définition de la superposition</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Vous pouvez maintenant basculer tous les clients pour accéder aux objets de la nouvelle réserve.
     </para>
    </step>
    <step>
     <para>
      Une fois toutes les données migrées vers la réserve codée à effacement « newpool », supprimez la superposition et l'ancienne réserve de cache « testpool » :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>cephadm@adm &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration effectuée</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Exécutez :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph tell mon.* injectargs \
 '--mon_debug_unsafe_allow_tier_with_nonempty_snaps=0'
</screen>
    </step>
   </procedure>
   <warning>
    <title>vous ne pouvez pas migrer des images RBD et des exportations CephFS vers une réserve EC</title>
    <para>
     Vous ne pouvez pas migrer des images RBD et des exportations CephFS depuis une réserve répliquée vers une réserve codée à effacement (EC). Les réserves EC peuvent stocker des données, mais pas des métadonnées. L'objet d'en-tête de RBD ne sera pas vidé. Il en va de même pour CephFS.
    </para>
   </warning>
  </sect2>

  <sect2 xml:id="migrate-rbd-image">
   <title>Migration d'une image de périphérique de bloc RADOS</title>
   <para>
    Voici la méthode recommandée pour migrer les images RBD d'une réserve répliquée vers une autre réserve répliquée.
   </para>
   <procedure>
    <step>
     <para>
      Empêchez les clients (une machine virtuelle, par exemple) d'accéder à l'image RBD.
     </para>
    </step>
    <step>
     <para>
      Créez une image dans la réserve cible, avec le parent défini sur l'image source :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     <tip>
      <title>migration uniquement des données vers une réserve EC</title>
      <para>
       Si vous devez migrer uniquement les données d'image vers une nouvelle réserve EC et laisser les métadonnées dans la réserve répliquée d'origine, exécutez la commande suivante à la place :
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration prepare <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable> \
 --data-pool <replaceable>TARGET_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
     </tip>
     <note>
      <title>support client et temps hors service</title>
      <para>
       La méthode <command>rbd migration</command> permet de migrer des images avec un temps hors service minimal du client. Vous ne devez arrêter le client qu'avant l'étape de préparation et pouvez le redémarrer après. Notez que seul un client <systemitem>librbd</systemitem> qui prend en charge cette fonction (Ceph Nautilus ou plus récent) sera en mesure d'ouvrir l'image juste après l'étape de préparation. Les clients <systemitem>librbd</systemitem> plus anciens ou les clients <systemitem>krbd</systemitem> ne pourront pas ouvrir l'image avant l'exécution de l'étape de validation.
      </para>
     </note>
    </step>
    <step>
     <para>
      Laissez les clients accéder à l'image dans la réserve cible.
     </para>
    </step>
    <step>
     <para>
      Migrez les données vers la réserve cible :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration execute <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
    <step>
     <para>
      Supprimez l'ancienne image :
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rbd migration commit <replaceable>SRC_POOL</replaceable>/<replaceable>IMAGE</replaceable>
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha-ceph-snapshots-pool">
  <title>Instantanés de réserve</title>

  <para>
   Les instantanés de réserve sont des instantanés de l'état de l'ensemble de la réserve Ceph. Avec les instantanés de réserve, vous pouvez conserver l'historique de l'état de la réserve. La création d'instantanés de réserve consomme un espace de stockage proportionnel à la taille de la réserve. Vérifiez toujours que le stockage associé possède un espace disque suffisant avant de créer un instantané d'une réserve.
  </para>

  <sect2>
   <title>Création d'un instantané d'une réserve</title>
   <para>
    Pour créer un instantané d'une réserve, exécutez :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool mksnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable>
</screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool mksnap pool1 snap1
created pool pool1 snap snap1</screen>
  </sect2>

  <sect2>
   <title>Liste des instantanés d'une réserve</title>
   <para>
    Pour lister les instantanés existants d'une réserve, exécutez :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lssnap -p <replaceable>POOL_NAME</replaceable>
</screen>
   <para>
    Par exemple :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lssnap -p pool1
1	snap1	2018.12.13 09:36:20
2	snap2	2018.12.13 09:46:03
2 snaps
</screen>
  </sect2>

  <sect2>
   <title>Suppression d'un instantané d'une réserve</title>
   <para>
    Pour supprimer un instantané d'une réserve, exécutez :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool rmsnap <replaceable>POOL-NAME</replaceable> <replaceable>SNAP-NAME</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ceph-pool-compression">
  <title>Compression des données</title>

  <para>
   BlueStore (voir <xref linkend="about-bluestore"/> pour plus de détails) fournit la compression des données à la volée pour économiser de l'espace disque. Le rapport de compression dépend des données stockées sur le système. Notez que la compression/décompression nécessite davantage de ressources processeur.
  </para>

  <para>
   Vous pouvez configurer la compression des données globalement (voir <xref linkend="sec-ceph-pool-bluestore-compression-options"/>), puis remplacer les paramètres de compression spécifiques pour chaque réserve.
  </para>

  <para>
   Vous pouvez activer ou désactiver la compression des données de réserve, ou modifier l'algorithme et le mode de compression à tout moment, que la réserve contienne des données ou non.
  </para>

  <para>
   Aucune compression ne sera appliquée aux données existantes après avoir activé la compression de la réserve.
  </para>

  <para>
   Après avoir désactivé la compression d'une réserve, toutes ses données seront décompressées.
  </para>

  <sect2 xml:id="sec-ceph-pool-compression-enable">
   <title>Activation de la compression</title>
   <para>
    Pour activer la compression des données pour une réserve nommée <replaceable>POOL_NAME</replaceable>, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm <replaceable>COMPRESSION_ALGORITHM</replaceable>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode <replaceable>COMPRESSION_MODE</replaceable>
</screen>
   <tip>
    <title>désactivation de la compression de la réserve</title>
    <para>
     Pour désactiver la compression des données pour une réserve, utilisez « none » comme algorithme de compression :
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_algorithm none
</screen>
   </tip>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-compression-options">
   <title>Options de compression de la réserve</title>
   <para>
    Liste complète de paramètres de compression :
   </para>
   <variablelist>
    <varlistentry xml:id="compr-algorithm">
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Les valeurs possibles sont <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>. La valeur par défaut est <literal>snappy</literal>.
      </para>
      <para>
       L'algorithme de compression à utiliser dépend du cas d'utilisation particulier. Voici quelques recommandations :
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Utilisez la valeur par défaut <literal>snappy</literal> tant que vous n'avez pas une raison valable d'en changer.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>zstd</literal> offre un bon rapport de compression, mais provoque un overhead important du processeur lors de la compression de petites quantités de données.
        </para>
       </listitem>

       <listitem>
        <para>
         Effectuez un banc d'essai de ces algorithmes sur un échantillon de vos données réelles tout en gardant un oeil sur l'utilisation du processeur et de la mémoire de votre grappe.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-mode">
     <term>compression_mode</term>
     <listitem>
      <para>
       Les valeurs possibles sont <literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal> et <literal>force</literal>. La valeur par défaut est <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal> : jamais de compression
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal> : compresser si <literal>COMPRESSIBLE</literal> est suggéré
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal> : compresser sauf si <literal>INCOMPRESSIBLE</literal> est suggéré
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal> : compresser toujours
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry xml:id="compr-ratio">
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valeur : Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. La valeur par défaut est <literal>0,875</literal>, ce qui signifie que si la compression ne réduit pas l'espace occupé d'au moins 12,5 %, l'objet ne sera pas compressé.
      </para>
      <para>
       Les objets au-dessus de ce ratio ne seront pas stockés dans un format compressé en raison du faible gain net.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille maximale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille minimale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec-ceph-pool-bluestore-compression-options">
   <title>Options de compression globales</title>
   <para>
    Les options de configuration suivantes peuvent être définies dans la configuration Ceph et s'appliquent à tous les OSD et non pas seulement à une réserve. La configuration spécifique de la réserve répertoriée à la <xref linkend="sec-ceph-pool-compression-options"/> prévaut.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Reportez-vous à la section <xref linkend="compr-algorithm"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Reportez-vous à la section <xref linkend="compr-mode"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Reportez-vous à la section <xref linkend="compr-ratio"/>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille minimale des objets compressés. Le paramètre est ignoré par défaut en faveur de <option>bluestore_compression_min_blob_size_hdd</option> et <option>bluestore_compression_min_blob_size_ssd</option>. Il est prioritaire lorsqu'il est défini sur une valeur différente de zéro.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille maximale des objets qui sont compressés avant d'être divisés en petites tranches. Le paramètre est ignoré par défaut en faveur de <option>bluestore_compression_max_blob_size_hdd</option> et <option>bluestore_compression_max_blob_size_ssd</option>. Il est prioritaire lorsqu'il est défini sur une valeur différente de zéro.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>8 000</literal>
      </para>
      <para>
       Taille minimale des objets compressés et stockés sur une unité SSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>64 000</literal>
      </para>
      <para>
       Taille maximale des objets qui sont compressés et stockés sur disque SSD (Solid-State Drive) avant qu'ils ne soient divisés en plus petites tranches.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>128 000</literal>
      </para>
      <para>
       Taille minimale des objets compressés et stockés sur disques durs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>512 000</literal>
      </para>
      <para>
       Taille maximale des objets qui sont compressés et stockés sur des disques durs avant qu'ils ne soient divisés en plus petites tranches.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
