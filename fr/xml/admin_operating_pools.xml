<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph.pools">
 <title>Gestion des réserves de stockage</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>oui</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph stocke les données dans des réserves. Les réserves sont des groupes logiques pour le stockage des objets. Lorsque vous déployez une grappe pour la première fois sans créer de réserve, Ceph utilise les réserves par défaut pour stocker les données. Une réserve vous offre les avantages suivants :
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Résistance</emphasis> : vous pouvez définir le nombre d'OSD pouvant échouer sans perte de données. Pour les réserves répliquées, il s'agit du nombre souhaité de copies/répliques d'un objet. Les nouvelles réserves sont créées avec un nombre par défaut de répliques défini sur 3. Comme la configuration typique stocke un objet et une copie supplémentaire, vous devez définir le nombre de répliques sur 2. Pour les réserves codées à effacement, il s'agit du nombre de tranches de codage (c'est-à-dire <emphasis>m=2</emphasis> dans le profil de code à effacement).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Groupes de placement</emphasis> : il s'agit de structures de données internes pour le stockage des données entre OSD d'une réserve. La façon dont Ceph stocke les données dans les groupes de placement est définie dans une assignation CRUSH. Vous pouvez définir le nombre de groupes de placement pour la réserve. Une configuration type utilise environ 100 groupes de placement par OSD pour fournir un équilibrage optimal sans nécessiter trop de ressources informatiques. Lors de la configuration de plusieurs grappes, veillez à définir un nombre raisonnable de groupes de placement pour la réserve et la grappe dans leur ensemble.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Règles CRUSH</emphasis> : lorsque vous stockez des données dans une réserve, un ensemble de règles CRUSH assigné sur la réserve permet à CRUSH d'identifier une règle pour le placement de l'objet et de ses répliques (ou de ses tranches pour les réserves codées à effacement) dans votre grappe. Vous pouvez créer une règle CRUSH personnalisée pour votre réserve.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Instantanés</emphasis> : lorsque vous créez des instantanés avec <command>ceph osd pool mksnap</command>, vous prenez effectivement un instantané d'une réserve en particulier.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Définir les droits de propriété</emphasis> : vous pouvez définir un ID utilisateur en tant que propriétaire d'une réserve.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Pour organiser les données en réserves, vous pouvez répertorier, créer et supprimer des réserves. Vous pouvez également afficher les statistiques d'utilisation pour chaque réserve.
 </para>
 <sect1 xml:id="ceph.pools.associate">
  <title>Association de réserves à une application</title>

  <para>
   Pour pouvoir utiliser les réserves, vous devez les associer à une application. Les réserves qui seront utilisées avec CephFS ou les réserves créées automatiquement par Object Gateway sont automatiquement associées. Les réserves destinées à être utilisées avec RBD doivent être initialisées à l'aide de l'outil <command>rbd</command> (reportez-vous à la <xref linkend="ceph.rbd.commands"/> pour plus d'informations).
  </para>

  <para>
   Pour les autres cas, vous pouvez associer manuellement un nom de l'application de format libre à une réserve :
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>noms d'application par défaut</title>
   <para>
    CephFS utilise le nom de l'application <literal>cephfs</literal>, le périphérique de traitement par blocs RADOS utilise <literal>rbd</literal> et la passerelle Object Gateway utilise <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   Une réserve peut être associée à plusieurs applications et chaque application peut avoir ses propres métadonnées. Vous pouvez afficher les métadonnées de l'application pour une réserve donnée à l'aide de la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.pools.operate">
  <title>Exploitation de réserves</title>

  <para>
   Cette section présente des informations pratiques pour effectuer des tâches de base avec les réserves. Vous pouvez découvrir comment répertorier, créer et supprimer des réserves, afficher des statistiques de réserve ou gérer des instantanés d'une réserve.
  </para>

  <sect2>
   <title>Liste de réserves</title>
   <para>
    Pour afficher la liste des réserves de la grappe, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Création d'une réserve</title>
   <para>
    Pour créer une réserve répliquée, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Pour créer une réserve codée à effacement, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    La commande <command>ceph osd pool create</command> peut échouer si vous dépassez la limite de groupes de placement par OSD. La limite est définie avec l'option <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       Nom de la réserve. Il doit être unique. Cette option est obligatoire. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre total de groupes de placement pour la réserve. Cette option est obligatoire. La valeur par défaut est 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre total de groupes de placement à des fins de placement. Il doit être égal au nombre total de groupes de placement, à l'exception des scénarios de fractionnement des groupes de placement. Cette option est obligatoire. La valeur par défaut est 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       Le type de réserve qui peut être soit <emphasis>répliqué</emphasis> pour récupérer des OSD perdus en conservant plusieurs copies des objets ou <emphasis>à effacement</emphasis> pour obtenir une sorte de fonctionnalité RAID5 généralisée. Les réserves répliquées nécessitent plus de stockage brut mais implémentent toutes les opérations Ceph. Les réserves codées à effacement nécessitent moins de stockage brut mais implémentent uniquement un sous-ensemble d'opérations disponibles. La valeur par défaut est « replicated ».
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       Nom de l'ensemble de règles CRUSH de cette réserve. Si l'ensemble de règles indiqué n'existe pas, la création de la réserve échoue avec -ENOENT. Cependant, la réserve répliquée crée un ensemble de règles d'effacement avec le nom indiqué. La valeur par défaut est « erasure-code » pour la réserve codée à effacement. Elle récupère la variable de configuration Ceph <option>osd_pool_default_crush_replicated_ruleset</option> pour la réserve répliquée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       Pour les réserves codées à effacement uniquement. Utilisez le profil de code d'effacement. Il doit s'agir d'un profil existant tel que défini par <command>osd erasure-code-profile set</command>.
      </para>
      <para>
       Lorsque vous créez une réserve, définissez le nombre de groupes de placement sur une valeur raisonnable (par exemple, 100). Considérez également le nombre total de groupes de placement par OSD. Les groupes de placement sont coûteux en termes de calcul de sorte que les performances se dégradent lorsque vous avez un grand nombre de réserves avec un grand nombre de groupes de placement (par exemple, 50 réserves dont chacune possède 100 groupes de placement). Le point à partir duquel la rentabilité diminue dépend de la puissance de l'hôte OSD.
      </para>
      <para>
       Reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Groupes de placement</link> pour plus d'informations sur le calcul du nombre de groupes de placement approprié pour votre réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       Nombre d'objets attendu pour cette réserve. En définissant cette valeur, le fractionnement du dossier PG se produit au moment de la création de la réserve. Cela évite l'impact de latence lié au fractionnement du dossier à l'exécution.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Définition de quotas de réserve</title>
   <para>
    Vous pouvez définir des quotas de réserve pour le nombre maximal d'octets et/ou le nombre maximal d'objets par réserve.
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Pour supprimer un quota, définissez sa valeur sur 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Suppression d'une réserve</title>
   <warning>
    <title>la suppression d'une réserve est irréversible</title>
    <para>
     Les réserves peuvent contenir des données importantes. La suppression d'une réserve entraîne la disparition de toutes les données qu'elle contient et l'impossibilité de la récupérer.
    </para>
   </warning>
   <para>
    Comme la suppression accidentelle d'une réserve constitue un réel danger, Ceph implémente deux mécanismes qui empêchent cette suppression. Ces deux mécanismes doivent être désactivés avant la suppression d'une réserve.
   </para>
   <para>
    Le premier mécanisme consiste à utiliser l'indicateur <literal>NODELETE</literal>. Chaque réserve possède cet indicateur dont la valeur par défaut est « false ». Pour connaître la valeur de cet indicateur sur une réserve, exécutez la commande suivante :
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Si elle génère <literal>nodelete: true</literal>, il n'est pas possible de supprimer la réserve tant que vous ne modifiez pas l'indicateur à l'aide de la commande suivante :
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    Le second mécanisme est le paramètre de configuration de la grappe <option>mon allow pool delete</option>, qui est « false » par défaut. Cela signifie que, par défaut, il n'est pas possible de supprimer une réserve. Le message d'erreur affiché est le suivant :
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Pour supprimer la réserve malgré ce paramètre de sécurité, vous pouvez définir temporairement <option>mon allow pool delete</option> sur « true », supprimer la réserve, puis renvoyer le paramètre avec « false » :
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    La commande <command>injectargs</command> affiche le message suivant :
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Cela confirme simplement que la commande a été exécutée avec succès. Il ne s'agit pas d'une erreur.
   </para>
   <para>
    Si vous avez défini vos propres ensembles de règles et règles pour une réserve que vous avez créée, vous devez envisager de les supprimer lorsque vous n'avez plus besoin de la réserve. Si vous avez créé des utilisateurs avec des autorisations strictement pour une réserve qui n'existe plus, vous devez également envisager de supprimer ces utilisateurs.
   </para>
  </sect2>

  <sect2>
   <title>Modification du nom d'une réserve</title>
   <para>
    Pour renommer une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Si vous renommez une réserve et que vous disposez de fonctions de réserve pour un utilisateur authentifié, vous devez mettre à jour les fonctions de l'utilisateur avec le nouveau nom de réserve.
   </para>
  </sect2>

  <sect2>
   <title>Affichage des statistiques de la réserve</title>
   <para>
    Pour afficher les statistiques d'utilisation d'une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Définition des valeurs d'une réserve</title>
   <para>
    Pour définir une valeur d'une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    Vous pouvez définir des valeurs pour les clés suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Définit le nombre de répliques d'objets dans la réserve. Pour plus d'informations, reportez-vous à la <xref linkend="ceph.pools.options.num_of_replicas"/>. Réserves répliquées uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Définit le nombre minimum de répliques requises pour les E/S. Reportez-vous à la <xref linkend="ceph.pools.options.num_of_replicas"/> pour plus de détails. Réserves répliquées uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       Nombre de secondes pendant lesquelles les clients peuvent relire les requêtes acquittées mais non validées.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre de groupes de placement pour la réserve. Si vous ajoutez des OSD à la grappe, vous devez augmenter la valeur des groupes de placement. Pour plus d'informations, reportez-vous à la <xref linkend="storage.bp.cluster_mntc.add_pgnum"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre effectif de groupes de placement à utiliser lors du calcul du placement des données.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       Ensemble de règles à utiliser pour l'assignation de placement d'objets dans la grappe.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Définissez (1) ou désélectionnez (0) l'indicateur HASHPSPOOL sur une réserve donnée. L'activation de cet indicateur modifie l'algorithme pour mieux répartir les PG sur les OSD. Après avoir activé cet indicateur sur une réserve dont l'indicateur HASHPSPOOL a été défini sur 0, la grappe commence à se remplir à nouveau afin de rétablir le placement correct de tous les groupes de placement. Sachez que cela peut créer une charge d'E/S assez importante sur une grappe. Une bonne planification doit donc être effectuée sur des grappes de production fortement chargées.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Empêche la suppression de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Empêche la modification des options <option>pg_num</option> et <option>pgp_num</option> de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Empêche la modification de la taille de la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Définissez/désélectionnez l'indicateur <literal>WRITE_FADVISE_DONTNEED</literal> sur une réserve donnée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub, nodeep-scrub</term>
     <listitem>
      <para>
       Désactive le nettoyage (en profondeur) des données pour la réserve en particulier afin de résoudre une charge d'E/S élevée temporaire.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Active le suivi des jeux d'accès pour les réserves de cache. Reportez-vous à l'article <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtre de Bloom</link> pour obtenir des informations complémentaires. Cette option accepte l'une des valeurs suivantes : <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. La valeur par défaut est <literal>bloom</literal>, les autres valeurs sont utilisées à des fins de test uniquement.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       Nombre de jeux d'accès à stocker dans les réserves de cache. Plus le nombre est élevé, plus le daemon <systemitem>ceph-osd</systemitem> consomme une quantité importante de mémoire vive. La valeur par défaut est <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       Durée d'une période de jeu d'accès définie en secondes pour les réserves de cache. Plus le nombre est élevé, plus le daemon <systemitem>ceph-osd</systemitem> consomme une quantité importante de mémoire vive.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       Probabilité de faux positifs pour le type de jeu d'accès de filtre de Bloom. Reportez-vous à l'article <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Filtre de Bloom</link> pour obtenir des informations complémentaires. La plage valide est comprise entre 0.0 et 1.0. La valeur par défaut est <literal>0.05</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Forcez les OSD à utiliser les horodatages GMT (Greenwich Mean Time) lors de la création d'un jeu d'accès pour la hiérarchisation du cache. Cela garantit que les noeuds situés dans des fuseaux horaires différents retournent le même résultat. La valeur par défaut est <literal>1</literal>. Cette valeur ne doit pas être modifiée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       Pourcentage de la réserve de cache contenant des objets modifiés (altérés) avant que l'agent de hiérarchisation du cache les transfère à la réserve de stockage de sauvegarde. La valeur par défaut est <literal>.4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       Vous pouvez indiquer l'âge minimal d'un objet récemment modifié (altéré) avant que l'agent de hiérarchisation du cache le transfère à la réserve de stockage de sauvegarde à une vitesse supérieure. La valeur par défaut est <literal>.6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       Pourcentage de la réserve de cache contenant des objets non modifiés (propres) avant que l'agent de hiérarchisation du cache les élimine de la réserve de cache. La valeur par défaut est <literal>.8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph commence à vider ou à éliminer des objets lorsque le seuil <option>max_bytes</option> est déclenché.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph commence à vider ou à éliminer des objets lorsque le seuil <option>max_objects</option> est déclenché.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Taux de baisse de la température entre deux <literal>hit_set</literal> successifs. Valeur par défaut : <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Comptez au plus <literal>N</literal> apparitions dans les valeurs de <literal>hit_set</literal> pour le calcul de la température. La valeur par défaut est <literal>1</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       Durée (en secondes) avant que l'agent de hiérarchisation du cache vide un objet de la réserve de cache vers la réserve de stockage.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       Durée (en secondes) avant que l'agent de hiérarchisation du cache élimine un objet de la réserve de cache.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Si cet indicateur est activé sur les réserves de codage à effacement, la demande de lecture émet des sous-lectures sur toutes les partitions et attend de recevoir un nombre suffisant de fragments à décoder afin de servir le client. Dans le cas des plug-ins <emphasis>jerasure</emphasis> et <emphasis>isa</emphasis>, lorsque les premières réponses <literal>K</literal> sont retournées, la requête du client est servie immédiatement avec les données décodées issues de ces réponses. Ceci permet d'obtenir des ressources pour de meilleures performances. Pour le moment, cet indicateur est pris en charge uniquement pour les pools de codage à effacement. La valeur par défaut est <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       Intervalle minimal en secondes pour le nettoyage des pools lorsque la charge de la grappe est faible. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_scrub_min_interval</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       Intervalle maximal en secondes pour le nettoyage des réserves, quelle que soit la charge de la grappe. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_scrub_max_interval</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       Intervalle en secondes pour le nettoyage <emphasis>en profondeur</emphasis> de la grappe. La valeur par défaut <literal>0</literal> signifie que la valeur de <option>osd_deep_scrub</option> du fichier de configuration Ceph est utilisée.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Obtention des valeurs d'une réserve</title>
   <para>
    Pour obtenir une valeur d'une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    Vous pouvez obtenir des valeurs pour les clés répertoriées à la <xref linkend="ceph.pools.values"/> ainsi que les clés suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Nombre de groupes de placement pour la réserve.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Nombre effectif de groupes de placement à utiliser lors du calcul du placement des données. La plage valide est égale ou inférieure à <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Définition du nombre de répliques d'objets</title>
   <para>
    Pour définir le nombre de répliques d'objets sur un réserve répliquée, exécutez la commande suivante :
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    <replaceable>num-replicas</replaceable> inclut l'objet lui-même. Si vous souhaitez par exemple l'objet et les deux copies de l'objet pour un total de trois instances de l'objet, indiquez 3.
   </para>
   <para>
    Si vous définissez <replaceable>num-replicas</replaceable> sur 2, <emphasis>une seule</emphasis> copie de vos données est disponible. Si vous perdez une instance d'objet, vous devez être sûr que l'autre copie n'a pas été endommagée depuis le dernier <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">nettoyage</link> pendant la récupération.
   </para>
   <para>
    La définition d'une réserve à réplique unique implique qu'il existe exactement <emphasis>une</emphasis> instance de l'objet de données dans la réserve. Si l'OSD échoue, vous perdez les données. Une utilisation possible d'une réserve avec une réplique consiste à stocker des données temporaires pendant une courte période.
   </para>
   <para>
    Définir plus de trois répliques pour une réserve entraîne une faible augmentation de la fiabilité, mais peut convenir dans de rares cas. N'oubliez pas que plus le nombre de répliques est élevé, plus l'espace disque nécessaire doit être étendu pour le stockage des copies d'objet. Si vous avez besoin d'une sécurité de données renforcée, nous vous recommandons d'utiliser des réserves codées à effacement. Pour plus d'informations, reportez-vous au <xref linkend="cha.ceph.erasure"/>.
   </para>
   <warning>
    <title>plus de deux répliques recommandées</title>
    <para>
     Nous vous déconseillons fortement d'utiliser deux répliques uniquement. En cas d'échec de l'OSD, il est fort probable que le second OSD échoue en raison d'une charge de travail élevée pendant la récupération.
    </para>
   </warning>
   <para>
    Par exemple :
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    Vous pouvez exécuter cette commande pour chaque réserve.
   </para>
   <note>
    <para>
     Un objet peut accepter des E/S en mode dégradé avec moins de <literal>pool size</literal> répliques. Pour définir un nombre minimum de répliques requis pour les E/S, vous devez utiliser le paramètre <literal>min_size</literal>. Par exemple :
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     Cela garantit qu'aucun objet de la réserve de données ne recevra d'E/S avec moins de <literal>min_size</literal> répliques.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Obtention du nombre de répliques d'objets</title>
   <para>
    Pour obtenir le nombre de répliques d'objet, exécutez la commande suivante :
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph dresse la liste des réserves en mettant en surbrillance l'attribut <literal>replicated size</literal>. Par défaut, Ceph crée deux répliques d'un objet (soit un total de trois copies ou une taille de 3).
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pgnum">
   <title>Augmentation du nombre de groupes de placement</title>
   <para>
    Lors de la création d'une réserve, vous indiquez le nombre de groupes de placement pour celle-ci (reportez-vous à la <xref linkend="ceph.pools.operate.add_pool"/>). Après avoir ajouté des OSD supplémentaires à la grappe, vous devez généralement augmenter le nombre de groupes de placement pour des raisons de performance et de durabilité des données. Pour chaque groupe de placement, les noeuds d'OSD et de moniteur ont besoin de mémoire, de réseau et de temps processeur à tout moment, et même davantage pendant la récupération. De ce fait, la réduction du nombre de groupes de placement permet d'économiser des quantités importantes de ressources.
   </para>
   <warning>
    <title>valeur excessive de <option>pg_num</option></title>
    <para>
     En changeant la valeur de <option>pg_num</option> d'une réserve, il peut arriver que le nouveau nombre de groupes de placement dépasse la limite autorisée. Par exemple
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     La limite est dérivée de la valeur de <option> mon_osd_max_split_count </option> et empêche la division extrême des groupes de placement.
    </para>
   </warning>
   <para>
    Déterminer le nouveau nombre approprié de groupes de placement pour une grappe redimensionnée est une tâche complexe. Une approche consiste à augmenter continuellement le nombre de groupes de placement jusqu'à l'état optimal des performances de la grappe. Pour déterminer le nouveau nombre incrémenté de groupes de placement, vous devez obtenir la valeur du paramètre <option>mon_osd_max_split_count</option> et l'ajouter au nombre actuel de groupes de placement. Pour vous donner un aperçu, consultez le script suivant :
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    Après avoir trouvé le nombre suivant de groupes de placement, augmentez-le avec
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pool">
   <title>Ajout d'une réserve</title>
   <para>
    Lorsque vous déployez une grappe pour la première fois, Ceph utilise les réserves par défaut pour stocker les données. Vous pouvez ensuite créer une réserve avec
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    Pour plus d'informations sur la création d'une réserve de grappe, reportez-vous à la <xref linkend="ceph.pools.operate.add_pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools.migration">
  <title>Migration d'une réserve</title>

  <para>
   Lors de la création d'une réserve (voir <xref linkend="ceph.pools.operate.add_pool"/>), vous devez indiquer ses paramètres initiaux, tels que le type de réserve ou le nombre de groupes de placement. Si vous décidez ultérieurement de modifier l'un de ces paramètres après avoir placé des données dans la réserve, vous devez migrer les données de la réserve vers une autre dont les paramètres conviennent à votre déploiement.
  </para>

  <para>
   Il existe plusieurs méthodes de migration de réserve. Nous vous recommandons d'utiliser le <emphasis>niveau de cache</emphasis> car cette méthode est transparente, réduit les temps d'arrêt de la grappe et évite la duplication de toutes les données de cette dernière.
  </para>

  <sect2 xml:id="pool.migrate.cache_tier">
   <title>Migration à l'aide d'un niveau de cache</title>
   <para>
    Le principe est simple : incluez la réserve dont vous avez besoin pour migrer dans un niveau de cache dans l'ordre inverse. Le <xref linkend="cha.ceph.tiered"/> fournit des informations détaillées sur les niveaux de cache. Par exemple, pour migrer une réserve répliquée nommée « testpool » vers une réserve codée à effacement, suivez ces étapes :
   </para>
   <procedure>
    <title>Migration d'une réserve répliquée vers une réserve codée à effacement</title>
    <step>
     <para>
      Créez une réserve codée à effacement nommée « newpool » :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      Vous avez maintenant deux réserves : la réserve répliquée initiale « testpool » contenant des données et la nouvelle réserve codée à effacement « newpool » :
     </para>
     <figure>
      <title>Réserves avant migration</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Configurez le niveau de cache et la réserve répliquée « testpool » en tant que réserve de cache :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      À partir de maintenant, tous les nouveaux objets sont créés sur la nouvelle réserve :
     </para>
     <figure>
      <title>Configuration du niveau de cache</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Forcez la réserve de cache à déplacer tous les objets vers la nouvelle réserve :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Vidage des données</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Basculez tous les clients vers la nouvelle réserve. Tant que toutes les données n'ont pas été vidées vers la nouvelle réserve codée à effacement, vous devez indiquer une superposition afin que les recherches d'objets s'effectuent dans l'ancienne réserve :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Avec la superposition, toutes les opérations sont réacheminées vers l'ancienne réserve « testpool » répliquée :
     </para>
     <figure>
      <title>Définition de la superposition</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Vous pouvez maintenant basculer tous les clients pour accéder aux objets de la nouvelle réserve.
     </para>
    </step>
    <step>
     <para>
      Une fois toutes les données migrées vers la réserve codée à effacement « newpool », supprimez la superposition et l'ancienne réserve de cache « testpool » :
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Migration effectuée</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Instantanés de réserve</title>

  <para>
   Les instantanés de réserve sont des instantanés de l'état de l'ensemble de la réserve Ceph. Avec les instantanés de réserve, vous pouvez conserver l'historique de l'état de la réserve. Selon la taille de la réserve, la création d'instantanés de réserve peut nécessiter un vaste espace de stockage. Vérifiez toujours que le stockage associé possède un espace disque suffisant avant de créer un instantané d'une réserve.
  </para>

  <sect2>
   <title>Création d'un instantané d'une réserve</title>
   <para>
    Pour créer un instantané d'une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    Par exemple :
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Suppression d'un instantané d'une réserve</title>
   <para>
    Pour supprimer un instantané d'une réserve, exécutez :
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.pool.compression">
  <title>Compression des données</title>

  <para>
   À compter de SUSE Enterprise Storage 5, BlueStore offre la compression de données à la volée pour économiser l'espace disque.
  </para>

  <sect2 xml:id="sec.ceph.pool.compression.enable">
   <title>Activation de la compression</title>
   <para>
    La compression des données d'une réserve peut être activée comme suit :
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    Remplacez <replaceable>POOL_NAME</replaceable> par la réserve pour laquelle vous voulez activer la compression.
   </para>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.compression.options">
   <title>Options de compression de la réserve</title>
   <para>
    Liste complète de paramètres de compression :
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Valeurs : <literal>aucune</literal>, <literal>zstd</literal>, <literal>snappy</literal>. Valeur par défaut : <literal>snappy</literal>.
      </para>
      <para>
       L'algorithme de compression à utiliser dépend du cas d'utilisation particulier. Voici plusieurs recommandations :
      </para>
      <itemizedlist>
       <listitem>
        <para>
         N'utilisez pas <literal>zlib</literal> : les algorithmes restants sont préférables.
        </para>
       </listitem>
       <listitem>
        <para>
         Si vous avez besoin d'un taux de compression correct, utilisez <literal>zstd</literal>. Notez que <literal>zstd</literal> n'est pas recommandé pour BlueStore en raison de la surcharge du processeur lors de la compression de petites quantités de données.
        </para>
       </listitem>
       <listitem>
        <para>
         Si vous avez besoin de réduire l'utilisation du processeur, utilisez <literal>lz4</literal> ou <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Effectuez un banc d'essai de ces algorithmes sur un échantillon de vos données réelles tout en gardant un oeil sur l'utilisation du processeur et de la mémoire de votre grappe.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       Valeur : {<literal>aucune</literal>, <literal>agressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Valeur par défaut : <literal>aucune</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>aucune</literal>: jamais de compression
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal> : compresser si <literal>COMPRESSIBLE</literal> est suggéré
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal> : compresser sauf si <literal>INCOMPRESSIBLE</literal> est suggéré
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal> : compresser toujours
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Pour plus d'informations sur la définition de l'indicateur <literal>COMPRESSIBLE</literal> ou <literal>INCOMPRESSIBLE</literal>, reportez-vous à la page <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valeur : Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Valeur par défaut : <literal>.875</literal>
      </para>
      <para>
       Les objets au-dessus de ce ratio ne seront pas stockés dans un format compressé en raison du faible gain net.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille minimale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille maximale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.bluestore_compression.options">
   <title>Options de compression globales</title>
   <para>
    Les options de configuration suivantes peuvent être définies dans la configuration Ceph et s'appliquent à tous les OSD et non pas seulement à une réserve. La configuration spécifique de la réserve répertoriée à la <xref linkend="sec.ceph.pool.compression.options"/> prévaut.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Valeurs : <literal>aucune</literal>, <literal>zstd</literal>, <literal>snappy</literal>, <literal>zlib</literal>. Valeur par défaut : <literal>snappy</literal>.
      </para>
      <para>
       L'algorithme de compression à utiliser dépend du cas d'utilisation particulier. Voici plusieurs recommandations :
      </para>
      <itemizedlist>
       <listitem>
        <para>
         N'utilisez pas <literal>zlib</literal>, les autres algorithmes sont meilleurs.
        </para>
       </listitem>
       <listitem>
        <para>
         Si vous avez besoin d'un taux de compression correct, utilisez <literal>zstd</literal>. Notez que <literal>zstd</literal> n'est pas recommandé pour BlueStore en raison de la surcharge du processeur lors de la compression de petites quantités de données.
        </para>
       </listitem>
       <listitem>
        <para>
         Si vous avez besoin de réduire l'utilisation du processeur, utilisez <literal>lz4</literal> ou <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Effectuez un banc d'essai de ces algorithmes sur un échantillon de vos données réelles tout en gardant un oeil sur l'utilisation du processeur et de la mémoire de votre grappe.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Valeur : {<literal>aucune</literal>, <literal>agressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Valeur par défaut : <literal>aucune</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>aucune</literal>: jamais de compression
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal> : compresser si <literal>COMPRESSIBLE</literal> est suggéré.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal> : compresser sauf si <literal>INCOMPRESSIBLE</literal> est suggéré
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal> : compresser toujours
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Pour plus d'informations sur la définition de l'indicateur <literal>COMPRESSIBLE</literal> ou <literal>INCOMPRESSIBLE</literal>, reportez-vous à la page <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Valeur : Double, Ratio = SIZE_COMPRESSED / SIZE_ORIGINAL. Valeur par défaut : <literal>.875</literal>
      </para>
      <para>
       Les objets au-dessus de ce ratio ne seront pas stockés dans un format compressé en raison du faible gain net.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille minimale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>0</literal>
      </para>
      <para>
       Taille maximale des objets compressés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>8 000</literal>
      </para>
      <para>
       Taille minimale des objets compressés et stockés sur une unité SSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>64 000</literal>
      </para>
      <para>
       Taille maximale des objets compressés et stockés sur une unité SSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>128 000</literal>
      </para>
      <para>
       Taille minimale des objets compressés et stockés sur disques durs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valeur : entier non signé, taille en octets. Valeur par défaut : <literal>512 000</literal>
      </para>
      <para>
       Taille maximale des objets compressés et stockés sur disques durs.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
