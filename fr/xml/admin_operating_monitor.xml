<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph.monitor">
 <title>Détermination de l'état de la grappe</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>oui</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Lorsque vous disposez d'une grappe en cours d'exécution, vous pouvez utiliser l'outil <command>ceph</command> pour la surveiller. La détermination de l'état de la grappe implique généralement la vérification de l'état de l'OSD, du moniteur, du groupe de placement et du serveur de métadonnées. <remark role="fixme">Maybe revert to old version of sentence: Determining the cluster state typically involves
  checking OSD status, monitor status, placement group status and metadata
  server status.</remark>
 </para>
 <tip>
  <title>Mode interactif</title>
  <para>
   Pour exécuter l'outil <command>ceph</command> en mode interactif, tapez <command>ceph</command> sans argument sur la ligne de commande. Le mode interactif est plus pratique si vous voulez entrer plusieurs commandes <command>ceph</command> consécutives. Par exemple :
  </para>
<screen><prompt>cephadm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor.health">
  <title>Vérification de l'état d'intégrité de la grappe</title>

  <para>
   Après avoir démarré votre grappe et avant de commencer à lire et/ou à écrire des données, vérifiez l'état d'intégrité de votre grappe :
  </para>

<screen><prompt>root # </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <para>
   La grappe Ceph renvoie l'un des codes d'intégrité suivants :
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Un ou plusieurs OSD sont marqués comme étant arrêtés. Le daemon OSD peut avoir été arrêté ou les OSD homologues peuvent ne pas être en mesure d'accéder à l'OSD via le réseau. Un daemon arrêté ou bloqué, un hôte en panne ou une panne réseau font partie des causes les plus courantes de ce problème.
     </para>
     <para>
      Vérifiez que l'hôte est intègre, que le daemon a été démarré et que le réseau fonctionne. Si le daemon est tombé en panne, le fichier journal du daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) peut contenir des informations de débogage.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>type crush</replaceable>_DOWN, par exemple, OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Tous les OSD d'une sous-arborescence CRUSH particulière sont marqués comme étant arrêtés, par exemple tous les OSD d'un hôte.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      Un OSD est référencé dans la hiérarchie des assignations CRUSH, mais n'existe pas. L'OSD peut être retiré de la hiérarchie CRUSH avec :
     </para>
<screen><prompt>root # </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Les seuils d'utilisation de <emphasis>backfillfull</emphasis>, <emphasis>nearfull</emphasis>, <emphasis>full</emphasis> et/ou <emphasis>failafe_full</emphasis> ne sont pas ascendants. <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> et <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis> sont agencés dans cet ordre. Les seuils peuvent être ajustés avec :
     </para>
<screen><prompt>root # </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Un ou plusieurs OSD ont dépassé le seuil <emphasis>full</emphasis> et empêchent la grappe de gérer les écritures. L'utilisation par réserve peut être vérifiée comme suit :
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
     <para>
      Le ratio <emphasis>full</emphasis> actuellement défini peut être vu avec :
     </para>
<screen><prompt>root # </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Une solution à court terme de restauration de la disponibilité en écriture consiste à augmenter légèrement le seuil full :
     </para>
<screen><prompt>root # </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Ajoutez un nouveau stockage à la grappe en déployant plus d'OSD ou supprimez les données existantes afin de libérer de l'espace.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Un ou plusieurs OSD ont dépassé le seuil <emphasis>backfillfull</emphasis>, ce qui empêche le rééquilibrage des données sur ce périphérique. Cet avertissement anticipé indique que le rééquilibrage peut ne pas aboutir et que la grappe approche de sa pleine capacité. L'utilisation par réserve peut être vérifiée comme suit :
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Un ou plusieurs OSD ont dépassé le seuil <emphasis>nearful</emphasis>. Cet avertissement anticipé indique que la grappe approche de sa pleine capacité. L'utilisation par réserve peut être vérifiée comme suit :
     </para>
<screen><prompt>root # </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Un ou plusieurs indicateurs de grappe présentant un intérêt ont été définis. À l'exception de <emphasis>full</emphasis>, ces indicateurs peuvent être définis ou effacés comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>root # </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Ces indicateurs comprennent :
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         La grappe est marquée comme pleine et ne peut donc pas traiter les écritures.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr </term>
       <listitem>
        <para>
         Les lectures et les écritures sont mises en pause.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         Les OSD ne sont pas autorisés à démarrer.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Les rapports d'échec OSD sont ignorés de sorte que les moniteurs ne marquent pas les OSD comme <emphasis>down</emphasis> (arrêtés).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Les OSD précédemment marqués comme <emphasis>out</emphasis> (hors service) ne sont pas marqués comme <emphasis>in</emphasis> (en service) lors de leur démarrage.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Les OSD <emphasis>down</emphasis> (arrêtés) ne seront pas automatiquement marqués comme <emphasis>out</emphasis> (hors service) à l'issue de l'intervalle configuré.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         La récupération ou le rééquilibrage des données est suspendu.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         Le nettoyage (reportez-vous à la <xref linkend="scrubbing"/>) est désactivé.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         L'activité de hiérarchisation du cache est suspendue.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Un ou plusieurs OSD possèdent chacun un indicateur OSD. Ces indicateurs comprennent :
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         L'OSD n'est pas autorisé à démarrer.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         Les rapports d'échec sont ignorés pour cet OSD.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Si cet OSD était auparavant marqué comme <emphasis>out</emphasis> automatiquement après un échec, il ne sera pas marqué comme <emphasis>in</emphasis> lors du démarrage.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Si cet OSD est arrêté, il n'est pas automatiquement marqué comme <emphasis>out</emphasis> à l'issue de l'intervalle configuré.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      Les indicateurs OSD peuvent être définis et effacés comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>root # </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      L'assignation CRUSH doit être mise à jour, car elle utilise des paramètres très anciens. Les paramètres les plus anciens qui peuvent être utilisés (c'est-à-dire la version client la plus ancienne pouvant se connecter à la grappe) sans déclencher cet avertissement d'intégrité sont déterminés par l'option de configuration <option>mon_crush_min_required_version</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      L'assignation CRUSH utilise une méthode non optimale plus ancienne afin de calculer les valeurs de pondération intermédiaires des compartiments straw. L'assignation CRUSH doit être mise à jour pour utiliser la nouvelle méthode (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Une ou plusieurs réserves de cache ne sont pas configurées avec un jeu d'accès pour le suivi de l'utilisation, ce qui empêche l'agent de hiérarchisation d'identifier les objets inactifs à évincer du cache. Les jeux d'accès peuvent être configurés sur la réserve de cache comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      Aucun OSD d'une version antérieure à la version 12 « Luminous » ne s'exécute actuellement, mais l'indicateur <option>sortbitwise</option> n'est pas défini. Vous devez définir l'indicateur <option>sortbitwise</option> pour permettre le démarrage des OSD antérieurs à la version 12 « Luminous » ou plus récents :
     </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Une ou plusieurs réserves ont atteint leur quota et n'autorisent plus les écritures. Vous pouvez définir des quotas de réserve et leur utilisation comme suit :
     </para>
<screen><prompt>root # </prompt>ceph df detail</screen>
     <para>
      Vous pouvez augmenter le quota de réserve comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      ou supprimer des données existantes pour réduire l'utilisation.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      La disponibilité des données est réduite, ce qui signifie que la grappe ne peut pas traiter les requêtes de lecture ou d'écriture potentielles pour certaines de ses données. Plus précisément, un ou plusieurs groupes de placement se trouvent dans un état qui ne permet pas de traiter les demandes d'E/S. Les états de groupe de placement posant problème sont les suivants : <emphasis>peering</emphasis> (homologation), <emphasis>stale</emphasis> (périmé), <emphasis>incomplete</emphasis> (incomplet) et l'absence d'état <emphasis>active</emphasis> (actif) (si ces conditions ne disparaissent pas rapidement). Pour plus de détails sur les groupes de placement affectés, exécutez la commande suivante :
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      Dans la plupart des cas, la cause première est due au fait qu'un ou plusieurs OSD sont actuellement arrêtés. Pour connaître l'état des groupes de placement incriminés, exécutez la commande suivante :
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      La redondance des données est réduite pour certaines données, ce qui signifie que la grappe ne dispose pas du nombre de répliques souhaité pour toutes les données (pour les réserves répliquées) ou pour les fragments de code d'effacement (pour les réserves codées à effacement). Plus précisément, l'indicateur <emphasis>degraded</emphasis> (altéré) ou <emphasis>undersized</emphasis> (insuffisant) est associé à un ou plusieurs groupes de placement (le nombre d'instances de ce groupe de placement est insuffisant dans la grappe) ou l'indicateur <emphasis>clean</emphasis> ne leur est pas associé depuis un certain temps. Pour plus de détails sur les groupes de placement affectés, exécutez la commande suivante :
     </para>
<screen><prompt>root # </prompt>ceph health detail</screen>
     <para>
      Dans la plupart des cas, la cause première est due au fait qu'un ou plusieurs OSD sont actuellement arrêtés. Pour connaître l'état des groupes de placement incriminés, exécutez la commande suivante :
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      La redondance des données peut être réduite ou menacée pour certaines données en raison d'un manque d'espace libre dans la grappe. Plus précisément, l'indicateur <emphasis>backfill_toofull</emphasis> ou <emphasis>recovery_toofull</emphasis> est associé à un ou plusieurs groupes de placement, ce qui signifie que la grappe ne parvient pas à migrer ou à récupérer les données, car un ou plusieurs OSD ont dépassé le seuil <emphasis>backfillfull</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      Le nettoyage des données (voir <xref linkend="scrubbing"/>) a détecté des problèmes de cohérence des données dans la grappe. Plus précisément, l'indicateur <emphasis>inconsistent</emphasis> ou <emphasis>snaptrim_error</emphasis> est associé à un ou plusieurs groupes de placement, ce qui indique qu'une opération de nettoyage antérieure a détecté un problème ou que l'indicateur <emphasis>repair</emphasis> est défini, car une réparation est actuellement en cours pour une telle incohérence.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Les nettoyages récents d'OSD ont révélé des incohérences.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Une réserve de niveau de cache est presque pleine. Dans ce contexte, l'état Full est déterminé par les propriétés <emphasis>target_max_bytes</emphasis> et <emphasis>target_max_objects</emphasis> de la réserve de cache. Lorsque la réserve atteint le seuil cible, les requêtes d'écriture dans la réserve peuvent être bloquées pendant que les données sont vidées et évincées du cache, un état qui entraîne généralement des latences très élevées et des performances médiocres. La taille cible de la réserve de cache peut être définie ainsi :
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      L'activité normale de vidage et d'éviction du cache peut également être entravée en raison de la disponibilité ou des performances réduites du niveau de base ou de la charge globale de la grappe.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      Le nombre de groupes de placement utilisés est inférieur au seuil configurable de <option>mon_pg_warn_min_per_osd</option> groupes de placement par OSD. Cela peut entraîner une distribution et un équilibrage sous-optimaux des données entre les OSD de la grappe, ce qui fait baisser les performances globales.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      Le nombre de groupes de placement utilisés est supérieur au seuil configurable de <option>mon_pg_warn_max_per_osd</option> groupes de placement par OSD. Cela peut conduire à une utilisation plus importante de la mémoire pour les daemons OSD, à une homologation plus lente après des changements d'état de grappe (redémarrages, ajouts ou suppressions d'OSD, par exemple) et à une charge plus élevée des instances Ceph Manager et Ceph Monitor.
     </para>
     <para>
      La valeur de <option>pg_num</option> ne peut être réduite pour les réserves existantes, contrairement à la valeur de <option>pgp_num</option>. Cela permet de regrouper efficacement certains groupes de placement sur les mêmes ensembles d'OSD, atténuant ainsi quelques-uns des effets négatifs décrits ci-dessus. Il est possible d'ajuster la valeur de <option>pgp_num</option> comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      La valeur de <option>pgp_num</option> est inférieure à celle de <option>pg_num</option> pour une ou plusieurs réserves. Ceci indique normalement que le nombre de groupes de placement a été augmenté indépendamment du comportement de placement. Ce problème est résolu en définissant <option>pgp_num</option> et <option>pg_num</option> sur la même valeur, ce qui déclenche la migration de données, comme suit :
     </para>
<screen>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Pour une ou plusieurs réserves, le nombre moyen d'objets par groupe de placement est sensiblement supérieur à la moyenne globale de la grappe. Le seuil spécifique est contrôlé par la valeur de configuration de <option>mon_pg_warn_max_object_skew</option>. Cela indique généralement que la ou les réserves contenant la plupart des données de la grappe possèdent un nombre trop faible de groupes de placement et/ou que les autres réserves ne contenant pas autant de données possèdent un nombre excessif de groupes de placement. Pour ne plus afficher l'avertissement d'intégrité, vous pouvez relever le seuil en ajustant l'option de configuration <option>mon_pg_warn_max_object_skew</option> sur les moniteurs.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Il existe une réserve qui contient un ou plusieurs objets, mais qui n'a pas été marquée pour une utilisation par une application particulière. Pour que cet avertissement ne s'affiche plus, étiquetez la réserve pour qu'elle soit utilisée par une application. Par exemple, si la réserve est utilisée par RBD :
     </para>
<screen><prompt>root # </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Si la réserve est utilisée par une application personnalisée « foo », vous pouvez également l'étiqueter à l'aide de la commande de bas niveau :
     </para>
<screen><prompt>root # </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Une ou plusieurs réserves ont atteint leur quota (ou sont proches des 100 %). Le seuil de déclenchement de cette condition d'erreur est contrôlé par l'option de configuration <option>mon_pool_quota_crit_threshold</option>. Les quotas de réserve peuvent être ajustés à la hausse ou à la baisse (voire supprimés) comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Définir la valeur de quota sur 0 désactive le quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Une ou plusieurs réserves se rapprochent de leur quota. Le seuil de déclenchement de cette condition d'avertissement est contrôlé par l'option de configuration <option>mon_pool_quota_warn_threshold</option>. Les quotas de réserve peuvent être ajustés à la hausse ou à la baisse (voire supprimés) comme suit :
     </para>
<screen><prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>root # </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Définir la valeur de quota sur 0 désactive le quota.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Un ou plusieurs objets de la grappe ne peuvent pas être stockés sur le noeud de destination. Cela indique que la migration de données n'a pas abouti en raison d'une modification récente de la grappe. Un mauvais placement des données n'est pas dangereux en soi. La cohérence des données n'est jamais compromise et les anciennes copies d'objets ne sont jamais supprimées tant que le nombre de nouvelles copies souhaité n'est pas atteint (dans les emplacements souhaités).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      Un ou plusieurs objets de la grappe sont introuvables. Plus précisément, les OSD savent qu'une copie nouvelle ou mise à jour d'un objet doit exister, mais la copie de cette version de l'objet est introuvable sur les OSD actuellement en ligne. Les requêtes de lecture ou d'écriture sur les objets « introuvables » seront bloquées. Idéalement, un OSD arrêté peut être remis en ligne avec la copie la plus récente de l'objet introuvable. Les OSD candidats peuvent être identifiés à partir de l'état d'homologation du ou des groupes de placement associés à l'objet introuvable :
     </para>
<screen><prompt>root # </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      Une ou plusieurs requêtes OSD sont longues à traiter. Cela peut indiquer une charge extrême, un périphérique de stockage lent ou un bogue logiciel. Vous pouvez interroger la file d'attente des requêtes sur le ou les OSD en question, exécutez la commande suivante à partir de l'hôte OSD :
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      Vous pouvez afficher le résumé des requêtes récentes les plus lentes :
     </para>
<screen><prompt>root # </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      Vous pouvez trouver l'emplacement d'un OSD avec :
     </para>
<screen><prompt>root # </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Une ou plusieurs requêtes OSD sont bloquées depuis très longtemps. Cela indique que la grappe n'a pas été intègre pendant une période prolongée (par exemple, en raison du faible nombre d'OSD actif) ou que l'OSD a un problème interne.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      Un ou plusieurs groupes de placement n'ont pas été nettoyés récemment (reportez-vous à la <xref linkend="scrubbing"/>). Les groupes de placement sont normalement nettoyés toutes les <option>mon_scrub_interval</option> secondes ; cet avertissement se déclenche lorsque <option>mon_warn_not_scrubbed</option> intervalles se sont écoulés sans nettoyage. Les groupes de placement ne sont pas nettoyés s'ils ne sont pas marqués comme propres, ce qui peut arriver s'ils sont mal placés ou altérés (voir PG_AVAILABILITY et PG_DEGRADED ci-dessus). Vous pouvez lancer manuellement le nettoyage d'un groupe de placement :
     </para>
<screen><prompt>root # </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Un ou plusieurs groupes de placement n'ont pas été nettoyés en profondeur récemment (reportez-vous à la <xref linkend="scrubbing"/>). Les groupes de placement sont normalement nettoyés toutes les <option>osd_deep_mon_scrub_interval</option> secondes ; cet avertissement se déclenche lorsque <option>mon_warn_not_deep_scrubbed</option> intervalles se sont écoulés sans nettoyage. Les groupes de placement n'ont pas été nettoyés (en profondeur) s'ils ne sont pas marqués comme propres, ce qui peut arriver s'ils sont mal placés ou altérés (voir PG_AVAILABILITY et PG_DEGRADED ci-dessus). Vous pouvez lancer manuellement le nettoyage d'un groupe de placement :
     </para>
<screen><prompt>root # </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Si vous avez choisi des emplacements autres que ceux par défaut pour votre configuration ou votre trousseau de clés, vous pouvez les indiquer ici :
   </para>
<screen><prompt>root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.watch">
  <title>Observation d'une grappe</title>

  <para>
   La commande <command>ceph -s</command> permet de connaître l'état immédiat de la grappe. Par exemple, une grappe Ceph minuscule composée d'un moniteur et de deux OSD peut produire le résultat suivant lorsqu'une charge de travail est en cours d'exécution :
  </para>

<screen>cluster:
  id:     6586341d-4565-3755-a4fd-b50f51bee248
  health: HEALTH_OK

services:
  mon: 3 daemons, quorum blueshark1,blueshark2,blueshark3
  mgr: blueshark3(active), standbys: blueshark2, blueshark1
  osd: 15 osds: 15 up, 15 in

data:
  pools:   8 pools, 340 pgs
  objects: 537 objects, 1985 MB
  usage:   23881 MB used, 5571 GB / 5595 GB avail
  pgs:     340 active+clean

io:
  client:   100 MB/s rd, 26256 op/s rd, 0 op/s wr</screen>

  <para>
   La sortie fournit les informations suivantes :
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     ID de grappe
    </para>
   </listitem>
   <listitem>
    <para>
     État d'intégrité de la grappe
    </para>
   </listitem>
   <listitem>
    <para>
     Époque d'assignation du moniteur et état du quorum du moniteur
    </para>
   </listitem>
   <listitem>
    <para>
     Époque d'assignation des OSD et état des OSD
    </para>
   </listitem>
   <listitem>
    <para>
     Version d'assignation des groupes de placement
    </para>
   </listitem>
   <listitem>
    <para>
     Nombre de groupes de placement et de réserves
    </para>
   </listitem>
   <listitem>
    <para>
     Quantité <emphasis>théorique</emphasis> de données stockées et nombre d'objets stockés
    </para>
   </listitem>
   <listitem>
    <para>
     Quantité totale de données stockées
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>méthode utilisée par Ceph pour calculer l'utilisation des données</title>
   <para>
    La valeur de <literal>used</literal> reflète la quantité réelle de stockage brut utilisée. La valeur de <literal>xxx Go/xxx Go</literal> désigne la quantité disponible de la capacité de stockage globale de la grappe (la quantité disponible correspond à la valeur inférieure). Le nombre théorique reflète la taille des données stockées avant qu'elles soient répliquées ou clonées ou qu'elles fassent l'objet d'un instantané. Par conséquent, la quantité de données réellement stockée dépasse généralement la quantité théorique stockée, car Ceph crée des répliques des données et peut également utiliser la capacité de stockage pour le clonage et la création d'instantanés.
   </para>
  </tip>

  <para>
   Les autres commandes affichant des informations d'état immédiat sont les suivantes :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Pour obtenir les informations mises à jour en temps réel, placez l'une de ces commandes (y compris <command>ceph -s</command>) dans une boucle d'attente, par exemple :
  </para>

<screen><systemitem class="username">root</systemitem>while true ; do ceph -s ; sleep 10 ; done</screen>

  <para>
   Appuyez sur <keycombo><keycap function="control"/><keycap>C</keycap></keycombo> pour refermer la sortie de la commande.
  </para>
 </sect1>
 <sect1 xml:id="monitor.stats">
  <title>Vérification des statistiques d'utilisation d'une grappe</title>

  <para>
   L'option <command>df</command> vous permet de vérifier l'utilisation des données d'une grappe et la répartition des données entre les grappes. Elle est comparable à l'option <command>df</command> de Linux. Exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph df
GLOBAL:
    SIZE       AVAIL      RAW USED     %RAW USED
    55886G     55826G       61731M          0.11
POOLS:
    NAME         ID     USED      %USED     MAX AVAIL     OBJECTS
    testpool     1          0         0        17676G           0
    ecpool       2      4077M      0.01        35352G        2102
    test1        3          0         0        17676G           0
    rbd          4         16         0        17676G           3
    rbd1         5         16         0        17676G           3
    ecpool1      6      5708M      0.02        35352G        2871</screen>

  <para>
   La section <literal>GLOBAL</literal> donne un aperçu de la quantité de stockage utilisée pour vos données par la grappe.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>SIZE</literal> : capacité de stockage globale de la grappe.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal> : quantité d'espace disponible dans la grappe.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal> : capacité de stockage brute utilisée.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal> : pourcentage de stockage brut utilisé. Utilisez ce nombre avec le ratio <literal>full</literal> et le ratio <literal>near full</literal> pour vous assurer que vous n'atteignez pas la capacité de votre grappe. Reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/mon-config-ref#storage-capacit">Capacité de stockage</link> pour plus de détails.
    </para>
    <note>
     <title>Niveau de remplissage de grappe</title>
     <para>
      Un niveau de remplissage de stockage brut de 70 % à 80 % indique qu'un nouveau stockage doit être ajouté à la grappe. Une utilisation plus élevée peut conduire à la saturation de certains OSD et à des problèmes d'intégrité de la grappe.
     </para>
     <para>
      Utilisez la commande <command>ceph osd df tree</command> pour établir la liste de niveau de remplissage de tous les OSD.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   La section <literal>POOLS</literal> de la sortie fournit la liste des réserves et l'utilisation théorique de chaque réserve. La sortie de cette section ne reflète <emphasis>pas</emphasis> les répliques, les clones ou les instantanés existants. Par exemple, si vous stockez un objet de 1 Mo de données, l'utilisation théorique est de 1 Mo, mais l'utilisation réelle peut être de 2 Mo ou plus selon le nombre de répliques, de clones et d'instantanés.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>NAME</literal> : nom de la réserve.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal> : identifiant de la réserve.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal> : quantité théorique de données stockées en kilo-octets, sauf si M (pour mégaoctets) ou G (pour gigaoctets) est adjoint à la valeur numérique.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% USED</literal> : pourcentage de stockage théorique utilisé par réserve.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal> : espace maximal disponible dans la réserve indiquée.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal> : nombre théorique d'objets stockés par réserve.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    Les nombres figurant dans la section POOLS sont théoriques. Ils n'incluent pas le nombre de répliques, d'instantanés ou de clones. Par conséquent, la somme des montants USED et %USED ne correspond pas aux montants RAW USED et %RAW USED dans la section %GLOBAL de la sortie.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor.status">
  <title>Vérification de l'état d'une grappe</title>

  <para>
   Pour vérifier l'état d'une grappe, exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph status</screen>

  <para>
   ou
  </para>

<screen><prompt>root # </prompt>ceph -s</screen>

  <para>
   En mode interactif, tapez <command>status</command> et appuyez sur <keycap function="enter"/>. 
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph permet d'imprimer l'état de la grappe. Par exemple, une grappe Ceph minuscule composée d'un moniteur et de deux OSD peut produire le résultat suivant :
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor.osdstatus">
  <title>Vérification de l'état des OSD</title>

  <para>
   Vous pouvez vérifier les OSD pour vous assurer qu'ils sont opérationnels et activés à l'aide de la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph osd stat</screen>

  <para>
   ou
  </para>

<screen><prompt>root # </prompt>ceph osd dump</screen>

  <para>
   Vous pouvez également afficher les OSD en fonction de leur position dans l'assignation CRUSH.
  </para>

<screen><prompt>root # </prompt>ceph osd tree</screen>

  <para>
   Ceph permet d'afficher une arborescence CRUSH avec un hôte, ses OSD, leur état et leur pondération.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage.bp.monitoring.fullosd">
  <title>Contrôle des OSD pleins</title>

  <para>
   Ceph vous empêche d'écrire sur un OSD plein afin de vous éviter de perdre des données. Pour une grappe opérationnelle, un message d'avertissement doit s'afficher lorsque celle-ci est sur le point d'atteindre son ratio complet. La valeur par défaut de <command>monosd full ratio</command> est 0.95, c'est-à-dire 95 % de la capacité au-delà de laquelle les clients ne peuvent plus écrire de données dans la grappe. La valeur par défaut de <command>monosd nearfull ratio</command> est de 0.85, c'est-à-dire 85 % de la capacité à partir de laquelle un message d'avertissement d'intégrité est émis.
  </para>

  <para>
   Les noeuds OSD pleins sont signalés par la commande <command>ceph health</command> :
  </para>

<screen>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   ou
  </para>

<screen>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   La meilleure façon de gérer une grappe pleine consiste à ajouter de nouveaux noeuds OSD permettant à la grappe de redistribuer les données l'espace de stockage nouvellement disponible.
  </para>

  <para>
   Si vous ne pouvez pas démarrer un OSD parce qu'il est plein, vous pouvez toujours effacer certaines données en supprimant des répertoires de groupes de placement dans l'OSD en question.
  </para>

  <tip>
   <title>Prévention de la saturation des OSD</title>
   <para>
    Un OSD plein utilise 100 % de son espace disque. Lorsqu'il atteint ce taux de remplissage, l'OSD se bloque sans avertissement. Voici quelques conseils à retenir lors de l'administration de noeuds OSD.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      L'espace disque de chaque OSD (généralement monté sous <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) doit être placé sur une partition ou un disque sous-jacent dédié.
     </para>
    </listitem>
    <listitem>
     <para>
      Vérifiez les fichiers de configuration Ceph et assurez-vous que Ceph ne stocke pas son fichier journal sur les partitions/disques dédiés aux OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Assurez-vous qu'aucun autre processus n'écrit sur les partitions/disques dédiés aux OSD.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor.monstatus">
  <title>Vérification de l'état du moniteur</title>

  <para>
   Si votre grappe possède plusieurs moniteurs (ce qui est probable), vous devez vérifier l'état du quorum des moniteurs après avoir démarré la grappe et avant de lire et/ou d'écrire des données. Un quorum doit être défini lorsque plusieurs moniteurs s'exécutent. Vous devez également vérifier périodiquement l'état des moniteurs pour vous assurer que ceux-ci fonctionnent.
  </para>

  <para>
   Pour afficher l'assignation des moniteurs, exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph mon stat</screen>

  <para>
   ou
  </para>

<screen><prompt>root # </prompt>ceph mon dump</screen>

  <para>
   Pour contrôler l'état du quorum de la grappe de moniteurs, exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph quorum_status</screen>

  <para>
   Ceph renvoie l'état du quorum. Par exemple, une grappe Ceph composée de trois moniteurs peut renvoyer les éléments suivants :
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "127.0.0.1:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "127.0.0.1:6790\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "127.0.0.1:6791\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor.pgroupstatus">
  <title>Vérification des états des groupes de placement</title>

  <para>
   Les groupes de placement assignent des objets aux OSD. Lorsque vous surveillez vos groupes de placement, vous voulez qu'ils soient <literal>actifs</literal> (active) et <literal>propres </literal> (clean). Pour plus d'informations, reportez-vous à la section <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/monitoring-osd-pg">Surveillance des OSD et des groupes de placement</link>.
  </para>
 </sect1>
 <sect1 xml:id="monitor.adminsocket">
  <title>Utilisation du socket Admin</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark> Le socket admin Ceph permet d'interroger un daemon via une interface de socket. Par défaut, les sockets Ceph résident sous <filename>/var/run/ceph</filename>. Pour accéder à un daemon via le socket admin, connectez-vous à l'hôte qui exécute le daemon et exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   Pour afficher les commandes du socket admin disponibles, exécutez la commande suivante :
  </para>

<screen><prompt>root # </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   La commande admin socket vous permet d'afficher et de définir votre configuration lors de l'exécution. Reportez-vous à <link xlink:href="http://docs.ceph.com/docs/master/rados/configuration/ceph-conf#ceph-runtime-config">Affichage d'une configuration lors de l'exécution</link> pour plus de détails.
  </para>

  <para>
   En outre, vous pouvez définir directement les valeurs de configuration lors de l'exécution (le socket admin ignore le moniteur, contrairement à <command>ceph tell</command> <replaceable>type-daemon</replaceable>.<replaceable>id</replaceable> injectargs, qui s'appuie sur le moniteur, mais ne vous oblige pas à vous connecter directement à l'hôte en question).
  </para>
 </sect1>
</chapter>
