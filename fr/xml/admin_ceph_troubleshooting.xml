<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_troubleshooting.xml" version="5.0" xml:id="storage.troubleshooting">
 <title>Dépannage</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>oui</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ce chapitre décrit les divers problèmes que vous pouvez rencontrer lorsque vous utilisez une grappe Ceph.
 </para>
 <sect1 xml:id="storage.bp.report_bug">
  <title>Déclaration des problèmes de logiciel</title>

  <para>
   Si vous rencontrez un problème lors de l'exécution de SUSE Enterprise Storage lié à certains de ses composants, tels que Ceph ou Object Gateway, signalez le problème au support technique de SUSE. Il est recommandé d'exécuter l'utilitaire <command>supportconfig</command>.
  </para>

  <tip>
   <para>
    Étant donné que <command>supportconfig</command> est un logiciel modulaire, assurez-vous que le paquetage <systemitem>supportutils-plug-in-ses</systemitem> est installé.
   </para>
<screen>rpm -q supportutils-plugin-ses</screen>
   <para>
    S'il est manquant sur le serveur Ceph, installez-le avec
   </para>
<screen>zypper ref &amp;&amp; zypper in supportutils-plugin-ses</screen>
  </tip>

  <para>
   Vous pouvez exécuter <command>supportconfig</command> sur la ligne de commande, mais nous vous recommandons d'utiliser le module YaST connexe. Pour plus d'informations sur <command>supportconfig</command>, reportez-vous à la page <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_admin/book_sle_admin.html#sec.admsupport.supportconfig"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.cluster_mntc.rados_striping">
  <title>L'envoi d'objets de grande taille à l'aide de <command>rados</command> échoue avec l'OSD complet</title>

  <para>
   <command>rados</command> est un utilitaire de ligne de commande permettant de gérer le stockage d'objets RADOS. Pour plus d'informations, reportez-vous à la page <command>man 8 rados</command>.
  </para>

  <para>
   Si vous envoyez un objet volumineux à une grappe Ceph à l'aide de l'utilitaire <command>rados</command> ainsi
  </para>

<screen>rados -p mypool put myobject /file/to/send</screen>

  <para>
   il peut remplir tout l'espace OSD associé et peser lourdement sur les performances de la grappe.
  </para>
 </sect1>
 <sect1 xml:id="ceph.xfs.corruption">
  <title>Système de fichiers XFS endommagé</title>

  <para>
   Dans de rares circonstances, comme un bogue du kernel ou un matériel désactivé/mal configuré, le système de fichiers sous-jacent (XFS) dans lequel un OSD stocke ses données peut être endommagé et impossible à monter.
  </para>

  <para>
   Si vous êtes certain que le matériel ne pose pas de problème et que le système est correctement configuré, signalez un bogue sur le sous-système XFS du kernel SUSE Linux Enterprise Server et marquez l'OSD comme suit :
  </para>

<screen>ceph osd down <replaceable>OSD identification</replaceable></screen>

  <warning>
   <title>non-intervention (formatage ou modification) sur le périphérique endommagé</title>
   <para>
    Évitez d'exécuter <command>xfs_repair</command> pour résoudre le problème sur le système de fichiers, car cette commande modifie le système de fichiers. L'OSD peut démarrer, mais son fonctionnement peut être influencé.
   </para>
  </warning>

  <para>
   Effacez le disque sous-jacent et recréez l'OSD en exécutant :
  </para>

<screen>ceph-disk prepare --zap $OSD_DISK_DEVICE $OSD_JOURNAL_DEVICE"</screen>

  <para>
   Par exemple :
  </para>

<screen>ceph-disk prepare --zap /dev/sdb /dev/sdd2</screen>
 </sect1>
 <sect1 xml:id="storage.bp.recover.toomanypgs">
  <title>Message d'état « Too Many PGs per OSD » (« Trop de groupes de placement par OSD »)</title>

  <para>
   Si le message <literal>Too Many PGs per OSD</literal> (« Trop de groupes de placement par OSD ») s'affiche à l'issue de l'exécution de <command>ceph status</command>, cela signifie que la valeur de <option>mon_pg_warn_max_per_osd</option> (300 par défaut) a été dépassée. Cette valeur est comparée au nombre de groupes de placement par ratio OSD. Cela signifie que la configuration de la grappe n'est pas optimale.
  </para>

  <para>
   Le nombre de groupes de placement ne peut pas être réduit après la création de la réserve. Les réserves qui ne contiennent pas encore de données peuvent être supprimées en toute sécurité, puis recréées avec un nombre inférieur de groupes de placement. Lorsque les réserves contiennent déjà des données, la seule solution consiste à ajouter des OSD à la grappe afin que le ratio de groupes de placement par OSD diminue.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.stuckinactive">
  <title>Message d'état « <emphasis>nn</emphasis> pg stuck inactive » (« nn pg bloqué inactif »)</title>

  <para>
   Si le message d'état <literal>stuck inactive</literal> (bloqué inactif) s'affiche à l'issue de l'exécution de <command>ceph status</command>, cela signifie que Ceph ne sait pas où répliquer les données stockées pour satisfaire aux règles de réplication. Cela peut se produire peu après la configuration initiale de Ceph et se corriger automatiquement. Dans d'autres cas, cela peut nécessiter une interaction manuelle, comme la remise en service d'un OSD désactivé ou l'ajout d'un nouvel OSD à la grappe. Dans de très rares cas, il peut être judicieux de réduire le niveau de réplication.
  </para>

  <para>
   Si les groupes de placement sont bloqués en permanence, vérifiez la sortie de <command>ceph osd tree</command>. La sortie doit être structurée en arborescence, comme dans l'exemple de la <xref linkend="storage.bp.recover.osddown"/>.
  </para>

  <para>
   Si la sortie de <command>ceph osd tree</command> est plutôt plate comme dans l'exemple suivant
  </para>

<screen>ceph osd tree
ID WEIGHT TYPE NAME    UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1      0 root default
 0      0 osd.0             up  1.00000          1.00000
 1      0 osd.1             up  1.00000          1.00000
 2      0 osd.2             up  1.00000          1.00000</screen>

  <para>
   vous devez vérifier que l'assignation CRUSH associée possède une structure en arborescence. Si elle est également plate ou ne comporte pas d'hôtes comme dans l'exemple ci-dessus, cela peut signifier que la résolution de noms d'hôte ne fonctionne pas correctement dans la grappe.
  </para>

  <para>
   Si la hiérarchie est incorrecte (par exemple, la racine contient des hôtes, mais que les OSD résident au niveau supérieur et ne sont pas eux-mêmes assignés aux hôtes), vous devez déplacer les OSD à l'emplacement correct dans la hiérarchie. Cela peut s'effectuer à l'aide de la commande <command>ceph osd crush move</command> et/ou de la commande <command>ceph osd crush set</command>. Pour plus de détails reportez-vous à la <xref linkend="op.crush"/>.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.osdweight">
  <title>La pondération OSD est nulle</title>

  <para>
   Lorsqu'un OSD démarre, une pondération lui est assignée. Plus la pondération est élevée, plus grande est la probabilité que la grappe écrive des données dans l'OSD. La pondération est soit indiquée dans une assignation CRUSH de grappe, soit calculée par le script de démarrage des OSD.
  </para>

  <para>
   Dans certains cas, la valeur calculée de la pondération des OSD peut être arrondie à zéro. Cela signifie que l'OSD n'est pas programmé pour stocker des données et qu'aucune donnée n'y est écrite. En effet, le disque est généralement trop petit (d'une capacité inférieure à 15 Go) et doit être remplacé par un disque de capacité supérieure.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.osddown">
  <title>L'OSD est en panne</title>

  <para>
   Le daemon OSD est en cours d'exécution ou arrêté/en panne. L'inactivité d'un OSD peut être causée par l'une des trois raisons principales :
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Échec du disque dur.
    </para>
   </listitem>
   <listitem>
    <para>
     L'OSD est tombé en panne.
    </para>
   </listitem>
   <listitem>
    <para>
     Le serveur est tombé en panne.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Pour connaître l'état détaillé des OSD, exécutez
  </para>

<screen>ceph osd tree
# id  weight  type name up/down reweight
 -1    0.02998  root default
 -2    0.009995   host doc-ceph1
 0     0.009995      osd.0 up  1
 -3    0.009995   host doc-ceph2
 1     0.009995      osd.1 up  1
 -4    0.009995   host doc-ceph3
 2     0.009995      osd.2 down  1</screen>

  <para>
   La liste exemple indique que l'OSD <literal>osd.2</literal> est arrêté. Vous pouvez ensuite vérifier si le disque sur lequel réside l'OSD est monté :
  </para>

<screen>lsblk -f
 [...]
 vdb
 ├─vdb1               /var/lib/ceph/osd/ceph-2
 └─vdb2</screen>

  <para>
   Pour connaître la raison de l'arrêt de l'OSD, vous pouvez examiner son fichier journal <filename>/var/log/ceph/ceph-osd.2.log</filename>. Après avoir identifié la cause du non-fonctionnement de l'OSD et résolu le problème, démarrez celui-ci avec
  </para>

<screen>sudo systemctl start ceph-osd@2.service</screen>

  <para>
   N'oubliez pas de remplacer <literal>2</literal> par le numéro réel de votre OSD arrêté.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.performance.slowosd">
  <title>Recherche des OSD lents</title>

  <para>
   Lors de l'optimisation des performances de la grappe, il est très important d'identifier les systèmes de stockage/OSD lents dans la grappe. En effet, si les données sont écrites sur le disque (le plus) lent, l'opération d'écriture complète ralentit, car elle attend toujours d'être achevée sur tous les disques associés.
  </para>

  <para>
   Il n'est pas anodin de localiser le goulot d'étranglement de stockage. Vous devez examiner chaque OSD afin de trouver ceux qui ralentissent le processus d'écriture. Pour lancer un banc d'essai sur un même OSD, exécutez :
  </para>

<screen role="ceph_tell_osd_bench"><command>ceph tell</command> osd.<replaceable>OSD_ID_NUMBER</replaceable> bench</screen>

  <para>
   Par exemple :
  </para>

<screen><prompt>root # </prompt>ceph tell osd.0 bench
 { "bytes_written": 1073741824,
   "blocksize": 4194304,
   "bytes_per_sec": "19377779.000000"}</screen>

  <para>
   Vous devez ensuite exécuter cette commande sur chaque OSD et comparer la valeur de <literal>bytes_per_sec</literal> pour identifier les OSD (les plus) lents.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.recover.clockskew">
  <title>Correction des avertissements liés aux décalages d'horloge</title>

  <para>
   Les informations de temps doivent être synchronisées sur tous les noeuds de grappe. Si l'heure d'un noeud n'est pas entièrement synchronisée, vous pouvez obtenir des avertissements de décalage d'horloge lors de la vérification de l'état de la grappe.
  </para>

  <para>
   La synchronisation de l'heure est gérée avec NTP (voir page <link xlink:href="http://en.wikipedia.org/wiki/Network_Time_Protocol"/>). Réglez chaque noeud pour synchroniser l'heure avec un ou plusieurs serveurs NTP appartenant de préférence au même groupe NTP. Si le décalage d'horloge perdure sur un noeud, corrigez-le comme suit :
  </para>

<screen>systemctl stop ntpd.service
systemctl stop ceph-mon.target
systemctl start ntpd.service
systemctl start ceph-mon.target</screen>

  <para>
   Vous pouvez ensuite interroger les homologues NTP et vérifier le décalage horaire avec <command>sudo ntpq -p</command>.
  </para>

  <para>
   Les moniteurs Ceph doivent synchroniser leurs horloges à moins de 0,05 seconde l'une de l'autre. Reportez-vous à la <xref linkend="Cluster_Time_Setting"/> pour plus d'informations.
  </para>
 </sect1>
 <sect1 xml:id="storage.bp.performance.net_issues">
  <title>Performances médiocres de la grappe en raison de problèmes sur le réseau</title>

  <para>
   Les faibles performances d'une grappe peuvent être dues à plusieurs facteurs, notamment à des problèmes réseau. Dans ce cas, vous pouvez remarquer que la grappe atteint le quorum, que les noeuds des OSD et de moniteur sont déconnectés, que les transferts de données sont lents ou que de nombreuses tentatives de reconnexion se produisent.
  </para>

  <para>
   Pour vérifier si les performances de la grappe sont pénalisées par des problèmes réseau, consultez les fichiers journaux Ceph dans le répertoire <filename>/var/log/ceph</filename>.
  </para>

  <para>
   Pour résoudre les problèmes réseau sur la grappe, intéressez-vous aux points suivants :
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Diagnostics réseau de base. Essayez l'exécuteur des outils de diagnostic DeepSea <literal>net.ping</literal> entre les noeuds de grappe pour vérifier si l'interface individuelle peut atteindre une interface spécifique et le temps de réponse moyen. Tout temps de réponse nettement supérieur à la moyenne sera également signalé. Par exemple :
    </para>
<screen><prompt>root@master # </prompt>salt-run net.ping
  Succeeded: 8 addresses from 7 minions average rtt 0.15 ms</screen>
    <para>
     Essayez de valider toute l'interface avec JumboFrame :
    </para>
<screen><prompt>root@master # </prompt>salt-run net.jumbo_ping
  Succeeded: 8 addresses from 7 minions average rtt 0.26 ms</screen>
   </listitem>
   <listitem>
    <para>
     Évaluation des performances du réseau Essayez l'exécuteur de performances réseau de DeepSea, <literal>net.iperf</literal> afin de tester la bande passante réseau du noeud interne. Chaque noeud de grappe donné possède un certain nombre de processus <command>iperf</command> qui sont lancés en tant que serveurs (en fonction du nombre de coeurs de processeur). Les noeuds de grappe restants seront utilisés en tant que clients pour générer du trafic réseau. La bande passante cumulée de tous les processus <command>iperf</command> au niveau de chaque noeud est signalée. Cela doit refléter le débit réseau maximal pouvant être atteint sur tous les noeuds de grappe. Par exemple :
    </para>
<screen><prompt>root@master # </prompt>salt-run net.iperf cluster=ceph output=full
192.168.128.1:
    8644.0 Mbits/sec
192.168.128.2:
    10360.0 Mbits/sec
192.168.128.3:
    9336.0 Mbits/sec
192.168.128.4:
    9588.56 Mbits/sec
192.168.128.5:
    10187.0 Mbits/sec
192.168.128.6:
    10465.0 Mbits/sec</screen>
   </listitem>
   <listitem>
    <para>
     Vérifiez les paramètres du pare-feu sur les noeuds de la grappe. Assurez-vous qu'ils ne bloquent pas les ports/protocoles requis pour le fonctionnement de Ceph. Pour plus d'informations sur les paramètres du pare-feu, reportez-vous à la <xref linkend="storage.bp.net.firewall"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Vérifiez que le matériel réseau, tel que les cartes réseau, les câbles et les commutateurs, fonctionnent correctement.
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>réseau distinct</title>
   <para>
    Pour garantir une communication réseau rapide et sécurisée entre les noeuds de grappe, configurez un réseau distinct utilisé exclusivement par les noeuds OSD et de moniteur de la grappe.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="trouble.jobcache">
  <title>Saturation de <filename>/var</filename></title>

  <para>
   Par défaut, Salt Master enregistre la valeur de retour de chaque tâche de chaque minion dans son <emphasis>cache de travail</emphasis>. Le cache peut alors être utilisé ultérieurement pour les résultats de recherche des tâches précédentes. Le répertoire de cache par défaut est <filename>/var/cache/salt/master/jobs/</filename>. 
  </para>

  <para>
   Chaque retour de chaque minion est enregistré dans un fichier unique. Avec le temps, ce répertoire peut devenir très volumineux, en fonction du nombre de travaux publiés et de la valeur de l'option <option>keep_jobs</option> dans le fichier <filename>/etc/salt/master</filename>. <option>keep_jobs</option> définit le nombre d'heures (24 par défaut) pour conserver les informations sur les tâches minion passées.
  </para>

<screen>keep_jobs: 24</screen>

  <important>
   <title>ne définissez pas <option>keep_jobs: 0</option></title>
   <para>
    Si <option>keep_jobs</option> est défini à « 0 », le nettoyeur du cache de travail ne s'exécute <emphasis>jamais</emphasis>, ce qui risque de saturer la partition.
   </para>
  </important>

  <para>
   Si vous souhaitez désactiver le cache de travail, définissez <option>job_cache</option> sur « False » :
  </para>

<screen>job_cache: False</screen>

  <tip>
   <title>Restauration de la partition remplie à cause du cache de travail</title>
   <para>
    Lorsque la partition comportant les fichiers du cache de travail est pleine en raison de la définition inadaptée du paramètre <option>keep_jobs</option>, procédez comme suit pour libérer de l'espace disque et améliorer les paramètres du cache de travail :
   </para>
   <procedure>
    <step>
     <para>
      Arrêtez le service Salt Master :
     </para>
<screen><prompt>root@master # </prompt>systemctl stop salt-master</screen>
    </step>
    <step>
     <para>
      Modifiez la configuration de Salt Master associée au cache de travail dans <filename>/etc/salt/master</filename> :
     </para>
<screen>job_cache: False
keep_jobs: 1</screen>
    </step>
    <step>
     <para>
      Effacez le cache de travail de Salt Master :
     </para>
<screen>rm -rfv /var/cache/salt/master/jobs/*</screen>
    </step>
    <step>
     <para>
      Démarrez le service Salt Master :
     </para>
<screen><prompt>root@master # </prompt>systemctl start salt-master</screen>
    </step>
   </procedure>
  </tip>
 </sect1>
</chapter>
