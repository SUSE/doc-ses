<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha : exportation de données Ceph via NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modification</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>oui</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha est un serveur NFS (voir <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Partage de systèmes de fichiers avec NFS</link>) qui s'exécute dans un espace d'adressage utilisateur et non pas dans le kernel du système d'exploitation. Avec NFS Ganesha, vous pouvez brancher votre propre mécanisme de stockage, tel que Ceph, et y accéder depuis n'importe quel client NFS.
 </para>
 <para>
  Les compartiments S3 sont exportés vers NFS suivant l'utilisateur, par exemple via le chemin <filename><replaceable>NOEUD_GANESHA:</replaceable>/<replaceable>NOM_UTILISATEUR</replaceable>/<replaceable>NOM_COMPARTIMENT</replaceable></filename>.
 </para>
 <para>
  Un CephFS est exporté par défaut via le chemin <filename><replaceable>NOEUD_GANESHA:</replaceable>/cephfs</filename>.
 </para>
 <note>
  <title>performances de NFS Ganesha</title>
  <para>
   En raison du overhead accru du protocole et de la latence supplémentaire causée par des sauts de réseau additionnels entre le client et le stockage, l'accès à Ceph via NFS Gateway peut réduire considérablement les performances de l'application par rapport aux clients CephFS ou Object Gateway natifs .
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Installation</title>

  <para>
   Pour connaître la procédure d'installation, reportez-vous au <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configuration</title>

  <para>
   Pour obtenir la liste de tous les paramètres disponibles dans le fichier de configuration, reportez-vous à :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> pour les options de la couche FSAL (couche d'abstraction du système de fichiers) de CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> pour les options FSAL d'Object Gateway.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Cette section vous aide à configurer le serveur NFS Ganesha pour exporter les données de grappe accessibles via Object Gateway et CephFS.
  </para>

  <para>
   La configuration de NFS Ganesha se compose de deux parties : configuration de service et configuration d'exportation. La configuration de service est contrôlée par <filename>/etc/ganesha/ganesha.conf</filename>. Notez que les modifications apportées à ce fichier sont remplacées lors de la phase 4 de DeepSea. Pour modifier les paramètres durablement, modifiez le fichier <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> situé sur Salt Master. La configuration d'exportation est stockée dans la grappe Ceph sous forme d'objets RADOS.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Configuration de service</title>
   <para>
    La configuration de service est stockée dans le fichier <filename>/etc/ganesha/ganesha.conf</filename> et contrôle tous les paramètres de daemon NFS Ganesha, y compris l'emplacement de stockage de la configuration d'exportation dans la grappe Ceph. Notez que les modifications apportées à ce fichier sont remplacées lors de la phase 4 de DeepSea. Pour modifier les paramètres durablement, modifiez le fichier <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> situé sur Salt Master.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>Section RADOS_URLS</title>
    <para>
     La section <literal>RADOS_URLS</literal> configure l'accès de la grappe Ceph pour la lecture de la configuration de NFS Ganesha à partir d'objets RADOS.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        Emplacement du fichier de configuration Ceph.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        ID utilisateur de cephx.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        URL d'objet RADOS pour surveiller les notifications de rechargement.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>Section RGW</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Pointe vers le fichier <filename>ceph.conf</filename>. Lors du déploiement réalisé avec DeepSea, il n'est pas nécessaire de modifier cette valeur.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        Nom de l'utilisateur client Ceph employé par NFS Ganesha.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Nom de la grappe Ceph. SUSE Enterprise Storage 6 ne prend actuellement en charge qu'un nom de grappe, <literal>ceph</literal> par défaut.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>URL d'objet RADOS</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     NFS Ganesha prend en charge la lecture de la configuration à partir d'un objet RADOS. La directive <literal>%url</literal> permet de spécifier une URL RADOS qui identifie l'emplacement de l'objet RADOS.
    </para>
    <para>
     Une URL RADOS peut présenter deux formes : <literal>rados://&lt;RÉSERVE&gt;/&lt;OBJET&gt;</literal> ou <literal>rados://&lt;RÉSERVE&gt;/&lt;ESPACE_NOMS&gt;/&lt;OBJET&gt;</literal>, où <literal>RÉSERVE</literal> correspond à la réserve RADOS dans laquelle l'objet est stocké, <literal>ESPACE_NOMS</literal> est l'espace de noms de la réserve dans lequel l'objet est stocké et <literal>OBJET</literal> indique le nom de l'objet.
    </para>
    <para>
     Pour prendre en charge les fonctionnalités de gestion de NFS Ganesha de Ceph Dashboard, vous devez suivre une convention concernant le nom de l'objet RADOS pour chaque daemon de service. Le nom de l'objet doit prendre la forme <literal>conf-<replaceable>ID_MINION</replaceable></literal> où ID_MINION correspond à l'ID du minion Salt du noeud sur lequel ce service est en cours d'exécution.
    </para>
    <para>
     DeepSea s'occupe déjà de générer correctement cette URL, et vous n'avez pas besoin de faire de changement.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Changement des ports NFS Ganesha par défaut</title>
    <para>
     Par défaut, NFS Ganesha utilise le port 2049 pour NFS et le port 875 pour la prise en charge de rquota. Pour changer les numéros de port par défaut, utilisez les options <option>NFS_Port</option> et <option>RQUOTA_Port</option> de la section <literal>NFS_CORE_PARAM</literal>, par exemple :
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Configuration des exportations</title>
   <para>
    La configuration des exportations est stockée sous forme d'objets RADOS dans la grappe Ceph. Chaque bloc d'exportation est stocké dans son propre objet RADOS nommé <literal>export-&lt;ID&gt;</literal>, où <literal>&lt;ID&gt;</literal> doit correspondre à l'attribut <literal>Export_ID</literal> de la configuration d'exportation. L'association entre les exportations et les services NFS Ganesha se fait par le biais des objets <literal>conf-ID_MINION</literal>. Chaque objet de service contient une liste d'URL RADOS pour chaque exportation effectuée par ce service. Un bloc d'exportation ressemble à ceci :
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    Afin de créer l'objet RADOS pour le bloc d'exportation ci-dessus, nous devons d'abord stocker le code du bloc d'exportation dans un fichier. Ensuite, nous pouvons utiliser l'outil CLI RADOS pour stocker le contenu du fichier précédemment enregistré dans un objet RADOS.
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    Une fois l'objet d'exportation créé, nous pouvons associer l'exportation à une instance de service en ajoutant l'URL RADOS correspondante de l'objet d'exportation à l'objet de service. Les sections suivantes décrivent comment configurer un bloc d'exportation.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Section EXPORT principale</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Chaque exportation doit disposer d'un « Export_Id » unique (obligatoire).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Chemin d'exportation dans la réserve CephFS associée (obligatoire). Cela permet l'exportation des sous-répertoires depuis CephFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Chemin d'exportation NFS cible (obligatoire pour NFSv4). Il définit le chemin d'exportation NFS sous lequel les données exportées sont disponibles.
       </para>
       <para>
        Exemple : avec la valeur <literal>/cephfs/</literal> et après exécution
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        Les données CephFS sont disponibles dans le répertoire <filename>/mnt/cephfs/</filename> du client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        « RO » pour un accès en lecture seule, « RW » pour un accès en lecture-écriture et « None » pour aucun accès.
       </para>
       <tip>
        <title>accès limité aux clients</title>
        <para>
         Si vous laissez <literal>Access_Type = RW</literal> dans la section <literal>EXPORT</literal> principale et limitez l'accès à un client spécifique dans la section <literal>CLIENT</literal>, d'autres clients pourront de toute façon se connecter. Pour désactiver l'accès à tous les clients et permettre l'accès à des clients spécifiques uniquement, définissez <literal>Access_Type = None</literal> dans la section <literal>EXPORT</literal>, puis spécifiez le mode d'accès moins restrictif pour un ou plusieurs clients dans la section <literal>CLIENT</literal> :
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        Option squash de NFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Exportation de la couche d'abstraction du système de fichiers (File System Abstraction Layer, FSAL). Reportez-vous à la <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>Sous-section FSAL</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Indique le nom du système dorsal utilisé par NFS Ganesha. Les valeurs autorisées sont <literal>CEPH</literal> pour CephFS ou <literal>RGW</literal> pour Object Gateway. Selon le choix, il est nécessaire de définir <literal>role-mds</literal> ou <literal>role-rgw</literal> dans le fichier <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Rôles NFS Ganesha personnalisés</title>

  <para>
   Il est possible de définir des rôles NFS Ganesha pour les noeuds de grappe. Ces rôles sont ensuite assignés aux noeuds via le fichier <filename>policy.cfg</filename>. Les rôles permettent :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     de séparer les noeuds NFS Ganesha pour l'accès à Object Gateway et CephFS ;
    </para>
   </listitem>
   <listitem>
    <para>
     d'assigner des utilisateurs Object Gateway différents aux noeuds NFS Ganesha.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Les noeuds NFS Ganesha disposent d'utilisateurs Object Gateway différents pour pouvoir accéder à des compartiments S3 différents. Les compartiments S3 peuvent être utilisés pour le contrôle d'accès. Remarque : les compartiments S3 ne doivent pas être confondus avec les compartiments Ceph utilisés dans la carte CRUSH.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Utilisateurs Object Gateway pour NFS Ganesha</title>
   <para>
    Cet exemple de procédure pour Salt Master montre comment créer deux rôles NFS Ganesha avec des utilisateurs Object Gateway différents. Il utilise les rôles <literal>gold</literal> et <literal>silver</literal> pour lesquels DeepSea fournit déjà des exemples de fichier de configuration.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Ouvrez le fichier <filename>/srv/pillar/ceph/stack/global.yml</filename> dans l'éditeur de votre choix. Créez le fichier s'il n'existe pas.
     </para>
    </step>
    <step>
     <para>
      Le fichier doit contenir les lignes suivantes :
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      Ces rôles peuvent être assignés ultérieurement dans le fichier <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Créez le fichier <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> et ajoutez-lui le contenu suivant :
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Créez un fichier <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> et ajoutez-lui les lignes suivantes :
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      À présent, les modèles de <filename>ganesha.conf</filename> doivent être créés pour chaque rôle. Le modèle original de DeepSea est une bonne base pour commencer. Créez deux copies :
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      Les nouveaux rôles requièrent des trousseaux pour pouvoir accéder à la grappe. Pour accorder l'accès, copiez le fichier <filename>ganesha.j2</filename> :
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copiez le trousseau de clés pour Object Gateway :
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway a également besoin de la configuration des différents rôles :
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Assignez les rôles nouvellement créés aux noeuds de grappe dans le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> :
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Remplacez <replaceable>NODE1</replaceable> et <replaceable>NODE2</replaceable> par les noms des noeuds auxquels vous voulez assigner les rôles.
     </para>
    </step>
    <step>
     <para>
      Exécutez les phases 0 à 4 de DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Séparation de la couche FSAL de CephFS et d'Object Gateway</title>
   <para>
    Cet exemple de procédure pour Salt Master montre comment créer deux rôles différents utilisant CephFS et Object Gateway :
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Ouvrez le fichier <filename>/srv/pillar/ceph/rgw.sls</filename> avec l'éditeur de votre choix. Créez le fichier s'il n'existe pas.
     </para>
    </step>
    <step>
     <para>
      Le fichier doit contenir les lignes suivantes :
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      Ces rôles peuvent être assignés ultérieurement dans le fichier <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      À présent, les modèles de <filename>ganesha.conf</filename> doivent être créés pour chaque rôle. Le modèle original de DeepSea est une bonne base pour commencer. Créez deux copies :
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Ouvrez le fichier <filename>ganesha_rgw.conf.j2</filename> pour supprimer la section suivante :
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Ouvrez le fichier <filename>ganesha_cfs.conf.j2</filename> pour supprimer la section suivante :
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Les nouveaux rôles requièrent des trousseaux pour pouvoir accéder à la grappe. Pour accorder l'accès, copiez le fichier <filename>ganesha.j2</filename> :
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      Vous pouvez supprimer la ligne <literal>caps mds = "allow *"</literal> du fichier <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copiez le trousseau de clés pour Object Gateway :
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      Object Gateway a besoin de la configuration du nouveau rôle :
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Assignez les rôles nouvellement créés aux noeuds de grappe dans le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> :
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Remplacez <replaceable>NODE1</replaceable> et <replaceable>NODE2</replaceable> par les noms des noeuds auxquels vous voulez assigner les rôles.
     </para>
    </step>
    <step>
     <para>
      Exécutez les phases 0 à 4 de DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Opérations prises en charge</title>
   <para>
    L'interface NFS RGW prend en charge la plupart des opérations sur les fichiers et les répertoires, avec les restrictions suivantes :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>Les liens, y compris les liens symboliques, ne sont pas pris en charge.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Les listes de contrôle d'accès (ACL) NFS ne sont pas prises en charge.</emphasis> La propriété et les autorisations d'utilisateur et de groupe Unix <emphasis>sont</emphasis> prises en charge.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Les répertoires ne peuvent pas être déplacés ou renommés,</emphasis> mais vous <emphasis>pouvez</emphasis> déplacer des fichiers entre les répertoires.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Seule l'écriture séquentielle complète des E/S est prise en charge.</emphasis> Par conséquent, les opérations d'écriture doivent forcément être des téléchargements. Beaucoup d'opérations classiques d'E/S, telles que l'édition des fichiers sur place, échoueront forcément étant donné qu'elles effectuent des enregistrements non séquentiels. Certains utilitaires de fichiers qui écrivent apparemment de façon séquentielle (par exemple, certaines versions de GNU <command>tar</command>) peuvent échouer en raison d'enregistrements non séquentiels occasionnels. Lors du montage via NFS, une E/S séquentielle d'une application peut généralement être forcée à d'effectuer des écritures séquentielles sur le serveur NFS via le montage synchrone (option <option>-o sync</option>). Les clients NFS qui ne peuvent pas effectuer de montage synchrone (par exemple, Microsoft Windows*) ne seront pas en mesure de télécharger des fichiers.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW prend en charge les opérations de lecture-écriture uniquement pour les blocs dont la taille est inférieure à 4 Mo.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Démarrage ou redémarrage de NFS Ganesha</title>

  <para>
   Pour activer et démarrer le service NFS Ganesha, exécutez :
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Redémarrez NFS Ganesha avec :
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   Lorsque NFS Ganesha est démarré ou redémarré, le délai de grâce est de 90 secondes pour NFS v4. Au cours de cette période bonus, les nouvelles requêtes provenant des clients sont activement rejetées. Par conséquent, les clients peuvent être confrontés au ralentissement des demandes lorsque NFS est en état de grâce.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Définition du niveau de journalisation</title>

  <para>
   Changez le niveau de débogage par défaut <literal>NIV_EVENT</literal> dans le fichier <filename>/etc/sysconfig/nfs-ganesha</filename>. Remplacez <literal>NIV_EVENT</literal> par <literal>NIV_DEBUG</literal> ou <literal>NIV_FULL_DEBUG</literal>. L'augmentation du niveau de détail de journalisation peut engendrer de grandes quantités de données dans les fichiers journaux.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   Le redémarrage du service est requis à l'issue de la modification du niveau de journalisation.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Vérification du partage NFS exporté</title>

  <para>
   Lors de l'utilisation NFS v3, vous pouvez vérifier si les partages NFS sont exportés sur le noeud du serveur NFS Ganesha :
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Montage du partage NFS exporté</title>

  <para>
   Pour monter le partage NFS exporté (tel que configuré à la <xref linkend="ceph-nfsganesha-config"/>) sur un hôte client, exécutez :
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
