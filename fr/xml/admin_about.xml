<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_about.xml" version="5.0" xml:id="cha-storage-about">
 <title>SUSE Enterprise Storage 6 et Ceph</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modification</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>oui</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  SUSE Enterprise Storage (SES) 6 est un système de stockage distribué conçu pour l'évolutivité, la fiabilité et les performances, et basé sur la technologie Ceph. Une grappe Ceph peut être exécutée sur des serveurs courants dans un réseau standard comme Ethernet. La grappe évolue facilement vers des milliers de serveurs (appelés plus tard noeuds) et dans la plage de pétaoctets. Contrairement aux systèmes conventionnels qui possèdent des tables d'allocation pour stocker et récupérer des données, Ceph utilise un algorithme déterministe pour allouer du stockage des données et ne dispose d'aucune structure centralisée des informations. Ceph suppose que, dans les grappes de stockage, l'ajout ou la suppression de matériel est la règle, et non l'exception. La grappe Ceph automatise les tâches de gestion telles que la distribution et la redistribution des données, la réplication des données, la détection des échecs et la récupération. Ceph peut à la fois s'autoréparer et s'autogérer, ce qui se traduit par une réduction des frais administratifs et budgétaires.
 </para>
 <para>
  Ce chapitre fournit un aperçu global de SUSE Enterprise Storage 6 et décrit brièvement les éléments les plus importants.
 </para>
 <tip>
  <para>
   Depuis SUSE Enterprise Storage 5, la seule méthode de déploiement de grappes est DeepSea. Reportez-vous au <xref linkend="ceph-install-saltstack"/> pour plus d'informations sur le processus de déploiement.
  </para>
 </tip>
 <sect1 xml:id="storage-intro-features">
  <title>Fonctions de Ceph</title>

  <para>
   L'environnement Ceph possède les fonctions suivantes :
  </para>

  <variablelist>
   <varlistentry>
    <term>Évolutivité</term>
    <listitem>
     <para>
      Ceph peut s'adapter à des milliers de noeuds et gérer le stockage dans la plage de pétaoctets.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Matériel courant</term>
    <listitem>
     <para>
      Aucun matériel spécial n'est requis pour exécuter une grappe Ceph. Pour plus d'informations, reportez-vous au <xref linkend="storage-bp-hwreq"/>
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Autogestion</term>
    <listitem>
     <para>
      La grappe Ceph est autogérée. Lorsque des noeuds sont ajoutés ou supprimés ou échouent, la grappe redistribue automatiquement les données. Elle a également connaissance des disques surchargés.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Aucun point d'échec unique</term>
    <listitem>
     <para>
      Aucun noeud d'une grappe ne stocke les informations importantes de manière isolée. Vous pouvez configurer le nombre de redondances.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Logiciels Open Source</term>
    <listitem>
     <para>
      Ceph est une solution logicielle Open Source et indépendante du matériel ou de fournisseurs spécifiques.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="storage-intro-core">
  <title>Composants de base</title>

  <para>
   Pour tirer pleinement parti de la puissance de Ceph, il est nécessaire de comprendre certains des composants et concepts de base. Cette section présente certaines parties de Ceph qui sont souvent référencées dans d'autres chapitres.
  </para>

  <sect2 xml:id="storage-intro-core-rados">
   <title>RADOS</title>
   <para>
    Le composant de base de Ceph est appelé <emphasis>RADOS</emphasis> <emphasis>(Reliable Autonomic Distributed Object Store)</emphasis> (Zone de stockage des objets distribués autonome fiable). Il est chargé de gérer les données stockées dans la grappe. Les données de Ceph sont généralement stockées en tant qu'objets. Chaque objet se compose d'un identificateur et des données.
   </para>
   <para>
    RADOS fournit les méthodes d'accès suivantes aux objets stockés qui couvrent de nombreux cas d'utilisation :
   </para>
   <variablelist>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway est une passerelle HTTP REST pour la zone de stockage des objets RADOS. Elle permet un accès direct aux objets stockés dans la grappe Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Périphérique de bloc RADOS</term>
     <listitem>
      <para>
       Les périphériques de bloc RADOS (RADOS Block Devices, RBD) sont accessibles comme n'importe quel autre périphérique de bloc. Ils peuvent être utilisés, par exemple, en combinaison avec <systemitem class="library">libvirt</systemitem> à des fins de virtualisation.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CephFS</term>
     <listitem>
      <para>
       Le système de fichiers Ceph est un système de fichiers compatible POSIX.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><systemitem class="library">librados</systemitem></term>
     <listitem>
      <para>
       <systemitem class="library">librados</systemitem> est une bibliothèque pouvant être utilisée avec de nombreux langages de programmation pour créer une application capable d'interagir directement avec la grappe de stockage.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    <systemitem class="library">librados</systemitem> est utilisé par Object Gateway et les RBD alors que CephFS s'interface directement avec RADOS <xref linkend="storage-intro-core-rados-figure"/>.
   </para>
   <figure xml:id="storage-intro-core-rados-figure">
    <title>Interfaces avec la zone de stockage des objets Ceph</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="rados-structure.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="rados-structure.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="storage-intro-core-crush">
   <title>CRUSH</title>
   <para>
    Au coeur d'une grappe Ceph se trouve l'algorithme <emphasis>CRUSH</emphasis>. CRUSH est l'acronyme de <emphasis>Controlled Replication Under Scalable Hashing</emphasis> (Réplication contrôlée sous hachage évolutif). CRUSH est une fonction qui gère l'allocation de stockage et nécessite relativement peu de paramètres. Cela signifie que seule une petite quantité d'informations est nécessaire pour calculer la position de stockage d'un objet. Les paramètres sont une carte actuelle de la grappe, comprenant l'état de santé, certaines règles de placement définies par l'administrateur et le nom de l'objet qui doit être stocké ou récupéré. Avec ces informations, tous les noeuds de la grappe Ceph sont en mesure de calculer l'emplacement auquel un objet et ses répliques sont stockés. Cela rend l'écriture ou la lecture des données très efficace. CRUSH tente de distribuer équitablement les données sur tous les noeuds de la grappe.
   </para>
   <para>
    La <emphasis>carte CRUSH</emphasis> contient tous les noeuds de stockage et les règles de placement définies par l'administrateur pour le stockage des objets dans la grappe. Elle définit une structure hiérarchique qui correspond généralement à la structure physique de la grappe. Par exemple, les disques contenant des données sont dans des hôtes, les hôtes sont dans des racks, les racks dans des rangées et les rangées dans des centres de données. Cette structure peut être utilisée pour définir des <emphasis>domaines de défaillance</emphasis>. Ceph s'assure ensuite que les réplications sont stockées sur différentes branches d'un domaine de défaillance spécifique.
   </para>
   <para>
    Si le domaine de défaillance est défini sur le niveau rack, les réplications d'objets sont réparties sur différents racks. Cela peut limiter les interruptions de service provoquées par un commutateur défaillant sur un rack. Si une unité de distribution de l'alimentation dessert une rangée de racks, le domaine de défaillance peut être défini sur rangée. En cas de défaillance de l'unité de distribution de l'alimentation, les données répliquées sont toujours disponibles sur les autres rangées.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-core-nodes">
   <title>Noeuds et daemons Ceph</title>
   <para>
    Dans Ceph, les noeuds sont des serveurs fonctionnant pour la grappe. Ils peuvent exécuter divers types de daemons. Il est recommandé de n'exécuter qu'un seul type de daemon sur chaque noeud, à l'exception des daemons Ceph Manager qui peuvent être colocalisés avec des daemons Ceph Monitor. Chaque grappe nécessite au moins des daemons Ceph Monitor, Ceph Manager, et Ceph OSD :
   </para>
   <variablelist>
    <varlistentry>
     <term>Noeud Admin</term>
     <listitem>
      <para>
       Le <emphasis>noeud Admin</emphasis> est un noeud de la grappe Ceph sur lequel le service Salt Master est en cours d'exécution. Le noeud Admin est un point central de la grappe Ceph, car il gère les autres noeuds de la grappe en interrogeant et en instruisant leurs services de minion Salt. En général, il inclut également d'autres services, par exemple l'interface utilisateur Web Ceph Dashboard avec le tableau de bord <emphasis>Grafana</emphasis> et le toolkit de surveillance <emphasis>Prometheus</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Monitor</term>
     <listitem>
      <para>
       Les noeuds <emphasis>Ceph Monitor</emphasis> (souvent abrégés en <emphasis>MON</emphasis>) gèrent des informations sur l'état de santé de la grappe, une carte de tous les noeuds et des règles de distribution des données (reportez-vous à la <xref linkend="storage-intro-core-crush"/>).
      </para>
      <para>
       En cas de défaillance ou de conflit, les noeuds Ceph Monitor de la grappe décident, à la majorité, des informations qui sont correctes. Pour former une majorité admissible, il est recommandé de disposer d'un nombre impair de noeuds Ceph Monitor et au minimum de trois d'entre eux.
      </para>
      <para>
       Si plusieurs sites sont utilisés, les noeuds Ceph Monitor doivent être distribués sur un nombre impair de sites. Le nombre de noeuds Ceph Monitor par site doit être tel que plus de 50 % des noeuds Ceph Monitor restent fonctionnels en cas de défaillance d'un site.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph Manager</term>
     <listitem>
      <para>
       Ceph Manager (MGR) collecte les informations d'état de l'ensemble de la grappe. Le daemon Ceph Manager s'exécute parallèlement aux daemons Ceph Monitor. Il fournit une surveillance supplémentaire et assure l'interface avec les systèmes de surveillance et de gestion externes.
      </para>
      <para>
       Ceph Manager ne requiert aucune configuration supplémentaire : il suffit de s'assurer qu'il s'exécute. Vous pouvez le déployer en tant que rôle distinct à l'aide de DeepSea.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Ceph OSD</term>
     <listitem>
      <para>
       <emphasis>Ceph OSD</emphasis> est un daemon qui gère des périphériques de stockage d'objets (<emphasis>Object Storage Devices</emphasis>, OSD) qui sont des unités de stockage physiques ou logiques (disques durs ou partitions). Les périphériques de stockage d'objets peuvent être des disques/partitions physiques ou des volumes logiques. Le daemon prend également en charge la réplication et le rééquilibrage des données en cas d'ajout ou de suppression de noeuds.
      </para>
      <para>
       Les daemons Ceph OSD communiquent avec les daemons Ceph Monitor et leur fournissent l'état des autres daemons OSD.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Pour utiliser CephFS, Object Gateway, NFS Ganesha ou la passerelle iSCSI, des noeuds supplémentaires sont requis :
   </para>
   <variablelist>
    <varlistentry>
     <term>Serveur de métadonnées (MDS)</term>
     <listitem>
      <para>
       Les serveurs de métadonnées stockent des métadonnées pour CephFS. En utilisant un MDS, vous pouvez exécuter des commandes du système de fichiers de base, par exemple <command>ls</command>, sans surcharger la grappe.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Object Gateway</term>
     <listitem>
      <para>
       Object Gateway est une passerelle HTTP REST pour la zone de stockage des objets RADOS. Elle est compatible avec OpenStack Swift et Amazon S3, et possède sa propre gestion des utilisateurs.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>NFS Ganesha</term>
     <listitem>
      <para>
       NFS Ganesha fournit un accès NFS à Object Gateway ou à CephFS. NFS Ganesha s'exécute dans l'espace utilisateur au lieu de l'espace kernel, et interagit directement avec Object Gateway ou CephFS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Passerelle iSCSI</term>
     <listitem>
      <para>
       iSCSI est un protocole de réseau de stockage qui permet aux clients d'envoyer des commandes SCSI à des périphériques de stockage SCSI (cibles) sur des serveurs distants.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Passerelle Samba</term>
     <listitem>
      <para>
       La passerelle Samba offre un accès SAMBA aux données stockées sur CephFS.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="storage-intro-structure">
  <title>Structure de stockage</title>

  <sect2 xml:id="storage-intro-structure-pool">
   <title>Pool</title>
   <para>
    Les objets qui sont stockés dans une grappe Ceph sont placés dans des <emphasis>réserves</emphasis>. Les réserves représentent les partitions logiques de la grappe vers le monde extérieur. Pour chaque réserve, un ensemble de règles peut être défini, par exemple, combien de réplications de chaque objet doivent exister. La configuration standard des réserves est appelée <emphasis>réserve répliquée</emphasis>.
   </para>
   <para>
    Les réserves contiennent généralement des objets, mais peuvent également être configurées pour agir de la même manière qu'un RAID 5. Dans cette configuration, les objets sont stockés sous forme de tranches avec d'autres tranches de codage. Les tranches de codage contiennent les informations redondantes. Le nombre de données et les tranches de codage peuvent être définis par l'administrateur. Dans cette configuration, les réserves sont appelées <emphasis>réserves codées à effacement</emphasis>.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-pg">
   <title>Groupe de placement</title>
   <para>
    Les <emphasis>groupes de placement</emphasis> (PG) sont utilisés pour la distribution des données au sein d'une réserve. Lorsque vous créez une réserve, un certain nombre de groupes de placement sont définis. Les groupes de placement sont utilisés en interne pour grouper des objets et sont un facteur important en termes de performances d'une grappe Ceph. Le groupe de placement d'un objet est déterminé par le nom de l'objet.
   </para>
  </sect2>

  <sect2 xml:id="storage-intro-structure-example">
   <title>Exemple</title>
   <para>
    Cette section fournit un exemple simplifié de la façon dont Ceph gère les données (reportez-vous à la <xref linkend="storage-intro-structure-example-figure"/>). Cet exemple ne représente pas une configuration recommandée pour une grappe Ceph. La configuration matérielle comprend trois noeuds de stockage ou noeuds Ceph OSD (<literal>Hôte 1</literal>, <literal>Hôte 2</literal>, <literal>Hôte 3</literal>). Chaque noeud comporte trois disques durs qui sont utilisés comme OSD (<literal>osd.1</literal> à <literal>osd.9</literal>). Les noeuds Ceph Monitor sont ignorés dans cet exemple.
   </para>
   <note>
    <title>différence entre Ceph OSD et OSD</title>
    <para>
     Alors que <emphasis>Ceph OSD</emphasis> ou <emphasis>daemon Ceph OSD</emphasis> fait référence à un daemon exécuté sur un noeud, le terme <emphasis>OSD</emphasis> fait référence au disque logique avec lequel le daemon interagit.
    </para>
   </note>
   <para>
    La grappe comporte deux réserves, <literal>Réserve A</literal> et <literal>Réserve B</literal>. Alors que Réserve A ne réplique les objets que deux fois, la résilience pour Réserve B est plus importante et comporte trois réplications pour chaque objet.
   </para>
   <para>
    Lorsqu'une application place un objet dans une réserve, par exemple via l'API REST, un groupe de placement (<literal>PG1</literal> à <literal>PG4</literal>) est sélectionné en fonction de la réserve et du nom de l'objet. L'algorithme CRUSH calcule ensuite les OSD sur lesquels l'objet est stocké, en fonction du groupe de placement qui contient l'objet.
   </para>
   <para>
    Dans cet exemple, le domaine de défaillance est défini sur le niveau hôte. Cela garantit que les réplications d'objets sont stockées sur des hôtes différents. En fonction du niveau de réplication défini pour une réserve, l'objet est stocké sur deux ou trois OSD utilisés par le groupe de placement.
   </para>
   <para>
    Une application qui écrit un objet interagit uniquement avec un Ceph OSD, le Ceph OSD primaire. Le Ceph OSD primaire se charge de la réplication et confirme l'achèvement du processus d'écriture une fois que tous les autres OSD ont stocké l'objet.
   </para>
   <para>
    Si <literal>osd.5</literal> échoue, tous les objets du <literal>PG1</literal> sont toujours disponibles sur <literal>osd.1</literal>. Dès que la grappe identifie un échec d'OSD, un autre OSD prend le relais. Dans cet exemple <literal>osd.4</literal> est utilisé en remplacement d'<literal>osd.5</literal>. Les objets stockés sur <literal>osd.1</literal> sont ensuite répliqués sur <literal>osd.4</literal> pour restaurer le niveau de la réplication.
   </para>
   <figure xml:id="storage-intro-structure-example-figure">
    <title>Exemple de Ceph à petite échelle</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="data-structure-example.svg" width="100%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="data-structure-example.png" width="100%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    Si un nouveau noeud avec de nouveaux OSD est ajouté à la grappe, la carte de la grappe va changer. La fonction CRUSH renvoie alors des emplacements différents pour les objets. Les objets qui reçoivent de nouveaux emplacements seront déplacés. Ce processus aboutit à une utilisation équilibrée de tous les OSD.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="about-bluestore">
  <title>BlueStore</title>

  <para>
   BlueStore est une nouvelle interface dorsale de stockage par défaut pour Ceph depuis la version 5 de SUSE Enterprise Storage. Il offre de meilleures performances que FileStore, une somme de contrôle de données complète et une compression intégrée.
  </para>

  <para>
   BlueStore gère un, deux ou trois périphériques de stockage. Dans le cas le plus simple, BlueStore utilise un seul périphérique de stockage primaire. Le périphérique de stockage est normalement partitionné en deux parties :
  </para>

  <orderedlist>
   <listitem>
    <para>
     Une petite partition nommée BlueFS qui met en oeuvre des fonctionnalités de type système de fichiers requises par RocksDB.
    </para>
   </listitem>
   <listitem>
    <para>
     Le reste du périphérique est généralement une grande partition occupant le reste du périphérique. Il est géré directement par BlueStore et contient toutes les données réelles. Ce périphérique primaire est normalement identifié par un lien symbolique de bloc dans le répertoire de données.
    </para>
   </listitem>
  </orderedlist>

  <para>
   Il est également possible de déployer BlueStore sur deux périphériques supplémentaires :
  </para>

  <para>
   Un <emphasis>périphérique WAL</emphasis> peut être utilisé pour le journal interne ou le journal d'écriture anticipée de BlueStore. Il est identifié par le lien symbolique <literal>block.wal</literal> dans le répertoire de données. Il n'est utile d'utiliser un périphérique WAL distinct que si le périphérique est plus rapide que le périphérique primaire ou le périphérique DB, par exemple dans les cas suivants :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Le périphérique WAL est une interface NVMe, le périphérique DB est un disque SSD et le périphérique de données est un disque SSD ou un disque HDD.
    </para>
   </listitem>
   <listitem>
    <para>
     Les périphériques WAL et DB sont des disques SSD distincts et le périphérique de données est un disque SSD ou un disque HDD.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Un <emphasis>périphérique DB</emphasis> peut être utilisé pour stocker les métadonnées internes de BlueStore. BlueStore (ou plutôt, la base de données RocksDB intégrée) mettra autant de métadonnées que possible sur le périphérique DB pour améliorer les performances. Encore une fois, il n'est utile de déployer un périphérique DB partagé que s'il est plus rapide que le périphérique primaire.
  </para>

  <tip>
   <title>planification de la taille du périphérique DB</title>
   <para>
    Planifiez minutieusement la taille du périphérique DB pour qu'elle soit suffisante. Si le périphérique DB sature, les métadonnées débordent sur le périphérique primaire, ce qui dégrade considérablement les performances de l'OSD.
   </para>
   <para>
    Vous pouvez vérifier si une partition WAL/DB est pleine et déborde avec la commande <command>ceph daemon osd<replaceable>.ID</replaceable> perf dump</command>. La valeur <option>slow_used_bytes</option> indique la quantité de données qui déborde :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph daemon osd<replaceable>.ID</replaceable> perf dump | jq '.bluefs'
"db_total_bytes": 1073741824,
"db_used_bytes": 33554432,
"wal_total_bytes": 0,
"wal_used_bytes": 0,
"slow_total_bytes": 554432,
"slow_used_bytes": 554432,
</screen>
  </tip>
 </sect1>
 <sect1 xml:id="storage-moreinfo">
  <title>Informations complémentaires</title>

  <itemizedlist>
   <listitem>
    <para>
     Ceph, en tant que projet communautaire, dispose de sa propre documentation en ligne. Pour les rubriques non trouvées dans ce manuel, reportez-vous au site <link xlink:href="http://docs.ceph.com/docs/master/"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     La publication d'origine <emphasis>CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data</emphasis> (CRUSH : placement contrôlé, évolutif et décentralisé des données répliquées) par <emphasis>S.A. Weil, S.A. Brandt, E.L. Miller, C. Maltzahn</emphasis> fournit des informations utiles sur le fonctionnement interne de Ceph. Il est surtout recommandé de le lire lors du déploiement de grappes à grande échelle. La publication peut être consultée à l'adresse <link xlink:href="http://www.ssrc.ucsc.edu/papers/weil-sc06.pdf"/>.
    </para>
   </listitem>
   <listitem>
     <para>
       SUSE Enterprise Storage peut être utilisé avec des distributions non-SUSE OpenStack. Les clients Ceph doivent être à un niveau compatible avec SUSE Enterprise Storage.
     </para>
     <note>
       <para>
         SUSE prend en charge le composant serveur du déploiement Ceph et le client est pris en charge par le fournisseur de la distribution OpenStack.
       </para>
     </note>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
