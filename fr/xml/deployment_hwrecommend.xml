<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>Configuration matérielle requise et recommandations</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>oui</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  La configuration matérielle requise de Ceph dépend fortement du workload des E/S. La configuration matérielle requise et les recommandations suivantes doivent être considérées comme un point de départ de la planification détaillée.
 </para>
 <para>
  En général, les recommandations données dans cette section dépendent du processus. Si plusieurs processus sont situés sur la même machine, les besoins en UC, RAM, disque et réseau doivent être additionnés.
 </para>
 <sect1 xml:id="multi-architecture">
  <title>Configurations d'architecture multiples</title>

  <para>
   SUSE Enterprise Storage prend en charge les architectures x86 et Arm. Si nous considérons chaque architecture, il est important de noter que d'un point de vue du nombre de coeurs par OSD, de la fréquence et de la mémoire RAM, il n'y a pas de différence réelle entre les architectures de processeurs en termes de dimensionnement.
  </para>

  <para>
   Comme pour les processeurs x86 plus petits (non-serveurs), les coeurs basés sur Arm moins performants peuvent ne pas fournir une expérience optimale, en particulier lorsqu'ils sont utilisés pour des réserves codées à effacement.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>Configuration minimale de la grappe</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Au moins 4 noeuds OSD, avec 8 disques OSD chacun, sont requis.
    </para>
   </listitem>
   <listitem>
    <para>
     Trois noeuds de moniteur Ceph (disque SSD requis pour l'unité du système d'exploitation dédié).
    </para>
   </listitem>
   <listitem>
    <para>
     Les noeuds Passerelles iSCSI, Object Gateway et MDS nécessitent 4 Go de RAM incrémentielle et quatre coeurs.
    </para>
   </listitem>
   <listitem>
    <para>
     Les noeuds Ceph Monitor, Object Gateway et MDS nécessitent un déploiement redondant.
    </para>
   </listitem>
   <listitem>
    <para>
     Un noeud Admin distinct est requis, avec 4 Go de RAM, quatre coeurs et 1 To de capacité. Il s'agit généralement du noeud Salt Master. Les services et passerelles Ceph, tels que Ceph Monitor, Ceph Manager, MDS, Ceph OSD, Object Gateway ou NFS Ganesha, ne sont pas pris en charge sur le noeud Admin.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>Noeuds de stockage des objets</title>

  <sect2 xml:id="sysreq-osd">
   <title>Configuration minimale requise</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Recommandations au niveau processeur :
     </para>
     <itemizedlist>
      <listitem>
       <para>
        1x thread d'UC 2 GHz par disque rotatif
       </para>
      </listitem>
      <listitem>
       <para>
        2x threads d'UC 2 GHz par disque SSD
       </para>
      </listitem>
      <listitem>
       <para>
        4x threads d'UC 2 GHz par disque NVMe
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Réseaux 10 GbE séparés (public/client et interface dorsale), 4x 10 GbE requis, 2x 25 GbE recommandés.
     </para>
    </listitem>
    <listitem>
     <para>
      Mémoire RAM totale requise = nombre d'OSD x (1 Go + <option>osd_memory_target</option>) + 16 Go
     </para>
     <para>
      Reportez-vous au <xref linkend="config-auto-cache-sizing"/> pour plus de détails sur <option>osd_memory_target</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      Disques OSD dans les configurations JBOD ou les configurations RAID-0 individuelles.
     </para>
    </listitem>
    <listitem>
     <para>
      Le journal OSD peut résider sur le disque OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Les disques OSD doivent être exclusivement utilisés par SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Disque/SSD dédié au système d'exploitation, de préférence dans une configuration RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Si cet hôte OSD héberge une partie d'une réserve de cache utilisée pour la hiérarchisation du cache, allouez au moins 4 Go de RAM supplémentaires.
     </para>
    </listitem>
    <listitem>
     <para>
      Les moniteurs Ceph, la passerelle et les serveurs de métadonnées peuvent résider sur les noeuds de stockage des objets.
     </para>
    </listitem>
    <listitem>
     <para>
      Pour des raisons de performances des disques, nous vous recommandons d'utiliser du matériel sans système d'exploitation pour les noeuds OSD, plutôt que des machines virtuelles.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>Taille minimale du disque</title>
   <para>
    Il existe deux types d'espace disque requis pour l'exécution sur un OSD : l'espace pour le journal du disque (pour FileStore) ou le périphérique WAL/DB (pour BlueStore) et l'espace primaire pour les données stockées. La valeur minimale (et par défaut) pour le journal/WAL/DB est de 6 Go. L'espace minimum pour les données est de 5 Go, car les partitions inférieures à 5 Go sont automatiquement assignées à une pondération de 0.
   </para>
   <para>
    Ainsi, bien que l'espace disque minimal pour un OSD soit de 11 Go, il est recommandé de ne pas utiliser un disque inférieur à 20 Go, même à des fins de test.
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>Taille recommandée pour les périphériques WAL et DB de BlueStore</title>
   <tip>
    <title>pour plus d'informations</title>
    <para>
     Reportez-vous à la <xref linkend="about-bluestore"/> pour plus d'informations sur BlueStore.
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      Nous vous recommandons de réserver 4 Go pour le périphérique WAL. La taille recommandée pour DB est de 64 Go pour la plupart des workloads.
     </para>
    </listitem>
    <listitem>
     <para>
      Si vous envisagez de placer les périphériques WAL et DB sur le même disque, il est recommandé d'utiliser une partition unique pour les deux périphériques, plutôt que d'utiliser une partition distincte pour chacun. Cela permet à Ceph d'utiliser également le périphérique DB pour les opérations WAL. La gestion de l'espace disque est donc plus efficace, car Ceph n'utilise la partition DB pour le WAL qu'en cas de nécessité. Un autre avantage est que la probabilité que la partition WAL soit pleine est très faible, et lorsqu'elle n'est pas entièrement utilisée, son espace n'est pas gaspillé, mais utilisé pour les opérations de DB.
     </para>
     <para>
      Pour partager le périphérique DB avec le WAL, ne spécifiez <emphasis>pas</emphasis> le périphérique WAL et indiquez uniquement le périphérique DB.
     </para>
     <para>
      Pour plus d'informations sur la spécification d'une disposition OSD, reportez-vous à la <xref linkend="ds-drive-groups"/>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>Utilisation des disques SSD pour les journaux OSD</title>
   <para>
    Les disques SSD n'ont pas de pièces mobiles. Cela réduit le temps d'accès aléatoire et la latence de lecture tout en accélérant le débit de données. Comme leur prix par Mo est significativement plus élevé que le prix des disques durs tournants, les disques SSD ne conviennent que pour un stockage plus petit.
   </para>
   <para>
    Les OSD peuvent connaître une amélioration significative des performances en stockant leur journal sur un disque SSD et les données d'objet sur un disque dur distinct.
   </para>
   <tip>
    <title>partage d'un disque SSD pour plusieurs journaux</title>
    <para>
     Comme les données de journal occupent relativement peu d'espace, vous pouvez monter plusieurs répertoires de journaux sur un seul disque SSD. N'oubliez pas qu'avec chaque journal partagé, les performances du disque SSD se dégradent. Il est déconseillé de partager plus de six journaux sur le même disque SSD et 12 sur des disques NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>Nombre maximal recommandé de disques</title>
   <para>
    Vous pouvez avoir autant de disques sur un serveur que ce dernier l'autorise. Il existe quelques aspects à considérer lorsque vous planifiez le nombre de disques par serveur :
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Bande passante du réseau.</emphasis> Plus vous avez de disques sur un serveur, plus il y a de données à transférer via la ou les cartes réseau pour les opérations d'écriture sur disque.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Mémoire.</emphasis> Plus de 2 Go de RAM sont utilisés pour le cache BlueStore. Avec la valeur par défaut de l'option <option>osd_memory_target</option>, à savoir 4 Go, le système dispose d'une taille de cache de départ raisonnable pour les supports rotatifs. Si vous utilisez des disques SSD ou NVMe, pensez à augmenter la taille du cache et l'allocation de RAM par OSD afin d'optimiser les performances.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolérance aux pannes.</emphasis> Si le serveur complet tombe en panne, plus il comporte de disques, plus le nombre d'OSD perdus temporairement par la grappe est important. En outre, pour maintenir les règles de réplication en cours d'exécution, vous devez copier toutes les données du serveur en panne vers les autres noeuds de la grappe.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>Noeuds de moniteur</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Au moins trois noeuds de moniteur Ceph sont requis. Le nombre de moniteurs doit toujours être impair (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 Go de RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processeur à quatre coeurs logiques.
    </para>
   </listitem>
   <listitem>
    <para>
     Un SSD ou un autre type de support de stockage suffisamment rapide est vivement recommandé pour les moniteurs, en particulier pour le chemin <filename>/var/lib/ceph</filename> sur chaque noeud de moniteur, car le quorum peut être instable avec des latences de disque élevées. Deux disques en configuration RAID 1 sont recommandés pour la redondance. Il est recommandé d'utiliser des disques distincts ou au moins des partitions de disque distinctes pour les processus Monitor, afin de protéger l'espace disque disponible du moniteur contre des phénomènes tels que le grossissement excessif du fichier journal.
    </para>
   </listitem>
   <listitem>
    <para>
     Il ne doit y avoir qu'un seul processus Monitor par noeud.
    </para>
   </listitem>
   <listitem>
    <para>
     La combinaison des noeuds OSD, Monitor ou Object Gateway n'est prise en charge que si des ressources matérielles suffisantes sont disponibles. Cela signifie que les besoins de tous les services doivent être additionnés.
    </para>
   </listitem>
   <listitem>
    <para>
     Deux interfaces réseau liées à plusieurs commutateurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>Noeuds Object Gateway</title>

  <para>
   Les noeuds Object Gateway doivent comporter six à huit coeurs d'UC et 32 Go de RAM (64 Go recommandés). Lorsque d'autres processus sont colocalisés sur la même machine, leurs besoins doivent être additionnés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>Noeuds MDS</title>

  <para>
   Le dimensionnement approprié des noeuds MDS (Metadata Server, Serveur de métadonnées) dépend du cas d'utilisation spécifique. En règle générale, plus le nombre de fichiers ouverts que le serveur de métadonnées doit traiter est important, plus l'UC et la RAM requises sont importantes. La configuration minimale requise est la suivante :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3 Go de RAM pour chaque daemon MDS.
    </para>
   </listitem>
   <listitem>
    <para>
     Interface réseau liée.
    </para>
   </listitem>
   <listitem>
    <para>
     2,5 GHz d'UC avec au moins 2 coeurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   Au moins 4 Go de RAM et une UC quadruple coeur sont requis. Cela inclut l'exécution de Ceph Dashboard sur le noeud Admin. Pour les grappes de grande taille avec des centaines de noeuds, 6 Go de RAM sont conseillés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>Noeuds iSCSI</title>

  <para>
   Les noeuds iSCSI doivent comporter six à huit coeurs d'UC et 16 Go de RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>Recommandations concernant le réseau</title>

  <para>
   L'environnement réseau dans lequel vous envisagez d'exécuter Ceph doit idéalement être un ensemble lié d'au moins deux interfaces réseau divisées logiquement en une partie publique et une partie interne approuvée en utilisant des VLAN. Le mode de liaison recommandé est 802.3ad, si possible, pour fournir une bande passante et une résilience maximales.
  </para>

  <para>
   Le VLAN public sert à fournir le service aux clients, alors que la partie interne assure les communications réseau Ceph authentifiées. La raison principale en est que, bien que Ceph fournisse une authentification et une protection contre les attaques une fois que les clés secrètes sont en place, les messages utilisés pour configurer ces clés peuvent être transférés ouvertement et sont vulnérables.
  </para>

  <tip>
   <title>noeuds configurés via DHCP</title>
   <para>
    Si les noeuds de stockage sont configurés via DHCP, les timeouts par défaut peuvent ne pas être suffisants pour que le réseau soit configuré correctement avant le démarrage des différents daemons Ceph. Dans ce cas, les daemons Ceph MON et OSD ne démarrent pas correctement (l'exécution de <command>systemctl status ceph\*</command> provoquera des erreurs de type « unable to bind » [liaison impossible]). Pour éviter ce problème, il est recommandé d'augmenter le timeout du client DHCP à au moins 30 secondes sur chaque noeud de votre grappe de stockage. Pour ce faire, vous pouvez modifier les paramètres suivants sur chaque noeud :
   </para>
   <para>
    Dans <filename>/etc/sysconfig/network/dhcp</filename>, définissez
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    Dans <filename>/etc/sysconfig/network/config</filename>, définissez
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>Ajout d'un réseau privé à une grappe en cours d'exécution</title>
   <para>
    Si vous ne spécifiez pas de réseau pour la grappe lors du déploiement de Ceph, il suppose un environnement de réseau public unique. Bien que Ceph fonctionne parfaitement avec un réseau public, ses performances et sa sécurité s'améliorent lorsque vous définissez un second réseau de grappe privé. Pour prendre en charge deux réseaux, chaque noeud Ceph doit disposer d'au moins deux cartes réseau.
   </para>
   <para>
    Vous devez apporter les modifications suivantes à chaque noeud Ceph. Ces modifications sont relativement rapides à apporter à une petite grappe, mais peuvent prendre beaucoup de temps si votre grappe est composée de centaines ou de milliers de noeuds.
   </para>
   <procedure>
    <step>
     <para>
      Arrêtez les services associés à Ceph sur chaque noeud de la grappe.
     </para>
     <para>
      Ajoutez une ligne à <filename>/etc/ceph/ceph.conf</filename> pour définir le réseau de la grappe, par exemple :
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      Si vous devez assigner des adresses IP statiques ou remplacer les paramètres <option>cluster network</option>, vous pouvez le faire avec la variable facultative <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Vérifiez que le réseau privé de la grappe fonctionne comme prévu au niveau du système d'exploitation.
     </para>
    </step>
    <step>
     <para>
      Démarrez les services associés à Ceph sur chaque noeud de la grappe.
     </para>
<screen><prompt>root # </prompt>systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>Noeuds de moniteur sur des sous-réseaux distincts</title>
   <para>
    Si les noeuds de moniteur se trouvent sur plusieurs sous-réseaux, par exemple s'ils se trouvent dans des pièces différentes et sont desservis par des commutateurs différents, vous devez corriger le fichier <filename>ceph.conf</filename> en conséquence. Par exemple, si les noeuds ont les adresses IP 192.168.123.12, 1.2.3.4 et 242.12.33.12, ajoutez les lignes suivantes à la section <literal>global</literal> :
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    En outre, si vous devez spécifier une adresse ou un réseau public par moniteur, vous devez ajouter une section <literal>[mon.<replaceable>X</replaceable>]</literal> pour chaque moniteur :
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>Limitations relatives à la dénomination</title>

  <para>
   Ceph ne prend généralement pas en charge les caractères non-ASCII dans les fichiers de configuration, les noms de réserve, les noms d'utilisateur, etc. Lors de la configuration d'une grappe Ceph, il est recommandé d'utiliser uniquement des caractères alphanumériques simples (A-Z, a-z, 0-9) et des ponctuations minimales (« . », « - », « _ ») dans tous les noms d'objet/de configuration Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>Noeuds OSD et Monitor partageant un même serveur</title>

  <para>
   Bien qu'il soit techniquement possible d'exécuter les noeuds Ceph OSD et Monitor sur le même serveur dans des environnements de test, il est vivement recommandé de disposer d'un serveur distinct pour chaque noeud de moniteur en production. La principale raison est la performance : plus la grappe comporte d'OSD, plus le nombre d'opérations d'E/S que les noeuds de moniteur doivent effectuer est grand. Et lorsqu'un serveur est partagé entre un noeud de moniteur et plusieurs OSD, les opérations d'E/S des OSD constituent un facteur limitant pour le noeud de moniteur.
  </para>

  <para>
   Une autre considération consiste à savoir s'il faut partager des disques entre un OSD, un noeud de moniteur et le système d'exploitation sur le serveur. La réponse est simple : si possible, dédiez un disque distinct à l'OSD et un serveur distinct au noeud de moniteur.
  </para>

  <para>
   Bien que Ceph prenne en charge les OSD basés sur les répertoires, un OSD doit toujours avoir un disque dédié autre que celui du système d'exploitation.
  </para>

  <tip>
   <para>
    S'il est <emphasis>vraiment</emphasis> nécessaire d'exécuter l'OSD et le noeud de moniteur sur le même serveur, exécutez le noeud de moniteur sur un disque distinct en montant le disque dans le répertoire <filename>/var/lib/ceph/mon</filename> pour obtenir de meilleures performances.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>Configuration recommandée de la grappe de production</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sept noeuds de stockage des objets
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Aucun noeud n'excède environ 15 % du stockage total
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gigabit Ethernet (quatre réseaux physiques liés à plusieurs commutateurs)
      </para>
     </listitem>
     <listitem>
      <para>
       Au moins 56 OSD par grappe de stockage
      </para>
     </listitem>
     <listitem>
      <para>
       Disques OS RAID 1 pour chaque noeud de stockage OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Disques SSD pour journal avec un ratio journal SSD à OSD de 6:1
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 Go de RAM par To de capacité OSD brute pour chaque noeud de stockage des objets
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz par OSD pour chaque noeud de stockage des objets
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Noeuds d'infrastructure physique dédiés
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Trois noeuds de moniteur Ceph : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
      </para>
     </listitem>
     <listitem>
      <para>
       Un noeud de gestion de SES : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
      </para>
     </listitem>
     <listitem>
      <para>
       Déploiement physique redondant des noeuds de passerelle ou MDS :
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Noeuds Object Gateway : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1
        </para>
       </listitem>
       <listitem>
        <para>
         Noeuds Passerelle iSCSI : 16 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
        </para>
       </listitem>
       <listitem>
        <para>
         Noeuds MDS (un actif/un de secours) : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage 6 et autres produits SUSE</title>

  <para>
   Cette section contient des informations importantes sur l'intégration de SUSE Enterprise Storage 6 à d'autres produits SUSE.
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager et SUSE Enterprise Storage n'étant pas intégrés, SUSE Manager ne peut actuellement pas gérer une grappe SUSE Enterprise Storage.
   </para>
  </sect2>
 </sect1>
</chapter>
