<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage.bp.hwreq">
 <title>Configuration matérielle requise et recommandations</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>oui</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  La configuration matérielle requise de Ceph dépend fortement du workload des E/S. La configuration matérielle requise et les recommandations suivantes doivent être considérées comme un point de départ de la planification détaillée.
 </para>
 <para>
  En général, les recommandations données dans cette section dépendent du processus. Si plusieurs processus sont situés sur la même machine, les besoins en UC, RAM, disque et réseau doivent être additionnés.
 </para>
 <sect1 xml:id="deployment.osd.recommendation">
  <title>Noeuds de stockage des objets</title>

  <sect2 xml:id="sysreq.osd">
   <title>Configuration minimale requise</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      Au moins 4 noeuds OSD, avec 8 disques OSD chacun, sont requis.
     </para>
    </listitem>
    <listitem>
     <para>
      Pour les OSD qui <emphasis>n'utilisent pas</emphasis> BlueStore, 1 Go de RAM par téraoctet de capacité OSD brute est requis au minimum pour chaque noeud de stockage OSD. 1,5 Go de RAM par téraoctet de capacité OSD brute est recommandé. Pendant la récupération, 2 Go de RAM par téraoctet de capacité OSD brute peuvent être optimaux.
     </para>
     <para>
      Pour les OSD qui <emphasis>utilisent</emphasis> BlueStore, calculez d'abord la capacité en RAM recommandée pour les OSD qui n'utilisent pas BlueStore, puis calculez 2 Go plus la taille du cache de la RAM BlueStore recommandée pour chaque processus OSD, et choisissez la plus grande valeur de RAM des deux résultats. Notez que le cache BlueStore par défaut est de 1 Go pour les disques HDD et de 3 Go pour les disques SSD par défaut. En résumé, choisissez la plus grande valeur entre :
     </para>
<screen>[1GB * OSD count * OSD size]</screen>
     <para>
      ou
     </para>
<screen>[(2 + BS cache) * OSD count]</screen>
    </listitem>
    <listitem>
     <para>
      1,5 GHz d'un coeur de processeur logique par OSD est le minimum requis pour chaque processus de daemon OSD. 2 GHz par processus de daemon OSD est recommandé. Notez que Ceph exécute un processus daemon OSD par disque de stockage. Ne comptabilisez pas les disques réservés uniquement à une utilisation en tant que journaux OSD, journaux WAL, métadonnées omap ou toute combinaison de ces trois cas.
     </para>
    </listitem>
    <listitem>
     <para>
      10 Gigabit Ethernet (deux interfaces réseau liées à plusieurs commutateurs).
     </para>
    </listitem>
    <listitem>
     <para>
      Disques OSD dans des configurations JBOD.
     </para>
    </listitem>
    <listitem>
     <para>
      Les disques OSD doivent être exclusivement utilisés par SUSE Enterprise Storage.
     </para>
    </listitem>
    <listitem>
     <para>
      Disque/SSD dédié au système d'exploitation, de préférence dans une configuration RAID 1.
     </para>
    </listitem>
    <listitem>
     <para>
      Si cet hôte OSD héberge une partie d'une réserve de cache utilisée pour la hiérarchisation du cache, allouez au moins 4 Go de RAM supplémentaires.
     </para>
    </listitem>
    <listitem>
     <para>
      Les noeuds OSD doivent être sans système d'exploitation et ne pas être virtualisés, pour des raisons de performances de disque.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.mindisk">
   <title>Taille minimale du disque</title>
   <para>
    Il existe deux types d'espace disque requis pour l'exécution sur un OSD : l'espace pour le journal du disque (pour FileStore) ou le périphérique WAL/DB (pour BlueStore) et l'espace primaire pour les données stockées. La valeur minimale (et par défaut) pour le journal/WAL/DB est de 6 Go. L'espace minimum pour les données est de 5 Go, car les partitions inférieures à 5 Go sont automatiquement assignées à une pondération de 0.
   </para>
   <para>
    Ainsi, bien que l'espace disque minimal pour un OSD soit de 11 Go, il est recommandé de ne pas utiliser un disque inférieur à 20 Go, même à des fins de test.
   </para>
  </sect2>

  <sect2 xml:id="rec.waldb.size">
   <title>Taille recommandée pour les périphériques WAL et DB de BlueStore</title>
   <tip>
    <title>pour plus d'informations</title>
    <para>
     Reportez-vous à la <xref linkend="about.bluestore"/> pour plus d'informations sur BlueStore.
    </para>
   </tip>
   <para>
    Voici plusieurs règles pour le dimensionnement des périphériques WAL/DB. Lors de l'utilisation de DeepSea pour déployer des OSD avec BlueStore, DeepSea applique automatiquement les règles recommandées et en informe l'administrateur.
   </para>
   <itemizedlist>
    <listitem>
     <para>
      10 Go du périphérique DB pour chaque téraoctet de la capacité de l'OSD (1/100ème de l'OSD).
     </para>
    </listitem>
    <listitem>
     <para>
      Entre 500 Mo et 2 Go pour le périphérique WAL. La taille du WAL dépend du trafic de données et du workload, non de la taille de l'OSD. Si vous savez qu'un OSD est physiquement capable de gérer de petits écrasements et écritures à un débit très élevé, il est préférable d'avoir plus de WAL que moins. Un périphérique WAL de 1 Go est un bon compromis qui répond à la plupart des déploiements.
     </para>
    </listitem>
    <listitem>
     <para>
      Si vous envisagez de placer les périphériques WAL et DB sur le même disque, il est recommandé d'utiliser une partition unique pour les deux périphériques, plutôt que d'utiliser une partition distincte pour chacun. Cela permet à Ceph d'utiliser également le périphérique DB pour les opérations WAL. La gestion de l'espace disque est donc plus efficace, car Ceph n'utilise la partition DB pour le WAL qu'en cas de nécessité. Un autre avantage est que la probabilité que la partition WAL soit pleine est très faible, et lorsqu'elle n'est pas entièrement utilisée, son espace n'est pas gaspillé, mais utilisé pour les opérations de DB.
     </para>
     <para>
      Pour partager le périphérique DB avec le WAL, ne spécifiez <emphasis>pas</emphasis> le périphérique WAL et indiquez uniquement le périphérique DB :
     </para>
<screen>
bluestore_block_db_path = "/path/to/db/device"
bluestore_block_db_size = 10737418240
bluestore_block_wal_path = ""
bluestore_block_wal_size = 0
</screen>
    </listitem>
    <listitem>
     <para>
      Vous pouvez également placer le WAL sur son propre périphérique. Dans ce cas, il est recommandé d'utiliser le périphérique le plus rapide pour le fonctionnement du WAL.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses.bp.share_ssd_journal">
   <title>Utilisation des disques SSD pour les journaux OSD</title>
   <para>
    Les disques SSD n'ont pas de pièces mobiles. Cela réduit le temps d'accès aléatoire et la latence de lecture tout en accélérant le débit de données. Comme leur prix par Mo est significativement plus élevé que le prix des disques durs tournants, les disques SSD ne conviennent que pour un stockage plus petit.
   </para>
   <para>
    Les OSD peuvent connaître une amélioration significative des performances en stockant leur journal sur un disque SSD et les données d'objet sur un disque dur distinct.
   </para>
   <tip>
    <title>partage d'un disque SSD pour plusieurs journaux</title>
    <para>
     Comme les données de journal occupent relativement peu d'espace, vous pouvez monter plusieurs répertoires de journaux sur un seul disque SSD. N'oubliez pas qu'avec chaque journal partagé, les performances du disque SSD se dégradent. Il est déconseillé de partager plus de six journaux sur le même disque SSD et 12 sur des disques NVMe.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum.count.of.disks.osd">
   <title>Nombre maximal recommandé de disques</title>
   <para>
    Vous pouvez avoir autant de disques sur un serveur que ce dernier l'autorise. Il existe quelques aspects à considérer lorsque vous planifiez le nombre de disques par serveur :
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>Bande passante du réseau.</emphasis> Plus vous avez de disques sur un serveur, plus il y a de données à transférer via la ou les cartes réseau pour les opérations d'écriture sur disque.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Mémoire.</emphasis> Pour des performances optimales, réservez au moins 2 Go de RAM par téraoctet d'espace disque installé.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Tolérance aux pannes.</emphasis> Si le serveur complet tombe en panne, plus il comporte de disques, plus le nombre d'OSD perdus temporairement par la grappe est important. En outre, pour maintenir les règles de réplication en cours d'exécution, vous devez copier toutes les données du serveur en panne vers les autres noeuds de la grappe.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.mon">
  <title>Noeuds de moniteur</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Au moins trois noeuds de moniteur Ceph sont requis. Le nombre de moniteurs doit toujours être impair (1+2n).
    </para>
   </listitem>
   <listitem>
    <para>
     4 Go de RAM.
    </para>
   </listitem>
   <listitem>
    <para>
     Processeur à quatre coeurs logiques.
    </para>
   </listitem>
   <listitem>
    <para>
     Un SSD ou un autre type de support de stockage suffisamment rapide est vivement recommandé pour les moniteurs, en particulier pour le chemin <filename>/var/lib/ceph</filename> sur chaque noeud de moniteur, car le quorum peut être instable avec des latences de disque élevées. Deux disques en configuration RAID 1 sont recommandés pour la redondance. Il est recommandé d'utiliser des disques distincts ou au moins des partitions de disque distinctes pour les processus Monitor, afin de protéger l'espace disque disponible du moniteur contre des phénomènes tels que le grossissement excessif du fichier journal.
    </para>
   </listitem>
   <listitem>
    <para>
     Il ne doit y avoir qu'un seul processus Monitor par noeud.
    </para>
   </listitem>
   <listitem>
    <para>
     La combinaison des noeuds OSD, Monitor ou Object Gateway n'est prise en charge que si des ressources matérielles suffisantes sont disponibles. Cela signifie que les besoins de tous les services doivent être additionnés.
    </para>
   </listitem>
   <listitem>
    <para>
     Deux interfaces réseau liées à plusieurs commutateurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.rgw">
  <title>Noeuds Object Gateway</title>

  <para>
   Les noeuds Object Gateway doivent comporter six à huit coeurs d'UC et 32 Go de RAM (64 Go recommandés). Lorsque d'autres processus sont colocalisés sur la même machine, leurs besoins doivent être additionnés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.mds">
  <title>Noeuds MDS</title>

  <para>
   Le dimensionnement approprié des noeuds MDS (Metadata Server, Serveur de métadonnées) dépend du cas d'utilisation spécifique. En règle générale, plus le nombre de fichiers ouverts que le serveur de métadonnées doit traiter est important, plus l'UC et la RAM requises sont importantes. Vous trouverez ci-dessous la configuration minimale requise :
  </para>

  <itemizedlist>
   <listitem>
    <para>
     3 Go de RAM par daemon MDS.
    </para>
   </listitem>
   <listitem>
    <para>
     Interface réseau liée.
    </para>
   </listitem>
   <listitem>
    <para>
     2,5 GHz d'UC avec au moins 2 coeurs.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq.smaster">
  <title>Salt Master</title>

  <para>
   Au moins 4 Go de RAM et une UC quadruple coeur sont requis. Cela inclut l'exécution d'openATTIC sur Salt Master. Pour les grappes de grande taille avec des centaines de noeuds, 6 Go de RAM sont conseillés.
  </para>
 </sect1>
 <sect1 xml:id="sysreq.iscsi">
  <title>Noeuds iSCSI</title>

  <para>
   Les noeuds iSCSI doivent comporter six à huit coeurs d'UC et 16 Go de RAM.
  </para>
 </sect1>
 <sect1 xml:id="ceph.install.ceph-deploy.network">
  <title>Recommandations concernant le réseau</title>

  <para>
   L'environnement réseau dans lequel vous envisagez d'exécuter Ceph doit idéalement être un ensemble lié d'au moins deux interfaces réseau divisées logiquement en une partie publique et une partie interne approuvée en utilisant des VLAN. Le mode de liaison recommandé est 802.3ad, si possible, pour fournir une bande passante et une résilience maximales.
  </para>

  <para>
   Le VLAN public sert à fournir le service aux clients, alors que la partie interne assure les communications réseau Ceph authentifiées. La raison principale en est que, bien que Ceph fournisse une authentification et une protection contre les attaques une fois que les clés secrètes sont en place, les messages utilisés pour configurer ces clés peuvent être transférés ouvertement et sont vulnérables.
  </para>

  <tip>
   <title>noeuds configurés via DHCP</title>
   <para>
    Si les noeuds de stockage sont configurés via DHCP, les timeouts par défaut peuvent ne pas être suffisants pour que le réseau soit configuré correctement avant le démarrage des différents daemons Ceph. Dans ce cas, les daemons Ceph MON et OSD ne démarrent pas correctement (l'exécution de <command>systemctl status ceph\*</command> provoquera des erreurs de type « unable to bind » [liaison impossible]). Pour éviter ce problème, il est recommandé d'augmenter le timeout du client DHCP à au moins 30 secondes sur chaque noeud de votre grappe de stockage. Pour ce faire, vous pouvez modifier les paramètres suivants sur chaque noeud :
   </para>
   <para>
    Dans <filename>/etc/sysconfig/network/dhcp</filename>, définissez
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    Dans <filename>/etc/sysconfig/network/config</filename>, définissez
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage.bp.net.private">
   <title>Ajout d'un réseau privé à une grappe en cours d'exécution</title>
   <para>
    Si vous ne spécifiez pas de réseau pour la grappe lors du déploiement de Ceph, il suppose un environnement de réseau public unique. Bien que Ceph fonctionne parfaitement avec un réseau public, ses performances et sa sécurité s'améliorent lorsque vous définissez un second réseau de grappe privé. Pour prendre en charge deux réseaux, chaque noeud Ceph doit disposer d'au moins deux cartes réseau.
   </para>
   <para>
    Vous devez apporter les modifications suivantes à chaque noeud Ceph. Ces modifications sont relativement rapides à apporter à une petite grappe, mais peuvent prendre beaucoup de temps si votre grappe est composée de centaines ou de milliers de noeuds.
   </para>
   <procedure>
    <step>
     <para>
      Arrêtez les services associés à Ceph sur chaque noeud de la grappe.
     </para>
     <para>
      Ajoutez une ligne à <filename>/etc/ceph/ceph.conf</filename> pour définir le réseau de la grappe, par exemple :
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      Si vous devez assigner des adresses IP statiques ou remplacer les paramètres <option>cluster network</option>, vous pouvez le faire avec la variable facultative <option>cluster addr</option>.
     </para>
    </step>
    <step>
     <para>
      Vérifiez que le réseau privé de la grappe fonctionne comme prévu au niveau du système d'exploitation.
     </para>
    </step>
    <step>
     <para>
      Démarrez les services associés à Ceph sur chaque noeud de la grappe.
     </para>
<screen>sudo systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage.bp.net.subnets">
   <title>noeuds de moniteur sur des sous-réseaux distincts</title>
   <para>
    Si les noeuds de moniteur se trouvent sur plusieurs sous-réseaux, par exemple s'ils se trouvent dans des pièces différentes et sont desservis par des commutateurs différents, vous devez corriger le fichier <filename>ceph.conf</filename> en conséquence. Par exemple, si les noeuds ont les adresses IP 192.168.123.12, 1.2.3.4 et 242.12.33.12, ajoutez les lignes suivantes à la section globale :
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    En outre, si vous devez spécifier une adresse ou un réseau public par Monitor, vous devez ajouter une section <literal>[mon.<replaceable>X</replaceable>]</literal> à chaque Monitor :
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq.naming">
  <title>Limitations relatives à la dénomination</title>

  <para>
   Ceph ne prend généralement pas en charge les caractères non-ASCII dans les fichiers de configuration, les noms de réserve, les noms d'utilisateur, etc. Lors de la configuration d'une grappe Ceph, il est recommandé d'utiliser uniquement des caractères alphanumériques simples (A-Z, a-z, 0-9) et des ponctuations minimales (« . », « - », « _ ») dans tous les noms d'objet/de configuration Ceph.
  </para>
 </sect1>
 <sect1 xml:id="ses.bp.diskshare">
  <title>Noeuds OSD et Monitor partageant un même serveur</title>

  <para>
   Bien qu'il soit techniquement possible d'exécuter les noeuds Ceph OSD et Monitor sur le même serveur dans des environnements de test, il est vivement recommandé de disposer d'un serveur distinct pour chaque noeud de moniteur en production. La principale raison est la performance : plus la grappe comporte d'OSD, plus le nombre d'opérations d'E/S que les noeuds de moniteur doivent effectuer est grand. Et lorsqu'un serveur est partagé entre un noeud de moniteur et plusieurs OSD, les opérations d'E/S des OSD constituent un facteur limitant pour le noeud de moniteur.
  </para>

  <para>
   Une autre considération consiste à savoir s'il faut partager des disques entre un OSD, un noeud de moniteur et le système d'exploitation sur le serveur. La réponse est simple : si possible, dédiez un disque distinct à l'OSD et un serveur distinct au noeud de moniteur.
  </para>

  <para>
   Bien que Ceph prenne en charge les OSD basés sur les répertoires, un OSD doit toujours avoir un disque dédié autre que celui du système d'exploitation.
  </para>

  <tip>
   <para>
    S'il est <emphasis>vraiment</emphasis> nécessaire d'exécuter l'OSD et le noeud de moniteur sur le même serveur, exécutez le noeud de moniteur sur un disque distinct en montant le disque dans le répertoire <filename>/var/lib/ceph/mon</filename> pour obtenir de meilleures performances.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses.bp.minimum_cluster">
  <title>Configuration minimale de la grappe</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Quatre noeuds de stockage des objets
    </para>
    <itemizedlist>
     <listitem>
      <para>
       10 Gigabit Ethernet (deux réseaux liés à plusieurs commutateurs).
      </para>
     </listitem>
     <listitem>
      <para>
       32 OSD par grappe de stockage
      </para>
     </listitem>
     <listitem>
      <para>
       Le journal OSD peut résider sur le disque OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Disque OS (Object Storage, stockage des objets) dédié pour chaque noeud de stockage des objets
      </para>
     </listitem>
     <listitem>
      <para>
       1 Go de RAM par To de capacité OSD brute pour chaque noeud de stockage des objets
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 GHz par OSD pour chaque noeud de stockage des objets
      </para>
     </listitem>
     <listitem>
      <para>
       Les moniteurs Ceph, la passerelle et les serveurs de métadonnées peuvent résider sur les noeuds de stockage des objets
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Trois noeuds de moniteur Ceph (disque SSD requis pour l'unité du système d'exploitation dédié)
        </para>
       </listitem>
       <listitem>
        <para>
         Les noeuds de moniteur Ceph, Object Gateway et MDS nécessitent un déploiement redondant
        </para>
       </listitem>
       <listitem>
        <para>
         Les noeuds Passerelles iSCSI, Object Gateway et MDS nécessitent 4 Go de RAM incrémentielle et quatre coeurs
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Noeud de gestion distinct avec 4 Go de RAM, quatre coeurs, 1 To de capacité
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ses.bp.production_cluster">
  <title>Configuration recommandée de la grappe de production</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Sept noeuds de stockage des objets
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Aucun noeud n'excède environ 15 % du stockage total
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gigabit Ethernet (quatre réseaux physiques liés à plusieurs commutateurs)
      </para>
     </listitem>
     <listitem>
      <para>
       Au moins 56 OSD par grappe de stockage
      </para>
     </listitem>
     <listitem>
      <para>
       Disques OS RAID 1 pour chaque noeud de stockage OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Disques SSD pour journal avec un ratio journal SSD à OSD de 6:1
      </para>
     </listitem>
     <listitem>
      <para>
       1,5 Go de RAM par To de capacité OSD brute pour chaque noeud de stockage des objets
      </para>
     </listitem>
     <listitem>
      <para>
       2 GHz par OSD pour chaque noeud de stockage des objets
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Noeuds d'infrastructure physique dédiés
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Trois noeuds de moniteur Ceph : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
      </para>
     </listitem>
     <listitem>
      <para>
       Un noeud de gestion de SES : 4 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
      </para>
     </listitem>
     <listitem>
      <para>
       Déploiement physique redondant des noeuds de passerelle ou MDS :
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Noeuds Object Gateway : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1
        </para>
       </listitem>
       <listitem>
        <para>
         Noeuds Passerelle iSCSI : 16 Go de RAM, processeur à 4 coeurs, disques SSD RAID 1
        </para>
       </listitem>
       <listitem>
        <para>
         Noeuds MDS (un actif/un de secours) : 32 Go de RAM, processeur à 8 coeurs, disques SSD RAID 1
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req.ses.other">
  <title>SUSE Enterprise Storage et autres produits SUSE</title>

  <para>
   Cette section contient des informations importantes sur l'intégration de SUSE Enterprise Storage à d'autres produits SUSE.
  </para>

  <sect2 xml:id="req.ses.suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager et SUSE Enterprise Storage n'étant pas intégrés, SUSE Manager ne peut actuellement pas gérer une grappe SUSE Enterprise Storage.
   </para>
  </sect2>
 </sect1>
</chapter>
