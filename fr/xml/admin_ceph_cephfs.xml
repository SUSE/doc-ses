<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>Système de fichiers en grappe</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modification</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>oui</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ce chapitre décrit les tâches d'administration normalement effectuées après la configuration de la grappe et l'exportation de CephFS. Pour plus d'informations sur la configuration de CephFS, reportez-vous au <xref linkend="cha-ceph-as-cephfs"/>.
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>Montage de CephFS</title>

  <para>
   Lorsque le système de fichiers est créé et que MDS est actif, vous êtes prêt à monter le système de fichiers à partir d'un hôte client.
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>Préparation du client</title>
   <para>
    Si l'hôte client exécute SUSE Linux Enterprise 12 SP2 ou SP3, vous pouvez ignorer cette section, car le système est prêt à monter CephFS dans sa version « prête à l'emploi ».
   </para>
   <para>
    Si l'hôte client exécute SUSE Linux Enterprise 12 SP1, vous devez appliquer tous les correctifs les plus récents avant de monter CephFS.
   </para>
   <para>
    Dans tous les cas, SUSE Linux Enterprise inclut tout ce qui est nécessaire au montage de CephFS. Le produit SUSE Enterprise Storage 6 n'est pas nécessaire.
   </para>
   <para>
    Pour prendre en charge la syntaxe complète de <command>mount</command>, il est nécessaire d'installer le paquetage
    <package>ceph-common</package> (fourni avec SUSE Linux Enterprise) pour pouvoir monter CephFS.
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>Création d'un fichier de secret</title>
   <para>
    L'authentification est activée par défaut pour la grappe Ceph active. Vous devez créer un fichier qui stocke votre clé secrète (et non pas le trousseau de clés lui-même). Pour obtenir la clé secrète d'un utilisateur particulier et créer ensuite le fichier, procédez comme suit :
   </para>
   <procedure>
    <title>Création d'une clé secrète</title>
    <step>
     <para>
      Affichez la clé d'un utilisateur particulier dans un fichier de trousseau de clés :
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      Copiez la clé de l'utilisateur qui emploiera le système de fichiers Ceph FS monté. La clé ressemble généralement à ceci :
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      Créez un fichier en indiquant le nom de l'utilisateur dans le nom de fichier, par exemple <filename>/etc/ceph/admin.secret</filename> pour l'utilisateur <emphasis>admin</emphasis>.
     </para>
    </step>
    <step>
     <para>
      Collez la valeur de la clé dans le fichier créé à l'étape précédente.
     </para>
    </step>
    <step>
     <para>
      Définissez les droits d'accès appropriés au fichier. L'utilisateur doit être le seul à pouvoir lire le fichier, les autres n'ont aucun droit d'accès. 
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>Montage de CephFS</title>
   <para>
    La commande <command>mount</command> permet de monter CephFS. Vous devez indiquer le nom d'hôte ou l'adresse IP du moniteur. Comme l'authentification <systemitem>cephx</systemitem> est activée par défaut dans SUSE Enterprise Storage, vous devez également spécifier un nom d'utilisateur et le secret qui lui est associé :
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    Étant donné que la commande précédente reste dans l'historique du shell, une approche plus sécurisée consiste à lire le secret d'un fichier :
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Notez que le fichier de secret ne doit contenir que le secret du trousseau de clés. Dans notre exemple, le fichier contient uniquement la ligne suivante :
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>spécification de plusieurs moniteurs</title>
    <para>
     Il est judicieux d'indiquer plusieurs moniteurs séparés par des virgules sur la ligne de commande <command>mount</command> dans le cas où un moniteur est arrêté au moment du montage. Chaque adresse de moniteur figure sous la forme <literal>hôte[:port]</literal>. Si le port n'est pas indiqué, le port par défaut est le port 6789.
    </para>
   </tip>
   <para>
    Créez le point de montage sur l'hôte local :
   </para>
<screen><prompt>root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    Montez le système de fichiers CephFS :
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Un sous-répertoire <filename>subdir</filename> peut être indiqué si un sous-ensemble du système de fichiers doit être monté :
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    Vous pouvez spécifier plusieurs hôtes de moniteur dans la commande <command>mount</command> :
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>accès en lecture au répertoire racine</title>
    <para>
     Si des clients avec restriction de chemin d'accès sont utilisés, les fonctionnalités MDS doivent inclure un accès en lecture au répertoire racine. Par exemple, un trousseau de clés peut ressembler à ceci :
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     La partie <literal>allow r path=/</literal> signifie que les clients dont le chemin est restreint peuvent voir le volume racine sans être autorisés à y écrire des données. Cela peut être un problème dans les cas où une isolation complète est requise.
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>Démontage de CephFS</title>

  <para>
   Pour démonter le système de fichiers CephFS, utilisez la commande <command>umount</command> :
  </para>

<screen><prompt>root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title>CephFS dans <filename>/etc/fstab</filename></title>

  <para>
   Pour monter automatiquement CephFS au démarrage du client, insérez la ligne correspondante dans sa table des systèmes de fichiers <filename>/etc/fstab</filename> :
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>Daemons MDS actifs multiples (MDS actif-actif)</title>

  <para>
   Par défaut, CephFS est configuré pour un seul daemon MDS actif. Pour mettre à l'échelle les performances des métadonnées pour les systèmes vastes, vous pouvez activer plusieurs daemons MDS actifs, qui partageront entre eux la charge de travail des métadonnées.
  </para>

  <sect2>
   <title>Utilisation de MDS actif-actif</title>
   <para>
    Vous pouvez envisager d'utiliser plusieurs daemons MDS actifs lorsque les performances de vos métadonnées sont bloquées sur le MDS unique par défaut.
   </para>
   <para>
    L'ajout de plusieurs daemons n'améliore pas les performances pour tous les types de charge de travail. Par exemple, une seule application s'exécutant sur un seul client ne bénéficie pas d'un nombre accru de daemons MDS à moins d'effectuer de nombreuses opérations de métadonnées en parallèle.
   </para>
   <para>
    Les charges de travail qui bénéficient généralement d'un nombre supérieur de daemons MDS actifs sont celles qui possèdent de nombreux clients, travaillant, le cas échéant, sur plusieurs répertoires distincts.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>Augmentation de la taille de la grappe active MDS</title>
   <para>
    Chaque système de fichiers CephFS possède un paramètre <option>max_mds</option> qui détermine le nombre de rangs à créer. Le nombre réel de rangs dans le système de fichiers n'augmentera que si un daemon supplémentaire est en mesure de prendre le nouveau rang créé. Par exemple, si un seul daemon MDS s'exécute et que la valeur <option>max_mds</option> est définie sur 2, aucun second rang n'est créé.
   </para>
   <para>
    Dans l'exemple suivant, nous définissons <option>max_mds</option> sur 2 pour créer un rang en dehors du rang par défaut. Pour voir les changements, lancez <command>ceph status</command> avant et après avoir défini <option>max_mds</option> et reportez-vous à la ligne contenant <literal>fsmap</literal> :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 2
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    Le rang nouvellement créé (1) passe à l'état « creating » (création en cours), puis à l'état« active » (actif).
   </para>
   <important>
    <title>daemons de secours</title>
    <para>
     Même avec plusieurs daemons MDS actifs, un système hautement disponible nécessite toujours des daemons de secours qui prennent le relais si l'un des serveurs exécutant un daemon actif tombe en panne.
    </para>
    <para>
     Par conséquent, la valeur maximale pratique de <option>max_mds</option> pour les systèmes hautement disponibles est égale au nombre total de serveurs MDS de votre système moins 1. Pour rester disponible en cas de défaillance de plusieurs serveurs, augmentez le nombre de daemons de secours du système pour qu'ils correspondent au nombre de pannes de serveur que vous devez pouvoir surmonter. 
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>Diminution du nombre de rangs</title>
   <para>
    Tous les rangs, y compris les rangs à supprimer, doivent d'abord être actifs. Cela signifie que vous devez disposer d'au moins <option>max_mds</option> daemons MDS disponibles.
   </para>
   <para>
    Commencez par définir <option>max_mds</option> sur un nombre inférieur. Par exemple, définissez un seul MDS actif :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 1
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    Notez que nous avons encore deux MDS actifs. Les rangs existent toujours même si nous avons diminué la valeur de <option>max_mds</option>, car <option>max_mds</option> limite seulement la création de rangs.
   </para>
   <para>
    Ensuite, supprimez le rang inutile à l'aide de la commande <command>ceph mds deactivate <replaceable>rang</replaceable></command> :
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    Le rang désactivé passe d'abord à l'état « stopping » (arrêt en cours) pour une période pendant laquelle il transmet sa part de métadonnées aux daemons actifs restants. Cette phase peut prendre de quelques secondes à quelques minutes. Si le MDS semble être bloqué à l'état « stopping », il se peut que cela soit dû à un bogue.
   </para>
   <para>
    Si un daemon MDS tombe en panne ou est arrêté alors que son état était « stopping » (arrêt), un daemon de secours prend le relais et redevient « active » (actif ). Vous pouvez essayer de le désactiver à nouveau quand le daemon MDS est de nouveau opérationnel.
   </para>
   <para>
    Quand un daemon finit par s'arrêter, il redémarre et reprend son rôle de daemon de secours.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>Épinglage manuel d'arborescences de répertoires à un rang</title>
   <para>
    Dans plusieurs configurations de serveur de métadonnées actives, l'équilibreur permet de répartir la charge de métadonnées uniformément dans la grappe. Cela fonctionne généralement assez bien pour la plupart des utilisateurs, mais il est parfois souhaitable de remplacer l'équilibreur dynamique par des assignations explicites de métadonnées à des rangs particuliers. L'administrateur ou les utilisateurs peuvent ainsi répartir uniformément la charge d'applications ou limiter l'impact des demandes de métadonnées des utilisateurs sur l'ensemble de la grappe.
   </para>
   <para>
    Le mécanisme prévu à cet effet s'appelle une « épingle d'exportation ». Il s'agit d'un attribut étendu des répertoires. Cet attribut étendu s'appelle <literal>ceph.dir.pin</literal>. Les utilisateurs peuvent définir cet attribut à l'aide de commandes standard :
   </para>
<screen><prompt>root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    La valeur (<option>-v</option>) de l'attribut étendu correspond au rang à attribuer à la sous-arborescence de répertoires. La valeur par défaut (-1) indique que le répertoire n'est pas épinglé.
   </para>
   <para>
    Une épingle d'exportation de répertoire est héritée de son parent le plus proche ayant une épingle d'exportation définie. Par conséquent, la définition de l'épingle d'exportation sur un répertoire affecte tous ses enfants. Cependant, il est possible d'annuler l'épingle du parent en définissant l'épingle d'exportation du répertoire enfant. Par exemple :
   </para>
<screen><prompt>root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>Gestion du basculement</title>

  <para>
   Si un daemon MDS cesse de communiquer avec le moniteur, le moniteur attend <option>mds_beacon_grace</option> secondes (par défaut, 15 secondes) avant de marquer le daemon comme <emphasis>laggy</emphasis> (lent à réagir). Vous pouvez configurer un ou plusieurs daemons de secours qui prendront le relais lors du basculement du daemon MDS.
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>Configuration des daemons de secours</title>
   <para>
    Plusieurs paramètres de configuration contrôlent le comportement d'un daemon de secours. Vous pouvez les spécifier dans le fichier <filename>ceph.conf</filename> sur l'hôte où le daemon MDS s'exécute. Le daemon charge ces paramètres au démarrage et les envoie au moniteur.
   </para>
   <para>
    Par défaut, si aucun de ces paramètres n'est utilisé, tous les daemons MDS qui ne détiennent pas de rang seront utilisés comme daemons de secours pour n'importe quel rang.
   </para>
   <para>
    Les paramètres qui associent un daemon de secours à un nom ou un rang particulier ne garantissent pas que le daemon sera uniquement utilisé pour ce rang. Ils signifient que, lorsque plusieurs daemons de secours sont disponibles, le daemon de secours associé est utilisé. Si un rang échoue et qu'un daemon de secours est disponible, ce dernier est utilisé même s'il est associé à un rang différent ou à un daemon nommé.
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       Si la valeur est true, le daemon de secours lit en continu le journal des métadonnées d'un rang supérieur. Le cache de métadonnées est donc très dynamique, ce qui accélère le processus de basculement en cas de panne du daemon desservant le rang.
      </para>
      <para>
       Un seul daemon de secours avec relecture peut être assigné à un rang supérieur. Si deux daemons sont configurés pour être daemons de secours avec relecture, l'un d'eux gagnera arbitrairement et l'autre passera en mode secours normal. 
      </para>
      <para>
       Lorsqu'un daemon passe à l'état de daemon de secours avec relecture, il ne peut être utilisé que comme daemon de secours pour le rang auquel il est associé. En cas d'échec d'un autre rang, ce daemon de secours avec relecture ne peut pas jouer le rôle de remplaçant même si tous les autres daemons de secours sont indisponibles.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       Définissez cette valeur pour que le daemon de secours ne prenne en charge un rang défaillant que si le dernier daemon qui le contient correspond à ce nom.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       Définissez cette option pour que le daemon de secours prenne en charge uniquement le rang spécifié. En cas d'échec d'un autre rang, ce daemon n'est pas utilisé pour le remplacer.
      </para>
      <para>
       Utilisez cette valeur conjointement avec <option>mds_standby_for_fscid</option> pour être précis quant au rang du système de fichiers que vous ciblez dans le cas où plusieurs systèmes de fichiers sont utilisés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       Si <option>mds_standby_for_rank</option> est défini, il s'agit simplement d'un qualificatif indiquant le rang du système de fichiers auquel il est fait référence.
      </para>
      <para>
       Si <option>mds_standby_for_rank</option> n'est pas défini, la définition de FSCID amène ce daemon à cibler n'importe quel rang dans le FSCID spécifié. Utilisez cette valeur si vous voulez utiliser un daemon pour n'importe quel rang, mais seulement sur un système de fichiers particulier.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       Ce paramètre est utilisé sur les hôtes de moniteur. La valeur par défaut est « true ».
      </para>
      <para>
       Si la valeur est « false », les daemons configurés avec <option>standby_replay=true</option> ne deviendront actifs que si le rang/nom qu'ils doivent suivre échoue. Par ailleurs, si ce paramètre est défini sur « true », un autre rang peut être assigné à un daemon configuré avec <option>standby_replay=true</option>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>Exemples</title>
   <para>
    Voici plusieurs exemples de configurations <filename>ceph.conf</filename>. Vous pouvez soit copier un fichier <filename>ceph.conf</filename> comportant la configuration de tous les daemons vers tous vos serveurs, soit disposer sur chaque serveur d'un fichier différent qui contient la configuration du daemon du serveur en question.
   </para>
   <sect3>
    <title>Paire simple</title>
    <para>
     Deux daemons MDS « a » et « b » agissant en tant que paire. Celui auquel aucun rang n'est assigné actuellement sera le suiveur de relecture de secours de l'autre.
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>Définition des quotas CephFS</title>

  <para>
   Vous pouvez définir des quotas sur n'importe quel sous-répertoire du système de fichiers Ceph. Le quota limite le nombre d'<emphasis role="bold">octets</emphasis> ou de <emphasis role="bold">fichiers</emphasis> stockés sous le point spécifié dans la hiérarchie de répertoires.
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>Limites</title>
   <para>
    L'utilisation de quotas avec CephFS présente les limites suivantes :
   </para>
   <variablelist>
    <varlistentry>
     <term>Les quotas sont coopératifs et non concurrentiels.</term>
     <listitem>
      <para>
       Les quotas Ceph s'appuient sur le client qui monte le système de fichiers pour arrêter d'écrire sur ce dernier lorsqu'une limite est atteinte. La partie serveur ne peut pas empêcher un client malveillant d'écrire autant de données qu'il en a besoin. N'utilisez pas de quotas pour empêcher la saturation du système de fichiers dans des environnements où les clients ne sont pas pleinement approuvés.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Les quotas sont imprécis.</term>
     <listitem>
      <para>
       Les processus qui écrivent sur le système de fichiers seront arrêtés peu de temps après que la limite de quota aura été atteinte. Ils seront inévitablement autorisés à écrire une certaine quantité de données au-delà de la limite configurée. Les systèmes d'écriture clients seront arrêtés quelques dixièmes de seconde après avoir franchi la limite configurée.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Les quotas sont implémentés dans le client du kernel à partir de la version 4.17.</term>
     <listitem>
      <para>
       Les quotas sont pris en charge par le client de l'espace utilisateur (libcephfs, ceph-fuse). Les clients de kernel Linux des versions 4.17 et ultérieures prennent en charge les quotas CephFS sur les grappes SUSE Enterprise Storage 6. Les clients de kernel (y compris les versions récentes) ne sont pas en mesure de gérer les quotas sur des grappes plus anciennes, même s'ils parviennent à définir les attributs étendus des quotas.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Configurez les quotas avec prudence lorsqu'ils sont utilisés avec des restrictions de montage basées sur le chemin.</term>
     <listitem>
      <para>
       Le client doit avoir accès à l'inode de répertoire sur lequel les quotas sont configurés afin de les appliquer. Si le client dispose d'un accès restreint à un chemin spécifique (par exemple <filename>/home/user</filename>) sur la base de la fonction MDS et qu'un quota est configuré sur un répertoire ancêtre auquel il n'a pas accès (<filename>/home</filename>), le client n'appliquera pas ce quota. Lorsque vous utilisez des restrictions d'accès basées sur le chemin, veillez à configurer le quota sur un répertoire auquel le client a accès (par exemple <filename>/home/user</filename> ou /home/user/quota_dir).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>Configuration</title>
   <para>
    Vous pouvez configurer les quotas CephFS à l'aide d'attributs étendus virtuels :
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       Configure une limite de <emphasis>fichiers</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       Configure une limite d'<emphasis>octets</emphasis>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    Si les attributs apparaissent sur un inode de répertoire, un quota y est configuré. S'ils ne sont pas présents, aucun quota n'est défini sur ce répertoire (mais il est encore possible d'en configurer un sur un répertoire parent).
   </para>
   <para>
    Pour définir un quota de 100 Mo, exécutez :
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Pour définir un quota de 10 000 fichiers, exécutez :
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    Pour afficher la configuration de quota, exécutez :
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>quota non défini</title>
    <para>
     Si la valeur de l'attribut étendu est « 0 », le quota n'est pas défini.
    </para>
   </note>
   <para>
    Pour supprimer un quota, exécutez :
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>Gestion des instantanés CephFS</title>

  <para>
   Les instantanés CephFS créent une vue en lecture seule du système de fichiers au moment où ils sont réalisés. Vous pouvez créer un instantané dans n'importe quel répertoire. L'instantané couvrira toutes les données du système de fichiers sous le répertoire spécifié. Après avoir créé un instantané, les données mises en mémoire tampon sont vidées de façon asynchrone à partir de divers clients. La création d'un instantané est dès lors très rapide.
  </para>

  <important>
   <title>systèmes de fichiers multiples</title>
   <para>
    Si vous avez plusieurs systèmes de fichiers CephFS partageant une réserve unique (via des espaces de noms), leurs instantanés entreront en collision, et la suppression d'un instantané entraînera des données de fichier manquantes pour d'autres instantanés partageant la même réserve.
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>Création d'instantanés</title>
   <para>
    La fonction d'instantané CephFS est activée par défaut sur les nouveaux systèmes de fichiers. Pour l'activer sur les systèmes de fichiers existants, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    Une fois que vous activez des instantanés, tous les répertoires de CephFS auront un sous-répertoire spécial <filename>.snap</filename>.
   </para>
   <para>
    Les clients de kernel CephFS ont une limite : ils ne peuvent pas gérer plus de 400 instantanés dans une arborescence de répertoires. Le nombre d'instantanés doit toujours être maintenu en dessous de cette limite, quel que soit le client que vous utilisez. Si vous utilisez des clients CephFS plus anciens, tels que SLE12-SP3, gardez à l'esprit que dépasser la limite de 400 instantanés est préjudiciable pour les opérations, car le client va se bloquer.
   </para>
   <tip>
    <title>nom personnalisé pour le sous-répertoire d'instantanés</title>
    <para>
     Vous pouvez configurer un nom différent pour le sous-répertoire d'instantanés en définissant le paramètre <option>client snapdir</option>.
    </para>
   </tip>
   <para>
    Pour créer un instantané, créez un sous-répertoire sous le répertoire <filename>.snap</filename> avec un nom personnalisé. Par exemple, pour créer un instantané du répertoire <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename>, exécutez la commande suivante :
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>Suppression d'instantanés</title>
   <para>
    Pour supprimer un instantané, supprimez son sous-répertoire au sein du répertoire <filename>.snap</filename> :
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
