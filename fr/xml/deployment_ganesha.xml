<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Installation de NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modification</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>oui</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha fournit un accès NFS à Object Gateway ou à CephFS. Dans SUSE Enterprise Storage 6, les versions 3 et 4 de NFS sont prises en charge. NFS Ganesha s'exécute dans l'espace utilisateur au lieu de l'espace kernel, et interagit directement avec Object Gateway ou CephFS.
 </para>
 <warning>
  <title>accès interprotocole</title>
  <para>
   Les clients CephFS et NFS natifs ne sont pas limités par les verrouillages de fichiers obtenus via Samba, et inversement. Les applications qui s'appuient sur le verrouillage de fichiers interprotocole peuvent présenter des altérations des données si les chemins de partage Samba soutenus par CephFS sont accessibles par d'autres moyens.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Préparation</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Informations générales</title>
   <para>
    Pour déployer NFS Ganesha, vous devez ajouter un élément <literal>role-ganesha</literal> à votre fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Pour plus d'informations, reportez-vous à la <xref linkend="policy-configuration"/>. NFS Ganesha a également besoin d'un <literal>role-rgw</literal> ou d'un <literal>role-mds</literal> présent dans le fichier <filename>policy.cfg</filename>.
   </para>
   <para>
    Bien qu'il soit possible d'installer et d'exécuter le serveur NFS Ganesha sur un noeud Ceph déjà existant, il est recommandé de l'exécuter sur un hôte dédié disposant d'un accès à la grappe Ceph. Les hôtes du client ne font généralement pas partie de la grappe, mais ils doivent disposer d'un accès réseau au serveur NFS Ganesha.
   </para>
   <para>
    Pour activer le serveur NFS Ganesha à tout moment après l'installation initiale, ajoutez le <literal>role-ganesha</literal> au fichier <filename>policy.cfg</filename> et réexécutez au moins les phases 2 et 4 de DeepSea. Pour plus d'informations, reportez-vous à la <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha est configuré via le fichier <filename>/etc/ganesha/ganesha.conf</filename> qui existe sur le noeud NFS Ganesha. Toutefois, ce fichier est écrasé chaque fois que la phase 4 de DeepSea est exécutée. Par conséquent, il est recommandé de modifier le modèle utilisé par Salt, qui est le fichier <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> sur Salt Master. Pour plus d'informations sur le fichier de configuration, reportez-vous au <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Résumé des conditions requises</title>
   <para>
    Les conditions suivantes doivent être remplies avant de pouvoir exécuter les phases 2 et 4 de DeepSea pour installer NFS Ganesha :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Au moins un noeud doit être assigné au <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      Vous ne pouvez définir qu'un seul <literal>role-ganesha</literal> par minion.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha a besoin d'une instance Object Gateway ou de CephFS pour fonctionner.
     </para>
    </listitem>
    <listitem>
     <para>
      L'instance NFS basée sur le kernel doit être désactivée sur les minions dotés du rôle <literal>role-ganesha</literal>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Exemple d'installation</title>

  <para>
   Cette procédure fournit un exemple d'installation qui utilise à la fois la passerelle Object Gateway et les couches d'abstraction du système de fichiers (FSAL) CephFS de NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Si vous ne l'avez pas encore fait, exécutez les phases 0 et 1 de DeepSea avant de poursuivre cette procédure.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Après avoir exécuté la phase 1 de DeepSea, modifiez le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> et ajoutez la ligne
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Remplacez <replaceable>NODENAME</replaceable> (NOM_NOEUD) par le nom d'un noeud de la grappe.
    </para>
    <para>
     Vérifiez également qu'un <literal>role-mds</literal> et un <literal>role-rgw</literal> sont assignés.
    </para>
   </step>
   <step>
    <para>
     Exécutez au moins les phases 2 et 4 de DeepSea. L'exécution de la phase 3 entre les deux est recommandée.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Vérifiez que NFS Ganesha fonctionne en contrôlant que le service NFS Ganesha est en cours d'exécution sur le noeud de minion :
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configuration haute disponibilité active-passive</title>

  <para>
   Cette section fournit un exemple de configuration active-passive à deux noeuds des serveurs NFS Ganesha. La configuration requiert l'extension SUSE Linux Enterprise High Availability Extension. Les deux noeuds sont appelés <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem>.
  </para>

  <important>
   <title>colocalisation des services</title>
   <para>
    Les services qui ont leur propre tolérance aux pannes et leur propre équilibrage de la charge ne doivent pas être exécutés sur des noeuds de grappe qui sont délimités pour les services de basculement. Par conséquent, n'exécutez pas les services Ceph Monitor, MDS, iSCSI ou Ceph OSD sur des configurations haute disponibilité.
   </para>
  </important>

  <para>
   Pour plus d'informations sur l'extension SUSE Linux Enterprise High Availability Extension, reportez-vous à l'adresse <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Installation classique</title>
   <para>
    Dans cette configuration, <systemitem class="domainname">earth</systemitem> a l'adresse IP <systemitem class="ipaddress">192.168.1.1</systemitem> et <systemitem class="domainname">mars</systemitem> <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Par ailleurs, deux adresses IP virtuelles flottantes sont utilisées, permettant aux clients de se connecter au service indépendamment du noeud physique sur lequel il s'exécute. L'adresse IP <systemitem class="ipaddress">192.168.1.10</systemitem> est utilisée pour l'administration des grappes avec Hawk2 et <systemitem class="ipaddress">192.168.2.1</systemitem> est utilisée exclusivement pour les exportations NFS. Cela facilite l'application des restrictions de sécurité ultérieurement.
   </para>
   <para>
    La procédure suivante décrit l'exemple d'installation. Pour plus d'informations, reportez-vous à l'adresse <link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Préparez les noeuds NFS Ganesha sur Salt Master :
     </para>
     <substeps>
      <step>
       <para>
        Exécutez les phases 0 et 1 de DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assignez aux noeuds <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem> le <literal>role-ganesha</literal> dans le fichier <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> :
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Exécutez les phases 2 à 4 de DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Enregistrez l'extension SUSE Linux Enterprise High Availability Extension sur <systemitem class="domainname">earth</systemitem> et <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installez <package>ha-cluster-bootstrap</package> sur les deux noeuds :
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Initialisez la grappe sur <systemitem class="domainname">earth</systemitem> :
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Permettez à <systemitem class="domainname">mars</systemitem> de rejoindre la grappe :
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Vérifiez l'état de la grappe. Vous devriez voir deux noeuds ajoutés à la grappe :
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Sur les deux noeuds, désactivez le lancement automatique du service NFS Ganesha au démarrage :
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Lancez le shell <command>crm</command> sur <systemitem class="domainname">earth</systemitem> :
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      Les commandes suivantes sont exécutées avec le shell crm.
     </para>
    </step>
    <step>
     <para>
      Sur <systemitem class="domainname">earth</systemitem>, exécutez le shell crm afin d'exécuter les commandes suivantes permettant de configurer la ressource pour les daemons NFS Ganesha en tant que clone du type de ressource systemd :
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Créez une IPAddr2 primitive avec le shell crm :
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Pour configurer une relation entre le serveur NFS Ganesha et l'adresse IP virtuelle flottante, nous utilisons la colocalisation et le classement.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilisez la commande <command>mount</command> à partir du client pour vous assurer que la configuration de la grappe est terminée :
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Nettoyage des ressources</title>
   <para>
    En cas d'échec de NFS Ganesha sur l'un des noeuds, par exemple <systemitem class="domainname">earth</systemitem>, résolvez le problème et nettoyez la ressource. Ce n'est que lorsque la ressource est nettoyée qu'elle peut effectuer un basculement vers <systemitem class="domainname">earth</systemitem> en cas d'échec de NFS Ganesha au niveau de <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Pour nettoyer la ressource :
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Configuration d'une ressource ping</title>
   <para>
    Il peut arriver que le serveur ne puisse pas contacter le client en raison d'un problème réseau. Une ressource ping peut détecter et atténuer ce problème. La configuration de cette ressource est facultative.
   </para>
   <procedure>
    <step>
     <para>
      Définissez la ressource ping :
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> est une liste d'adresses IP séparées par des espaces. Une ressource ping sera régulièrement envoyée aux adresses IP pour vérifier les pannes du réseau. Si un client doit toujours avoir accès au serveur NFS, ajoutez-le à la liste <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Créez un clone :
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      La commande suivante crée une contrainte pour le service NFS Ganesha. Elle impose au service de se déplacer vers un autre noeud lorsque <literal>host_list</literal> est inaccessible.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HA et DeepSea</title>
   <para>
    DeepSea ne prend pas en charge la configuration NFS Ganesha HA. Pour empêcher l'échec de DeepSea après la configuration de NFS Ganesha HA, excluez le démarrage et l'arrêt du service NFS Ganesha à partir de la phase 4 de DeepSea :
   </para>
   <procedure>
    <step>
     <para>
      Copiez <filename>/srv/salt/ceph/ganesha/default.sls</filename> vers <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Supprimez l'entrée <literal>.service</literal> du fichier <filename>/srv/salt/ceph/ganesha/ha.sls</filename> afin qu'il se présente comme suit :
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Ajoutez la ligne suivante à <filename>/srv/pillar/ceph/stack/global.yml</filename> :
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Configuration active-active</title>

  <para>
   Cette section fournit un exemple de configuration NFS Ganesha active-active simple. L'objectif est de déployer deux serveurs NFS Ganesha superposés au-dessus du même CephFS existant. Les serveurs correspondent à deux noeuds de grappe Ceph avec des adresses séparées. Les clients doivent être répartis entre eux manuellement. Dans cette configuration, un <quote>basculement</quote> désigne le démontage et le remontage manuels de l'autre serveur sur le client.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Conditions préalables</title>
   <para>
    Pour notre exemple de configuration, vous avez besoin des éléments suivants :
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Une grappe Ceph en cours d'exécution. Reportez-vous à la <xref linkend="ceph-install-stack"/> pour plus de détails sur le déploiement et la configuration de la grappe Ceph à l'aide de DeepSea.
     </para>
    </listitem>
    <listitem>
     <para>
      Au moins une instance CephFS configurée. Reportez-vous au <xref linkend="cha-ceph-as-cephfs"/> pour plus de détails sur le déploiement et la configuration de CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Deux noeuds de grappe Ceph avec NFS Ganesha déployé. Reportez-vous au <xref linkend="cha-as-ganesha"/> pour plus de détails sur le déploiement de NFS Ganesha.
     </para>
     <tip>
      <title>utilisez des serveurs dédiés</title>
      <para>
       Bien que les noeuds NFS Ganesha puissent partager des ressources avec d'autres services associés à Ceph, nous vous recommandons d'utiliser des serveurs dédiés pour de meilleures performances.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    Après avoir déployé les noeuds NFS Ganesha, vérifiez que la grappe est opérationnelle et que les réserves CephFS par défaut sont là :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Configuration de NFS Ganesha</title>
   <para>
    Vérifiez que les deux noeuds NFS Ganesha ont le fichier <filename>/etc/ganesha/ganesha.conf</filename> installé. Ajoutez les blocs suivants, s'ils n'existent pas encore, au fichier de configuration, afin d'activer RADOS comme interface dorsale de récupération de NFS Ganesha.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   Vous pouvez trouver les valeurs de <replaceable>rados_pool</replaceable> et <replaceable>pool_namespace</replaceable> en vérifiant la ligne déjà existante dans la configuration du formulaire :</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   La valeur de l'option <replaceable>nodeid</replaceable> correspond au nom de domaine complet de la machine, et la valeur des options <replaceable>UserId</replaceable> et <replaceable>Ceph_Conf</replaceable> est disponible dans le bloc <replaceable>RADOS_URLS</replaceable> déjà existant.
   </para>
   <para>
    Étant donné que les versions héritées de NFS nous empêchent de supprimer la période bonus de manière précoce et prolongent donc un redémarrage serveur, nous désactivons les options de NFS antérieures à la version 4.2. Nous désactivons également une grosse partie du caching de NFS Ganesha puisque les bibliothèques Ceph effectuent déjà un caching agressif.
   </para>
   <para>
    L'interface dorsale de récupération « rados_cluster » stocke ses informations dans les objets RADOS. Bien que cela ne représente pas un gros volume de données, nous voulons qu'elles soient hautement disponibles. Nous utilisons la réserve de métadonnées CephFS à cette fin et déclarons un nouvel espace de noms « ganesha » au sein de cette réserve afin d'établir une séparation par rapport aux objets CephFS.
   </para>
   <note>
    <title>ID de noeud de grappe</title>
    <para>
     La majeure partie de la configuration est identique entre les deux hôtes, mais l'option <option>nodeid</option> dans le bloc « RADOS_KV » doit être une chaîne unique pour chaque noeud. Par défaut, NFS Ganesha définit <option>nodeid</option> sur le nom d'hôte du noeud.
    </para>
    <para>
     Si vous devez utiliser des valeurs fixes différentes des noms d'hôte, vous pouvez par exemple définir <option>nodeid = 'a'</option> sur un noeud et <option>nodeid = 'b'</option> sur l'autre.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Remplissage de la base de données de bonus de grappe</title>
   <para>
    Nous devons vérifier que tous les noeuds de la grappe se connaissent. Pour ce faire, nous utilisons un objet RADOS qui est partagé entre les hôtes. NFS Ganesha utilise cet objet pour communiquer l'état actuel concernant une période bonus.
   </para>
   <para>
    Le paquetage <package>nfs-ganesha-rados-grace</package> contient un outil de ligne de commande pour interroger et manipuler cette base de données. Si le paquetage n'est pas installé sur au moins un des noeuds, installez-le avec la commande suivante :
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    Nous allons utiliser la commande pour créer la base de données et ajouter les deux ID de noeud (<option>nodeid</option>). Dans notre exemple, les deux noeuds NFS Ganesha sont nommés <literal>ses6min1.example.com</literal> et <literal>ses6min2.example.com</literal>. Sur l'un des hôtes NFS Ganesha, exécutez la commande suivante :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    Cela crée la base de données de bonus et y ajoute à la fois « ses6min1.example.com » et « ses6min2.example.com ». La dernière commande vide l'état actuel. Les hôtes récemment ajoutés sont toujours considérés comme appliquant la période bonus, de sorte qu'ils ont tous deux le drapeau « E ». Les valeurs « cur » et « rec » montrent les périodes actuelles et de récupération, ce qui nous permet de déterminer quels sont les hôtes autorisés à effectuer la récupération et quand.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Redémarrage des services NFS Ganesha</title>
   <para>
    Sur les deux noeuds NFS Ganesha, redémarrez les services associés :
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    Une fois les services redémarrés, vérifiez la base de données de bonus :
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>effacement du drapeau « E »</title>
    <para>
     Notez que les deux noeuds ont effacé leur drapeau « E », indiquant qu'ils n'appliquent plus la période bonus et sont maintenant en mode de fonctionnement normal.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Conclusion</title>
   <para>
    Après avoir terminé toutes les étapes précédentes, vous pouvez monter le NFS exporté à partir de l'un des deux serveurs NFS Ganesha et effectuer des opérations NFS normales sur ces derniers.
   </para>
   <para>
    Notre exemple de configuration suppose que si l'un des deux serveurs NFS Ganesha tombe en panne, vous le redémarrez manuellement dans les 5 minutes. Après 5 minutes, le serveur de métadonnées peut annuler la session que le client NFS Ganesha détenait et tout l'état qui lui était associé. Si les fonctionnalités de la session sont annulées avant que le reste de la grappe entre dans la période bonus, les clients du serveur risquent de ne pas être en mesure de récupérer la totalité de leur état.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>Pour plus d'informations</title>

  <para>
   Pour plus d'informations, reportez-vous au <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
