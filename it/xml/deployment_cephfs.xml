<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>Installazione di CephFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modifica</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sì</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Il file system Ceph (CephFS) è un file system POSIX compatibile che utilizza un cluster di storage Ceph per memorizzare i dati. CephFS utilizza lo stesso sistema di cluster dei dispositivi di blocco Ceph, storage oggetto Ceph con le API S3 e Swift o associazioni native (<systemitem>librados</systemitem>).
 </para>
 <para>
  Per utilizzare CephFS, deve essere presente un cluster di storage Ceph in esecuzione e almeno un <emphasis>server metadati Ceph</emphasis> in esecuzione.
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>Scenari e guida CephFS supportati</title>

  <para>
   Con SUSE Enterprise Storage 6, SUSE introduce il supporto ufficiale per molti scenari in cui si utilizza il componente CephFS distribuito e scalato. Questa voce descrive i limiti concreti e fornisce una guida per i casi d'uso suggeriti.
  </para>

  <para>
   Una distribuzione CephFS supportata deve rispettare questi requisiti:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Almeno un Metadata Server. SUSE consiglia di distribuire diversi nodi con il ruolo MDS. Solo uno sarà <literal>attivo</literal> e il resto <literal>passivo</literal>. Ricordare di menzionare tutti i nodi MON nel comando <command>mount</command> quando si monta il CephFS da un client.
    </para>
   </listitem>
   <listitem>
    <para>
     I client sono SUSE Linux Enterprise Server 12 SP3 o versioni più recenti o SUSE Linux Enterprise Server 15 o versioni più recenti e utilizzano il driver del modulo <literal>cephfs</literal> del kernel. Il modulo FUSE non è supportato.
    </para>
   </listitem>
   <listitem>
    <para>
     Le quote CephFS sono supportate in SUSE Enterprise Storage 6 ed è possibile impostarle su qualsiasi sottodirectory del file system Ceph. La quota consente di limitare il numero di <literal>byte</literal> o <literal>file</literal> memorizzati al di sotto del punto specificato nella gerarchia della directory. Per ulteriori informazioni, vedere <xref linkend="cephfs-quotas"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS supporta modifiche del layout del file come documentato in <xref linkend="cephfs-layouts"/>. Tuttavia, mentre il file system è montato da qualsiasi client, i nuovi pool di dati potrebbero non venire aggiunti a un file system CephFS esistente (<literal>ceph mds add_data_pool</literal>). Possono essere aggiunti solo mentre il file system non è montato.
    </para>
   </listitem>
   <listitem>
     <para>
       Almeno un server di metadati. SUSE consiglia di distribuire diversi nodi con il ruolo MDS. Per default, i daemon MDS aggiuntivi iniziano come daemon di <literal>standby</literal>, fungendo da backup per l'MDS attivo. Sono inoltre supportati più daemon MDS attivi (fare riferimento alla <xref linkend="ceph-cephfs-multimds"/>).
     </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Metadata Server Ceph</title>

  <para>
   Il Metadata Server (MDS) Ceph memorizza i metadati per il CephFS. I dispositivi di blocco Ceph e lo storage oggetto Ceph <emphasis>non</emphasis> utilizzano MDS. I MDS consentono agli utenti del file system POSIX di eseguire comandi di base, come <command>ls</command> o <command>find</command>, senza caricare eccessivamente il cluster di storage Ceph.
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>Aggiunta di un Metadata Server</title>
   <para>
    È possibile distribuire MDS durante il processo di distribuzione iniziale del cluster come descritto in <xref linkend="ceph-install-stack"/>, oppure aggiungerlo a un cluster già distribuito come descritto in <xref linkend="salt-adding-nodes"/>.
   </para>
   <para>
    Dopo aver distribuito MDS, consentire al servizio <literal>Ceph OSD/MDS</literal> nelle impostazioni del firewall del server dove è distribuito MDS: avviare <literal>yast</literal>, selezionare <menuchoice> <guimenu>Security and Users (Sicurezza e utenti)</guimenu> <guimenu>Firewall</guimenu> <guimenu>Allowed Services (Servizi consentiti)</guimenu> </menuchoice> e nel menu a discesa <guimenu>Service to Allow (Servizio da consentire)</guimenu> selezionare <guimenu>Ceph OSD/MDS</guimenu>. Se per il nodo Ceph MDS non è consentito traffico completo, il montaggio di un file system non riesce, anche se altre operazioni possono funzionare correttamente.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>Configurazione di un Metadata Server</title>
   <para>
    È possibile definire con precisione il comportamento di MDS inserendo opzioni pertinenti nel file di configurazione <filename>ceph.conf</filename>.
   </para>
   <variablelist>
    <title>Impostazioni del server di metadati</title>
    <varlistentry>
     <term>mon force standby active</term>
     <listitem>
      <para>
       Se è impostata su "true" (default), i monitor forzano l'attivazione della modalità di riproduzione in standby. Viene impostata nelle sezioni <literal>[mon]</literal> o <literal>[global]</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache memory limit</option></term>
     <listitem>
      <para>
       Il limite di memoria software (in byte) che il MDS applica per la cache. Gli amministratori devono utilizzare questa invece della precedente impostazione <option>mds cache size</option>. Il valore predefinito è 1 GB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option></term>
     <listitem>
      <para>
       La prenotazione della cache (memoria o nodi) da mantenere per la cache MDS. Il MDS, quando inizia a toccare la prenotazione, revoca le capacità del client finché la dimensione della cache si riduce per ripristinare la riserva. Il default è 0,05.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache size</term>
     <listitem>
      <para>
       Numero di inode da memorizzare nella cache. Un valore pari a 0 (default) indica un numero illimitato. Si consiglia di utilizzare <option>mds cache memory limit</option> per limitare la quantità di memoria utilizzata dalla cache MDS.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache mid</term>
     <listitem>
      <para>
       Punto di inserimento di nuovi elementi nell'LRU della cache (a partire dall'alto). Il valore di default è 0.7.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir commit ratio</term>
     <listitem>
      <para>
       Frazione della directory modificata prima che Ceph esegua il commit tramite un aggiornamento completo al posto di uno parziale. Il valore di default è 0,5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir max commit size</term>
     <listitem>
      <para>
       Dimensioni massime di un aggiornamento della directory prima che Ceph lo suddivida in transazioni più piccole. Il valore di default è 90 MB.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds decay halflife</term>
     <listitem>
      <para>
       Half-life della temperatura della cache MDS. Il valore di default è 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon interval</term>
     <listitem>
      <para>
       Frequenza espressa in secondi dei messaggi beacon inviati al monitor. Il valore di default è 4.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon grace</term>
     <listitem>
      <para>
       Intervallo senza beacon prima che Ceph dichiari lento un MDS e possibilmente lo sostituisca. Il valore di default è 15.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds blacklist interval</term>
     <listitem>
      <para>
       Permanenza nel black list degli MDS con errori nella mappa OSD. Questa impostazione consente di controllare il tempo di permanenza dei daemon MDS con errori nel black list della mappa OSD. Non influisce sulla permanenza nel back list degli elementi aggiunti manualmente da un amministratore. Ad esempio, il comando <command>ceph osd blacklist add</command> utilizzerà il tempo di black list di default. Il valore di default è 24 * 60.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds reconnect timeout</term>
     <listitem>
      <para>
       Intervallo di tempo espresso in secondi da attendere per la riconnessione dei client durante il riavvio di MDS. Il valore di default è 45.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds tick interval</term>
     <listitem>
      <para>
       Frequenza di esecuzione dei task periodici interni da parte di MDS. Il valore di default è 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dirstat min interval</term>
     <listitem>
      <para>
       Intervallo di tempo minimo espresso in secondi in cui tentare di evitare la propagazione delle statistiche ricorsive nell'albero. Il valore di default è 1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds scatter nudge interval</term>
     <listitem>
      <para>
       Rapidità di propagazione delle modifiche dirstat. Il valore di default è 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds client prealloc inos</term>
     <listitem>
      <para>
       Quantità di numeri inode da preallocare per sessione client. Il valore di default è 1000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds early reply</term>
     <listitem>
      <para>
       Determina se MDS deve consentire ai client di visualizzare i risultati delle richieste prima di eseguire il commit sul journal. L'impostazione di default è "true".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds use tmap</term>
     <listitem>
      <para>
       Utilizza la mappa semplice per gli aggiornamenti della directory. L'impostazione di default è "true".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds default dir hash</term>
     <listitem>
      <para>
       Funzione da utilizzare per l'hashing dei file sui frammenti della directory. Il valore di default è 2 (ovvero "rjenkins").
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log skip corrupt events</term>
     <listitem>
      <para>
       Determina se MDS deve tentare di ignorare gli eventi del journal danneggiati durante la riproduzione del journal. L'impostazione di default è "false".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max events</term>
     <listitem>
      <para>
       Numero massimo di eventi nel journal prima dell'avvio della limitazione. Impostarla a 1 (default), per disabilitare i limiti.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max segments</term>
     <listitem>
      <para>
       Numero massimo di segmenti (oggetti) nel journal prima dell'avvio della limitazione. Impostarla a -1 per disabilitare i limiti. Il valore di default è 30.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max expiring</term>
     <listitem>
      <para>
       Numero massimo di segmenti da estinguere in parallelo. Il valore di default è 20.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log eopen size</term>
     <listitem>
      <para>
       Numero massimo di inode in un evento EOpen. Il valore di default è 100.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal sample interval</term>
     <listitem>
      <para>
       Determina la frequenza di campionamento della temperatura della directory per le decisioni di frammentazione. Il valore di default è 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal replicate threshold</term>
     <listitem>
      <para>
       Temperatura massima prima che Ceph tenti di replicare i metadati su altri nodi. Il valore di default è 8000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal unreplicate threshold</term>
     <listitem>
      <para>
       Temperatura minima prima che Ceph interrompa la replica dei metadati su altri nodi. Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split size</term>
     <listitem>
      <para>
       Dimensioni massime della directory prima che MDS suddivida un frammento di directory in bit più piccoli. Il valore di default è 10000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split rd</term>
     <listitem>
      <para>
       Temperatura di lettura della directory massima prima che Ceph suddivida un frammento di directory. Il valore di default è 25000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split wr</term>
     <listitem>
      <para>
       Temperatura di scrittura della directory massima prima che Ceph suddivida un frammento di directory. Il valore di default è 10000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split bits</term>
     <listitem>
      <para>
       Numero di bit in base a cui suddividere un frammento di directory. Il valore di default è 3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal merge size</term>
     <listitem>
      <para>
       Dimensioni minime della directory prima che Ceph tenti di unire i frammenti di directory adiacenti. Il valore di default è 50.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal interval</term>
     <listitem>
      <para>
       Frequenza espressa in secondi degli scambi di workload tra MDS. Il valore di default è 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment interval</term>
     <listitem>
      <para>
       Ritardo espresso in secondi tra la capacità di suddivisione o unione di un frammento e l'esecuzione della modifica della frammentazione. Il valore di default è 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment fast factor</term>
     <listitem>
      <para>
       Rapporto in base al quale i frammenti possono superare le dimensioni di suddivisione prima che venga eseguita immediatamente una suddivisione ignorando l'intervallo di frammentazione. Il valore di default è 1.5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment size max</term>
     <listitem>
      <para>
       Dimensioni massime di un frammento prima che eventuali nuove voci vengano rifiutate con ENOSPC. Il valore di default è 100000.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal idle threshold</term>
     <listitem>
      <para>
       Temperatura minima prima che Ceph riesegua la migrazione di un sottoalbero al relativo albero superiore. Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal mode</term>
     <listitem>
      <para>
       Metodo di calcolo del carico MDS:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         0 = Ibrido.
        </para>
       </listitem>
       <listitem>
        <para>
         1 = Velocità e latenza della richiesta.
        </para>
       </listitem>
       <listitem>
        <para>
         2 = Carico della CPU.
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min rebalance</term>
     <listitem>
      <para>
       Temperatura minima del sottoalbero prima che Ceph esegua la migrazione. Il valore di default è 0.1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min start</term>
     <listitem>
      <para>
       Temperatura minima del sottoalbero prima che Ceph esegua la ricerca di un sottoalbero. Il valore di default è 0.2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need min</term>
     <listitem>
      <para>
       Frazione minima delle dimensioni di destinazione di un sottoalbero da accettare. Il valore di default è 0,8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need max</term>
     <listitem>
      <para>
       Frazione massima delle dimensioni di destinazione di un sottoalbero da accettare. Il valore di default è 1.2.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal midchunk</term>
     <listitem>
      <para>
       Ceph eseguirà la migrazione dei sottoalberi superiori a tale frazione delle dimensioni di destinazione del sottoalbero. Il valore di default è 0.3.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal minchunk</term>
     <listitem>
      <para>
       Ceph ignorerà i sottoalberi inferiori a tale frazione delle dimensioni di destinazione del sottoalbero. Il valore di default è 0,001.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal min</term>
     <listitem>
      <para>
       Numero minimo di iterazioni del servizio di bilanciamento prima che Ceph rimuova una destinazione MDS meno recente dalla mappa MDS. Il valore di default è 5.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal max</term>
     <listitem>
      <para>
       Numero massimo di iterazioni del servizio di bilanciamento prima che Ceph rimuova una destinazione MDS meno recente dalla mappa MDS. Il valore di default è 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds replay interval</term>
     <listitem>
      <para>
       Intervallo di polling del journal quando in modalità di riproduzione in standby ("hot standby"). Il valore di default è 1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds shutdown check</term>
     <listitem>
      <para>
       Intervallo di polling della cache durante la chiusura di MDS. Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds thrash fragments</term>
     <listitem>
      <para>
       Ceph unirà o suddividerà in frammenti le directory in modo casuale. Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache on map</term>
     <listitem>
      <para>
       Ceph eseguirà il dump dei contenuti della cache MDS in un file su ciascuna mappa MDS. L'impostazione di default è "false".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache after rejoin</term>
     <listitem>
      <para>
       Ceph eseguirà il dump dei contenuti della cache MDS in un file dopo essere rientrato nella cache durante il processo di recupero. L'impostazione di default è "false".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for name</term>
     <listitem>
      <para>
       Un daemon MDS andrà in standby per un altro daemon MDS il cui nome è specificato in questa impostazione.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for rank</term>
     <listitem>
      <para>
       Un daemon MDS andrà in standby per un altro daemon MDS di questa classificazione. Il valore di default è -1.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby replay</term>
     <listitem>
      <para>
       Determina se un daemon MDS Ceph deve effettuare il polling e riprodurre il log di un MDS attivo ("hot standby"). L'impostazione di default è "false".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds min caps per client</term>
     <listitem>
      <para>
       Imposta il numero minimo di capacità che un client può sospendere. Il valore di default è 100.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds max ratio caps per client</term>
     <listitem>
      <para>
       Imposta il rapporto massimo di capacità correnti che possono essere richiamate durante la pressione della cache MDS. Il valore di default è 0,8.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>Impostazioni dell'utilità di journaling del server di metadati</title>
    <varlistentry>
     <term>journaler write head interval</term>
     <listitem>
      <para>
       Frequenza con cui aggiornare l'oggetto di intestazione del journal. Il valore di default è 15.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler prefetch periods</term>
     <listitem>
      <para>
       Numero di intervalli di striping per la lettura in avanti durante la riproduzione del journal. Il valore di default è 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journal prezero periods</term>
     <listitem>
      <para>
       Numero di intervalli di striping da azzerare prima della posizione di scrittura. Il valore di default è 10.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch interval</term>
     <listitem>
      <para>
       Latenza aggiuntiva massima espressa in secondi impostata in modo artificiale. Il valore di default è 0,001.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch max</term>
     <listitem>
      <para>
       Numero massimo di byte in base a cui ritardare lo svuotamento. Il valore di default è 0.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   Se si ha un cluster di storage Ceph in stato corretto con almeno un server metadati Ceph, è possibile creare e montare il file system Ceph. Accertare che il client disponga di connettività di rete e di un corretto portachiavi di autenticazione.
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>Creazione di CephFS</title>
   <para>
    Un CephFS richiede almeno due pool RADOS: uno per i <emphasis>dati</emphasis> e uno per i <emphasis>metadati</emphasis>. Quando si configurano tali pool, considerare:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      L'utilizzo di un più alto livello di replica per il pool metadati, in quanto eventuali perdite di dati in questo pool possono rendere l'intero file system inaccessibile.
     </para>
    </listitem>
    <listitem>
     <para>
      L'utilizzo di storage a più bassa latenza, come i SSD per il pool di metadati, in quanto verrà migliorata la latenza osservata delle operazioni del file system sui client.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Quando si assegna un <literal>role-mds</literal> in <filename>policy.cfg</filename>, i pool richiesti vengono creati automaticamente. È possibile creare manualmente i pool <literal>cephfs_data</literal> e <literal>cephfs_metadata</literal> per l'ottimizzazione delle prestazioni manuali prima di configurare il Metadata Server. DeepSea non crea questi pool se esistono già.
   </para>
   <para>
    Per ulteriori informazioni sulla gestione dei pool, vedere <xref linkend="ceph-pools"/>.
   </para>
   <para>
    Per creare i due pool richiesti, ad esempio "cephfs_data" e "cephfs_metadata", con le impostazioni di default da utilizzare con CephFS, eseguire i comandi indicati:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    È possibile utilizzare i pool EC invece dei pool replicati. Si consiglia di utilizzare solo pool EC per requisiti di basse prestazioni e accesso casuale non frequente, ad esempio storage a freddo, backup, archiviazione. CephFS sui pool EC richiede l'attivazione di BlueStore e il pool deve avere l'opzione <literal>allow_ec_overwrite</literal> impostata. È possibile impostare questa opzione eseguendo <command>ceph osd pool set ec_pool allow_ec_overwrites true</command>.
   </para>
   <para>
    La codifica di cancellazione aggiunge significativo overhead alle operazioni del file system, in particolare i piccoli aggiornamenti. Questo overhead è inerente all'impiego della codifica di cancellazione come meccanismo di tolleranza dell'errore. Questa penalità è un compromesso per overhead di spazio di storage sensibilmente ridotto.
   </para>
   <para>
    Quando si creano i pool, è possibile abilitare il file system con il comando <command>ceph fs new</command>:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    Ad esempio:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    È possibile controllare che il file system sia stato creato elencando tutti i CephFS disponibili:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Dopo aver creato il file system, il MDS sarà in grado di entrare in uno stato <emphasis>attivo</emphasis>. Ad esempio, in un singolo sistema MDS:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>altri argomenti</title>
    <para>
     Ulteriori informazioni su task specifici, ad esempio montaggio, smontaggio e impostazione CephFS avanzata, sono disponibili in <xref linkend="cha-ceph-cephfs"/>.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>Dimensione cluster MDS</title>
   <para>
    Un'istanza CephFS può essere servita da più daemon MDS attivi. Tutti i daemon MDS attivi assegnati a un'istanza CephFS distribuiscono la struttura di directory del file system tra di loro e perciò aumentano il carico dei client concorrenti. Per aggiungere un daemon MDS attivo a un'istanza CephFS, ne è necessario uno di standby di riserva. Avviare un daemon aggiuntivo o utilizzare un'istanza standby esistente.
   </para>
   <para>
    Il comando seguente visualizza il numero corrente di daemon MDS attivi e passivi.
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mds stat</screen>
   <para>
    Il comando seguente imposta il numero di MDS attivi a due in un'istanza del file system.
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    Per ridurre il cluster MDS prima di un aggiornamento, sono necessari due passaggi. Innanzitutto, impostare <option>max_mds</option> in modo che rimanga solo un'istanza:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    disattivare quindi esplicitamente gli altri daemon MDS attivi:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    dove <replaceable>rank</replaceable> è il numero di un daemon MDS attivo di un'istanza del file system, compreso tra 0 e <option>max_mds</option>-1.
   </para>
   <para>
    Si consiglia di lasciare almeno un MDS come daemon di standby.
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>Aggiornamenti e cluster MDS </title>
   <para>
    Durante gli aggiornamenti di Ceph, i flag della caratteristica su un'istanza del file system possono cambiare (di solito aggiungendo nuove caratteristiche). I daemon incompatibili (ad esempio le versioni precedenti) non sono in grado di funzionare con una caratteristica incompatibile impostata e rifiutano di avviarsi. Questo significa che l'aggiornamento e il riavvio di un daemon può provocare l'arresto e il rifiuto di avvio di tutti gli altri daemon non ancora aggiornati. Per questo motivo, si consiglia di ridurre il cluster MDS attivo alla dimensione uno e di interrompere tutti i daemon in standby prima di aggiornare Ceph. I passaggi manuali di questa procedura di aggiornamento sono i seguenti:
   </para>
   <procedure>
    <step>
     <para>
      Aggiornare i pacchetti relativi al Ceph con <command>zypper</command>.
     </para>
    </step>
    <step>
     <para>
      Ridurre a un'istanza il cluster MDS attivo come descritto sopra e interrompere tutti i daemon MDS di standby tramite le relative unità <systemitem class="daemon">systemd</systemitem> su tutti gli altri nodi:
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Riavviare quindi solo il singolo daemon MDS rimanente, provocandone il riavvio con il binario aggiornato.
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      Riavviare tutti gli altri daemon MDS e reimpostare l'impostazione <option>max_mds</option> desiderata.
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    Se si utilizza DeepSea, seguire questa procedura nel caso in cui il pacchetto
    <package>ceph</package> sia stato aggiornato durante le fasi 0 e 4. È possibile eseguire questa procedura mentre i client hanno l'istanza CephFS montata e l'I/O è in corso. Tenere tuttavia presente che vi sarà una breve pausa di I/O durante il riavvio del MDS. I client si ripristinano automaticamente.
   </para>
   <para>
    È opportuno ridurre al massimo possibile il carico di I/O prima di aggiornare un cluster MDS. Un cluster MDS inattivo passa più rapidamente attraverso questa procedura di aggiornamento. Al contrario, su un cluster molto carico con più daemon MDS è essenziale ridurre il carico in anticipo per impedire di travolgere un singolo daemon MDS da I/O ininterrotti.
   </para>
  </sect2>

  <sect2 xml:id="cephfs-layouts">
   <title>Layout di file</title>
   <para>
    Tramite il layout dei file è possibile controllare in che modo i relativi contenuti vengono mappati agli oggetti Ceph RADOS. È possibile leggere il e scrivere sul layout di un file utilizzando gli <emphasis>attributi estesi virtuali (</emphasis> o <emphasis>xattrs</emphasis>).
   </para>
   <para>
    Il nome degli attributi xattrs di layout è diverso, a seconda se si tratta di un file normale o di una directory. Gli attributi xattrs di layout dei file normali sono denominati <literal>ceph.file.layout</literal>, mentre quelli delle directory sono chiamati <literal>ceph.dir.layout</literal>. Laddove negli esempi si fa riferimento a <literal>ceph.file.layout</literal>, sostituire la parte <literal>.dir.</literal> come opportuno nel caso delle directory.
   </para>
   <sect3>
    <title>Campi di layout</title>
    <para>
     Sono riconosciuti i seguenti campi di attributo:
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        ID o nome di un pool RADOS in cui verranno memorizzati gli oggetti dati del file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pool_namespace</term>
      <listitem>
       <para>
        Spazio dei nomi RADOS all'interno di un pool di dati su verranno scritti gli oggetti. È vuoto per default, per indicare uno spazio dei nomi di default.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_unit</term>
      <listitem>
       <para>
        Dimensioni espresse in byte di un blocco di dati utilizzato nella distribuzione RAID 0 (Striping) di un file. Tutte le unità di striping di un file hanno le stesse dimensioni. L'ultima unità di striping in genere è incompleta: rappresenta i dati nella parte finale del file e lo "spazio" inutilizzato, fino a raggiungere le dimensioni definite per l'unità di striping.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_count</term>
      <listitem>
       <para>
        Numero di unità di striping consecutive che costituiscono uno "segmento" RAID 0 (Striping) dei dati del file.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>object_size</term>
      <listitem>
       <para>
        Dimensioni espresse in byte degli oggetti RADOS in cui i dati del file sono suddivisi in porzioni.
       </para>
       <tip>
        <title>dimensioni oggetto</title>
        <para>
         RADOS applica un limite configurabile alle dimensioni dell'oggetto. Se si aumentano le dimensioni dell'oggetto CephFS oltre tale limite, le operazioni di scrittura potrebbero non riuscire. L'impostazione OSD è <option>osd_max_object_size</option>, pari a 128 MB per default. Gli oggetti RADOS di dimensioni molto elevate potrebbero impedire un normale svolgimento delle operazioni del cluster, pertanto non si consiglia di aumentare il limite delle dimensioni dell'oggetto oltre quello di default.
        </para>
       </tip>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>Lettura del layout con <command>getfattr</command></title>
    <para>
     Utilizzare il comando <command>getfattr</command> per leggere come stringa singola le informazioni sul layout di un file <filename>file</filename> di esempio:
    </para>
<screen>
<prompt>root # </prompt>touch file
<prompt>root # </prompt>getfattr -n ceph.file.layout file
# file: file
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=419430
</screen>
    <para>
     Per leggere i singoli campi di layout:
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.file.layout.pool file
# file: file
ceph.file.layout.pool="cephfs_data"
<prompt>root # </prompt>getfattr -n ceph.file.layout.stripe_unit file
# file: file
ceph.file.layout.stripe_unit="4194304"
</screen>
    <tip>
     <title>ID o nome del pool</title>
     <para>
      Durante la lettura dei layout, in genere il pool viene indicato con il nome. Tuttavia, nei rari casi in cui i pool sono appena stati creati, potrebbe venire invece restituito un ID.
     </para>
    </tip>
    <para>
     Le directory non disporranno di un layout esplicito finché questo non viene personalizzato. Se il layout non è mai stato modificato, i tentativi di lettura non riusciranno e di conseguenza verrà utilizzato il layout della directory antenato successiva con un layout esplicito.
    </para>
<screen>
<prompt>root # </prompt>mkdir dir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
dir: ceph.dir.layout: No such attribute
<prompt>root # </prompt>setfattr -n ceph.dir.layout.stripe_count -v 2 dir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>Scrittura dei layout con <command>setfattr</command></title>
    <para>
     Utilizzare il comando <command>setfattr</command> per modificare i campi di layout di un file <command>file</command> di esempio:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd lspools
0 rbd
1 cephfs_data
2 cephfs_metadata
<prompt>root # </prompt>setfattr -n ceph.file.layout.stripe_unit -v 1048576 file
<prompt>root # </prompt>setfattr -n ceph.file.layout.stripe_count -v 8 file
# Setting pool by ID:
<prompt>root # </prompt>setfattr -n ceph.file.layout.pool -v 1 file
# Setting pool by name:
<prompt>root # </prompt>setfattr -n ceph.file.layout.pool -v cephfs_data file
</screen>
    <note>
     <title>file vuoto</title>
     <para>
      Quando i campi di layout di un file vengono modificati tramite <command>setfattr</command>, questo file deve essere vuoto, altrimenti si verificherà un errore.
     </para>
    </note>
   </sect3>
   <sect3>
    <title>Cancellazione dei layout</title>
    <para>
     Se si desidera rimuovere un layout esplicito da una directory <filename>mydir</filename> di esempio e ripristinare l'ereditarietà del layout del relativo antenato, eseguire quanto riportato di seguito:
    </para>
<screen>
<prompt>root # </prompt>setfattr -x ceph.dir.layout mydir
</screen>
    <para>
     Allo stesso modo, se è stato impostato l'attributo "pool_namespace" e si desidera modificare il layout per utilizzare invece lo spazio dei nomi di default, eseguire:
    </para>
<screen>
# Create a directory and set a namespace on it
<prompt>root # </prompt>mkdir mydir
<prompt>root # </prompt>setfattr -n ceph.dir.layout.pool_namespace -v foons mydir
<prompt>root # </prompt>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a pool_namespace=foons"

# Clear the namespace from the directory's layout
<prompt>root # </prompt>setfattr -x ceph.dir.layout.pool_namespace mydir
<prompt>root # </prompt>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a"
</screen>
   </sect3>
   <sect3>
    <title>Ereditarietà dei layout</title>
    <para>
     Quando vengono creati, i file ereditano il layout della relativa directory superiore. Tuttavia, le successive modifiche al layout della directory superiore non vengono applicate a quelle secondarie:
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# file1 inherits its parent's layout
<prompt>root # </prompt>touch dir/file1
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# update the layout of the directory before creating a second file
<prompt>root # </prompt>setfattr -n ceph.dir.layout.stripe_count -v 4 dir
<prompt>root # </prompt>touch dir/file2

# file1's layout is unchanged
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# ...while file2 has the parent directory's new layout
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file2
# file: dir/file2
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
    <para>
     I file creati come discendenti della directory ereditano anche il suo layout se non è stato impostato nessun layout per le directory intermedie:
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
<prompt>root # </prompt>mkdir dir/childdir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir/childdir
dir/childdir: ceph.dir.layout: No such attribute
<prompt>root # </prompt>touch dir/childdir/grandchild
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/childdir/grandchild
# file: dir/childdir/grandchild
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>Aggiunta di un pool di dati al server di metadati</title>
    <para>
     Prima che sia possibile utilizzare un pool con CephFS, è necessario aggiungerlo al server di metadati:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs add_data_pool cephfs cephfs_data_ssd
<prompt>cephadm@adm &gt; </prompt>ceph fs ls  # Pool should now show up
.... data pools: [cephfs_data cephfs_data_ssd ]
</screen>
    <tip>
     <title>chiavi cephx</title>
     <para>
      Assicurarsi che le chiavi cephx in uso consentano al client di accedere a questo nuovo pool.
     </para>
    </tip>
    <para>
     È quindi possibile aggiornare il layout su una directory in CephFS per utilizzare il pool aggiunto:
    </para>
<screen>
<prompt>root # </prompt>mkdir /mnt/cephfs/myssddir
<prompt>root # </prompt>setfattr -n ceph.dir.layout.pool -v cephfs_data_ssd /mnt/cephfs/myssddir
</screen>
    <para>
     Tutti i nuovi file creati all'interno di tale directory erediteranno il suo layout e posizioneranno i dati nel pool appena aggiunto. Il numero di oggetti nel pool di dati primario potrebbe continuare ad aumentare, anche se i file vengono creati nel pool appena aggiunto. Si tratta di un comportamento normale: i dati del file vengono memorizzati nel pool specificato dal layout, ma una piccola quantità di metadati viene conservata nel pool di dati primario per tutti i file.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
