<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_pools.xml" version="5.0" xml:id="ceph.pools">
 <title>Gestione di pool di memorizzazione</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:translation>sì</dm:translation>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph memorizza i dati nei pool. I pool sono gruppi logici per la memorizzazione di oggetti. Quando inizialmente si installa un cluster senza creare un pool, Ceph utilizza i pool di default per la memorizzazione dei dati. Un pool fornisce:
 </para>
 <itemizedlist mark="bullet" spacing="normal">
  <listitem>
   <para>
    <emphasis>Resilienza</emphasis>: è possibile impostare il numero di OSD cui sono consentiti errori senza perdere dati. Per i pool replicati, è il numero desiderato di copie/repliche di un oggetto. I nuovi pool vengono creati con un numero di repliche di default impostato a 3. Poiché in una configurazione tipica vengono memorizzati un oggetto e una copia aggiuntiva, è necessario impostare il numero di repliche a 2. Per i pool con codice di cancellazione, è il numero di porzioni di codifica (vale a dire <emphasis>m=2</emphasis> nel profilo del codice di cancellazione).
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Gruppi di posizionamento</emphasis>: sono strutture di dati interne per la memorizzazione dei dati in un pool negli OSD. Il modo in cui i dati vengono memorizzati nei gruppi di posizionamento in Ceph viene definito in una mappa CRUSH. È possibile impostare il numero di gruppi di posizionamento per il pool. In una configurazione tipica si utilizzano circa 100 gruppi di posizionamento per OSD per fornire il bilanciamento ottimale senza utilizzare troppe risorse di calcolo. Quando si configurano più pool, assicurarsi di impostare un numero ragionevole di gruppi di posizionamento per entrambi il pool e il cluster insieme.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Regole CRUSH</emphasis>: quando si memorizzano i dati in un pool, un set di regole a CRUSH mappato al pool consente a CRUSH di identificare una regola per il posizionamento dell'oggetto e delle rispettive repliche (o porzioni per i pool con codice di cancellazione) nel cluster. È possibile creare una regola CRUSH personalizzata per il pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Snapshot</emphasis>: quando si creano snapshot con <command>ceph osd pool mksnap</command>, si crea effettivamente uno snapshot di un determinato pool.
   </para>
  </listitem>
  <listitem>
   <para>
    <emphasis>Impostazione della proprietà</emphasis>: è possibile impostare un ID utente come proprietario di un pool.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  Per organizzare i dati nei pool, è possibile elencare, creare e rimuovere pool. È inoltre possibile visualizzare le statistiche di utilizzo per ciascun pool.
 </para>
 <sect1 xml:id="ceph.pools.associate">
  <title>Associazione di pool a un'applicazione</title>

  <para>
   Prima di utilizzare i pool, è necessario associare loro un'applicazione. I pool che verranno utilizzati con CephFS o quelli creati automaticamente da Object Gateway vengono associati automaticamente. I pool da utilizzare con RBD devono essere inizializzati mediante lo strumento <command>rbd</command> (vedere <xref linkend="ceph.rbd.commands"/> per ulteriori informazioni).
  </para>

  <para>
   Negli altri casi, è possibile associare manualmente un nome applicazione in formato libero a un pool:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application enable <replaceable>pool_name</replaceable> <replaceable>application_name</replaceable></screen>

  <tip>
   <title>nomi applicazioni di default</title>
   <para>
    CephFS utilizza il nome applicazione <literal>cephfs</literal>, per il RADOS Block Device (dispositivo di blocco RADOS) viene utilizzato <literal>rbd</literal> e per Object Gateway viene utilizzato <literal>rgw</literal>.
   </para>
  </tip>

  <para>
   È possibile associare un pool a più applicazioni, ciascuna delle quali può disporre di metadati propri. È possibile visualizzare i metadati dell'applicazione per un determinato pool mediante l'uso del seguente comando:
  </para>

<screen><prompt>root # </prompt>ceph osd pool application get <replaceable>pool_name</replaceable></screen>
 </sect1>
 <sect1 xml:id="ceph.pools.operate">
  <title>Pool operativi</title>

  <para>
   In questa sezione vengono fornite informazioni pratiche per eseguire task di base con i pool. È possibile scoprire come elencare, creare ed eliminare pool, nonché visualizzare le statistiche dei pool o gestire gli snapshot di un pool.
  </para>

  <sect2>
   <title>Elenco di pool</title>
   <para>
    Per elencare il pool del cluster, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd lspools
0 rbd, 1 photo_collection, 2 foo_pool,</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.add_pool">
   <title>Creazione di un pool</title>
   <para>
    Per creare un pool replicato, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> replicated <replaceable>crush_ruleset_name</replaceable> \
<replaceable>expected_num_objects</replaceable></screen>
   <para>
    Per creare un pool con codice di cancellazione, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool create <replaceable>pool_name</replaceable> <replaceable>pg_num</replaceable> <replaceable>pgp_num</replaceable> erasure <replaceable>erasure_code_profile</replaceable> \
 <replaceable>crush_ruleset_name</replaceable> <replaceable>expected_num_objects</replaceable></screen>
   <para>
    Se si supera il limite di gruppi di posizionamento per OSD, è possibile che <command>ceph osd pool create</command> si concluda con un errore. Il limite viene impostato con l'opzione <option>mon_max_pg_per_osd</option>.
   </para>
   <variablelist>
    <varlistentry>
     <term>pool_name</term>
     <listitem>
      <para>
       Indica il nome del pool. Deve essere univoco. Questa opzione è obbligatoria. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Indica il numero totale dei gruppi di posizionamento per il pool. Questa opzione è obbligatoria. Il valore di default è 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Indica il numero totale dei gruppi di posizionamento ai fini del posizionamento stesso. Deve essere uguale al numero totale di gruppi di posizionamento, tranne nei casi in cui questi vengono suddivisi. Questa opzione è obbligatoria. Il valore di default è 8.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_type</term>
     <listitem>
      <para>
       Indica il tipo di pool che può essere sia <emphasis>replicato</emphasis>, per recuperare dagli OSD persi mantenendo più copie degli oggetti, o <emphasis>di cancellazione</emphasis> per ottenere un tipo di capacità RAID5 generalizzata. Per i pool replicati è richiesto uno spazio di memorizzazione non elaborato maggiore, ma consentono di implementare tutte le operazioni Ceph. Per i pool con codice di cancellazione è richiesto uno spazio di memorizzazione non elaborato minore, ma consentono di implementare solo un sottoinsieme di operazioni disponibili. L'impostazione di default è "replicated".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset_name</term>
     <listitem>
      <para>
       Indica il nome del set di regole CRUSH per il pool. Se il set di regole specificato non esiste, la creazione del pool replicato si concluderà con un errore con -ENOENT. Il pool replicato creerà tuttavia un nuovo set di regole di cancellazione specificandone il nome. Il valore di default è "erasure-code" per il pool con codice di cancellazione. Per il pool replicato, viene selezionata la variabile di configurazione Ceph <option>osd_pool_default_crush_replicated_ruleset</option>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>erasure_code_profile=profile</term>
     <listitem>
      <para>
       Solo per i pool con codice di cancellazione. Utilizzare il profilo del codice di cancellazione. Deve essere un profilo esistente, come definito da <command>osd erasure-code-profile set</command>.
      </para>
      <para>
       Quando si crea un pool, impostare il numero dei gruppi di posizionamento a un valore ragionevole (ad esempio 100). Considerare anche il numero totale di gruppi di posizionamento per OSD. I gruppi di posizionamento sono costosi a livello di computer, pertanto le prestazioni saranno compromesse quando sono presenti molti pool con molti gruppi di posizionamento (ad esempio 50 pool con 100 gruppi di posizionamento ciascuno). Il punto di diminuzione delle restituzioni dipende dalla potenza dell'host OSD.
      </para>
      <para>
       Per informazioni più dettagliate sul calcolo del numero appropriato di gruppi di posizionamento per il pool, vedere <link xlink:href="http://docs.ceph.com/docs/master/rados/operations/placement-groups/">Placement Groups</link> (in lingua inglese).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>expected_num_objects</term>
     <listitem>
      <para>
       Indica il numero di oggetti previsto per il pool. Impostando questo valore, la suddivisione della cartella dei gruppi di posizionamento ha luogo al momento della creazione del pool. In tal modo si evita l'impatto di latenza con una suddivisione della cartella di runtime.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Impostazione delle quote del pool</title>
   <para>
    È possibile impostare le quote del pool per il numero massimo di byte e/o il numero massimo di oggetti per pool.
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota <replaceable>pool-name</replaceable> <replaceable>max_objects</replaceable> <replaceable>obj-count</replaceable> <replaceable>max_bytes</replaceable> <replaceable>bytes</replaceable></screen>
   <para>
    Ad esempio:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set-quota data max_objects 10000</screen>
   <para>
    Per rimuovere una quota, impostarne il valore a 0.
   </para>
  </sect2>

  <sect2 xml:id="ceph.pools.operate.del_pool">
   <title>Eliminazione di un pool</title>
   <warning>
    <title>l'eliminazione del pool è irreversibile</title>
    <para>
     I pool possono contenere dati importanti. L'eliminazione di un pool comporta la cancellazione di tutti i dati in esso contenuti e non è possibile recuperarli in alcun modo.
    </para>
   </warning>
   <para>
    Poiché l'eliminazione involontaria di un pool è un pericolo effettivo, Ceph implementa due meccanismi che impediscono che ciò accada. Prima di poter eliminare un pool è necessario disabilitare entrambi i meccanismi.
   </para>
   <para>
    Il primo di questi è il flag <literal>NODELETE</literal>. Ciascun pool è dotato di questo flag, il cui valore di default è "false". Per scoprire il valore di questo flag su un pool, eseguire il seguente comando:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool_name</replaceable> nodelete</screen>
   <para>
    Se l'output è <literal>nodelete: true</literal>, non è possibile eliminare il pool fino a quando non si modifica il flag con il seguente comando:
   </para>
<screen>ceph osd pool set <replaceable>pool_name</replaceable> nodelete false</screen>
   <para>
    Il secondo meccanismo è il parametro di configurazione esteso a tutto il cluster <option>mon allow pool delete</option>, il cui valore di default è "false". Vale a dire che per default non è possibile eliminare un pool. Il messaggio di errore visualizzato è:
   </para>
<screen>Error EPERM: pool deletion is disabled; you must first set the
mon_allow_pool_delete config option to true before you can destroy a pool</screen>
   <para>
    Per eliminare il pool malgrado questa impostazione di sicurezza, è possibile impostare temporaneamente <option>mon allow pool delete</option> su "true", eliminare il pool e riportare il parametro su "false":
   </para>
<screen><prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=true
<prompt>root # </prompt>ceph osd pool delete pool_name pool_name --yes-i-really-really-mean-it
<prompt>root # </prompt>ceph tell mon.* injectargs --mon-allow-pool-delete=false</screen>
   <para>
    Con il comando <command>injectargs</command> viene visualizzato il seguente messaggio:
   </para>
<screen>injectargs:mon_allow_pool_delete = 'true' (not observed, change may require restart)</screen>
   <para>
    Ciò conferma semplicemente che il comando è stato eseguito correttamente. Non si tratta di un errore.
   </para>
   <para>
    Se sono stati creati set di regole propri e regole per un pool creato precedentemente, considerare di rimuovere tali regole quando il pool non è più necessario. Se sono stati creati utenti con autorizzazioni esclusivamente per un pool non più esistente, considerare di eliminare anche tali utenti.
   </para>
  </sect2>

  <sect2>
   <title>Ridenominazione di un pool</title>
   <para>
    Per rinominare un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rename <replaceable>current-pool-name</replaceable> <replaceable>new-pool-name</replaceable></screen>
   <para>
    Se si rinomina un pool e si dispone di funzionalità specifiche per il pool per un utente autenticato, è necessario aggiornare le funzionalità dell'utente con il nuovo nome pool.
   </para>
  </sect2>

  <sect2>
   <title>Visualizzazione delle statistiche del pool</title>
   <para>
    Per visualizzare le statistiche di utilizzo di un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>rados df
pool name  category  KB  objects   lones  degraded  unfound  rd  rd KB  wr  wr KB
cold-storage    -   228   1         0      0          0       0   0      1   228
data            -    1    4         0      0          0       0   0      4    4
hot-storage     -    1    2         0      0          0       15  10     5   231
metadata        -    0    0         0      0          0       0   0      0    0
pool1           -    0    0         0      0          0       0   0      0    0
rbd             -    0    0         0      0          0       0   0      0    0
total used          266268          7
total avail       27966296
total space       28232564</screen>
  </sect2>

  <sect2 xml:id="ceph.pools.values">
   <title>Impostazione dei valori del pool </title>
   <para>
    Per impostare un valore a un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool-name</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>
   <para>
    È possibile impostare i valori per le seguenti chiavi:
   </para>
   <variablelist>
    <varlistentry>
     <term>size</term>
     <listitem>
      <para>
       Imposta il numero di repliche per gli oggetti nel pool. Per ulteriori informazioni, vedere <xref linkend="ceph.pools.options.num_of_replicas"/>. Solo pool replicati.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>min_size</term>
     <listitem>
      <para>
       Imposta il numero minimo di repliche richiesto per I/O. Per ulteriori informazioni, vedere <xref linkend="ceph.pools.options.num_of_replicas"/>. Solo pool replicati.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crash_replay_interval</term>
     <listitem>
      <para>
       Indica il numero di secondi da consentire ai client per riprodurre richieste riconosciute, ma non sottoposte a commit.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Indica il numero di gruppi di posizionamento per il pool. Nel caso in cui si aggiungano OSD al cluster, si deve aumentare il valore dei gruppi di posizionamento. Per informazioni dettagliate, fare riferimento a <xref linkend="storage.bp.cluster_mntc.add_pgnum"/>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Indica il numero effettivo dei gruppi di posizionamento da utilizzare quando si calcola il posizionamento dei dati.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>crush_ruleset</term>
     <listitem>
      <para>
       Indica il set di regole da utilizzare per la mappatura del posizionamento oggetti nel cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hashpspool</term>
     <listitem>
      <para>
       Impostare (1) o annullare l'impostazione (0) del flag HASHPSPOOL su un determinato pool. Se si abilita questo flag, l'algoritmo viene modificato per distribuire meglio i gruppi di posizionamento agli OSD. Dopo aver abilitato questo flag su un pool il cui flag HASHPSPOOL è stato impostato a 0, nel cluster viene avviato il recupero informazioni in modo che il posizionamento di tutti i gruppi di posizionamento sia di nuovo corretto. Si tenga presente che ciò può creare un carico I/O piuttosto elevato in un cluster, pertanto è opportuno che venga effettuata una buona pianificazione nei cluster di produzione con carico elevato.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nodelete</term>
     <listitem>
      <para>
       Impedisce la rimozione del pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nopgchange</term>
     <listitem>
      <para>
       Impedisce di modificare i valori <option>pg_num</option> e <option>pgp_num</option> del pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>nosizechange</term>
     <listitem>
      <para>
       Impedisce di modificare le dimensioni del pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>write_fadvise_dontneed</term>
     <listitem>
      <para>
       Consente di impostare/annullare l'impostazione del flag <literal>WRITE_FADVISE_DONTNEED</literal> su un determinato pool.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>noscrub,nodeep-scrub</term>
     <listitem>
      <para>
       Disabilita la pulitura (approfondita) dei dati per un pool specifico per risolvere carichi I/O temporaneamente elevati.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_type</term>
     <listitem>
      <para>
       Consente di controllare i set di accessi per i pool di cache. Per ulteriori informazioni, vedere <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link> (in lingua inglese). Questa opzione può contenere i seguenti valori: <literal>bloom</literal>, <literal>explicit_hash</literal>, <literal>explicit_object</literal>. Il valore di default è <literal>bloom</literal>, gli altri valori sono solo ai fini dei test.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_count</term>
     <listitem>
      <para>
       Indica il numero di set di accessi per i pool di cache. Più elevato è il numero, maggiore è il consumo di RAM da parte del daemon <systemitem>ceph-osd</systemitem>. Il valore di default è <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_period</term>
     <listitem>
      <para>
       Indica la durata, espressa in secondi, di un periodo di set di accessi per i pool di cache. Più elevato è il numero, maggiore è il consumo di RAM da parte del daemon <systemitem>ceph-osd</systemitem>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_fpp</term>
     <listitem>
      <para>
       Probabilità falsa positiva per il tipo di set di accessi bloom. Per ulteriori informazioni, vedere <link xlink:href="http://en.wikipedia.org/wiki/Bloom_filter">Bloom Filter</link> (in lingua inglese). L'intervallo valido è da 0,0 a 1,0; il valore di default è <literal>0,05</literal>
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>use_gmt_hitset</term>
     <listitem>
      <para>
       Forza gli OSD a utilizzare le registrazioni dell'orario GMT (ora di Greenwich) quando si crea un set di accessi per la suddivisione in livelli di cache. In tal modo ci si assicura che i nodi in fusi orari diversi restituiscano lo stesso risultato. Il valore di default è <literal>1</literal>. Tale valore non deve essere modificato.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_ratio</term>
     <listitem>
      <para>
       Indica la percentuale del pool di cache contenente oggetti modificati prima che l'agente di suddivisione in livelli di cache li svuoti nel pool di memorizzazione di supporto. Il valore di default è <literal>0,4</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_dirty_high_ratio</term>
     <listitem>
      <para>
       Indica la percentuale del pool di cache contenente oggetti modificati prima che l'agente di suddivisione in livelli di cache li svuoti nel pool di memorizzazione di supporto con una velocità più elevata. Il valore di default è <literal>0,6</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_target_full_ratio</term>
     <listitem>
      <para>
       Indica la percentuale del pool di cache contenente oggetti non modificati prima che l'agente di suddivisione in livelli di cache li rimuova dal pool di cache. Il valore di default è <literal>0,8</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_bytes</term>
     <listitem>
      <para>
       Ceph inizierà lo svuotamento o la rimozione degli oggetti quando viene attivata la soglia <option>max_bytes</option>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>target_max_objects</term>
     <listitem>
      <para>
       Ceph inizierà lo svuotamento o la rimozione degli oggetti quando viene attivata la soglia <option>max_objects</option>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_grade_decay_rate</term>
     <listitem>
      <para>
       Grado di decadimento della temperatura tra due <literal>hit_set</literal> consecutivi. Il valore di default è <literal>20</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>hit_set_search_last_n</term>
     <listitem>
      <para>
       Numero massimo <literal>N</literal> di visualizzazioni negli <literal>hit_set</literal> per il calcolo della temperatura. Il valore di default è <literal>1</literal>. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_flush_age</term>
     <listitem>
      <para>
       Tempo (in secondi) prima che l'agente di suddivisione in livelli di cache svuoti un oggetto dal pool di cache nel pool di memorizzazione.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>cache_min_evict_age</term>
     <listitem>
      <para>
       Tempo (in secondi) prima che l'agente di suddivisione in livelli di cache rimuova un oggetto dal pool di cache.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>fast_read</term>
     <listitem>
      <para>
       Se questo flag è abilitato nei pool con codice di cancellazione, la richiesta di lettura emette sottoletture a tutte le partizioni e attende di ricevere un numero sufficiente di partizioni da decodificare per servire il client. Nel caso dei plug-in di cancellazione <emphasis>jerasure</emphasis> e <emphasis>isa</emphasis>, quando vengono restituite le prime risposte <literal>K</literal>, la richiesta del client viene eseguita immediatamente utilizzando i dati decodificati da tali risposte. In tal modo si ottengono alcune risorse per migliorare le prestazioni. Attualmente il flag è supportato solo per i pool con codice di cancellazione. Il valore di default è <literal>0</literal>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_min_interval</term>
     <listitem>
      <para>
       Intervallo minimo espresso in secondi per la pulitura dei pool quando il carico del cluster è basso. Il valore di default <literal>0</literal> significa che viene utilizzato il valore <option>osd_scrub_min_interval</option> ricavato dal file di configurazione Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>scrub_max_interval</term>
     <listitem>
      <para>
       Intevallo massimo espresso in secondi per la pulitura dei pool, indipendentemente dal carico del cluster. Il valore di default <literal>0</literal> significa che viene utilizzato il valore <option>osd_scrub_max_interval</option> ricavato dal file di configurazione Ceph.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>deep_scrub_interval</term>
     <listitem>
      <para>
       Intervallo espresso in secondi per la pulitura <emphasis>approfondita</emphasis> del pool. Il valore di default <literal>0</literal> significa che viene utilizzato il valore <option>osd_deep_scrub</option> ricavato dal file di configurazione Ceph.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2>
   <title>Ottenimento dei valori del pool</title>
   <para>
    Per ottenere un valore da un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool get <replaceable>pool-name</replaceable> <replaceable>key</replaceable></screen>
   <para>
    È possibile ottenere valori per le chiavi elencate nella <xref linkend="ceph.pools.values"/> oltre che per le chiavi seguenti:
   </para>
   <variablelist>
    <varlistentry>
     <term>pg_num</term>
     <listitem>
      <para>
       Indica il numero di gruppi di posizionamento per il pool. 
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>pgp_num</term>
     <listitem>
      <para>
       Indica il numero effettivo dei gruppi di posizionamento da utilizzare quando si calcola il posizionamento dei dati. L'intervallo valido è uguale o minore di <literal>pg_num</literal>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph.pools.options.num_of_replicas">
   <title>Impostazione del numero di repliche di oggetti</title>
   <para>
    Per impostare il numero di repliche di oggetti in un pool replicato, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>poolname</replaceable> size <replaceable>num-replicas</replaceable></screen>
   <para>
    In <replaceable>num-replicas</replaceable> è incluso l'oggetto stesso. Se ad esempio si desiderano l'oggetto e due copie dello stesso per un totale di tre istanze dell'oggetto, specificare 3.
   </para>
   <para>
    Se si imposta <replaceable>num-replicas</replaceable> a 2, si otterrà solo <emphasis>una</emphasis> copia dei dati. Se si perde un'istanza oggetto, è necessario confidare nel fatto che l'altra copia non sia corrotta, ad esempio dall'ultima <link xlink:href="http://ceph.com/docs/master/rados/configuration/osd-config-ref/#scrubbing">pulitura</link> effettuata durante il recupero.
   </para>
   <para>
    L'impostazione di un pool a una replica significa che nel pool è presente <emphasis>una</emphasis> istanza esatta dell'oggetto Dati. In caso di errore dell'OSD, i dati vengono persi. Un utilizzo possibile di un pool con una replica consiste nella memorizzazione temporanea dei dati per un breve periodo.
   </para>
   <para>
    L'impostazione di più di tre repliche per un pool ne aumenta solo leggermente l'affidabilità, ma in casi rari può essere adatta. Si tenga presente che più è elevato il numero di repliche, maggiore è lo spazio su disco necessario per memorizzare le copie degli oggetti. Per la massima sicurezza dei dati, si consiglia di utilizzare i pool con codice di cancellazione. Per ulteriori informazioni, consultare <xref linkend="cha.ceph.erasure"/>.
   </para>
   <warning>
    <title>si consiglia di utilizzare più di due repliche</title>
    <para>
     Si consiglia vivamente di utilizzare solo due repliche. In caso di errore di un OSD, è molto probabile che anche il secondo OSD venga a meno a causa dell'elevato workload durante il recupero.
    </para>
   </warning>
   <para>
    Ad esempio:
   </para>
<screen><prompt>root # </prompt>ceph osd pool set data size 3</screen>
   <para>
    È possibile eseguire questo comando per ciascun pool.
   </para>
   <note>
    <para>
     Un oggetto potrebbe accettare I/O danneggiati con meno di <literal>pool size</literal> repliche. Per impostare un numero minimo di repliche obbligatorie per I/O, si deve utilizzare l'impostazione <literal>min_size</literal>. Ad esempio:
    </para>
<screen><prompt>root # </prompt>ceph osd pool set data min_size 2</screen>
    <para>
     In tal modo ci si assicura che nessun oggetto nel pool di dati riceva I/O con meno di <literal>min_size</literal> repliche.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>Ottenimento del numero di repliche di oggetti</title>
   <para>
    Per ottenere il numero di repliche di oggetti, eseguire quanto riportato di seguito:
   </para>
<screen><prompt>root # </prompt>ceph osd dump | grep 'replicated size'</screen>
   <para>
    Ceph elencherà i pool, con l'attributo <literal>replicated size</literal> evidenziato. Per default, Ceph crea due repliche di un oggetto (per un totale di tre copie o una dimensione pari a 3).
   </para>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pgnum">
   <title>Aumento del numero di gruppi di posizionamento</title>
   <para>
    Quando si crea un nuovo pool, è necessario specificare il numero dei gruppi di posizionamento per il pool (vedere <xref linkend="ceph.pools.operate.add_pool"/>). Dopo aver aggiunto più OSD nel cluster, di norma è necessario aumentare anche il numero dei gruppi di posizionamento per migliorare le prestazioni e garantire la durata dei dati. Per ciascun gruppo di posizionamento, i nodi OSD e di monitoraggio necessitano sempre di memoria, rete e CPU, soprattutto durante il recupero. Per cui, riducendo al minimo il numero dei gruppi di posizionamento è possibile salvare un numero significativo di risorse.
   </para>
   <warning>
    <title>valore di <option>pg_num</option> troppo elevato</title>
    <para>
     Quando si modifica il valore <option>pg_num</option> di un pool, è possibile che il nuovo numero di gruppi di posizionamento superi il limite consentito. Ad esempio
    </para>
<screen><prompt>root # </prompt>ceph osd pool set rbd pg_num 4096
 Error E2BIG: specified pg_num 3500 is too large (creating 4096 new PGs \
 on ~64 OSDs exceeds per-OSD max of 32)</screen>
    <para>
     Il limite impedisce la suddivisione estrema dei gruppi di posizionamento e deriva dal valore <option>mon_osd_max_split_count</option>.
    </para>
   </warning>
   <para>
    Determinare il nuovo numero corretto di gruppi di posizionamento per un cluster ridimensionato è un task complesso. Un approccio consiste nell'aumentare continuamente il numero dei gruppi di posizionamento fino a quando le prestazioni del cluster non sono ottimali. Per determinare il nuovo numero incrementato di gruppi di posizionamento, è necessario ottenere il valore del parametro <option>mon_osd_max_split_count</option> e aggiungerlo all'attuale numero di gruppi di posizionamento. Per farsi un'idea generale, osservare il seguente script:
   </para>
<screen><prompt>cephadm &gt; </prompt>max_inc=`ceph daemon mon.a config get mon_osd_max_split_count 2&gt;&amp;1 \
  | tr -d '\n ' | sed 's/.*"\([[:digit:]]\+\)".*/\1/'`
<prompt>cephadm &gt; </prompt>pg_num=`ceph osd pool get rbd pg_num | cut -f2 -d: | tr -d ' '`
<prompt>cephadm &gt; </prompt>echo "current pg_num value: $pg_num, max increment: $max_inc"
<prompt>cephadm &gt; </prompt>next_pg_num="$(($pg_num+$max_inc))"
<prompt>cephadm &gt; </prompt>echo "allowed increment of pg_num: $next_pg_num"</screen>
   <para>
    Una volta individuato il numero di gruppi di posizionamento successivo, aumentarlo con
   </para>
<screen><prompt>root # </prompt>ceph osd pool set <replaceable>pool_name</replaceable> pg_num <replaceable>next_pg_num</replaceable></screen>
  </sect2>

  <sect2 xml:id="storage.bp.cluster_mntc.add_pool">
   <title>Aggiunta di un pool</title>
   <para>
    Dopo aver prima installato un cluster, Ceph utilizza i pool di default per memorizzare i dati. Successivamente è possibile creare un nuovo pool con
   </para>
<screen><prompt>root # </prompt>ceph osd pool create</screen>
   <para>
    Per ulteriori informazioni sulla creazione del pool del cluster, vedere <xref linkend="ceph.pools.operate.add_pool"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="pools.migration">
  <title>Migrazione del pool</title>

  <para>
   Quando si crea un pool (vedere <xref linkend="ceph.pools.operate.add_pool"/>) è necessario specificarne i parametri iniziali, come il tipo di pool o il numero dei gruppi di posizionamento. Se successivamente si decide di modificare uno di questi parametri, dopo aver posizionato i dati nel pool, è necessario eseguire la migrazione dei dati del pool in un altro, i cui parametri sono adatti per la distribuzione.
  </para>

  <para>
   Esistono vari metodi di migrazione dei pool. Si consiglia di utilizzare il <emphasis>livello di cache</emphasis> perché questo metodo è trasparente, riduce il tempo di inattività del cluster e consente di evitare la duplicazione di tutti i dati del pool.
  </para>

  <sect2 xml:id="pool.migrate.cache_tier">
   <title>Migrazione con il livello di cache</title>
   <para>
    Il principio è semplice: includere il pool di cui eseguire la migrazione in un livello di cache in ordine inverso. Ulteriori dettagli sui livelli di cache sono disponibili nel <xref linkend="cha.ceph.tiered"/>. Ad esempio, per eseguire la migrazione di un pool replicato denominato "testpool" in un pool con codice di cancellazione, seguire la procedura indicata di seguito:
   </para>
   <procedure>
    <title>Migrazione del pool replicato nel pool con codice di cancellazione</title>
    <step>
     <para>
      Creare un novo pool con codice di cancellazione denominato "newpool":
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd pool create newpool 4096 4096 erasure default
</screen>
     <para>
      Adesso si hanno due pool: quello originale replicato "testpool" compilato con i dati e il nuovo pool con codice di cancellazione vuoto "newpool":
     </para>
     <figure>
      <title>Pool prima della migrazione</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate1.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Impostare il livello di cache e configurare il pool replicato "testpool" come pool di cache:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier add newpool testpool --force-nonempty
<prompt>root@minion &gt; </prompt>ceph osd cache-mode testpool forward
</screen>
     <para>
      A partire da adesso, tutti gli oggetti nuovi vengono creati nel nuovo pool:
     </para>
     <figure>
      <title>Impostazione del livello di cache</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate2.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Forzare il pool di cache in modo che tutti gli oggetti vengano spostati nel nuovo pool:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rados -p testpool cache-flush-evict-all
</screen>
     <figure>
      <title>Svuotamento dei dati</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate3.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Commutare tutti i client al nuovo pool. Fino a quando lo svuotamento di tutti i dati nel pool con codice di cancellazione non è completato, è necessario specificare un overlay in modo che la ricerca degli oggetti venga eseguita nel pool precedente:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier set-overlay newpool testpool
</screen>
     <para>
      Con l'overlay, tutte le operazioni vengono inoltrate al "testpool" replicato precedente:
     </para>
     <figure>
      <title>Impostazione dell'overlay</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate4.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Adesso è possibile commutare tutti i client agli oggetti di accesso nel nuovo pool.
     </para>
    </step>
    <step>
     <para>
      Dopo la migrazione di tutti i dati nel "newpool" con codice di cancellazione, rimuovere l'overlay e il pool di cache "testpool" precedente:
     </para>
<screen>
<prompt>root@minion &gt; </prompt>ceph osd tier remove-overlay newpool
<prompt>root@minion &gt; </prompt>ceph osd tier remove newpool testpool
</screen>
     <figure>
      <title>Completamento della migrazione</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="pool_migrate5.png" width="60%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="cha.ceph.snapshots.pool">
  <title>Snapshot del pool</title>

  <para>
   Gli snapshot del pool riprendono lo stato dell'interno pool Ceph. Con gli snapshot del pool è possibile mantenere la cronologia dello stato del pool. A seconda delle dimensioni del pool, è possibile che per la creazione degli snapshot del pool sia richiesto molto spazio di memorizzazione. Verificare sempre che lo spazio su disco nello spazio di memorizzazione correlato sia sufficiente prima di creare lo snapshot di un pool.
  </para>

  <sect2>
   <title>Creazione dello snapshot di un pool</title>
   <para>
    Per effettuare lo snapshot di un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
   <para>
    Ad esempio:
   </para>
<screen><prompt>root # </prompt>ceph osd pool mksnap pool1 snapshot1
created pool pool1 snap snapshot1</screen>
  </sect2>

  <sect2>
   <title>Rimozione dello snapshot di un pool</title>
   <para>
    Per rimuovere lo snapshot di un pool, eseguire:
   </para>
<screen><prompt>root # </prompt>ceph osd pool rmsnap <replaceable>pool-name</replaceable> <replaceable>snap-name</replaceable></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.ceph.pool.compression">
  <title>Compressione dei dati</title>

  <para>
   A partire da SUSE Enterprise Storage 5, con BlueStore viene fornita la compressione dei dati rapida per salvare spazio su disco.
  </para>

  <sect2 xml:id="sec.ceph.pool.compression.enable">
   <title>Abilitazione della compressione</title>
   <para>
    È possibile abilitare la compressione dei dati per un pool con:
   </para>
<screen><prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> ompression_algorithm snappy
<prompt>root # </prompt><command>ceph</command> osd pool set <replaceable>POOL_NAME</replaceable> compression_mode aggressive</screen>
   <para>
    Sostituire <replaceable>POOL_NAME</replaceable> con il pool per il quale si desidera abilitare la compressione.
   </para>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.compression.options">
   <title>Opzioni di compressione del pool</title>
   <para>
    Elenco completo delle opzioni di compressione:
   </para>
   <variablelist>
    <varlistentry>
     <term>compression_algorithm</term>
     <listitem>
      <para>
       Valori: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>. Default: <literal>snappy</literal>.
      </para>
      <para>
       L'algoritmo di compressione da utilizzare dipenda dal caso di utilizzo specifico. Di seguito sono riportate diverse raccomandazioni:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Non utilizzare <literal>zlib</literal>: gli algoritmi rimanenti sono migliori.
        </para>
       </listitem>
       <listitem>
        <para>
         Se è necessario un buon rapporto di compressione, utilizzare <literal>zstd</literal>. <literal>zstd</literal> non è consigliato per BlueStore a causa dell'elevato overhead della CPU durante la compressione di piccole quantità di dati.
        </para>
       </listitem>
       <listitem>
        <para>
         Se è necessario un consumo minore CPU, utilizzare <literal>lz4</literal> o <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Eseguire un benchmark di questi algoritmi su un campione dei dati effettivi continuando a osservare la CPU e il consumo di memoria del cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_mode</term>
     <listitem>
      <para>
       Valore: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Default: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: la compressione non viene eseguita mai
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: la compressione viene eseguita se suggerita come <literal>COMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: la compressione viene eseguita a meno che non sia suggerita come <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: la compressione viene eseguita sempre
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Per informazioni su come impostare il flag <literal>COMPRESSIBLE</literal> o <literal>INCOMPRESSIBLE</literal>, vedere <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/> (in lingua inglese).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_required_ratio</term>
     <listitem>
      <para>
       Valore: doppio, rapporto = SIZE_COMPRESSED / SIZE_ORIGINAL. Default: <literal>0,875</literal>
      </para>
      <para>
       Gli oggetti al di sopra di tale rapporto non verranno memorizzati compressi a causa del basso guadagno netto.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_max_blob_size</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>0</literal>
      </para>
      <para>
       Dimensioni minime degli oggetti che vengono compressi.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>compression_min_blob_size</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>0</literal>
      </para>
      <para>
       Dimensioni massime degli oggetti che vengono compressi.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.ceph.pool.bluestore_compression.options">
   <title>Opzioni di compressione globali</title>
   <para>
    È possibile impostare le seguenti opzioni di configurazione nella configurazione di Ceph e applicarle a tutti gli OSD, non solo a un singolo pool. La configurazione specifica per il pool indicata nella <xref linkend="sec.ceph.pool.compression.options"/> ha la precedenza.
   </para>
   <variablelist>
    <varlistentry>
     <term>bluestore_compression_algorithm</term>
     <listitem>
      <para>
       Valori: <literal>none</literal>, <literal>zstd</literal>, <literal>snappy</literal>, <literal>zlib</literal>. Default: <literal>snappy</literal>.
      </para>
      <para>
       L'algoritmo di compressione da utilizzare dipenda dal caso di utilizzo specifico. Di seguito sono riportate diverse raccomandazioni:
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Non utilizzare <literal>zlib</literal>, gli algoritmi rimanenti sono migliori.
        </para>
       </listitem>
       <listitem>
        <para>
         Se è necessario un buon rapporto di compressione, utilizzare <literal>zstd</literal>. <literal>zstd</literal> non è consigliato per BlueStore a causa dell'elevato overhead della CPU durante la compressione di piccole quantità di dati.
        </para>
       </listitem>
       <listitem>
        <para>
         Se è necessario un consumo minore CPU, utilizzare <literal>lz4</literal> o <literal>snappy</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Eseguire un benchmark di questi algoritmi su un campione dei dati effettivi continuando a osservare la CPU e il consumo di memoria del cluster.
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_mode</term>
     <listitem>
      <para>
       Valore: {<literal>none</literal>, <literal>aggressive</literal>, <literal>passive</literal>, <literal>force</literal>}. Default: <literal>none</literal>.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         <literal>none</literal>: la compressione non viene eseguita mai
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>passive</literal>: la compressione viene eseguita se suggerita come <literal>COMPRESSIBLE</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>aggressive</literal>: la compressione viene eseguita a meno che non sia suggerita come <literal>INCOMPRESSIBLE</literal>
        </para>
       </listitem>
       <listitem>
        <para>
         <literal>force</literal>: la compressione viene eseguita sempre
        </para>
       </listitem>
      </itemizedlist>
      <para>
       Per informazioni su come impostare il flag <literal>COMPRESSIBLE</literal> o <literal>INCOMPRESSIBLE</literal>, vedere <link xlink:href="http://docs.ceph.com/docs/doc-12.2.0-major-changes/rados/api/librados/#rados_set_alloc_hint"/> (in lingua inglese).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_required_ratio</term>
     <listitem>
      <para>
       Valore: doppio, rapporto = SIZE_COMPRESSED / SIZE_ORIGINAL. Default: <literal>0,875</literal>
      </para>
      <para>
       Gli oggetti al di sopra di tale rapporto non verranno memorizzati compressi a causa del basso guadagno netto.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>0</literal>
      </para>
      <para>
       Dimensioni massime degli oggetti che vengono compressi.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>0</literal>
      </para>
      <para>
       Dimensioni massime degli oggetti che vengono compressi.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_ssd</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>8 K</literal>
      </para>
      <para>
       Dimensioni minime degli oggetti che vengono compressi e memorizzati su un'unità SSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_ssd</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>64 K</literal>
      </para>
      <para>
       Dimensioni massime degli oggetti che vengono compressi e memorizzati su un'unità SSD.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_min_blob_size_hdd</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>128 K</literal>
      </para>
      <para>
       Dimensioni minime degli oggetti che vengono compressi e memorizzati su dischi rigidi.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>bluestore_compression_max_blob_size_hdd</term>
     <listitem>
      <para>
       Valore: intero non firmato, dimensioni in byte. Default: <literal>512 K</literal>
      </para>
      <para>
       Dimensioni massime degli oggetti che vengono compressi e memorizzati su dischi rigidi.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
</chapter>
