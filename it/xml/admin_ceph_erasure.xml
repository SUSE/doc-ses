<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_erasure.xml" version="5.0" xml:id="cha-ceph-erasure">
 <title>Pool con codice di cancellazione</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modifica</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sì</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph fornisce un'alternativa alla replica normale dei dati nei pool denominata pool <emphasis>di cancellazione</emphasis> o <emphasis>con codice di cancellazione</emphasis>. I pool di cancellazione non forniscono le funzionalità complete dei pool <emphasis>replicati</emphasis> (ad esempio, non sono in grado di memorizzare i metadati dei pool RBD), ma richiedono meno spazio di storage nominale. Un pool di cancellazione di default con 1 TB di storage di dati richiede 1,5 TB di spazio di storage nominale e prevede un solo errore del disco. Si tratta sicuramente di un vantaggio rispetto al pool replicato, che richiede 2 TB di spazio di storage nominale per lo stesso scopo.
 </para>
 <para>
  Per ulteriori informazioni sul codice di cancellazione, vedere <link xlink:href="https://en.wikipedia.org/wiki/Erasure_code"/>.
 </para>
 <note>
  <para>
   Quando si utilizza FileStore, non è possibile accedere ai pool con codice di cancellazione tramite l'interfaccia RBD a meno che non sia stato configurato un livello di cache. Fare riferimento alla <xref linkend="ceph-tier-erasure"/> per ulteriori dettagli oppure utilizzare il BlueStore di default (consultare questo riferimento: <xref linkend="about-bluestore"/>).
  </para>
 </note>
 <sect1 xml:id="ec-prerequisite">
  <title>Prerequisiti dei pool con codice di cancellazione</title>

  <para>
   Per utilizzare il codice di cancellazione, è necessario:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Definire una regola di cancellazione nella mappa CRUSH.
    </para>
   </listitem>
   <listitem>
    <para>
     Definire un profilo con codice di cancellazione che specifichi l'algoritmo da utilizzare.
    </para>
   </listitem>
   <listitem>
    <para>
     Creare un pool utilizzando la regola e il profilo menzionati in precedenza.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Tenere presente che non sarà possibile modificare il profilo e i relativi dettagli in seguito alla creazione del pool e all'aggiunta di dati.
  </para>

  <para>
   Assicurarsi che le regole CRUSH per i <emphasis>pool di cancellazione</emphasis> utilizzino <literal>indep</literal> per <literal>step</literal>. Per informazioni, vedere <xref linkend="datamgm-rules-step-mode"/>.
  </para>
 </sect1>
 <sect1 xml:id="cha-ceph-erasure-default-profile">
  <title>Creazione di un pool con codice di cancellazione di esempio</title>

  <para>
   Il pool con codice di cancellazione più semplice equivale a RAID5 e richiede almeno tre host. In questa procedura è illustrato come creare un pool ai fini del test.
  </para>

  <procedure>
   <step>
    <para>
     Il comando <command>ceph osd pool create</command> viene utilizzato per creare un pool di tipo <emphasis>cancellazione</emphasis>. <literal>12</literal> sta per il numero di gruppi di posizionamento. Con i parametri di default, il pool è in grado di gestire gli errori di un OSD.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create ecpool 12 12 erasure
pool 'ecpool' created</screen>
   </step>
   <step>
    <para>
     La stringa <literal>ABCDEFGHI</literal> viene scritta in un oggetto denominato <literal>NYAN</literal>.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -</screen>
   </step>
   <step>
    <para>
     Ai fini del test, adesso è possibile disabilitare gli OSD, ad esempio disconnettendoli dalla rete.
    </para>
   </step>
   <step>
    <para>
     Per verificare se il pool è in grado di gestire gli errori dei dispositivi, è possibile accedere al contenuto del file mediante il comando <command>rados</command>.
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cha-ceph-erasure-erasure-profiles">
  <title>Profili dei codici di cancellazione</title>

  <para>
   Quando si richiama il comando <command>ceph osd pool create</command> per creare un <emphasis>pool di cancellazione</emphasis>, viene utilizzato il profilo di default a meno che non se ne specifichi un altro. I profili definiscono la ridondanza dei dati. A tal fine impostare due parametri, denominati arbitrariamente <literal>k</literal> ed <literal>m</literal>. k ed m definiscono il numero di <literal>porzioni</literal> in cui vengono suddivisi i dati e quante porzioni di codifica vengono create. Le porzioni ridondanti vengono quindi memorizzate in OSD diversi.
  </para>

  <para>
   Definizioni necessarie per i profili dei pool di cancellazione:
  </para>

  <variablelist>
   <varlistentry>
    <term>chunk</term>
    <listitem>
     <para>
      quando si richiama la funzione di codifica, vengono restituite porzioni della stessa dimensione: le porzioni di dati che è possibile concatenare per ricostruire l'oggetto originale e le porzioni di codifica che è possibile utilizzare per ricompilare una porzione persa.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>k</term>
    <listitem>
     <para>
      il numero di porzioni di dati, ovvero il numero di porzioni in cui è suddiviso l'oggetto originale. Ad esempio, se <literal>k = 2</literal>, un oggetto da 10 KB verrà suddiviso in <literal>k</literal> oggetti da 5 KB ciascuno. Il valore <literal>min_size</literal> di default nei pool con codice di cancellazione è <literal>k + 1</literal>. Tuttavia, si consiglia di impostare il valore <literal>min_size</literal> su almeno <literal>k + 2</literal> per evitare la perdita di dati e operazioni di scrittura.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>m</term>
    <listitem>
     <para>
      il numero di porzioni di codifica, ovvero il numero di porzioni aggiuntive calcolato dalle funzioni di codifica. Esistono 2 porzioni di codifica, vale a dire che 2 OSD possono essere fuori senza perdere dati.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>crush-failure-domain</term>
    <listitem>
     <para>
      definisce a quali dispositivi vengono distribuite le porzioni. È necessario impostare come valore un tipo di compartimento. Per tutti i tipi di compartimenti, vedere <xref linkend="datamgm-buckets"/>. Se il dominio dell'errore è <literal>rack</literal>, le porzioni saranno memorizzate in rack diversi al fine di aumentare la resilienza in caso di errore dei rack. Tenere presente che ciò richiede dei rack k+m.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Con il profilo con codice di cancellazione di default utilizzato in <xref linkend="cha-ceph-erasure-default-profile"/>, i dati del cluster non andranno persi in caso di errore di un singolo OSD o di un host. Pertanto, per memorizzare 1 TB di dati è necessario uno spazio di memorizzazione effettivo di altri 0,5 TB. Ciò vuol dire che per 1 TB di dati sono necessari 1,5 TB di spazio di storage nominale (perché k=2, m=1). Ciò equivale a una normale configurazione RAID 5. Un pool replicato necessita invece di 2 TB di spazio di storage nominale per memorizzare 1 TB di dati.
  </para>

  <para>
   È possibile visualizzare le impostazioni del profilo di default con:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd erasure-code-profile get default
directory=.libs
k=2
m=1
plugin=jerasure
crush-failure-domain=host
technique=reed_sol_van</screen>

  <para>
   È importante scegliere il profilo giusto perché non è possibile modificarlo dopo la creazione del pool. È necessario creare un nuovo pool con un profilo diverso e trasferirvi tutti gli oggetti del pool precedente (vedere la <xref linkend="pools-migration"/>).
  </para>

  <para>
   I parametri più importanti del profilo sono <literal>k</literal>, <literal>m</literal> e <literal>crush-failure-domain</literal> in quanto definiscono l'overhead di memorizzazione e la durata dei dati. Ad esempio, se l'architettura desiderata deve sostenere la perdita di due rack con un overhead di storage del 66%, è possibile definire il profilo seguente. Tenere presente che ciò si applica solo alle mappe CRUSH contenenti compartimenti di tipo "rack":
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd erasure-code-profile set <replaceable>myprofile</replaceable> \
   k=3 \
   m=2 \
   crush-failure-domain=rack</screen>

  <para>
   È possibile ripetere l'esempio della <xref linkend="cha-ceph-erasure-default-profile"/> con questo nuovo profilo:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create ecpool 12 12 erasure <replaceable>myprofile</replaceable>
<prompt>cephadm@adm &gt; </prompt>echo ABCDEFGHI | rados --pool ecpool put NYAN -
<prompt>cephadm@adm &gt; </prompt>rados --pool ecpool get NYAN -
ABCDEFGHI</screen>

  <para>
   L'oggetto NYAN verrà diviso in tre (<literal>k=3</literal>) e verranno create due porzioni aggiuntive (<literal>m=2</literal>). Il valore di <literal>m</literal> definisce quanti OSD è possibile perdere simultaneamente senza perdere alcun dato. Con il comando <literal>crush-failure-domain=rack</literal> verrà creato un set di regole CRUSH che garantisce che le due porzioni non vengano memorizzate nello stesso rack.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_erasure_obj.png" width="80%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_erasure_obj.png" width="60%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </informalfigure>

  <sect2 xml:id="ec-create">
   <title>Creazione di un nuovo profilo con codice di cancellazione</title>
   <para>
    Il comando seguente consente di creare un nuovo profilo con codice di cancellazione:
   </para>
<screen>
<prompt>root # </prompt>ceph osd erasure-code-profile set <replaceable>NAME</replaceable> \
 directory=<replaceable>DIRECTORY</replaceable> \
 plugin=<replaceable>PLUGIN</replaceable> \
 stripe_unit=<replaceable>STRIPE_UNIT</replaceable> \
 <replaceable>KEY</replaceable>=<replaceable>VALUE</replaceable> ... \
 --force
</screen>
   <variablelist>
    <varlistentry>
     <term>DIRECTORY</term>
     <listitem>
      <para>
       Facoltativo. Impostare il nome della directory da cui viene caricato il plug-in del codice di cancellazione. Quello di default è <filename>/usr/lib/ceph/erasure-code</filename>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PLUGIN</term>
     <listitem>
      <para>
       Facoltativo. Utilizzare il plug-in del codice di cancellazione per calcolare le porzioni di codifica e recuperare quelle mancanti. I plug-in disponibili sono "jerasure", "isa", "lrc" e "shes". Il plug-in di default è "jerasure".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STRIPE_UNIT</term>
     <listitem>
      <para>
       Facoltativo. La quantità di dati in una porzione di dati per segmento. Ad esempio, un profilo con 2 porzioni di dati e stripe_unit=4K inserisce l'intervallo 0-4K nella porzione 0, 4K-8K nella porzione 1 e quindi 8K-12K nuovamente nella porzione 0. Per le migliori prestazioni, deve essere un multiplo di 4K. Il valore di default viene estrapolato dall'opzione di configurazione di monitoraggio <option>osd_pool_erasure_code_stripe_unit</option> al momento della creazione di un pool. Il valore "stripe_width" di un pool che utilizza questo profilo sarà il numero delle porzioni di dati moltiplicato per questo valore di "stripe_unit".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>KEY=VALUE</term>
     <listitem>
      <para>
       Le coppie di opzioni di chiavi/valori specifiche per il plug-in del codice di cancellazione selezionato.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>--force</term>
     <listitem>
      <para>
       Facoltativo. Ignora un profilo esistente con lo stesso nome e consente l'impostazione di una stripe_unit non allineata a 4K.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ec-rm">
   <title>Rimozione di un profilo con codice di cancellazione</title>
   <para>
    Il comando seguente consente di rimuovere un profilo con codice di cancellazione identificato dal relativo nome (<replaceable>NAME</replaceable>):
   </para>
<screen>
<prompt>root # </prompt>ceph osd erasure-code-profile rm <replaceable>NAME</replaceable>
</screen>
   <important>
    <para>
     Se è presente un pool che fa riferimento al profilo, l'eliminazione non riesce.
    </para>
   </important>
  </sect2>

  <sect2 xml:id="ec-get">
   <title>Visualizzazione dei dettagli di un profilo con codice di cancellazione</title>
   <para>
    Il comando seguente consente di visualizzare i dettagli di un profilo con codice di cancellazione identificato dal relativo nome (<replaceable>NAME</replaceable>):
   </para>
<screen>
<prompt>root # </prompt>ceph osd erasure-code-profile get <replaceable>NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="ec-ls">
   <title>Elenco dei profili con codice di cancellazione</title>
   <para>
    Il comando seguente consente di visualizzare un elenco dei nomi di tutti i profili con codice di cancellazione:
   </para>
<screen>
<prompt>root # </prompt>ceph osd erasure-code-profile ls
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ec-rbd">
  <title>Pool con codice di cancellazione con RADOS Block Device (dispositivo di blocco RADOS)</title>

  <para>
   Per contrassegnare un pool EC come pool RBD, applicare il rispettivo tag:
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool application enable rbd <replaceable>ec_pool_name</replaceable>
</screen>

  <para>
   RBD può memorizzare <emphasis>data</emphasis> immagine nei pool EC. Tuttavia, l'intestazione di immagine e i metadati devono comunque essere memorizzati in un pool replicato. A tal fine, presupporre di disporre di un pool denominato "rbd":
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>rbd create rbd/<replaceable>image_name</replaceable> --size 1T --data-pool <replaceable>ec_pool_name</replaceable>
</screen>

  <para>
   È possibile utilizzare normalmente l'immagine come qualsiasi altra, con la differenza che tutti i dati saranno memorizzati nel pool <replaceable>ec_pool_name</replaceable> al posto del pool "rbd".
  </para>
 </sect1>
</chapter>
