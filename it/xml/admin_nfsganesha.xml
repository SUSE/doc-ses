<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha: esportazione dei dati Ceph tramite NFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modifica</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sì</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha è un server NFS (fare riferimento a <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">Sharing File Systems with NFS</link>, in lingua inglese) che viene eseguito in uno spazio di indirizzo dell'utente come parte del kernel del sistema operativo. Con NFS Ganesha, è possibile collegare un meccanismo di memorizzazione, come Ceph, e accedervi da qualsiasi client NFS.
 </para>
 <para>
  I compartimenti S3 vengono esportati in NFS in base a ogni singolo utente, ad esempio tramite il percorso <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename>.
 </para>
 <para>
  Un CephFS viene esportato per default tramite il percorso <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename>.
 </para>
 <note>
  <title>prestazioni di NFS Ganesha</title>
  <para>
   A causa dell'aumento dell'overhead del protocollo e della latenza aggiuntiva causati dagli hop di rete extra tra il client e lo storage, l'accesso a Ceph tramite un gateway NFS potrebbe ridurre notevolmente le prestazioni dell'applicazione se confrontate con quelle dei client CephFS o degli Object Gateway nativi.
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>Installazione</title>

  <para>
   Per istruzioni sull'installazione, vedere <xref linkend="cha-as-ganesha"/>.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>Configurazione</title>

  <para>
   Per un elenco di tutti i parametri disponibili nel file di configurazione, vedere:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command> per le opzioni dello strato di astrazione del file system (FSAL, File System Abstraction Layer) CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command> per le opzioni FSAL Object Gateway.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   In questa sezione sono incluse informazioni utili per configurare il server NFS Ganesha in modo che i dati del cluster siano accessibili tramite Object Gateway e CephFS.
  </para>

  <para>
   La configurazione NFS Ganesha è composta da due parti: configurazione del servizio e configurazione delle esportazioni. La configurazione del servizio è controllata dal file <filename>/etc/ganesha/ganesha.conf</filename>. Tenere presente che le modifiche apportate a questo file vengono sovrascritte quando si esegue la fase 4 di DeepSea. Per modificare le impostazioni in modo permanente, modificare il file <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> ubicato nel Salt master. La configurazione delle esportazioni viene memorizzata nel cluster Ceph come oggetto RADOS.
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>Configurazione del servizio</title>
   <para>
    La configurazione del servizio viene memorizzata in <filename>/etc/ganesha/ganesha.conf</filename> e controlla tutte le impostazioni del daemon NFS Ganesha, inclusa la posizione di memorizzazione della configurazione delle esportazioni nel cluster Ceph. Tenere presente che le modifiche apportate a questo file vengono sovrascritte quando si esegue la fase 4 di DeepSea. Per modificare le impostazioni in modo permanente, modificare il file <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> ubicato nel Salt master.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>Sezione RADOS_URLS</title>
    <para>
     La sezione <literal>RADOS_URLS</literal> consente di configurare l'accesso al cluster Ceph per la lettura della configurazione NFS Ganesha dagli oggetti RADOS.
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        Ubicazione del percorso del file di configurazione Ceph.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        ID utente cephx.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        URL dell'oggetto RADOS per il controllo delle notifiche di ricaricamento.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>Sezione RGW</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        Punta al file <filename>ceph.conf</filename>. Quando si esegue la distribuzione con DeepSea, non è necessario modificare questo valore.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        Nome dell'utente client Ceph utilizzato da NFS Ganesha.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Nome del cluster Ceph. Attualmente SUSE Enterprise Storage 6 supporta solo un nome cluster, che per default è <literal>ceph</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>URL dell'oggetto RADOS</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     NFS Ganesha supporta la lettura della configurazione da un oggetto RADOS. La direttiva <literal>%url</literal> consente di specificare un URL RADOS che identifica l'ubicazione dell'oggetto RADOS.
    </para>
    <para>
     Un URL RADOS può essere in due formati: <literal>rados://&lt;POOL&gt;/&lt;OBJECT&gt;</literal> o <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</literal>, dove <literal>POOL</literal> è il pool RADOS in cui è memorizzato l'oggetto, <literal>NAMESPACE</literal> è lo spazio dei nomi del pool in cui è memorizzato l'oggetto e <literal>OBJECT</literal> è il nome dell'oggetto.
    </para>
    <para>
     Per supportare le capacità di gestione di NFS Ganesha del Ceph Dashboard, è necessario seguire una convenzione sul nome dell'oggetto RADOS per ogni daemon del servizio. Il nome dell'oggetto deve essere nel formato <literal>conf-<replaceable>MINION_ID</replaceable></literal>, dove MINION_ID corrisponde all'ID del Salt minion del nodo su cui è in esecuzione il servizio.
    </para>
    <para>
     Questo URL viene già generato da DeepSea e non è pertanto necessario apportarvi modifiche.
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>Modifica delle porte NFS Ganesha di default</title>
    <para>
     Per default, NFS Ganesha utilizza la porta 2049 per NFS e la 875 per il supporto rquota. Per modificare i numeri di porta di default, utilizzare le opzioni <option>NFS_Port</option> e <option>RQUOTA_Port</option> incluse nella sezione <literal>NFS_CORE_PARAM</literal>, ad esempio:
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>Configurazione delle esportazioni</title>
   <para>
    La configurazione delle esportazioni viene memorizzata come oggetto RADOS nel cluster Ceph. Ogni blocco di esportazione viene memorizzato nel proprio oggetto RADOS denominato <literal>export-&lt;id&gt;</literal>, dove <literal>&lt;id&gt;</literal> deve corrispondere all'attributo <literal>Export_ID</literal> della configurazione dell'esportazione. L'associazione tra le esportazioni e i servizi NFS Ganesha viene effettuata tramite gli oggetti <literal>conf-MINION_ID</literal>. Ogni oggetto servizio contiene un elenco degli URL RADOS per ciascuna esportazione esportata da tale servizio. Un blocco di esportazione è simile a quanto riportato di seguito:
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    Per creare l'oggetto RADOS per il blocco di esportazione riportato sopra, è necessario innanzitutto memorizzare il codice del blocco di esportazione in un file. Quindi, sarà possibile utilizzare lo strumento dell'interfaccia riga di comando RADOS per memorizzare i contenuti del file salvato in precedenza in un oggetto RADOS.
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    In seguito alla creazione dell'oggetto di esportazione, sarà possibile associare l'esportazione a un'istanza del servizio aggiungendo l'URL RADOS corrispondente dell'oggetto esportazione all'oggetto servizio. Le sezioni seguenti descrivono come configurare un blocco di esportazione.
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Sezione di esportazione principale</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        Ciascuna esportazione deve disporre di un "Export_Id" (Id di esportazione) (obbligatorio).
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        Percorso di esportazione nel pool CephFS correlato (obbligatorio). Consente di esportare le sottodirectory da CephFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        Percorso di esportazione NFS di destinazione (obbligatorio per NFSv4). Definisce sotto quale percorso di esportazione NFS sono disponibili i dati esportati.
       </para>
       <para>
        Esempio: con il valore <literal>/cephfs/</literal> e dopo aver eseguito
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        I dati CephFS sono disponibili nella directory <filename>/mnt/cephfs/</filename> sul client.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        "RO" per l'accesso in sola lettura, "RW" per l'accesso in sola scrittura e "None" per nessun accesso.
       </para>
       <tip>
        <title>restrizione dell'accesso ai client</title>
        <para>
         Se si lascia invariata l'impostazione <literal>Access_Type = RW</literal> nella sezione <literal>EXPORT</literal> principale e si limita l'accesso a un client specifico nella sezione <literal>CLIENT</literal>, gli altri client saranno comunque in grado di eseguire la connessione. Per disabilitare l'accesso a tutti i client e abilitarlo soltanto per client specifici, impostare <literal>Access_Type = None</literal> nella sezione <literal>EXPORT</literal> e quindi specificare una modalità di accesso meno restrittiva per uno o più client nella sezione <literal>CLIENT</literal>:
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        Opzione squash NFS.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        Esportazione dello strato di astrazione del file system (FSAL, File System Abstraction Layer). Vedere <xref linkend="ceph-nfsganesha-config-general-fsal"/>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>Sottosezione FSAL</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        Definisce il back end utilizzato da NFS Ganesha. I valori consentiti sono <literal>CEPH</literal> per CephFS o <literal>RGW</literal> per Object Gateway. A seconda della scelta, è necessario definire <literal>role-mds</literal> o <literal>role-rgw</literal> in <filename>policy.cfg</filename>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>Ruoli NFS Ganesha personalizzati</title>

  <para>
   È possibile definire ruoli NFS Ganesha personalizzati per i nodi del cluster. Tali ruoli vengono quindi assegnati ai ruoli in <filename>policy.cfg</filename>. I ruoli consentono:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     a nodi NFS Ganesha separati di accedere a Object Gateway e CephFS.
    </para>
   </listitem>
   <listitem>
    <para>
     di assegnare diversi utenti Object Gateway ai nodi NFS Ganesha.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Il fatto di disporre di diversi utenti Object Gateway consente ai nodi NFS Ganesha di accedere a diversi compartimenti S3. È possibile utilizzare i compartimenti S3 per il controllo dell'accesso. Nota: non confondere i compartimenti S3 con i compartimenti Ceph utilizzati nella mappa CRUSH.
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>Utenti Object Gateway diversi per NFS Ganesha</title>
   <para>
    Nella seguente procedura di esempio per Salt master è illustrato come creare due ruoli NFS Ganesha con diversi utenti Object Gateway. In questo esempio, vengono utilizzati i ruoli <literal>gold</literal> e <literal>silver</literal>, per i quali DeepSea fornisce già i file di configurazione di esempio.
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      Aprire il file <filename>/srv/pillar/ceph/stack/global.yml</filename> con un editor a scelta. Creare il file qualora non esistesse.
     </para>
    </step>
    <step>
     <para>
      Il file deve contenere le righe seguenti:
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      Successivamente, è possibile assegnare questi ruoli nel file <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Creare un file <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> e aggiungere il contenuto seguente:
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      Creare un file <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> e aggiungere il contenuto seguente:
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      Adesso, per ogni ruolo è necessario creare i modelli per il file <filename>ganesha.conf</filename>. Il modello originale di DeepSea è un buon inizio. Creare due copie:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      Sono richiesti i portachiavi per i nuovi ruoli affinché questi accedano al cluster. Per fornire l'accesso, copiare <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      Copiare il portachiavi per Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      In Object Gateway è inoltre necessaria la configurazione per i ruoli diversi:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      Assegnare i ruoli appena creati ai nodi del cluster in <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      Sostituire<replaceable>NODE1</replaceable> e <replaceable>NODE2</replaceable> con i nomi dei nodi cui si desidera assegnare i ruoli.
     </para>
    </step>
    <step>
     <para>
      Eseguire le fasi da 0 a 4 di DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>Separazione di FSAL CephFS e Object Gateway</title>
   <para>
    Nella seguente procedura di esempio per Salt master è illustrato come creare due nuovi ruoli diversi che utilizzano CephFS e Object Gateway:
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      Aprire il file <filename>/srv/pillar/ceph/rgw.sls</filename> con un editor a scelta. Creare il file qualora non esistesse.
     </para>
    </step>
    <step>
     <para>
      Il file deve contenere le righe seguenti:
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      Successivamente, è possibile assegnare questi ruoli nel file <filename>policy.cfg</filename>.
     </para>
    </step>
    <step>
     <para>
      Adesso, per ogni ruolo è necessario creare i modelli per il file <filename>ganesha.conf</filename>. Il modello originale di DeepSea è un buon inizio. Creare due copie:
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      Modificare <filename>ganesha_rgw.conf.j2</filename> e rimuovere la sezione:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Modificare <filename>ganesha_cfs.conf.j2</filename> e rimuovere la sezione:
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      Sono richiesti i portachiavi per i nuovi ruoli affinché questi accedano al cluster. Per fornire l'accesso, copiare <filename>ganesha.j2</filename>:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      È possibile rimuovere la riga <literal>caps mds = "allow *"</literal> da <filename>ganesha_rgw.j2</filename>.
     </para>
    </step>
    <step>
     <para>
      Copiare il portachiavi per Object Gateway:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      Per Object Gateway è necessaria la configurazione del nuovo ruolo:
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      Assegnare i ruoli appena creati ai nodi del cluster in <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      Sostituire<replaceable>NODE1</replaceable> e <replaceable>NODE2</replaceable> con i nomi dei nodi cui si desidera assegnare i ruoli.
     </para>
    </step>
    <step>
     <para>
      Eseguire le fasi da 0 a 4 di DeepSea.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>Operazioni supportate</title>
   <para>
    L'interfaccia RGW NFS supporta la maggior parte delle operazioni su file e directory, con le restrizioni seguenti:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>I collegamenti che includono collegamenti simbolici non sono supportati.</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Gli elenchi di controllo dell'accesso NFS non sono supportati.</emphasis> La proprietà e le autorizzazioni di gruppo e utente Unix <emphasis>sono</emphasis> supportate.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Non è possibile spostare o rinominare le directory.</emphasis> È <emphasis>possibile</emphasis> spostare i file tra le directory.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>Sono supportate soltanto le operazioni di I/O di scrittura sequenziali e complete.</emphasis> Pertanto, le operazioni di scrittura sono applicate come upload. Molte operazioni di I/O tipiche, come la modifica di file esistenti, non riusciranno perché eseguono memorizzazioni non sequenziali. Alcune utility di file scrivono apparentemente in modo sequenziale (ad esempio alcune versioni di <command>tar</command> GNU), ma potrebbero non riuscire a causa di memorizzazioni occasionali non sequenziali. Generalmente, se si effettua il montaggio tramite NFS, è possibile forzare un'operazione di I/O sequenziale dell'applicazione a eseguire operazioni di scrittura in sequenza sul server NFS tramite il montaggio sincrono (l'opzione <option>-o sync</option>). I client NFS che non sono in grado di effettuare il montaggio sincrono (ad esempio Microsoft Windows*) non potranno eseguire l'upload dei file.
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW supporta le operazioni di lettura-scrittura soltanto per i blocchi di dimensioni inferiori a 4 MB.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>Avvio o riavvio di NFS Ganesha</title>

  <para>
   Per abilitare e avviare il servizio NFS Ganesha, eseguire:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   Riavviare NFS Ganesha con:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   Quando si avvia o riavvia NFS Ganesha, il timeout di tolleranza per NFS v4 è di 90 secondi. Durante la moratoria, le nuove richieste dai client vengono rifiutate attivamente. Pertanto, è possibile che i client subiscano un rallentamento delle richieste durante lo stato di moratoria di NFS.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>Impostazione del livello di log</title>

  <para>
   È possibile modificare il livello di debug di default <literal>NIV_EVENT</literal> modificando il file <filename>/etc/sysconfig/nfs-ganesha</filename>. Sostituire <literal>NIV_EVENT</literal> con <literal>NIV_DEBUG</literal> o <literal>NIV_FULL_DEBUG</literal>. L'aumento del livello di dettaglio del log può produrre grandi quantità di dati nei file di log.
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   Quando si modifica il livello di log è necessario riavviare il servizio.
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>Verifica della condivisione NFS esportata</title>

  <para>
   Quando si utilizza NFS v3, è possibile verificare se le condivisioni NFS vengono esportate nel nodo server di NFS Ganesha:
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>Montaggio della condivisione NFS esportata</title>

  <para>
   Per montare la condivisione NFS esportata (come configurato nella <xref linkend="ceph-nfsganesha-config"/>) su un host client, eseguire:
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
