<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph-monitor">
 <title>Determinazione dello stato del cluster</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>sì</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Quando un cluster è in esecuzione, è possibile utilizzare lo strumento <command>ceph</command> per monitorarlo. Per determinare lo stato del cluster, in genere occorre verificare lo stato dei Ceph OSD, di Ceph Monitor, dei gruppi di posizionamento e dei server di metadati.
 </para>
 <tip>
  <title>modalità interattiva</title>
  <para>
   Per eseguire lo strumento <command>ceph</command> in modalità interattiva, digitare <command>ceph</command> nella riga di comando senza argomenti. La modalità interattiva è più pratica se si devono immettere più comandi <command>ceph</command> in una riga. Ad esempio:
  </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>Verifica dello stato di un cluster</title>

  <para>
   Per verificare lo stato di un cluster, eseguire quanto riportato di seguito:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph status</screen>

  <para>
   oppure
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>

  <para>
   In modalità interattiva, digitare <command>status</command> e premere<keycap function="enter"/>.
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph eseguirà la stampa dello stato del cluster. Ad esempio, un cluster Ceph di piccole dimensioni costituito da un monitoraggio e due OSD può stampare quanto riportato di seguito:
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>Verifica dello stato di integrità del cluster</title>

  <para>
   Dopo l'avvio del cluster e prima della lettura e/o scrittura dei dati, verificare lo stato di integrità del cluster:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    Se sono state specificate ubicazioni non di default per la configurazione o il portachiavi, è possibile specificarne le ubicazioni:
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   Il cluster Ceph restituisce uno dei seguenti codici di stato di integrità:
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      Uno o più OSD sono contrassegnati. È possibile che il deamon OSD sia stato interrotto o gli OSD peer potrebbero non essere in grado di raggiungere l'OSD nella rete. Tra le cause comuni sono inclusi un'interruzione o crash del daemon, un host inattivo o un'interruzione della rete.
     </para>
     <para>
      Verificare che l'host sia integro, il daemon avviato e la rete funzionante. Se ha avuto luogo un crash del daemon, è possibile che il file di log del daemon (<filename>/var/log/ceph/ceph-osd.*</filename>) contenga informazioni di debug.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN, ad esempio OSD_HOST_DOWN</term>
    <listitem>
     <para>
      Tutti gli OSD in un determinato sottoalbero CRUSH vengono contrassegnati, ad esempio tutti gli OSD in un host.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      Un OSD è un riferimento nella gerarchia della mappa CRUSH, ma non esiste. È possibile rimuovere l'OSD dalla gerarchia CRUSH con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      Le soglie di utilizzo per <emphasis>backfillfull</emphasis> (il valore di default è 0,90), <emphasis>nearfull</emphasis> (il valore di default è 0,85), <emphasis>full</emphasis> (il valore di default è 0,95) e/o <emphasis>failsafe_full</emphasis> non sono crescenti. In particolare, ci si aspetta <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>, <emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> e <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>.
     </para>
     <para>
      Per leggere i valori attuali, eseguire:
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      È possibile modificare le soglie con i comandi seguenti:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      Uno o più OSD hanno superato la soglia <emphasis>full</emphasis> e impediscono al cluster di fornire servizi di scrittura. È possibile verificare l'utilizzo da parte del pool con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
     <para>
      È possibile visualizzare il rapporto <emphasis>full</emphasis> attualmente definito con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      Una soluzione immediata per ripristinare la disponibilità di scrittura consiste nell'aumentare leggermente la soglia completa (full):
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      Aggiungere un nuovo spazio di memorizzazione al cluster installando più OSD, o eliminare i dati esistenti per liberare spazio.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      Uno o più OSD hanno superato la soglia <emphasis>backfillfull</emphasis>, impedendo il ribilanciamento dei dati nel dispositivo. Questo è un avviso preliminare che informa l'utente sull'impossibilità di completare il ribilanciamento e che il cluster è quasi pieno. È possibile verificare l'utilizzo da parte del pool con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      Uno o più OSD hanno superato la soglia <emphasis>nearfull</emphasis>. Questo è un avviso preliminare che informa l'utente che il cluster è quasi pieno. È possibile verificare l'utilizzo da parte del pool con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      Sono stati impostati uno o più flag del cluster interessato. Ad eccezione di <emphasis>full</emphasis>, è possibile impostare o eliminare i flag con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      Tali flag includono:
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         Il cluster è contrassegnato come full (pieno) e non può fornire servizi di scrittura.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd, pausewr</term>
       <listitem>
        <para>
         Letture o scritture in pausa
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         Viene impedito l'avvio degli OSD.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         I rapporti sugli errori degli OSD vengono ignorati, ad esempio quando i monitoraggi non contrassegnano gli OSD come <emphasis>down</emphasis> (inattivi).
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Gli OSD contrassegnati precedentemente come <emphasis>out</emphasis> non verranno contrassegnati di nuovo come <emphasis>in</emphasis> all'avvio.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Gli OSD <emphasis>down</emphasis> (inattivi) non verranno contrassegnati automaticamente come <emphasis>out</emphasis> dopo l'intervallo configurato.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill, norecover, norebalance</term>
       <listitem>
        <para>
         Il recupero o il ribilanciamento dei dati è sospeso.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub, nodeep_scrub</term>
       <listitem>
        <para>
         La pulitura (vedere <xref linkend="scrubbing"/>) è disabilitata.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         L'attività di suddivisione in livelli di cache è sospesa.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      Uno o più OSD presentano un flag per OSD del set di interesse. Tali flag includono:
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         All'OSD non è consentito l'avvio.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         I rapporti di errore per l'OSD specificato verranno ignorati.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         Se precedentemente questo OSD è stato contrassegnato automaticamente come <emphasis>out</emphasis> in seguito a un errore, non verrà contrassegnato come <emphasis>in</emphasis> al suo avvio.
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         Se l'OSD è inattivo, non verrà contrassegnato automaticamente come <emphasis>out</emphasis> dopo l'intervallo configurato.
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      È possibile impostare ed eliminare i flag per OSD con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      Nella mappa CRUSH vengono utilizzate impostazioni molto obsolete e deve essere aggiornata. Gli elementi ottimizzabili più obsoleti (vale a dire la versione client più vecchia in grado di connettersi al cluster) che è possibile utilizzare senza attivare questo avviso di stato di integrità vengono determinati dall'opzione di configurazione <option>mon_crush_min_required_version</option>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      Nella mappa CRUSH viene utilizzato un metodo precedente, non ottimale per calcolare i valori del peso intermedio per i compartimenti straw. La mappa CRUSH deve essere aggiornata per utilizzare il metodo più recente (<option>straw_calc_version</option>=1).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      Uno o più pool di cache non sono configurati con un set di accessi per controllare l'utilizzo, impedendo all'agente di suddivisione in livelli di identificare gli oggetti a caldo di essere svuotati o rimossi dalla cache. È possibile configurare i set di accessi nel pool di cache con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
     <para>
      Per ulteriori informazioni sulla suddivisione in livelli di cache, vedere il <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      Nessun OSD di versione precedente a Luminous v12 in esecuzione, ma il flag <option>sortbitwise</option> non è stato impostato. È necessario impostare il flag <option>sortbitwise</option> prima di poter avviare gli OSD Luminous v12 o versione più recente:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Uno o più pool hanno raggiunto la rispettiva quota e non consentono più le scritture. È possibile impostare le quote dei pool e l'utilizzo con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df detail</screen>
     <para>
      È possibile aumentare la quota del pool con
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      o eliminare alcuni dati esistenti per ridurre l'utilizzo.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      La disponibilità dei dati è ridotta, vale a dire che il cluster non è in grado di fornire servizi di richieste di potenziali letture o scritture per alcuni dati nel cluster. Nello specifico, è impossibile fornire servizi a uno o più gruppi di posizionamento il cui stato non consente richieste I/O. Gli stati dei gruppi di posizionamento problematici includono <emphasis>peering</emphasis>, <emphasis>stale (inattivo)</emphasis>, <emphasis>incomplete (incompleto)</emphasis> e la mancanza di <emphasis>active (attivo)</emphasis> (se tali condizioni non vengono annullate rapidamente). Informazioni dettagliate sui gruppi di posizionamento interessati sono recuperabili da:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph health detail</screen>
     <para>
      Nella maggior parte dei casi, la causa radice risiede nell'attuale stato di inattività di uno o più OSD. È possibile interrogare lo stato di specifici gruppi di posizionamento problematici con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      La ridondanza dei dati è ridotta per alcuni dati, vale a dire che il cluster non dispone del numero desiderato di repliche per tutti i dati (per pool replicati) o di frammenti di codice di cancellazione (per pool con codice di cancellazione). Nello specifico, per uno o più gruppi di posizionamento è impostato il flag <emphasis>degraded</emphasis> o <emphasis>undersized </emphasis> (le istanze di tale gruppo di posizionamento nel cluster non sono sufficienti) oppure non è impostato il flag <emphasis>clean</emphasis> per un periodo di tempo. Informazioni dettagliate sui gruppi di posizionamento interessati sono recuperabili da:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph health detail</screen>
     <para>
      Nella maggior parte dei casi, la causa radice risiede nell'attuale stato di inattività di uno o più OSD. È possibile interrogare lo stato di specifici gruppi di posizionamento problematici con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      È possibile che la ridondanza dei dati può sia ridotta o a rischio per alcuni dati a causa della mancanza di spazio libero nel cluster. Nello specifico, per uno o più gruppi di posizionamento è impostato il flag <emphasis>backfill_toofull</emphasis> o <emphasis>recovery_toofull</emphasis>, vale a dire che il cluster non è in grado di eseguire la migrazione o recuperare i dati perché uno o più OSD superano la soglia <emphasis>backfillfull</emphasis>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      In seguito alla pulitura dei dati (vedere <xref linkend="scrubbing"/>), nel cluster sono stati rilevati alcuni problemi di incoerenza dei dati. Nello specifico, in uno o più gruppi di posizionamento è impostato il flag <emphasis>inconsistent</emphasis> o <emphasis>snaptrim_error</emphasis>, a indicare che a seguito di un'operazione di pulitura precedente è stato individuato un problema, oppure è impostato il flag <emphasis>repair</emphasis>, a indicare che attualmente è in corso una riparazione per tale incoerenza.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      Dalle puliture dell'OSD sono state rilevate incoerenze.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Un pool di livelli di cache è quasi pieno. "Pieno" in questo contesto è determinato dalla proprietà <emphasis>target_max_bytes</emphasis> e <emphasis>target_max_objects</emphasis> nel pool di cache. Quando il pool raggiunge la soglia di destinazione, è possibile che le richieste di scrittura nel pool si blocchino quando i dati vengono svuotati e rimossi dalla cache, uno stato che di norma comporta latenze molto elevate e prestazioni scarse. È possibile regolare le dimensioni di destinazione del pool di cache con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      È inoltre possibile che le attività di svuotamento e rimozione siano bloccate a causa della disponibilità ridotta, delle prestazioni del livello base o a causa del carico complessivo del cluster.
     </para>
     <para>
      Per ulteriori informazioni sulla suddivisione in livelli di cache, vedere <xref linkend="cha-ceph-tiered"/>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      Il numero di gruppi di posizionamento in uso è sotto la soglia configurabile dei gruppi di posizionamento per OSD <option>mon_pg_warn_min_per_osd</option>. Ciò può comportare una distribuzione e bilanciamento dei dati non ottimali negli OSD del cluster, riducendo le prestazioni complessive.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      Il numero di gruppi di posizionamento in uso supera la soglia configurabile di gruppi di posizionamento per OSD <option>mon_pg_warn_max_per_osd</option>. Ciò comporta un utilizzo della memoria maggiore per i daemon OSD, un peering più lento dopo le modifiche allo stato del cluster (ad esempio, riavvii, aggiunte o rimozioni di OSD) e un carico più elevato nei Ceph Manager e Ceph Monitor.
     </para>
     <para>
      Mentre è impossibile ridurre il valore <option>pg_num</option> per i pool esistenti, il valore <option>pgp_num</option> può essere ridotto. Ciò colloca effettivamente alcuni gruppi di posizionamento sugli stessi set di OSD, mitigando alcuni impatti negativi descritti sopra. È possibile regolare il valore <option>pgp_num</option> con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      Il valore <option>pgp_num</option> di uno o più pool è inferiore a <option>pg_num</option>. Di norma ciò indica che il numero di gruppi di posizionamento è stato incrementato senza incrementare anche comportamento del posizionamento. Di norma questo problema viene risolto impostando <option>pgp_num</option> in modo che corrisponda a <option>pg_num</option>, attivando la migrazione dei dati, con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      Uno o più pool presentano un numero medio di oggetti per gruppo di posizionamento che è significativamente più elevato della media complessiva del cluster. La soglia specifica è controllata dal valore di configurazione <option>mon_pg_warn_max_object_skew</option>. Di norma ciò indica che i pool che contengono la maggior parte dei dati nel cluster hanno un numero di gruppi di posizionamento insufficiente e/o che altri pool che non contengono una tale quantità di dati hanno troppi gruppi di posizionamento. È possibile aumentare la soglia per annullare l'avviso di stato di integrità regolando l'opzione di configurazione <option>mon_pg_warn_max_object_skew</option> nei monitoraggi.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED</term>
    <listitem>
     <para>
      Un pool contiene uno o più oggetti, ma non è stato contrassegnato per l'utilizzo da parte di un'applicazione particolare. Risolvere questo avviso etichettando il pool per l'utilizzo da parte di un'applicazione. Ad esempio, se il pool viene utilizzato da RBD:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      Se il pool viene utilizzato da un'applicazione personalizzata "foo", è inoltre possibile etichettarla con il comando di livello basso:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      Uno o più pool hanno raggiunto (o sono prossimi a raggiungere) la rispettiva quota. La soglia per attivare questa condizione di errore è controllata dall'opzione di configurazione <option>mon_pool_quota_crit_threshold</option>. È possibile regolare verso l'alto o verso il basso (o rimuovere) le quote dei pool con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Impostando il valore di quota a 0, questa verrà disabilitata.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      Uno o più pool stanno per raggiungere la rispettiva quota. La soglia per attivare questa condizione di avviso è controllata dall'opzione di configurazione <option>mon_pool_quota_warn_threshold</option>. È possibile regolare verso l'alto o verso il basso (o rimuovere) le quote dei pool con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      Impostando il valore di quota a 0, questa verrà disabilitata.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      Uno o più oggetti nel cluster non vengono memorizzati nel nodo specificato dal cluster. Ciò indica che la migrazione dei dati causata da una modifica recente del cluster non è stata ancora completata. La posizione errata dei dati non rappresenta una condizione pericolosa. La coerenza dei dati non è mai a rischio e le copie precedenti degli oggetti non vengono mai rimosse finché è presente il numero di copie nuove desiderato (nelle ubicazioni desiderate).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      Impossibile individuare uno o più oggetti nel cluster. Nello specifico, gli OSD sanno che deve esistere una copia nuova o aggiornata di un oggetto, ma negli OSD attualmente attivi non è stata trovata una copia di tale versione dell'oggetto. Le richieste di lettura o scrittura negli oggetti "non trovati" verranno bloccate. Idealmente, è possibile riattivare l'OSD inattivo in cui è presente la copia più recente dell'oggetto non trovato. È possibile identificare gli OSD candidati in stato di peering per i gruppi di posizionamento responsabili dell'oggetto non trovato:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      Una o più richieste OSD impiegano molto tempo per l'elaborazione. Ciò può essere un'indicazione di carico estremo, dispositivo di memorizzazione lento o bug del software. È possibile interrogare la coda delle richieste sugli OSD in questione mediante l'esecuzione del seguente comando dall'host OSD:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      È possibile visualizzare un riepilogo delle richieste recenti più lente:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      È possibile individuare l'ubicazione di un OSD con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      Una o più richieste OSD sono state bloccate per un intervallo di tempo relativamente lungo, ad esempio 4.096 secondi. Ciò indica che lo stato del cluster non è integro da un periodo di tempo prolungato (ad esempio il numero di OSD in esecuzione o di gruppi di posizionamento inattivi non è sufficiente) o che sono presenti alcuni problemi interni dell'OSD.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      Di recente non è stata eseguita la pulitura di uno o più gruppi di posizionamento (vedere la <xref linkend="scrubbing"/>). Di norma la pulitura dei gruppi di posizionamento viene eseguita ogni <option>mon_scrub_interval</option> secondi e questo avviso si attiva quando sono trascorsi <option>mon_warn_not_scrubbed</option> secondi senza che abbia avuto luogo una pulitura. La pulitura dei gruppi di posizionamento non verrà eseguita se questi non sono contrassegnati come puliti, il che può verificarsi se sono posizionati male o sono danneggiati (vedere PG_AVAILABILITY e PG_DEGRADED di cui sopra). È possibile avviare manualmente la pulitura di un gruppo di posizionamento pulito con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      Di recente non è stata eseguita la pulitura approfondita di uno o più gruppi di posizionamento (vedere <xref linkend="scrubbing"/>). Di norma la pulitura dei gruppi di posizionamento viene eseguita ogni <option>osd_deep_mon_scrub_interval</option> secondi e questo avviso si attiva quando sono trascorsi <option>mon_warn_not_deep_scrubbed</option> secondi senza che abbia avuto luogo una pulitura. La pulitura (approfondita) dei gruppi di posizionamento non verrà eseguita se questi non sono contrassegnati come puliti, il che può verificarsi se sono posizionati male o sono danneggiati (vedere PG_AVAILABILITY e PG_DEGRADED di cui sopra). È possibile avviare manualmente la pulitura di un gruppo di posizionamento pulito con:
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    Se sono state specificate ubicazioni non di default per la configurazione o il portachiavi, è possibile specificarne le ubicazioni:
   </para>
<screen><prompt>root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-watch">
  <title>Osservazione di un cluster</title>

  <para>
   È possibile individuare lo stato immediato del cluster mediante <command>ceph -s</command>. Ad esempio, un cluster Ceph di piccole dimensioni costituito da un monitoraggio e due OSD può stampare quanto riportato di seguito quando è in esecuzione un workload:
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 &gt; max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean
</screen>

  <para>
   L'output fornisce le seguenti informazioni:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     ID cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Stato di integrità del cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Epoca della mappa di monitoraggio e stato del quorum del monitoraggio
    </para>
   </listitem>
   <listitem>
    <para>
     Epoca della mappa OSD e stato degli OSD
    </para>
   </listitem>
   <listitem>
    <para>
     Stato di Ceph Manager
    </para>
   </listitem>
   <listitem>
    <para>
     Stato di Object Gateway
    </para>
   </listitem>
   <listitem>
    <para>
     Versione della mappa del gruppo di posizionamento
    </para>
   </listitem>
   <listitem>
    <para>
     Numero di gruppi di posizionamento e pool
    </para>
   </listitem>
   <listitem>
    <para>
     Quantità <emphasis>nozionale</emphasis> di dati memorizzati e numero di oggetti memorizzati
    </para>
   </listitem>
   <listitem>
    <para>
     Quantità totale di dati memorizzati
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>calcolo dell'utilizzo dei dati da parte di Ceph</title>
   <para>
    Il valore <literal>used</literal> riflette la quantità effettiva di spazio di memorizzazione non elaborato utilizzato. Il valore <literal>xxx GB / xxx GB</literal> indica la quantità disponibile (il numero inferiore) della capacità di memorizzazione complessiva del cluster. Il numero nozionale riflette le dimensioni dei dati memorizzati prima che vengano replicati, clonati o che ne venga eseguito lo snapshot. Pertanto, di norma la quantità di dati effettivamente memorizzata supera la quantità nozionale memorizzata, poiché Ceph crea repliche dei dati e può anche utilizzare capacità di memorizzazione per la clonazione e gli snapshot.
   </para>
  </tip>

  <para>
   Altri comandi che consentono di visualizzare informazioni immediate sullo stato sono:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Per ottenere le informazioni aggiornate in tempo reale, inserire uno di questi comandi (incluso <command>ceph -s</command>) come argomento del comando <command>watch</command>:
  </para>

<screen><prompt>root # </prompt>watch -n 10 'ceph -s'</screen>

  <para>
   Premere <keycombo><keycap function="control"/><keycap>C</keycap></keycombo> quando non si desidera più visualizzare le informazioni.
  </para>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>Verifica delle statistiche sull'utilizzo di un cluster</title>

  <para>
   Per verificare l'utilizzo e la distribuzione dei dati del cluster nei diversi pool, utilizzare il comando <command>ceph df</command>. Per ottenere ulteriori dettagli, utilizzare <command>ceph df detail</command>.
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph df
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
    TOTAL     40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
POOLS:
    POOL             ID     STORED     OBJECTS    USED       %USED    MAX AVAIL
    iscsi-images      1     3.9 KiB          8    769 KiB        0       10 GiB
    cephfs_data       2     1.6 KiB          5    960 KiB        0       10 GiB
    cephfs_metadata   3      54 KiB         22    1.5 MiB        0       10 GiB
[...]
</screen>

  <para>
   Nella sezione <literal>RAW STORAGE</literal> dell'output è fornita una panoramica dello spazio di storage utilizzato dal cluster per i dati.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>CLASS</literal>: classe di storage del dispositivo. Per ulteriori dettagli sulle classi di dispositivi, fare riferimento alla <xref linkend="crush-devclasses"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>SIZE</literal>: capacità di memorizzazione complessiva del cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>: quantità di spazio libero disponibile nel cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: spazio (accumulato su tutti gli OSD) allocato esclusivamente per gli oggetti dati conservati nel dispositivo di blocco.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>: somma tra lo spazio utilizzato ("USED") e quello allocato/riservato sul dispositivo di blocco per Ceph, ad esempio la parte BlueFS per BlueStore.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>: percentuale di spazio di memorizzazione di dati non elaborati utilizzato. Utilizzare questo numero insieme a <literal>full ratio</literal> e <literal>near full ratio</literal> per assicurarsi che non si stia raggiungendo la capacità del cluster. Per ulteriori dettagli, vedere la <xref linkend="storage-capacity"/>.
    </para>
    <note>
     <title>livello di riempimento del cluster</title>
     <para>
      Quando il livello di riempimento dello spazio di storage nominale si avvicina al 100%, è necessario aggiungere ulteriore spazio di storage al cluster. Un utilizzo più elevato può comportare a singoli OSD pieni e a problemi di integrità del cluster.
     </para>
     <para>
      Utilizzare il comando <command>ceph osd df tree</command> per elencare il livello di riempimento di tutti gli OSD.
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   Nella sezione <literal>POOLS</literal> dell'output è fornito un elenco di pool e l'utilizzo nozionale di ciascuno di essi. L'output di questa sezione <emphasis>non</emphasis> riflette repliche, cloni o snapshot. Ad esempio, se si memorizza un oggetto con 1 MB di dati, l'utilizzo nozionale sarà 1 MB, ma quello effettivo può essere di 2 MB o più a seconda del numero di repliche, cloni e snapshot.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>POOL</literal>: nome del pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>: ID del pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>STORED</literal>: quantità di dati memorizzati dall'utente.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>: numero nozionale di oggetti memorizzati per pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>: quantità di spazio allocato esclusivamente ai dati da tutti i nodi OSD, indicato in KB.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>: percentuale nozionale di spazio di memorizzazione utilizzato per ciascun pool.
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>: spazio massimo disponibile nel pool specificato.
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    I numeri nella sezione POOLS sono nozionali. Non includono il numero di repliche, snapshot o cloni. Ne risulta che la somma delle quantità <literal>USED</literal> e <literal>%USED</literal> non verrà aggiunta alle quantità <literal>RAW USED</literal> e <literal>%RAW USED</literal> nella sezione <literal>RAW STORAGE</literal> dell'output.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>Verifica dello stato degli OSD</title>

  <para>
   È possibile verificare lo stato degli OSD per assicurarsi che siano attivi e funzionanti eseguendo:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd stat</screen>

  <para>
   oppure
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump</screen>

  <para>
   È inoltre possibile visualizzare gli OSD in base alla rispettiva posizione nella mappa CRUSH.
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tree</screen>

  <para>
   Ceph eseguirà la stampa di un albero CRUSH con un host, i rispettivi OSD, se attivi, e il relativo peso.
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>Verifica degli OSD pieni</title>

  <para>
   Ceph impedisce la scrittura in un OSD pieno in modo da evitare perdite di dati. In un cluster operativo, quando questo è prossimo al rispettivo rapporto di riempimento si riceve un avviso. L'impostazione di default di <command>mon osd full ratio</command> è 0,95 o 95% della capacità, prima che venga interrotta la scrittura dei dati da parte dei client. L'impostazione di default di <command>mon osd nearfull ratio</command> è 0,85 o 85% della capacità, quando viene generato avviso sullo stato di integrità.
  </para>

  <para>
   I nodi OSD pieni verranno segnalati da <command>ceph health</command>:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   oppure
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   Il modo migliore per gestire un cluster pieno consiste nell'aggiungere nuovi host/dischi OSD che consentano al cluster di ridistribuire dati nello spazio di storage che si è appena reso disponibile.
  </para>

  <tip>
   <title>esclusione degli OSD pieni</title>
   <para>
    Quando un OSD si riempie (utilizza il 100% del rispettivo spazio su disco), di norma questo si blocca rapidamente senza alcun avviso. Di seguito sono riportati alcuni suggerimenti utili per l'amministrazione dei nodi OSD.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      È necessario posizionare lo spazio su disco di ciascun OSD (di norma montato in <filename>/var/lib/ceph/osd/osd-{1,2..}</filename>) su un disco o una partizione dedicati sottostanti.
     </para>
    </listitem>
    <listitem>
     <para>
      Controllare i file di configurazione Ceph e assicurarsi che il log file Ceph non venga memorizzato nei dischi o nelle partizioni dedicati all'uso da parte degli OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      Assicurarsi che la scrittura nei dischi o nelle partizioni dedicati all'uso da parte degli OSD non venga eseguita da altri processi.
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>Verifica dello stato del monitoraggio</title>

  <para>
   Dopo aver avviato il cluster e precedentemente alla prima lettura e/o scrittura dei dati, verificare lo stato del quorum dei Ceph Monitor. Se il cluster sta già provvedendo alle richieste, controllare periodicamente lo stato dei Ceph Monitor per assicurarsi che siano in esecuzione.
  </para>

  <para>
   Per visualizzare la mappa del monitoraggio, eseguire quanto riportato di seguito:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph mon stat</screen>

  <para>
   oppure
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph mon dump</screen>

  <para>
   Per verificare lo stato del quorum per il cluster di monitoraggio, eseguire quanto riportato di seguito:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph quorum_status</screen>

  <para>
   Ceph restituirà lo stato del quorum. Ad esempio, un cluster Ceph costituito da tre monitoraggi può restituire quanto riportato di seguito:
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>Verifica degli stati dei gruppi di posizionamento</title>

  <para>
   Oggetti della mappa dei gruppi di posizionamento negli OSD. Quando si monitorano i gruppi di posizionamento, questi dovranno essere <literal>active</literal> e <literal>clean</literal>. Per una discussione dettagliata, fare riferimento alla <xref linkend="op-mon-osd-pg"/>.
  </para>
 </sect1>
 <sect1 xml:id="monitor-adminsocket">
  <title>Utilizzo del socket amministrativo</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark> Il socket amministrativo Ceph consente di interrogare un daemon tramite un'interfaccia socket. Per default, i socket Ceph risiedono in <filename>/var/run/ceph</filename>. Per accedere a un daemon tramite il socket amministrativo, eseguire il login all'host sul quale è in esecuzione il daemon e utilizzare il seguente comando:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   Per visualizzare i comandi del socket amministrativo disponibili, eseguire il seguente comando:
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   I comandi del socket amministrativo consentono di mostrare e impostare la configurazione al momento del runtime. Per ulteriori dettagli, vedere <xref linkend="ceph-config-runtime"/>.
  </para>

  <para>
   È inoltre possibile impostare i valori di configurazione direttamente in fase di runtime (il socket amministrativo evita il monitoraggio, diversamente dall'injectarg <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable>, che fa affidamento al monitoraggio ma non richiede il login diretto all'host in questione).
  </para>
 </sect1>
 <sect1 xml:id="storage-capacity">
  <title>Capacità di memorizzazione</title>

  <para>
   Quando un cluster di memorizzazione Ceph si avvicina alla capacità massima, Ceph impedisce la scrittura o la lettura dai Ceph OSD come misura di sicurezza per evitare perdite di dati. Pertanto, non è consigliabile far avvicinare un cluster di produzione al rapporto di riempimento. Questa situazione andrebbe infatti a discapito dell'elevata disponibilità. Per default il rapporto di riempimento è impostato su 0,95, ovvero il 95% della capacità. Si tratta di un valore molto aggressivo per un cluster di test contenente un numero ridotto di OSD.
  </para>

  <tip>
   <title>aumento della capacità di storage</title>
   <para>
    Durante il monitoraggio del cluster, prestare attenzione agli avvisi relativi alla percentuale <literal>nearfull</literal>. Ciò indica che nel caso di errori su alcuni OSD, potrebbe verificarsi una temporanea interruzione del servizio. Valutare la possibilità di aggiungere altri OSD per aumentare la capacità di storage.
   </para>
  </tip>

  <para>
   In uno scenario comune per i cluster di prova, l'amministratore di sistema rimuove un Ceph OSD dal cluster di memorizzazione Ceph per osservare il ribilanciamento del cluster. Quindi, rimuove un altro Ceph OSD e così via fin quando il cluster non raggiunge il rapporto di riempimento e si blocca. Si consiglia di eseguire la pianificazione della capacità anche per il cluster di prova. In questo modo, sarà possibile calcolare la capacità di riserva necessaria per mantenere un'elevata disponibilità. Idealmente, è consigliabile pianificare una serie di errori dei Ceph OSD durante cui sia possibile recuperare il cluster e ripristinarlo allo stato <literal>active + clean</literal> senza sostituire immediatamente i Ceph OSD con errori. Anche se è possibile eseguire un cluster nello stato <literal>active + degraded</literal>, non è l'ideale per un normale funzionamento.
  </para>

  <para>
   Nel diagramma seguente viene illustrato un cluster di memorizzazione Ceph semplificato che contiene 33 nodi Ceph e un Ceph OSD per host, ciascuno in grado di eseguire operazioni di lettura e scrittura su un'unità da 3 TB. La capacità effettiva di questo cluster di esempio è di 99 TB. L'opzione <option>mon osd full ratio</option> è impostata a 0,95. Se il cluster raggiunge i 5 TB di capacità rimanente, non consentirà ai client di leggere e scrivere dati. Di conseguenza, la capacità operativa del cluster di memorizzazione è 95 TB, non 99 TB.
  </para>

  <figure>
   <title>Cluster Ceph</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   In tale cluster, è normale che si verifichino errori su uno o due OSD. In uno scenario meno frequente ma possibile, può verificarsi un errore del router o dell'alimentazione del rack che causa la disattivazione simultanea di più OSD (ad esempio degli OSD da 7 a 12). In tale scenario, è consigliabile comunque puntare ad avere un cluster in grado di restare operativo e raggiungere lo stato <literal>active + clean</literal>, anche se ciò comporta l'aggiunta di alcuni host con ulteriori OSD in tempi brevi. Se l'utilizzo della capacità è troppo elevato, potrebbero verificarsi perdite di dati. Tuttavia, se l'utilizzo della capacità del cluster supera il rapporto di riempimento, è comunque preferibile scendere a compromessi sulla disponibilità dei dati per risolvere un'interruzione all'interno di un dominio di errore. Per questo motivo, si consiglia di effettuare una pianificazione seppur approssimativa della capacità.
  </para>

  <para>
   Identificare due numeri per il cluster:
  </para>

  <orderedlist>
   <listitem>
    <para>
     Il numero di OSD.
    </para>
   </listitem>
   <listitem>
    <para>
     La capacità complessiva del cluster.
    </para>
   </listitem>
  </orderedlist>

  <para>
   Se si divide la capacità totale del cluster per il numero di OSD contenuti, è possibile individuare la capacità media di un OSD del cluster. Moltiplicare questo numero per il numero di OSD su cui si prevede che si verificheranno errori in simultanea durante il normale funzionamento (un numero relativamente ridotto). Moltiplicare poi la capacità del cluster per il rapporto di riempimento fino a raggiungere la capacità operativa massima. Infine, sottrarre la quantità di dati degli OSD su cui si prevede che si verificheranno errori fino a raggiungere un rapporto di riempimento ragionevole. Ripetere la procedura indicata con un numero più elevato di errori sugli OSD (un rack di OSD) fino a ottenere un numero ragionevole per un rapporto di riempimento quasi completo.
  </para>

  <para>
   Le impostazioni seguenti si applicano soltanto durante la creazione del cluster e vengono quindi memorizzate nella mappa OSD:
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    Queste impostazioni si applicano soltanto durante alla creazione del cluster. In seguito, è necessario modificarle nella mappa OSD utilizzando i comandi <command>ceph osd set-nearfull-ratio</command> e <command>ceph osd set-full-ratio</command>.
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      Percentuale di spazio su disco utilizzata prima che un OSD venga considerato pieno (<literal>full</literal>). Il valore di default è 0,95
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      Percentuale di spazio su disco utilizzata prima che un OSD venga considerato troppo pieno (<literal>full</literal>) per il backfill. Il valore di default è 0,90
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      Percentuale di spazio su disco utilizzata prima che un OSD venga considerato quasi pieno (<literal>nearfull</literal>). Il valore di default è 0,85
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>verifica del peso dell'OSD</title>
   <para>
    Se alcuni OSD sono <literal>nearfull</literal>, ma in altri è invece disponibile una capacità elevata, potrebbero verificarsi dei problemi con il peso CRUSH per gli OSD <literal>nearfull</literal>.
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op-mon-osd-pg">
  <title>Monitoraggio di OSD e gruppi di posizionamento</title>

  <para>
   Per ottenere un'elevata disponibilità e un'alta affidabilità occorre adottare un approccio a tolleranza di errore nella gestione dei problemi hardware e software. Ceph non dispone di un single point of failure ed è in grado di provvedere alle richieste di dati nella modalità "degraded" (danneggiata). Il posizionamento dei dati di Ceph introduce un livello di riferimento indiretto per fare in modo che i dati non vengano direttamente associati a specifici indirizzi OSD. Vale a dire che per il controllo degli errori di sistema occorre individuare il gruppo di posizionamento e gli OSD sottostanti alla radice del problema.
  </para>

  <tip>
   <title>accesso in caso di errore</title>
   <para>
    Gli errori che si verificano in una parte del cluster possono impedire l'accesso a un determinato oggetto, ma non ad altri oggetti. Quando si verifica un errore, seguire la procedura di monitoraggio degli OSD e dei gruppi di posizionamento. Quindi, procedere con la risoluzione dei problemi.
   </para>
  </tip>

  <para>
   Ceph in genere esegue il ripristino automatico. Tuttavia, se i problemi continuano a verificarsi, il monitoraggio degli OSD e dei gruppi di posizionamento consentirà di identificare il problema.
  </para>

  <sect2 xml:id="op-mon-osds">
   <title>Monitoraggio degli OSD</title>
   <para>
    Un OSD può essere nello stato "in" (<emphasis>dentro il cluster</emphasis>) o nello stato "out" (<emphasis>fuori dal cluster</emphasis>). Contemporaneamente, può essere "up" (<emphasis>attivo e in esecuzione</emphasis>) o "down" (<emphasis>disattivato e non in esecuzione</emphasis>). Se un OSD è nello stato "up", può trovarsi dentro il cluster (sarà possibile leggere e scrivere i dati) o al di fuori di esso. Se si trovava all'interno del cluster e di recente è stato spostato al di fuori, Ceph eseguirà la migrazione dei gruppi di posizionamento in altri OSD. Se un OSD si trova fuori dal cluster, CRUSH non gli assegnerà alcun gruppo di posizionamento. Se un OSD è nello stato "down", deve essere anche "out".
   </para>
   <note>
    <title>stato non integro</title>
    <para>
     Se un OSD è "down" e "in", è presente un problema e il cluster si trova nello stato non integro.
    </para>
   </note>
   <para>
    Se si esegue un comando come <command>ceph health</command> <command>ceph -s</command> o <command>ceph -w</command>, è possibile notare che il cluster non rimanda sempre lo stato <literal>HEALTH OK</literal>. Per quanto riguarda gli OSD, è prevedibile che il cluster <emphasis>non</emphasis> rimanderà lo stato <literal>HEALTH OK</literal> se:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Il cluster non è stato ancora avviato (non risponderà).
     </para>
    </listitem>
    <listitem>
     <para>
      Il cluster è stato appena avviato o riavviato e non è ancora pronto, poiché è in corso la creazione dei gruppi di posizionamento e gli OSD sono in fase di peering.
     </para>
    </listitem>
    <listitem>
     <para>
      Un OSD è stato appena aggiunto o rimosso.
     </para>
    </listitem>
    <listitem>
     <para>
      La mappa del cluster è stata appena modificata.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Un aspetto importante del monitoraggio degli OSD consiste nell'assicurarsi che tutti gli OSD dentro il cluster siano attivi e in esecuzione quando lo è il cluster. Per verificare se tutti gli OSD sono in esecuzione, eseguire:
   </para>
<screen>
<prompt>root # </prompt>ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    Il risultato dovrebbe indicare il numero totale di OSD (x), quanti di questi sono nello stato "up" (y) e nello stato "in" (z) e l'epoca della mappa (eNNNN). Se il numero di OSD dentro il cluster ("in") superiore al numero di OSD attivi ("up"), eseguire il comando seguente per individuare i daemon <literal>ceph-osd</literal> non in esecuzione:
   </para>
<screen>
<prompt>root # </prompt>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    Se ad esempio un OSD con ID 1 è disattivato, avviarlo:
   </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>sudo systemctl start ceph-osd@1.service
</screen>
   <para>
    Per informazioni sui problemi associati agli OSD interrotti o che non verranno riavviati, vedere la <xref linkend="op-osd-not-running"/>.
   </para>
  </sect2>

  <sect2 xml:id="op-pgsets">
   <title>Set di gruppi di posizionamento</title>
   <para>
    Quando CRUSH assegna i gruppi di posizionamento agli OSD, osserva il numero di repliche del pool e assegna il gruppo di posizionamento agli OSD in modo che ogni replica di tale gruppo venga assegnata a un OSD diverso. Ad esempio, se il pool richiede tre repliche di un gruppo di posizionamento, CRUSH potrebbe assegnarle rispettivamente a <literal>osd.1</literal>, <literal>osd.2</literal> e <literal>osd.3</literal>. CRUSH in realtà individua un posizionamento pseudo-casuale che tiene conto dei domini di errore impostati nella mappa CRUSH. Di conseguenza, in un cluster di grandi dimensioni sarà raro vedere gruppi di posizionamento assegnati agli OSD più vicini. Il set di OSD che deve contenere le repliche di un determinato gruppo di posizionamento viene chiamato <emphasis>acting set</emphasis>. In alcuni casi, un OSD dell'acting set è disattivato oppure non è in grado di provvedere alle richieste relative agli oggetti nel gruppo di posizionamento. In queste situazioni, gli scenari potrebbero essere i seguenti:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Un OSD è stato aggiunto o rimosso. Quindi, CRUSH ha riassegnato il gruppo di posizionamento agli altri OSD e ha pertanto modificato la composizione dell'<emphasis>acting set</emphasis>, causando la migrazione dei dati con una procedura di "backfill".
     </para>
    </listitem>
    <listitem>
     <para>
      Un OSD era nello stato "down", è stato riavviato ed è in corso di recupero.
     </para>
    </listitem>
    <listitem>
     <para>
      Un OSD nell'<emphasis>acting set</emphasis> si trova nello stato "down" o non è in grado di provvedere alle richieste e un altro OSD ne assume provvisoriamente i compiti.
     </para>
     <para>
      Ceph elabora una richiesta client tramite l'<emphasis>up set</emphasis>, ovvero il set degli OSD che provvederanno effettivamente alle richieste. Nella maggior parte dei casi, l'<emphasis>up set</emphasis> e l'<emphasis>acting set</emphasis> sono praticamente identici. Se differiscono, potrebbe significare che Ceph sta eseguendo la migrazione dei dati, che è in corso il recupero di un OSD o che si è verificato un problema (ad esempio, in tali scenari generalmente Ceph rimanda uno stato <literal>HEALTH WARN</literal> con il messaggio "stuck stale").
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Per recuperare un elenco dei gruppi di posizionamento, eseguire:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>;ceph pg dump
</screen>
   <para>
    Per visualizzare gli OSD che si trovano all'interno dell'<emphasis>acting set</emphasis> o dell'<emphasis>up set</emphasis> per un determinato gruppo di posizionamento, eseguire:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg map<replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -&gt; up [0,1,2] acting [0,1,2]
</screen>
   <para>
    Il risultato dovrebbe indicare l'epoca di osdmap (eNNN), il numero di gruppi di posizionamento (<replaceable>PG_NUM</replaceable>), gli OSD nell'<emphasis>up set</emphasis> ("up") e quelli nell'<emphasis>acting set</emphasis> ("acting"):
   </para>
   <tip>
    <title>indicatore di problemi nel cluster</title>
    <para>
     Una mancata corrispondenza tra l'<emphasis>up set</emphasis> e l'<emphasis>acting set</emphasis> potrebbe indicare che è in corso il ribilanciamento del cluster o un suo potenziale problema.
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-peering">
   <title>Peering</title>
   <para>
    Prima che sia possibile scrivere dati su un gruppo di posizionamento, quest'ultimo deve essere negli stati "active" e "clean". Affinché Ceph possa determinare lo stato corrente di un gruppo di posizionamento, l'OSD primario di tale gruppo (il primo OSD nell'<emphasis>acting set</emphasis>) esegue il peering con gli OSD secondario e terziario per accordarsi sullo stato corrente del gruppo di posizionamento (presupponendo che si tratti di un pool con tre repliche del gruppo di posizionamento).
   </para>
   <figure>
    <title>Schema di peering</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op-mon-pg-states">
   <title>Monitoraggio degli stati del gruppo di posizionamento</title>
   <para>
    Se si esegue un comando come <command>ceph health</command>, <command>ceph -s</command> o <command>ceph -w</command>, è possibile notare che il cluster non rimanda sempre il messaggio <literal>HEALTH OK</literal>. Dopo aver verificato che tutti gli OSD siano in esecuzione, controllare anche gli stati del gruppo di posizionamento.
   </para>
   <para>
    È prevedibile che il cluster <emphasis role="bold">non</emphasis> rimanderà lo stato <literal>HEALTH OK</literal> in alcune circostanze correlate al peering del gruppo di posizionamento:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      È stato appena creato un pool e non è stato ancora eseguito il peering dei gruppi di posizionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      È in corso il recupero dei gruppi di posizionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      Un OSD è stato appena aggiunto al cluster o rimosso da quest'ultimo.
     </para>
    </listitem>
    <listitem>
     <para>
      La mappa CRUSH è stata appena modificata ed è in corso la migrazione dei gruppi di posizionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      Sono presenti dati incoerenti in repliche diverse di un gruppo di posizionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph sta eseguendo la pulitura delle repliche di un gruppo di posizionamento.
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph non dispone di sufficiente capacità di storage per il completamento delle operazioni di backfill.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Se a causa di una di queste circostanze Ceph rimanda il messaggio <literal>HEALTH WARN</literal>, non preoccuparsi. In molti casi, il cluster si ripristinerà automaticamente. In alcuni casi, può essere necessario intervenire. Un aspetto importante del monitoraggio dei gruppi di posizionamento consiste nell'assicurarsi che quando un cluster è attivo e in esecuzione, tutti i gruppi di posizionamento siano nello stato "active" e preferibilmente anche in quello "clean state". Per visualizzare lo stato di tutti i gruppi di posizionamento, eseguire:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    Il risultato dovrebbe indicare il numero totale di gruppi di posizionamento (x), quanti di questi si trovano in un determinato stato, ad esempio "active+clean" (y), e la quantità di dati memorizzati (z).
   </para>
   <para>
    Oltre agli stati del gruppo di posizionamento, Ceph rimanderà anche la capacità di storage utilizzata (aa), quella rimanente (bb) e la capacità di storage totale del gruppo di posizionamento. Tali valori possono essere importanti nei casi in cui:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Ci si sta avvicinando al valore impostato per <option>near full ratio</option> o <option>full ratio</option>.
     </para>
    </listitem>
    <listitem>
     <para>
      I dati non vengono distribuiti nel cluster a causa di un errore nella configurazione CRUSH.
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>ID dei gruppi di posizionamento</title>
    <para>
     Gli ID dei gruppi di posizionamento sono costituiti dal numero del pool (e non dal nome) seguito da un punto (.) e dall'ID del gruppo di posizionamento (un numero esadecimale). È possibile visualizzare il numero e il nome del pool dall'output di <command>ceph osd lspools</command>. Ad esempio, il pool di default <literal>rbd</literal> corrisponde al numero di pool 0. Un ID del gruppo di posizionamento completo è strutturato nel modo seguente:
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     E in genere ha il seguente aspetto:
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    Per recuperare un elenco dei gruppi di posizionamento, eseguire quanto riportato di seguito:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    È inoltre possibile formattare l'output in formato JSON e salvarlo in un file:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    Per interrogare un determinato gruppo di posizionamento, eseguire quanto riportato di seguito:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    L'elenco seguente descrive in modo dettagliato gli stati dei gruppi di posizionamento più comuni.
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING</term>
     <listitem>
      <para>
       Quando si crea un pool, quest'ultimo creerà il numero di gruppi di posizionamento specificato. Ceph rimanderà lo stato "creating" durante la creazione di uno o più gruppi di posizionamento. In seguito alla creazione, verrà avviato il peering degli OSD che fanno parte dell'<emphasis>acting set</emphasis> del gruppo di posizionamento. Al completamento del peering, lo stato del gruppo di posizionamento deve essere "active+clean", per indicare che un client Ceph può avviare la scrittura su tale gruppo.
      </para>
      <figure>
       <title>Stato dei gruppi di posizionamento</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING</term>
     <listitem>
      <para>
       Quando Ceph esegue il peering di un gruppo di posizionamento fa in modo che gli OSD che ne memorizzano le repliche concordino sullo stato degli oggetti e dei metadati contenuti nel gruppo. Quando Ceph completa il peering, vuol dire che gli OSD che memorizzano il gruppo di posizionamento concordano sullo stato corrente del gruppo di posizionamento. Tuttavia, il completamento del processo di peering <emphasis role="bold">non</emphasis> implica che in ogni replica siano presenti i contenuti più recenti.
      </para>
      <note>
       <title>cronologia autorevole</title>
       <para>
        Ceph <emphasis role="bold">non</emphasis> riconoscerà le operazioni di scrittura su un client finché tutti gli OSD dell'<emphasis>acting set</emphasis> non le salvano in modo permanente. Questa pratica consente di assicurare che almeno un membro dell'<emphasis>acting set</emphasis> conterrà un record di tutte le operazioni di scrittura riconosciute dall'ultima operazione di peering riuscita.
       </para>
       <para>
        Tramite un record accurato delle operazioni di scrittura riconosciute, Ceph è in grado di creare e ampliare una nuova cronologia autorevole del gruppo di posizionamento, un set di operazioni completo e in corretta sequenza che, se eseguito, consente di aggiornare la copia dell'OSD di un gruppo di posizionamento.
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ACTIVE</term>
     <listitem>
      <para>
       Quando Ceph completa il processo di peering, il gruppo di posizionamento può passare allo stato "active". Tale stato indica che i dati nel gruppo di posizionamento sono di norma disponibili all'interno del gruppo di posizionamento primario e nelle repliche delle operazioni di lettura e scrittura.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN</term>
     <listitem>
      <para>
       Quando un gruppo di posizionamento si trova nello stato "clean", l'OSD primario e gli OSD di replica hanno eseguito correttamente il peering e non sono presenti repliche residue relative al gruppo di posizionamento. Ceph ha eseguito la replica di tutti gli oggetti nel gruppo di posizionamento per il numero corretto di volte.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED</term>
     <listitem>
      <para>
       Quando un client scrive un oggetto nell'OSD primario, questo è responsabile della scrittura delle repliche sugli OSD di replica. Quando l'OSD primario scrive l'oggetto nello storage, il gruppo di posizionamento rimane nello stato danneggiato ("degraded") finché gli OSD di replica non confermano all'OSD primario la riuscita della creazione degli oggetti di replica da parte di Ceph.
      </para>
      <para>
       Il motivo per cui un gruppo di posizionamento può avere lo stato "active+degraded" è il fatto che un OSD può essere nello stato "active" anche se non contiene ancora tutti gli oggetti. Se un OSD diventa disattivato, Ceph contrassegna ciascun gruppo di posizionamento assegnato a tale OSD come "degraded". Quando l'OSD ritorna attivo, gli altri OSD devono ripetere il peering. Tuttavia, un client può comunque scrivere un nuovo oggetto in un gruppo di posizionamento danneggiato se si trova nello stato "active".
      </para>
      <para>
       Se un OSD è nello stato "down" e la condizione "degraded" persiste, Ceph può contrassegnare l'OSD disattivato come esterno al cluster ("out") e rimappare i dati da questo OSD a un altro OSD. L'intervallo di tempo in cui un OSD viene contrassegnato dallo stato "down" a quello "out" è controllato dall'opzione <option>mon osd down out interval</option>, impostata per default su 600 secondi.
      </para>
      <para>
       Inoltre, un gruppo di posizionamento può trovarsi nello stato "degraded" se Ceph non riesce a trovare uno o più oggetti al suo interno. Sebbene non sia possibile eseguire operazioni di lettura o scrittura sugli oggetti non trovati, sarà comunque possibile accedere a tutti gli altri oggetti nel gruppo di posizionamento nello stato "degraded".
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING</term>
     <listitem>
      <para>
       Ceph è stato progettato per garantire una tolleranza agli errori su vasta scala in caso di problemi hardware e software. Quando un OSD diventa disattivato, i relativi contenuti potrebbero non essere aggiornati rispetto allo stato corrente delle altre repliche nei gruppi di posizionamento. Quando l'OSD ritorna attivo, i contenuti dei gruppi di posizionamento devono essere aggiornati per riflettere lo stato corrente. Durante questo intervallo di tempo, l'OSD potrebbe riportare lo stato "recovering".
      </para>
      <para>
       La procedura di recupero non è sempre semplice, perché errori dell'hardware potrebbero causare errori a cascata in più OSD. Ad esempio, potrebbero verificarsi errori su uno switch di rete di un rack o di uno schedario a causa dei quali l'aggiornamento degli OSD di più computer host rispetto allo stato corrente del cluster non va a buon fine. Quando l'errore viene risolto, è necessario recuperare tutti gli OSD.
      </para>
      <para>
       Ceph fornisce diverse impostazioni per bilanciare il conflitto delle risorse tra le nuove richieste di servizio e la necessità di recuperare gli oggetti dati e ripristinare i gruppi di posizionamento allo stato corrente. L'impostazione <option>osd recovery delay start</option> consente a un OSD di riavviarsi, ripetere il peering e persino elaborare alcune richieste di riproduzione prima di avviare il processo di recupero. L'impostazione <option>osd recovery thread timeout</option> consente di impostare un timeout del thread, poiché più OSD potrebbero generare errori, riavviarsi e ripetere il peering a frequenza sfalsata. L'impostazione <option>osd recovery max active</option> consente di limitare il numero di richieste di recupero che verranno elaborate in contemporanea da un OSD per impedire errori di gestione delle richieste su tale OSD. L'impostazione <option>osd recovery max chunk</option> consente di limitare le dimensioni delle porzioni di dati recuperate per evitare un traffico eccessivo sulla rete.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING</term>
     <listitem>
      <para>
       Se viene aggiunto un nuovo OSD al cluster, CRUSH riassegnerà i gruppi di posizionamento dagli OSD già nel cluster a quello appena aggiunto. Se si forza il nuovo OSD ad accettare immediatamente i gruppi di posizionamento riassegnati, si può generare un carico eccessivo su quest'ultimo. Tramite il backfill dell'OSD con i gruppi di posizionamento, è possibile avviare questa procedura in background. Al completamento del backfill, il nuovo OSD inizierà a provvedere alle richieste quando sarà pronto.
      </para>
      <para>
       Durante le operazioni di backfill, potrebbero essere visualizzati diversi stati: "backfill_wait" indica che un'operazione di backfill è in sospeso, ma non ancora in corso; "backfill" indica che un'operazione di backfill è in corso; "backfill_too_full" indica che un'operazione di backfill è stata richiesta, ma non è stato possibile completarla perché la capacità di storage è insufficiente. Se non è possibile eseguire il backfill di un gruppo di posizionamento, questo potrebbe essere considerato come incompleto ("incomplete").
      </para>
      <para>
       In Ceph sono disponibili diverse impostazioni per la gestione del carico associato alla riassegnazione dei gruppi di posizionamento a un OSD (soprattutto ai nuovi OSD). Per default, <option>osd max backfills</option> consente di impostare su 10 il numero massimo di operazioni di backfill simultanee verso o da un OSD. Con <option>backfill full ratio</option> un OSD può rifiutare una richiesta di backfill se sta raggiungendo la percentuale di riempimento (90% per default) e apportare modifiche con il comando <command>ceph osd set-backfillfull-ratio</command>. Se un OSD rifiuta una richiesta di backfill, tramite <option>osd backfill retry interval</option> l'OSD può ripetere la richiesta (per default dopo 10 secondi). Gli OSD possono inoltre impostare i valori <option>osd backfill scan min</option> e <option>osd backfill scan max</option> per gestire gli intervalli di scansione (impostati su 64 e 512 per default).
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED</term>
     <listitem>
      <para>
       Se l'<emphasis>acting set</emphasis> che gestisce un gruppo di posizionamento cambia, viene eseguita la migrazione dei dati dall'<emphasis>acting set</emphasis> precedente all'<emphasis>acting set</emphasis> nuovo. La gestione delle richieste da parte del nuovo OSD primario può richiedere del tempo. Il processo potrebbe quindi chiedere al precedente OSD primario di continuare a provvedere alle richieste fino al completamento della migrazione del gruppo di posizionamento. Al termine della migrazione dei dati, la mappatura utilizza l'OSD primario del nuovo <emphasis>acting set</emphasis>.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STALE</term>
     <listitem>
      <para>
       Mentre Ceph utilizza gli heartbeat per verificare che gli host e i daemon siano in esecuzione, i daemon <literal>ceph-osd</literal> possono passare anche allo stato "stuck" e non segnalare statistiche in modo puntuale (ad esempio, in caso di un errore di rete temporaneo). Per default, i daemon OSD segnalano il relativo gruppo di posizionamento e le statistiche di avvio ed errori ogni mezzo secondo (0,5), ovvero con maggiore frequenza rispetto alle soglie di heartbeat. Se l'OSD primario dell'<emphasis>acting set</emphasis> di un gruppo di posizionamento non riesce a inviare segnalazioni al monitor o se gli altri OSD hanno segnalato l'OSD primario come disattivato, i monitor contrassegneranno il gruppo di posizionamento come inattivo ("stale").
      </para>
      <para>
       All'avvio del cluster, è normale visualizzare lo stato "stale" fino al completamento del processo di peering. Se il cluster è in esecuzione da un po' di tempo ma i gruppi di posizionamento continuano a essere nello stato "stale", vuol dire che l'OSD primario di tali gruppi di posizionamento è disattivato o non segnala al cluster monitoraggio le statistiche relative al gruppo.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-pg-stuck-states">
   <title>Identificazione dei gruppi di posizionamento con problemi</title>
   <para>
    Come notato in precedenza, un gruppo di posizionamento che non si trova nello stato "active+clean" non è necessariamente problematico. Di norma, la funzionalità di riparazione automatica di Ceph potrebbe non funzionare quando i gruppi di posizionamento si bloccano. Gli stati bloccati includono:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Unclean</emphasis>: i gruppi di posizionamento contengono oggetti che non sono stati replicati per il numero di volte richiesto. Devono essere in fase di recupero.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Inactive</emphasis>: i gruppi di posizionamento non sono in grado di elaborare le operazioni di lettura o di scrittura perché sono in attesa della riattivazione di un OSD con i dati più recenti.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Stale</emphasis>: i gruppi di posizionamento si trovano in uno stato sconosciuto perché gli OSD che li ospitano non informano il cluster di monitoraggio da un determinato intervallo di tempo (configurato dall'opzione <option>mon_osd_report_timeout</option>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Per identificare i gruppi di posizionamento bloccati, eseguire quanto riportato di seguito:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
</screen>
  </sect2>

  <sect2 xml:id="op-pg-objectfinding">
   <title>Individuazione dell'ubicazione di un oggetto</title>
   <para>
    Per memorizzare i dati oggetto nel Ceph Object Store, il client Ceph deve impostare un nome oggetto e specificare un pool correlato. Il client Ceph recupera la mappa del cluster più recente, mentre l'algoritmo CRUSH calcola in che modo mappare l'oggetto a un gruppo di posizionamento e in che modo assegnare tale gruppo in modo dinamico a un OSD. Per individuare l'ubicazione dell'oggetto, servono solo il nome dell'oggetto e quello del pool. Ad esempio:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>Individuazione di un oggetto</title>
    <para>
     Procedere con la creazione di un oggetto di esempio. Specificare il nome oggetto "test-object-1", il percorso di un file di esempio "testfile.txt" contenente alcuni dati oggetto e il nome pool "data" tramite il comando <command>rados put</command> nella riga di comando:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     Per verificare che Ceph Object Store abbia memorizzato l'oggetto, eseguire quanto riportato di seguito:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p data ls
</screen>
    <para>
     Adesso, individuare l'ubicazione dell'oggetto. L'output restituito da Ceph sarà l'ubicazione dell'oggetto:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     Per rimuovere l'oggetto di esempio, basta eliminarlo tramite il comando <command>rados rm</command>:
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
 <sect1 xml:id="op-osd-not-running">
  <title>OSD non in esecuzione</title>

  <para>
   In circostanze normali, è sufficiente riavviare il daemon <literal>ceph-osd</literal> per fare in modo che l'OSD si riunisca al cluster e venga recuperato.
  </para>

  <sect2 xml:id="op-osd-not-start">
   <title>Errore di avvio di OSD</title>
   <para>
    Se un OSD non si avvia dopo aver avviato il cluster, controllare quanto segue:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">File di configurazione</emphasis>: se non è stato possibile eseguire gli OSD da una nuova installazione, controllare il file di configurazione per assicurarsi che sia conforme (ad esempio, deve essere presente <literal>host</literal> e non <literal>hostname</literal>).
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Controllare i percorsi</emphasis>: controllare i percorsi nella configurazione e quelli effettivi relativi a dati e journal. Se i dati dell'OSD vengono separati da quelli del journal e sono presenti errori nel file di configurazione o nei montaggi effettivi, potrebbero verificarsi dei problemi con l'avvio degli OSD. Se si desidera memorizzare il journal su un dispositivo di blocco, è necessario suddividere il disco del journal in partizioni e assegnarne una a ciascun OSD.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Controllare il numero max di thread</emphasis>: se il nodo contiene più OSD, è facile raggiungere il numero massimo di thread di default (in genere 32.000), soprattutto durante il processo di recupero. È possibile aumentare il numero di thread con il comando <command>sysctl</command> per verificare se impostandolo sul limite massimo consentito (ad esempio, 4194303) si riesce a risolvere il problema:
     </para>
<screen>
<prompt>root # </prompt>sysctl -w kernel.pid_max=4194303
</screen>
     <para>
      In caso affermativo, è possibile rendere questa operazione permanente includendo l'impostazione <option>kernel.pid_max</option> nel file <filename>/etc/sysctl.conf</filename>:
     </para>
<screen>
kernel.pid_max = 4194303
</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="op-osd-failed">
   <title>Errori in un OSD</title>
   <para>
    Se il processo <literal>ceph-osd</literal> viene interrotto, il monitor raccoglierà informazioni sull'errore dai daemon<literal>ceph-osd</literal> esistenti e lo segnalerà tramite il comando <command>ceph health</command>:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 1/3 in osds are down
</screen>
   <para>
    Nello specifico, si riceverà un avviso ogni volta che sono presenti processi <literal>ceph-osd</literal> contrassegnati come "in" e "down". È possibile individuare i <literal>ceph-osds</literal> disattivati con:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
</screen>
   <para>
    Se si verifica un errore sul disco o altri errori che impediscono il funzionamento o il riavvio di <literal>ceph-osd</literal>, viene visualizzato un messaggio di errore nel relativo file di log in <filename>/var/log/ceph</filename>.
   </para>
   <para>
    Se il daemon è stato interrotto a causa di un errore di heartbeat, è possibile che il file system del kernel sottostante non risponda. Esaminare l'output del comando <command>dmesg</command> per verificare la presenza di errori del disco o di altri errori del kernel.
   </para>
  </sect2>

  <sect2 xml:id="op-no-disk-space">
   <title>Spazio libero su disco non disponibile</title>
   <para>
    Per evitare perdite di dati, Ceph non consente le operazioni di scrittura su un OSD pieno. In un cluster operativo, quando questo è prossimo al rispettivo rapporto di riempimento si riceve un avviso. L'opzione <option>mon osd full ratio</option> passa al valore di default di 0,95, ovvero il 95% della capacità, prima che le operazioni di scrittura dei dati da parte dei client vengano interrotte. L'opzione <option>mon osd backfillfull ratio</option> passa al valore di default di 0,90, ovvero il 90% della capacità, quando l'avvio dei processi di backfill viene bloccato. L'opzione OSD nearfull ratio passa al valore di default di 0,85, ovvero l'85% della capacità, quando viene generato un avviso sullo stato di integrità. È possibile modificare il valore di "nearfull" tramite il comando seguente:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    In genere, durante il test della gestione degli errori dell'OSD in un cluster di piccole dimensioni da parte di Ceph, si verificano problemi di riempimento completo del cluster. Quando un nodo contiene un'elevata percentuale di dati del cluster, il cluster può superare facilmente e con rapidità i valori impostati per i rapporti "nearfull" e "full". Se si sta testando la reazione di Ceph agli errori dell'OSD in un cluster di piccole dimensioni, si consiglia di lasciare sufficiente spazio libero su disco e di ridurre temporaneamente i valori impostati per i rapporti "full", "backfillfull" e "nearfull" dell'OSD con questi comandi:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>0.0 to 1.0</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    <command>ceph health</command> segnala i Ceph OSD pieni:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 1 nearfull osd(s)
</screen>
   <para>
    oppure
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
   <para>
    Il modo migliore per gestire un cluster pieno consiste nell'aggiungere nuovi Ceph OSD, che consentono al cluster di ridistribuire dati nello spazio di storage che si è appena reso disponibile.
   </para>
   <para>
    Se non è possibile avviare un OSD perché è pieno, è possibile eliminare alcuni dati eliminando alcune directory dei gruppi di posizionamento nell'OSD pieno.
   </para>
   <important>
    <title>eliminazione della directory di un gruppo di posizionamento</title>
    <para>
     Se si sceglie di eliminare la directory di un gruppo di posizionamento in un OSD pieno, <emphasis role="bold">non</emphasis> eliminare la stessa directory del gruppo di posizionamento in un altro OSD pieno per evitare di <emphasis role="bold">perdere i dati</emphasis>. È <emphasis role="bold">necessario</emphasis> mantenere almeno una copia dei dati in almeno un OSD.
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
