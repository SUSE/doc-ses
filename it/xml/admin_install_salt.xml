<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>Installazione con DeepSea/Salt</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>sì</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Salt insieme con DeepSea è uno <emphasis>stack</emphasis> di componenti che consentono di installare e gestire l'infrastruttura del server, risultando scalabile, veloce e relativamente semplice da eseguire. Prima di avviare l'installazione del cluster con Salt, leggere le seguenti considerazioni:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    I <emphasis>Salt minion</emphasis> sono i nodi controllati da un nodo dedicato denominato Salt master. I Salt minion hanno ruoli, ad esempio Ceph OSD, Ceph Monitor, Ceph Manager, Object Gateway, iSCSI Gateway o NFS Ganesha.
   </para>
  </listitem>
  <listitem>
   <para>
    Un Salt master esegue il proprio Salt minion, richiesto per eseguire task privilegiati, ad esempio creazione, autorizzazione e copia di chiavi sui minion, in modo che i minion remoti non debbano mai eseguire task privilegiati.
   </para>
   <tip>
    <title>condivisione di più ruoli per server</title>
    <para>
     È possibile ottenere le prestazioni migliori dal cluster Ceph quando ogni ruolo viene distribuito su un nodo separato. Le installazioni reali, tuttavia, a volte richiedono la condivisione di un nodo per più ruoli. Per evitare problemi di prestazioni e con la procedura di upgrade, non distribuire il ruolo Ceph OSD, del server di metadati o Ceph Monitor sul nodo admin.
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    I Salt minion devono risolvere correttamente il nome host del Salt master in rete. Per impostazione predefinita, viene cercato il nome host <systemitem>salt</systemitem>, ma è possibile specificare altri nomi host individuabili in rete nel file <filename>/etc/salt/minion</filename>, vedere <xref linkend="ceph-install-stack"/>.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>Lettura delle note di rilascio</title>

  <para>
   Nelle note di rilascio è possibile trovare informazioni aggiuntive sulle modifiche apportate rispetto alla release precedente di SUSE Enterprise Storage. Controllare le note di rilascio per vedere se:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     l'hardware necessita di considerazioni speciali;
    </para>
   </listitem>
   <listitem>
    <para>
     i pacchetti software utilizzati hanno subito modifiche significative;
    </para>
   </listitem>
   <listitem>
    <para>
     è necessario adottare precauzioni speciali per l'installazione.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Le note di rilascio forniscono inoltre informazioni che non si è fatto in tempo a riportare nel manuale. Contengono anche alcune note su problemi noti.
  </para>

  <para>
   Dopo aver installato il pacchetto <package>release-notes-ses</package>, individuare localmente le note di rilascio nella directory <filename>/usr/share/doc/release-notes</filename> o online all'indirizzo <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>Introduzione a DeepSea</title>

  <para>
   DeepSea consente di far risparmiare tempo all'amministratore ed eseguire in sicurezza operazioni complesse su un cluster Ceph.
  </para>

  <para>
   Ceph è una soluzione software altamente configurabile che aumenta la libertà e la responsabilità degli amministratori di sistema.
  </para>

  <para>
   La configurazione minima di Ceph è ottima per scopi dimostrativi, ma non mostra le funzionalità interessanti di Ceph visibili con un alto numero di nodi.
  </para>

  <para>
   DeepSea raccoglie e memorizza i dati sui singoli server, ad esempio indirizzi e nomi dei dispositivi. Per un sistema di storage distribuito come Ceph, possono esistere centinaia di tali voci da raccogliere e memorizzare. La raccolta delle informazioni e l'immissione manuale dei dati in uno strumento di gestione della configurazione è un'operazione complessa in cui è facile fare degli errori.
  </para>

  <para>
   I passaggi necessari per preparare i server, raccogliere la configurazione, configurare e distribuire Ceph sono quasi uguali. Tuttavia, ciò non riguarda la gestione di funzioni separate. Per le operazioni giornaliere, è richiesta la capacità di aggiungere hardware a una data funzione e rimuoverlo senza problemi.
  </para>

  <para>
   DeepSea gestisce queste osservazioni con la seguente strategia: DeepSea consolida le decisioni dell'amministratore in un singolo file. Le decisioni comprendono assegnazione del cluster, assegnazione del ruolo e assegnazione del profilo, mentre DeepSea raccoglie ciascun insieme di task in un semplice obiettivo. Ogni obiettivo è una <emphasis>fase</emphasis>:
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>Descrizione delle fasi di DeepSea</title>
   <listitem>
    <para>
     <emphasis role="bold">Fase 0</emphasis>, la <emphasis role="bold">preparazione</emphasis>, durante questa fase, vengono applicati tutti gli aggiornamenti richiesti e il sistema potrebbe riavviarsi.
    </para>
    <important>
     <title>nuova esecuzione della fase 0 dopo il riavvio del nodo admin</title>
     <para>
      Se, durante la fase 0, il nodo admin si riavvia per caricare la nuova versione del kernel, occorre eseguire di nuovo la fase 0, in caso contrario i minion non vengono indirizzati.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Fase 1</emphasis>, la <emphasis role="bold">rilevazione</emphasis>, viene rilevato tutto l'hardware nel cluster e vengono raccolte le informazioni necessarie per la configurazione Ceph. Per informazioni sulla configurazione, consultare <xref linkend="deepsea-pillar-salt-configuration"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Fase 2</emphasis>, la <emphasis role="bold">configurazione</emphasis>, è necessario preparare i dati di configurazione in un formato particolare.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Fase 3</emphasis>, l'<emphasis role="bold">installazione</emphasis>, crea un cluster Ceph di base con i servizi Ceph obbligatori. Per l'elenco, vedere <xref linkend="storage-intro-core-nodes"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Fase 4</emphasis>, i <emphasis role="bold">servizi</emphasis>, in questa fase è possibile installare funzionalità aggiuntive di Ceph come iSCSI, Object Gateway e CephFS. Sono tutte opzionali.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Fase 5</emphasis>, la fase di rimozione. Questa fase non è obbligatoria e durante la configurazione iniziale non è in genere necessaria. In questa fase vengono rimossi i ruoli dei minion e anche la configurazione del cluster. È necessario eseguire questa fase quando occorre rimuovere un nodo di storage dal cluster. Per ulteriori informazioni, vedere la <xref linkend="salt-node-removing"/>.
    </para>
   </listitem>
  </itemizedlist>



  <sect2 xml:id="deepsea-organisation-locations">
   <title>Organizzazione e ubicazioni importanti</title>
   <para>
    Salt dispone di diverse ubicazioni standard e diverse convenzioni di assegnazione del nome utilizzate sul nodo master:
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename></term>
     <listitem>
      <para>
       La directory memorizza i dati di configurazione per i minion del cluster. <emphasis>Pillar</emphasis> è un'interfaccia che fornisce valori di configurazione globali a tutti i minion del cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename></term>
     <listitem>
      <para>
       La directory memorizza i file di stato di Salt (denominati anche file <emphasis>sls</emphasis>). I file di stato sono descrizioni formattate di stati in cui deve trovarsi il cluster.

      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename></term>
     <listitem>
      <para>
       La directory memorizza script Python noti come runner. I runner vengono eseguiti sul nodo master.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename></term>
     <listitem>
      <para>
       La directory memorizza gli script Python denominati moduli. I moduli vengono applicati a tutti i minion nel cluster.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename></term>
     <listitem>
      <para>
       La directory è utilizzata da DeepSea. I dati della configurazione raccolti vengono memorizzati qui.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename></term>
     <listitem>
      <para>
       Una directory utilizzata da DeepSea. Memorizza i file sls che possono essere in formati diversi, ma ogni sottodirectory contiene file sls. Ciascuna sottodirectory contiene solo un tipo di file sls. Ad esempio, <filename>/srv/salt/ceph/stage</filename> contiene i file di orchestrazione eseguiti da <command>salt-run state.orchestrate</command>.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>Indirizzamento dei minion</title>
   <para>
    I comandi DeepSea vengono eseguiti tramite l'infrastruttura Salt. Quando si utilizza il comando <command>salt</command>, occorre specificare un insieme di Salt minion interessati dal comando. L'insieme dei minion viene descritto come <emphasis>destinazione</emphasis> per il comando <command>salt</command>. Le sezioni seguenti descrivono i possibili metodi di individuare i minion.
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>Corrispondenza del nome del minion</title>
    <para>
     È possibile individuare un minion o un gruppo di minion tramite corrispondenza dei nomi. Il nome di un minion è in genere il nome host breve del nodo in cui viene eseguito il minion. Questo è in genere un metodo di indirizzamento Salt, non correlato a DeepSea. Per limitare il campo dei nomi di minion, è possibile utilizzare caratteri jolly, espressioni regolari o elenchi. Segue la sintassi generica:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>cluster solo Ceph</title>
     <para>
      Se tutti i Salt minion nell'ambiente appartengono al cluster Ceph, è possibile sostituire in sicurezza <replaceable>target</replaceable> con <literal>"*"</literal> per includere <emphasis>tutti</emphasis> i minion registrati.
     </para>
    </tip>
    <para>
     Far corrispondere tutti i minion nel dominio example.net (supponendo che i nomi dei minion siano identici ai loro nomi host "completi"):
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     Far corrispondere i minion da "web1" a "web5":
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     Far corrispondere i minion "web1-prod" e "web1-devel" utilizzando un'espressione regolare:
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     Far corrispondere un semplice elenco di minion:
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     Far corrispondere tutti i minion nel cluster:
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>Indirizzamento con un grain DeepSea</title>
    <para>
     In un ambiente eterogeneo gestito da Salt in cui SUSE Enterprise Storage 6 è distribuito su un sottoinsieme di nodi insieme ad altre soluzioni cluster, è necessario contrassegnare i minion pertinenti applicandovi un grain "deepsea" prima di eseguire la fase 0 di DeepSea. In questo modo è possibile indirizzare con facilità i minion DeepSea negli ambienti dove è problematica la corrispondenza del nome.
    </para>
    <para>
     Per applicare il grain "deepsea" a un gruppo di minion, eseguire:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     Per rimuovere il grain "deepsea" da un gruppo di minion, eseguire:
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     Dopo aver applicato il grain "deepsea" ai minion pertinenti, è possibile individuarli come segue:
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     Il comando seguente è un equivalente:
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title>Impostare l'opzione <option>deepsea_minions</option></title>
    <para>
     L'impostazione della destinazione dell'opzione <option>deepsea_minions</option> è un requisito per le distribuzioni di DeepSea. DeepSea la utilizza per istruire i minion durante l'esecuzione delle fasi (per i dettagli, fare riferimento a <xref linkend="deepsea-stage-description"/>).
    </para>
    <para>
     Per impostare o modificare l'opzione <option>deepsea_minions</option>, modificare il file <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> sul Salt master e aggiungere o sostituire la riga seguente:
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title>destinazione <option>deepsea_minions</option></title>
     <para>
      Come <replaceable>target</replaceable> per l'opzione <option>deepsea_minions</option>, è possibile utilizzare uno dei metodi di indirizzamento: <xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/> e <xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>.
     </para>
     <para>
      Far corrispondere tutti i minion Salt nel cluster:
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      Far corrispondere tutti i minion con il grain "deepsea":
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>Ulteriori informazioni</title>
    <para>
     È possibile utilizzare metodi più avanzati per l'indirizzamento dei minion mediante l'infrastruttura Salt. La pagina della documentazione "deepsea-minions" fornisce ulteriori dettagli sull'indirizzamento di DeepSea (<command>man 7 deepsea_minions</command>).
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>Distribuzione del cluster</title>

  <para>
   Il processo di distribuzione del cluster comprende fasi diverse. Primo, occorre preparare tutti i nodi del cluster configurando Salt, quindi distribuire e configurare Ceph.
  </para>

  <tip xml:id="dev-env">
   <title>distribuzione dei nodi monitor senza definire profili OSD</title>
   <para>
    Se occorre ignorare la definizione dei ruoli di storage per OSD come descritto nella <xref linkend="policy-role-assignment"/> e distribuire prima i nodi Ceph Monitor, è possibile farlo impostando la variabile <option>DEV_ENV</option>
   </para>
   <para>
    che consente di distribuire i monitor senza la presenza della directory <filename>role-storage/</filename>, oltre a distribuire un cluster Ceph con almeno <emphasis>un</emphasis> ruolo storage, monitor e manager.
   </para>
   <para>
    Per impostare la variabile ambientale, abilitarla globalmente nel file <filename>/srv/pillar/ceph/stack/global.yml</filename>, oppure impostarla solo per la sessione di shell corrente:
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
   <para>
    Ad esempio, è possibile creare <filename>/srv/pillar/ceph/stack/global.yml</filename> con i contenuti seguenti:
   </para>
<screen>DEV_ENV: <replaceable>True</replaceable></screen>
  </tip>

  <para>
   La procedura seguente descrive la preparazione del cluster in modo dettagliato.
  </para>

  <procedure>
   <step>
    <para>
     Installare e registrare SUSE Linux Enterprise Server 15 SP1 insieme con l'estensione SUSE Enterprise Storage 6 in ciascun nodo del cluster.
    </para>
   </step>
   <step>
    <para>
     Verificare che i prodotti corretti siano installati e registrati elencando i repository software esistenti. Eseguire <command>zypper lr -E</command> e confrontare l'output con l'elenco seguente:
    </para>
<screen>
 SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates
</screen>
   </step>
   <step>
    <para>
     Configurare le impostazioni di rete compresa la risoluzione del nome DNS corretto su ogni nodo. Il Salt master e tutti i Salt minion devono risolvere ciascuno mediante i loro nomi host. Per ulteriori informazioni sulla configurazione di una rete, vedere <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_network_yast.html"/>. Per ulteriori informazioni sulla configurazione di un server DNS, vedere <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_dns.html"/>.
    </para>
   </step>
   <step>
    <para>
     Selezionare uno o più server dell'orario/pool e sincronizzare l'orario locale rispetto a questi ultimi. Verificare che il servizio di sincronizzazione dell'orario sia abilitato per ogni avvio di sistema. È possibile utilizzare il comando <command>yast ntp-client</command> trovato nel pacchetto <package>yast2-ntp-client</package> per configurare la sincronizzazione dell'orario.
    </para>
    <tip>
     <para>
      Le macchine virtuali non sono origini NTP attendibili.
     </para>
    </tip>
    <para>
     Ulteriori informazioni sulla configurazione di NTP sono disponibili in <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html"/>.
    </para>
   </step>
   <step>
    <para>
     Installare i pacchetti <literal>salt-master</literal> e <literal>salt-minion</literal> sul nodo Salt master:
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     Verificare che il servizio <systemitem>salt-master</systemitem> sia abilitato e avviato, se necessario abilitarlo e avviarlo:
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     Se si intende utilizzare il firewall, verificare che il nodo master abbia le porte 4505 e 4506 aperte per tutti i nodi Salt minion. Se le porte sono chiuse, è possibile aprirle con il comando <command>yast2 firewall</command> consentendo il servizio <guimenu>SaltStack</guimenu>.
    </para>
    <warning>
     <title>le fasi di DeepSea non riescono con il firewall</title>
     <para>
      Le fasi di installazione di DeepSea non riescono se il firewall è attivo (e anche configurato). Per eseguire le fasi correttamente, occorre disattivare il firewall eseguendo
     </para>
<screen>
    <prompt>root # </prompt>systemctl stop firewalld.service
</screen>
     <para>
      oppure impostare l'opzione <option>FAIL_ON_WARNING</option> su "False" in <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     Installare il pacchetto <literal>salt-minion</literal> su tutti i nodi minion.
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     Accertare che il <emphasis>nome di dominio qualificato completo</emphasis> di ciascun nodo possa essere risolto sull'indirizzo IP della rete pubblica da tutti gli altri nodi.
    </para>
   </step>
   <step>
    <para>
     Configurare tutti i minion (compreso il minion master) per il collegamento al master. Se il Salt master non è raggiungibile dal nome host <literal>salt</literal>, modificare il file <filename>/etc/salt/minion</filename> oppure creare un nuovo file <filename>/etc/salt/minion.d/master.conf</filename> con il seguente contenuto:
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     Se sono state apportate modifiche ai file di configurazione menzionati sopra, riavviare il servizio Salt su tutti i Salt minion:
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verificare che il servizio <systemitem>salt-minion</systemitem> sia abilitato e avviato su tutti i nodi. Abilitarlo e avviarlo se necessario:
    </para>
<screen><prompt>root # </prompt>systemctl enable salt-minion.service
<prompt>root # </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     Verificare ogni impronta del Salt minion e accettare tutte le chiavi salt sul Salt master se le impronte corrispondono.
    </para>
    <note>
     <para>
      Se l'impronta digitale del Salt minion viene restituita vuota, assicurarsi che il Salt minion disponga di una configurazione Salt master e che sia in grado di comunicare con quest'ultimo.
     </para>
    </note>
    <para>
     Visualizzare l'impronta di ogni minion:
    </para>
<screen><prompt>root@master # </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Dopo aver raccolto le impronte di tutti i Salt minion, elencare le impronte di tutte le chiavi minion non accettate sul Salt master:
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     Se le impronte digitali dei minion corrispondono, accettarle:
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     Verificare che le chiavi siano state accettate:
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     Prima di distribuire SUSE Enterprise Storage 6, cancellare manualmente tutti i dischi. Ricordare di sostituire "X" con la lettera corretta del disco:
    </para>
    <substeps>
     <step>
      <para>
       Interrompere tutti i processi che utilizzano il disco specifico.
      </para>
     </step>
     <step>
      <para>
       Verificare se eventuali partizioni sul disco sono montate e smontare se necessario.
      </para>
     </step>
     <step>
      <para>
       Se il disco è gestito da LVM, disattivare ed eliminare l'intera infrastruttura LVM. Per ulteriori dettagli, fare riferimento a <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/cha_lvm.html"/>.
      </para>
     </step>
     <step>
      <para>
       Se il disco fa parte di MD RAID, disattivare RAID. Per ulteriori dettagli, fare riferimento a <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/part_software_raid.html"/>.
      </para>
     </step>
     <step>
      <tip>
       <title>riavvio del server</title>
       <para>
        Se si ricevono messaggi di errore come "partition in use" o "kernel can not be updated with the new partition table" durante le fasi seguenti, riavviare il server.
       </para>
      </tip>
      <para>
       Cancellare la parte iniziale di ogni partizione (come <systemitem class="username">root</systemitem>):
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       Cancellare la parte iniziale dell'unità:
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
</screen>
     </step>
     <step>
      <para>
       Cancellare la parte finale dell'unità:
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
</screen>
     </step>
     <step>
      <para>
       Verificare che l'unità sia vuota (senza strutture GPT) utilizzando:
      </para>
<screen>
<prompt>root # </prompt>parted -s /dev/sdX print free
</screen>
      <para>
       oppure
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Facoltativamente, se è necessario preconfigurare le impostazioni di rete del cluster prima dell'installazione del pacchetto <package>deepsea</package> creare manualmente <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> e impostare le opzioni <option>cluster_network:</option> e <option>public_network:</option>. Tenere presente che il file non verrà sovrascritto dopo l'installazione di <package>deepsea</package>.
    </para>
    <tip>
     <title>abilitazione di IPv6</title>
     <para>
      Se è necessario abilitare l'indirizzamento della rete IPv6, fare riferimento alla <xref linkend="ds-modify-ipv6"/>
     </para>
    </tip>
   </step>
   <step>
    <para>
     Installare DeepSea sul nodo Salt master:
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     Il valore del parametro <option>master_minion</option> viene derivato dinamicamente dal file <filename>/etc/salt/minion_id</filename> sul Salt master. Se è necessario sostituire il valore rilevato, modificare il file <filename>/srv/pillar/ceph/stack/global.yml</filename> e impostare un valore pertinente:
    </para>
<screen>
master_minion: <replaceable>MASTER_MINION_NAME</replaceable>
</screen>
    <para>
     Se il Salt master è raggiungibile tramite altri nomi host, utilizzare il nome del Salt minion per il cluster di memorizzazione restituito dal comando <command>salt-key -L</command>. Se è stato utilizzato il nome host predefinito per il Salt master, <emphasis>salt</emphasis>, nel dominio <emphasis>ses</emphasis>, l'aspetto del file è:
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   Ora è possibile distribuire e configurare Ceph. Se non specificato diversamente, tutti i passaggi sono obbligatori.
  </para>

  <note>
   <title>convenzioni del comando Salt</title>
   <para>
    È possibile eseguire <command>salt-run state.orch</command> in due modi: uno è con "stage.<replaceable>STAGE_NUMBER</replaceable>", l'altro è con il nome della fase. Entrambe le annotazioni hanno lo stesso impatto e la scelta del comando da utilizzare dipende dall'utente.
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>Esecuzione delle fasi di distribuzione</title>
   <step>
    <para>
     Assicurarsi che i Salt minion appartenenti al cluster Ceph siano indirizzati correttamente tramite l'opzione <option>deepsea_minions</option> in <filename>/srv/pillar/ceph/deepsea_minions.sls</filename>. Per ulteriori informazioni consultare <xref linkend="ds-minion-targeting-dsminions"/>.
    </para>
   </step>
   <step>
    <para>
     Per default, DeepSea distribuisce i cluster Ceph con profili ottimizzati attivi sui nodi Ceph Monitor, Ceph Manager e Ceph OSD. In alcuni casi potrebbe essere necessario effettuare la distribuzione senza profili ottimizzati. A questo scopo, inserire le righe seguenti in <filename>/srv/pillar/ceph/stack/global.yml</filename> prima di eseguire le fasi di DeepSea:
    </para>
<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>
   </step>
   <step>
    <para>
     <emphasis>Facoltativo</emphasis>: creare sotto volumi Btrfs per <filename>/var/lib/ceph/</filename>. Questa fase deve essere eseguita prima della fase 0 di DeepSea. Per eseguire la migrazione delle directory esistenti o per ulteriori dettagli, consultare questo riferimento: <xref linkend="storage-tips-ceph-btrfs-subvol"/>.
    </para>
    <para>
     Applicare i comandi seguenti a ciascuno dei Salt minion:
    </para>
<screen><prompt>root@master # </prompt>salt 'MONITOR_NODES' saltutil.sync_all
<prompt>root@master # </prompt>salt 'MONITOR_NODES' state.apply ceph.subvolume
</screen>
    <note>
     <para>
      Il comando Ceph.subvolume consente di creare <filename>/var/lib/ceph</filename> come sottovolume Btrfs <filename>@/var/lib/ceph</filename>.
     </para>
    </note>
    <para>
     Il nuovo sottovolume viene a questo punto montato e <literal>/etc/fstab</literal> viene aggiornato.
    </para>
   </step>
   <step>
    <para>
     Preparare il cluster. Per ulteriori dettagli, fare riferimento a <xref linkend="deepsea-stage-description"/>.
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     oppure
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>eseguire o monitorare le fasi con DeepSea CLI</title>
     <para>
      Con DeepSea CLI, è possibile seguire l'avanzamento dell'esecuzione delle fasi in tempo reale, eseguendo DeepSea CLI nella modalità di monitoraggio o eseguendo la fase direttamente tramite DeepSea CLI. Per ulteriori informazioni, vedere la <xref linkend="deepsea-cli"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     La fase di rilevamento raccoglie i dati da tutti i minion e crea frammenti di configurazione memorizzati nella directory <filename>/srv/pillar/ceph/proposals</filename>. I dati sono memorizzati nel formato YAML nei file *.sls o *.yml.
    </para>
    <para>
     Per attivare la fase di rilevazione, eseguire il comando seguente:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     oppure
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     Dopo il corretto completamento di un comando, creare un file <filename>policy.cfg</filename> in <filename>/srv/pillar/ceph/proposals</filename>. Per ulteriori informazioni, vedere la <xref linkend="policy-configuration"/>.
    </para>
    <tip>
     <para>
      Se occorre modificare l'impostazione di rete del cluster, modificare <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename> e modificare le righe che iniziano con <literal>cluster_network:</literal> e <literal>public_network:</literal>.
     </para>
    </tip>
   </step>
   <step>
    <para>
     La fase di configurazione analizza il file <filename>policy.cfg</filename> e unisce i file inclusi nella forma finale. Il contenuto relativo a cluster e ruolo viene posto in <filename>/srv/pillar/ceph/cluster</filename>, mentre il contenuto specifico di Ceph in <filename>/srv/pillar/ceph/stack/default</filename>.
    </para>
    <para>
     Per attivare la fase di configurazione, eseguire il comando seguente:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     oppure
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     La fase di configurazione può richiedere diversi secondi. Al completamento del comando, è possibile visualizzare i dati di Pillar per i minion specificati (ad esempio, denominati <literal>ceph_minion1</literal>, <literal>ceph_minion2</literal>, ecc.) eseguendo:
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <tip>
     <title>modifica del layout dell'OSD</title>
     <para>
      Se si desidera modificare il layout di default dell'OSD e la configurazione dei gruppi di unità, seguire la procedura descritta nel riferimento <xref linkend="ds-drive-groups"/>.
     </para>
    </tip>
    <note>
     <title>sovrascrittura dei default</title>
     <para>
      Non appena il comando viene completato, è possibile visualizzare la configurazione di default e modificarla in base alle esigenze. Per ulteriori informazioni, vedere la <xref linkend="ceph-deploy-ds-custom"/>.
     </para>
    </note>
   </step>
   <step>
    <para>
     Ora è possibile eseguire la fase di installazione. In questa fase viene convalidato il Pillar e vengono avviati i daemon Ceph Monitor e Ceph OSD:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     oppure
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy</screen>
    <para>
     Il comando può richiedere alcuni minuti. In caso di errore, risolvere il problema ed eseguire di nuovo le fasi precedenti. Al corretto completamento del comando, eseguire questo comando per verificare lo stato:
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     L'ultima fase dell'installazione del cluster Ceph è quella dei <emphasis>servizi</emphasis>. Qui è possibile creare un'istanza di uno dei servizi correntemente supportati: iSCSI Gateway, CephFS, Object Gateway e NFS Ganesha. In questa fase, vengono creati i pool necessari, i portachiavi di autorizzazione e i servizi di avvio. Per avviare la fase, eseguire il comando:
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     oppure
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     In base alla configurazione, l'esecuzione del comando può richiedere vari minuti.
    </para>
   </step>
   <step>
    <para>
     Prima di continuare, si consiglia di abilitare il modulo di telemetria Ceph. Per ulteriori informazioni e istruzioni, consultare questo riferimento: <xref linkend="mgr-modules-telemetry"/>.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSea fornisce inoltre uno strumento di interfaccia riga di comando (CLI) che consente all'utente di monitorare o eseguire le fasi mentre visualizza l'avanzamento dell'esecuzione in tempo reale. Verificare che la versione del pacchetto <package>deepsea-cli</package> sia installato prima di aprire l'eseguibile <command>deepsea</command>.
  </para>

  <para>
   Per visualizzare l'avanzamento dell'esecuzione di una fase sono supportate due modalità:
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>Modalità DeepSea CLI</title>
   <listitem>
    <para>
     <emphasis role="bold">Modalità di monitoraggio</emphasis>: visualizza l'avanzamento dell'esecuzione di una fase di DeepSea attivata dal comando <command>salt-run</command> emesso in un'altra sessione del terminale.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">Modalità stand-alone</emphasis>: esegue una fase di DeepSea fornendo la visualizzazione in tempo reale delle relative fasi componenti durante l'esecuzione.
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>comandi DeepSea CLI</title>
   <para>
    I comandi dell'interfaccia riga di comando DeepSea possono essere eseguiti solo nel nodo Salt master, con privilegi di <systemitem class="username">root</systemitem>.
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI: nodo monitor</title>
   <para>
    Il monitor di avanzamento fornisce una visualizzazione dettagliata in tempo reale di quanto si verifica durante l'esecuzione delle fasi mediante i comandi <command>salt-run state.orch</command> in altre sessioni del terminale.
   </para>
   <tip>
    <title>avvio del monitor in una nuova sessione del terminale</title>
    <para>
     Per fare in modo che il monitor rilevi l'avvio dell'esecuzione della fase, è necessario avviarlo in una nuova finestra terminale <emphasis>prima</emphasis> di eseguire <command>salt-run state.orch</command>.
    </para>
   </tip>
   <para>
    Se si avvia il monitor dopo aver eseguito il comando <command>salt-run state.orch</command>, non viene mostrato alcun avanzamento dell'esecuzione.
   </para>
   <para>
    È possibile avviare la modalità monitor utilizzando il comando che segue:
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    Per ulteriori informazioni sulle opzioni della riga di comando disponibili del comando <command>deepsea monitor</command>, consultare la relativa documentazione:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI: modalità stand-alone</title>
   <para>
    Nella modalità stand-alone, è possibile utilizzare DeepSea CLI per eseguire una fase DeepSea, mostrandone l'esecuzione in tempo reale.
   </para>
   <para>
    Il comando per eseguire una fase DeepSea da DeepSea CLI ha la forma seguente:
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    dove <replaceable>stage-name</replaceable> corrisponde al modo in cui viene fatto riferimento ai file di stato di orchestrazione Salt. Ad esempio, alla fase <emphasis role="bold">distribuzione</emphasis>, che corrisponde alla directory ubicata in <filename>/srv/salt/ceph/stage/deploy</filename>, si fa riferimento come <emphasis role="bold">ceph.stage.deploy</emphasis>.
   </para>
   <para>
    Questo comando è un'alternativa ai comandi basati su Salt per eseguire le fasi DeepSea (o qualunque file di stato di orchestrazione DeepSea).
   </para>
   <para>
    Il comando <command>deepsea stage run ceph.stage.0</command> è equivalente a <command>salt-run state.orch ceph.stage.0</command>.
   </para>
   <para>
    Per ulteriori informazioni sulle opzioni della riga di comando accettate dal comando <command>deepsea stage run</command> disponibili, consultare la relativa documentazione:
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    La figura seguente mostra un esempio del risultato di DeepSea CLI quando si esegue la <emphasis role="underline">Fase 2</emphasis>:
   </para>
   <figure>
    <title>Risultato avanzamento esecuzione fase DeepSea CLI</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>Alias <command>stage run</command> DeepSea CLI</title>
    <para>
     Per gli utenti avanzati di Salt, è inoltre supportato un alias per eseguire una fase di DeepSea che utilizza il comando Salt per eseguire una fase, ad esempio, <command>salt-run state.orch <replaceable>stage-name</replaceable></command>, come comando di DeepSea CLI.
    </para>
    <para>
     Esempio:
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>Configurazione e personalizzazione</title>

  <sect2 xml:id="policy-configuration">
   <title>Il file <filename>policy.cfg</filename></title>
   <para>
    Il file di configurazione <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> consente di determinare i ruoli dei singoli nodi del cluster. Ad esempio, quali nodi fungono da Ceph OSD o da Ceph Monitor. Modificare <filename>policy.cfg</filename> per riflettere la configurazione del cluster desiderata. L'ordine delle sezioni è arbitrario, ma il contenuto delle righe incluse sovrascrive le chiavi corrispondenti dal contenuto delle righe precedenti.
   </para>
   <tip>
    <title>esempi di <filename>policy.cfg</filename></title>
    <para>
     È possibile trovare diversi esempi dei file di policy completi nella directory <filename>/usr/share/doc/packages/deepsea/examples/</filename>.
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>Assegnazione cluster</title>
    <para>
     Nella sezione <emphasis role="bold">cluster</emphasis> è possibile selezionare i minion per il cluster. È possibile selezionare tutti i minion, oppure inserire i minion in blacklist o whitelist. Di seguito vengono forniti esempi per un cluster denominato <emphasis role="bold">ceph</emphasis>.
    </para>
    <para>
     Per includere <emphasis role="bold">tutti</emphasis> i minion, aggiungere le righe seguenti:
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     Per inserire nella <emphasis role="bold">whitelist</emphasis> un minion particolare:
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     oppure un gruppo di minion, è possibile utilizzare la corrispondenza con caratteri jolly della shell:
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     Per inserire nella <emphasis role="bold">blacklist</emphasis> i minion, impostarli su <literal>unassigned</literal>:
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>Assegnazione ruolo</title>
    <para>
     Questa sezione fornisce i dettagli per l'assegnazione dei "ruoli" ai nodi cluster. Un "ruolo" in questo contesto indica il servizio che occorre eseguire sul nodo, come Ceph Monitor, Object Gateway o iSCSI Gateway. Nessun ruolo viene assegnato automaticamente, vengono distribuiti solo i ruoli aggiunti a <command>policy.cfg</command>.
    </para>
    <para>
     L'assegnazione segue questo schema:
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     Dove le voci hanno i seguenti significati e valori:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable> è uno dei seguenti: "master", "admin", "mon", "mgr", "storage", "mds", "igw", "rgw", "ganesha", "grafana" o "prometheus".
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable> è un percorso di directory relativo ai file .sls o .yml. Nel caso dei file .sls, in genere è <filename>cluster</filename>, mentre i file .yml si trovano in <filename>stack/default/ceph/minions</filename>.
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable> sono i file di stato Salt o i file di configurazione YAML che consistono normalmente di nomi host dei Salt minion, ad esempio <filename>ses5min2.yml</filename>. Per una corrispondenza più specifica è possibile utilizzare la corrispondenza con caratteri jolly della shell.
      </para>
     </listitem>
    </itemizedlist>
    <para>
     Segue un esempio per ogni ruolo:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - il nodo ha portachiavi admin su tutti i cluster Ceph. Attualmente, è supportato solo un singolo cluster Ceph. Poiché il ruolo <emphasis>master</emphasis> è obbligatorio, aggiungere sempre una riga simile a:
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - il minion ha un portachiavi admin. Definire il ruolo nel modo seguente:
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - il minion fornisce il servizio di monitoraggio al cluster Ceph. Questo ruolo richiede gli indirizzi dei minion assegnati. A partire da SUSE Enterprise Storage 5, gli indirizzi pubblici vengono calcolati dinamicamente e non sono più necessari in salt pillar.
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       Nell'esempio, il ruolo di monitoraggio viene assegnato a un gruppo di minion.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mgr</emphasis> - il daemon manager Ceph che raccoglie tutte le informazioni sullo stato dall'intero cluster. Distribuirlo su tutti i minion dove si pianifica di distribuire il ruolo monitor Ceph.
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>storage</emphasis> - utilizzare questo ruolo per specificare i nodi di storage.
      </para>
<screen>role-storage/cluster/data*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - il minion fornisce il servizio metadati per supportare CephFS.
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - il minion funge da iSCSI Gateway. Questo ruolo richiede gli indirizzi dei minion assegnati, perciò occorre anche includere i file della directory <filename>stack</filename>:
      </para>
<screen>role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - il minion funge da Object Gateway:
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>ganesha</emphasis> - il minion funge da server NFS Ganesha. Il ruolo "ganesha" richiede un ruolo "rgw" o "mds" nel cluster, in caso contrario la convalida non riesce nella Fase 3.
      </para>
<screen>role-ganesha/cluster/ganesha*.sls</screen>
      <para>
       Per installare correttamente NFS Ganesha, è richiesta una configurazione aggiuntiva. Se si desidera utilizzare NFS Ganesha, leggere <xref linkend="cha-as-ganesha"/> prima di eseguire le fasi 2 e 4. Tuttavia, è possibile installare NFS Ganesha in seguito.
      </para>
      <para>
       In alcuni casi può essere utile definire ruoli personalizzati per i nodi NFS Ganesha. Per informazioni, vedere <xref linkend="ceph-nfsganesha-customrole"/>.
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>grafana</emphasis>, <emphasis>prometheus</emphasis> - questo nodo aggiunge i grafici Grafana basati sugli avvisi Prometheus sul Ceph Dashboard. Per la descrizione dettagliata, consultare questo riferimento: <xref linkend="ceph-dashboard"/>.
      </para>
<screen>role-grafana/cluster/grafana*.sls</screen>
<screen>role-prometheus/cluster/prometheus*.sls</screen>
     </listitem>
    </itemizedlist>
    <note>
     <title>ruoli multipli dei nodi del cluster</title>
     <para>
      È possibile assegnare più ruoli a un singolo nodo. Ad esempio, è possibile assegnare i ruoli "mds" ai nodi monitor:
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>Configurazione comune</title>
    <para>
     La sezione di configurazione comune comprende i file di configurazione generati durante la <emphasis>rilevazione (fase 1)</emphasis>. Questi file di configurazione memorizzano parametri come <literal>fsid</literal> o <literal>public_network</literal>. Per includere la configurazione comune Ceph richiesta, aggiungere le righe seguenti:
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>Filtraggio voci</title>
    <para>
     A volte non è pratico includere tutti i file di una data directory con il carattere jolly *.sls. L'analizzatore di file <filename>policy.cfg</filename> comprende i seguenti filtri:
    </para>
    <warning>
     <title>tecniche avanzate</title>
     <para>
      Questa sezione descrive le tecniche di filtraggio per utenti avanzati. Se non utilizzati correttamente, i filtri possono provocare problemi, ad esempio se cambia la numerazione del nodo.
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        Utilizzare il filtro slice per includere solo le voci da <emphasis>start</emphasis> a <emphasis>end-1</emphasis>. Tenere presente che le voci nella directory data sono in ordine alfabetico. La riga seguente comprende i file dal terzo al quinto della sottodirectory <filename>role-mon/cluster/</filename>:
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        Utilizzare il filtro dell'espressione regolare per includere solo le voci che corrispondono alle espressioni date. Ad esempio:
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title>File <filename>policy.cfg</filename> di esempio</title>
    <para>
     Di seguito viene fornito un esempio di un file <filename>policy.cfg</filename> di base:
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# STORAGE
role-storage/cluster/ses-example-[5,6,7,8].sls <co xml:id="co-policy-storage"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>
</screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       Indica che tutti i minion sono inclusi nel cluster Ceph. Se sono presenti minion che non si desidera includere nel cluster Ceph, utilizzare:
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       La prima riga indica tutti i minion come non assegnati. La seconda riga sovrascrive i minion che corrispondono a "ses-example-*.sls" e li assegna al cluster Ceph.
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       Il minion denominato "examplesesadmin" ha il ruolo "master", quindi ottiene le chiavi admin per il cluster.
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       Anche tutti i minion che corrispondono a "sesclient*" ottengono le chiavi admin.
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       Tutti i minion che corrispondono a "ses-example-[123]" (presumibilmente tre minion: ses-example-1, ses-example-2 e ses-example-3) vengono impostati come nodi MON.
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       Tutti i minion che corrispondono a "ses-example-[123]" (tutti i nodi MON nell'esempio) vengono impostati come nodi MGR.
      </para>
     </callout>
     <callout arearefs="co-policy-storage">
      <para>
       Tutti i minion corrispondenti a "ses-example-[5,6,7,8]" verranno configurati come nodi di storage.
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       Il minion "ses-example-4" avrà il ruolo MDS.
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       Il minion "ses-example-4" avrà il ruolo IGW.
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       Il minion "ses-example-4" avrà il ruolo RGW.
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       Significa che si accettano i valori di default per i parametri di configurazione comuni come <option>fsid</option> e <option>public_network</option>.
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       Significa che si accettano i valori di default per i parametri di configurazione comuni come <option>fsid</option> e <option>public_network</option>.
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="ds-drive-groups">
   <title>DriveGroups</title>
   <para>
    I <emphasis>DriveGroups</emphasis> specificano i layout degli OSD nel cluster Ceph. Questi sono definiti in un singolo file <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>.
   </para>
   <para>
    L'amministratore deve specificare manualmente un gruppo di OSD correlati tra di loro (OSD ibridi distribuiti su unità a stato solido e a rotazione) o condividere le stesse opzioni di distribuzione (ad esempio lo stesso archivio dati, la stessa opzione di cifratura, gli stessi OSD stand-alone). Per evitare di elencare esplicitamente i dispositivi, i DriveGroups utilizzano un elenco di elementi di filtro che corrispondono ad alcuni campi selezionati dei rapporti di archivio di <command>ceph-volume</command>. Nel caso più semplice, può ad esempio trattarsi del flag "rotational" (tutte le unità a stato solido devono essere dispositivi di database e quelle a rotazione dispositivi di dati) o un elemento più specifico, come le dimensioni o le stringhe "model". In DeepSea è fornito un codice che traduce tali DriveGroups in elenchi di dispositivi effettivi che l'utente potrà esaminare.
   </para>
   <para>
    Di seguito è riportata una procedura di base in cui è illustrato il workflow di base durante la configurazione dei DriveGroups:
   </para>
   <procedure>
    <step>
     <para>
      Esaminare le proprietà del disco in uso tramite il comando <command>ceph-volume</command>. Soltanto le proprietà seguenti sono accettate dai DriveGroups:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.details
</screen>
    </step>
    <step>
     <para>
      Aprire il file YAML<filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> e modificarlo in base alle esigenze. Vedere la <xref linkend="ds-drive-groups-specs"/>. Ricordare di utilizzare gli spazi al posto dei caratteri di tabulazione. Nella <xref linkend="ds-drive-groups-examples"/> sono riportati esempi più avanzati. L'esempio seguente include tutte le unità disponibili per Ceph come OSD:
     </para>
<screen>
default_drive_group_name:
  target: '*'
  data_devices:
    all: true
</screen>
    </step>
    <step>
     <para>
      Verificare i nuovi layout:
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
     <para>
      Questo strumento di esecuzione restituisce una struttura di dischi corrispondenti basata sui DriveGroups. Se il risultato non è soddisfacente, ripetere il passaggio precedente.
     </para>
     <tip>
      <title>rapporto dettagliato</title>
      <para>
       Oltre allo strumento di esecuzione <command>disks.list</command>, è disponibile uno strumento di esecuzione <command>disks.report</command> che stampa un rapporto dettagliato di ciò che si verifica quando viene richiamata la successiva fase 3 di DeepSea.
      </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
     </tip>
    </step>
    <step>
     <para>
      Distribuire gli OSD. Quando viene richiamata la successiva fase 3 di DeepSea, i dischi OSD verranno distribuiti in base alla specifica del gruppo di unità.
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ds-drive-groups-specs">
    <title>Specifica</title>
    <para>
     <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> accetta le opzioni seguenti:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  db_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  wal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    <para>
     Nelle configurazioni FileStore, <filename>drive_groups.yml</filename> può avere l'aspetto seguente:
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  journal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  format: filestore
  encryption: True
</screen>
   </sect3>
   <sect3>
    <title>Creazione di corrispondenze dei dispositivi disco</title>
    <para>
     È possibile descrivere la specifica tramite i filtri seguenti:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       In base al modello di disco:
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       In base al produttore del disco:
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>stringa del produttore in lettere minuscole</title>
       <para>
        Scrivere sempre <replaceable>DISK_VENDOR_STRING</replaceable> in lettere minuscole.
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       Per indicare se si tratta di un disco rotativo o meno. Le unità SSD e NVME non sono rotative.
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       Per distribuire un nodo utilizzando <emphasis>tutte</emphasis> le unità disponibili per gli OSD:
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       Inoltre, tramite la limitazione del numero di dischi corrispondenti:
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>Aggiunta di filtri ai dispositivi in base alle dimensioni</title>
    <para>
     È possibile filtrare i dispositivi disco in base alle dimensioni (valore esatto o intervallo di dimensioni). Il parametro <option>size:</option> accetta gli argomenti nel formato seguente:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       "10G" - include i dischi di dimensioni esatte.
      </para>
     </listitem>
     <listitem>
      <para>
       "10G:40G" - include i dischi di dimensioni comprese nell'intervallo.
      </para>
     </listitem>
     <listitem>
      <para>
       ":10G" - include i dischi di dimensioni inferiori o uguali a 10 GB.
      </para>
     </listitem>
     <listitem>
      <para>
       "40G" - include i dischi di dimensioni uguali o superiori a 40 GB.
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>Corrispondenza in base alle dimensioni del disco</title>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <note>
     <title>virgolette obbligatorie</title>
     <para>
      Quando si utilizza il delimitatore ":", occorre racchiudere le dimensioni tra virgolette altrimenti il simbolo ":" verrà interpretato come un nuovo hash di configurazione.
     </para>
    </note>
    <tip>
     <title>scorciatoie di unità</title>
     <para>
      Invece di (G)igabyte, è possibile specificare le dimensioni anche in (M)egabyte o (T)erabyte.
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>Esempi</title>
    <para>
     Questa sezione include esempi di diverse configurazioni OSD.
    </para>
    <example>
     <title>Configurazione semplice</title>
     <para>
      Questo esempio descrive due nodi con la stessa configurazione:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Il file <filename>drive_groups.yml</filename> corrispondente avrà l'aspetto seguente:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ
   </screen>
     <para>
      Tale configurazione è semplice e valida. Tuttavia, in futuro un amministratore può aggiungere dischi di altri produttori che non verranno inclusi. È possibile ovviare a questo problema riducendo i filtri sulle proprietà di base delle unità:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
   </screen>
     <para>
      Nell'esempio precedente, viene forzata la dichiarazione di tutti i dispositivi a rotazione come "dispositivi di dati" e tutti i dispositivi non a rotazione verranno utilizzati come "dispositivi condivisi" (wal, db).
     </para>
     <para>
      Presupponendo che le unità di più di 2 TB saranno sempre i dispositivi di dati più lenti, è possibile applicare dei filtri in base alle dimensioni:
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <example>
     <title>Configurazione avanzata</title>
     <para>
      Questo esempio descrive due configurazioni diverse: 20 HDD devono condividere 2 SSD, mentre 10 SSD devono condividere 2 NVMe.
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      È possibile definire tale configurazione con due layout come segue:
     </para>
<screen>
drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
</screen>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB
</screen>
    </example>
    <example>
     <title>Configurazione avanzata con nodi non uniformi</title>
     <para>
      Gli esempi precedenti sono basati sul presupposto che tutti i nodi dispongano delle stesse unità. Tuttavia, non è sempre questo il caso:
     </para>
     <para>
      Nodi da 1 a 5:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      Nodi da 6 a 10:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      È possibile utilizzare la chiave "target" nel layout per indirizzare nodi specifici. La notazione della destinazione Salt consente di non complicare la configurazione:
     </para>
<screen>
drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
</screen>
     <para>
      seguito da
     </para>
<screen>
drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>Configurazione esperta</title>
     <para>
      In tutti i casi descritti in precedenza si presupponeva che i WAL e i DB utilizzassero lo stesso dispositivo. È tuttavia possibile anche distribuire i WAL su un dispositivo dedicato:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>Configurazione complessa (e improbabile)</title>
     <para>
      Nella configurazione seguente, si tenterà di definire:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 HDD supportati da 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 HDD supportati da 1 SSD (db) e 1 NVMe (wal)
       </para>
      </listitem>
      <listitem>
       <para>
        8 SSD supportati da 1 NVMe
       </para>
      </listitem>
      <listitem>
       <para>
        2 SSD stand-alone (cifrati)
       </para>
      </listitem>
      <listitem>
       <para>
        1 HDD è di riserva e non deve essere distribuito.
       </para>
      </listitem>
     </itemizedlist>
     <para>
      Di seguito è riportato il riepilogo delle unità utilizzate:
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Intel
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Micron
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          Produttore: Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          Modello: NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          Dimensioni: 256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      La definizione dei DriveGroups sarà la seguente:
     </para>
<screen>
drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True
 </screen>
     <para>
      Rimarrà un'unità HDD, poiché il file viene analizzato dall'alto verso il basso.
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2>
   <title>Regolazione di <filename>ceph.conf</filename> tramite le impostazioni personalizzate</title>
   <para>
    Se occorre inserire impostazioni personalizzate nel file di configurazione <filename>ceph.conf</filename>, per ulteriori informazioni, vedere <xref linkend="ds-custom-cephconf"/>.
   </para>
  </sect2>
 </sect1>
</chapter>
