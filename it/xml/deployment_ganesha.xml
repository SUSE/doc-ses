<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>Installazione di NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>modifica</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>sì</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha fornisce accesso NFS a Object Gateway o a CephFS. In SUSE Enterprise Storage 6, sono supportate le versioni NFS 3 e 4. NFS Ganesha viene eseguito nello spazio utente invece che nello spazio kernel e interagisce direttamente con l'Object Gateway o CephFS.
 </para>
 <warning>
  <title>accesso su più protocolli</title>
  <para>
   I client CephFS e NFS nativi non sono limitati dai blocchi di file ottenuti tramite Samba e viceversa. I dati delle applicazioni basate sul blocco di file su più protocolli potrebbero risultare danneggiati se l'accesso ai percorsi della condivisione Samba supportati da CephFS viene effettuato in altri modi.
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>Preparazione</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>Informazioni generali</title>
   <para>
    Per installare correttamente NFS Ganesha, occorre aggiungere un <literal>role-ganesha</literal> a <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>. Per informazioni, vedere <xref linkend="policy-configuration"/>. NFS Ganesha richiede inoltre un <literal>role-rgw</literal> o un <literal>role-mds</literal> presente in <filename>policy.cfg</filename>.
   </para>
   <para>
    Sebbene sia possibile installare ed eseguire il server NFS Ganesha su un nodo Ceph già esistente, si consiglia di eseguirlo su un host dedicato con accesso al cluster Ceph. Gli host client non fanno in genere parte del cluster, ma devono avere accesso di rete al server NFS Ganesha.
   </para>
   <para>
    Per abilitare il server NFS Ganesha in qualsiasi punto dopo l'installazione iniziale, aggiungere <literal>role-ganesha</literal> a <filename>policy.cfg</filename> e ripetere almeno le fasi 2 e 4 di DeepSea. Per informazioni, vedere <xref linkend="ceph-install-stack"/>.
   </para>
   <para>
    NFS Ganesha è configurato tramite il file <filename>/etc/ganesha/ganesha.conf</filename> esistente sul nodo NFS Ganesha. Tuttavia, tale file viene sovrascritto ogni volta che si esegue la fase 4 di DeepSea. Perciò si consiglia di modificare il modello utilizzato da Salt, ossia il file <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> sul Salt master. Per informazioni sul file di configurazione, vedere <xref linkend="ceph-nfsganesha-config"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>Riepilogo dei requisiti</title>
   <para>
    Prima di poter eseguire le fasi 2 e 4 di DeepSea per installare NFS Ganesha, occorre soddisfare i seguenti requisiti:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Almeno un nodo deve essere assegnato a <literal>role-ganesha</literal>.
     </para>
    </listitem>
    <listitem>
     <para>
      È possibile definire solo un <literal>role-ganesha</literal> per minion.
     </para>
    </listitem>
    <listitem>
     <para>
      Per il funzionamento, NFS Ganesha richiede un Object Gateway o CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      L'NFS basato sul kernel deve essere disabilitato sui minion con il ruolo <literal>role-ganesha</literal>.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>Installazione di esempio</title>

  <para>
   Questa procedura fornisce un'installazione di esempio che utilizza l'Object Gateway e CephFS File System Abstraction Layers (FSAL) di NFS Ganesha.
  </para>

  <procedure>
   <step>
    <para>
     Se non è già stato fatto, eseguire le fasi 0 e 1 di DeepSea prima di continuare con questa procedura.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     Dopo aver eseguito la fase 1 di DeepSea, modificare <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> e aggiungere la riga
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     Sostituire <replaceable>NODENAME</replaceable> con il nome di un nodo nel cluster.
    </para>
    <para>
     Accertare inoltre che siano assegnati un <literal>role-mds</literal> e un <literal>role-rgw</literal>.
    </para>
   </step>
   <step>
    <para>
     Eseguire almeno le fasi 2 e 4 di DeepSea. Si consiglia l'esecuzione della fase 3 tra le altre due fasi.
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     Controllare che il servizio NFS Ganesha sia in esecuzione sul nodo minion per assicurarsi che NFS Ganesha sia attivo:
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>Configurazione attiva-passiva ad alta disponibilità</title>

  <para>
   Questa sezione fornisce un esempio di come impostare una configurazione attiva-passiva a due nodi dei server NFS Ganesha. La configurazione richiede SUSE Linux Enterprise High Availability Extension. I due nodi sono denominati <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem>.
  </para>

  <important>
   <title>co-location dei servizi</title>
   <para>
    I servizi che dispongono di una propria tolleranza agli errori e di un proprio bilanciamento del carico non devono essere in esecuzione sui nodi del cluster a cui viene applicata la priorità per i servizi di failover. Pertanto, non eseguire i servizi Ceph Monitor, del server di metadati, iSCSI o Ceph OSD sulle configurazioni a elevata disponibilità.
   </para>
  </important>

  <para>
   Per informazioni su SUSE Linux Enterprise High Availability Extension, vedere <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>.
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>Installazione di base</title>
   <para>
    In questa configurazione <systemitem class="domainname">earth</systemitem> ha l'indirizzo IP <systemitem class="ipaddress">192.168.1.1</systemitem> e <systemitem class="domainname">mars</systemitem> l'indirizzo <systemitem class="ipaddress">192.168.1.2</systemitem>.
   </para>
   <para>
    Inoltre, vengono utilizzati due indirizzi IP virtuali mobili che consentono ai client di connettersi al servizio indipendentemente dal nodo fisico sul quale è in esecuzione. <systemitem class="ipaddress">192.168.1.10</systemitem> è utilizzato per amministrazione del cluster con Hawk2 e <systemitem class="ipaddress">192.168.2.1</systemitem> esclusivamente per esportazioni NFS. Ciò semplifica la successiva applicazione delle limitazioni di sicurezza.
   </para>
   <para>
    La procedura seguente descrive l'installazione di esempio. Ulteriori informazioni sono disponibili all'indirizzo <link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/>.
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      Preparare i nodi NFS Ganesha sul Salt master:
     </para>
     <substeps>
      <step>
       <para>
        Eseguire le fasi 0 e 1 di DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        Assegnare ai nodi <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem> il <literal>role-ganesha</literal> in <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>:
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        Eseguire le fasi da 2 a 4 di DeepSea.
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Registrare SUSE Linux Enterprise High Availability Extension su <systemitem class="domainname">earth</systemitem> e <systemitem class="domainname">mars</systemitem>.
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      Installare <package>ha-cluster-bootstrap</package> su entrambi i nodi:
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        Inizializzare il cluster su <systemitem class="domainname">earth</systemitem>:
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        Lasciare che <systemitem class="domainname">mars</systemitem> si unisca al cluster:
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      Verificare lo stato del cluster. Si dovrebbero vedere due nodi aggiunti al cluster:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      Su entrambi i nodi, disabilitare l'avvio automatico del servizio NFS Ganesha durante il boot:
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      Avviare la shell <command>crm</command> su <systemitem class="domainname">earth</systemitem>:
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      I comandi successivi vengono eseguiti nella shell crm.
     </para>
    </step>
    <step>
     <para>
      Su <systemitem class="domainname">earth</systemitem>, avviare la shell crm per eseguire i comandi indicati per configurare la risorsa per i daemon NFS Ganesha come cloni del tipo di risorsa systemd:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      Creare un IPAddr2 primitivo con la shell crm:
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      Per configurare una relazione tra il server NFS Ganesha e l'IP virtuale mobile, si utilizza collocazione e ordinamento.
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      Utilizzare il comando <command>mount</command> dal client per assicurare che la configurazione del cluster sia completa:
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>Effettuare la pulizia delle risorse</title>
   <para>
    Nel caso di errore di NFS Ganesha in uno dei nodi, ad esempio <systemitem class="domainname">earth</systemitem>, risolvere il problema ed effettuare la pulizia della risorsa. Solo dopo aver effettuato la pulizia, la risorsa può riposizionarsi in sicurezza su <systemitem class="domainname">earth</systemitem> in caso di errore di NFS Ganesha su <systemitem class="domainname">mars</systemitem>.
   </para>
   <para>
    Per effettuare la pulizia della risorsa:
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>Impostazione della risorsa di ping</title>
   <para>
    In alcune situazioni, il server potrebbe non essere in grado di raggiungere il client a causa di un problema di rete. Una risorsa di ping può rilevare e mitigare questo problema. La configurazione della risorsa è facoltativa.
   </para>
   <procedure>
    <step>
     <para>
      Definire la risorsa di ping:
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> è un elenco di indirizzi IP separati da spazi. Viene eseguito regolarmente il ping sugli indirizzi IP per controllare eventuali indisponibilità di rete. Se un client deve sempre avere accesso al server NFS, aggiungerlo a <literal>host_list</literal>.
     </para>
    </step>
    <step>
     <para>
      Creare un clone:
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      Il comando seguente consente di creare un vincolo per il servizio NFS Ganesha. Forza il servizio a spostarsi su un altro nodo quando <literal>host_list</literal> non è raggiungibile.
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>DeepSea e HA di NFS Ganesha</title>
   <para>
    DeepSea non supporta la configurazione HA (alta disponibilità) di NFS Ganesha. Per impedire a DeepSea di andare in errore dopo aver configurato HA di NFS Ganesha, escludere l'avvio e l'interruzione del servizio NFS Ganesha dalla fase 4 di DeepSea:
   </para>
   <procedure>
    <step>
     <para>
      Copiare <filename>/srv/salt/ceph/ganesha/default.sls</filename> in <filename>/srv/salt/ceph/ganesha/ha.sls</filename>.
     </para>
    </step>
    <step>
     <para>
      Rimuovere la voce <literal>.service</literal> da <filename>/srv/salt/ceph/ganesha/ha.sls</filename> in modo che abbia l'aspetto seguente:
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      Aggiungere la riga seguente a <filename>/srv/pillar/ceph/stack/global.yml</filename>:
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>Configurazione attiva-attiva</title>

  <para>
   Questa sezione fornisce un esempio di una configurazione attiva-attiva semplice di NFS Ganesha. Lo scopo di questa configurazione consiste nel distribuire due server NFS Ganesha a un livello superiore dello stesso CephFS esistente. I server saranno due nodi del cluster Ceph con indirizzi separati. I client devono essere distribuiti tra questi ultimi manualmente. Il termine <quote>failover</quote> in questa configurazione indica lo smontaggio e il rimontaggio manuali dell'altro server sul client.
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>Prerequisiti</title>
   <para>
    Per questa configurazione di esempio, è necessario quanto segue:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Cluster Ceph in esecuzione. Vedere la <xref linkend="ceph-install-stack"/> per i dettagli sulla distribuzione e la configurazione del cluster Ceph tramite DeepSea.
     </para>
    </listitem>
    <listitem>
     <para>
      Almeno un CephFS configurato. Vedere il <xref linkend="cha-ceph-as-cephfs"/> per ulteriori dettagli sulla distribuzione e la configurazione di CephFS.
     </para>
    </listitem>
    <listitem>
     <para>
      Due nodi del cluster Ceph con NFS Ganesha distribuito. Vedere il <xref linkend="cha-as-ganesha"/> per ulteriori dettagli sulla distribuzione di NFS Ganesha.
     </para>
     <tip>
      <title>utilizzo di server dedicati</title>
      <para>
       Sebbene i nodi NFS Ganesha possano condividere le risorse con altri servizi correlati a Ceph, si consiglia di utilizzare server dedicati per migliorare le prestazioni.
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    Dopo aver distribuito i nodi NFS Ganesha, verificare che il cluster sia operativo e che i pool CephFS di default siano presenti:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>Configurazione di NFS Ganesha</title>
   <para>
    Controllare che su entrambi i nodi NFS Ganesha si installato il file<filename>/etc/ganesha/ganesha.conf</filename>. Aggiungere i blocchi seguenti, se non ancora esistenti, al file di configurazione per abilitare RADOS come back-end di recupero di NFS Ganesha.
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   È possibile individuare i valori di <replaceable>rados_pool</replaceable> e <replaceable>pool_namespace</replaceable> controllando la riga già esistente nella configurazione del modulo:</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   Il valore dell'opzione <replaceable>nodeid</replaceable> corrisponde all'FQDN del computer e il valore delle opzioni <replaceable>UserId</replaceable> e <replaceable>Ceph_Conf</replaceable> può essere individuato nel blocco <replaceable>RADOS_URLS</replaceable> già esistente.
   </para>
   <para>
    Le opzioni di NFS precedenti alla versione 4.2 sono disabilitate poiché tali versioni legacy di NFS impediscono la revoca della moratoria in una fase precedente, prolungando di conseguenza un riavvio del server. Inoltre, è disabilitata anche la maggior parte delle funzioni di memorizzazione nella cache di NFS Ganesha poiché le librerie Ceph eseguono già una memorizzazione aggressiva nella cache.
   </para>
   <para>
    Il back-end di recupero "rados_cluster" memorizza le relative informazioni negli oggetti RADOS. Sebbene non si tratti di un'elevata quantità di dati, è preferibile impostarlo come altamente disponibile. A tal fine, viene utilizzato il pool di metadati CephFS all'interno del quale viene dichiarato un nuovo spazio dei nomi "ganesha" per distinguerlo dagli oggetti CephFS.
   </para>
   <note>
    <title>ID del nodo del cluster</title>
    <para>
     La maggior parte delle impostazioni di configurazione è identica per i due host, tuttavia l'opzione <option>nodeid</option> nel blocco "RADOS_KV" deve essere una stringa univoca per ciascun nodo. Per default, NFS Ganesha imposta <option>nodeid</option> sul nome host del nodo.
    </para>
    <para>
     Se è necessario utilizzare valori fissi diversi dai nomi host, è possibile ad esempio impostare <option>nodeid = 'a'</option> su un nodo e <option>nodeid = 'b'</option> sull'altro.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>Popolamento del database extra del cluster</title>
   <para>
    È necessario verificare che tutti i nodi nel cluster siano consapevoli della reciproca esistenza. Per farlo, ci si serve di un oggetto RADOS condiviso tra gli host. NFS Ganesha utilizza tale oggetto per comunicare lo stato corrente relativamente a una moratoria.
   </para>
   <para>
    Il pacchetto <package>nfs-ganesha-rados-grace</package> contiene uno strumento a riga di comando per l'interrogazione e la gestione di questo database. Se il pacchetto non è installato su almeno uno dei nodi, installarlo con
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    Verrà utilizzato il comando per creare il DB e aggiungere entrambi i <option>nodeid</option>. Nell'esempio, i due nodi NFS Ganesha sono denominati <literal>ses6min1.example.com</literal> e <literal>ses6min2.example.com</literal>. Su uno degli host NFS Ganesha, eseguire
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    Questa procedura consente di creare il database extra e di aggiungervi "ses6min1.example.com" e "ses6min2.example.com". L'ultimo comando consente di eseguire il dump dello stato corrente. Si presuppone sempre che gli host appena aggiunti applichino la moratoria e di conseguenza viene impostato per entrambi il flag "E". I valori "cur" e "rec" indicano rispettivamente le epoche attuale e di recupero, tramite cui è possibile tenere traccia degli host a cui è consentito eseguire il recupero e quando.
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>Riavvio dei servizi NFS Ganesha</title>
   <para>
    Su entrambi i nodi NFS Ganesha, riavviare i servizi correlati:
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    In seguito al riavvio dei servizi, controllare il database extra:
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>flag "E" cancellato</title>
    <para>
     Notare che il flag "E" è stato cancellato da entrambi i nodi per indicare che questi non applicano più la moratoria e che si trovano adesso in modalità di funzionamento normale.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>Conclusioni</title>
   <para>
    Dopo aver completato tutti i passaggi precedenti, è possibile montare l'NFS esportato da uno dei due server NFS Ganesha ed eseguirvi le normali operazioni NFS.
   </para>
   <para>
    Nella configurazione di esempio si presuppone che se uno dei due server NFS diventa inattivo, verrà riavviato manualmente dall'utente entro 5 minuti. Dopo questo intervallo di tempo, il server di metadati potrebbe annullare la sessione messa in attesa dal client NFS Ganesha e tutto lo stato a questa associato. Se le capacità della sessione vengono annullate prima che il resto del cluster entri nella moratoria, i client del server potrebbero non essere in grado di recuperare tutto il relativo stato.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>ulteriori informazioni</title>

  <para>
   Ulteriori informazioni sono disponibili all'indirizzo <xref linkend="cha-ceph-nfsganesha"/>.
  </para>
 </sect1>
</chapter>
