<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_saltcluster.xml" version="5.0" xml:id="storage-salt-cluster">
 <title>Salt 集群管理</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>是</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  部署 Ceph 集群之后，您可能偶尔需要对其进行若干修改。这些修改包括添加或删除新的节点、磁盘或服务。本章介绍该如何完成这些管理任务。
 </para>
 <sect1 xml:id="salt-adding-nodes">
  <title>添加新的集群节点</title>

  <para>
   为集群添加新节点的过程与<xref linkend="ceph-install-saltstack"/>中所述的初始集群节点部署过程几乎完全相同。
  </para>

  <tip>
   <title>防止重新平衡</title>
   <para>
    将 OSD 添加到现有集群时请注意，集群将在此后的一段时间内进行重新平衡。为了最大限度地缩短重新平衡的时间，请一次即添加所有要添加的 OSD。
   </para>
   <para>
    另一种方法是在添加 OSD 之前，在 <filename>ceph.conf</filename> 文件中设置 <option>osd crush initial weight = 0</option> 选项：
   </para>
   <procedure>
    <step>
     <para>
      在 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 中添加 <option>osd crush initial weight = 0</option>。
     </para>
    </step>
    <step>
     <para>
      在 Salt Master 节点上创建新配置：
     </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>SALT_MASTER_NODE</replaceable>' state.apply ceph.configuration.create
</screen>
    </step>
    <step>
     <para>
      将新配置应用到目标 OSD Minion：
     </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>OSD_MINIONS</replaceable>' state.apply ceph.configuration
</screen>
    </step>
    <step>
     <para>
      添加新 OSD 之后，根据需要使用 <command>ceph osd crush reweight</command> 命令调整其权重。
     </para>
    </step>
   </procedure>
  </tip>

  <procedure>
   <step>
    <para>
     在新节点上安装 SUSE Linux Enterprise Server 15 SP1，并配置其网络设置，使它能够正确解析 Salt Master 主机名。确认其正确连接到公共网络和集群网络，并且为其正确配置了时间同步。然后安装 <systemitem>salt-minion</systemitem> 包：
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     如果 Salt Master 的主机名不是 <literal>salt</literal>，请编辑 <filename>/etc/salt/minion</filename> 并添加下面一行：
    </para>
<screen>master: <replaceable>DNS_name_of_your_salt_master</replaceable></screen>
    <para>
     如果您对上面提到的配置文件进行了任何更改，请重启动 <systemitem>salt.minion</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     在 Salt Master 上接受新节点的 Salt 密钥：
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept <replaceable>NEW_NODE_KEY</replaceable></screen>
   </step>
   <step>
    <para>
     确认 <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> 针对的是新 Salt Minion，以及/或者设置了正确的 DeepSea 粒度。有关更多详细信息，请参见<xref linkend="ds-minion-targeting-name"/>或<xref linkend="ds-depl-stages"/>。
    </para>
   </step>
   <step>
    <para>
     运行准备阶段。该阶段会同步扩展模块和粒度，以使新 Minion 可以提供 DeepSea 所需的所有信息。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <important>
     <title>可能需要重启动 DeepSea 阶段 0</title>
     <para>
      如果 Salt Master 在其内核更新之后进行了重引导，则您需要重启动 DeepSea 阶段 0。
     </para>
    </important>
   </step>
   <step>
    <para>
     运行发现阶段。该阶段将在 <filename>/srv/pillar/ceph/proposals</filename> 目录中写入新的文件项，您可在其中编辑相关的 .yml 文件：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     （可选）如果新添加的主机与现有命名模式不匹配，请更改 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。有关详细信息，请参见<xref linkend="policy-configuration"/>。
    </para>
   </step>
   <step>
    <para>
     运行配置阶段。该阶段会读取 <filename>/srv/pillar/ceph</filename> 下的所有内容，并相应地更新 Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     Pillar 用于存储可以使用以下命令访问的数据：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.items</screen>
    <tip>
     <title>修改 OSD 的布局</title>
     <para>
      如果您要修改默认的 OSD 布局并更改 DriveGroups 配置，请执行<xref linkend="ds-drive-groups"/>中所述的过程。
     </para>
    </tip>
   </step>
   <step>
    <para>
     配置和部署阶段包含新添加的节点：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-adding-services">
  <title>为节点添加新的角色</title>

  <para>
   您可通过 DeepSea 部署所有受支持的角色类型。有关受支持角色类型的更多信息以及匹配示例，请参见<xref linkend="policy-role-assignment"/>。
  </para>

  <para>
   要将新服务添加到现有节点，请执行下列步骤：
  </para>

  <procedure>
   <step>
    <para>
     修改 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>，以使现有主机与新角色匹配。有关详细信息，请参见<xref linkend="policy-configuration"/>。例如，如果您需要在 MON 节点上运行对象网关，命令行类似下方所示：
    </para>
<screen>role-rgw/xx/x/example.mon-1.sls</screen>
   </step>
   <step>
    <para>
     运行阶段 2 以更新 Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     运行阶段 3 以部署核心服务，或者运行阶段 4 以部署可选服务。同时运行这两个阶段也没有问题。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-removing">
  <title>删除和重新安装集群节点</title>

  <tip>
   <title>临时删除集群节点</title>
   <para>
    Salt Master 预期集群中的所有 Minion 都已启动且能够响应。如果某个 Minion 由于中断而不再能够响应，将会导致 Salt 基础架构（主要是 DeepSea 和 Ceph Dashboard）产生问题。
   </para>
   <para>
    修复该 Minion 之前，请暂时从 Salt Master 中删除其密钥：
   </para>
<screen>
<prompt>root@master # </prompt>salt-key -d <replaceable>MINION_HOST_NAME</replaceable>
</screen>
   <para>
    修复 Minion 之后，请再次将其密钥添加到 Salt Master 中：
   </para>
<screen>
<prompt>root@master # </prompt>salt-key -a <replaceable>MINION_HOST_NAME</replaceable>
</screen>
  </tip>

  <para>
   要从集群中删除角色，请编辑 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 并删除相应的行。然后按<xref linkend="ceph-install-stack"/>中所述运行阶段 2 和 5。
  </para>

  <note>
   <title>从集群中删除 OSD</title>
   <para>
    如果您需要从集群中删除特定 OSD 节点，请确保集群的可用磁盘空间多于要删除的磁盘空间。切记，删除 OSD 会导致整个集群进行重新平衡。
   </para>
   <para>
    运行阶段 5 以执行实际删除之前，始终应检查 DeepSea 将要删除的是哪些 OSD：
   </para>
<screen><prompt>root@master # </prompt>salt-run rescinded.ids</screen>
  </note>

  <para>
   从 Minion 中删除角色时，其目的是撤销与该角色相关的所有更改。对于大部分角色，要实现该任务都很简单，但可能会存在包依赖项问题。如果包被卸装，其依赖项并不会被卸装。
  </para>

  <para>
   删除的 OSD 会显示为空白驱动器。相关任务除了会擦除分区表外，还会重写文件系统的开头并删除备份分区。
  </para>

  <note>
   <title>将保留通过其他方法创建的分区</title>
   <para>
    先前通过其他方法（例如 <command>ceph-deploy</command>）配置的磁盘驱动器可能仍然包含分区。DeepSea 不会自动销毁这些分区。管理员必须手动回收这些驱动器。
   </para>
  </note>

  <example xml:id="ex-ds-rmnode">
   <title>从集群中删除 Salt Minion</title>
   <para>
    举例来说，如果您的存储 Minion 名为“data1.ceph”、“data2.ceph”...“data6.ceph”，则 <filename>policy.cfg</filename> 中的相关行类似下方所示：
   </para>
<screen>[...]
# Hardware Profile
role-storage/cluster/data*.sls
[...]</screen>
   <para>
    要删除 Salt Minion“data2.ceph”，请将这些行更改为：
   </para>
<screen>
[...]
# Hardware Profile
role-storage/cluster/data[1,3-6]*.sls
[...]</screen>
   <para>
    此外，请记得调整 drive_groups.yml 文件以匹配新目标。
   </para>
<screen>
    [...]
    drive_group_name:
      target: 'data[1,3-6]*'
    [...]</screen>
   <para>
    然后运行阶段 2，检查将要删除的是哪些 OSD，最后运行阶段 5 以完成该过程：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run rescinded.ids
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>

  <example xml:id="ex-ds-mignode">
   <title>迁移节点</title>
   <para>
    假设出现下面的情况：在全新安装集群期间，您（管理员）在等待网关硬件就绪时，将其中一个存储节点分配为独立的对象网关。现在，当网关的永久硬件就绪时，您就可以将所需角色最终指定给备用存储节点并删除网关角色。
   </para>
   <para>
    在针对新硬件运行阶段 0 和 1（请参见<xref linkend="ds-depl-stages"/>）之后，您将新网关命名为 <literal>rgw1</literal>。如果节点 <literal>data8</literal> 需要删除对象网关角色并添加存储角色，且当前的 <filename>policy.cfg</filename> 类似下方所示：
   </para>
<screen># Hardware Profile
role-storage/cluster/data[1-7]*.sls

# Roles
role-rgw/cluster/data8*.sls</screen>
   <para>
    则将它更改为：
   </para>
<screen># Hardware Profile
role-storage/cluster/data[1-8]*.sls

# Roles
role-rgw/cluster/rgw1*.sls</screen>
   <para>
    运行阶段 2 到 4，检查可能要删除的是哪些 OSD，然后运行阶段 5 以完成该过程。阶段 3 会将 <literal>data8</literal> 添加为存储节点。稍候片刻，<literal>data8</literal> 将同时具有两个角色。阶段 4 将会为 <literal>rgw1</literal> 添加对象网关角色，而阶段 5 将会从 <literal>data8</literal> 中删除对象网关角色：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
<prompt>root@master # </prompt>salt-run rescinded.ids
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5</screen>
  </example>
 </sect1>
 <sect1 xml:id="ds-mon">
  <title>重新部署 Monitor 节点</title>

  <para>
   一个或多个 Monitor 节点发生故障且不响应时，您需要将发生故障的 Monitor 从集群中删除，然后再添加回集群（如有需要）。
  </para>

  <important>
   <title>至少须有三个 Monitor 节点</title>
   <para>
    Monitor 节点的数量不能少于 3。如果某个 Monitor 节点发生故障，导致您的集群中只有两个 Monitor 节点，在重新部署发生故障的 Monitor 节点之前，您需要暂时将其 Monitor 角色指定给其他集群节点。重新部署发生故障的 Monitor 节点后，便可以卸装临时 Monitor 角色。
   </para>
   <para>
    有关向 Ceph 集群添加新节点/角色的详细信息，请参见<xref linkend="salt-adding-nodes"/>和<xref linkend="salt-adding-services"/>。
   </para>
   <para>
    有关删除集群节点的详细信息，请参见<xref linkend="salt-node-removing"/>。
   </para>
  </important>

  <para>
   Ceph 节点故障分为以下两种基本程度：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Salt Minion 主机发生硬件或 OS 级别损坏，无法响应 <command>salt '<replaceable>minion_name</replaceable>' test.ping</command> 调用。在此情况下，您需要按照<xref linkend="ceph-install-stack"/>中的相关说明，对服务器进行彻底的重新部署。
    </para>
   </listitem>
   <listitem>
    <para>
     Monitor 相关服务失败并拒绝恢复，但主机会响应 <command>salt '<replaceable>minion_name</replaceable>' test.ping</command> 调用。在此情况下，请执行以下步骤：
    </para>
   </listitem>
  </itemizedlist>

  <procedure>
   <step>
    <para>
     编辑 Salt Master 上的 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>，删除或更新发生故障的 Monitor 节点对应的行，使它们现在指向正常工作的 Monitor 节点。例如：
    </para>
<screen>
[...]
# MON
#role-mon/cluster/ses-example-failed1.sls
#role-mon/cluster/ses-example-failed2.sls
role-mon/cluster/ses-example-new1.sls
role-mon/cluster/ses-example-new2.sls
[...]
</screen>
   </step>
   <step>
    <para>
     运行 DeepSea 阶段 2 到 5 以应用这些更改：
    </para>
<screen>
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.2
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.4
<prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.5
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-node-add-disk">
  <title>向节点添加 OSD 磁盘</title>

  <para>
   要向现有 OSD 节点添加磁盘，请校验是否已删除并擦除磁盘上的所有分区。有关详细信息，请参见<xref linkend="ceph-install-stack"/>中的<xref linkend="deploy-wiping-disk"/>。相应地调整 <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename>（有关详细信息，请参见<xref linkend="ds-drive-groups"/>）。保存文件后，运行 DeepSea 的阶段 3：
  </para>

<screen><prompt>root@master # </prompt><command>deepsea</command> stage run ceph.stage.3</screen>
 </sect1>
 <sect1 xml:id="salt-removing-osd">
  <title>删除 OSD</title>

  <para>
   您可以通过运行以下命令从集群中删除 Ceph OSD：
  </para>

<screen><prompt>root@master # </prompt>salt-run osd.remove <replaceable>OSD_ID</replaceable></screen>

  <para>
   <replaceable>OSD_ID</replaceable> 必须是 OSD 的编号，不含 <literal>osd.</literal> 前缀。例如，对于 <literal>osd.3</literal>，仅使用数字 <literal>3</literal>。
  </para>

  <sect2 xml:id="osd-removal-multiple">
   <title>删除多个 OSD</title>
   <para>
    使用<xref linkend="salt-removing-osd"/>中所述的相同过程，但只需提供多个 OSD ID：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run osd.remove 2 6 11 15
Removing osd 2 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.2 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 6 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.6 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 11 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.11 is safe to destroy
Purging from the crushmap
Zapping the device


Removing osd 15 on host data1
Draining the OSD
Waiting for ceph to catch up.
osd.15 is safe to destroy
Purging from the crushmap
Zapping the device


2:
True
6:
True
11:
True
15:
True

</screen>
  </sect2>

  <sect2 xml:id="remove-all-osds-per-host">
   <title>删除主机上的所有 OSD</title>
   <para>
    要删除特定主机上的所有 OSD，请运行以下命令：
   </para>
<screen><prompt>root@master # </prompt>salt-run osd.remove <replaceable>OSD_HOST_NAME</replaceable></screen>
  </sect2>

  <sect2 xml:id="osd-forced-removal">
   <title>强制删除已中止的 OSD</title>
   <para>
    有时会出现无法正常删除 OSD 的情况（请参见<xref linkend="salt-removing-osd"/>）。例如，如果 OSD 或其日志、WAL 或 DB 损坏、I/O 操作挂起或 OSD 磁盘无法卸载，可能就会发生这种情况。
   </para>
<screen><prompt>root@master # </prompt>salt-run osd.remove <replaceable>OSD_ID</replaceable> force=True</screen>
   <tip>
    <title>挂起的装入</title>
    <para>
     如果正在删除的磁盘上仍装载着分区，命令将退出，并显示“卸载失败 - 请检查 <replaceable>DEVICE</replaceable> 上的进程”讯息。然后，您可以使用 <command>fuser -m <replaceable>DEVICE</replaceable></command> 列出访问文件系统的所有进程。如果 <command>fuser</command> 未返回任何内容，请尝试手动执行 <command>unmount <replaceable>DEVICE</replaceable></command>，然后查看 <command>dmesg</command> 或 <command>journalctl</command> 命令的输出。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="validate-osd-lvm">
   <title>验证 OSD LVM 元数据</title>
   <para>
    使用 <command>salt-run osd.remove <replaceable>ID</replaceable></command> 或通过其他 ceph 命令删除 OSD 之后，可能无法完全删除 LVM 元数据。这意味着如果您想要重新部署一个新 OSD，系统却会使用旧 LVM 元数据。
   </para>
   <procedure>
    <step>
     <para>
      首先检查 OSD 是否已删除：
     </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-volume lvm list</screen>
     <para>
      即使某个 OSD 已成功删除，也仍可能会列出。例如，如果您删除了 <literal>osd.2</literal>，输出内容将如下所示：
     </para>
<screen>
  ====== osd.2 =======

  [block] /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380

  block device /dev/ceph-a2189611-4380-46f7-b9a2-8b0080a1f9fd/osd-data-ddc508bc-6cee-4890-9a42-250e30a72380
  block uuid kH9aNy-vnCT-ExmQ-cAsI-H7Gw-LupE-cvSJO9
  cephx lockbox secret
  cluster fsid 6b6bbac4-eb11-45cc-b325-637e3ff9fa0c
  cluster name ceph
  crush device class None
  encrypted 0
  osd fsid aac51485-131c-442b-a243-47c9186067db
  osd id 2
  type block
  vdo 0
  devices /dev/sda
</screen>
     <para>
      在此示例中，您可以看到 <literal>osd.2</literal> 仍然位于 <filename>/dev/sda</filename> 中。
     </para>
    </step>
    <step>
     <para>
      验证 OSD 节点上的 LVM 元数据：
     </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-volume inventory</screen>
     <para>
      运行 <command>ceph-volume inventory</command> 命令生成的输出中将 <filename>/dev/sda</filename> 可用性标记为 <literal>False</literal>。例如：
     </para>
<screen>
  Device Path Size rotates available Model name
  /dev/sda 40.00 GB True False QEMU HARDDISK
  /dev/sdb 40.00 GB True False QEMU HARDDISK
  /dev/sdc 40.00 GB True False QEMU HARDDISK
  /dev/sdd 40.00 GB True False QEMU HARDDISK
  /dev/sde 40.00 GB True False QEMU HARDDISK
  /dev/sdf 40.00 GB True False QEMU HARDDISK
  /dev/vda 25.00 GB True False
</screen>
    </step>
    <step>
     <para>
      在 OSD 节点上运行以下命令，以完全删除 LVM 元数据：
     </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-volume lvm zap --osd-id <replaceable>ID</replaceable> --destroy </screen>
    </step>
    <step>
     <para>
      再次运行 <command>inventory</command> 命令，以确认 <filename>/dev/sda</filename> 可用性是否返回 <literal>True</literal>。例如：
     </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-volume inventory
Device Path Size rotates available Model name
/dev/sda 40.00 GB True True QEMU HARDDISK
/dev/sdb 40.00 GB True False QEMU HARDDISK
/dev/sdc 40.00 GB True False QEMU HARDDISK
/dev/sdd 40.00 GB True False QEMU HARDDISK
/dev/sde 40.00 GB True False QEMU HARDDISK
/dev/sdf 40.00 GB True False QEMU HARDDISK
/dev/vda 25.00 GB True False</screen>
     <para>
      LVM 元数据现已删除。现在可以安全地对设备执行 <command>dd</command> 命令。
     </para>
    </step>
    <step>
     <para>
      现在可以重新部署 OSD，而无需重引导 OSD 节点：
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ds-osd-replace">
  <title>更换 OSD 磁盘</title>

  <para>
   有多种原因会导致您可能需要更换 OSD 磁盘，例如：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     根据 SMART 信息，OSD 磁盘已发生故障或很快将发生故障，并且将不再能用来安全地存储数据。
    </para>
   </listitem>
   <listitem>
    <para>
     您需要升级 OSD 磁盘，例如增加其大小。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   这两种情况的更换过程相同，对于默认 CRUSH 索引和自定义 CRUSH 索引也同样适用。
  </para>

  <procedure>
   <step>
    <para>
     例如，假设其磁盘需要更换的 OSD 的 ID 为“5”。以下命令会在 CRUSH 索引中将其标记为 <emphasis role="bold">destroyed</emphasis>，但会保留其原始 ID：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run osd.replace 5
</screen>
    <tip>
     <title><command>osd.replace</command> 和 <command>osd.remove</command></title>
     <para>
      Salt 的 <command>osd.replace</command> 和 <command>osd.remove</command>（请参见<xref linkend="salt-removing-osd"/>）命令作用相同，只不过 <command>osd.replace</command> 会在 CRUSH 索引中将 OSD 保留为“destroyed”状态，而 <command>osd.remove</command> 会从 CRUSH 索引中删除所有痕迹。
     </para>
    </tip>
   </step>
   <step>
    <para>
     手动更换发生故障/升级的 OSD 驱动器。
    </para>
   </step>
   <step>
    <para>
     如果您要修改默认的 OSD 布局并更改 DriveGroups 配置，请执行<xref linkend="ds-drive-groups"/>中所述的过程。
    </para>
   </step>
   <step>
    <para>
     运行部署阶段 3，以部署更换的 OSD 磁盘：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds-osd-recover">
  <title>恢复重新安装的 OSD 节点</title>

  <para>
   如果您的某个 OSD 节点上的操作系统损坏且无法恢复，请执行以下步骤恢复该节点，并在集群数据保持不变的情况下重新部署该节点的 OSD 角色：
  </para>

  <procedure>
   <step>
    <para>
     在其 OS 已损坏的节点上重新安装基本 SUSE Linux Enterprise 操作系统。在 OSD 节点上安装 <package>salt-minion</package> 包，删除 Salt Master 上的旧 Salt Minion 密钥，并在 Salt Master 中注册新 Salt Minion 的密钥。有关初始部署的详细信息，请参见<xref linkend="ceph-install-stack"/>。
    </para>
   </step>
   <step>
    <para>
     不要运行整个阶段 0，而是运行以下部分：
    </para>
<screen>
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.sync
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.packages.common
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.mines
<prompt>root@master # </prompt>salt '<replaceable>osd_node</replaceable>' state.apply ceph.updates
</screen>
   </step>
   <step>
    <para>
     运行 DeepSea 阶段 1 到 5：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.5
</screen>
   </step>
   <step>
    <para>
     运行 DeepSea 阶段 0：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
</screen>
   </step>
   <step>
    <para>
     重引导相关的 OSD 节点。系统将重新发现并重新使用所有 OSD 磁盘。
    </para>
   </step>
   <step>
    <para>
     安装/运行 Prometheus 的节点导出程序：
    </para>
<screen><prompt>root@master # </prompt>salt '<replaceable>RECOVERED_MINION</replaceable>' \
 state.apply ceph.monitoring.prometheus.exporters.node_exporter</screen>
   </step>
   <step>
    <para>
     更新 Salt 粒度：
    </para>
<screen><prompt>root@master # </prompt>salt '<replaceable>RECOVERED_MINION</replaceable>' osd.retain</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="moving-saltmaster">
  <title>将管理节点移至新服务器</title>

  <para>
   如果您需要将管理节点主机更换为新主机，则需要移动 Salt Master 和 DeepSea 文件。使用您偏好的同步工具传输文件。在此过程中，我们使用的是 <command>rsync</command>，因为它是 SUSE Linux Enterprise Server 15 SP1 软件储存库中提供的标准工具。
  </para>

  <procedure>
   <step>
    <para>
     停止旧管理节点上的 <systemitem class="daemon">salt-master</systemitem> 和 <systemitem class="daemon">salt-minion</systemitem> 服务：
    </para>
<screen>
<prompt>root@master # </prompt>systemctl stop salt-master.service
<prompt>root@master # </prompt>systemctl stop salt-minion.service
</screen>
   </step>
   <step>
    <para>
     在新管理节点上配置 Salt，以使 Salt Master 与 Salt Minions 可以通讯。有关详细信息，请参见<xref linkend="ceph-install-stack"/>。
    </para>
    <tip>
     <title>转换 Salt Minion</title>
     <para>
      为便于将 Salt Minion 转换到新管理节点，请从每个 Salt Minion 中删除原来的 Salt Master 的公共密钥：
     </para>
<screen>
<prompt>root@minion &gt; </prompt>rm /etc/salt/pki/minion/minion_master.pub
<prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service
</screen>
    </tip>
   </step>
   <step>
    <para>
     确认是否已安装  <package>deepsea </package> 包，如果没有，则予以安装。
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     更改 <literal>role-master</literal> 一行以自定义 <filename>policy.cfg</filename> 文件。有关详细信息，请参见<xref linkend="policy-configuration"/>。
    </para>
   </step>
   <step>
    <para>
     将旧管理节点上的 <filename>/srv/pillar</filename> 和 <filename>/srv/salt</filename> 目录同步到新管理节点。
    </para>
    <tip>
     <title><command>rsync</command> 试运行和符号链接</title>
     <para>
      如果可能，请先通过试运行试验文件同步，以确定将传输的是哪些文件（<command>rsync</command> 的选项 <option>-n</option>）。此外，请包含符号链接（<command>rsync</command> 的选项 <option>-a</option>）。使用 <command>rsync</command> 的同步命令将如下所示：
     </para>
<screen><prompt>root@master # </prompt>rsync -avn /srv/pillar/ <replaceable>NEW-ADMIN-HOSTNAME:</replaceable>/srv/pillar</screen>
    </tip>
   </step>
   <step>
    <para>
     如果您对 <filename>/srv/pillar</filename> 和 <filename>/srv/salt</filename> 外部的文件（例如 <filename>/etc/salt/master</filename> 或 <filename>/etc/salt/master.d</filename>）进行过自定义更改，请一并同步这些文件。
    </para>
   </step>
   <step>
    <para>
     现在，您便可从新管理节点上运行 DeepSea 阶段。有关其详细说明，请参见<xref linkend="deepsea-description"/>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="salt-automated-installation">
  <title>通过 Salt 实现自动安装</title>

  <para>
   通过使用 Salt 反应器可让安装自动进行。对于虚拟环境或一致的硬件环境，此配置将允许创建具有指定行为的 Ceph 集群。
  </para>

  <warning>
   <para>
    Salt 无法根据反应器事件执行依赖项检查。存在可能使 Salt Master 过载而无法响应的风险。
   </para>
  </warning>

  <para>
   自动安装需要：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     正确创建的 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。
    </para>
   </listitem>
   <listitem>
    <para>
     准备好并已放入 <filename>/srv/pillar/ceph/stack</filename> 目录中的自定义配置。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   默认反应器配置只会运行阶段 0 和 1。如此不必等待后续阶段完成即可测试反应器。
  </para>

  <para>
   第一个 salt-minion 启动时，阶段 0 即会开始。使用锁定可防止有多个实例。所有 Minion 都完成阶段 0 后，阶段 1 将会开始。
  </para>

  <para>
   如果正确执行了操作，请编辑文件
  </para>

<screen>/etc/salt/master.d/reactor.conf</screen>

  <para>
   并将下面一行
  </para>

<screen>- /srv/salt/ceph/reactor/discovery.sls</screen>

  <para>
   替换成
  </para>

<screen>- /srv/salt/ceph/reactor/all_stages.sls</screen>

  <para>
   确认该行未被注释掉。
  </para>
 </sect1>
 <sect1 xml:id="deepsea-rolling-updates">
  <title>更新集群节点</title>

  <para>
   定期应用滚动更新，以使 Ceph 集群节点保持最新。
  </para>

  <sect2 xml:id="rolling-updates-repos">
   <title>软件储存库</title>
   <para>
    使用最新的软件包增补集群之前，请确认集群的所有节点均可访问相关的储存库。有关所需储存库的完整列表，请参见<xref linkend="upgrade-one-node-manual"/>。
   </para>
  </sect2>

  <sect2 xml:id="rolling-upgrades-staging">
   <title>储存库暂存</title>
   <para>
    如果您使用向集群节点提供软件储存库的暂存工具（例如 SUSE Manager、订阅管理工具或储存库镜像工具），请确认 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage 的“更新”储存库的阶段都是在同一时刻创建的。
   </para>
   <para>
    我们强烈建议使用暂存工具来应用<emphasis role="bold">增补程序级别为冻结/暂存</emphasis>的增补程序。这样可确保加入集群的新节点具有与已在集群中运行的节点相同的增补程序级别。通过这种方法，您无需向集群的所有节点都应用最新增补程序，新节点也能加入集群。
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-patch-or-dup">
   <title><command>zypper patch</command> 或 <command>zypper dup</command></title>
   <para>
    集群节点默认通过 <command>zypper dup</command> 命令升级。如果您更喜欢使用 <command>zypper patch</command> 命令来更新系统，请编辑 <filename>/srv/pillar/ceph/stack/global.yml</filename>，在其中添加下面一行：
   </para>
<screen>update_method_init: zypper-patch</screen>
  </sect2>

  <sect2 xml:id="rolling-updates-reboots">
   <title>集群节点重引导</title>
   <para>
    如果更新进程在更新期间升级了集群节点的内核，则您可以选择重引导节点。如果您要避免强制重引导节点（可能是所有节点）的可能性，请确认 Ceph 节点上已安装且正在运行最新的内核，或者如<xref linkend="ds-disable-reboots"/>中所述禁用节点自动重引导。
   </para>
  </sect2>

  <sect2>
   <title>Ceph 服务停机时间</title>
   <para>
    如<xref linkend="rolling-updates-reboots"/>中所述，集群节点可能会在更新期间重引导，具体视配置而定。如果对象网关、Samba 网关、NFS Ganesha 或 iSCSI 等服务存在单一故障点，客户端计算机可能会暂时与相应节点正在重引导的服务断开连接。
   </para>
  </sect2>

  <sect2 xml:id="rolling-updates-running">
   <title>运行更新</title>
   <para>
    要将集群的所有节点上的软件包更新到最新版本，请执行以下步骤：
   </para>
   <procedure>
    <step>
     <para>
      在 Salt Master 上更新 <package>deepsea </package>、 <package>salt-master</package>和 <package>salt-minion</package> 包，并重启动相关服务：
     </para>
<screen><prompt>root@master # </prompt>salt -I 'roles:master' state.apply ceph.updates.master</screen>
    </step>
    <step>
     <para>
      在集群的所有节点上更新并重启动 <package>salt-minion</package> 包：
     </para>
<screen><prompt>root@master # </prompt>salt -I 'cluster:ceph' state.apply ceph.updates.salt</screen>
    </step>
    <step>
     <para>
      在集群上更新所有其他软件包：
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-salt-cluster-reboot">
  <title>停止或重引导集群</title>

  <para>
   在某些情况下，可能需要停止或重引导整个集群。建议您仔细检查运行中服务的依赖项。下列步骤概要说明如何停止和启动集群：
  </para>

  <procedure>
   <step>
    <para>
     告知 Ceph 集群不要将 OSD 标记为 out：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd set noout</screen>
   </step>
   <step>
    <para>
     按下面的顺序停止守护进程和节点：
    </para>
    <orderedlist>
     <listitem>
      <para>
       存储客户端
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或对象网关
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     根据需要执行维护任务。
    </para>
   </step>
   <step>
    <para>
     以与关闭过程相反的顺序启动节点和服务器：
    </para>
    <orderedlist>
     <listitem>
      <para>
       Ceph Monitor
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       网关，例如 NFS Ganesha 或对象网关
      </para>
     </listitem>
     <listitem>
      <para>
       存储客户端
      </para>
     </listitem>
    </orderedlist>
   </step>
   <step>
    <para>
     删除 noout 标志：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> osd unset noout</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ds-custom-cephconf">
  <title>使用自定义设置调整 <filename>ceph.conf</filename></title>

  <para>
   如果您需要将自定义设置放入 <filename>ceph.conf</filename> 文件中，可通过修改 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d</filename> 目录中的配置文件来实现：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     global.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mon.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mgr.conf
    </para>
   </listitem>
   <listitem>
    <para>
     mds.conf
    </para>
   </listitem>
   <listitem>
    <para>
     osd.conf
    </para>
   </listitem>
   <listitem>
    <para>
     client.conf
    </para>
   </listitem>
   <listitem>
    <para>
     rgw.conf
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>独特的 <filename>rgw.conf</filename></title>
   <para>
    对象网关提供了很大的灵活性，而且具有 <filename>ceph.conf</filename> 的其他段落所没有的独特之处。所有其他 Ceph 组件都包含静态标题，例如 <literal>[mon]</literal> 或 <literal>[osd]</literal>。对象网关具有独特的标题，例如 <literal>[client.rgw.rgw1]</literal>。也就是说，<filename>rgw.conf</filename> 文件需要有标题项。有关示例，请参见
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw.conf</filename>
</screen>
   <para>
    或者
   </para>
<screen>
<filename>/srv/salt/ceph/configuration/files/rgw-ssl.conf</filename>
</screen>
  </note>

  <important>
   <title>运行阶段 3</title>
   <para>
    对上述配置文件进行自定义更改后，运行阶段 3 和 4 以将这些更改应用到集群节点：
   </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
  </important>

  <para>
   这些文件通过 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 模板文件加入，与 Ceph 配置文件接受的不同段落相对应。将配置片段放入正确的文件，可让 DeepSea 将其放入正确的段落。您不需要添加任何段落标题。
  </para>

  <tip>
   <para>
    要将任何配置选项仅应用于守护进程的特定实例，请添加标题，例如 <literal>[osd.1]</literal>。以下配置选项将只应用于 ID 为 1 的 OSD 守护进程。
   </para>
  </tip>

  <sect2>
   <title>覆盖默认值</title>
   <para>
    段落中位于后面的语句会重写前面的语句。因此，可以按照 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 模板中指定的内容来覆盖默认配置。例如，要关闭 cephx 身份验证，可将下面三行添加到 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 文件：
   </para>
<screen>auth cluster required = none
auth service required = none
auth client required = none</screen>
   <para>
    重新定义默认值时，与 Ceph 相关的工具（例如 <command>rados</command>）可能会发出警告，指出 <filename>ceph.conf.j2</filename> 中的特定值已在 <filename>global.conf</filename> 中重新定义。这些警告之所以出现，是因为在最终的 <filename>ceph.conf</filename> 中将一个参数指定了两次。
   </para>
   <para>
    要解决这种特定情况，请执行以下步骤：
   </para>
   <procedure>
    <step>
     <para>
      将当前目录切换到 <filename>/srv/salt/ceph/configuration/create</filename>：
     </para>
<screen>
<prompt>root@master # </prompt>cd /srv/salt/ceph/configuration/create
</screen>
    </step>
    <step>
     <para>
      将 <filename>default.sls</filename> 复制到 <filename>custom.sls</filename>：
     </para>
<screen>
<prompt>root@master # </prompt>cp default.sls custom.sls
</screen>
    </step>
    <step>
     <para>
      编辑 <filename>custom.sls</filename>，并将 <option>ceph.conf.j2</option> 更改为 <option>custom-ceph.conf.j2</option>。
     </para>
    </step>
    <step>
     <para>
      将当前目录切换到 <filename>/srv/salt/ceph/configuration/files</filename>：
     </para>
<screen>
<prompt>root@master # </prompt>cd /srv/salt/ceph/configuration/files
</screen>
    </step>
    <step>
     <para>
      将 <filename>ceph.conf.j2</filename> 复制到 <filename>custom-ceph.conf.j2</filename>：
     </para>
<screen>
<prompt>root@master # </prompt>cp ceph.conf.j2 custom-ceph.conf.j2
</screen>
    </step>
    <step>
     <para>
      编辑 <filename>custom-ceph.conf.j2</filename>，删除下面一行：
     </para>
<screen>
{% include "ceph/configuration/files/rbd.conf" %}
</screen>
     <para>
      编辑 <filename>global.yml</filename>，添加下面一行：
     </para>
<screen>
configuration_create: custom
</screen>
    </step>
    <step>
     <para>
      刷新 Pillar：
     </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> saltutil.pillar_refresh
</screen>
    </step>
    <step>
     <para>
      运行阶段 3：
     </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    </step>
   </procedure>
   <para>
    现在，每个值定义便应该只有一项了。要重新创建配置，请运行以下命令：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.configuration.create
</screen>
   <para>
    然后确认 <filename>/srv/salt/ceph/configuration/cache/ceph.conf</filename> 的内容。
   </para>
  </sect2>

  <sect2>
   <title>包括配置文件</title>
   <para>
    如果您需要应用许多自定义配置，请在自定义配置文件中使用以下 include 语句来让文件管理更轻松。下面是 <filename>osd.conf</filename> 文件的示例：
   </para>
<screen>[osd.1]
{% include "ceph/configuration/files/ceph.conf.d/osd1.conf" ignore missing %}
[osd.2]
{% include "ceph/configuration/files/ceph.conf.d/osd2.conf" ignore missing %}
[osd.3]
{% include "ceph/configuration/files/ceph.conf.d/osd3.conf" ignore missing %}
[osd.4]
{% include "ceph/configuration/files/ceph.conf.d/osd4.conf" ignore missing %}</screen>
   <para>
    在前面的示例中，<filename>osd1.conf</filename>、<filename>osd2.conf</filename>、<filename>osd3.conf</filename> 和 <filename>osd4.conf</filename> 文件包含特定于相关 OSD 的配置选项。
   </para>
   <tip>
    <title>运行时配置</title>
    <para>
     对 Ceph 配置文件所做的更改将在相关 Ceph 守护进程重启动之后生效。有关更改 Ceph 运行时配置的详细信息，请参见<xref linkend="ceph-config-runtime"/>。
    </para>
   </tip>
  </sect2>
 </sect1>
 <sect1 xml:id="admin-apparmor">
  <title>启用 AppArmor 配置</title>

  <para>
   AppArmor 是一套通过特定配置对程序进行限制的安全解决方案。有关详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sles-15/book_security/data/part_apparmor.html"/>。
  </para>

  <para>
   DeepSea 针对 AppArmor 配置提供了以下三种状态：“强制”、“控诉”和“禁用”。要激活某种 AppArmor 状态，请运行以下命令：
  </para>

<screen>
salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-<replaceable>STATE</replaceable>
</screen>

  <para>
   要将 AppArmor 配置置于“强制”状态，请运行以下命令：
  </para>

<screen>
<prompt>root@master # </prompt>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-enforce
</screen>

  <para>
   要将 AppArmor 配置置于“控诉”状态，请运行以下命令：
  </para>

<screen>
<prompt>root@master # </prompt>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-complain
</screen>

  <para>
   要禁用 AppArmor 配置，请运行以下命令：
  </para>

<screen>
<prompt>root@master # </prompt>salt -I "deepsea_minions:*" state.apply ceph.apparmor.default-disable
</screen>

  <tip>
   <title>启用 AppArmor 服务</title>
   <para>
    上面调用的三个命令都会确认 AppArmor 是否已安装，如果未安装，将会安装该软件，然后启动并启用相关的 <systemitem class="daemon">systemd</systemitem> 服务。如果 AppArmor 是通过另一种方式安装并启动/启用的，会导致在没有 DeepSea 配置的情况下运行，对此 DeepSea 将会向您发出警告。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="deactivate-tuned-profiles">
  <title>停用经调整的配置</title>

  <para>
   默认情况下，DeepSea 会使用 Ceph Monitor、Ceph Manager 和 Ceph OSD 上工作的经调整配置来部署 Ceph 集群。在某些情况下，您可能需要永久停用经调整的配置。要完成此操作，请在 <filename>/srv/pillar/ceph/stack/global.yml</filename> 中添加下面几行，然后重新运行阶段 3：
  </para>

<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>

<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
 </sect1>
 <sect1 xml:id="deepsea-ceph-purge">
  <title>删除整个 Ceph 集群</title>

  <para>
   <command>ceph.purge</command> 运行程序会删除整个 Ceph 集群。这样，您便可以在测试不同的设置时清理集群环境。<command>ceph.purge</command> 完成之后，Salt 集群将还原至 DeepSea 阶段 1 结束时的状态。然后，您便可以更改 <filename>policy.cfg</filename>（请参见<xref linkend="policy-configuration"/>），或对该设置继续执行 DeepSea 阶段 2。
  </para>

  <para>
   为防止意外删除，编制流程会检查是否解除了安全措施。您可以通过运行以下命令来解除安全措施并删除 Ceph 集群：
  </para>

<screen>
<prompt>root@master # </prompt>salt-run disengage.safety
<prompt>root@master # </prompt>salt-run state.orch ceph.purge
</screen>

  <tip>
   <title>禁止删除 Ceph 集群</title>
   <para>
    如果您要阻止任何人运行 <command>ceph.purge</command> 运行程序，请在 <filename>/srv/salt/ceph/purge</filename> 目录中创建名为 <filename>disabled.sls</filename> 的文件，然后在 <filename>/srv/pillar/ceph/stack/global.yml</filename> 文件中插入下面一行：
   </para>
<screen>purge_init: disabled</screen>
  </tip>

  <important>
   <title>撤消自定义角色</title>
   <para>
    如果您之前为 Ceph Dashboard 创建了自定义角色（有关详细信息，请参见<xref linkend="dashboard-adding-roles"/>和<xref linkend="dashboard-permissions"/>），在运行 <command>ceph.purge</command> 运行程序之前，您需要执行一些手动步骤将这些角色清除。例如，如果对象网关的自定义角色名为“us-east-1”，请执行以下步骤：
   </para>
<screen>
<prompt>root@master # </prompt>cd /srv/salt/ceph/rescind
<prompt>root@master # </prompt>rsync -a rgw/ us-east-1
<prompt>root@master # </prompt>sed -i 's!rgw!us-east-1!' us-east-1/*.sls
</screen>
  </important>
 </sect1>
</chapter>
