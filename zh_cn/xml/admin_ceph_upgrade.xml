<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>从旧版本升级</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>编辑</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  本章说明将 SUSE Enterprise Storage 5.5 升级到版本 6 的步骤。请注意，版本 5.5 是在版本 5 的基础上应用了所有最新增补程序后的版本。
 </para>
 <note>
  <title>不支持从更早的版本升级</title>
  <para>
   不支持从低于 5.5 的 SUSE Enterprise Storage 版本升级。您需要先升级到 SUSE Enterprise Storage 5.5 的最新版本，然后再按照本章中所述的步骤操作。
  </para>
 </note>
 <sect1 xml:id="upgrade-consider-points">
  <title>升级前需考虑的要点</title>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>阅读发行说明</emphasis> - 发行说明提供了有关自 SUSE Enterprise Storage 上一个版本发行后所进行的更改的其他信息。检查发行说明以了解：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       您的硬件是否有特殊的注意事项。
      </para>
     </listitem>
     <listitem>
      <para>
       所用的任何软件包是否已发生重大更改。
      </para>
     </listitem>
     <listitem>
      <para>
       是否需要对您的安装实施特殊预防措施。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     发行说明还提供未能及时编入手册中的信息。它们还包含有关已知问题的说明。
    </para>
    <para>
     安装包 <package>release-notes-ses</package>之后，可在本地的 <filename>/usr/share/doc/release-notes</filename> 目录中或 <link xlink:href="https://www.suse.com/releasenotes/"/> 网页上找到发行说明。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您先前是从版本 4 升级的，请确认升级到版本 5 的过程已成功完成：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       检查以下文件是否存在
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
      <para>
       从 SES 4 升级到 5 的过程中，engulf 进程会创建该文件。另外，会在以下文件中设置 <option>configuration_init: default-import</option> 选项
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <para>
       如果 <option>configuration_init</option> 仍设为 <option>default-import</option>，则表示集群是使用 <filename>ceph.conf.import</filename> 而非 DeepSea 的默认 <filename>ceph.conf</filename> 作为其配置文件的，后者是通过对以下目录中的文件进行编译而生成的
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       因此，您需要检查 <filename>ceph.conf.import</filename> 中是否有任何自定义配置，并根据需要将相应配置移到以下目录中的其中一个文件中
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       然后，从下面的文件中删除 <option>configuration_init: default-import</option> 一行
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <warning>
       <title>默认 DeepSea 配置</title>
       <para>
        如果您<emphasis role="bold">未</emphasis>合并 <filename>ceph.conf.import</filename> 中的配置，并删除了 <option>configuration_init: default-import</option> 选项，那么我们随 DeepSea 一并提供的任何默认配置设置（存储在 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 中）都不会应用到集群。
       </para>
      </warning>
     </listitem>
     <listitem>
      <para>
       检查集群是否使用了新存储桶类型“straw2”：
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep straw
</screen>
     </listitem>
     <listitem>
      <para>
       检查是否使用了 Ceph“jewel”配置：
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep profile
</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     如果使用的是旧 RBD 内核客户端（低于 SUSE Linux Enterprise Server 12 SP3），请参见<xref linkend="rbd-old-clients-map"/>。如果可能，建议升级旧 RBD 内核客户端。
    </para>
   </listitem>
   <listitem>
    <para>
     如果 openATTIC 位于管理节点上，当您升级该节点后，openATTIC 将不可用。在您使用 DeepSea 部署新 Ceph Dashboard 前，该仪表盘将不可用。
    </para>
   </listitem>
   <listitem>
    <para>
     升级集群可能需要花很长时间 - 所需时间大约为升级一台计算机的时间乘以集群节点数。
    </para>
   </listitem>
   <listitem>
    <para>
     如果某个节点运行的是先前的 SUSE Linux Enterprise Server 版本，则无法进行升级，需要将其重引导到新版本的安装程序。因此，该节点提供的服务将在一段时间内不可用。内核集群服务将仍然可用，例如，如果在升级期间有一个 MON 停用，至少还有两个 MON 处于活跃状态。但不幸的是，单一 iSCSI 网关这样的单一实例服务将不可用。
    </para>
   </listitem>
   <listitem>
    <para>
     某些类型的守护进程依赖于其他守护进程。例如，Ceph 对象网关依赖于 Ceph MON 和 Ceph OSD 守护进程。建议按以下顺序升级：
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       管理节点
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor/Ceph Manager
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       对象网关
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI 网关
      </para>
     </listitem>
     <listitem>
      <para>
       NFS Ganesha
      </para>
     </listitem>
     <listitem>
      <para>
       Samba 网关
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     如果您之前是在“控诉”或“强制”模式下使用 AppArmor 的，则需要在升级前设置 Salt Pillar 变量。由于 SUSE Linux Enterprise Server 15 SP1 默认随附了 AppArmor，因此 DeepSea 阶段 0 中已集成 AppArmor 管理。SUSE Enterprise Storage 6 默认会删除 AppArmor 和相关配置。如果您要保留在 SUSE Enterprise Storage 5.5 中配置的行为，请在开始升级前确认 <filename>/srv/pillar/ceph/stack/global.yml</filename> 文件中是否存在以下其中一行：
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     或者
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     自 SUSE Enterprise Storage 6 起，不再允许使用以数字开头的 MDS 名称，否则 MDS 守护进程将拒绝启动。您可以通过运行 <command>ceph fs status</command> 命令来检查守护进程是否使用了此类名称，也可以通过重启动 MDS，然后检查其日志中是否存在以下讯息来确认：
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     如果您看到上述讯息，则需要在尝试升级到 SUSE Enterprise Storage 6 前迁移 MDS 名称。DeepSea 提供了一种编制流程来自动完成此迁移。它会在以数字开头的 MDS 名称前加上“mds.”前缀：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <tip>
     <title>与 MDS 名称绑定的自定义配置</title>
     <para>
      如果您有与 MDS 名称绑定的配置设置，且 MDS 守护进程的名称以数字开头，请确认您的配置设置是否同样适用于新名称（带有“mds.”前缀）。假设 <filename>/etc/ceph/ceph.conf</filename> 文件中包含以下示例段落：
     </para>
<screen>
[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
mds cache memory limit = 1073741824
mds standby for name = 456-another-mds
</screen>
     <para>
      <command>ceph.mds.migrate-numerical-names</command> 协调程序会将 MDS 守护进程名称“123-my-mds”更改为“mds.123-my-mds”。您需要调整配置以反映新名称：
     </para>
<screen>
[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds
</screen>
    </tip>
    <para>
     如此将会在删除旧 MDS 守护进程前添加使用新名称的 MDS 守护进程。因此，短时间内 MDS 守护进程的数量会翻倍。系统将会有一个短时暂停来进行故障转移，然后客户端才能访问 CephFS。因此，请将迁移安排在预期只有少量或没有 CephFS 负载的时段进行。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-backup">
  <title>备份集群数据</title>

  <para>
   虽然创建集群配置和数据的备份并非强制性操作，但我们仍强烈建议备份重要的配置文件和集群数据。有关更多详细信息，请参见 <xref linkend="cha-deployment-backup"/>。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-ntp">
  <title>从 <systemitem class="daemon">ntpd</systemitem> 迁移到 <systemitem class="daemon">chronyd</systemitem></title>

  <para>
   SUSE Linux Enterprise Server 15 SP1 不再使用 <systemitem class="daemon">ntpd</systemitem> 来同步本地主机时间，而是使用 <systemitem class="daemon">chronyd</systemitem>。您需要迁移每个集群节点上的时间同步守护进程。您可以在升级集群<emphasis role="bold">前</emphasis>迁移到 <systemitem>chronyd</systemitem>，也可以先升级集群，然后再迁移到 <systemitem class="daemon">chronyd</systemitem>。<emphasis role="bold"/>
  </para>

  <procedure>
   <title>在升级集群<emphasis>前</emphasis>迁移到 <systemitem class="daemon">chronyd</systemitem> </title>
   <step>
    <para>
     安装 <package>chrony</package> 包：
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper install chrony</screen>
   </step>
   <step>
    <para>
     编辑 <systemitem class="daemon">chronyd</systemitem> 配置文件 <filename>/etc/chrony.conf</filename>，并添加在 <filename>/etc/ntp.conf</filename> 中的当前 <systemitem class="daemon">ntpd</systemitem> 配置中指定的 NTP 源。
    </para>
    <tip>
     <title>有关 <systemitem class="daemon">chronyd</systemitem> 配置的更多详细信息</title>
     <para>
      有关如何在 <systemitem class="daemon">chronyd</systemitem> 配置中添加时间源的更多详细信息，请参见 <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>。
     </para>
    </tip>
   </step>
   <step>
    <para>
     禁用并停止 <systemitem class="daemon">ntpd</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     启动并启用 <systemitem class="daemon">chronyd</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     确认 chrony 的状态：
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
  </procedure>

  <procedure>
   <title>在升级集群<emphasis>后</emphasis>迁移到 <systemitem class="daemon">chronyd</systemitem> </title>
   <step>
    <para>
     在集群升级期间，请添加以下软件储存库：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     将集群升级到版本 6。
    </para>
   </step>
   <step>
    <para>
     编辑 <systemitem class="daemon">chronyd</systemitem> 配置文件 <filename>/etc/chrony.conf</filename>，并添加在 <filename>/etc/ntp.conf</filename> 中的当前 <systemitem class="daemon">ntpd</systemitem> 配置中指定的 NTP 源。
    </para>
    <tip>
     <title>有关 <systemitem class="daemon">chronyd</systemitem> 配置的更多详细信息</title>
     <para>
      有关如何在 <systemitem class="daemon">chronyd</systemitem> 配置中添加时间源的更多详细信息，请参见 <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>。
     </para>
    </tip>
   </step>
   <step>
    <para>
     禁用并停止 <systemitem class="daemon">ntpd</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     启动并启用 <systemitem class="daemon">chronyd</systemitem> 服务：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     从 <systemitem class="daemon">ntpd</systemitem> 迁移到 <systemitem class="daemon">chronyd</systemitem>。
    </para>
   </step>
   <step>
    <para>
     确认 chrony 的状态：
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
   <step>
    <para>
     删除您在升级过程中为了在系统中保留 <systemitem class="daemon">ntpd</systemitem> 而添加的旧版软件储存库。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-prepare">
  <title>升级前对集群应用增补程序</title>

  <para>
   在升级前，需要对所有集群节点应用最新的增补程序。
  </para>

  <sect2 xml:id="upgrade-prepare-repos">
   <title>所需的软件储存库</title>
   <para>
    检查是否已在每个集群节点上配置所需储存库。要列出所有可用储存库，请运行以下命令
   </para>
<screen>
<prompt>root@minion &gt; </prompt>zypper lr
</screen>
   <para>
    SUSE Enterprise Storage 5.5 需要：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    SUSE Linux Enterprise Server 12 SP3 上运行的 SLE-HA 上的 NFS/SMB 网关需要：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-staging">
   <title>储存库暂存系统</title>
   <para>
    如果您在使用某个储存库暂存系统（SMT、RMT；或SUSE Manager），请为当前和新的 SUSE Enterprise Storage 版本创建新的冻结增补程序级别。
   </para>
   <para>
    有关详细信息，请参见：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-manager-3/index.html"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-patch">
   <title>对整个集群应用最新的增补程序</title>
   <procedure>
    <step>
     <para>
      对每个 Ceph 集群节点应用 SUSE Enterprise Storage 5.5 和 SUSE Linux Enterprise Server 12 SP3 的最新增补程序。确认已将正确的软件储存库连接到每个集群节点（请参见<xref linkend="upgrade-prepare-repos"/>），然后运行 DeepSea 阶段 0：
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      阶段 0 运行完成后，确认每个集群节点的状态是否包含“HEALTH_OK”。如果未包含，请先解决问题，然后再进行后续步骤中可能的重引导。
     </para>
    </step>
    <step>
     <para>
      运行 <command>zypper ps</command> 以检查是否存在可能使用过时的库或二进制运行的进程，如果存在，则进行重引导。
     </para>
    </step>
    <step>
     <para>
      确认运行中的内核是否为最新版本，如果不是，则进行重引导。检查以下命令的输出：
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>uname -a
<prompt>cephadm@adm &gt; </prompt>rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      确认是否已安装  <package>ceph </package> 包为 12.2.12 或更新版本。确认是否已安装  <package>deepsea </package> 包为 0.8.9 或更新版本。
     </para>
    </step>
    <step>
     <para>
      如果您之前使用了任何 <option>bluestore_cache</option> 设置，从 <package>ceph</package>
      12.2.10 版本开始，这些设置将不再有效。新设置 <option>bluestore_cache_autotune</option>（默认设为“true”）会禁止手动调整缓存大小。要启用旧行为，您需要设置 <option>bluestore_cache_autotune=false</option>。有关详细信息，请参见<xref linkend="config-auto-cache-sizing"/>。
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-verify-current">
  <title>确认当前环境</title>

  <itemizedlist>
   <listitem>
    <para>
     如果系统存在明显问题，请修复后再开始升级。升级过程并不会修复现有的系统问题。
    </para>
   </listitem>
   <listitem>
    <para>
     检查集群性能。您可以使用 <command>rados bench</command>、<command>ceph tell osd.* bench</command> 或 <command>iperf3</command> 等命令。
    </para>
   </listitem>
   <listitem>
    <para>
     确认对网关（如 iSCSI 网关或对象网关）和 RADOS 块设备的访问权限。
    </para>
   </listitem>
   <listitem>
    <para>
     记录系统设置的特定部分，如网络设置、分区或安装详细信息。
    </para>
   </listitem>
   <listitem>
    <para>
     使用 <command>supportconfig</command> 收集重要的系统信息，并将其保存在集群节点之外。有关详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_admsupport_supportconfig.html"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     确保每个集群节点上都有充足的可用磁盘空间。请使用 <command>df -h</command> 检查可用磁盘空间。必要时，请通过删除不需要的文件/目录或过时的 OS 快照来释放磁盘空间。如果没有足够的可用磁盘空间，在释放出充足的磁盘空间前，请不要继续升级。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-verify-state">
  <title>检查集群的状态</title>

  <itemizedlist>
   <listitem>
    <para>
     开始升级过程前，请使用 <command>cluster health</command> 命令检查集群健康状况。除非所有集群节点都报告“HEALTH_OK”，否则请不要开始升级。
    </para>
   </listitem>
   <listitem>
    <para>
     确认所有服务都在运行：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Salt Master 和 Salt Master 守护进程。
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph Monitor 和 Ceph Manager 守护进程。
      </para>
     </listitem>
     <listitem>
      <para>
       元数据服务器守护进程。
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD 守护进程。
      </para>
     </listitem>
     <listitem>
      <para>
       对象网关守护进程。
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI 网关守护进程。
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   以下命令提供集群状态以及特定配置的详细信息：
  </para>

  <variablelist>
   <varlistentry>
    <term><command>ceph -s</command></term>
    <listitem>
     <para>
      列显 Ceph 集群健康状况、正在运行的服务、数据使用率以及 I/O 统计数据的简短摘要。请在开始升级前确认该命令是否报告了“HEALTH_OK”。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph health detail</command></term>
    <listitem>
     <para>
      如果 Ceph 集群健康状况并非报告为 OK，该命令可列显详细信息。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph versions</command></term>
    <listitem>
     <para>
      列显正在运行的 Ceph 守护进程版本。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph df</command></term>
    <listitem>
     <para>
      列显集群上的总磁盘空间和可用磁盘空间。如果集群的可用磁盘空间不到总磁盘空间的 25%，请不要开始升级。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>salt '*' cephprocesses.check results=true</command></term>
    <listitem>
     <para>
      列显正在运行的 Ceph 进程及其 PID（按 Salt Minion 排序）。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph osd dump | grep ^flags</command></term>
    <listitem>
     <para>
      确认是否存在“recovery_deletes”和“purged_snapdirs”标志。如果不存在，您可以通过运行以下命令强制对所有归置组执行洗刷。请注意，此强制洗刷可能会对您 Ceph 客户端的性能造成负面影响。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>CTDB 集群的脱机升级</title>

  <para>
   CTDB 提供 Samba 网关使用的集群化数据库。CTDB 协议非常简单，不支持节点之间使用不同协议版本进行通讯的集群。因此需要先将 CTDB 节点脱机才能执行升级。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-one-node">
  <title>按节点升级 - 基本过程</title>

  <para>
   为了确保内核集群服务在升级期间可用，您需要按顺序逐一升级集群节点。您可以使用两种方式执行节点升级：使用<emphasis>安装程序 DVD</emphasis>，或者使用<emphasis>发行套件迁移系统</emphasis>。
  </para>

  <para>
   升级每个节点后，我们建议运行 <command>rpmconfigcheck</command> 命令来检查是否存在任何在本地编辑过的已更新配置文件。如果命令返回后缀为 <filename>.rpmnew</filename>、<filename>.rpmorig</filename> 或 <filename>.rpmsave</filename> 的文件名列表，请将这些文件与当前配置文件进行比较，以确保没有丢失任何本地更改。如果需要，请更新受影响的文件。有关处理 <filename>.rpmnew</filename>、<filename>.rpmorig</filename> 和 <filename>.rpmsave</filename> 文件的详细信息，请参见 <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-sw-cl.html#sec-rpm-packages-manage"/>。
  </para>

  <tip>
   <title>孤立包</title>
   <para>
    升级一个节点后，有一些包将处于“孤立”状态，没有父储存库。这是因为与 python3 相关的包不会废弃 python2 包。
   </para>
   <para>
    有关列出孤立包的详细信息，请参见 <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_zypper.html#sec_zypper_softup_orphaned"/>。
   </para>
  </tip>

  <sect2 xml:id="upgrade-one-node-manual">
   <title>使用安装程序 DVD 手动升级节点</title>
   <procedure>
    <step>
     <para>
      从 SUSE Linux Enterprise Server 15 SP1 安装程序 DVD/映像重引导节点。
     </para>
    </step>
    <step>
     <para>
      从引导菜单中选择<guimenu>升级</guimenu>。
     </para>
    </step>
    <step>
     <para>
      在<guimenu>选择迁移目标</guimenu>屏幕上，确认选择了“SUSE Linux Enterprise Server 15 SP1”，然后激活<guimenu>手动调整用于迁移的储存库</guimenu>复选框。
     </para>
     <figure>
      <title>选择迁移目标</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      选择以下扩展模块加以安装：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SUSE Enterprise Storage 6 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Basesystem Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Desktop Applications Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Legacy Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Server Applications Module 15 SP1 x86_64
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      在<guimenu>以前使用过的储存库</guimenu>屏幕上，确认选择了正确的储存库。如果未在 SCC/SMT 中注册系统，您需要手动添加储存库。
     </para>
     <para>
      SUSE Enterprise Storage 6 需要：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      如果您要在迁移 SES 后将 <systemitem>ntpd</systemitem> 迁移到 <systemitem class="daemon">chronyd</systemitem>（请参见<xref linkend="upgrade-ntp"/>），请包含以下储存库：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      SUSE Linux Enterprise Server 15 SP1 上运行的 SLE-HA 上的 NFS/SMB 网关需要：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      检查<guimenu>安装设置</guimenu>，然后点击<guimenu>更新</guimenu>开始安装过程。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-one-node-auto">
   <title>使用 SUSE 发行套件迁移系统升级节点</title>
   <para>
    <emphasis>发行套件迁移系统</emphasis> (DMS) 提供将安装的 SUSE Linux Enterprise 系统从一个主要版本升级到另一个主要版本的升级路径。以下过程使用 DMS 将 SUSE Enterprise Storage 5.5 升级到版本 6，其中包括从基础 SUSE Linux Enterprise Server 12 SP3 迁移到 SUSE Linux Enterprise Server 15 SP1。
   </para>
   <para>
    有关 DMS 的一般和详细信息，请参见 <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>。
   </para>
   <procedure>
    <step>
     <para>
      安装迁移 RPM 包。它们会调整 GRUB 引导加载程序，以在下次重引导时自动触发升级。安装
      <package>SLES15-SES-Migration</package> 和
      <package>suse-migration-sle15-activation</package> 包：
     </para>
<screen><prompt>root@minion &gt; </prompt>zypper install SLES15-SES-Migration suse-migration-sle15-activation</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        如果<emphasis role="bold">已</emphasis>在储存库暂存系统（如 SCC、SMT、RMT 或 SUSE Manager）中注册要升级的节点，请创建包含以下内容的 <filename>/etc/sle-migration-service.yml</filename> 文件：
       </para>
<screen>
use_zypper_migration: true
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
      </step>
      <step>
       <para>
        如果<emphasis role="bold">未</emphasis>在储存库暂存系统（如 SCC、SMT、RMT 或 SUSE Manager）中注册要升级的节点，请进行以下更改：
       </para>
       <substeps>
        <step>
         <para>
          创建包含以下内容的 <filename>/etc/sle-migration-service.yml</filename> 文件：
         </para>
<screen>
use_zypper_migration: false
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
        </step>
        <step>
         <para>
          禁用或删除 SLE 12 SP3 和 SES 5 储存库，并添加 SLE 15 SP1 和 SES 6 储存库。<xref linkend="upgrade-prepare-repos"/>中列出了相关的储存库。
         </para>
        </step>
       </substeps>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      重引导以开始升级。当升级运行期间，您可以从主机系统上使用现有 SSH 密钥通过 <command>ssh</command> 以迁移用户身份登录已升级节点（如 <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/> 中所述）。对于 SUSE Enterprise Storage，如果您能够亲身访问计算机或可直接通过控制台访问，则还可以在系统控制台上使用密码 <literal>sesupgrade</literal> 以 <systemitem class="username">root</systemitem> 身份登录。节点将在升级后自动重引导。
     </para>
     <tip>
      <title>升级失败</title>
      <para>
       如果升级失败，请检查 <filename>/var/log/distro_migration.log</filename>。修复问题，重新安装迁移 RPM 包，然后重引导节点。
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-adm">
  <title>升级管理节点</title>

  <itemizedlist>
   <listitem>
    <para>
     虽然 Salt Minion 运行的是旧版 Ceph 和 Salt，但下列命令仍有效：<command>salt '*' test.ping</command> 和 <command>ceph status</command>
    </para>
   </listitem>
   <listitem>
    <para>
     升级管理节点后，将不再安装 openATTIC。
    </para>
   </listitem>
   <listitem>
    <para>
     如果管理节点之前托管着 SMT，请将其迁移到 RMT（请参见 <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_migrate.html"/>）。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>集群节点的状态</title>
   <para>
    升级管理节点后，您可以运行 <command>salt-run upgrade.status</command> 命令来查看有关集群节点的有用信息。该命令会列出所有节点的 Ceph 和 OS 版本，并会建议应按何顺序升级仍在运行旧版本的所有节点。
   </para>
<screen><prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
  ceph: ceph version 14.2.1-468-g994fd9e0cc (994fd9e0ccc50c2f3a55a3b7a3d4e0ba74786d50) nautilus (stable)
  os: SUSE Linux Enterprise Server 15 SP1

Nodes running these software versions:
  admin.ceph (assigned roles: master)
  mon2.ceph (assigned roles: admin, mon, mgr)

Nodes running older software versions must be upgraded in the following order:
   1: mon1.ceph (assigned roles: admin, mon, mgr)
   2: mon3.ceph (assigned roles: admin, mon, mgr)
   3: data1.ceph (assigned roles: storage)
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="upgrade-mons">
  <title>升级 Ceph Monitor/Ceph Manager 节点</title>

  <itemizedlist>
   <listitem>
    <para>
     如果您的集群<emphasis role="bold">未使用</emphasis> MDS 角色，请逐一升级 MON/MGR 节点。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您的集群<emphasis role="bold">使用</emphasis> MDS 角色，且 MON/MGR 与 MDS 角色共置，您需要收缩 MDS 集群，然后升级共置的节点。有关更多详细信息，请参见 <xref linkend="upgrade-mds"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您的集群<emphasis role="bold">使用</emphasis> MDS 角色，且它们在<emphasis role="bold">专用</emphasis>服务器上运行，请逐一升级所有 MON/MGR 节点，然后收缩 MDS 集群并进行升级。有关更多详细信息，请参见 <xref linkend="upgrade-mds"/>。
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Ceph Monitor 升级</title>
   <para>
    由于 Ceph Monitor 设计存在局限性，一旦有两个 MON 已升级到 SUSE Enterprise Storage 6 且构成了仲裁，那么第三个 MON（仍在运行 SUSE Enterprise Storage 5.5）出于某种原因重启动（包括节点重引导）后将不会重新加入 MON 集群。因此，当有两个 MON 升级后，最好尽快升级其余 MON。
   </para>
  </note>

  <para>
   <emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
  </para>
 </sect1>
 <sect1 xml:id="upgrade-mds">
  <title>升级元数据服务器</title>

  <para>
   您需要收缩元数据服务器 (MDS) 集群。由于 SUSE Enterprise Storage 版本 5.5 与 6 之间的功能不兼容，较旧版本的 MDS 守护进程一旦发现有 SES 6 级别的 MDS 加入集群，他们即会关停。因此，在 MDS 节点升级期间，必须将 MDS 集群收缩至只有一个主动 MDS（没有待机 MDS）。升级第二个节点后，就可以再次扩充 MDS 集群。
  </para>

  <tip>
   <para>
    在负载过重的 MDS 集群上，您可能需要降低负载（例如通过停止客户端），以便单个主动 MDS 就足以处理工作负载。
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     记下 <option>max_mds</option> 选项的当前值：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     如果您有多个主动 MDS 守护进程（即 <option>max_mds</option> &gt; 1），请收缩 MDS 集群。要收缩 MDS 集群，请运行以下命令
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     其中，<replaceable>FS_NAME</replaceable> 是您 CephFS 实例的名称（默认为“cephfs”）。
    </para>
   </step>
   <step>
    <para>
     找到托管某个待机 MDS 守护进程的节点。查看 <command>ceph fs status</command> 命令的输出，然后开始升级此节点上的 MDS 集群。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     在此示例中，您需要在节点“mon3-6”或“mon2-6”上开始升级过程。
    </para>
   </step>
   <step>
    <para>
     升级具有待机 MDS 守护进程的节点。经过升级的 MDS 节点启动后，过时的 MDS 守护进程将自动关停。此时，客户端可能会经历一段短暂的 CephFS 服务停机时间。
    </para>
    <para>
     <emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
    </para>
   </step>
   <step>
    <para>
     升级其余 MDS 节点。
    </para>
   </step>
   <step>
    <para>
     将 <option>max_mds</option> 重设置为所需配置：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-main-osd">
  <title>升级 Ceph OSD</title>

  <para>
   针对每个存储节点执行以下步骤：
  </para>

  <procedure>
   <step>
    <para>
     识别特定节点上正在运行的 OSD 守护进程：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd tree
</screen>
   </step>
   <step>
    <para>
     为要升级的节点上的每个 OSD 守护进程设置“noout”标志：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</screen>
    <para>
     使用以下命令进行验证：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     或者
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</screen>
   </step>
   <step>
    <para>
     通过在待升级节点上运行以下命令，为所有现有 OSD 创建 <filename>/etc/ceph/osd/*.json</filename> 文件：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     升级 OSD 节点。<emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
    </para>
   </step>
   <step>
    <para>
     激活系统中发现的所有 OSD：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>;ceph-volume simple activate --all
</screen>
    <tip>
     <title>分别激活各数据分区</title>
     <para>
      如果您要分别激活各数据分区，则需要知道用于激活相应分区的正确 <command>ceph-volume</command> 命令。请将 <replaceable>X1</replaceable> 替换为分区的正确盘符/编号：
     </para>
<screen>
 <prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      例如：
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      输出的最后一行包含用于激活该分区的命令：
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--&gt; All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--&gt; Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     确认 OSD 节点在重引导后是否将会正常启动。
    </para>
   </step>
   <step>
    <para>
     解决“Legacy BlueStore stats reporting detected on XX OSD(s)”讯息：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
    <para>
     在将 Ceph 升级到 14.2.2 时通常会出现该警告。您可以通过以下设置来禁用它：
    </para>
<screen>bluestore_warn_on_legacy_statfs = false</screen>
    <para>
     正确的修复方法是在所有由于该问题而停止的 OSD 上运行以下命令：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</screen>
    <para>
     下面的帮助程序脚本将会对 <replaceable>NODE_NAME</replaceable> 节点上的所有 OSD 运行 <command>ceph-bluestore-tool repair</command> 命令：
    </para>
<screen>OSDNODE=<replaceable>OSD_NODE_NAME</replaceable>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE cmd.run 'systemctl stop ceph-osd@$OSD';\
 salt $OSDNODE cmd.run 'ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD';\
 salt $OSDNODE cmd.run 'systemctl start ceph-osd@$OSD';\
 done</screen>
   </step>
   <step>
    <para>
     对已升级节点上的每个 OSD 守护进程取消设置“noout”标志：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</screen>
    <para>
     使用以下命令进行验证：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     注意：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
   </step>
   <step>
    <para>
     确认集群状态。其输出将如下所示：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
   </step>
   <step>
    <para>
     确认所有 OSD 节点均已重引导，且重引导后 OSD 已自动启动。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="filestore2bluestore">
  <title>将 OSD 迁移到 BlueStore</title>

  <para>
   OSD BlueStore 是 OSD 守护进程的新后端。从 SUSE Enterprise Storage 5 开始，它是默认的选项。与以文件形式将对象存储在 XFS 文件系统中的 FileStore 相比，BlueStore 可提供更高的性能，因为它直接将对象存储在底层块设备中。BlueStore 还能实现 FileStore 所不能提供的其他功能，例如内置压缩和 EC 重写。
  </para>

  <para>
   具体而言，在 BlueStore 中，OSD 包含一个“wal”（预写式日志）设备和一个“db”（RocksDB 数据库）设备。RocksDB 数据库会保存 BlueStore OSD 的元数据。默认情况下，这两个设备将驻留在 OSD 所在的同一台设备上，但也可以将它们放置在其他（例如速度更快的）媒体上。
  </para>

  <para>
   在 SUSE Enterprise Storage 5 中，FileStore 和 BlueStore 均受支持，FileStore 和 BlueStore OSD 可在单个集群中共存。在 SUSE Enterprise Storage 升级过程中，FileStore OSD 不会自动转换到 BlueStore。请注意，在尚未迁移到 BlueStore 的 OSD 上，将无法使用特定于 BlueStore 的功能。
  </para>

  <para>
   在转换到 BlueStore 之前，OSD 需要运行 SUSE Enterprise Storage 5。转换是个缓慢的过程，因为所有数据需要重新写入两次。尽管迁移过程可能需要很长时间才能完成，但在此期间，集群的工作不会中断，所有客户端都可继续访问集群。但是，迁移期间的性能势必会下降，原因是需要重新平衡和回填集群数据。
  </para>

  <para>
   请执行以下过程将 FileStore OSD 迁移到 BlueStore：
  </para>

  <tip>
   <title>关闭安全措施</title>
   <para>
    运行迁移所需的 Salt 命令会被安全措施阻止。要关闭这些预防措施，请运行下面的命令：
   </para>
<screen>
 <prompt>root@master # </prompt>salt-run disengage.safety
 </screen>
   <para>
    在继续操作前先重构建节点：
   </para>
<screen>
 <prompt>root@master # </prompt> salt-run rebuild.node <replaceable>TARGET</replaceable>
 </screen>
   <para>
    您也可以选择单独重构建每个节点。例如：
   </para>
<screen>
<prompt>root@master # </prompt> salt-run rebuild.node data1.ceph
 </screen>
   <para>
    <literal>rebuild.node</literal> 始终会先删除节点上的所有 OSD，然后再重新创建这些 OSD。
   </para>
   <important>
    <para>
     如果一个 OSD 无法转换，重新运行重构建会销毁已转换的 BlueStore OSD。如果不想重新运行重构建，您可以运行以下命令：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.deploy <replaceable>TARGET</replaceable>
 </screen>
   </important>
  </tip>

  <para>
   迁移到 BlueStore 之后，对象计数将保持不变，磁盘使用率也几乎相同。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-appnodes-order">
  <title>升级应用节点</title>

  <para>
   请按以下顺序升级应用节点：
  </para>

  <orderedlist>
   <listitem>
    <para>
     对象网关
    </para>
    <itemizedlist>
     <listitem>
      <para>
       如果对象网关基于负载平衡器，便可以在服务不中断的情况下进行对象网关的滚动升级。
      </para>
     </listitem>
     <listitem>
      <para>
       每次升级后，验证对象网关守护进程是否正在运行，并使用 S3/Swift 客户端进行测试。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     iSCSI 网关
    </para>
    <itemizedlist>
     <listitem>
      <para>
       如果为 iSCSI 授权人配置了多路径，便可以在服务不中断的情况下进行 iSCSI 网关的滚动升级。
      </para>
     </listitem>
     <listitem>
      <para>
       每次升级后，验证 <systemitem class="daemon">lrbd</systemitem> 守护进程是否正在运行，并使用授权人进行测试。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     NFS Ganesha。<emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     Samba 网关。<emphasis role="bold">按照<xref linkend="upgrade-one-node"/>中所述的过程操作。</emphasis>
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="upgrade-main-policy">
  <title>更新 <filename>policy.cfg</filename> 并使用 DeepSea 部署 Ceph Dashboard</title>

  <para>
   在管理节点上编辑<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>，并应用以下更改：
  </para>

  <important>
   <title>无新服务</title>
   <para>
    在集群升级期间，请勿向 <filename>policy.cfg</filename> 文件添加新服务。请仅在升级完成后再更改集群体系结构。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     删除 <literal>role-openattic</literal>。
    </para>
   </step>
   <step>
    <para>
     将 <literal>role-prometheus</literal> 和 <literal>role-grafana</literal> 添加到安装了 Prometheus 和 Grafana 的节点（通常是管理节点）。
    </para>
   </step>
   <step>
    <para>
     现在，系统会忽略 <literal>profile-<replaceable>PROFILE_NAME</replaceable></literal> 角色。添加新的相应角色，即 <literal>role-storage</literal> 一行。例如，针对现有
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     添加
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     同步所有 Salt 扩展模块：
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     运行 DeepSea 阶段 1 和阶段 2 来更新 Salt Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     清理 openATTIC：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
<prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic</screen>
   </step>
   <step>
    <para>
     取消设置“restart_igw”粒度，以防阶段 0 重启动尚未安装的 iSCSI 网关：
    </para>
<screen>Salt mastersalt '*' grains.delkey restart_igw</screen>
   </step>
   <step>
    <para>
     最后，运行完整的 DeepSea 阶段 0-4：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <tip>
     <title>阶段 3 运行期间发生“subvolume missing”错误</title>
     <para>
      DeepSea 阶段 3 可能会失败，显示如下错误：
     </para>
<screen>subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</screen>
     <para>
      在此情况下，您需要编辑 <filename role="bold">/srv/pillar/ceph/stack/global.yml</filename>，并添加下面一行：
     </para>
<screen>subvolume_init: disabled</screen>
     <para>
      然后刷新 Salt Pillar 并重新运行 DeepSea stage.3：
     </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar
 <prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     <para>
      DeepSea 成功完成 stage.3 后，Ceph Dashboard 即会运行。有关 Ceph Dashboard 功能的详细概述，请参见<xref linkend="ceph-dashboard"/>。
     </para>
     <para>
      要列出运行仪表盘的节点，请运行以下命令：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mgr services | grep dashboard</screen>
     <para>
      要列出管理员身份凭证，请运行以下命令：
     </para>
<screen><prompt>root@master # </prompt>salt-call grains.get dashboard_creds</screen>
    </tip>
   </step>
   <step>
    <para>
     按顺序重启动对象网关服务，以使用“beast”Web 服务器取代过时的“civetweb”：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.rgw.force</screen>
   </step>
   <step>
    <para>
     继续之前，我们强烈建议先启用 Ceph 遥测扩展模块。有关详细信息和说明，请参见<xref linkend="mgr-modules-telemetry"/>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-drive-groups">
  <title>从基于配置的部署迁移到 DriveGroups</title>

  <para>
   在 SUSE Enterprise Storage 5.5 中，DeepSea 提供了一个称为“配置”的方法来描述您的 OSD 布局。自 SUSE Enterprise Storage 6 起，我们改用另一种称为 <emphasis>DriveGroups</emphasis> 的方法（有关更多详细信息，请参见<xref linkend="ds-drive-groups"/>）。
  </para>

  <note>
   <para>
    您不必立即就迁移到新方法。如 <command>salt-run osd.remove</command>、<command>salt-run osd.replace</command> 或 <command>salt-run osd.purge</command> 之类的破坏性操作仍然可用。但如果要添加新 OSD，便需要迁移到新方法。
   </para>
  </note>

  <para>
   由于这些实施的方法不同，我们未提供自动迁移路径。不过，我们提供了各种工具（Salt 运行程序）来尽可能使迁移变得简单。
  </para>

  <sect2>
   <title>分析当前布局</title>
   <para>
    要查看有关当前部署的 OSD 的信息，请使用以下命令：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.discover
</screen>
   <para>
    另外，您可以检查 <filename>/srv/pillar/ceph/proposals/profile-*/</filename> 目录中的文件内容。其结构如下所示：
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>创建与当前布局匹配的 DriveGroups</title>
   <para>
    有关 DriveGroups 规范的更多详细信息，请参见<xref linkend="ds-drive-groups-specs"/>。
   </para>
   <para>
    全新部署与升级场景的不同之处在于要迁移的驱动器已“使用过”。由于
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
   <para>
    仅会查找未使用的磁盘，因此请使用以下命令
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list include_unavailable=True
</screen>
   <para>
    调整 DriveGroups，直至其与您当前的设置匹配。如需更直观地了解将要发生的情况，请使用以下命令。注意，如果没有可用磁盘，该命令将不会生成任何输出：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report bypass_pillar=True
</screen>
   <para>
    如果您确认 DriveGroups 配置正确且您要应用新方法，请从 <filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename> 目录中删除文件，从 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 文件中删除相应的 <literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal> 行，并运行 DeepSea 阶段 2 以刷新 Salt Pillar。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
</screen>
   <para>
    请运行以下命令确认结果：
   </para>
<screen>
<prompt>root@master # </prompt>salt target_node pillar.get ceph:storage
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   <warning>
    <title>DriveGroups 配置不正确</title>
    <para>
     如果您的 DriveGroups 未正确配置，且您的设置中有备用磁盘，系统将按您指定的方式部署这些磁盘。我们建议运行以下命令：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2 xml:id="upgrade-osd-deployment">
   <title>OSD 部署</title>
   <para>
    对于简单的情形（例如独立的 OSD），一段时间后将会发生迁移。每当您在集群中删除或替换 OSD 时，该 OSD 便会被基于 LVM 的新 OSD 替换。
   </para>
   <tip>
    <title>迁移到 LVM 形式</title>
    <para>
     一旦节点上有一个“旧版”OSD 需要替换，所有与其共享设备的 OSD 都需要迁移到基于 LVM 的形式。
    </para>
    <para>
     为确保完整性，请考虑迁移整个节点上的 OSD。
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>更复杂的设置</title>
   <para>
    如果您使用的是比独立 OSD 更复杂的设置，例如专用 WAL/DB 或加密 OSD，则只有当指定给该 WAL/DB 设备的所有 OSD 都被删除时，才会发生迁移。这是因为 <command>ceph-volume</command> 命令会在部署前在磁盘上创建逻辑卷。这样可防止用户混用基于分区的部署和基于 LV 的部署。在此情况下，最好手动删除指定给 WAL/DB 设备的所有 OSD，并使用 DriveGroups 方法重新部署它们。
   </para>
  </sect2>
 </sect1>
</chapter>
