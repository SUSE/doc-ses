<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_ganesha.xml" version="5.0" xml:id="cha-as-ganesha">

 <title>安裝 NFS Ganesha</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  使用 NFS Ganesha 可透過 NFS 存取物件閘道或 CephFS。SUSE Enterprise Storage 6 支援 NFS 版本 3 和 4。NFS Ganesha 在使用者空間而不是核心空間中執行，直接與物件閘道或 CephFS 互動。
 </para>
 <warning>
  <title>跨通訊協定存取</title>
  <para>
   原生 CephFS 和 NFS 用戶端不受透過 Samba 獲取的檔案鎖定限制，反之亦然。如果透過其他方式存取 CephFS 支援的 Samba 共用路徑，則依賴跨通訊協定檔案鎖定的應用程式可能會出現資料損毀。
  </para>
 </warning>
 <sect1 xml:id="sec-as-ganesha-preparation">
  <title>準備</title>

  <sect2 xml:id="sec-as-ganesha-preparation-general">
   <title>一般資訊</title>
   <para>
    若要成功部署 NFS Ganesha，需要將 <literal>role-ganesha</literal> 新增到 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。如需詳細資料，請參閱<xref linkend="policy-configuration"/>。若要使用 NFS Ganesha，還需要在 <filename>policy.cfg</filename> 中指定 <literal>role-rgw</literal> 或 <literal>role-mds</literal>。
   </para>
   <para>
    儘管可以在現有的 Ceph 節點上安裝並執行 NFS Ganesha 伺服器，但還是建議您在能夠存取 Ceph 叢集的專屬主機上執行該伺服器。用戶端主機一般不是叢集的一部分，但需要能夠透過網路存取 NFS Ganesha 伺服器。
   </para>
   <para>
    若要在完成啟始安裝後隨時啟用 NFS Ganesha 伺服器，請將 <literal>role-ganesha</literal> 新增至 <filename>policy.cfg</filename>，並至少重新執行 DeepSea 階段 2 和 4。如需詳細資料，請參閱<xref linkend="ceph-install-stack"/>。
   </para>
   <para>
    NFS Ganesha 是透過 NFS Ganesha 節點上的 <filename>/etc/ganesha/ganesha.conf</filename> 檔案設定的。但是，每次執行 DeepSea 階段 4，都會覆寫此檔案。因此，建議您編輯 Salt 使用的範本，即 Salt Master 上的 <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename> 檔案。如需組態檔案的詳細資料，請參閱<xref linkend="ceph-nfsganesha-config"/>。
   </para>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-preparation-requirements">
   <title>要求摘要</title>
   <para>
    在執行 DeepSea 階段 2 和 4 來安裝 NFS Ganesha 之前，必須滿足以下要求：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      至少為一個節點指定 <literal>role-ganesha</literal>。
     </para>
    </listitem>
    <listitem>
     <para>
      對於每個 Minion，只能定義一個 <literal>role-ganesha</literal>。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS Ganesha 需要物件閘道或 CephFS 才能正常運作。
     </para>
    </listitem>
    <listitem>
     <para>
      需要停用具有 <literal>role-ganesha</literal> 角色的 Minion 上基於核心的 NFS。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-basic-example">
  <title>範例安裝</title>

  <para>
   此程序提供了一個範例安裝，該範例使用 NFS Ganesha 的物件閘道和 CephFS 檔案系統抽象層 (FSAL)。
  </para>

  <procedure>
   <step>
    <para>
     先要執行 DeepSea 階段 0 和 1 (如果您尚未執行)，然後才能繼續執行此程序。
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     執行 DeepSea 的階段 1 之後，編輯 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 並新增下行
    </para>
<screen>role-ganesha/cluster/<replaceable>NODENAME</replaceable></screen>
    <para>
     使用叢集中某個節點的名稱取代 <replaceable>NODENAME</replaceable>。
    </para>
    <para>
     另外，請確定已指定 <literal>role-mds</literal> 和 <literal>role-rgw</literal>。
    </para>
   </step>
   <step>
    <para>
     至少執行 DeepSea 的階段 2 和 4。建議執行中間的階段 3。
    </para>
<screen><prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3 # optional but recommended
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4</screen>
   </step>
   <step>
    <para>
     透過檢查 Minion 節點上是否正在執行 NFS Ganesha 服務，以確認 NFS Ganesha 是否在運作：
    </para>
<screen><prompt>root@master # </prompt><command>salt</command> -I roles:ganesha service.status nfs-ganesha
<replaceable>MINION_ID</replaceable>:
    True</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-ha-ap">
  <title>高可用性主動-被動組態</title>

  <para>
   本節提供一個範例來展示如何設定 NFS Ganesha 伺服器的雙節點主動-被動組態。該設定需要 SUSE Linux Enterprise High Availability Extension。兩個節點分別名為 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem>。
  </para>

  <important>
   <title>服務共置</title>
   <para>
    自身具有容錯和負載平衡能力的服務不應在已圍籬區隔以執行容錯移轉服務的叢集節點上執行。因此，請勿在高可用性設定中執行 Ceph 監控程式、中繼資料伺服器、iSCSI 或 Ceph OSD 服務。
   </para>
  </important>

  <para>
   如需 SUSE Linux Enterprise High Availability Extension 的詳細資料，請參閱 <link xlink:href="https://www.suse.com/documentation/sle-ha-15/"/>。
  </para>

  <sect2 xml:id="sec-as-ganesha-ha-ap-basic">
   <title>基本安裝</title>
   <para>
    在此設定中，<systemitem class="domainname">earth</systemitem> 的 IP 位址為 <systemitem class="ipaddress">192.168.1.1</systemitem>，<systemitem class="domainname">mars</systemitem> 的位址為 <systemitem class="ipaddress">192.168.1.2</systemitem>。
   </para>
   <para>
    此外，使用兩個浮動的虛擬 IP 位址，可讓用戶端都可連接到服務，而不管該服務在哪個實體節點上執行。<systemitem class="ipaddress">192.168.1.10</systemitem> 用於透過 Hawk2 進行叢集管理，<systemitem class="ipaddress">192.168.2.1</systemitem> 專門用於 NFS 輸出。如此可讓日後能更輕鬆地套用安全性限制。
   </para>
   <para>
    以下程序說明範例安裝。<link xlink:href="https://www.suse.com/documentation/sle-ha-15/book_sleha_quickstarts/data/art_sleha_install_quick.html"/> 上提供了更多詳細資料。
   </para>
   <procedure xml:id="proc-as-ganesha-ha-ap">
    <step>
     <para>
      在 Salt Master 上準備 NFS Ganesha 節點：
     </para>
     <substeps>
      <step>
       <para>
        執行 DeepSea 階段 0 和 1。
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.0
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.1
</screen>
      </step>
      <step>
       <para>
        在 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 中為節點 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 指定 <literal>role-ganesha</literal>：
       </para>
<screen>role-ganesha/cluster/earth*.sls
role-ganesha/cluster/mars*.sls</screen>
      </step>
      <step>
       <para>
        執行 DeepSea 階段 2 到 4。
       </para>
<screen>
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.2
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.3
<prompt>root@master # </prompt><command>salt-run</command> state.orch ceph.stage.4
</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 上註冊 SUSE Linux Enterprise High Availability Extension。
     </para>
<screen>
<prompt>root # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable>
</screen>
    </step>
    <step>
     <para>
      在兩個節點上安裝 <package>ha-cluster-bootstrap</package> ：
     </para>
<screen><prompt>root # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        在 <systemitem class="domainname">earth</systemitem> 上啟始化叢集：
       </para>
<screen><prompt>root@earth # </prompt><command>ha-cluster-init</command></screen>
      </step>
      <step>
       <para>
        讓 <systemitem class="domainname">mars</systemitem> 加入該叢集：
       </para>
<screen><prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth</screen>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      檢查叢集的狀態。您應該會看到兩個節點都已新增到叢集中：
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status</screen>
    </step>
    <step>
     <para>
      在這兩個節點上，停用開機時自動啟動 NFS Ganesha 服務的功能：
     </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable nfs-ganesha</screen>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 上啟動 <command>crm</command> 外圍程序：
     </para>
<screen><prompt>root@earth # </prompt><command>crm</command> configure</screen>
     <para>
      後續指令在 crm 外圍程序中執行。
     </para>
    </step>
    <step>
     <para>
      在 <systemitem class="domainname">earth</systemitem> 上，執行 crm 外圍程序來執行以下指令，以便將 NFS Ganesha 精靈的資源設定為 systemd 資源類型的複製：
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive nfs-ganesha-server systemd:nfs-ganesha \
op monitor interval=30s
<prompt>crm(live)configure# </prompt>clone nfs-ganesha-clone nfs-ganesha-server meta interleave=true
<prompt>crm(live)configure# </prompt>commit
<prompt>crm(live)configure# </prompt>status
    2 nodes configured
    2 resources configured

    Online: [ earth mars ]

    Full list of resources:
         Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
         Started:  [ earth mars ]</screen>
    </step>
    <step>
     <para>
      使用 crm 外圍程序建立一個原始 IPAddr2：
     </para>
<screen>
<prompt>crm(live)configure# </prompt>primitive ganesha-ip IPaddr2 \
params ip=192.168.2.1 cidr_netmask=24 nic=eth0 \
op monitor interval=10 timeout=20

<prompt>crm(live)# </prompt>status
Online: [ earth mars  ]
Full list of resources:
 Clone Set: nfs-ganesha-clone [nfs-ganesha-server]
     Started: [ earth mars ]
 ganesha-ip    (ocf::heartbeat:IPaddr2):    Started earth</screen>
    </step>
    <step>
     <para>
      為了在 NFS Ganesha 伺服器與浮動虛擬 IP 之間建立關係，我們使用了併置和順序約束。
     </para>
<screen>
<prompt>crm(live)configure# </prompt>colocation ganesha-ip-with-nfs-ganesha-server inf: ganesha-ip nfs-ganesha-clone
<prompt>crm(live)configure# </prompt>order ganesha-ip-after-nfs-ganesha-server Mandatory: nfs-ganesha-clone ganesha-ip
</screen>
    </step>
    <step>
     <para>
      從用戶端使用 <command>mount</command> 指令，以確定完成叢集設定：
     </para>
<screen><prompt>root # </prompt><command>mount</command> -t nfs -v -o sync,nfsvers=4 192.168.2.1:/ /mnt</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-cleanup">
   <title>清理資源</title>
   <para>
    如果其中一個節點 (例如 <systemitem class="domainname">earth</systemitem>) 上發生 NFS Ganesha 故障，請解決問題並清理資源。如果 NFS Ganesha 在 <systemitem class="domainname">mars</systemitem> 上發生故障，則只有在清理資源之後，該資源才能錯誤回復至 <systemitem class="domainname">earth</systemitem>。
   </para>
   <para>
    若要清理資源，請執行以下指令：
   </para>
<screen><prompt>root@earth # </prompt><command>crm</command> resource cleanup nfs-ganesha-clone earth
<prompt>root@earth # </prompt><command>crm</command> resource cleanup ganesha-ip earth</screen>
  </sect2>

  <sect2 xml:id="sec-as-ganesha-ha-ap-ping-resource">
   <title>設定 Ping 資源</title>
   <para>
    有時，伺服器可能由於網路問題而無法連接用戶端。Ping 資源可以偵測並緩解此問題。設定此資源的操作是選擇性的。
   </para>
   <procedure>
    <step>
     <para>
      定義 ping 資源：
     </para>
<screen><prompt>crm(live)configure# </prompt>primitive ganesha-ping ocf:pacemaker:ping \
        params name=ping dampen=3s multiplier=100 host_list="<replaceable>CLIENT1</replaceable> <replaceable>CLIENT2</replaceable>" \
        op monitor interval=60 timeout=60 \
        op start interval=0 timeout=60 \
        op stop interval=0 timeout=60</screen>
     <para>
      <literal>host_list</literal> 是以空格字元分隔的 IP 位址清單。系統將會定期 ping 這些 IP 位址，以檢查網路中斷問題。如果某個用戶端必須永遠能夠存取 NFS 伺服器，請將該用戶端新增到 <literal>host_list</literal>。
     </para>
    </step>
    <step>
     <para>
      建立複製：
     </para>
<screen><prompt>crm(live)configure# </prompt>clone ganesha-ping-clone ganesha-ping \
        meta interleave=true</screen>
    </step>
    <step>
     <para>
      以下指令會建立 NFS Ganesha 服務的約束。當 <literal>host_list</literal> 不可存取時，此約束會強制服務轉移到另一節點。
     </para>
<screen><prompt>crm(live)configure# </prompt>location nfs-ganesha-server-with-ganesha-ping
        nfs-ganesha-clone \
        rule -inf: not_defined ping or ping lte 0</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-ha-deepsea">
   <title>NFS Ganesha HA 和 DeepSea</title>
   <para>
    DeepSea 不支援設定 NFS Ganesha HA。為了防止在設定 NFS Ganesha HA 之後 DeepSea 失敗，請從 DeepSea 階段 4 中排除 NFS Ganesha 服務的啟動和停止：
   </para>
   <procedure>
    <step>
     <para>
      將 <filename>/srv/salt/ceph/ganesha/default.sls</filename> 複製到 <filename>/srv/salt/ceph/ganesha/ha.sls</filename>。
     </para>
    </step>
    <step>
     <para>
      從 <filename>/srv/salt/ceph/ganesha/ha.sls</filename> 中移除 <literal>.service</literal> 項目，如下所示：
     </para>
<screen>include:
- .keyring
- .install
- .configure</screen>
    </step>
    <step>
     <para>
      將下行新增到 <filename>/srv/pillar/ceph/stack/global.yml</filename>：
     </para>
<screen>ganesha_init: ha</screen>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-ganesha-active-active">
  <title>主動-主動組態</title>

  <para>
   本節提供簡單的主動-主動 NFS Ganesha 設定範例。該範例的目標是部署兩部放置在同一現有 CephFS 上的 NFS Ganesha 伺服器。伺服器是兩個具有不同位址的 Ceph 叢集節點。需要在這兩個節點之間手動分佈用戶端。此組態中的<quote>容錯移轉</quote>指的是在用戶端上手動卸載並重新掛接另一部伺服器。
  </para>

  <sect2 xml:id="sec-ganesha-active-active-prerequisites">
   <title>先決條件</title>
   <para>
    對於該範例組態，您需要擁有以下資源：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      執行中的 Ceph 叢集。如需使用 DeepSea 部署和設定 Ceph 叢集的詳細資料，請參閱<xref linkend="ceph-install-stack"/>。
     </para>
    </listitem>
    <listitem>
     <para>
      至少有一個設定好的 CephFS。如需部署和設定 CephFS 的更多詳細資料，請參閱<xref linkend="cha-ceph-as-cephfs"/>。
     </para>
    </listitem>
    <listitem>
     <para>
      兩個部署了 NFS Ganesha 的 Ceph 叢集節點。如需部署 NFS Ganesha 的更多詳細資料，請參閱<xref linkend="cha-as-ganesha"/>。
     </para>
     <tip>
      <title>使用專屬伺服器</title>
      <para>
       儘管 NFS Ganesha 節點可與其他 Ceph 相關服務共用資源，我們仍建議使用專屬伺服器，以便提升效能。
      </para>
     </tip>
    </listitem>
   </itemizedlist>
   <para>
    部署 NFS Ganesha 節點之後，請確認叢集是否可運作，並且存在預設的 CephFS 池：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados lspools
cephfs_data
cephfs_metadata
</screen>
  </sect2>

  <sect2 xml:id="sec-ganesha-active-active-configure">
   <title>設定 NFS Ganesha</title>
   <para>
    檢查兩個 NFS Ganesha 節點是否安裝了檔案 <filename>/etc/ganesha/ganesha.conf</filename>。將以下區塊 (如果尚未存在) 新增至組態檔案中，以便將 RADOS 做為 NFS Ganesha 的復原後端。
   </para>
<screen>
NFS_CORE_PARAM
{
    Enable_NLM = false;
    Enable_RQUOTA = false;
    Protocols = 4;
}
NFSv4
{
    RecoveryBackend = rados_cluster;
    Minor_Versions = 1,2;
}
CACHEINODE {
    Dir_Chunk = 0;
    NParts = 1;
    Cache_Size = 1;
}
RADOS_KV
{
    pool = "<replaceable>rados_pool</replaceable>";
    namespace = "<replaceable>pool_namespace</replaceable>";
    nodeid = "<replaceable>fqdn</replaceable>"
    UserId = "<replaceable>cephx_user_id</replaceable>";
    Ceph_Conf = "<replaceable>path_to_ceph.conf</replaceable>"
}
</screen>
   <para>
   您可以檢查組態格式如下的現有行，以確定 <replaceable>rados_pool</replaceable> 和 <replaceable>pool_namespace</replaceable> 的值：</para>
<screen>
%url rados://<replaceable>rados_pool</replaceable>/<replaceable>pool_namespace</replaceable>/...
</screen>
   <para>
   <replaceable>nodeid</replaceable> 選項的值對應至機器的 FQDN，並且可在現有的 <replaceable>RADOS_URLS</replaceable> 區塊中找到 <replaceable>UserId</replaceable> 和 <replaceable>Ceph_Conf</replaceable> 選項值。
   </para>
   <para>
    由於舊版 NFS 不允許我們透過將寬限期間提前來延長伺服器重新啟動程序，我們在早於 4.2 的版本中停用了 NFS 選項。此外，我們還停用了大多數 NFS Ganesha 快取，因為 Ceph 程式庫已經在使用激進快取模式。
   </para>
   <para>
    「rados_cluster」復原後端將其資訊儲存在 RADOS 物件中。儘管資料量不大，我們仍希望其高度可用。出於此目的，我們使用 CephFS 中繼資料池，並在其中宣告一個新的「ganesha」名稱空間，使其不同於 CephFS 物件。
   </para>
   <note>
    <title>叢集節點 ID</title>
    <para>
     兩個主機的大多數組態都是相同的，不過，各節點「RADOS_KV」區塊中的 <option>nodeid</option> 選項需要設定為不同的字串。NFS Ganesha 預設會將 <option>nodeid</option> 設定為節點的主機名稱。
    </para>
    <para>
     如果您需要使用主機名稱以外的其他固定值，可以在一個節點上設定 <option>nodeid = 'a'</option> (範例設定)，在另一個節點上設定 <option>nodeid = 'b'</option>。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-grace-db">
   <title>填入叢集寬限資料庫</title>
   <para>
    我們需要確認叢集中的所有節點都知道彼此的存在。透過在主機之間共用的 RADOS 物件來實現此目的。NFS Ganesha 使用此物件傳達與寬限期間有關的目前狀態。
   </para>
   <para>
    此 <package>nfs-ganesha-rados-grace</package> 套件包含一個用於查詢和操縱此資料庫的指令行工具。如果至少有一個節點上未安裝該套件，請執行以下指令加以安裝：
   </para>
<screen>
<prompt>root # </prompt>zypper install nfs-ganesha-rados-grace
</screen>
   <para>
    我們將使用該指令來建立資料庫並新增兩個 <option>nodeid</option>。在本範例中，兩個 NFS Ganesha 節點分別命名為 <literal>ses6min1.example.com</literal> 和 <literal>ses6min2.example.com</literal>。在其中一個 NFS Ganesha 主機上執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min1.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha add ses6min2.example.com
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=1 rec=0
======================================================
ses6min1.example.com     E
ses6min2.example.com     E
</screen>
   <para>
    此指令會建立寬限資料庫，並將「ses6min1.example.com」和「ses6min2.example.com」新增至該資料庫。最後一個指令可傾印目前狀態。新增的主機一律視為將強制執行寬限期間，因此會為它們設定「E」旗標。「cur」和「rec」值顯示目前版本編號和復原版本編號，我們透過這種方式來追蹤允許主機復原的內容以及時間。
   </para>
  </sect2>

  <sect2 xml:id="ganesha-active-active-restart-servers">
   <title>重新啟動 NFS Ganesha 服務</title>
   <para>
    在兩個 NFS Ganesha 節點上，重新啟動相關服務：
   </para>
<screen>
<prompt>root # </prompt>systemctl restart nfs-ganesha.service
</screen>
   <para>
    重新啟動服務後，檢查寬限資料庫：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ganesha-rados-grace -p cephfs_metadata -n ganesha
cur=3 rec=0
======================================================
ses6min1.example.com
ses6min2.example.com
</screen>
   <note>
    <title>清除了「E」旗標</title>
    <para>
     請注意，兩個節點均已清除其「E」旗標，表示它們不再強制執行寬限期間，並且現在處於正常操作模式。
    </para>
   </note>
  </sect2>

  <sect2 xml:id="ganesha-active-active-conclusion">
   <title>結論</title>
   <para>
    完成上述所有步驟之後，您可以掛接從這兩部 NFS Ganesha 伺服器中的任意一部輸出的 NFS，並對它們執行一般的 NFS 操作。
   </para>
   <para>
    我們的範例組態假設，如果這兩部 NFS Ganesha 伺服器中的其中一部變為停機狀態，您將在 5 分鐘內手動將其重新啟動。5 分鐘後，中繼資料伺服器可能會取消 NFS Ganesha 用戶端擁有的工作階段以及與其關聯的所有狀態。如果在叢集的其餘節點進入寬限期間之前取消該工作階段的功能，伺服器的用戶端可能無法復原其所有狀態。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="sec-as-ganesha-info">
  <title>更多資訊</title>

  <para>
   如需詳細資訊，請參閱<xref linkend="cha-ceph-nfsganesha"/>。
  </para>
 </sect1>
</chapter>
