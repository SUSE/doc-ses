<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_operating_monitor.xml" version="5.0" xml:id="ceph-monitor">
 <title>確定叢集狀態</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>是</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  當叢集正在執行時，您可以使用 <command>ceph</command> 工具來監控它。若要確定叢集狀態，通常需要檢查 Ceph OSD、Ceph 監控程式、放置群組和中繼資料伺服器的狀態。
 </para>
 <tip>
  <title>互動模式</title>
  <para>
   若要以互動模式執行 <command>ceph</command> 工具，請不帶任何引數在指令行中輸入 <command>ceph</command>。如果要在一行中輸入多條 <command>ceph</command> 指令，則使用互動模式較為方便。例如：
  </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph
ceph&gt; health
ceph&gt; status
ceph&gt; quorum_status
ceph&gt; mon_status</screen>
 </tip>
 <sect1 xml:id="monitor-status">
  <title>檢查叢集的狀態</title>

  <para>
   若要檢查叢集的狀態，請執行以下指令：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph status</screen>

  <para>
   或
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>

  <para>
   在互動模式下，輸入 <command>status</command>，然後按 <keycap function="enter"/>。
  </para>

<screen>ceph&gt; status</screen>

  <para>
   Ceph 將列印叢集狀態。例如，由一個監控程式和兩個 OSD 組成的微型 Ceph 叢集可能會列印以下內容：
  </para>

<screen>cluster b370a29d-9287-4ca3-ab57-3d824f65e339
 health HEALTH_OK
 monmap e1: 1 mons at {ceph1=10.0.0.8:6789/0}, election epoch 2, quorum 0 ceph1
 osdmap e63: 2 osds: 2 up, 2 in
  pgmap v41332: 952 pgs, 20 pools, 17130 MB data, 2199 objects
        115 GB used, 167 GB / 297 GB avail
               1 active+clean+scrubbing+deep
             951 active+clean</screen>
 </sect1>
 <sect1 xml:id="monitor-health">
  <title>檢查叢集狀態</title>

  <para>
   在啟動叢集後到開始讀取和/或寫入資料期間，檢查叢集的狀態：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 10 pgs degraded; 100 pgs stuck unclean; 1 mons down, quorum 0,2 \
node-1,node-2,node-3</screen>

  <tip>
   <para>
    如果之前為您的組態或金鑰圈指定了非預設位置，則此時可以指定它們的位置：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>

  <para>
   Ceph 叢集會傳回下列狀態代碼之一：
  </para>

  <variablelist>
   <varlistentry>
    <term>OSD_DOWN</term>
    <listitem>
     <para>
      一或多個 OSD 標示為已停機。OSD 精靈可能已停止，或對等 OSD 可能無法透過網路連接 OSD。常見原因包括精靈已停止或已當機、主機已停機或網路中斷。
     </para>
     <para>
      驗證主機是否執行良好，精靈是否已啟動，並且網路是否正常運作。如果精靈已當機，精靈記錄檔案 (<filename>/var/log/ceph/ceph-osd.*</filename>) 可能會包含除錯資訊。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_<replaceable>crush type</replaceable>_DOWN，例如 OSD_HOST_DOWN</term>
    <listitem>
     <para>
      特定 CRUSH 子樹中的所有 OSD (例如主機上的所有 OSD) 均標示為已停機。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_ORPHAN</term>
    <listitem>
     <para>
      在 CRUSH 地圖階層中參考了 OSD，但它不存在。可使用以下指令從 CRUSH 階層中移除 OSD：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd crush rm osd.<replaceable>ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_OUT_OF_ORDER_FULL</term>
    <listitem>
     <para>
      以下各項的使用率閾值並不是遞增的：<emphasis>backfillfull</emphasis> (預設值為 0.90)、<emphasis>nearfull</emphasis> (預設值為 0.85)、<emphasis>full</emphasis> (預設值為 0.95)、<emphasis>failsafe_full</emphasis>。特別是，我們需要 <emphasis>backfillfull</emphasis> &lt; <emphasis>nearfull</emphasis>，<emphasis>nearfull</emphasis> &lt; <emphasis>full</emphasis> 且 <emphasis>full</emphasis> &lt; <emphasis>failsafe_full</emphasis>。
     </para>
     <para>
      若要讀取最新的值，請執行以下指令：
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
     <para>
      可以使用以下指令調整閾值：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>ratio</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>ratio</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FULL</term>
    <listitem>
     <para>
      一或多個 OSD 超出了 <emphasis>full</emphasis> 閾值，阻止叢集處理寫入操作。可使用以下指令檢查各池的使用量：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
     <para>
      可使用以下指令查看目前定義的 <emphasis>full</emphasis> 比率：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump | grep full_ratio</screen>
     <para>
      還原寫入可用性的臨時解決辦法是稍稍提高 full 閾值：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>ratio</replaceable></screen>
     <para>
      請透過部署更多 OSD 將新的儲存新增至叢集，或者刪除現有資料來騰出空間。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_BACKFILLFULL</term>
    <listitem>
     <para>
      一或多個 OSD 超出了 <emphasis>backfillfull</emphasis> 閾值，因而不允許將資料重新平衡到此裝置。這是一則預警，表示重新平衡可能無法完成，並且叢集將滿。可使用以下指令檢查各池的使用量：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NEARFULL</term>
    <listitem>
     <para>
      一或多個 OSD 超出了 <emphasis>nearfull</emphasis> 閾值。這是一則預警，表示叢集將滿。可使用以下指令檢查各池的使用量：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSDMAP_FLAGS</term>
    <listitem>
     <para>
      已設定一或多個所需的叢集旗標。可使用以下指令設定或清除這些旗標 (<emphasis>full</emphasis> 除外)：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set <replaceable>flag</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd unset <replaceable>flag</replaceable></screen>
     <para>
      這些旗標包括：
     </para>
     <variablelist>
      <varlistentry>
       <term>full</term>
       <listitem>
        <para>
         叢集標記為已滿，無法處理寫入操作。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>pauserd、pausewr</term>
       <listitem>
        <para>
         已暫停讀取或寫入。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         不允許 OSD 啟動。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         將會忽略 OSD 故障報告，如此監控程式便不會將 OSD 標示為 <emphasis>down</emphasis>。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         先前標示為 <emphasis>out</emphasis> 的 OSD 在啟動時將不會重新標示為 <emphasis>in</emphasis>。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         <emphasis>停機</emphasis>的 OSD 在設定間隔過後將不會自動標示為 <emphasis>out</emphasis>。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nobackfill、norecover、norebalance</term>
       <listitem>
        <para>
         復原或資料重新平衡程序已暫停。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noscrub、nodeep_scrub</term>
       <listitem>
        <para>
         整理程序已停用 (請參閱<xref linkend="scrubbing"/>)。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>notieragent</term>
       <listitem>
        <para>
         快取分層活動已暫停。
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_FLAGS</term>
    <listitem>
     <para>
      一或多個 OSD 設定了所需的每 OSD 旗標。這些旗標包括：
     </para>
     <variablelist>
      <varlistentry>
       <term>noup</term>
       <listitem>
        <para>
         不允許 OSD 啟動。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>nodown</term>
       <listitem>
        <para>
         將會忽略此 OSD 的故障報告。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noin</term>
       <listitem>
        <para>
         如果此 OSD 先前在發生故障後自動標示為 <emphasis>out</emphasis>，當它啟動時將不會標示為 <emphasis>in</emphasis>。
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noout</term>
       <listitem>
        <para>
         如果此 OSD 已停機，則在設定的間隔過後，它將不會自動標示為 <emphasis>out</emphasis>。
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <para>
      可使用以下指令來設定和清除每 OSD 旗標：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd add-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd rm-<replaceable>flag</replaceable> <replaceable>osd-ID</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_TUNABLES</term>
    <listitem>
     <para>
      CRUSH 地圖目前使用的設定很舊，應予以更新。<option>mon_crush_min_required_version</option> 組態選項可確定使用時不會觸發此狀態警告的最舊可調參數 (即能夠連接到叢集的最舊用戶端版本)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OLD_CRUSH_STRAW_CALC_VERSION</term>
    <listitem>
     <para>
      CRUSH 地圖目前使用較舊的非最佳方法來計算 straw 桶的中間權數值。應該更新 CRUSH 地圖，以使用較新的方法 (<option>straw_calc_version</option>=1)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NO_HIT_SET</term>
    <listitem>
     <para>
      一或多個快取池未設定命中集來追蹤使用量，這使分層代理程式無法識別要從快取中衝洗和逐出的冷物件。可使用以下指令對快取池設定命中集：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_type <replaceable>type</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_period <replaceable>period-in-seconds</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_count <replaceable>number-of-hitsets</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>poolname</replaceable> hit_set_fpp <replaceable>target-false-positive-rate</replaceable></screen>
     <para>
      如需快取分層的詳細資訊，請參閱<xref linkend="cha-ceph-tiered"/>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_NO_SORTBITWISE</term>
    <listitem>
     <para>
      未在執行 Luminous 12 以下版本的 OSD，但是尚未設定 <option>sortbitwise</option> 旗標。您需要先設定 <option>sortbitwise</option> 旗標，Luminous 12 或更新版本的 OSD 才能啟動：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd set sortbitwise</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      一或多個池已達到其定額，不再允許寫入。您可使用以下指令來設定池定額和使用量：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph df detail</screen>
     <para>
      您可以使用以下指令來提高池定額
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_objects <replaceable>num-objects</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>poolname</replaceable> max_bytes <replaceable>num-bytes</replaceable></screen>
     <para>
      或者刪除一些現有資料以減少使用量。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_AVAILABILITY</term>
    <listitem>
     <para>
      資料可用性下降，這表示叢集無法處理針對叢集中某些資料的潛在讀取或寫入要求。具體而言，一或多個 PG 處於不允許處理 I/O 要求的狀態。有問題的 PG 狀態包括<emphasis>互聯建立中</emphasis>、<emphasis>陳舊</emphasis>、<emphasis>不完整</emphasis>和不處於<emphasis>使用中</emphasis> (如果不迅速解決這些狀況)。執行以下指令可獲得有關哪些 PG 受影響的詳細資訊：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph health detail</screen>
     <para>
      大多數情況下，出現此情形的根本原因在於一或多個 OSD 目前已停機。可使用以下指令來查詢特定的有問題 PG 的狀態：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED</term>
    <listitem>
     <para>
      某些資料的資料備援降低，這表示叢集沒有所需數量的複本用於所有資料 (對於副本池) 或糾刪碼片段 (對於糾刪碼池)。具體而言，一或多個 PG 設定了 <emphasis>degraded</emphasis> 或 <emphasis>undersized</emphasis> 旗標 (叢集中沒有該放置群組的足夠例項)，或者有一段時間未設定 <emphasis>clean</emphasis> 旗標。執行以下指令可獲得有關哪些 PG 受影響的詳細資訊：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph health detail</screen>
     <para>
      大多數情況下，出現此情形的根本原因在於一或多個 OSD 目前已停機。可使用以下指令來查詢特定的有問題 PG 的狀態：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DEGRADED_FULL</term>
    <listitem>
     <para>
      由於叢集中的可用空間不足，某些資料的資料備援可能已降低或面臨風險。具體而言，一或多個 PG 設定了 <emphasis>backfill_toofull</emphasis> 或 <emphasis>recovery_tooful</emphasis> 旗標，這表示叢集無法移轉或復原資料，原因是一或多個 OSD 高於 <emphasis>backfillfull</emphasis> 閾值。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_DAMAGED</term>
    <listitem>
     <para>
      資料整理 (請參閱<xref linkend="scrubbing"/>) 程序探查到叢集中存在某些資料一致性問題。具體而言，一或多個 PG 設定了 <emphasis>inconsistent</emphasis> 或 <emphasis>snaptrim_error</emphasis> 旗標 (表示某個較早的整理操作發現問題)，或者設定了 <emphasis>repair</emphasis> 旗標 (表示目前正在修復此類不一致問題)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OSD_SCRUB_ERRORS</term>
    <listitem>
     <para>
      最近的 OSD 整理操作發現了不一致問題。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>CACHE_POOL_NEAR_FULL</term>
    <listitem>
     <para>
      快取層池將滿。此環境中的「滿」由快取池的 <emphasis>target_max_bytes</emphasis> 和 <emphasis>target_max_objects</emphasis> 內容確定。當池達到目標閾值時，如果正在從快取衝洗和逐出資料，寫入池的要求可能會被阻擋，出現常會導致延遲很高且效能變差的狀態。可使用以下指令調整快取池目標大小：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cache-pool-name</replaceable> target_max_objects <replaceable>objects</replaceable></screen>
     <para>
      正常的快取衝洗和逐出活動還可能因基礎層級可用性或效能下降或者叢集的整體負載較高而受到限制。
     </para>
     <para>
      如需快取分層的詳細資訊，請參閱<xref linkend="cha-ceph-tiered"/>。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_FEW_PGS</term>
    <listitem>
     <para>
      使用中的 PG 數量低於每個 OSD 的 PG 數的可設定閾值 <option>mon_pg_warn_min_per_osd</option>。這可能導致叢集中各 OSD 間的資料分發和平衡未達到最佳，以致降低整體效能。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>TOO_MANY_PGS</term>
    <listitem>
     <para>
      使用中的 PG 數量高於每個 OSD 的 PG 數的可設定閾值 <option>mon_pg_warn_max_per_osd</option>。這可能導致 OSD 精靈的記憶體使用率較高，叢集狀態變更 (例如 OSD 重新啟動、新增或移除) 之後互聯速度降低，並且 Ceph 管理員和 Ceph 監控程式上的負載較高。
     </para>
     <para>
      雖然不能降低現有池的 <option>pg_num</option> 值，但可以降低 <option>pgp_num</option> 值。這樣可有效地將一些 PG 併置在同組 OSD 上，從而減輕上述的一些負面影響。可使用以下指令調整 <option>pgp_num</option> 值：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>SMALLER_PGP_NUM</term>
    <listitem>
     <para>
      一或多個池的 <option>pgp_num</option> 值小於 <option>pg_num</option>。這通常表示 PG 計數有所提高，但未同時提升放置行為。使用以下指令設定 <option>pgp_num</option>，使其與觸發資料移轉的 <option>pg_num</option> 相符，通常便可解決此問題：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>pool</replaceable> pgp_num <replaceable>pg_num_value</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>MANY_OBJECTS_PER_PG</term>
    <listitem>
     <para>
      一或多個池的每 PG 平均物件數大大高於叢集的整體平均值。透過 <option>mon_pg_warn_max_object_skew</option> 組態值來控制該特定閾值。這通常表示包含叢集中大部分資料的池所具有的 PG 太少，以及/或者不包含這麼多資料的其他池具有的 PG 太多。可透過調整監控程式上的 <option>mon_pg_warn_max_object_skew</option> 組態選項提高閾值，來消除該狀態警告。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_APP_NOT_ENABLED¶</term>
    <listitem>
     <para>
      存在包含一或多個物件，但尚未標記為供特定應用程式使用的池。將池標記為供某個應用程式使用即可消除此警告。例如，如果池由 RBD 使用：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>rbd pool init <replaceable>pool_name</replaceable></screen>
     <para>
      如果池正由自訂應用程式「foo」使用，您還可以使用低層級指令標記它：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool application enable foo</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_FULL</term>
    <listitem>
     <para>
      一或多個池已達到 (或幾乎要達到) 其定額。透過 <option>mon_pool_quota_crit_threshold</option> 組態選項來控制觸發此錯誤狀況的閾值。可使用以下指令上調、下調 (或移除) 池定額：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      將定額值設定為 0 將停用定額。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>POOL_NEAR_FULL</term>
    <listitem>
     <para>
      一或多個池接近其定額。透過 <option>mon_pool_quota_warn_threshold</option> 組態選項來控制觸發此警告狀況的閾值。可使用以下指令上調、下調 (或移除) 池定額：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_bytes <replaceable>bytes</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd osd pool set-quota <replaceable>pool</replaceable> max_objects <replaceable>objects</replaceable></screen>
     <para>
      將定額值設定為 0 將停用定額。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_MISPLACED</term>
    <listitem>
     <para>
      叢集中的一或多個物件未儲存在叢集希望儲存的節點上。這表示叢集最近的某項變更導致的資料移轉尚未完成。誤放的資料本質上不屬於危險狀況。資料一致性方面永遠不會有風險，僅當所需位置放置了物件所需份數的新副本之後，系統才會移除物件的舊副本。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>OBJECT_UNFOUND</term>
    <listitem>
     <para>
      找不到叢集中的一或多個物件。具體而言，OSD 知道物件的新複本或更新複本應該存在，但在目前啟用的 OSD 上卻找不到該版物件的複本。系統將阻止對「未找到」物件的讀取或寫入要求。從理論上講，可以將具有未找到物件最近複本的已停機 OSD 重新啟用。可透過負責處理未找到物件的 PG 的互聯狀態識別候選 OSD：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph tell <replaceable>pgid</replaceable> query</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_SLOW</term>
    <listitem>
     <para>
      正花費很長的時間處理一或多個 OSD 要求。這可能表示負載極重、儲存裝置速度緩慢或有軟體錯誤。您可以從 OSD 主機上執行以下指令，來查詢有問題的 OSD 上的要求佇列：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> ops</screen>
     <para>
      您可以查看近期最慢的要求摘要：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph daemon osd.<replaceable>id</replaceable> dump_historic_ops</screen>
     <para>
      可使用以下指令尋找 OSD 的位置：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd find osd.<replaceable>id</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>REQUEST_STUCK</term>
    <listitem>
     <para>
      已將一或多個 OSD 要求阻擋一段相當長的時間，例如 4096 秒。這表示叢集已有很長一段時間處於狀況不良狀態 (例如，沒有足夠的執行中 OSD 或非使用中 PG)，或者 OSD 存在某種內部問題。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_SCRUBBED</term>
    <listitem>
     <para>
      最近未整理 (請參閱<xref linkend="scrubbing"/>) 一或多個 PG。通常每 <option>mon_scrub_interval</option> 秒整理一次 PG，當 <option>mon_warn_not_scrubbed</option> 這類間隔已過但未進行整理時，就會觸發此警告。如果 PG 未標記為清理，系統將不會整理它們。如果 PG 放置錯誤或已降級，就會出現這種情況 (請參閱上文中的 PG_AVAILABILITY 和 PG_DEGRADED)。您可使用以下指令手動對標記為清理的 PG 啟動整理：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph pg scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>PG_NOT_DEEP_SCRUBBED</term>
    <listitem>
     <para>
      最近未深層整理 (請參閱<xref linkend="scrubbing"/>) 一或多個 PG。系統通常每 <option>osd_deep_scrub_interval</option> 秒整理一次 PG，當 <option>mon_warn_not_deep_scrubbed</option> 秒已過但未進行整理時，就會觸發此警告。如果 PG 未標記為清理，系統將不會 (深層) 整理它們。如果 PG 放置錯誤或已降級，就會出現這種情況 (請參閱上文中的 PG_AVAILABILITY 和 PG_DEGRADED)。您可使用以下指令手動對標記為清理的 PG 啟動整理：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph pg deep-scrub <replaceable>pgid</replaceable></screen>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <para>
    如果之前為您的組態或金鑰圈指定了非預設位置，則此時可以指定它們的位置：
   </para>
<screen><prompt>root # </prompt>ceph -c <replaceable>/path/to/conf</replaceable> -k <replaceable>/path/to/keyring</replaceable> health</screen>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-watch">
  <title>監看叢集</title>

  <para>
   您可以使用 <command>ceph -s</command> 瞭解叢集的即時狀態。例如，由一個監控程式和兩個 OSD 組成的微型 Ceph 叢集可在某工作負載正在執行時列印以下內容：
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph -s
cluster:
  id:     ea4cf6ce-80c6-3583-bb5e-95fa303c893f
  health: HEALTH_WARN
          too many PGs per OSD (408 &gt; max 300)

services:
  mon: 3 daemons, quorum ses5min1,ses5min3,ses5min2
  mgr: ses5min1(active), standbys: ses5min3, ses5min2
  mds: cephfs-1/1/1 up  {0=ses5min3=up:active}
  osd: 4 osds: 4 up, 4 in
  rgw: 1 daemon active

data:
  pools:   8 pools, 544 pgs
  objects: 253 objects, 3821 bytes
  usage:   6252 MB used, 13823 MB / 20075 MB avail
  pgs:     544 active+clean
</screen>

  <para>
   輸出內容提供了以下資訊：
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     叢集 ID
    </para>
   </listitem>
   <listitem>
    <para>
     叢集執行狀態
    </para>
   </listitem>
   <listitem>
    <para>
     監控程式地圖版本編號和監控程式仲裁的狀態
    </para>
   </listitem>
   <listitem>
    <para>
     OSD 地圖版本編號和 OSD 的狀態
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph 管理員的狀態
    </para>
   </listitem>
   <listitem>
    <para>
     物件閘道的狀態
    </para>
   </listitem>
   <listitem>
    <para>
     放置群組地圖版本
    </para>
   </listitem>
   <listitem>
    <para>
     放置群組和池數量
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>理論上</emphasis>儲存的資料量和儲存的物件數量
    </para>
   </listitem>
   <listitem>
    <para>
     儲存的資料總量
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>Ceph 計算資料使用量的方式</title>
   <para>
    <literal>used</literal> 值反映實際使用的原始儲存量。<literal>xxx GB / xxx GB</literal> 值表示叢集可用容量 (兩者中的較小數值)，以及叢集的整體儲存容量。理論數量反映在複製所儲存資料或建立其快照前這些資料的大小。因此，實際儲存的資料量一般會超出理論上的儲存量，因為 Ceph 會建立資料的複本，可能還會將儲存容量用於複製和建立快照。
   </para>
  </tip>

  <para>
   顯示即時狀態資訊的其他指令如下：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>ceph pg stat</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph osd pool stats</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>ceph df detail</command>
    </para>
   </listitem>
  </itemizedlist>

  <para>
   若要即時更新資訊，請在 <command>watch</command> 指令中以引數的方式使用以上任意指令 (包括 <command>ceph -s</command>)：
  </para>

<screen><prompt>root # </prompt>watch -n 10 'ceph -s'</screen>

  <para>
   如果您看累了，請按 <keycombo><keycap function="control"/><keycap>C</keycap></keycombo>。
  </para>
 </sect1>
 <sect1 xml:id="monitor-stats">
  <title>檢查叢集的使用量統計資料</title>

  <para>
   若要檢視叢集的資料使用率以及資料在多個池之間的分佈，請使用 <command>ceph df</command> 指令。若要獲取更多詳細資料，請使用 <command>ceph df detail</command>。
  </para>

<screen>
<prompt>cephadm@adm &gt; </prompt>ceph df
RAW STORAGE:
    CLASS     SIZE       AVAIL      USED        RAW USED     %RAW USED
    hdd       40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
    TOTAL     40 GiB     32 GiB     137 MiB      8.1 GiB         20.33
POOLS:
    POOL             ID     STORED     OBJECTS    USED       %USED    MAX AVAIL
    iscsi-images      1     3.9 KiB          8    769 KiB        0       10 GiB
    cephfs_data       2     1.6 KiB          5    960 KiB        0       10 GiB
    cephfs_metadata   3      54 KiB         22    1.5 MiB        0       10 GiB
[...]
</screen>

  <para>
   輸出中的 <literal>RAW STORAGE</literal> 區段提供叢集用於資料的儲存空間容量綜覽。
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>CLASS</literal>：裝置的儲存類別。如需裝置類別的更多詳細資料，請參閱<xref linkend="crush-devclasses"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>SIZE</literal>：叢集的整體儲存容量。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>AVAIL</literal>：叢集中的可用空間容量。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>：單純為區塊裝置中儲存的資料物件配置的空間 (所有 OSD 上的累計空間)。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>RAW USED</literal>：「USED」空間與區塊裝置上為實現 Ceph 而配置/保留的空間 (例如 BlueStore 的 BlueFS 部分) 之和。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>% RAW USED</literal>：已用的原始儲存量百分比。將此數字與 <literal>full ratio</literal> 和 <literal>near full ratio</literal> 結合使用，可確保您不會用完叢集的容量。如需其他詳細資料，請參閱<xref linkend="storage-capacity"/>。
    </para>
    <note>
     <title>叢集填滿程度</title>
     <para>
      當原始儲存填滿層級接近 100% 時，您需要新增新儲存空間至叢集。較高的使用量可能導致單個 OSD 填滿，叢集處於不良狀態。
     </para>
     <para>
      使用指令 <command>ceph osd df tree</command> 可列出所有 OSD 的填滿程度。
     </para>
    </note>
   </listitem>
  </itemizedlist>

  <para>
   輸出內容的 <literal>POOLS</literal> 區段提供了池清單和每個池的理論使用量。此區段的輸出<emphasis>不</emphasis>反映複本、複製品或快照。例如，如果您儲存含 1MB 資料的物件，理論使用量將是 1MB，但是根據複本、複製品或快照數量，實際使用量可能是 2MB 或更多。
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <literal>POOL</literal>：池的名稱。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>ID</literal>：池 ID。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>STORED</literal>：使用者儲存的資料量。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>OBJECTS</literal>：每個池的理論已儲存物件數。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>USED</literal>：所有 OSD 節點單純為儲存資料配置的空間容量 (以 kB 計)。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>%USED</literal>：每個池的理論已用儲存百分比。
    </para>
   </listitem>
   <listitem>
    <para>
     <literal>MAX AVAIL</literal>：給定池中的最大可用空間。
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <para>
    POOLS 區段中的數字是理論上的。它們不包括複本、快照或複製品數量。因此，<literal>USED</literal> 與 %<literal>USED</literal> 數量之和不會加總到輸出內容 <literal>RAW STORAGE</literal> 區段中的 <literal>RAW USED</literal> 和 <literal>%RAW USED</literal> 數量中。
   </para>
  </note>
 </sect1>
 <sect1 xml:id="monitor-osdstatus">
  <title>檢查 OSD 狀態</title>

  <para>
   您可透過執行以下指令來檢查 OSD，以確定它們已啟動且正在執行：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd stat</screen>

  <para>
   或
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd dump</screen>

  <para>
   您也可以根據 OSD 在 CRUSH 地圖中的位置檢視 OSD。
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tree</screen>

  <para>
   Ceph 將列印 CRUSH 樹及主機、它的 OSD、OSD 是否已啟動及其權數。
  </para>

<screen># id    weight  type name       up/down reweight
-1      3       pool default
-3      3               rack mainrack
-2      3                       host osd-host
0       1                               osd.0   up      1
1       1                               osd.1   up      1
2       1                               osd.2   up      1</screen>
 </sect1>
 <sect1 xml:id="storage-bp-monitoring-fullosd">
  <title>檢查填滿的 OSD</title>

  <para>
   Ceph 可阻止您向填滿的 OSD 寫入資料，以防遺失資料。在正常運作的叢集中，當叢集接近其填滿比率時，您會收到警告。<command>mon osd full ratio</command> 預設設為容量的 0.95 (95%)，達到該比率後，叢集會阻止用戶端寫入資料。<command>mon osd nearfull ratio</command> 預設設為容量的 0.85 (85%)，達到該比率時，叢集會產生狀態警告。
  </para>

  <para>
   可透過 <command>ceph health</command> 指令報告填滿的 OSD 節點：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
  HEALTH_WARN 1 nearfull osds
  osd.2 is near full at 85%</screen>

  <para>
   或
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph health
  HEALTH_ERR 1 nearfull osds, 1 full osds
  osd.2 is near full at 85%
  osd.3 is full at 97%</screen>

  <para>
   處理填滿的叢集的最佳方法是新增新的 OSD 主機/磁碟，以便讓叢集將資料重新分佈到新的可用儲存空間。
  </para>

  <tip>
   <title>防止 OSD 填滿</title>
   <para>
    OSD 變滿 (即用完 100% 的磁碟空間) 之後，往往會迅速當機而不發出警告。管理 OSD 節點時需記住下面幾點提示。
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      每個 OSD 的磁碟空間 (通常掛接於 <filename>/var/lib/ceph/osd/osd-{1,2..}</filename> 下) 需放置在專屬的基礎磁碟或分割區上。
     </para>
    </listitem>
    <listitem>
     <para>
      檢查 Ceph 組態檔案，確定 Ceph 不會將其記錄檔案儲存在專供 OSD 使用的磁碟/分割區上。
     </para>
    </listitem>
    <listitem>
     <para>
      確定沒有其他程序寫入專供 OSD 使用的磁碟/分割區。
     </para>
    </listitem>
   </itemizedlist>
  </tip>
 </sect1>
 <sect1 xml:id="monitor-monstatus">
  <title>檢查監控程式狀態</title>

  <para>
   啟動叢集後，請在第一次讀取和/或寫入資料之前檢查 Ceph 監控程式的仲裁狀態。如果叢集已在處理要求，請定期檢查 Ceph 監控程式的狀態，以確定其正在執行。
  </para>

  <para>
   若要顯示監控程式地圖，請執行以下指令：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph mon stat</screen>

  <para>
   或
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph mon dump</screen>

  <para>
   若要檢查監控程式叢集的仲裁狀態，請執行以下指令：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph quorum_status</screen>

  <para>
   Ceph 將傳回仲裁狀態。例如，由三個監控程式組成的 Ceph 叢集可能傳回以下內容：
  </para>

<screen>{ "election_epoch": 10,
  "quorum": [
        0,
        1,
        2],
  "monmap": { "epoch": 1,
      "fsid": "444b489c-4f16-4b75-83f0-cb8097468898",
      "modified": "2011-12-12 13:28:27.505520",
      "created": "2011-12-12 13:28:27.505520",
      "mons": [
            { "rank": 0,
              "name": "a",
              "addr": "192.168.1.10:6789\/0"},
            { "rank": 1,
              "name": "b",
              "addr": "192.168.1.11:6789\/0"},
            { "rank": 2,
              "name": "c",
              "addr": "192.168.1.12:6789\/0"}
           ]
    }
}</screen>
 </sect1>
 <sect1 xml:id="monitor-pgroupstatus">
  <title>檢查放置群組狀態</title>

  <para>
   放置群組會將物件對應到 OSD。監控放置群組時，您希望它們處於 <literal>active</literal> 和 <literal>clean</literal> 狀態。如需詳細內容，請參閱<xref linkend="op-mon-osd-pg"/>。
  </para>
 </sect1>
 <sect1 xml:id="monitor-adminsocket">
  <title>使用管理通訊端</title>

  <para>
   <remark role="fixme">Maybe give an example use case? No obvious difference to normal ceph command?!</remark>Ceph 管理通訊端可讓您透過通訊端介面查詢精靈。依預設，Ceph 通訊端存放在 <filename>/var/run/ceph</filename> 下。若要透過管理通訊端存取精靈，請登入執行精靈的主機，並使用以下指令：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable></screen>

  <para>
   若要檢視可用的管理通訊端指令，請執行以下指令：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph --admin-daemon /var/run/ceph/<replaceable>socket-name</replaceable> help</screen>

  <para>
   使用管理通訊端指令可在執行時期顯示和設定您的組態。如需詳細資訊，請參閱<xref linkend="ceph-config-runtime"/>。
  </para>

  <para>
   另外，您也可以直接在執行時期設定組態 (管理通訊端會繞過監控程式，這與 <command>ceph tell</command>
   <replaceable>daemon-type</replaceable>.<replaceable>id</replaceable> injectargs 不同，後者依賴於監控程式，但不需要您直接登入有問題的主機)。
  </para>
 </sect1>
 <sect1 xml:id="storage-capacity">
  <title>儲存容量</title>

  <para>
   做為防止資料遺失的安全措施，當 Ceph 儲存叢集接近其容量上限時，Ceph 將阻止您向 Ceph OSD 寫入或從中讀取資料。因此，讓生產叢集接近其填滿比率不是一種好的做法，因為這樣會犧牲高可用性。預設的填滿比率設定為 0.95，即容量的 95%。對於所含 OSD 數量較少的測試叢集而言，如此設定是非常激進的。
  </para>

  <tip>
   <title>增加儲存容量</title>
   <para>
    在監控叢集時，請注意與 <literal>nearfull</literal> 比率相關的警示。出現該警示表示，如果一或多個 OSD 發生故障，某些 OSD 的故障可能會導致服務暫時中斷。請考慮新增更多 OSD 以增加儲存容量。
   </para>
  </tip>

  <para>
   測試叢集的一種常見情境是，系統管理員從 Ceph 儲存叢集中移除 Ceph OSD，等待叢集重新達到平衡。然後再移除另一個 Ceph OSD，依此類推，直至叢集最終達到填滿比率並鎖死。我們建議即使使用測試叢集時也進行一定的容量規劃。透過規劃，您可以預估維持高可用性所需的備用容量。從理論上講，您需要規劃能夠應對一系列 Ceph OSD 發生故障的情況的方案，使叢集無需立即取代這些 Ceph OSD 也可復原到 <literal>active + clean</literal> 狀態。您可以執行 <literal>active + degraded</literal> 狀態的叢集，但這不適合正常運作狀態。
  </para>

  <para>
   下圖展示了一個包含 33 個 Ceph 節點的簡化 Ceph 儲存叢集，其中每個主機有一個 Ceph OSD，每個 Ceph OSD 從 3 TB 磁碟機讀取以及向其中寫入資料。此範例叢集實際的容量上限為 99 TB。<option>mon osd full ratio</option> 選項設定為 0.95。如果叢集的剩餘容量降至 5 TB，叢集將不允許用戶端讀取和寫入資料。因此，儲存叢集的運作容量為 95 TB，而不是 99 TB。
  </para>

  <figure>
   <title>Ceph 叢集</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="ceph_cluster.png" width="85%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   在這樣的叢集中，有一或兩個 OSD 發生故障屬於正常現象。一種不常發生但合乎常理的情況是機架的路由器或電源發生故障，導致多個 OSD (例如 OSD 7-12) 同時停機。在這種情況下，您仍然應該設法使叢集保持正常執行並達到 <literal>active + clean</literal> 狀態，即使這表示需要立即新增一些主機及額外的 OSD。如果容量使用率過高，您可能不會遺失資料。但是，如果叢集的容量使用率超過填滿比率，您雖然解決了故障網域內發生的中斷問題，卻可能會損失資料可用性。因此，我們建議至少進行大致的容量規劃。
  </para>

  <para>
   針對您的叢集確定以下兩個數值：
  </para>

  <orderedlist>
   <listitem>
    <para>
     OSD 的數量。
    </para>
   </listitem>
   <listitem>
    <para>
     叢集的總容量。
    </para>
   </listitem>
  </orderedlist>

  <para>
   如果您將叢集的總容量除以叢集中的 OSD 數量，將得到叢集內單個 OSD 的平均容量。將該數值與您預期正常運作期間將同時發生故障的 OSD 數量 (一個相對較小的數值) 相乘。最後，將叢集容量與填滿比率相乘得到運作容量上限。然後，減去您預期將發生故障的 OSD 中的資料量，即可得到一個合理的填滿比率。使用更高的 OSD 故障數 (整個機架的 OSD) 重複上述過程，即可得到一個合理的接近填滿比率數值。
  </para>

  <para>
   以下設定僅在建立叢集時適用，隨後會儲存在 OSD 地圖中：
  </para>

<screen>
[global]
 mon osd full ratio = .80
 mon osd backfillfull ratio = .75
 mon osd nearfull ratio = .70
</screen>

  <tip>
   <para>
    這些設定僅在建立叢集時適用。此後，需要使用 <command>ceph osd set-nearfull-ratio</command> 和 <command>ceph osd set-full-ratio</command> 指令在 OSD 地圖中變更這些設定。
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>mon osd full ratio</term>
    <listitem>
     <para>
      在將 OSD 視為<literal>已滿</literal>之前使用的磁碟空間百分比。預設值為 0.95
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd backfillfull ratio</term>
    <listitem>
     <para>
      在將 OSD 視為過<literal>滿</literal>而無法回填之前使用的磁碟空間百分比。預設值為 0.90
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>mon osd nearfull ratio</term>
    <listitem>
     <para>
      在將 OSD 視為<literal>將滿</literal>之前使用的磁碟空間百分比。預設值為 0.85
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <tip>
   <title>檢查 OSD 權數</title>
   <para>
    如果某些 OSD <literal>將滿</literal>，但其他 OSD 的容量充足，則表示<literal>將滿</literal> OSD 的 CRUSH 權數可能有問題。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="op-mon-osd-pg">
  <title>監控 OSD 和放置群組</title>

  <para>
   高可用性和高可靠性要求採用容錯方法來管理硬體和軟體問題。Ceph 沒有單一故障點，並且可以在「已降級」模式下處理資料要求。Ceph 的資料放置引入了一個間接層，以確定資料不會直接結合至特定 OSD 位址。這表示追蹤系統故障原因需要找到屬於問題根源的放置群組和基礎 OSD。
  </para>

  <tip>
   <title>發生故障時的存取</title>
   <para>
    如果叢集的某個組件發生故障，叢集可能會阻止您存取某個特定物件，但這並不表示您無法存取其他物件。遇到故障時，請執行相關步驟來監控 OSD 和放置群組。然後開始進行疑難排解。
   </para>
  </tip>

  <para>
   Ceph 一般情況下會進行自我修復。但如果問題仍然存在，監控 OSD 和放置群組將有助於您找到問題所在。
  </para>

  <sect2 xml:id="op-mon-osds">
   <title>監控 OSD</title>
   <para>
    OSD 可能處於<emphasis>在叢集內</emphasis> (「in」) 狀態，也可能處於<emphasis>在叢集外</emphasis> (「out」) 狀態。同時，它也可能處於<emphasis>啟用並執行中</emphasis> (「up」) 或 <emphasis>停機且未執行</emphasis> (「down」) 狀態。如果某個 OSD 處於「up」狀態，則它可能在叢集內 (您可以讀取和寫入資料)，也可能在叢集外。如果該 OSD 之前在叢集內，最近已移出叢集，則 Ceph 會將放置群組移轉至其他 OSD。如果某個 OSD 在叢集外，CRUSH 將不會為其指定放置群組。如果某個 OSD 處於「down」狀態，則它應該也處於「out」狀態。
   </para>
   <note>
    <title>狀況不良狀態</title>
    <para>
     如果某個 OSD 處於「down」和「in」狀態，則表示存在問題，並且叢集將處於狀況不良狀態。
    </para>
   </note>
   <para>
    如果您執行 <command>ceph health</command>、<command>ceph -s</command> 或 <command>ceph -w</command> 等指令，可能會注意到叢集並非永遠回應 <literal>HEALTH OK</literal>。對於 OSD，您應當預期叢集在以下情況下<emphasis>不會</emphasis>回應 <literal>HEALTH OK</literal>：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      您尚未啟動叢集 (它不會回應)。
     </para>
    </listitem>
    <listitem>
     <para>
      您剛啟動或重新啟動了叢集，但它尚未準備就緒，因為系統正在建立放置群組，並且 OSD 正在互聯。
     </para>
    </listitem>
    <listitem>
     <para>
      您剛新增或移除了某個 OSD。
     </para>
    </listitem>
    <listitem>
     <para>
      您剛修改了叢集地圖。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    監控 OSD 的一個重要目的是確定當叢集已啟用且在執行時，叢集中的所有 OSD 也已啟用且在執行。若要確定是否所有 OSD 都在執行，請執行以下指令：
   </para>
<screen>
<prompt>root # </prompt>ceph osd stat
x osds: y up, z in; epoch: eNNNN
</screen>
   <para>
    結果應顯示 OSD 總數 (x)、處於「up」狀態的 OSD 數量 (y)、處於「in」狀態的 OSD 數量 (z)，以及地圖版本編號 (eNNNN)。如果在叢集內 (「in」) 的 OSD 數量大於處於「up」狀態的 OSD 數量，請執行以下指令來確定未在執行的 <literal>ceph-osd</literal> 精靈：
   </para>
<screen>
<prompt>root # </prompt>ceph osd tree
#ID CLASS WEIGHT  TYPE NAME             STATUS REWEIGHT PRI-AFF
-1       2.00000 pool openstack
-3       2.00000 rack dell-2950-rack-A
-2       2.00000 host dell-2950-A1
0   ssd 1.00000      osd.0                up  1.00000 1.00000
1   ssd 1.00000      osd.1              down  1.00000 1.00000
</screen>
   <para>
    如果某個 OSD (例如 ID 為 1 的 OSD) 停機，請將其啟動：
   </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>sudo systemctl start ceph-osd@1.service
</screen>
   <para>
    對於與已停止或不會重新啟動的 OSD 相關的問題，請參閱<xref linkend="op-osd-not-running"/>。
   </para>
  </sect2>

  <sect2 xml:id="op-pgsets">
   <title>放置群組集</title>
   <para>
    CRUSH 向 OSD 指定放置群組時，會查看池的複本數量，然後再為 OSD 指定放置群組，以便將每個放置群組複本都指定給不同的 OSD。例如，如果池需要三個放置群組複本，CRUSH 可能會將這三個複本分別指定給 <literal>osd.1</literal>、<literal>osd.2</literal> 和 <literal>osd.3</literal>。CRUSH 實際上會尋找一種虛擬隨機放置方法，這種方法會將您在 CRUSH 地圖中設定的故障網域納入考量，因此在大型叢集中，您很少會看到放置群組指定給最鄰近的 OSD 的情況。我們將應包含特定放置群組的複本的 OSD 集稱為<emphasis>在任集</emphasis>。在某些情況下，在任集中的 OSD 會處於停機狀態，或者無法處理要存取放置群組中的物件的要求。當以下其中一種情況發生時，可能會出現這些情況：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      您新增或移除了某個 OSD。CRUSH 隨後會將放置群組重新指定給其他 OSD，因而變更了<emphasis>在任集</emphasis>的組成部分，導致系統透過「回填」程序移轉資料。
     </para>
    </listitem>
    <listitem>
     <para>
      某個 OSD 之前處於「down」狀態、之前進行了重新啟動，而現在正在復原。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>在任集</emphasis>中的某個 OSD 處於「down」狀態，或者無法處理要求，並且另一個 OSD 已暫代其職。
     </para>
     <para>
      Ceph 使用<emphasis>啟用集</emphasis>來處理用戶端要求，啟用集是實際處理要求的 OSD 集。在大多數情況下，<emphasis>啟用集</emphasis>和<emphasis>在任集</emphasis>幾乎完全相同。當兩者不同時，可能表示 Ceph 正在移轉資料、某個 OSD 正在復原，或者叢集存在問題 (例如，在此類情況下，Ceph 通常會回應 <literal>HEALTH WARN</literal> 狀態及「stuck stale」訊息)。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    若要擷取放置群組清單，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>;ceph pg dump
</screen>
   <para>
    若要檢視哪些 OSD 在給定放置群組的<emphasis>在任集</emphasis>或<emphasis>啟用集</emphasis>內，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg map<replaceable>PG_NUM</replaceable>
osdmap eNNN pg <replaceable>RAW_PG_NUM</replaceable> (<replaceable>PG_NUM</replaceable>) -&gt; up [0,1,2] acting [0,1,2]
</screen>
   <para>
    結果應該會顯示 OSD 地圖版本編號 (eNNN)、放置群組數量 (<replaceable>PG_NUM</replaceable>)、<emphasis>啟用集</emphasis> (「up」) 中的 OSD，以及<emphasis>在任集</emphasis> (「acting」) 中的 OSD：
   </para>
   <tip>
    <title>叢集問題指示器</title>
    <para>
     如果<emphasis>啟用集</emphasis>與<emphasis>在任集</emphasis>不相符，則可能表示叢集正在自行重新平衡，或者叢集可能存在問題。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="op-peering">
   <title>建立互聯</title>
   <para>
    放置群組必須處於「active」及「clean」狀態，您才能將資料寫入其中。為了讓 Ceph 確定某個放置群組的目前狀態，該放置群組的主 OSD (<emphasis>在任集</emphasis>中的第一個 OSD) 會與第二個和第三個 OSD 建立互聯，以便就放置群組的目前狀態達成一致 (假設池中包含三個放置群組複本)。
   </para>
   <figure>
    <title>互聯綱要</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="ceph_peering.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
  </sect2>

  <sect2 xml:id="op-mon-pg-states">
   <title>監控放置群組狀態</title>
   <para>
    如果您執行 <command>ceph health</command>、<command>ceph -s</command> 或 <command>ceph -w</command> 等指令，可能會注意到叢集並非永遠回應 <literal>HEALTH OK</literal> 訊息。檢查 OSD 是否正在執行之後，還應檢查放置群組狀態。
   </para>
   <para>
    在一些與放置群組互聯相關的情況下，叢集預期將<emphasis role="bold">不會</emphasis>回應 <literal>HEALTH OK</literal>：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      您剛建立了池，並且放置群組尚未互聯。
     </para>
    </listitem>
    <listitem>
     <para>
      放置群組正在復原。
     </para>
    </listitem>
    <listitem>
     <para>
      您剛新增了 OSD 至叢集，或剛從叢集中移除了 OSD。
     </para>
    </listitem>
    <listitem>
     <para>
      您剛修改了 CRUSH 地圖，並且您的放置群組正在移轉。
     </para>
    </listitem>
    <listitem>
     <para>
      在不同的放置群組複本中存在資料不一致的情況。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph 正在整理放置群組的複本。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph 的儲存容量不足，無法完成回填操作。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    如果上述其中一種情況導致 Ceph 回應 <literal>HEALTH WARN</literal>，請不要驚慌。叢集在許多情況下都會自行復原。在有些情況下，您可能需要採取措施。監控放置群組的一個重要目的是要確定當叢集已啟用且在執行時，所有放置群組都處於「active」狀態，並且最好處於「clean」狀態。若要查看所有放置群組的狀態，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg stat
x pgs: y active+clean; z bytes data, aa MB used, bb GB / cc GB avail
</screen>
   <para>
    結果應該會顯示放置群組總數 (x)、處於特定狀態 (例如「active+clean」) 的放置群組數量 (y)，以及儲存的資料量 (z)。
   </para>
   <para>
    除了放置群組狀態之外，Ceph 還會回應使用的儲存容量 (aa)、剩餘的儲存容量 (bb)，以及放置群組的總儲存容量。在以下情況下，這些數值可能非常重要：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      已達到 <option>near full ratio</option> 或 <option>full ratio</option>。
     </para>
    </listitem>
    <listitem>
     <para>
      由於您的 CRUSH 組態中存在錯誤，您的資料未在叢集中分佈。
     </para>
    </listitem>
   </itemizedlist>
   <tip>
    <title>放置群組 ID</title>
    <para>
     放置群組 ID 由池編號 (並非池名稱) 加一個句點 (.)和放置群組 ID (一個十六進位數) 組成。您可以在 <command>ceph osd lspools</command> 的輸出中檢視池編號及其名稱。例如，預設池 <literal>rbd</literal> 與池編號 0 對應。完全合格的放置群組 ID 的格式如下：
    </para>
<screen>
<replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable>
</screen>
    <para>
     通常顯示如下：
    </para>
<screen>
0.1f
</screen>
   </tip>
   <para>
    若要擷取放置群組清單，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump
</screen>
   <para>
    您還可以將輸出內容設定為 JSON 格式，並將其儲存到檔案中：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump -o <replaceable>FILE_NAME</replaceable> --format=json
</screen>
   <para>
    若要查詢特定的放置群組，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg <replaceable>POOL_NUM</replaceable>.<replaceable>PG_ID</replaceable> query
</screen>
   <para>
    以下清單詳細說明了常見的放置群組狀態。
   </para>
   <variablelist>
    <varlistentry>
     <term>CREATING (建立中)</term>
     <listitem>
      <para>
       當您建立池時，Ceph 會建立您指定數量的放置群組。Ceph 會在建立一或多個放置群組時回應「creating」。建立放置群組之後，屬於放置群組<emphasis>在任集</emphasis>的各 OSD 將會互聯。完成互聯程序時，放置群組狀態應該為「active+clean」，這表示 Ceph 用戶端可以開始向放置群組寫入資料。
      </para>
      <figure>
       <title>放置群組狀態</title>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="ceph_pg_creating.png" width="80%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </figure>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>PEERING (正在互聯)</term>
     <listitem>
      <para>
       當 Ceph 在對放置群組執行互聯操作時，會在儲存放置群組複本的各 OSD 之間就該放置群組中物件和中繼資料的狀態達成一致。當 Ceph 完成互聯程序時，便表示儲存放置群組的各 OSD 之間就放置群組的目前狀態達成一致。不過，完成互聯程序並<emphasis role="bold">不</emphasis>表示每個複本都有最新的內容。
      </para>
      <note>
       <title>權威歷程</title>
       <para>
        在<emphasis>在任集</emphasis>的所有 OSD 都持續進行寫入操作之前，Ceph 將<emphasis role="bold">不會</emphasis>向用戶端確認寫入操作。這樣做可確保在上次成功互聯之後，至少有一個<emphasis>在任集</emphasis>成員將擁有每個確認的寫入操作的記錄。
       </para>
       <para>
        透過準確記錄每個確認的寫入操作，Ceph 可以建構並擴充一個新的權威放置群組歷程，即一個完整且完全有序的操作集，如果執行該操作集，會將 OSD 的放置群組複本更新至最新狀態。
       </para>
      </note>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>ACTIVE (使用中)</term>
     <listitem>
      <para>
       當 Ceph 完成互聯程序時，放置群組可能會變為「active」狀態。「Active」狀態表示一般可在主放置群組和複本中使用放置群組中的資料來進行讀取和寫入操作。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>CLEAN (正常)</term>
     <listitem>
      <para>
       如果放置群組處於「clean」狀態，則表示主 OSD 和複本 OSD 已成功互聯，並且該放置群組沒有流浪複本。Ceph 已將放置群組中的所有物件複製正確的次數。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>DEGRADED (已降級)</term>
     <listitem>
      <para>
       當用戶端將物件寫入主 OSD 時，該主 OSD 負責將複本寫入複本 OSD。主 OSD 將物件寫入儲存空間之後，放置群組將保持「degraded」狀態，直至主 OSD 收到了複本 OSD 傳送的 Ceph 已成功建立複本物件的確認訊息。
      </para>
      <para>
       放置群組有可能處於「active+degraded」狀態，這是因為即使 OSD 尚未儲存所有物件，它也可能處於「active」狀態。如果某個 OSD 變成停機狀態，Ceph 會將指定給該 OSD 的每個放置群組都標示為「degraded」。當該 OSD 恢復啟用狀態後，各 OSD 必須再次互聯。不過，如果某個已降級放置群組處於「active」狀態，用戶端仍然可以將新物件寫入該放置群組。
      </para>
      <para>
       如果某個 OSD 處於「down」狀態，並且持續保持「degraded」狀況，Ceph 可能會將該停機的 OSD 標示為「out」(表示移出叢集)，並將停機 (「down」) 的 OSD 的資料重新對應至另一個 OSD。透過 <option>mon osd down out interval</option> 選項來控制從將 OSD 標示為「down」到將其標示為「out」相隔的時間，該選項預設設定為 600 秒。
      </para>
      <para>
       放置群組也可能處於「degraded」狀態，當 Ceph 找不到應在放置群組中的一或多個物件時，便會發生此情況。雖然您無法讀取未找到的物件或向其寫入資料，卻仍然可以存取「degraded」狀態的放置群組中的所有其他物件。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>RECOVERING (正在復原)</term>
     <listitem>
      <para>
       Ceph 設計用於在發生硬體和軟體問題時進行大規模容錯。當 OSD 變成「down」狀態時，其內容可能落後於放置群組中其他複本的目前狀態。當 OSD 恢復「up」狀態時，必須更新放置群組的內容，以反映最新狀態。在此期間，OSD 可能會顯現出「recovering」狀態。
      </para>
      <para>
       復原並非永遠都是無足輕重的，因為硬體故障可能會導致多個 OSD 發生串聯故障。例如，一個機架或機櫃的網路交換器可能會發生故障，這可能會導致一些主機的 OSD 落後於叢集的目前狀態。解決故障之後，必須復原每個 OSD。
      </para>
      <para>
       Ceph 提供了一些設定，用來平衡新服務要求與復原資料物件並將放置群組還原到最新狀態的需求之間的資源爭用。<option>osd recovery delay start</option> 設定允許 OSD 在啟動復原程序之前重新啟動、重新互聯，甚至處理一些重播要求。<option>osd recovery thread timeout</option> 用於設定線串逾時，因為有可能會有多個 OSD 交錯發生故障、重新啟動以及重新互聯。<option>osd recovery max active</option> 設定用於限制 OSD 將同時處理的復原要求數，以防止 OSD 無法處理要求。<option>osd recovery max chunk</option> 設定用於限制復原的資料區塊大小，以避免出現網路阻塞。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>BACK FILLING (正在回填)</term>
     <listitem>
      <para>
       當新 OSD 加入叢集時，CRUSH 會將叢集中 OSD 的放置群組重新指定給新增的 OSD。強制新 OSD 立即接受重新指定的放置群組可能會使新 OSD 過載。向 OSD 回填放置群組可讓此程序在背景中開始。完成回填後，新 OSD 將在準備就緒時開始處理要求。
      </para>
      <para>
       在執行回填操作期間，系統可能會顯示以下其中一種狀態：「backfill_wait」表示回填操作待處理，但尚未進行；「backfill」表示正在進行回填操作；「backfill_too_full」表示已要求進行回填操作，但由於儲存容量不足而無法完成。如果無法回填某個放置群組，則可能會將其視為「incomplete」。
      </para>
      <para>
       Ceph 提供了一些設定來管理與向某個 OSD (尤其是新 OSD) 重新指定放置群組有關的負載。<option>osd max backfills</option> 預設將向或從一個 OSD 同時進行的最大回填數設定為 10。<option>backfill full ratio</option> 允許 OSD 在接近其填滿比率 (預設為 90%) 時拒絕回填要求，並使用 <command>ceph osd set-backfillfull-ratio</command> 指令進行變更。如果某個 OSD 拒絕回填要求，<option>osd backfill retry interval</option> 可讓 OSD 重試要求 (預設在 10 秒後)。OSD 還可以設定 <option>osd backfill scan min</option> 和 <option>osd backfill scan max</option>，以管理掃描間隔 (預設值分別為 64 和 512)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>REMAPPED (已重新對應)</term>
     <listitem>
      <para>
       當用於處理放置群組的<emphasis>在任集</emphasis>發生變化時，資料會從舊<emphasis>在任集</emphasis>移轉至新<emphasis>在任集</emphasis>。新主 OSD 可能需要一段時間才能處理要求。因此，新主 OSD 可能會要求舊主 OSD 繼續處理要求，直至放置群組移轉完成。資料移轉完成時，對應將使用新<emphasis>在任集</emphasis>的主 OSD。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>STALE (過時)</term>
     <listitem>
      <para>
       儘管 Ceph 使用活動訊號來確定主機和精靈正在執行，但 <literal>ceph-osd</literal> 精靈也可能會卡住，無法及時報告統計資料 (例如，當發生暫時的網路故障時)。依預設，OSD 精靈每半秒鐘 (0.5) 報告一次其放置群組、開機及故障統計資料，這個頻率高於活動訊號閾值。如果某個放置群組<emphasis>在任集</emphasis>的主 OSD 未能向監控程式報告，或者其他 OSD 已將該主 OSD 報告為「down」，則監控程式會將該放置群組標示為「stale」。
      </para>
      <para>
       當您啟動叢集後，叢集常常會在互聯程序完成之前顯示為「stale」狀態。叢集執行一段時間之後，如果放置群組顯示為「stale」狀態，則表示這些放置群組的主 OSD 處於停機狀態，或者未向監控程式報告放置群組統計資料。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="op-pg-stuck-states">
   <title>識別存在問題的放置群組</title>
   <para>
    如前文所述，狀態不是「active+clean」的放置群組未必存在問題。一般而言，當放置群組卡住後，Ceph 的自我修復功能可能就無法運作。卡住的狀態包括：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">Unclean</emphasis> (不正常)：放置群組包含未複製所需次數的物件。這些放置群組應該正在復原。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Inactive</emphasis> (非使用中)：放置群組無法處理讀取或寫入操作，因為它們正在等待某個具有最新資料的 OSD 恢復為啟用狀態。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">Stale</emphasis> (過時)：放置群組處於未知狀態，因為代管它們的 OSD 已有一段時間未向監控程式叢集報告 (透過 <option>mon osd report timeout</option> 選項設定)。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    若要識別卡住的放置群組，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump_stuck [unclean|inactive|stale|undersized|degraded]
</screen>
  </sect2>

  <sect2 xml:id="op-pg-objectfinding">
   <title>尋找物件位置</title>
   <para>
    若要在 Ceph 物件儲存中儲存物件資料，Ceph 用戶端需要設定物件名稱並指定相關的池。Ceph 用戶端會擷取最新的叢集地圖，並且 CRUSH 演算法會計算如何將物件對應至放置群組，然後計算如何以動態方式將該放置群組指定給 OSD。若要尋找物件位置，您只需知道物件名稱和池名稱。例如：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd map <replaceable>POOL_NAME</replaceable> <replaceable>OBJECT_NAME</replaceable> [<replaceable>NAMESPACE</replaceable>]
</screen>
   <example>
    <title>尋找物件</title>
    <para>
     做為範例，我們來建立一個物件。在指令行上使用 <command>rados put</command> 指令指定物件名稱「test-object-1」、包含一些物件資料的範例檔案「testfile.txt」的路徑，以及池名稱「data」。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados put test-object-1 testfile.txt --pool=data
</screen>
    <para>
     若要確認 Ceph 物件儲存是否已儲存物件，請執行以下指令：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p data ls
</screen>
    <para>
     現在，我們來確定物件位置。Ceph 將會輸出物件的位置：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd map data test-object-1
osdmap e537 pool 'data' (0) object 'test-object-1' -&gt; pg 0.d1743484 \
(0.4) -&gt; up ([1,0], p0) acting ([1,0], p0)
</screen>
    <para>
     若要移除範例物件，只需使用 <command>rados rm</command> 指令將其刪除：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados rm test-object-1 --pool=data
</screen>
   </example>
  </sect2>
 </sect1>
 <sect1 xml:id="op-osd-not-running">
  <title>OSD 未在執行</title>

  <para>
   在正常情況下，只需重新啟動 <literal>ceph-osd</literal> 精靈便可讓 OSD 重新加入叢集並復原。
  </para>

  <sect2 xml:id="op-osd-not-start">
   <title>OSD 不啟動</title>
   <para>
    如果您啟動叢集後，某個 OSD 不啟動，請檢查以下幾項：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis role="bold">組態檔案</emphasis>：如果您無法透過執行全新安裝讓 OSD 保持執行狀態，請檢查組態檔案以確定其符合標準 (例如，使用 <literal>host</literal> 而非 <literal>hostname</literal>)。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">檢查路徑</emphasis>：檢查您的組態中的路徑以及資料和記錄的實際路徑。如果您將 OSD 資料與記錄資料分開儲存，並且您的組態檔案或實際掛接中存在錯誤，則可能無法啟動 OSD。如果要在區塊裝置上儲存記錄，則需要將記錄磁碟進行分割，並向每個 OSD 指定一個分割區。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis role="bold">檢查最大線串計數</emphasis>：如果您的某個節點具有大量 OSD，您可能會達到預設的最大線串數 (通常是 32,000)，在復原期間更是如此。您可以使用 <command>sysctl</command> 指令來增加線串數，看看將最大線串數增加至允許的最大可能線串數 (例如 4194303) 是否會有幫助：
     </para>
<screen>
<prompt>root # </prompt>sysctl -w kernel.pid_max=4194303
</screen>
     <para>
      如果透過增加最大線串計數能夠解決問題，您可以在 <filename>/etc/sysctl.conf</filename> 檔案中指定 <option>kernel.pid_max</option> 設定，以便永久使用該組態。
     </para>
<screen>
kernel.pid_max = 4194303
</screen>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="op-osd-failed">
   <title>OSD 發生故障</title>
   <para>
    當 <literal>ceph-osd</literal> 程序非正常終止時，監控程式將透過正常執行的 <literal>ceph-osd</literal> 精靈瞭解故障情況，並使用 <command>ceph health</command> 指令報告該資訊：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 1/3 in osds are down
</screen>
   <para>
    具體而言，當有 <literal>ceph-osd</literal> 程序標示為「in」和「down」時，您將收到警告。您可以透過以下指令確定已停機的 <literal>ceph-osd</literal>：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_WARN 1/3 in osds are down
osd.0 is down since epoch 23, last address 192.168.106.220:6800/11080
</screen>
   <para>
    如果發生阻止 <literal>ceph-osd</literal> 正常執行或重新啟動的磁碟故障或其他故障，<filename>/var/log/ceph</filename> 下該 OSD 的記錄檔案中應該會出現相應的錯誤訊息。
   </para>
   <para>
    如果由於活動訊號故障導致精靈停止，基礎核心檔案系統可能無法回應。請檢查 <command>dmesg</command> 指令的輸出內容，以確定是磁碟錯誤還是其他核心錯誤。
   </para>
  </sect2>

  <sect2 xml:id="op-no-disk-space">
   <title>無可用磁碟空間</title>
   <para>
    Ceph 會阻止您向填滿的 OSD 寫入資料，以避免資料遺失。在正常運作的叢集中，當叢集接近其填滿比率時，您會收到警告。<option>mon osd full ratio</option> 選項預設設為容量的 0.95 (95%)，達到該比率時，叢集會阻止用戶端寫入資料。<option>mon osd backfillfull ratio</option> 預設設為容量的 0.90 (90%)，達到該比率時，叢集會阻止回填程序開始。OSD nearfull ratio 預設設為容量的 0.85 (85%)，達到該比率時，叢集會產生狀態警告。您可以使用以下指令變更「nearfull」的值：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    在小型叢集上測試 Ceph 如何處理 OSD 故障時，通常會出現叢集填滿問題。如果有一個節點的叢集資料百分比很高，叢集很容易就會立即超過「nearfull」和「full」比率。如果您在小型叢集上測試 Ceph 如何應對 OSD 故障，則應留出充足的可用磁碟空間，並考慮使用以下指令暫時降低 OSD full ratio、OSD backfillfull ratio 和 OSD nearfull ratio：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-nearfull-ratio <replaceable>0.0 to 1.0</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-full-ratio <replaceable>0.0 to 1.0</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd set-backfillfull-ratio <replaceable>0.0 to 1.0</replaceable>
</screen>
   <para>
    可透過 <command>ceph health</command> 指令報告填滿的 Ceph OSD：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health
HEALTH_WARN 1 nearfull osd(s)
</screen>
   <para>
    或
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph health detail
HEALTH_ERR 1 full osd(s); 1 backfillfull osd(s); 1 nearfull osd(s)
osd.3 is full at 97%
osd.4 is backfill full at 91%
osd.2 is near full at 87%
</screen>
   <para>
    處理填滿的叢集的最佳方法是新增新的 Ceph OSD，以便讓叢集將資料重新分佈到新的可用儲存空間。
   </para>
   <para>
    如果 OSD 因填滿而無法啟動，您可以透過刪除已滿 OSD 中的一些放置群組目錄來刪除一些資料。
   </para>
   <important>
    <title>刪除放置群組目錄</title>
    <para>
     如果您選擇刪除某個填滿的 OSD 上的放置群組目錄，請<emphasis role="bold">不要</emphasis>刪除其他填滿的 OSD 上的同一放置群組目錄，否則可能會出現<emphasis role="bold">資料遺失</emphasis>。您<emphasis role="bold">必須</emphasis>至少在一個 OSD 上保留至少一個資料複本。
    </para>
   </important>
  </sect2>
 </sect1>
</chapter>
