<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_tiered_storage.xml" version="5.0" xml:id="cha-ceph-tiered">

 <title>快取分層</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  <emphasis>快取層</emphasis>是在用戶端與標準儲存之間實作的附加儲存層。該層用於加快存取低速硬碟中儲存的池以及糾刪碼池的速度。
 </para>
 <para>
  通常，快取分層涉及到建立一個由相對快速的儲存裝置 (例如 SSD 磁碟機) 組成且設定為充當快取層的池，以及一個由較慢但較便宜的裝置組成且設定為充當儲存層的後備池。快取池大小通常是儲存池大小的 10-20%。
 </para>
 <sect1>
  <title>分層儲存的相關術語</title>

  <para>
   快取分層可識別兩種類型的池：<emphasis>快取池</emphasis>和<emphasis>儲存池</emphasis>。
  </para>

  <tip>
   <para>
    如需池的一般資訊，請參閱<xref linkend="ceph-pools"/>。
   </para>
  </tip>

  <variablelist>
   <varlistentry>
    <term>儲存池</term>
    <listitem>
     <para>
      在 Ceph 儲存叢集中儲存一個物件的多個副本的標準副本池，或糾刪碼池 (請參閱<xref linkend="cha-ceph-erasure"/>)。
     </para>
     <para>
      儲存池有時稱為「後備」儲存或「冷」儲存。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>快取池</term>
    <listitem>
     <para>
      標準副本池，儲存在相對較小但速度較快的儲存裝置上，在 CRUSH 地圖中具有自己的規則集。
     </para>
     <para>
      快取池也稱為「熱」儲存。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-caution">
  <title>需考量的要點</title>

  <para>
   快取分層可能會<emphasis>降低</emphasis>特定工作負載的叢集效能。以下幾點指出了您需要考量的有關快取分層的幾個方面：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>取決於工作負載</emphasis>：快取能否提高效能取決於工作負載。由於將物件移入或移出快取會耗費資源，因此，如果大多數要求只涉及到少量的物件，則使用快取可能更高效。快取池的大小應該足以接收工作負載的工作集，以避免效能大幅波動。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>難以進行基準測試</emphasis>：大多數效能基準測試可能會反映使用快取分層時效能較低。原因在於，這些基準測試要求了大量的物件，而快取的「預熱」需要較長時間。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>效能可能較低</emphasis>：對於不適合進行快取分層的工作負載而言，其效能往往比不啟用快取分層的普通副本池更低。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis><systemitem>librados</systemitem> 物件列舉</emphasis>：如果應用程式直接使用 <systemitem>librados</systemitem> 並依賴於物件列舉，則快取分層可能無法依預期運作 (對於物件閘道、RBD 或 CephFS 而言，這不會造成問題)。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1>
  <title>何時使用快取分層</title>

  <para>
   在以下情況下，可考慮使用快取分層：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     您的糾刪碼池儲存在 FileStore 上，並且您需要透過 RADOS 區塊裝置來存取它們。如需 RBD 的詳細資訊，請參閱<xref linkend="ceph-rbd"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     您的糾刪碼池儲存在 FileStore 上，並且您需要透過 iSCSI 存取它們。如需 iSCSI 的詳細資訊，請參閱<xref linkend="cha-ceph-iscsi"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     您的高效能儲存數量有限，而低效能儲存眾多，您需要更快地存取儲存的資料。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec-ceph-tiered-cachemodes">
  <title>快取模式</title>

  <para>
   快取分層代理程式可處理快取層與後備儲存層之間的資料移轉。管理員可以設定如何進行這種移轉。主要有兩種方案：
  </para>

  <variablelist>
   <varlistentry>
    <term>寫回模式</term>
    <listitem>
     <para>
      在寫回模式下，Ceph 用戶端將資料寫入快取層，並從快取層接收確認回應。一段時間後，寫入快取層的資料將移轉至儲存層，並從快取層中衝洗出去。從概念上講，快取層疊加在後備儲存層的「前面」。當 Ceph 用戶端需要存放於儲存層中的資料時，快取分層代理程式會在讀取時將資料移轉至快取層，然後，資料將傳送到 Ceph 用戶端。因此，用戶端可以使用快取層執行 I/O，直到資料變為非使用中狀態。這種做法非常適合可變的資料，例如，相片或視訊編輯，或交易資料。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>唯讀模式</term>
    <listitem>
     <para>
      在唯讀模式下，Ceph 用戶端將資料直接寫入後備層。在讀取時，Ceph 將要求的物件從後備層複製到快取層。系統會根據定義的規則從快取層中移除過時物件。這種做法非常適合不可變的資料，例如，在社交網路上呈現的圖片或視訊、DNA 資料或 X 光影像，因為從可能包含過時資料的快取池中讀取資料會導致一致性很差。不要對可變的資料使用唯讀模式。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ceph-tier-erasure">
  <title>糾刪碼池和快取分層</title>

  <para>
   糾刪碼池需要的資源比複本池多。若要克服這些限制，建議在糾刪碼池的前面設定一個快取層。使用 FileStore 時需要這麼做。
  </para>

  <para>
   例如，如果 <quote>hot-storage</quote> 池由高速儲存裝置構成，則可使用以下指令來加速<xref linkend="cha-ceph-erasure-erasure-profiles"/>中建立的<quote>ecpool</quote>：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add ecpool hot-storage
<prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback
<prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay ecpool hot-storage</screen>

  <para>
   這會將充當 ecpool 池的一個層的 <quote>hot-storage</quote> 池置於寫回模式，以便每次在 ecpool 中寫入和讀取資料時實際使用的都是熱儲存，從而受益於熱儲存的靈活性和速度。
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>rbd --pool ecpool create --size 10 myvolume</screen>

  <para>
   如需快取分層的詳細資訊，請參閱<xref linkend="cha-ceph-tiered"/>。
  </para>
 </sect1>
 <sect1 xml:id="ses-tiered-storage">
  <title>設定範例分層儲存</title>

  <para>
   本節說明如何在標準硬碟 (冷儲存) 的前面設定一個高速 SSD 快取層 (熱儲存)。
  </para>

  <tip>
   <para>
    下面的範例只是用於說明，其中的設定包含存在於單個 Ceph 節點上的 SSD 部件的一個根及一條規則。
   </para>
   <para>
    在生產環境中，叢集設定一般包含熱儲存以及混合節點 (配有 SSD 和 SATA 磁碟) 的更多根和規則項目。
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     建立兩個額外的 CRUSH 規則「replicated_ssd」和「replicated_hdd」，前者用於較快的 SSD 快取裝置類別，後者用於較慢的 HDD 裝置類別：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_ssd default host ssd
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rule create-replicated replicated_hdd default host hdd
</screen>
   </step>
   <step>
    <para>
     將所有現有池都切換為使用「replicated_hdd」規則。這樣可防止 Ceph 將資料儲存到新增的 SSD 裝置：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>POOL_NAME</replaceable> crush_rule replicated_hdd
</screen>
   </step>
   <step>
    <para>
     使用 DeepSea 將該機器轉變成 Ceph 節點。依<xref linkend="salt-adding-nodes"/>所述安裝軟體並設定主機機器。我們假設此節點名為 <replaceable>node-4</replaceable>。此節點需要有 4 個 OSD 磁碟。
    </para>
<screen>[...]
host node-4 {
   id -5  # do not change unnecessarily
   # weight 0.012
   alg straw
   hash 0  # rjenkins1
   item osd.6 weight 0.003
   item osd.7 weight 0.003
   item osd.8 weight 0.003
   item osd.9 weight 0.003
}
[...]</screen>
   </step>
   <step>
    <para>
     編輯熱儲存池 (與受高速 SSD 磁碟機支援的 OSD 對應) 的 CRUSH 地圖。定義另一個包含 SSD 根節點的階層 (指令為「root ssd」)。此外，請為 SSD 變更權數並新增一個 CRUSH 規則。如需 CRUSH 地圖的詳細資訊，請參閱<xref linkend="op-crush"/>。
    </para>
    <para>
     使用 <command>getcrushmap</command> 和 <command>crushtool</command> 等指令行工具直接編輯 CRUSH 地圖：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush rm-device-class osd.6 osd.7 osd.8 osd.9
<prompt>cephadm@adm &gt; </prompt>ceph osd crush set-device-class ssd osd.6 osd.7 osd.8 osd.9
</screen>
   </step>
   <step>
    <para>
     建立用於快取分層的熱儲存池。對該池使用新的「ssd」規則：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create hot-storage 100 100 replicated ssd</screen>
   </step>
   <step>
    <para>
     使用預設的「replicated_ruleset」規則建立冷儲存池：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create cold-storage 100 100 replicated replicated_ruleset</screen>
   </step>
   <step>
    <para>
     然後，設定快取層的過程就涉及到將後備儲存池與快取池關聯，在本例中，需要將冷儲存 (即儲存池) 與熱儲存 (即快取池) 關聯：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier add cold-storage hot-storage</screen>
   </step>
   <step>
    <para>
     若要將快取模式設定為「寫回」，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier cache-mode hot-storage writeback</screen>
    <para>
     如需快取模式的詳細資訊，請參閱<xref linkend="sec-ceph-tiered-cachemodes"/>。
    </para>
    <para>
     寫回快取層疊加在後備儲存池上，因此需要執行一個額外的步驟：必須將來自儲存池的所有用戶端流量導向至快取池。例如，若要將用戶端流量直接導向至快取池，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd tier set-overlay cold-storage hot-storage</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="cache-tier-configure">
  <title>設定快取層</title>

  <para>
   可以使用多個選項來設定快取層。使用以下語法：
  </para>

<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> <replaceable>key</replaceable> <replaceable>value</replaceable></screen>

  <sect2 xml:id="ses-tiered-hitset">
   <title>命中集</title>
   <para>
    使用<emphasis>命中集</emphasis>參數可以調整<emphasis>快取池</emphasis>。Ceph 中的命中集通常是布隆過濾器，提供節省記憶體使用量的方式來追蹤已存放於快取池的物件。
   </para>
   <para>
    命中集是一個位元陣列，用於儲存對物件名稱套用的一組雜湊函數的結果。所有位元最初都設定為 <literal>0</literal>。將一個物件新增至命中集後，系統會對該物件的名稱進行雜湊，並且結果將對應在命中集中的不同位置，那時，相應位元的值會設定為 <literal>1</literal>。
   </para>
   <para>
    為了確定某個物件是否存在於快取中，將會再次對物件名稱進行雜湊。如果任一位元為 <literal>0</literal>，則表示該物件一定不在快取中，需要從冷儲存中擷取它。
   </para>
   <para>
    不同物件的結果可能會儲存在命中集的同一位置。在巧合的情況下，可能所有位元都是 <literal>1</literal>，而物件卻不在快取中。因此，處理布隆過濾器的命中集只能確定某個物件是否一定不在快取中，需要從冷儲存擷取它。
   </para>
   <para>
    一個快取池可以使用多個命中集來追蹤各期間的檔案存取。<literal>hit_set_count</literal> 設定定義所用的命中集數量，<literal>hit_set_period</literal> 定義每個命中集已使用了多長時間。該時期過期後，將使用下一個命中集。如果用盡了命中集，將會釋放最舊命中集的記憶體，並建立一個新的命中集。將 <literal>hit_set_count</literal> 和 <literal>hit_set_period</literal> 的值相乘可定義已追蹤物件存取的整個時間範圍。
   </para>
   <figure xml:id="ses-tiered-hitset-overview-bloom">
    <title>包含 3 個儲存物件的布隆過濾器</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="bloom-filter.svg" width="70%" format="SVG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="bloom-filter.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <para>
    與進行雜湊的物件數量相比，基於布隆過濾器的命中集非常節省記憶體的使用量。只需使用不到 10 個位元即可將誤報率降低到 1% 以下。可以使用 <literal>hit_set_fpp</literal> 定義誤報率。Ceph 可根據放置群組中的物件數量及誤報率自動計算命中集的大小。
   </para>
   <para>
    可以使用 <literal>min_write_recency_for_promote</literal> 和 <literal>min_read_recency_for_promote</literal> 限制快取池中所需的儲存。如果將值設定為 <literal>0</literal>，則所有物件在被讀取或寫入後，會立即提升到快取池，並且在被逐出之前會一直保持這種模式。使用大於 <literal>0</literal> 的任何值可定義在其中搜尋物件的命中集 (已依存留期排序) 的數量。如果在某個命中集中找到了該物件，會將該物件提升到快取池。請記住，對物件執行備份操作也可能導致將物件提升到快取中。如果執行值設定為「0」的完全備份，可能會導致所有資料都提升到快取層，而使用中資料會從快取層中移除。因此，依據備份策略變更此設定的做法可能會有幫助。
   </para>
   <note>
    <para>
     期間越長，<option>min_read_recency_for_promote</option> 和 <option>min_write_recency_for_promote</option> 的值越大，<systemitem class="process">ceph-osd</systemitem> 精靈耗費的 RAM 就越多。特別是，當代理程式正在衝洗或逐出快取物件時，所有 <option>hit_set_count</option> 命中集都會載入到 RAM 中。
    </para>
   </note>
   <sect3 xml:id="ceph-tier-gmt-hitset">
    <title>對命中集使用 GMT</title>
    <para>
     快取層設定包含一個稱作<emphasis>命中集</emphasis>的布隆過濾器。該過濾器測試某個物件是屬於一組熱物件還是冷物件。物件將新增至命中集，其名稱後面附有時戳。
    </para>
    <para>
     如果叢集機器位於不同的時區，且時戳根據當地時間衍生，則命中集中物件的名稱可能會包含將來或過去的時戳，致使使用者產生誤解。在最壞的情況下，物件可能根本不在命中集中。
    </para>
    <para>
     為防止這種問題發生，在新建立的快取層設定中，<option>use_gmt_hitset</option> 預設設為「1」。這樣，您便可以在建立命中集的物件名稱時，強制 OSD 使用 GMT (格林威治標準時間) 時戳。
    </para>
    <warning>
     <title>保留預設值</title>
     <para>
      不要變更 <option>use_gmt_hitset</option> 的預設值「1」。如果與此選項相關的錯誤不是因叢集設定造成，切勿手動變更此選項。否則，叢集的行為可能變得無法預測。
     </para>
    </warning>
   </sect3>
  </sect2>

  <sect2>
   <title>快取大小調整</title>
   <para>
    快取分層代理程式執行兩項主要功能：
   </para>
   <variablelist>
    <varlistentry>
     <term>衝洗</term>
     <listitem>
      <para>
       代理程式識別已修改的 (髒) 物件，並將其轉遞到儲存池以長期儲存。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>逐出</term>
     <listitem>
      <para>
       代理程式識別未曾修改的 (乾淨) 物件，並將其中最近用得最少的物件從快取中逐出。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <sect3 xml:id="cache-tier-config-absizing">
    <title>絕對大小調整</title>
    <para>
     快取分層代理程式可根據位元組總數或物件總數來衝洗或逐出物件。若要指定最大位元組數，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_bytes <replaceable>num_of_bytes</replaceable></screen>
    <para>
     若要指定最大物件數，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> target_max_objects <replaceable>num_of_objects</replaceable></screen>
    <note>
     <para>
      Ceph 無法自動確定快取池的大小，因此需要設定絕對大小。否則，衝洗和逐出功能將無法正常運作。如果您指定了這兩項限制，則一旦觸發任一閾值，快取分層代理程式即開始執行衝洗或逐出。
     </para>
    </note>
    <note>
     <para>
      僅當達到 <option>target_max_bytes</option> 或 <option>target_max_objects</option> 時，才會阻擋所有用戶端要求。
     </para>
    </note>
   </sect3>
   <sect3 xml:id="cache-tier-config-relsizing">
    <title>相對大小調整</title>
    <para>
     快取分層代理程式可以根據快取池的相對大小 (透過<xref linkend="cache-tier-config-absizing"/>中所述的 <option>target_max_bytes</option> 或 <option>target_max_objects</option> 指定) 衝洗或逐出物件。當快取池中的已修改 (髒) 物件達到特定百分比時，快取分層代理程式會將這些物件衝洗到儲存池。若要設定 <option>cache_target_dirty_ratio</option>，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_ratio <replaceable>0.0...1.0</replaceable></screen>
    <para>
     例如，如果將值設定為 0.4，則當已修改 (髒) 物件的大小達到快取池容量的 40% 時，就會開始衝洗這些物件。
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set hot-storage cache_target_dirty_ratio 0.4</screen>
    <para>
     當髒物件的大小達到容量的特定百分比時，將以更高的速度衝洗。使用 <option>cache_target_dirty_high_ratio</option>：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_dirty_high_ratio <replaceable>0.0..1.0</replaceable></screen>
    <para>
     當快取池的大小達到其容量的特定百分比時，快取分層代理程式會逐出物件，以維持可用容量。若要設定 <option>cache_target_full_ratio</option>，請執行以下指令：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_target_full_ratio <replaceable>0.0..1.0</replaceable></screen>
   </sect3>
  </sect2>

  <sect2>
   <title>快取存留期</title>
   <para>
    您可以指定在快取分層代理程式將最近修改過 (改動) 的物件衝洗到後備儲存池之前，這些物件可存留的最短時間。請注意，此指令僅在快取確實需要衝洗/逐出物件時適用：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_flush_age <replaceable>num_of_seconds</replaceable></screen>
   <para>
    您可以指定在將某個物件逐出快取層之前，該物件至少可存留的時間：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool set <replaceable>cachepool</replaceable> cache_min_evict_age <replaceable>num_of_seconds</replaceable></screen>
  </sect2>

  <sect2 xml:id="ses-tiered-hitset-examples">
   <title>範例</title>
   <sect3 xml:id="ses-tiered-hitset-examples-memory">
    <title>大型快取池和少量記憶體</title>
    <para>
     如果有大量的儲存，但只有少量的 RAM 可用，則所有物件在被存取後可立即提升到快取池。命中集保持較小的規模。下面是一組範例組態值：
    </para>
<screen>hit_set_count = 1
hit_set_period = 3600
hit_set_fpp = 0.05
min_write_recency_for_promote = 0
min_read_recency_for_promote = 0</screen>
   </sect3>
   <sect3 xml:id="ses-tiered-hitset-examples-storage">
    <title>小型快取池和大量記憶體</title>
    <para>
     如果只有少量的儲存，但可用的記憶體量相對較大，則可以將快取層設定為將有限數量的物件提升到快取池。如果有 12 個命中集，並且在 14,400 秒期限內可以使用其中每個命中集，則這些命中集總共可提供 48 小時的追蹤。如果在過去 8 小時內存取了某個物件，該物件將提升到快取池。這種情況的一組範例組態值如下：
    </para>
<screen>hit_set_count = 12
hit_set_period = 14400
hit_set_fpp = 0.01
min_write_recency_for_promote = 2
min_read_recency_for_promote = 2</screen>
   </sect3>
  </sect2>
 </sect1>
</chapter>
