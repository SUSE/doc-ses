<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha.ceph.as.cephfs">

 <title>安裝 CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編輯</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  Ceph 檔案系統 (CephFS) 是符合 POSIX 標準的檔案系統，它使用 Ceph 儲存叢集來儲存其資料。CephFS 使用與 Ceph 區塊裝置相同的叢集系統︰Ceph 物件儲存及其 S3 和 Swift API 或原生結合 (<systemitem>librados</systemitem>)。
 </para>
 <para>
  若要使用 CephFS，需要一個正在執行的 Ceph 儲存叢集，並至少要有一部正在執行的 <emphasis>Ceph 中繼資料伺服器</emphasis>。
 </para>
 <sect1 xml:id="ceph.cephfs.limitations">
  <title>支援的 CephFS 情境和指導原則</title>

  <para>
   借助 SUSE Enterprise Storage，SUSE 引入了對使用延伸和分散式元件 CephFS 的眾多情境的正式支援。本節介紹硬限制，並提供建議使用案例的指導原則。
  </para>

  <para>
   支援的 CephFS 部署必須符合以下要求︰
  </para>

  <itemizedlist>
   <listitem>
    <para>
     至少有一部中繼資料伺服器。SUSE 建議部署多個具有 MDS 角色的節點。其中只有一個節點是「主動」節點，其餘節點是「被動」節點。從用戶端掛接 CephFS 時，請記得在 <command>mount</command> 指令中指定所有 MDS 節點。
    </para>
   </listitem>
   <listitem>
    <para>
     在此版本中，CephFS 快照預設為停用狀態，不受支援。
    </para>
   </listitem>
   <listitem>
    <para>
     用戶端基於 SUSE Linux Enterprise Server 12 SP2 或 SP3，使用 <literal>cephfs</literal> 核心模組驅動程式。不支援 FUSE 模組。
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Enterprise Storage 中不支援 CephFS 配額，因為僅在 FUSE 用戶端中實作配額支援。
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS 支援 <link xlink:href="http://docs.ceph.com/docs/jewel/cephfs/file-layouts/"/> 中所述的檔案配置變更。但是，儘管檔案系統可由任何用戶端掛接，但無法將新資料池新增到現有的 CephFS 檔案系統 (<literal>ceph mds add_data_pool</literal>)。只能在檔案系統已卸載時新增這些池。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>Ceph 中繼資料伺服器</title>

  <para>
   Ceph 中繼資料伺服器 (MDS) 儲存 CephFS 的中繼資料。Ceph 區塊裝置和 Ceph 物件儲存<emphasis>不</emphasis>使用 MDS。POSIX 檔案系統使用者可透過 MDS 執行基本指令 (例如 <command>ls</command> 或 <command>find</command>)，因而不會對 Ceph 儲存叢集施加巨大負擔。
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>新增中繼資料伺服器</title>
   <para>
    您可以依據<xref linkend="ceph.install.stack"/>中所述，在啟始叢集部署期間部署 MDS；或者依據<xref linkend="salt.adding.nodes"/>中所述，將 MDS 新增到已部署的叢集。
   </para>
   <para>
    部署 MDS 後，請在部署 MDS 的伺服器的防火牆設定中允許 <literal>Ceph OSD/MDS</literal> 服務︰啟動 <literal>yast</literal>，導覽至 <menuchoice> <guimenu>Security and Users (安全性與使用者)</guimenu> <guimenu>Firewall (防火牆)</guimenu> <guimenu>Allowed Services (允許的服務)</guimenu> </menuchoice>，然後在 <guimenu>Service to Allow (要允許的服務)</guimenu> 下拉式功能表中選取 <guimenu>Ceph OSD/MDS</guimenu>。如果不允許在 Ceph MDS 節點中傳送完整流量，則即使其他操作可以正常進行，掛接檔案系統也會失敗。
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mds.config">
   <title>設定中繼資料伺服器</title>
   <para>
    您可以透過在 <filename>ceph.conf</filename> 組態檔案中插入相關的選項來微調 MDS 的行為。
   </para>
   <variablelist>
    <title>MDS 快取大小</title>
    <varlistentry>
     <term><option>mds cache memory limit</option>
     </term>
     <listitem>
      <para>
       MDS 將針對其快取強制執行軟記憶體限制 (以位元組數為單位)。管理員應使用此項設定取代舊的 <option>mds cache size</option> 設定。預設大小為 1 GB。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option>
     </term>
     <listitem>
      <para>
       MDS 快取要維護的快取保留 (記憶體或 inode)。當 MDS 開始接近其保留大小時，會撤銷用戶端狀態，直到其快取大小縮小至可還原保留為止。預設值為 0.05。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    如需 MDS 相關組態選項的詳細清單，請參閱 <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>。
   </para>
   <para>
    如需 MDS 記錄程式組態選項的詳細清單，請參閱 <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
  <title>CephFS</title>

  <para>
   部署至少包含一部 Ceph 中繼資料伺服器的正常 Ceph 儲存叢集後，可以建立並掛接 Ceph 檔案系統。請確定用戶端可連接到網路，並具有正確的驗證金鑰圈。
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>建立 CephFS</title>
   <para>
    CephFS 至少需要兩個 RADOS 池︰一個用於儲存<emphasis>資料</emphasis>，另一個用於儲存<emphasis>中繼資料</emphasis>。設定這些池時，可以考慮︰
   </para>
   <itemizedlist>
    <listitem>
     <para>
      對中繼資料池使用較高的複製層級，因為丟失此池中的任何資料都可能會導致整個檔案系統無法存取。
     </para>
    </listitem>
    <listitem>
     <para>
      對中繼資料池使用延遲較低的儲存，例如 SSD，因為在用戶端上執行檔案系統操作時，這樣可以改善使用者可發現的延遲。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    在 <filename>policy.cfg</filename> 中指定 <literal>role-mds</literal> 時，會自動建立所需的池。在設定中繼資料伺服器之前，您可以手動建立池 <literal>cephfs_data</literal> 和 <literal>cephfs_metadata</literal>，以手動調整效能。如果這些池已存在，DeepSea 將不會建立它們。
   </para>
   <para>
    如需管理池的詳細資訊，請參閱<xref linkend="ceph.pools"/>。
   </para>
   <para>
    若要使用預設設定建立兩個需要用於 CephFS 的池 (例如「cephfs_data」和「cephfs_metadata」)，請執行以下指令︰
   </para>
<screen><prompt>root # </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>root # </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    您可以使用 EC 池取代複本池。建議您僅針對低效能要求和不經常發生的隨機存取 (例如冷儲存、備份和歸檔) 使用 EC 池。EC 池中的 CephFS 關係企業，並且必須為池設定 <literal>allow_ec_overwrite</literal> 選項。您可以透過執行 <command>ceph osd pool set ec_pool allow_ec_overwrites true</command> 來設定此選項。
   </para>
   <para>
    糾刪碼會顯著增加檔案系統操作的負擔，尤其是執行小規模更新時。使用糾刪碼做為容錯機制必然會產生這種負擔。這一代價與顯著減小的儲存空間相互抵銷。
   </para>
   <para>
    建立池時，您可以使用 <command>ceph fs new</command> 指令來啟用檔案系統︰
   </para>
<screen><prompt>root # </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    例如︰
   </para>
<screen><prompt>root # </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    您可以透過列出所有可用的 CephFS 來檢查是否已建立檔案系統︰
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    建立檔案系統後，MDS 將能夠進入<emphasis>主動</emphasis>狀態。例如，在單一 MDS 系統中︰
   </para>
<screen><prompt>root # </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>更多主題</title>
    <para>
     您可以在<xref linkend="cha.ceph.cephfs"/>中找到特定任務 (例如掛接、卸載和進階 CephFS 設定) 的更多資訊。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph.cephfs.multimds">
   <title>MDS 叢集大小</title>
   <para>
    一個 CephFS 例項可由多個主動 MDS 精靈提供支援。指定給 CephFS 例項的所有主動 MDS 精靈將在彼此之間分發檔案系統的目錄樹，以此來分散並行用戶端的負載。若要為 CephFS 例項新增主動 MDS 精靈，需要一個備用待命精靈。請啟動其他精靈或使用現有待命例項。
   </para>
   <para>
    以下指令將顯示目前主動和被動 MDS 精靈的數量。
   </para>
<screen><prompt>root # </prompt>ceph mds stat</screen>
   <para>
    以下指令將檔案系統例項中的主動 MDS 的數量設定為兩個。
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    若要在更新前縮小 MDS 叢集，需要執行以下兩個步驟。首先設定 <option>max_mds</option>，以只留一個例項︰
   </para>
<screen><prompt>root # </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    然後明確停用其他主動 MDS 精靈︰
   </para>
<screen><prompt>root # </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    其中 <replaceable>rank</replaceable> 為檔案系統例項的主動 MDS 精靈的數量，範圍介於 0 到 <option>max_mds</option>-1 之間。如需其他資訊，請參閱 <link xlink:href="http://docs.ceph.com/docs/luminous/cephfs/multimds/"/>。
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.multimds_updates">
   <title>MDS 叢集和更新</title>
   <para>
    在 Ceph 更新期間，檔案系統例項上的功能旗標可能會發生變化 (通常在新增新功能時發生)。不相容的精靈 (例如舊版本) 無法與不相容的功能集搭配使用，並將拒絕啟動。這意味著更新並重新啟動一個精靈可能會導致尚未更新的其他所有精靈都將停止並拒絕啟動。出於此原因，我們建議將主動 MDS 叢集的大小縮小為一個，並在更新 Ceph 之前停止所有待命精靈。此更新程序的手動步驟如下所述︰
   </para>
   <procedure>
    <step>
     <para>
      使用 <command>zypper</command> 更新 Ceph 相關的套件。
     </para>
    </step>
    <step>
     <para>
      按上述說明將主動 MDS 叢集縮小至 1 個例項，並使用所有其他節點上的 <systemitem class="daemon">systemd</systemitem> 單元停止所有待命 MDS 精靈︰
     </para>
<screen><prompt>root # </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      然後再重新啟動其餘一個 MDS 精靈，以便使用更新的二進位檔案將其重新啟動。
     </para>
<screen><prompt>root # </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      重新啟動所有其他 MDS 精靈並重設所需的 <option>max_mds</option> 設定。
     </para>
<screen><prompt>root # </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    如果您使用 DeepSea，則在階段 0 和 4 更新
    <package>ceph</package> 套件時，它會遵循此程序。當用戶端掛接 CephFS 例項並且正在進行 I/O 操作時，可能會執行此程序。不過請注意，當主動 MDS 重新啟動時，會有一個短暫的 I/O 暫停。用戶端將自動復原。
   </para>
   <para>
    最好在更新 MDS 叢集之前盡可能減少 I/O 負載。如此閒置的 MDS 叢集將能更快地完成此更新。反之，在一個負載較重且具有多個 MDS 精靈的叢集上，必須預先減少負載以防止進行中的 I/O 超出單一 MDS 精靈的負載能力。
   </para>
  </sect2>
 </sect1>
</chapter>
