<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cephfs.xml" version="5.0" xml:id="cha-ceph-as-cephfs">

 <title>安裝 CephFS</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph 檔案系統 (CephFS) 是符合 POSIX 標準的檔案系統，它使用 Ceph 儲存叢集來儲存其資料。CephFS 使用與 Ceph 區塊裝置相同的叢集系統：Ceph 物件儲存及其 S3 和 Swift API 或原生結合 (<systemitem>librados</systemitem>)。
 </para>
 <para>
  若要使用 CephFS，需要一個正在執行的 Ceph 儲存叢集，並至少要有一部正在執行的 <emphasis>Ceph 中繼資料伺服器</emphasis>。
 </para>
 <sect1 xml:id="ceph-cephfs-limitations">
  <title>支援的 CephFS 情境和指導原則</title>

  <para>
   憑藉 SUSE Enterprise Storage 6，SUSE 引入了對分散式縮放元件 CephFS 的眾多使用情境的正式支援。本節介紹硬限制，並提供建議使用案例的指導原則。
  </para>

  <para>
   支援的 CephFS 部署必須符合以下要求：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     至少有一部中繼資料伺服器。SUSE 建議部署多個具有 MDS 角色的節點。其中只有一個節點是<literal>主動</literal>節點，其餘節點都是<literal>被動</literal>節點。從用戶端掛接 CephFS 時，請記得在 <command>mount</command> 指令中指定所有 MON 節點。
    </para>
   </listitem>
   <listitem>
    <para>
     用戶端是使用 <literal>cephfs</literal> 核心模組驅動程式的 SUSE Linux Enterprise Server 12 SP3 (或更新版本) 或 SUSE Linux Enterprise Server 15 (或更新版本)。不支援 FUSE 模組。
    </para>
   </listitem>
   <listitem>
    <para>
     SUSE Enterprise Storage 6 中支援 CephFS 定額，且可對 Ceph 檔案系統的任何子目錄設定此定額。定額可限制目錄階層中指定點下所儲存的<literal>位元組</literal>或<literal>檔案</literal>數。如需詳細資訊，請參閱 <xref linkend="cephfs-quotas"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     CephFS 支援 <xref linkend="cephfs-layouts"/> 中所述的檔案配置變更。但是，儘管檔案系統可由任何用戶端掛接，但無法將新資料池新增到現有的 CephFS 檔案系統 (<literal>ceph mds add_data_pool</literal>)。只能在檔案系統已卸載時新增這些池。
    </para>
   </listitem>
   <listitem>
     <para>
       至少有一部中繼資料伺服器。SUSE 建議部署多個具有 MDS 角色的節點。依預設，其他 MDS 精靈啟動時處於<literal>待命</literal>狀態，充當使用中 MDS 的後備。系統也支援多個使用中 MDS 精靈 (請參閱<xref linkend="ceph-cephfs-multimds"/>)。
     </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph-cephfs-mds">
  <title>Ceph 中繼資料伺服器</title>

  <para>
   Ceph 中繼資料伺服器 (MDS) 儲存 CephFS 的中繼資料。Ceph 區塊裝置和 Ceph 物件儲存<emphasis>不</emphasis>使用 MDS。POSIX 檔案系統使用者可透過 MDS 執行基本指令 (例如 <command>ls</command> 或 <command>find</command>)，因而不會對 Ceph 儲存叢集施加巨大負擔。
  </para>

  <sect2 xml:id="ceph-cephfs-mdf-add">
   <title>新增中繼資料伺服器</title>
   <para>
    您可以依據<xref linkend="ceph-install-stack"/>中所述，在啟始叢集部署期間部署 MDS；或者依據<xref linkend="salt-adding-nodes"/>中所述，將 MDS 新增到已部署的叢集。
   </para>
   <para>
    部署 MDS 後，請在部署 MDS 的伺服器的防火牆設定中允許 <literal>Ceph OSD/MDS</literal> 服務：啟動 <literal>yast</literal>，導覽至 <menuchoice> <guimenu>Security and Users (安全性與使用者)</guimenu> <guimenu>Firewall (防火牆)</guimenu> <guimenu>Allowed Services (允許的服務)</guimenu> </menuchoice>，然後在 <guimenu>Service to Allow (要允許的服務)</guimenu> 下拉式功能表中選取 <guimenu>Ceph OSD/MDS</guimenu>。如果不允許在 Ceph MDS 節點中傳送完整流量，則即使其他操作可以正常進行，掛接檔案系統也會失敗。
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-mds-config">
   <title>設定中繼資料伺服器</title>
   <para>
    您可以透過在 <filename>ceph.conf</filename> 組態檔案中插入相關的選項來微調 MDS 的行為。
   </para>
   <variablelist>
    <title>中繼資料伺服器設定</title>
    <varlistentry>
     <term>mon force standby active</term>
     <listitem>
      <para>
       如果設為「true」(預設值)，監控程式會將 standby-replay 模式強制設為 active 模式。在 <literal>[mon]</literal> 或 <literal>[global]</literal> 區段下設定。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache memory limit</option></term>
     <listitem>
      <para>
       MDS 將針對其快取強制執行軟記憶體限制 (以位元組數為單位)。管理員應使用此項設定取代舊的 <option>mds cache size</option> 設定。預設設為 1 GB。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>mds cache reservation</option></term>
     <listitem>
      <para>
       MDS 快取要維護的快取保留 (記憶體或 inode)。當 MDS 開始接近其保留大小時，會撤銷用戶端狀態，直到其快取大小縮小至可還原保留為止。預設值為 0.05。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache size</term>
     <listitem>
      <para>
       要快取的 Inode 數量。值為 0 (預設值) 表示數量不限。建議使用 <option>mds cache memory limit</option> 來限制 MDS 快取使用的記憶體數量。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds cache mid</term>
     <listitem>
      <para>
       新項目在快取 LRU 中的插入點 (從頂部插入)。預設值為 0.7。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir commit ratio</term>
     <listitem>
      <para>
       觸發 Ceph 使用完整更新而非部分更新提交的改動目錄比率。預設值為 0.5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dir max commit size</term>
     <listitem>
      <para>
       觸發 Ceph 將其分解為多個更小交易的目錄更新的大小上限。預設設為 90 MB。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds decay halflife</term>
     <listitem>
      <para>
       MDS 快取溫度的半衰期。預設值為 5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon interval</term>
     <listitem>
      <para>
       向監控程式傳送指標訊息的頻率 (以秒計)。預設值為 4。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds beacon grace</term>
     <listitem>
      <para>
       從監控程式未收到指標到 Ceph 宣告 MDS 延隔並可能將其取代相隔的時間。預設值為 15。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds blacklist interval</term>
     <listitem>
      <para>
       在 OSD 地圖中將故障 MDS 列入黑名單的期間。此設定可控制故障 MDS 精靈保留在 OSD 地圖黑名單中的時長，但無法控制管理員手動加入黑名單的項目的保留時長。例如，<command>ceph osd blacklist add</command> 指令仍將使用預設黑名單時間。預設設為 24 * 60。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds reconnect timeout</term>
     <listitem>
      <para>
       MDS 重新啟動期間等待用戶端重新連接的間隔 (以秒計)。預設值為 45。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds tick interval</term>
     <listitem>
      <para>
       MDS 執行內部週期任務的頻率。預設值為 5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dirstat min interval</term>
     <listitem>
      <para>
       嘗試避免在樹狀結構中向上傳播遞迴統計資料的最短間隔 (以秒計)。預設值為 1。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds scatter nudge interval</term>
     <listitem>
      <para>
       dirstat 變更向上傳播的速度。預設值為 5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds client prealloc inos</term>
     <listitem>
      <para>
       要為每個用戶端工作階段預先配置的 Inode 數量。預設值為 1000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds early reply</term>
     <listitem>
      <para>
       決定 MDS 是否應允許用戶端在提交到記錄前查看要求的結果。預設值為「true」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds use tmap</term>
     <listitem>
      <para>
       對目錄更新使用一般地圖。預設值為「true」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds default dir hash</term>
     <listitem>
      <para>
       用於跨目錄片段對檔案進行雜湊處理的函數。預設設為 2 (即「rjenkins」)。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log skip corrupt events</term>
     <listitem>
      <para>
       決定在記錄重播期間，MDS 是否應嘗試跳過損毀的記錄事件。預設值為「false」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max events</term>
     <listitem>
      <para>
       我們啟動修剪前記錄中的事件數上限。設為 -1 (預設值) 表示停用上限。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max segments</term>
     <listitem>
      <para>
       我們啟動修剪前記錄中的區段 (物件) 數上限。設為 -1 表示停用上限。預設值為 30。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log max expiring</term>
     <listitem>
      <para>
       同時過期的區段數上限。預設值為 20。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds log eopen size</term>
     <listitem>
      <para>
       EOpen 事件中的 Inode 數上限。預設值為 100。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal sample interval</term>
     <listitem>
      <para>
       決定目錄溫度的取樣頻率 (用於制定分區決策)。預設值為 3。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal replicate threshold</term>
     <listitem>
      <para>
       觸發 Ceph 嘗試將中繼資料複製到其他節點的溫度上限。預設值為 8000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal unreplicate threshold</term>
     <listitem>
      <para>
       觸發 Ceph 停止將中繼資料複製到其他節點的溫度下限。預設值為 0。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split size</term>
     <listitem>
      <para>
       觸發 MDS 將目錄分區分割為更小位元的目錄大小上限。預設值為 10000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split rd</term>
     <listitem>
      <para>
       觸發 Ceph 分割目錄分區的目錄讀取溫度上限。預設值為 25000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split wr</term>
     <listitem>
      <para>
       觸發 Ceph 分割目錄分區的目錄寫入溫度上限。預設值為 10000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal split bits</term>
     <listitem>
      <para>
       將目錄分區分割成的位元數。預設值為 3。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal merge size</term>
     <listitem>
      <para>
       觸發 Ceph 嘗試合併相鄰目錄分區的目錄大小下限。預設值為 50。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal interval</term>
     <listitem>
      <para>
       在 MDS 之間交換工作負載的頻率 (以秒計)。預設值為 10。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment interval</term>
     <listitem>
      <para>
       從分區可分割或合併到執行分區變更之間的延遲時間 (以秒計)。預設值為 5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment fast factor</term>
     <listitem>
      <para>
       指定分區可超出分割大小的比率，達到該比率後將跳過分區間隔，立即執行分割。預設值為 1.5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal fragment size max</term>
     <listitem>
      <para>
       觸發因 ENOSPC 錯誤而拒絕所有新項目的分區大小上限。預設值為 100000。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal idle threshold</term>
     <listitem>
      <para>
       觸發 Ceph 將子樹狀結構移轉回其父項的溫度下限。預設值為 0。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal mode</term>
     <listitem>
      <para>
       計算 MDS 負載的方法：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         0 = 混合式。
        </para>
       </listitem>
       <listitem>
        <para>
         1 = 要求率和延遲。
        </para>
       </listitem>
       <listitem>
        <para>
         2 = CPU 負載。
        </para>
       </listitem>
      </itemizedlist>
      <para>
       預設值為 0。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min rebalance</term>
     <listitem>
      <para>
       觸發 Ceph 移轉的子樹狀結構溫度下限。預設值為 0.1。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal min start</term>
     <listitem>
      <para>
       觸發 Ceph 搜尋子樹狀結構的子樹狀結構溫度下限。預設值為 0.2。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need min</term>
     <listitem>
      <para>
       可接受的目標子樹狀結構大小比率下限。預設值為 0.8。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal need max</term>
     <listitem>
      <para>
       可接受的目標子樹狀結構大小比率上限。預設值為 1.2。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal midchunk</term>
     <listitem>
      <para>
       Ceph 將移轉任何超過此目標子樹狀結構大小比率的子樹狀結構。預設值為 0.3。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal minchunk</term>
     <listitem>
      <para>
       Ceph 將忽略任何小於此目標子樹狀結構大小比率的子樹狀結構。預設值為 0.001。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal min</term>
     <listitem>
      <para>
       觸發 Ceph 從 MDS 地圖中移除舊 MDS 目標的平衡器反覆項目數下限。預設值為 5。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds bal target removal max</term>
     <listitem>
      <para>
       觸發 Ceph 從 MDS 地圖中移除舊 MDS 目標的平衡器反覆項目數上限。預設值為 10。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds replay interval</term>
     <listitem>
      <para>
       處於 standby-replay 模式 (「熱待命」) 時的記錄輪詢間隔。預設值為 1。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds shutdown check</term>
     <listitem>
      <para>
       MDS 關閉期間輪詢快取的間隔。預設值為 0。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds thrash fragments</term>
     <listitem>
      <para>
       Ceph 會將目錄隨機分區或合併。預設值為 0。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache on map</term>
     <listitem>
      <para>
       Ceph 會將 MDS 快取內容傾印到每個 MDS 地圖上的某個檔案中。預設值為「false」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds dump cache after rejoin</term>
     <listitem>
      <para>
       復原期間重新加入快取後，Ceph 會將 MDS 快取內容傾印到檔案中。預設值為「false」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for name</term>
     <listitem>
      <para>
       此設定中以名稱指定的 MDS 精靈將存在一個待命 MDS 精靈。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby for rank</term>
     <listitem>
      <para>
       具有此階層的 MDS 精靈將存在一個待命 MDS 精靈。預設值為 -1。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds standby replay</term>
     <listitem>
      <para>
       決定 Ceph MDS 精靈是否應輪詢並重播使用中 MDS 的記錄(「熱待命」)。預設值為「false」。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds min caps per client</term>
     <listitem>
      <para>
       設定一個用戶端可具有的最小功能數。預設值為 100。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds max ratio caps per client</term>
     <listitem>
      <para>
       設定處於 MDS 快取壓力期間，可重新呼叫的目前功能的最大比率。預設值為 0.8。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <variablelist>
    <title>中繼資料伺服器記錄程式設定</title>
    <varlistentry>
     <term>journaler write head interval</term>
     <listitem>
      <para>
       更新記錄標題物件的頻率。預設值為 15。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler prefetch periods</term>
     <listitem>
      <para>
       記錄重播前讀取的分割週期數。預設值為 10。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journal prezero periods</term>
     <listitem>
      <para>
       寫入位置前置零的分割週期數。預設值為 10。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch interval</term>
     <listitem>
      <para>
       人為導致的最大額外延遲時間 (以秒計)。預設值為 0.001。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>journaler batch max</term>
     <listitem>
      <para>
       一次延遲衝洗的最大位元組數。預設值為 0。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs">
  <title>CephFS</title>

  <para>
   部署至少包含一部 Ceph 中繼資料伺服器的正常 Ceph 儲存叢集後，可以建立並掛接 Ceph 檔案系統。請確定用戶端可連接到網路，並具有正確的驗證金鑰圈。
  </para>

  <sect2 xml:id="ceph-cephfs-cephfs-create">
   <title>建立 CephFS</title>
   <para>
    CephFS 至少需要兩個 RADOS 池：一個用於儲存<emphasis>資料</emphasis>，另一個用於儲存<emphasis>中繼資料</emphasis>。設定這些池時，可以考慮：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      對中繼資料池使用較高的複製層級，因為丟失此池中的任何資料都可能會導致整個檔案系統無法存取。
     </para>
    </listitem>
    <listitem>
     <para>
      對中繼資料池使用延遲較低的儲存，例如 SSD，因為在用戶端上執行檔案系統操作時，這樣可以改善使用者可發現的延遲。
     </para>
    </listitem>
   </itemizedlist>
   <para>
    在 <filename>policy.cfg</filename> 中指定 <literal>role-mds</literal> 時，會自動建立所需的池。在設定中繼資料伺服器之前，您可以手動建立池 <literal>cephfs_data</literal> 和 <literal>cephfs_metadata</literal>，以手動調整效能。如果這些池已存在，DeepSea 將不會建立它們。
   </para>
   <para>
    如需管理池的詳細資訊，請參閱<xref linkend="ceph-pools"/>。
   </para>
   <para>
    若要使用預設設定建立兩個需要用於 CephFS 的池 (例如「cephfs_data」和「cephfs_metadata」)，請執行以下指令：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
<prompt>cephadm@adm &gt; </prompt>ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    您可以使用 EC 池取代複本池。建議您僅針對低效能要求和不經常發生的隨機存取 (例如冷儲存、備份和歸檔) 使用 EC 池。EC 池中的 CephFS 關係企業，並且必須為池設定 <literal>allow_ec_overwrite</literal> 選項。您可以透過執行 <command>ceph osd pool set ec_pool allow_ec_overwrites true</command> 來設定此選項。
   </para>
   <para>
    糾刪碼會顯著增加檔案系統操作的負擔，尤其是執行小規模更新時。使用糾刪碼做為容錯機制必然會產生這種負擔。這一代價與顯著減小的儲存空間相互抵銷。
   </para>
   <para>
    建立池時，您可以使用 <command>ceph fs new</command> 指令來啟用檔案系統：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    例如：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    您可以透過列出所有可用的 CephFS 來檢查是否已建立檔案系統：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> <option>fs ls</option>
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    建立檔案系統後，MDS 將能夠進入<emphasis>使用中</emphasis>狀態。例如，在單一 MDS 系統中：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> <option>mds stat</option>
e5: 1/1/1 up</screen>
   <tip>
    <title>更多主題</title>
    <para>
     您可以在<xref linkend="cha-ceph-cephfs"/>中找到特定任務 (例如掛接、卸載和進階 CephFS 設定) 的更多資訊。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds">
   <title>MDS 叢集大小</title>
   <para>
    一個 CephFS 例項可由多個使用中 MDS 精靈提供支援。指定給 CephFS 例項的所有使用中 MDS 精靈將在彼此之間分發檔案系統的目錄樹，以此來分散並行用戶端的負載。若要為 CephFS 例項新增使用中 MDS 精靈，需要一個備用待命精靈。請啟動其他精靈或使用現有待命例項。
   </para>
   <para>
    以下指令將顯示目前主動和被動 MDS 精靈的數量。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mds stat</screen>
   <para>
    以下指令在檔案系統例項中將使用中 MDS 的數量設定為兩個。
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 2</screen>
   <para>
    若要在更新前縮小 MDS 叢集，需要執行以下兩個步驟。首先，將 <option>max_mds</option> 設定為只保留一個例項：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>fs_name</replaceable> max_mds 1</screen>
   <para>
    然後明確停用另一個使用中 MDS 精靈：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mds deactivate <replaceable>fs_name</replaceable>:<replaceable>rank</replaceable></screen>
   <para>
    其中 <replaceable>rank</replaceable> 為檔案系統例項的使用中 MDS 精靈的數量，範圍介於 0 到 <option>max_mds</option>-1 之間。
   </para>
   <para>
    我們建議至少保留一個 MDS 做為待命精靈。
   </para>
  </sect2>

  <sect2 xml:id="ceph-cephfs-multimds-updates">
   <title>MDS 叢集和更新</title>
   <para>
    在 Ceph 更新期間，檔案系統例項上的功能旗標可能會發生變化 (通常在新增新功能時發生)。不相容的精靈 (例如舊版本) 無法與不相容的功能集搭配使用，並將拒絕啟動。這意味著更新並重新啟動一個精靈可能會導致尚未更新的其他所有精靈都將停止並拒絕啟動。出於此原因，我們建議將使用中 MDS 叢集縮減為僅包含一個例項，並在更新 Ceph 之前停止所有待命精靈。此更新程序的手動步驟如下所述：
   </para>
   <procedure>
    <step>
     <para>
      使用 <command>zypper</command> 更新 Ceph 相關的套件。
     </para>
    </step>
    <step>
     <para>
      依上述說明將使用中 MDS 叢集縮減至一個例項，並在所有其他節點上使用所有待命 MDS 精靈的 <systemitem class="daemon">systemd</systemitem> 單位將它們停止：
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl stop ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      然後再重新啟動其餘一個 MDS 精靈，以便使用更新的二進位檔案將其重新啟動。
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl restart ceph-mds\*.service ceph-mds.target</screen>
    </step>
    <step>
     <para>
      重新啟動所有其他 MDS 精靈，並重設所需的 <option>max_mds</option> 設定。
     </para>
<screen><prompt>cephadm@mds &gt; </prompt>systemctl start ceph-mds.target</screen>
    </step>
   </procedure>
   <para>
    如果您使用 DeepSea，則在階段 0 和 4 中更新
    <package>ceph</package> 套件時，它會遵循此程序。當用戶端掛接 CephFS 例項並且正在進行 I/O 操作時，可能會執行此程序。不過請注意，當使用中 MDS 重新啟動時，會有一個短暫的 I/O 暫停。用戶端將自動復原。
   </para>
   <para>
    最好在更新 MDS 叢集之前盡可能減少 I/O 負載。如此閒置的 MDS 叢集將能更快地完成此更新。反之，在一個負載較重且具有多個 MDS 精靈的叢集上，必須預先減少負載以防止進行中的 I/O 超出單一 MDS 精靈的負載能力。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-layouts">
   <title>檔案配置</title>
   <para>
    檔案的配置可控制其內容對應至 Ceph RADOS 物件的方式。您可以使用<emphasis>虛擬延伸屬性</emphasis>或 <emphasis>xattrs</emphasis> 短暫讀取和寫入檔案的配置。
   </para>
   <para>
    配置 xattrs 的名稱取決於檔案是一般檔案還是目錄。一般檔案的配置 xattrs 名為 <literal>ceph.file.layout</literal>，而目錄的配置 xattrs 名為 <literal>ceph.dir.layout</literal>。範例中將使用 <literal>ceph.file.layout</literal> 名稱，處理目錄時則相應地以 <literal>.dir.</literal> 部分取代。
   </para>
   <sect3>
    <title>配置欄位</title>
    <para>
     系統可識別下列屬性欄位：
    </para>
    <variablelist>
     <varlistentry>
      <term>pool</term>
      <listitem>
       <para>
        將在其中儲存檔案資料物件的 RADOS 池的 ID 或名稱。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>pool_namespace</term>
      <listitem>
       <para>
        資料池內物件將寫入其中的 RADOS 名稱空間。預設為空白，表示使用預設名稱空間。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_unit</term>
      <listitem>
       <para>
        在檔案的 RAID 0 分佈中使用的資料區塊大小 (以位元組計)。檔案的所有分割單位大小都相同。最後一個分割單位通常不完整，它代表檔案末尾的資料，以及檔案末尾至固定大小的分割單位末端未使用的「空間」。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>stripe_count</term>
      <listitem>
       <para>
        構成檔案資料的 RAID 0「分割」的連續分割單位數量。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>object_size</term>
      <listitem>
       <para>
        用於儲存檔案資料區塊的 RADOS 物件的大小 (以位元組計)。
       </para>
       <tip>
        <title>物件大小</title>
        <para>
         RADOS 會強制施加一個可設定的物件大小上限。如果您將 CephFS 物件大小增加到超出該上限的值，則寫入將失敗。OSD 設定為 <option>osd_max_object_size</option>，預設值為 128 MB。RADOS 物件太大可能會妨礙叢集順利運作，因此不建議將物件大小上限指定為超出預設值的數值。
        </para>
       </tip>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3>
    <title>使用 <command>getfattr</command> 讀取配置</title>
    <para>
     使用 <command>getfattr</command> 指令可讀取範例檔案 <filename>file</filename> 的配置資訊，並將其輸出為單個字串：
    </para>
<screen>
<prompt>root # </prompt>touch file
<prompt>root # </prompt>getfattr -n ceph.file.layout file
# file: file
ceph.file.layout="stripe_unit=4194304 stripe_count=1 object_size=419430
</screen>
    <para>
     讀取個別配置欄位：
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.file.layout.pool file
# file: file
ceph.file.layout.pool="cephfs_data"
<prompt>root # </prompt>getfattr -n ceph.file.layout.stripe_unit file
# file: file
ceph.file.layout.stripe_unit="4194304"
</screen>
    <tip>
     <title>池 ID 或名稱</title>
     <para>
      讀取配置時，通常使用名稱指示池。但在極少數情況下，當剛剛建立池時，可能會輸出 ID 而非名稱。
     </para>
    </tip>
    <para>
     目錄沒有明確配置，除非對其進行了自訂。如果從未修改過配置，則嘗試讀取配置的操作將失敗：這表示系統將使用具有明確配置的上一層目錄的配置。
    </para>
<screen>
<prompt>root # </prompt>mkdir dir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
dir: ceph.dir.layout: No such attribute
<prompt>root # </prompt>setfattr -n ceph.dir.layout.stripe_count -v 2 dir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>使用 <command>setfattr</command> 寫入配置</title>
    <para>
     使用 <command>setfattr</command> 指令可修改範例檔案 <command>file</command> 的配置欄位：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd lspools
0 rbd
1 cephfs_data
2 cephfs_metadata
<prompt>root # </prompt>setfattr -n ceph.file.layout.stripe_unit -v 1048576 file
<prompt>root # </prompt>setfattr -n ceph.file.layout.stripe_count -v 8 file
# Setting pool by ID:
<prompt>root # </prompt>setfattr -n ceph.file.layout.pool -v 1 file
# Setting pool by name:
<prompt>root # </prompt>setfattr -n ceph.file.layout.pool -v cephfs_data file
</screen>
    <note>
     <title>清空檔案</title>
     <para>
      使用 <command>setfattr</command> 修改檔案的配置欄位時，需要清空此檔案，否則會發生錯誤。
     </para>
    </note>
   </sect3>
   <sect3>
    <title>清空配置</title>
    <para>
     如果您要從範例目錄 <filename>mydir</filename> 中移除明確配置，並回復為繼承上一層目錄的配置，請執行以下指令：
    </para>
<screen>
<prompt>root # </prompt>setfattr -x ceph.dir.layout mydir
</screen>
    <para>
     同樣，如果您設定了「pool_namespace」屬性，現在希望修改配置以轉為使用預設名稱空間，請執行以下指令：
    </para>
<screen>
# Create a directory and set a namespace on it
<prompt>root # </prompt>mkdir mydir
<prompt>root # </prompt>setfattr -n ceph.dir.layout.pool_namespace -v foons mydir
<prompt>root # </prompt>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a pool_namespace=foons"

# Clear the namespace from the directory's layout
<prompt>root # </prompt>setfattr -x ceph.dir.layout.pool_namespace mydir
<prompt>root # </prompt>getfattr -n ceph.dir.layout mydir
ceph.dir.layout="stripe_unit=4194304 stripe_count=1 object_size=4194304 \
 pool=cephfs_data_a"
</screen>
   </sect3>
   <sect3>
    <title>配置的繼承</title>
    <para>
     檔案在建立時會繼承其父目錄的配置。但在此之後，對父目錄配置所做的變更不會影響其子項：
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# file1 inherits its parent's layout
<prompt>root # </prompt>touch dir/file1
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# update the layout of the directory before creating a second file
<prompt>root # </prompt>setfattr -n ceph.dir.layout.stripe_count -v 4 dir
<prompt>root # </prompt>touch dir/file2

# file1's layout is unchanged
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file1
# file: dir/file1
ceph.file.layout="stripe_unit=4194304 stripe_count=2 object_size=4194304 \
 pool=cephfs_data"

# ...while file2 has the parent directory's new layout
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/file2
# file: dir/file2
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
    <para>
     如果未為中間目錄設定配置，則所建立做為目錄後代的檔案也將繼承目錄的配置：
    </para>
<screen>
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir
# file: dir
ceph.dir.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
<prompt>root # </prompt>mkdir dir/childdir
<prompt>root # </prompt>getfattr -n ceph.dir.layout dir/childdir
dir/childdir: ceph.dir.layout: No such attribute
<prompt>root # </prompt>touch dir/childdir/grandchild
<prompt>root # </prompt>getfattr -n ceph.file.layout dir/childdir/grandchild
# file: dir/childdir/grandchild
ceph.file.layout="stripe_unit=4194304 stripe_count=4 object_size=4194304 \
 pool=cephfs_data"
</screen>
   </sect3>
   <sect3>
    <title>將資料池新增至中繼資料伺服器</title>
    <para>
     若要對 CephFS 使用池，需要先將池新增至中繼資料伺服器：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs add_data_pool cephfs cephfs_data_ssd
<prompt>cephadm@adm &gt; </prompt>ceph fs ls  # Pool should now show up
.... data pools: [cephfs_data cephfs_data_ssd ]
</screen>
    <tip>
     <title>cephx 金鑰</title>
     <para>
      請確定您的 cephx 金鑰允許用戶端存取此新池。
     </para>
    </tip>
    <para>
     然後，您可以在 CephFS 中更新目錄的配置，以使用您新增的池：
    </para>
<screen>
<prompt>root # </prompt>mkdir /mnt/cephfs/myssddir
<prompt>root # </prompt>setfattr -n ceph.dir.layout.pool -v cephfs_data_ssd /mnt/cephfs/myssddir
</screen>
    <para>
     所有在該目錄中建立的新檔案現在都將繼承該目錄的配置，並會將其資料存放到新增的池中。您可能會發現主要資料池中的物件數量會持續增加，即使檔案是在您剛才新增的池中建立的也是如此。這是正常現象：檔案資料會儲存到配置所指定的池中，但仍會在主要資料池中為所有檔案儲存少量中繼資料。
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
