<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_nfsganesha.xml" version="5.0" xml:id="cha-ceph-nfsganesha">

 <title>NFS Ganesha：透過 NFS 輸出 Ceph 資料</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  NFS Ganesha 是一部 NFS 伺服器 (請參閱<link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-nfs.html">與 NFS 共用檔案系統</link>)，它在使用者位址空間中執行，而不是做為作業系統核心的一部分執行。憑藉 NFS Ganesha，您可以插入自己的儲存機制 (例如 Ceph)，並從任何 NFS 用戶端存取它。
 </para>
 <para>
  系統依使用者將 S3 桶輸出到 NFS，例如，透過路徑 <filename><replaceable>GANESHA_NODE:</replaceable>/<replaceable>USERNAME</replaceable>/<replaceable>BUCKETNAME</replaceable></filename> 輸出。
 </para>
 <para>
  預設透過路徑 <filename><replaceable>GANESHA_NODE:</replaceable>/cephfs</filename> 輸出 CephFS。
 </para>
 <note>
  <title>NFS Ganesha 效能</title>
  <para>
   由於用戶端與儲存區之間的額外網路躍程會導致通訊協定負擔增加並產生額外的延遲，因此與使用原生 CephFS 或物件閘道用戶端相比，透過 NFS 閘道存取 Ceph 可能會使應用程式效能大幅降低。
  </para>
 </note>
 <sect1 xml:id="ceph-nfsganesha-install">
  <title>安裝</title>

  <para>
   如需安裝說明，請參閱<xref linkend="cha-as-ganesha"/>。
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-config">
  <title>組態</title>

  <para>
   如需可在組態檔案中使用的所有參數的清單，請參閱：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     <command>man ganesha-config</command>
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-ceph-config</command>，用於 CephFS 檔案系統抽象層 (FSAL) 選項。
    </para>
   </listitem>
   <listitem>
    <para>
     <command>man ganesha-rgw-config</command>，用於物件閘道 FSAL 選項。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   本節包含的資訊可協助您設定 NFS Ganesha 伺服器，以輸出可透過物件閘道和 CephFS 存取的叢集資料。
  </para>

  <para>
   NFS Ganesha 組態包含兩個部分：服務組態和輸出組態。透過 <filename>/etc/ganesha/ganesha.conf</filename> 來控制服務組態。注意，在執行 DeepSea 階段 4 時將會覆寫對此檔案所做的變更。若要永久變更這些設定，請編輯位於 Salt Master 上的檔案 <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>。輸出組態以 RADOS 物件的形式儲存在 Ceph 叢集中。
  </para>

  <sect2 xml:id="ceph-nfsganesha-config-service-general">
   <title>服務組態</title>
   <para>
    服務組態儲存在 <filename>/etc/ganesha/ganesha.conf</filename> 中，用於控制所有 NFS Ganesha 精靈設定，包括 Ceph 叢集中儲存輸出組態的位置。注意，在執行 DeepSea 階段 4 時將會覆寫對此檔案所做的變更。若要永久變更這些設定，請編輯位於 Salt Master 上的檔案 <filename>/srv/salt/ceph/ganesha/files/ganesha.conf.j2</filename>。
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-service-rados">
    <title>RADOS_URLS 區段</title>
    <para>
     <literal>RADOS_URLS</literal> 區段用於設定為了從 RADOS 物件讀取 NFS Ganesha 組態所需進行的 Ceph 叢集存取。
    </para>
<screen>RADOS_URLS {
  Ceph_Conf = /etc/ceph/ceph.conf;

  UserId = "ganesha.<replaceable>MINION_ID</replaceable>";
  watch_url = "rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable>";
}</screen>
    <variablelist>
     <varlistentry>
      <term>Ceph_Conf</term>
      <listitem>
       <para>
        Ceph 組態檔案路徑位置。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>UserId</term>
      <listitem>
       <para>
        CephX 使用者 ID。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>watch_url</term>
      <listitem>
       <para>
        用於監看重新載入通知的 RADOS 物件 URL。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-rgw">
    <title>RGW 區段</title>
<screen>RGW {
  ceph_conf = "/etc/ceph/ceph.conf";
  name = "name";
  cluster = "ceph";
}</screen>
    <variablelist>
     <varlistentry>
      <term>ceph_conf</term>
      <listitem>
       <para>
        指向 <filename>ceph.conf</filename> 檔案。與 DeepSea 一起部署時，不需要變更此值。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>name</term>
      <listitem>
       <para>
        NFS Ganesha 使用的 Ceph 用戶端使用者名稱。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>cluster</term>
      <listitem>
       <para>
        Ceph 叢集的名稱。SUSE Enterprise Storage 6 目前僅支援一個叢集名稱，預設為 <literal>ceph</literal>。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-service-url">
    <title>RADOS 物件 URL</title>
<screen>%url rados://<replaceable>RADOS_POOL</replaceable>/ganesha/conf-<replaceable>MINION_ID</replaceable></screen>
    <para>
     NFS Ganesha 支援從 RADOS 物件讀取組態。使用 <literal>%url</literal> 指令可指定用於識別 RADOS 物件位置的 RADOS URL。
    </para>
    <para>
     RADOS URL 可採用以下兩種格式：<literal>rados://&lt;POOL&gt;/&lt;OBJECT&gt;</literal> 或 <literal>rados://&lt;POOL&gt;/&lt;NAMESPACE&gt;/&lt;OBJECT&gt;</literal>，其中 <literal>POOL</literal> 是儲存物件的 RADOS 池，<literal>NAMESPACE</literal> 是儲存物件的池名稱空間，<literal>OBJECT</literal> 是物件名稱。
    </para>
    <para>
     為了支援 Ceph Dashboard 的 NFS Ganesha 管理功能，您需要遵循每個服務精靈的 RADOS 物件的命名慣例。物件名稱必須採用 <literal>conf-<replaceable>MINION_ID</replaceable></literal> 格式，其中 MINION_ID 是正在執行此服務的節點的 Salt Minion ID。
    </para>
    <para>
     DeepSea 已負責執行正確產生此 URL 的工作，您無需再進行任何變更。
    </para>
   </sect3>
   <sect3 xml:id="ganesha-nfsport">
    <title>變更預設 NFS Ganesha 連接埠</title>
    <para>
     NFS Ganesha 預設使用連接埠 2049 提供 NFS 支援，使用 875 提供 rquota 支援。若要變更預設連接埠號碼，請在 <literal>NFS_CORE_PARAM</literal> 區段中使用 <option>NFS_Port</option> 和 <option>RQUOTA_Port</option> 選項，例如：
    </para>
<screen>
NFS_CORE_PARAM
{
NFS_Port = 2060;
RQUOTA_Port = 876;
}
</screen>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-config-exports-general">
   <title>輸出組態</title>
   <para>
    輸出組態以 RADOS 物件的形式儲存在 Ceph 叢集中。每個輸出區塊都儲存在各自名為 <literal>export-&lt;id&gt;</literal> 的 RADOS 物件中，其中，<literal>&lt;id&gt;</literal> 必須與輸出組態的 <literal>Export_ID</literal> 屬性相符。透過 <literal>conf-MINION_ID</literal> 物件來實現輸出與 NFS Ganesha 服務之間的關聯。每個服務物件包含該服務所輸出的每個輸出的 RADOS URL 清單。輸出區塊如下所示：
   </para>
<screen>EXPORT
{
  Export_Id = 1;
  Path = "/";
  Pseudo = "/";
  Access_Type = RW;
  Squash = No_Root_Squash;
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
   <para>
    若要為上面的輸出區塊建立 RADOS 物件，我們需要先將輸出區塊代碼儲存到一個檔案中。然後可以使用 RADOS CLI 工具將之前所儲存檔案的內容儲存到 RADOS 物件中。
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>rados -p <replaceable>POOL</replaceable> -N <replaceable>NAMESPACE</replaceable> put export-<replaceable>EXPORT_ID</replaceable> <replaceable>EXPORT_FILE</replaceable>
</screen>
   <para>
    建立輸出物件後，我們便可將輸出與服務例項進行關聯，方法是將輸出物件的相應 RADOS URL 新增至服務物件。下面的章節介紹如何設定輸出區塊。
   </para>
   <sect3 xml:id="ceph-nfsganesha-config-general-export">
    <title>Export 主區段</title>
    <variablelist>
     <varlistentry>
      <term>Export_Id</term>
      <listitem>
       <para>
        每個輸出項都需有唯一的「Export_Id」(強制)。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Path</term>
      <listitem>
       <para>
        相關 CephFS 池中的輸出項路徑 (強制)。允許從 CephFS 中輸出子目錄。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Pseudo</term>
      <listitem>
       <para>
        目標 NFS 輸出項路徑 (對於 NFSv4 為強制)。它定義在哪個 NFS 輸出項路徑下可獲得輸出的資料。
       </para>
       <para>
        範例：使用值 <literal>/cephfs/</literal> 並執行
       </para>
<screen>
<prompt>root # </prompt>mount <replaceable>GANESHA_IP</replaceable>:/cephfs/ /mnt/
</screen>
       <para>
        之後，將可在用戶端上的 <filename>/mnt/cephfs/</filename> 目錄中獲得 CephFS 資料。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Access_Type</term>
      <listitem>
       <para>
        「RO」表示唯讀存取權，「RW」表示讀取寫入存取權，「None」表示禁止存取。
       </para>
       <tip>
        <title>限制用戶端的存取</title>
        <para>
         如果您在主 <literal>EXPORT</literal> 區段保留 <literal>Access_Type = RW</literal>，並在 <literal>CLIENT</literal> 區段限制某個特定用戶端的存取，其他用戶端仍可連接。若要禁止所有用戶端的存取，而只允許特定用戶端進行存取，請在 <literal>EXPORT</literal> 區段設定 <literal>Access_Type = None</literal>，然後在 <literal>CLIENT</literal> 區段為一或多個用戶端指定限制較少的存取模式：
        </para>
<screen>
EXPORT {

	FSAL {
 access_type = "none";
 [...]
 }

 CLIENT {
		clients = 192.168.124.9;
		access_type = "RW";
		[...]
 }
[...]
}
</screen>
       </tip>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Squash</term>
      <listitem>
       <para>
        NFS squash 選項。
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>FSAL</term>
      <listitem>
       <para>
        輸出「檔案系統抽象層」。請參閱<xref linkend="ceph-nfsganesha-config-general-fsal"/>。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="ceph-nfsganesha-config-general-fsal">
    <title>FSAL 子區段</title>
<screen>EXPORT
{
  [...]
  FSAL {
    Name = CEPH;
  }
}</screen>
    <variablelist>
     <varlistentry>
      <term>Name</term>
      <listitem>
       <para>
        定義 NFS Ganesha 使用的後端。允許的值為 <literal>CEPH</literal> (表示 CephFS) 或 <literal>RGW</literal> (表示物件閘道)。根據您的選擇，必須在 <filename>policy.cfg</filename> 中定義 <literal>role-mds</literal> 或 <literal>role-rgw</literal>。
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-customrole">
  <title>自訂 NFS Ganesha 角色</title>

  <para>
   可為叢集節點定義自訂 NFS Ganesha 角色。然後可在 <filename>policy.cfg</filename> 中將這些角色指定給節點。角色允許：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     分別使用不同的 NFS Ganesha 節點來存取物件閘道和 CephFS。
    </para>
   </listitem>
   <listitem>
    <para>
     將不同的物件閘道使用者指定給 NFS Ganesha 節點。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   擁有不同的物件閘道使用者可讓 NFS Ganesha 節點存取不同的 S3 桶。S3 桶可用於進行存取控制。注意：不要將 S3 桶與 CRUSH 地圖中使用的 Ceph 桶混淆。
  </para>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-multiusers">
   <title>NFS Ganesha 的不同物件閘道使用者</title>
   <para>
    下面針對 Salt Master 的範例程序展示如何建立兩個具有不同物件閘道使用者的 NFS Ganesha 角色。在此範例中，使用了角色 <literal>gold</literal> 和 <literal>silver</literal>，DeepSea 已經提供了它們的範例組態檔案。
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-rgw-multiusers">
    <step>
     <para>
      使用您選擇的編輯器開啟 <filename>/srv/pillar/ceph/stack/global.yml</filename> 檔案。如果該檔案不存在，請予以建立。
     </para>
    </step>
    <step>
     <para>
      該檔案需要包含以下數行：
     </para>
<screen>rgw_configurations:
  - rgw
  - silver
  - gold
ganesha_configurations:
  - silver
  - gold</screen>
     <para>
      稍後可以在 <filename>policy.cfg</filename> 中指定這些角色。
     </para>
    </step>
    <step>
     <para>
      建立 <filename>/srv/salt/ceph/rgw/users/users.d/gold.yml</filename> 檔案並新增以下內容：
     </para>
<screen>- { uid: "gold1", name: "gold1", email: "gold1@demo.nil" }</screen>
     <para>
      建立 <filename>/srv/salt/ceph/rgw/users/users.d/silver.yml</filename> 檔案並新增以下內容：
     </para>
<screen>- { uid: "silver1", name: "silver1", email: "silver1@demo.nil" }</screen>
    </step>
    <step>
     <para>
      現在，需要為每個角色建立 <filename>ganesha.conf</filename> 的範本。使用 DeepSea 的原始範本是較佳的做法。建立兩個副本：
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 silver.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 gold.conf.j2</screen>
    </step>
    <step>
     <para>
      新角色需要金鑰圈來存取叢集。若要提供存取權，請複製 <filename>ganesha.j2</filename>：
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      複製物件閘道的金鑰圈：
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/rgw/files/
<prompt>root@master # </prompt><command>cp</command> rgw.j2 silver.j2
<prompt>root@master # </prompt><command>cp</command> rgw.j2 gold.j2</screen>
    </step>
    <step>
     <para>
      物件閘道還需要不同角色的組態：
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/configuration/files/
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw silver.conf
<prompt>root@master # </prompt><command>cp</command> ceph.conf.rgw gold.conf</screen>
    </step>
    <step>
     <para>
      在 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 中將新建立的角色指定給叢集節點：
     </para>
<screen>role-silver/cluster/<replaceable>NODE1</replaceable>.sls
role-gold/cluster/<replaceable>NODE2</replaceable>.sls
 </screen>
     <para>
      分別以要將角色指定給的節點名稱取代 <replaceable>NODE1</replaceable> 和 <replaceable>NODE2</replaceable>。
     </para>
    </step>
    <step>
     <para>
      執行 DeepSea 階段 0 到 4。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-nfsganesha-customrole-rgw-cephfs">
   <title>分隔 CephFS 和物件閘道 FSAL</title>
   <para>
    下面針對 Salt Master 的範例程序展示如何建立使用 CephFS 和物件閘道的兩個不同的新角色：
   </para>
   <procedure xml:id="proc-ceph-nfsganesha-customrole">
    <step>
     <para>
      使用您選擇的編輯器開啟檔案 <filename>/srv/pillar/ceph/rgw.sls</filename>。如果該檔案不存在，請予以建立。
     </para>
    </step>
    <step>
     <para>
      該檔案需要包含以下數行：
     </para>
<screen>rgw_configurations:
  ganesha_cfs:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }
  ganesha_rgw:
    users:
      - { uid: "demo", name: "Demo", email: "demo@demo.nil" }

ganesha_configurations:
  - ganesha_cfs
  - ganesha_rgw</screen>
     <para>
      稍後可以在 <filename>policy.cfg</filename> 中指定這些角色。
     </para>
    </step>
    <step>
     <para>
      現在，需要為每個角色建立 <filename>ganesha.conf</filename> 的範本。使用 DeepSea 的原始範本是較佳的做法。建立兩個副本：
     </para>
<screen><prompt>root@master # </prompt><command>cd</command> /srv/salt/ceph/ganesha/files/
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_rgw.conf.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.conf.j2 ganesha_cfs.conf.j2</screen>
    </step>
    <step>
     <para>
      編輯 <filename>ganesha_rgw.conf.j2</filename>，移除以下區段：
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles='mds') != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      編輯 <filename>ganesha_cfs.conf.j2</filename>，移除以下區段：
     </para>
<screen>{% if salt.saltutil.runner('select.minions', cluster='ceph', roles=role) != [] %}
        [...]
{% endif %}</screen>
    </step>
    <step>
     <para>
      新角色需要金鑰圈來存取叢集。若要提供存取權，請複製 <filename>ganesha.j2</filename>：
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_rgw.j2
<prompt>root@master # </prompt><command>cp</command> ganesha.j2 ganesha_cfs.j2</screen>
     <para>
      可從 <filename>ganesha_rgw.j2</filename> 中移除 <literal>caps mds = "allow *"</literal> 這一行。
     </para>
    </step>
    <step>
     <para>
      複製物件閘道的金鑰圈：
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/rgw/files/rgw.j2 \
/srv/salt/ceph/rgw/files/ganesha_rgw.j2</screen>
    </step>
    <step>
     <para>
      物件閘道需要您對新角色進行組態設定：
     </para>
<screen><prompt>root@master # </prompt><command>cp</command> /srv/salt/ceph/configuration/files/ceph.conf.rgw \
/srv/salt/ceph/configuration/files/ceph.conf.ganesha_rgw</screen>
    </step>
    <step>
     <para>
      在 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 中將新建立的角色指定給叢集節點：
     </para>
<screen>role-ganesha_rgw/cluster/<replaceable>NODE1</replaceable>.sls
role-ganesha_cfs/cluster/<replaceable>NODE1</replaceable>.sls
 </screen>
     <para>
      分別以要將角色指定給的節點名稱取代 <replaceable>NODE1</replaceable> 和 <replaceable>NODE2</replaceable>。
     </para>
    </step>
    <step>
     <para>
      執行 DeepSea 階段 0 到 4。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ganesha-rgw-supported-operations">
   <title>支援操作</title>
   <para>
    RGW NFS 介面支援對檔案及目錄執行的大部分操作，但存在以下限制：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>不支援包含符號連結的連結。</emphasis>
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>不支援 NFS 存取控制清單 (ACL)。</emphasis><emphasis>支援</emphasis> Unix 使用者及群組擁有權和許可權。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>目錄不可移動或重新命名。</emphasis>您<emphasis>可</emphasis>在目錄之間移動檔案。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>僅支援完整的循序寫入 I/O。</emphasis>因此，它會強制上傳寫入操作。許多執行非循序儲存的一般 I/O 操作 (如就地編輯檔案) 必定會失敗。有一些歸檔公用程式 (例如，GNU <command>tar</command> 的某些版本) 表面上是循序寫入，但也可能會因偶爾的非循序儲存而失敗。透過 NFS 掛接時，一般可透過同步掛接 (<option>-o sync</option> 選項) 強制應用程式的循序 I/O 向 NFS 伺服器循序寫入資料。無法同步掛接的 NFS 用戶端 (例如 Microsoft Windows*) 將無法上傳檔案。
     </para>
    </listitem>
    <listitem>
     <para>
      NFS RGW 僅支援對小於 4 MB 的儲存區塊執行讀取寫入操作。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-services">
  <title>啟動或重新啟動 NFS Ganesha</title>

  <para>
   若要啟用並啟動 NFS Ganesha 服務，請執行以下指令：
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> enable nfs-ganesha
<prompt>root@minion &gt; </prompt><command>systemctl</command> start nfs-ganesha</screen>

  <para>
   若要重新啟動 NFS Ganesha，請執行以下指令：
  </para>

<screen><prompt>root@minion &gt; </prompt><command>systemctl</command> restart nfs-ganesha</screen>

  <para>
   啟動或重新啟動 NFS Ganesha 時，NFS v4 會有 90 秒的逾時寬限期間。在寬限期間，系統會主動拒絕來自用戶端的新要求。因此，當 NFS 處於寬限狀態時，用戶端可能會發生要求處理速度變慢的情況。
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-loglevel">
  <title>設定記錄層級</title>

  <para>
   您可以透過編輯 <filename>/etc/sysconfig/nfs-ganesha</filename> 檔案，來變更預設除錯層級 <literal>NIV_EVENT</literal>。以 <literal>NIV_DEBUG</literal> 或 <literal>NIV_FULL_DEBUG</literal> 取代 <literal>NIV_EVENT</literal>。提高記錄詳細層級可能會在記錄檔案中產生大量資料。
  </para>

<screen>OPTIONS="-L /var/log/ganesha/ganesha.log -f /etc/ganesha/ganesha.conf -N NIV_EVENT"</screen>

  <para>
   變更記錄層級時，需要重新啟動服務。
  </para>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-verify">
  <title>驗證輸出的 NFS 共用</title>

  <para>
   使用 NFS v3 時，您可以在 NFS Ganesha 伺服器節點上驗證是否輸出了 NFS 共用：
  </para>

<screen><prompt>root@minion &gt; </prompt><command>showmount</command> -e
/ (everything)</screen>
 </sect1>
 <sect1 xml:id="ceph-nfsganesha-mount">
  <title>掛接輸出的 NFS 共用</title>

  <para>
   若要在用戶端主機上掛接輸出的 NFS 共用 (依據<xref linkend="ceph-nfsganesha-config"/>中的設定)，請執行以下指令：
  </para>

<screen><prompt>root # </prompt><command>mount</command> -t nfs -o rw,noatime,sync \
 <replaceable>nfs_ganesha_server_hostname:/ /path/to/local/mountpoint</replaceable></screen>
 </sect1>
</chapter>
