<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_cifs.xml" version="5.0" xml:id="cha.ses.cifs">

 <title>透過 Samba 輸出 CephFS</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編輯</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  本章介紹如何透過 Samba/CIFS 共用輸出 CephFS。您可在 Windows* 用戶端中使用 Samba 共用。
 </para>
 <warning>
  <title>技術預覽</title>
  <para>
   從 SUSE Enterprise Storage 5 開始，輸出 Samba 共用被視為一項技術預覽，不再受支援。
  </para>
 </warning>
 <sect1 xml:id="sec.ses.cifs.example">
  <title>範例安裝</title>

  <para>
   輸出 CephFS 是一項技術預覽，並不受支援。若要輸出 Samba 共用，需要在一個叢集節點上手動安裝 Samba 並對其進行設定。您可以透過 CTDB 和 SUSE Linux Enterprise High Availability Extension 提供容錯移轉功能。
  </para>

  <procedure>
   <step>
    <para>
     請確定叢集中已存在一個正常運作的 CephFS。如需詳細資料，請參閱<xref linkend="cha.ceph.as.cephfs"/>。
    </para>
   </step>
   <step>
    <para>
     在 Salt Master 上建立一個特定於 Samba 閘道的金鑰圈，並將其複製到 Samba 閘道節點︰
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <replaceable>SAMBA_NODE</replaceable>:/etc/ceph/</screen>
    <para>
     使用 Samba 閘道節點的名稱取代 <replaceable>SAMBA_NODE</replaceable>。
    </para>
   </step>
   <step>
    <para>
     在 Samba 閘道節點上執行以下步驟。在 Samba 閘道節點上安裝 Samba 精靈︰
    </para>
<screen><prompt>root # </prompt><command>zypper</command> in samba samba-ceph</screen>
   </step>
   <step>
    <para>
     編輯 <filename>/etc/samba/smb.conf</filename> 並新增以下區段︰
    </para>
<screen>[<replaceable>SHARE_NAME</replaceable>]
        path = /
        vfs objects = ceph
        ceph:config_file = /etc/ceph/ceph.conf
        ceph: user_id = samba.gw
        read only = no</screen>
   </step>
   <step>
    <para>
     啟動並啟用 Samba 精靈︰
    </para>
<screen><prompt>root # </prompt><command>systemctl</command> start smb.service
<prompt>root # </prompt><command>systemctl</command> enable smb.service
<prompt>root # </prompt><command>systemctl</command> start nmb.service
<prompt>root # </prompt><command>systemctl</command> enable nmb.service</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec.ses.cifs.ha">
  <title>高可用性組態</title>

  <para>
   本節提供一個範例來展示如何設定 Samba 伺服器的雙節點高可用性組態。該設定需要 SUSE Linux Enterprise High Availability Extension。兩個節點分別名為 <systemitem class="domainname">earth</systemitem> (<systemitem class="ipaddress">192.168.1.1</systemitem>) 和 <systemitem class="domainname">mars</systemitem> (<systemitem class="ipaddress">192.168.1.2</systemitem>)。
  </para>

  <para>
   如需 SUSE Linux Enterprise High Availability Extension 的詳細資料，請參閱 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/"/>。
  </para>

  <para>
   此外，使用兩個浮動的虛擬 IP 位址可讓用戶端連接到服務，而不管該服務在哪個實體節點上執行。<systemitem class="ipaddress">192.168.1.10</systemitem> 用於透過 Hawk2 進行叢集管理，<systemitem class="ipaddress">192.168.2.1</systemitem> 專門用於 CIFS 輸出。如此可讓日後能更輕鬆地套用安全性限制。
  </para>

  <para>
   以下程序說明範例安裝。<link xlink:href="https://www.suse.com/documentation/sle-ha-12/install-quick/data/install-quick.html"/> 上提供了更多詳細資料。
  </para>

  <procedure xml:id="proc.sec.ses.cifs.ha">
   <step>
    <para>
     在 Salt Master 上建立一個特定於 Samba 閘道的金鑰圈，並將其複製到上述兩個節點︰
    </para>
<screen><prompt>root@master # </prompt><command>ceph</command> auth get-or-create client.samba.gw mon 'allow r' \
    osd 'allow *' mds 'allow *' -o ceph.client.samba.gw.keyring
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">earth</systemitem>:/etc/ceph/
<prompt>root@master # </prompt><command>scp</command> ceph.client.samba.gw.keyring <systemitem class="domainname">mars</systemitem>:/etc/ceph/</screen>
   </step>
   <step>
    <para>
     準備好 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem>，以代管 Samba 服務︰
    </para>
    <substeps>
     <step>
      <para>
       繼續操作前，請確定已安裝下列套件︰
       <package>ctdb</package>、 <package>tdb-tools</package>和
       <package>samba</package>  (smb 和 nmb 資源需要這些套件)。
      </para>
<screen><prompt>root # </prompt><command>zypper</command> in ctdb tdb-tools samba samba-ceph</screen>
     </step>
     <step>
      <para>
       確定 <literal>ctdb</literal>、<literal>smb</literal> 和 <literal>nmb</literal> 服務已停止且停用︰
      </para>
<screen><prompt>root # </prompt><command>systemctl</command> disable ctdb
<prompt>root # </prompt><command>systemctl</command> disable smb
<prompt>root # </prompt><command>systemctl</command> disable nmb
<prompt>root # </prompt><command>systemctl</command> stop smb
<prompt>root # </prompt><command>systemctl</command> stop nmb</screen>
     </step>
     <step>
      <para>
       在所有節點上開啟防火牆的連接埠 <literal>4379</literal>。這是為了使 CTDB 能夠與其他叢集節點通訊。
      </para>
     </step>
     <step>
      <para>
       在共享檔案系統上建立一個 CTDB 鎖定目錄︰
      </para>
<screen><prompt>root # </prompt><command>mkdir</command> -p /srv/samba/</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     在 <systemitem class="domainname">earth</systemitem> 上建立 Samba 的組態檔案。這些檔案稍後將自動同步到 <systemitem class="domainname">mars</systemitem>。
    </para>
    <substeps>
     <step>
      <para>
       在 <filename>/etc/ctdb/nodes</filename> 中插入包含叢集中每個節點之全部私人 IP 位址的所有節點︰
      </para>
<screen>192.168.1.1
192.168.1.2</screen>
     </step>
     <step>
      <para>
       設定 Samba。在 <filename>/etc/samba/smb.conf</filename> 的 <literal>[global]</literal> 區段中新增以下行。使用所選的主機名稱取代「CTDB-SERVER」(叢集中的所有節點將顯示為一個此名稱的大節點，以方便操作)︰
      </para>
<screen>[global]
    netbios name = CTDB-SERVER
    clustering = yes
    idmap config * : backend = tdb2
    passdb backend = tdbsam
    ctdbd socket = /var/lib/ctdb/ctdb.socket</screen>
      <para>
       如需 <command>csync2</command> 的詳細資料，請參閱 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/singlehtml/book_sleha/book_sleha.html#pro.ha.installation.setup.csync2.start"/>。
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     安裝並開機 SUSE Linux Enterprise High Availability 叢集。
    </para>
    <substeps>
     <step>
      <para>
       在 <systemitem class="domainname">earth</systemitem> 和 <systemitem class="domainname">mars</systemitem> 上註冊 SUSE Linux Enterprise High Availability Extension。
      </para>
<screen><prompt>root@earth # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
<screen><prompt>root@mars # </prompt><command>SUSEConnect</command> -r <replaceable>ACTIVATION_CODE</replaceable> -e <replaceable>E_MAIL</replaceable></screen>
     </step>
     <step>
      <para>
       在兩個節點上安裝 <package>ha-cluster-bootstrap</package> ︰
      </para>
<screen><prompt>root@earth # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
<screen><prompt>root@mars # </prompt><command>zypper</command> in ha-cluster-bootstrap</screen>
     </step>
     <step>
      <para>
       在 <systemitem class="domainname">earth</systemitem> 上啟始化叢集︰
      </para>
<screen>
<prompt>root@earth # </prompt><command>ha-cluster-init</command>
      </screen>
     </step>
     <step>
      <para>
       讓 <systemitem class="domainname">mars</systemitem> 加入該叢集︰
      </para>
<screen>
<prompt>root@mars # </prompt><command>ha-cluster-join</command> -c earth
      </screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     檢查叢集的狀態。您應該會看到兩個節點都已新增到叢集中︰
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
2 nodes configured
1 resource configured

Online: [ earth mars ]

Full list of resources:

 admin-ip       (ocf::heartbeat:IPaddr2):       Started earth</screen>
   </step>
   <step>
    <para>
     在 <systemitem class="domainname">earth</systemitem> 上執行以下指令，以設定 CTDB 資源︰
    </para>

<screen><prompt>root@earth # </prompt><command>crm</command> configure
<prompt>crm(live)configure# </prompt><command>primitive</command> ctdb ocf:heartbeat:CTDB params \
    ctdb_manages_winbind="false" \
    ctdb_manages_samba="false" \
    ctdb_recovery_lock="!/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper
        ceph client.samba.gw cephfs_metadata ctdb-mutex"
    ctdb_socket="/var/lib/ctdb/ctdb.socket" \
        op monitor interval="10" timeout="20" \
        op start interval="0" timeout="90" \
        op stop interval="0" timeout="100"
<prompt>crm(live)configure# </prompt><command>primitive</command> nmb systemd:nmb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>primitive</command> smb systemd:smb \
    op start timeout="60" interval="0" \
    op stop timeout="60" interval="0" \
    op monitor interval="60" timeout="60"
<prompt>crm(live)configure# </prompt><command>group</command> g-ctdb ctdb nmb smb
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ctdb g-ctdb meta interleave="true"
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     組態選項 <literal>ctdb_recovery_lock</literal> 中的二進位檔案 <command>/usr/lib64/ctdb/ctdb_mutex_ceph_rados_helper</command> 依序包含參數 <replaceable>CLUSTER_NAME</replaceable>
     <replaceable>CEPHX_USER</replaceable> <replaceable>CEPH_POOL</replaceable>
     <replaceable>CEPHX_USER</replaceable>。
    </para>
   </step>
   <step>
    <para>
     新增叢集化 IP 位址︰
    </para>
<screen><prompt>crm(live)configure# </prompt><command>primitive</command> ip ocf:heartbeat:IPaddr2 params ip=192.168.2.1 \
    unique_clone_address="true" \
    op monitor interval="60" \
    meta resource-stickiness="0"
<prompt>crm(live)configure# </prompt><command>clone</command> cl-ip ip \
    meta interleave="true" clone-node-max="2" globally-unique="true"
<prompt>crm(live)configure# </prompt><command>colocation</command> col-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>order</command> o-with-ctdb 0: cl-ip cl-ctdb
<prompt>crm(live)configure# </prompt><command>commit</command></screen>
    <para>
     如果 <literal>unique_clone_address</literal> 設定為 <literal>true</literal>，IPaddr2 資源代辦會向指定的位址新增一個複製品 ID，從而導致出現三個不同的 IP 位址。這些位址通常是不需要的，但有助於實現負載平衡。如需有關此主題的詳細資訊，請參閱 <link xlink:href="https://www.suse.com/documentation/sle-ha-12/book_sleha/data/cha_ha_lb.html"/>。
    </para>
   </step>
   <step>
    <para>
     檢查結果︰
    </para>
<screen><prompt>root@earth # </prompt><command>crm</command> status
Clone Set: base-clone [dlm]
     Started: [ factory-1 ]
     Stopped: [ factory-0 ]
 Clone Set: cl-ctdb [g-ctdb]
     Started: [ factory-1 ]
     Started: [ factory-0 ]
 Clone Set: cl-ip [ip] (unique)
     ip:0       (ocf:heartbeat:IPaddr2):       Started factory-0
     ip:1       (ocf:heartbeat:IPaddr2):       Started factory-1</screen>
   </step>
   <step>
    <para>
     從用戶端機器執行測試。在 Linux 用戶端上執行以下指令，確定是否可以將檔案複製到系統或從系統複製檔案︰
    </para>
<screen><prompt>root # </prompt><command>smbclient</command> <option>//192.168.2.1/myshare</option></screen>
   </step>
  </procedure>
 </sect1>
</chapter>
