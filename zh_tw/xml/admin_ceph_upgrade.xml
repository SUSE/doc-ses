<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha-ceph-upgrade">
 <title>從舊版本升級</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  本章說明將 SUSE Enterprise Storage 5.5 升級至版本 6 的步驟。請注意，版本 5.5 是在版本 5 的基礎上套用了所有最新修補程式後的版本。
 </para>
 <note>
  <title>不支援從更早的版本升級</title>
  <para>
   不支援從早於 5.5 的 SUSE Enterprise Storage 版本升級。您需要先升級至 SUSE Enterprise Storage 5.5 的最新版本，然後再依照本章所述的步驟操作。
  </para>
 </note>
 <sect1 xml:id="upgrade-consider-points">
  <title>升級前的注意事項</title>

  <itemizedlist>
   <listitem>
    <para>
     <emphasis>閱讀版本說明</emphasis> - 版本說明提供了有關自 SUSE Enterprise Storage 上一個版本發行後所進行的變更的其他資訊。檢查版本說明以瞭解：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       您的硬體是否有特殊的注意事項。
      </para>
     </listitem>
     <listitem>
      <para>
       所用的任何軟體套件是否已發生重大變更。
      </para>
     </listitem>
     <listitem>
      <para>
       是否需要對您的安裝施行特殊預防措施。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     版本說明還會提供無法及時編入手冊的資訊。它們還包含有關已知問題的說明。
    </para>
    <para>
     安裝套件 <package>release-notes-ses</package>之後，本地的 <filename>/usr/share/doc/release-notes</filename> 目錄中或 <link xlink:href="https://www.suse.com/releasenotes/"/> 網頁上會提供版本說明。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您先前是從版本 4 升級的，請確認升級至版本 5 的程序已成功完成：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       檢查以下檔案是否存在
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.import</screen>
      <para>
       從 SES 4 升級至 5 的過程中，engulf 程序會建立該檔案。另外，會在以下檔案中設定 <option>configuration_init: default-import</option> 選項
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <para>
       如果 <option>configuration_init</option> 仍設為 <option>default-import</option>，則表示叢集是使用 <filename>ceph.conf.import</filename> 而非 DeepSea 的預設 <filename>ceph.conf</filename> 做為其組態檔案的，後者是透過對以下目錄中的檔案進行編譯而產生的
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       因此，您需要檢查 <filename>ceph.conf.import</filename> 中是否有任何自訂組態，並視需要將相應組態移至以下目錄中的其中一個檔案中
      </para>
<screen>/srv/salt/ceph/configuration/files/ceph.conf.d/</screen>
      <para>
       然後，從下面的檔案中移除 <option>configuration_init: default-import</option> 一行
      </para>
<screen>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</screen>
      <warning>
       <title>預設 DeepSea 組態</title>
       <para>
        如果您<emphasis role="bold">未</emphasis>合併 <filename>ceph.conf.import</filename> 中的組態，並移除了 <option>configuration_init: default-import</option> 選項，那麼我們隨 DeepSea 一併提供的任何預設組態設定 (在 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename> 中儲存) 都不會套用至叢集。
       </para>
      </warning>
     </listitem>
     <listitem>
      <para>
       檢查叢集是否使用了新桶類型「straw2」：
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep straw
</screen>
     </listitem>
     <listitem>
      <para>
       檢查是否使用了 Ceph「jewel」設定檔：
      </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph osd crush dump | grep profile
</screen>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     如果使用的是舊 RBD 核心用戶端 (早於 SUSE Linux Enterprise Server 12 SP3)，請參閱<xref linkend="rbd-old-clients-map"/>。如果可能，建議升級舊 RBD 核心用戶端。
    </para>
   </listitem>
   <listitem>
    <para>
     如果 openATTIC 位於管理節點上，當您升級該節點後，openATTIC 將不可用。在您使用 DeepSea 部署新 Ceph Dashboard 前，該儀表板將不可用。
    </para>
   </listitem>
   <listitem>
    <para>
     升級叢集可能需要花很長時間 - 所需時間大約為升級一部機器的時間乘以叢集節點數。
    </para>
   </listitem>
   <listitem>
    <para>
     如果某個節點執行的是先前的 SUSE Linux Enterprise Server 版本，則無法進行升級，需要將其重新開機到新版本的安裝程式。因此，該節點提供的服務將在一段時間內無法使用。核心叢集服務將仍然可用，例如，如果在升級期間有一個 MON 停機，至少還有兩個 MON 處於使用中狀態。但不幸的是，單一 iSCSI 閘道這樣的單一例項服務將無法使用。
    </para>
   </listitem>
   <listitem>
    <para>
     某些類型的精靈相依於其他精靈。例如，Ceph 物件閘道相依於 Ceph MON 和 Ceph OSD 精靈。建議您依照以下順序升級：
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       管理節點
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 監控程式/Ceph 管理員
      </para>
     </listitem>
     <listitem>
      <para>
       中繼資料伺服器
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD
      </para>
     </listitem>
     <listitem>
      <para>
       物件閘道
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI 閘道
      </para>
     </listitem>
     <listitem>
      <para>
       NFS Ganesha
      </para>
     </listitem>
     <listitem>
      <para>
       Samba 閘道
      </para>
     </listitem>
    </orderedlist>
   </listitem>
   <listitem>
    <para>
     如果您之前是在「控訴」或「強制」模式下使用 AppArmor 的，則需要在升級前設定 Salt Pillar 變數。由於 SUSE Linux Enterprise Server 15 SP1 預設隨附了 AppArmor，因此 DeepSea 階段 0 中已整合 AppArmor 管理。SUSE Enterprise Storage 6 預設會移除 AppArmor 和相關組態。如果您要保留在 SUSE Enterprise Storage 5.5 中設定的行為，請在開始升級前確認 <filename>/srv/pillar/ceph/stack/global.yml</filename> 檔案中是否存在以下其中一行：
    </para>
<screen>
apparmor_init: default-enforce
</screen>
    <para>
     或
    </para>
<screen>
apparmor_init: default-complain
</screen>
   </listitem>
   <listitem>
    <para>
     自 SUSE Enterprise Storage 6 起，不再允許使用以數字開頭的 MDS 名稱，否則 MDS 精靈將拒絕啟動。您可以透過執行 <command>ceph fs status</command> 指令來檢查精靈是否使用了此類名稱，也可以透過重新啟動 MDS，然後檢查其記錄中是否存在以下訊息來確認：
    </para>
<screen>
deprecation warning: MDS id 'mds.1mon1' is invalid and will be forbidden in
a future version.  MDS names may not start with a numeric digit.
</screen>
    <para>
     如果您看到上述訊息，則需要在嘗試升級至 SUSE Enterprise Storage 6 前移轉 MDS 名稱。DeepSea 提供了一種協調程序來自動完成此移轉。它會在以數字開頭的 MDS 名稱前加上「mds.」字首：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.mds.migrate-numerical-names
</screen>
    <tip>
     <title>結合到 MDS 名稱的自訂組態</title>
     <para>
      如果您有結合到 MDS 名稱的組態設定，且 MDS 精靈的名稱以數字開頭，請確認您的組態設定是否同樣適用於新名稱 (帶有「mds.」字首)。假設 <filename>/etc/ceph/ceph.conf</filename> 檔案中包含以下範例區段：
     </para>
<screen>
[mds.123-my-mds] # config setting specific to MDS name with a name starting with a digit
mds cache memory limit = 1073741824
mds standby for name = 456-another-mds
</screen>
     <para>
      <command>ceph.mds.migrate-numerical-names</command> 協調器會將 MDS 精靈名稱「123-my-mds」變更為「mds.123-my-mds」。您需要調整組態以反映新名稱：
     </para>
<screen>
[mds.mds,123-my-mds] # config setting specific to MDS name with the new name
mds cache memory limit = 1073741824
mds standby for name = mds.456-another-mds
</screen>
    </tip>
    <para>
     如此將會在移除舊 MDS 精靈前新增使用新名稱的 MDS 精靈。因此，短時間內 MDS 精靈的數量會翻倍。系統將會有一個短時暫停來進行容錯移轉，然後用戶端才能存取 CephFS。因此，請將移轉排程在預期只有少量或沒有 CephFS 負載的時間進行。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-backup">
  <title>備份叢集資料</title>

  <para>
   雖然建立叢集組態和資料的備份並非強制性操作，但我們仍強烈建議備份重要的組態檔案和叢集資料。如需更多詳細資料，請參閱 <xref linkend="cha-deployment-backup"/>。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-ntp">
  <title>從 <systemitem class="daemon">ntpd</systemitem> 移轉至 <systemitem class="daemon">chronyd</systemitem></title>

  <para>
   SUSE Linux Enterprise Server 15 SP1 不再使用 <systemitem class="daemon">ntpd</systemitem> 來同步本地主機時間，而是使用 <systemitem class="daemon">chronyd</systemitem>。您需要移轉每個叢集節點上的時間同步精靈。您可以在升級叢集<emphasis role="bold">前</emphasis>移轉至 <systemitem>chronyd</systemitem>，也可以先升級叢集，然後再移轉至 <systemitem class="daemon">chronyd</systemitem>
   <emphasis role="bold"/>。
  </para>

  <procedure>
   <title>在升級叢集<emphasis>前</emphasis>移轉至 <systemitem class="daemon">chronyd</systemitem> </title>
   <step>
    <para>
     安裝 <package>chrony</package> 套件：
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper install chrony</screen>
   </step>
   <step>
    <para>
     編輯 <systemitem class="daemon">chronyd</systemitem> 組態檔案 <filename>/etc/chrony.conf</filename>，並新增在 <filename>/etc/ntp.conf</filename> 中的目前 <systemitem class="daemon">ntpd</systemitem> 組態中指定的 NTP 來源。
    </para>
    <tip>
     <title>有關 <systemitem class="daemon">chronyd</systemitem> 組態的更多詳細資料</title>
     <para>
      如需如何在 <systemitem class="daemon">chronyd</systemitem> 組態中包含時間來源的更多詳細資料，請參閱 <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>。
     </para>
    </tip>
   </step>
   <step>
    <para>
     停用並停止 <systemitem class="daemon">ntpd</systemitem> 服務：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     啟動並啟用 <systemitem class="daemon">chronyd</systemitem> 服務：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     確認 chrony 的狀態：
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
  </procedure>

  <procedure>
   <title>在升級叢集<emphasis>後</emphasis>移轉至 <systemitem class="daemon">chronyd</systemitem> </title>
   <step>
    <para>
     在叢集升級期間，請新增以下軟體儲存庫：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Pool
      </para>
     </listitem>
     <listitem>
      <para>
       SLE-Module-Legacy15-SP1-Updates
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     將叢集升級至版本 6。
    </para>
   </step>
   <step>
    <para>
     編輯 <systemitem class="daemon">chronyd</systemitem> 組態檔案 <filename>/etc/chrony.conf</filename>，並新增在 <filename>/etc/ntp.conf</filename> 中的目前 <systemitem class="daemon">ntpd</systemitem> 組態中指定的 NTP 來源。
    </para>
    <tip>
     <title>有關 <systemitem class="daemon">chronyd</systemitem> 組態的更多詳細資料</title>
     <para>
      如需如何在 <systemitem class="daemon">chronyd</systemitem> 組態中包含時間來源的更多詳細資料，請參閱 <link xlink:href="https://documentation.suse.com/sles/15-SP1/html/SLES-all/cha-ntp.html"/>。
     </para>
    </tip>
   </step>
   <step>
    <para>
     停用並停止 <systemitem class="daemon">ntpd</systemitem> 服務：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl disable ntpd.service &amp;&amp; systemctl stop ntpd.service</screen>
   </step>
   <step>
    <para>
     啟動並啟用 <systemitem class="daemon">chronyd</systemitem> 服務：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl start chronyd.service &amp;&amp; systemctl enable chronyd.service</screen>
   </step>
   <step>
    <para>
     從 <systemitem class="daemon">ntpd</systemitem> 移轉至 <systemitem class="daemon">chronyd</systemitem>。
    </para>
   </step>
   <step>
    <para>
     確認 chrony 的狀態：
    </para>
<screen><prompt>root@minion &gt; </prompt>chronyc tracking</screen>
   </step>
   <step>
    <para>
     移除您在升級過程中為了在系統中保留 <systemitem class="daemon">ntpd</systemitem> 而新增的舊軟體儲存庫。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-prepare">
  <title>升級前對叢集套用修補程式</title>

  <para>
   在升級前，需要對所有叢集節點套用最新的修補程式。
  </para>

  <sect2 xml:id="upgrade-prepare-repos">
   <title>所需的軟體儲存庫</title>
   <para>
    檢查是否已在每個叢集節點上設定所需儲存庫。若要列出所有可用儲存庫，請執行以下指令
   </para>
<screen>
<prompt>root@minion &gt; </prompt>zypper lr
</screen>
   <para>
    SUSE Enterprise Storage 5.5 需要：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLES12-SP3-Installer-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLES12-SP3-Updates
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE-Enterprise-Storage-5-Updates
     </para>
    </listitem>
   </itemizedlist>
   <para>
    SUSE Linux Enterprise Server 12 SP3 上執行的 SLE-HA 上的 NFS/SMB 閘道需要：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SLE-HA12-SP3-Pool
     </para>
    </listitem>
    <listitem>
     <para>
      SLE-HA12-SP3-Updates
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-staging">
   <title>儲存庫暫存系統</title>
   <para>
    如果您在使用某個儲存庫暫存系統 (SMT、RMT 或 SUSE Manager)，請為目前和新的 SUSE Enterprise Storage 版本建立新的凍結修補程式層級。
   </para>
   <para>
    如需詳細資訊，請參閱：
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-12/book_smt/data/book_smt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="https://www.suse.com/documentation/suse-manager-3/index.html"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="upgrade-prepare-patch">
   <title>對整個叢集套用最新的修補程式</title>
   <procedure>
    <step>
     <para>
      對每個 Ceph 叢集節點套用 SUSE Enterprise Storage 5.5 和 SUSE Linux Enterprise Server 12 SP3 的最新修補程式。確認已將正確的軟體儲存庫連接至每個叢集節點 (請參閱<xref linkend="upgrade-prepare-repos"/>)，然後執行 DeepSea 階段 0：
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    </step>
    <step>
     <para>
      執行完階段 0 後，確認每個叢集節點的狀態是否包含「HEALTH_OK」。如果未包含，請先解決問題，然後再進行後續步驟中可能的重新開機。
     </para>
    </step>
    <step>
     <para>
      執行 <command>zypper ps</command> 以檢查是否存在可能使用過時程式庫或二進位執行的程序，如果存在，則進行重新開機。
     </para>
    </step>
    <step>
     <para>
      確認執行中核心是否為最新版本，如果不是，則進行重新開機。檢查以下指令的輸出：
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>uname -a
<prompt>cephadm@adm &gt; </prompt>rpm -qa kernel-default
</screen>
    </step>
    <step>
     <para>
      確認是否已安裝 <package>ceph</package> 套件為 12.2.12 或更新版本。確認是否已安裝 <package>deepsea</package> 套件為 0.8.9 或更新版本。
     </para>
    </step>
    <step>
     <para>
      如果您之前使用了任何 <option>bluestore_cache</option> 設定，從 <package>ceph</package>
      12.2.10 版本開始，這些設定將不再有效。新設定 <option>bluestore_cache_autotune</option> (預設設為「true」) 會禁止手動調整快取大小。若要啟用舊行為，您需要設定 <option>bluestore_cache_autotune=false</option>。如需詳細資訊，請參閱<xref linkend="config-auto-cache-sizing"/>。
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-verify-current">
  <title>確認目前環境</title>

  <itemizedlist>
   <listitem>
    <para>
     如果系統存在明顯問題，請修復後再開始升級。升級程序並不會修復現有的系統問題。
    </para>
   </listitem>
   <listitem>
    <para>
     檢查叢集效能。您可以使用 <command>rados bench</command>、<command>ceph tell osd.* bench</command> 或 <command>iperf3</command> 等指令。
    </para>
   </listitem>
   <listitem>
    <para>
     確認對閘道 (例如 iSCSI 閘道或物件閘道) 和 RADOS 區塊裝置的存取權。
    </para>
   </listitem>
   <listitem>
    <para>
     記錄系統設定的特定部分，例如網路設定、磁碟分割或安裝詳細資料。
    </para>
   </listitem>
   <listitem>
    <para>
     使用 <command>supportconfig</command> 收集重要的系統資訊，並將其儲存在叢集節點之外。如需詳細資訊，請參閱<link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_admsupport_supportconfig.html"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     確定每個叢集節點上都有充足的可用磁碟空間。請使用 <command>df -h</command> 檢查可用磁碟空間。必要時，請透過移除不需要的檔案/目錄或過時的作業系統快照來釋放磁碟空間。如果沒有足夠的可用磁碟空間，在釋放出充足的磁碟空間前，請不要繼續升級。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="upgrade-verify-state">
  <title>檢查叢集的狀態</title>

  <itemizedlist>
   <listitem>
    <para>
     開始升級程序前，請使用 <command>cluster health</command> 指令檢查叢集狀態。除非所有叢集節點都報告「HEALTH_OK」，否則請不要開始升級。
    </para>
   </listitem>
   <listitem>
    <para>
     確認所有服務都在執行中：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Salt Master 和 Salt Master 精靈。
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph 監控程式和 Ceph 管理員精靈。
      </para>
     </listitem>
     <listitem>
      <para>
       中繼資料伺服器精靈。
      </para>
     </listitem>
     <listitem>
      <para>
       Ceph OSD 精靈。
      </para>
     </listitem>
     <listitem>
      <para>
       物件閘道精靈。
      </para>
     </listitem>
     <listitem>
      <para>
       iSCSI 閘道精靈。
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   以下指令提供叢集狀態以及特定組態的詳細資料：
  </para>

  <variablelist>
   <varlistentry>
    <term><command>ceph -s</command></term>
    <listitem>
     <para>
      列印 Ceph 叢集狀態、執行中服務、資料使用率以及 I/O 統計資料的簡短摘要。請在開始升級前確認該指令是否報告了「HEALTH_OK」。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph health detail</command></term>
    <listitem>
     <para>
      如果 Ceph 叢集狀態並非報告為 OK，該指令可列印詳細資料。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph versions</command></term>
    <listitem>
     <para>
      列印執行中 Ceph 精靈的版本。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph df</command></term>
    <listitem>
     <para>
      列印叢集上的總磁碟空間和可用磁碟空間。如果叢集的可用磁碟空間不到總磁碟空間的 25%，請不要開始升級。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>salt '*' cephprocesses.check results=true</command></term>
    <listitem>
     <para>
      列印執行中 Ceph 程序及其 PID (依 Salt Minion 排序)。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><command>ceph osd dump | grep ^flags</command></term>
    <listitem>
     <para>
      確認是否存在「recovery_deletes」和「purged_snapdirs」旗標。如果不存在，您可以透過執行以下指令來強制對所有放置群組執行整理。請注意，此強制整理可能會對您 Ceph 用戶端的效能造成負面影響。
     </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph pg dump pgs_brief | cut -d " " -f 1 | xargs -n1 ceph pg scrub
</screen>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1>
  <title>CTDB 叢集的離線升級</title>

  <para>
   CTDB 提供 Samba 閘道使用的叢集化資料庫。CTDB 通訊協定非常簡單，不支援節點之間使用不同通訊協定版本進行通訊的叢集。因此需要先將 CTDB 節點置於離線狀態才能執行升級。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-one-node">
  <title>依節點升級 - 基本程序</title>

  <para>
   為了確保核心叢集服務在升級期間可用，您需要循序逐一升級叢集節點。您可以使用兩種方式執行節點升級：使用<emphasis>安裝程式 DVD</emphasis>，或者使用<emphasis>套裝作業移轉系統</emphasis>。
  </para>

  <para>
   升級每個節點後，我們建議執行 <command>rpmconfigcheck</command> 指令來檢查是否存在任何在本地編輯過的已更新組態檔案。如果指令傳回尾碼為 <filename>.rpmnew</filename>、<filename>.rpmorig</filename> 或 <filename>.rpmsave</filename> 的檔案名稱清單，請將這些檔案與目前的組態檔案進行比較，以確定沒有遺失任何本地變更。如果需要，請更新受影響的檔案。如需處理 <filename>.rpmnew</filename>、<filename>.rpmorig</filename> 和 <filename>.rpmsave</filename> 檔案的詳細資訊，請參閱 <link xlink:href="https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#sec-rpm-packages-manage"/>。
  </para>

  <tip>
   <title>孤立套件</title>
   <para>
    升級一個節點後，有一些套件將處於「孤立」狀態，沒有父儲存庫。這是因為與 python3 相關的套件不會廢棄 python2 套件。
   </para>
   <para>
    如需有關列出孤立套件的詳細資訊，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_zypper.html#sec_zypper_softup_orphaned"/>。
   </para>
  </tip>

  <sect2 xml:id="upgrade-one-node-manual">
   <title>使用安裝程式 DVD 手動升級節點</title>
   <procedure>
    <step>
     <para>
      從 SUSE Linux Enterprise Server 15 SP1 安裝程式 DVD/影像將節點重新開機。
     </para>
    </step>
    <step>
     <para>
      從開機功能表中選取<guimenu>升級</guimenu>。
     </para>
    </step>
    <step>
     <para>
      在<guimenu>選取移轉目標</guimenu>螢幕上，確認選取了「SUSE Linux Enterprise Server 15 SP1」，然後啟用<guimenu>手動調整用於移轉的儲存庫</guimenu>核取方塊。
     </para>
     <figure>
      <title>選取移轉目標</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="migration-target.png" width="75%"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      選取以下模組加以安裝：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SUSE Enterprise Storage 6 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Basesystem Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Desktop Applications Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Legacy Module 15 SP1 x86_64
       </para>
      </listitem>
      <listitem>
       <para>
        Server Applications Module 15 SP1 x86_64
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      在<guimenu>之前使用的儲存庫</guimenu>螢幕上，確認選取了正確的儲存庫。如果未在 SCC/SMT 中註冊系統，您需要手動新增儲存庫。
     </para>
     <para>
      SUSE Enterprise Storage 6 需要：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Basesystem15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Basesystem15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Module-Server-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Desktop-Applications15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SLE-Product-SLES15-SP1-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SLE15-SP1-Installer-Updates
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Pool
       </para>
      </listitem>
      <listitem>
       <para>
         SUSE-Enterprise-Storage-6-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      如果您要在移轉 SES 後將 <systemitem>ntpd</systemitem> 移轉至 <systemitem class="daemon">chronyd</systemitem> (請參閱<xref linkend="upgrade-ntp"/>)，請包含以下儲存庫：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Module-Legacy15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
     <para>
      SUSE Linux Enterprise Server 15 SP1 上執行的 SLE-HA 上的 NFS/SMB 閘道需要：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Pool
       </para>
      </listitem>
      <listitem>
       <para>
        SLE-Product-HA15-SP1-Updates
       </para>
      </listitem>
     </itemizedlist>
    </step>
    <step>
     <para>
      檢查<guimenu>安裝設定</guimenu>，然後按一下<guimenu>更新</guimenu>以開始安裝程序。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="upgrade-one-node-auto">
   <title>使用 SUSE 套裝作業系統移轉系統升級節點</title>
   <para>
    <emphasis>套裝作業系統移轉系統</emphasis> (DMS) 提供將安裝的 SUSE Linux Enterprise 系統從一個主要版本升級至另一個主要版本的升級路徑。以下程序使用 DMS 將 SUSE Enterprise Storage 5.5 升級至版本 6，其中包括從基礎 SUSE Linux Enterprise Server 12 SP3 移轉至 SUSE Linux Enterprise Server 15 SP1。
   </para>
   <para>
    如需 DMS 的一般和詳細資訊，請參閱 <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/>。
   </para>
   <procedure>
    <step>
     <para>
      安裝移轉 RPM 套件。它們會調整 GRUB 開機載入程式，以在下次重新開機時自動觸發升級。安裝
      <package>SLES15-SES-Migration</package> 與
      <package>suse-migration-sle15-activation</package> 套件:
     </para>
<screen><prompt>root@minion &gt; </prompt>zypper install SLES15-SES-Migration suse-migration-sle15-activation</screen>
    </step>
    <step>
     <substeps>
      <step>
       <para>
        如果<emphasis role="bold">已</emphasis>在儲存庫暫存系統 (如 SCC、SMT、RMT 或 SUSE Manager) 中註冊要升級的節點，請建立包含以下內容的 <filename>/etc/sle-migration-service.yml</filename> 檔案：
       </para>
<screen>
use_zypper_migration: true
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
      </step>
      <step>
       <para>
        如果<emphasis role="bold">未</emphasis>在儲存庫暫存系統 (如 SCC、SMT、RMT 或 SUSE Manager) 中註冊要升級的節點，請進行以下變更：
       </para>
       <substeps>
        <step>
         <para>
          建立包含以下內容的 <filename>/etc/sle-migration-service.yml</filename> 檔案：
         </para>
<screen>
use_zypper_migration: false
preserve:
  rules:
    - /etc/udev/rules.d/70-persistent-net.rules
</screen>
        </step>
        <step>
         <para>
          停用或移除 SLE 12 SP3 和 SES 5 儲存庫，並新增 SLE 15 SP1 和 SES 6 儲存庫。<xref linkend="upgrade-prepare-repos"/>中列出了相關的儲存庫。
         </para>
        </step>
       </substeps>
      </step>
     </substeps>
    </step>
    <step>
     <para>
      重新開機以開始升級。在執行升級期間，您可以從主機系統上使用現有 SSH 金鑰透過 <command>ssh</command> 以移轉使用者的身分登入已升級節點 (如 <link xlink:href="https://documentation.suse.com/suse-distribution-migration-system/1.0/single-html/distribution-migration-system/"/> 所述)。對於 SUSE Enterprise Storage，如果您能夠實體存取機器或可直接透過主控台存取，則還可以在系統主控台上使用密碼 <literal>sesupgrade</literal> 以 <systemitem class="username">root</systemitem> 身分登入。節點將在升級後自動重新開機。
     </para>
     <tip>
      <title>升級失敗</title>
      <para>
       如果升級失敗，請檢查 <filename>/var/log/distro_migration.log</filename>。修復問題，重新安裝移轉 RPM 套件，然後將節點重新開機。
      </para>
     </tip>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="upgrade-adm">
  <title>升級管理節點</title>

  <itemizedlist>
   <listitem>
    <para>
     雖然 Salt Minion 執行的是舊版 Ceph 和 Salt，但下列指令仍有效：<command>salt '*' test.ping</command> 和 <command>ceph status</command>
    </para>
   </listitem>
   <listitem>
    <para>
     升級管理節點後，將不再安裝 openATTIC。
    </para>
   </listitem>
   <listitem>
    <para>
     如果管理節點之前代管著 SMT，請將其移轉至 RMT (請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_migrate.html"/>)。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
    </para>
   </listitem>
  </itemizedlist>

  <tip>
   <title>叢集節點的狀態</title>
   <para>
    升級管理節點後，您可以執行 <command>salt-run upgrade.status</command> 指令，以檢視有關叢集節點的有用資訊。該指令會列出所有節點的 Ceph 和作業系統版本，並會建議應依何順序升級仍在執行舊版本的所有節點。
   </para>
<screen><prompt>root@master # </prompt>salt-run upgrade.status
The newest installed software versions are:
  ceph: ceph version 14.2.1-468-g994fd9e0cc (994fd9e0ccc50c2f3a55a3b7a3d4e0ba74786d50) nautilus (stable)
  os: SUSE Linux Enterprise Server 15 SP1

Nodes running these software versions:
  admin.ceph (assigned roles: master)
  mon2.ceph (assigned roles: admin, mon, mgr)

Nodes running older software versions must be upgraded in the following order:
   1: mon1.ceph (assigned roles: admin, mon, mgr)
   2: mon3.ceph (assigned roles: admin, mon, mgr)
   3: data1.ceph (assigned roles: storage)
[...]</screen>
  </tip>
 </sect1>
 <sect1 xml:id="upgrade-mons">
  <title>升級 Ceph 監控程式/Ceph 管理員節點</title>

  <itemizedlist>
   <listitem>
    <para>
     如果您的叢集<emphasis role="bold">未使用</emphasis> MDS 角色，請逐一升級 MON/MGR 節點。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您的叢集<emphasis role="bold">使用</emphasis> MDS 角色，且 MON/MGR 與 MDS 角色共置，您需要縮減 MDS 叢集，然後升級共置的節點。如需更多詳細資料，請參閱 <xref linkend="upgrade-mds"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     如果您的叢集<emphasis role="bold">使用</emphasis> MDS 角色，且它們在<emphasis role="bold">專屬</emphasis>伺服器上執行，請逐一升級所有 MON/MGR 節點，然後縮減 MDS 叢集並進行升級。如需更多詳細資料，請參閱 <xref linkend="upgrade-mds"/>。
    </para>
   </listitem>
  </itemizedlist>

  <note>
   <title>Ceph 監控程式升級</title>
   <para>
    由於 Ceph 監控程式設計存在局限性，一旦有兩個 MON 已升級至 SUSE Enterprise Storage 6 且構成了仲裁，那麼第三個 MON (仍在執行 SUSE Enterprise Storage 5.5) 出於某種原因重新啟動 (包括節點重新開機) 後，將不會重新加入 MON 叢集。因此，當有兩個 MON 升級後，最好儘快升級其餘 MON。
   </para>
  </note>

  <para>
   <emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
  </para>
 </sect1>
 <sect1 xml:id="upgrade-mds">
  <title>升級中繼資料伺服器</title>

  <para>
   您需要縮減中繼資料伺服器 (MDS) 叢集。由於 SUSE Enterprise Storage 版本 5.5 與 6 之間的功能不相容，較舊版本的 MDS 精靈一旦發現有 SES 6 層級的 MDS 加入叢集，他們即會關閉。因此，在 MDS 節點升級期間，必須將 MDS 叢集縮減到只有一個使用中 MDS (沒有待命 MDS)。升級第二個節點後，就可以再次擴充 MDS 叢集。
  </para>

  <tip>
   <para>
    在負載過重的 MDS 叢集上，您可能需要降低負載 (例如透過停止用戶端)，以便單個使用中 MDS 就足以處理工作負載。
   </para>
  </tip>

  <procedure>
   <step>
    <para>
     記下 <option>max_mds</option> 選項的目前值：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs get cephfs | grep max_mds
</screen>
   </step>
   <step>
    <para>
     如果您有多個使用中 MDS 精靈 (即 <option>max_mds</option> &gt; 1)，請縮減 MDS 叢集。若要縮減 MDS 叢集，請執行以下指令
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds 1
</screen>
    <para>
     其中，<replaceable>FS_NAME</replaceable> 是您 CephFS 例項的名稱 (預設為「cephfs」)。
    </para>
   </step>
   <step>
    <para>
     找到代管某個待命 MDS 精靈的節點。查看 <command>ceph fs status</command> 指令的輸出，然後開始升級此節點上的 MDS 叢集。
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs status
cephfs - 2 clients
======
+------+--------+--------+---------------+-------+-------+
| Rank | State  |  MDS   |    Activity   |  dns  |  inos |
+------+--------+--------+---------------+-------+-------+
|  0   | active | mon1-6 | Reqs:    0 /s |   13  |   16  |
+------+--------+--------+---------------+-------+-------+
+-----------------+----------+-------+-------+
|       Pool      |   type   |  used | avail |
+-----------------+----------+-------+-------+
| cephfs_metadata | metadata | 2688k | 96.8G |
|   cephfs_data   |   data   |    0  | 96.8G |
+-----------------+----------+-------+-------+
+-------------+
| Standby MDS |
+-------------+
|    mon3-6   |
|    mon2-6   |
+-------------+
</screen>
    <para>
     在此範例中，您需要在節點「mon3-6」或「mon2-6」上開始升級程序。
    </para>
   </step>
   <step>
    <para>
     升級具有待命 MDS 精靈的節點。升級過的 MDS 節點啟動後，過時的 MDS 精靈將自動關閉。此時，用戶端可能會經歷一段短暫的 CephFS 服務停機時間。
    </para>
    <para>
     <emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
    </para>
   </step>
   <step>
    <para>
     升級其餘 MDS 節點。
    </para>
   </step>
   <step>
    <para>
     將 <option>max_mds</option> 重設為所需組態：
    </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>FS_NAME</replaceable> max_mds <replaceable>ACTIVE_MDS_COUNT</replaceable>
</screen>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-main-osd">
  <title>升級 Ceph OSD</title>

  <para>
   針對每個儲存節點執行以下步驟：
  </para>

  <procedure>
   <step>
    <para>
     識別特定節點上的執行中 OSD 精靈：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd tree
</screen>
   </step>
   <step>
    <para>
     為要升級的節點上的每個 OSD 精靈設定「noout」旗標：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd add-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd add-noout osd.$i; done</screen>
    <para>
     使用以下指令進行驗證：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     或
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
      6 OSDs or CRUSH {nodes, device-classes} have {NOUP,NODOWN,NOIN,NOOUT} flags set</screen>
   </step>
   <step>
    <para>
     透過在待升級節點上執行以下指令，為所有現有 OSD 建立 <filename>/etc/ceph/osd/*.json</filename> 檔案：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan --force
</screen>
   </step>
   <step>
    <para>
     升級 OSD 節點。<emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
    </para>
   </step>
   <step>
    <para>
     啟用系統中發現的所有 OSD：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>;ceph-volume simple activate --all
</screen>
    <tip>
     <title>個別啟用各資料分割區</title>
     <para>
      如果您要個別啟用各資料分割區，則需要知道用於啟用相應分割區的正確 <command>ceph-volume</command> 指令。請以分割區的正確代號/編號取代 <replaceable>X1</replaceable>：
     </para>
<screen>
 <prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/sd<replaceable>X1</replaceable>
</screen>
     <para>
      例如：
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple scan /dev/vdb1
[...]
--&gt; OSD 8 got scanned and metadata persisted to file:
/etc/ceph/osd/8-d7bd2685-5b92-4074-8161-30d146cd0290.json
--&gt; To take over management of this scanned OSD, and disable ceph-disk
and udev, run:
--&gt;     ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
     <para>
      輸出的最後一行包含用於啟用該分割區的指令：
     </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph-volume simple activate 8 d7bd2685-5b92-4074-8161-30d146cd0290
[...]
--&gt; All ceph-disk systemd units have been disabled to prevent OSDs
getting triggered by UDEV events
[...]
Running command: /bin/systemctl start ceph-osd@8
--&gt; Successfully activated OSD 8 with FSID
d7bd2685-5b92-4074-8161-30d146cd0290
</screen>
    </tip>
   </step>
   <step>
    <para>
     確認 OSD 節點在重新開機後是否將正常啟動。
    </para>
   </step>
   <step>
    <para>
     解決「Legacy BlueStore stats reporting detected on XX OSD(s)」訊息：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
    <para>
     在將 Ceph 升級至 14.2.2 時通常會出現該警告。您可以透過以下設定來停用它：
    </para>
<screen>bluestore_warn_on_legacy_statfs = false</screen>
    <para>
     正確的修復方法是在所有由於該問題而停止的 OSD 上執行以下指令：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-XXX</screen>
    <para>
     下面的輔助程式程序檔將會對 <replaceable>NODE_NAME</replaceable> 節點上的所有 OSD 執行 <command>ceph-bluestore-tool repair</command> 指令：
    </para>
<screen>OSDNODE=<replaceable>OSD_NODE_NAME</replaceable>;\
 for OSD in $(ceph osd ls-tree $OSDNODE);\
 do echo "osd=" $OSD;\
 salt $OSDNODE cmd.run 'systemctl stop ceph-osd@$OSD';\
 salt $OSDNODE cmd.run 'ceph-bluestore-tool repair --path /var/lib/ceph/osd/ceph-$OSD';\
 salt $OSDNODE cmd.run 'systemctl start ceph-osd@$OSD';\
 done</screen>
   </step>
   <step>
    <para>
     對已升級節點上的每個 OSD 精靈取消設定「noout」旗標：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph osd rm-noout osd.<replaceable>OSD_ID</replaceable>
</screen>
    <para>
     例如：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>for i in $(ceph osd ls-tree <replaceable>OSD_NODE_NAME</replaceable>);do echo "osd: $i"; ceph osd rm-noout osd.$i; done</screen>
    <para>
     使用以下指令進行驗證：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph health detail | grep noout</screen>
    <para>
     注意：
    </para>
<screen><prompt>cephadm@osd &gt; </prompt>ceph –s
cluster:
 id:     44442296-033b-3275-a803-345337dc53da
 health: HEALTH_WARN
 <emphasis role="bold">Legacy BlueStore stats reporting detected on 6 OSD(s)</emphasis></screen>
   </step>
   <step>
    <para>
     確認叢集狀態。其輸出將如下所示：
    </para>
<screen>
<prompt>cephadm@osd &gt; </prompt>ceph status
cluster:
  id:     e0d53d64-6812-3dfe-8b72-fd454a6dcf12
  health: HEALTH_WARN
          3 monitors have not enabled msgr2

services:
  mon: 3 daemons, quorum mon1,mon2,mon3 (age 2h)
  mgr: mon2(active, since 22m), standbys: mon1, mon3
  osd: 30 osds: 30 up, 30 in

data:
  pools:   1 pools, 1024 pgs
  objects: 0 objects, 0 B
  usage:   31 GiB used, 566 GiB / 597 GiB avail
  pgs:     1024 active+clean
</screen>
   </step>
   <step>
    <para>
     確認所有 OSD 節點均已重新開機，且重新開機後 OSD 已自動啟動。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="filestore2bluestore">
  <title>將 OSD 移轉至 BlueStore</title>

  <para>
   OSD BlueStore 是 OSD 精靈的新後端。從 SUSE Enterprise Storage 5 開始，它是預設的選項。與以檔案形式將物件儲存於 XFS 檔案系統中的 FileStore 相比，BlueStore 可提供更高的效能，因為它直接將物件儲存於基礎區塊裝置中。BlueStore 還能提供 FileStore 所不具備的其他功能，例如內建壓縮和 EC 覆寫。
  </para>

  <para>
   具體而言，在 BlueStore 中，OSD 包含一個「wal」(預寫記錄檔) 裝置和一個「db」(RocksDB 資料庫) 裝置。RocksDB 資料庫會儲存 BlueStore OSD 的中繼資料。依預設，這兩部裝置將位於 OSD 所在的同一部裝置上，但也可以將它們放置在其他 (例如速度更快的) 媒體上。
  </para>

  <para>
   在 SUSE Enterprise Storage 5 中，FileStore 和 BlueStore 均受支援，FileStore 和 BlueStore OSD 可在單個叢集中共存。在 SUSE Enterprise Storage 升級期間，FileStore OSD 不會自動轉換至 BlueStore。請注意，在尚未移轉至 BlueStore 的 OSD 上，將無法使用特定於 BlueStore 的功能。
  </para>

  <para>
   在轉換至 BlueStore 之前，OSD 需要執行 SUSE Enterprise Storage 5。轉換是個緩慢的過程，因為所有資料都需要重新寫入兩次。儘管移轉程序可能需要很長時間才能完成，但在此期間，叢集的工作不會中斷，所有用戶端都可繼續存取叢集。但是，移轉期間的效能勢必會下降，原因是需要重新平衡和回填叢集資料。
  </para>

  <para>
   請執行以下程序將 FileStore OSD 移轉至 BlueStore：
  </para>

  <tip>
   <title>關閉安全措施</title>
   <para>
    執行移轉所需的 Salt 指令會被安全措施阻止。若要關閉這些預防措施，請執行以下指令：
   </para>
<screen>
 <prompt>root@master # </prompt>salt-run disengage.safety
 </screen>
   <para>
    在繼續操作前先重建節點：
   </para>
<screen>
 <prompt>root@master # </prompt> salt-run rebuild.node <replaceable>TARGET</replaceable>
 </screen>
   <para>
    您也可以選擇個別重建每個節點。例如：
   </para>
<screen>
<prompt>root@master # </prompt> salt-run rebuild.node data1.ceph
 </screen>
   <para>
    <literal>rebuild.node</literal> 永遠會先移除節點上的所有 OSD，然後再重新建立這些 OSD。
   </para>
   <important>
    <para>
     如果一個 OSD 無法轉換，重新執行重建會損毀已轉換的 BlueStore OSD。如果不想重新執行重建，您可以執行以下指令：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.deploy <replaceable>TARGET</replaceable>
 </screen>
   </important>
  </tip>

  <para>
   移轉至 BlueStore 之後，物件計數將保持不變，磁碟使用率也幾乎相同。
  </para>
 </sect1>
 <sect1 xml:id="upgrade-appnodes-order">
  <title>升級應用程式節點</title>

  <para>
   請依以下順序升級應用程式節點：
  </para>

  <orderedlist>
   <listitem>
    <para>
     物件閘道
    </para>
    <itemizedlist>
     <listitem>
      <para>
       如果物件閘道的前端是負載平衡器，便可以在服務不中斷的情況下進行物件閘道的滾存升級。
      </para>
     </listitem>
     <listitem>
      <para>
       每次升級後，驗證物件閘道精靈是否正在執行，並使用 S3/Swift 用戶端進行測試。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     iSCSI 閘道
    </para>
    <itemizedlist>
     <listitem>
      <para>
       如果為 iSCSI 啟動器設定了多重路徑，便可以在服務不中斷的情況下進行 iSCSI 閘道的滾存升級。
      </para>
     </listitem>
     <listitem>
      <para>
       每次升級後，驗證 <systemitem class="daemon">lrbd</systemitem> 精靈是否正在執行，並使用啟動器進行測試。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     NFS Ganesha。<emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
    </para>
   </listitem>
   <listitem>
    <para>
     Samba 閘道。<emphasis role="bold">依照<xref linkend="upgrade-one-node"/>中所述的程序操作。</emphasis>
    </para>
   </listitem>
  </orderedlist>
 </sect1>
 <sect1 xml:id="upgrade-main-policy">
  <title>更新 <filename>policy.cfg</filename> 並使用 DeepSea 部署 Ceph Dashboard</title>

  <para>
   在管理節點上編輯<filename>/srv/pillar/ceph/proposals/policy.cfg</filename>，並套用以下變更：
  </para>

  <important>
   <title>無新服務</title>
   <para>
    在叢集升級期間，請新增新服務至 <filename>policy.cfg</filename> 檔案。請僅在完成升級後再變更叢集架構。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     移除 <literal>role-openattic</literal>。
    </para>
   </step>
   <step>
    <para>
     將 <literal>role-prometheus</literal> 和 <literal>role-grafana</literal> 新增至安裝了 Prometheus 和 Grafana 的節點 (通常是管理節點)。
    </para>
   </step>
   <step>
    <para>
     現在，系統會忽略 <literal>profile-<replaceable>PROFILE_NAME</replaceable></literal> 角色。新增新的相應角色，即 <literal>role-storage</literal> 一行。例如，針對現有
    </para>
<screen>
profile-default/cluster/*.sls
</screen>
    <para>
     新增
    </para>
<screen>
role-storage/cluster/*.sls
</screen>
   </step>
   <step>
    <para>
     同步所有 Salt 模組：
    </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.sync_all</screen>
   </step>
   <step>
    <para>
     執行 DeepSea 階段 1 和階段 2 來更新 Salt Pillar：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
   </step>
   <step>
    <para>
     清理 openATTIC：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.rescind.openattic
<prompt>root@master # </prompt>salt <replaceable>OA_MINION</replaceable> state.apply ceph.remove.openattic</screen>
   </step>
   <step>
    <para>
     取消設定「restart_igw」粒紋，以防階段 0 重新啟動尚未安裝的 iSCSI 閘道：
    </para>
<screen>Salt mastersalt '*' grains.delkey restart_igw</screen>
   </step>
   <step>
    <para>
     最後，執行完整的 DeepSea 階段 0-4：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <tip>
     <title>階段 3 執行期間發生「subvolume missing」錯誤</title>
     <para>
      DeepSea 階段 3 可能會失敗，並顯示如下錯誤：
     </para>
<screen>subvolume : ['/var/lib/ceph subvolume missing on 4510-2', \
'/var/lib/ceph subvolume missing on 4510-1', \
[...]
'See /srv/salt/ceph/subvolume/README.md']</screen>
     <para>
      在此情況下，您需要編輯 <filename role="bold">/srv/pillar/ceph/stack/global.yml</filename>，並新增下面一行：
     </para>
<screen>subvolume_init: disabled</screen>
     <para>
      然後重新整理 Salt Pillar 並重新執行 DeepSea stage.3：
     </para>
<screen><prompt>root@master # </prompt>salt '*' saltutil.refresh_pillar
 <prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     <para>
      DeepSea 成功完成 stage.3 後，Ceph Dashboard 即會執行。如需 Ceph Dashboard 功能的詳細綜覽，請參閱<xref linkend="ceph-dashboard"/>。
     </para>
     <para>
      若要列出執行儀表板的節點，請執行以下指令：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph mgr services | grep dashboard</screen>
     <para>
      若要列出管理員身分證明，請執行以下指令：
     </para>
<screen><prompt>root@master # </prompt>salt-call grains.get dashboard_creds</screen>
    </tip>
   </step>
   <step>
    <para>
     循序重新啟動物件閘道服務，以使用「beast」Web 伺服器取代過時的「civetweb」：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.restart.rgw.force</screen>
   </step>
   <step>
    <para>
     繼續之前，我們強烈建議先啟用 Ceph 遙測模組。如需詳細資訊和說明，請參閱<xref linkend="mgr-modules-telemetry"/>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="upgrade-drive-groups">
  <title>從以設定檔為基礎的部署移轉至 DriveGroups</title>

  <para>
   在 SUSE Enterprise Storage 5.5 中，DeepSea 提供了一個稱為「設定檔」的方法來描述您的 OSD 配置。自 SUSE Enterprise Storage 6 起，我們改用另一種稱為 <emphasis>DriveGroups</emphasis> 的方法 (如需更多詳細資料，請參閱<xref linkend="ds-drive-groups"/>)。
  </para>

  <note>
   <para>
    您不必立即就移轉至新方法。如 <command>salt-run osd.remove</command>、<command>salt-run osd.replace</command> 或 <command>salt-run osd.purge</command> 之類的破壞性操作仍然可用。但若要新增新 OSD，便需要移轉至新方法。
   </para>
  </note>

  <para>
   由於這些實作的方法不同，我們未提供自動移轉路徑。不過，我們提供了各種工具 (Salt 執行程式) 來儘可能使移轉變得簡單。
  </para>

  <sect2>
   <title>分析目前配置</title>
   <para>
    若要檢視有關目前部署的 OSD 的資訊，請使用以下指令：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.discover
</screen>
   <para>
    此外，您可以檢查 <filename>/srv/pillar/ceph/proposals/profile-*/</filename> 目錄中的檔案內容。其結構如下所示：
   </para>
<screen>
ceph:
  storage:
    osds:
      /dev/disk/by-id/scsi-drive_name: format: bluestore
      /dev/disk/by-id/scsi-drive_name2: format: bluestore
     </screen>
  </sect2>

  <sect2>
   <title>建立與目前配置相符的 DriveGroups</title>
   <para>
    如需 DriveGroups 規格的更多詳細資料，請參閱<xref linkend="ds-drive-groups-specs"/>。
   </para>
   <para>
    全新部署與升級場景的不同之處在於，要移轉的磁碟機已「使用過」。由於
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
   <para>
    僅會尋找未使用的磁碟，因此請使用以下指令
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list include_unavailable=True
</screen>
   <para>
    調整 DriveGroups，直至其與您目前的設定相符。如需更直觀地了解將要發生的情況，請使用以下指令。注意，如果沒有可用磁碟，該指令將不會產生任何輸出：
   </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report bypass_pillar=True
</screen>
   <para>
    如果您確認 DriveGroups 組態正確且您要套用新方法，請從 <filename>/srv/pillar/ceph/proposals/profile-<replaceable>PROFILE_NAME</replaceable>/</filename> 目錄中移除檔案，從 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 檔案中移除相應的 <literal>profile-<replaceable>PROFILE_NAME</replaceable>/cluster/*.sls</literal> 行，並執行 DeepSea 階段 2 以重新整理 Salt Pillar。
   </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
</screen>
   <para>
    請執行以下指令以確認結果：
   </para>
<screen>
<prompt>root@master # </prompt>salt target_node pillar.get ceph:storage
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   <warning>
    <title>DriveGroups 組態不正確</title>
    <para>
     如果未正確設定您的 DriveGroups，且您的設定中有備用磁碟，系統將依您指定的方式部署這些磁碟。我們建議執行以下指令：
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
   </warning>
  </sect2>

  <sect2 xml:id="upgrade-osd-deployment">
   <title>OSD 部署</title>
   <para>
    對於簡單的情形 (例如獨立的 OSD)，一段時間後將會發生移轉。每當您在叢集中移除或取代 OSD 時，系統便會以基於 LVM 的新 OSD 取代該 OSD。
   </para>
   <tip>
    <title>移轉至 LVM 形式</title>
    <para>
     一旦節點上有一個需要取代的「舊」OSD，所有與其共用裝置的 OSD 都需要移轉至基於 LVM 的形式。
    </para>
    <para>
     為確保完整性，請考慮移轉整個節點上的 OSD。
    </para>
   </tip>
  </sect2>

  <sect2>
   <title>更複雜的設定</title>
   <para>
    如果您使用的是比獨立 OSD 更複雜的設定，例如專屬 WAL/DB 或加密 OSD，則僅當移除了指定給該 WAL/DB 裝置的所有 OSD 時，才會發生移轉。這是因為 <command>ceph-volume</command> 指令會在部署前在磁碟上建立邏輯磁碟區。這樣可防止使用者混用基於分割區的部署和基於 LV 的部署。在此情況下，最好手動移除指定給 WAL/DB 裝置的所有 OSD，並使用 DriveGroups 方法重新部署它們。
   </para>
  </sect2>
 </sect1>
</chapter>
