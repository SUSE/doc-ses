<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_upgrade.xml" version="5.0" xml:id="cha.ceph.upgrade">
 <title>從舊版本升級</title>
 <info>
      <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
        <dm:maintainer>tbazant@suse.com</dm:maintainer>
        <dm:status>編輯</dm:status>
        <dm:deadline/>
        <dm:priority/>
        <dm:translation>yes</dm:translation>
        <dm:languages/>
        <dm:release>SES 5</dm:release>
      </dm:docmanager>
    </info>
    <para>
  本章介紹將 SUSE Enterprise Storage 從舊版本升級至最新版本的步驟。
 </para>
 <sect1 xml:id="ceph.upgrade.relnotes">
  <title>閱讀版本說明</title>

  <para>
   在版本說明中，您可以找到其他有關自 SUSE Enterprise Storage 的上一個版本發行後所進行的變更的資訊。檢查版本說明以瞭解︰
  </para>

  <itemizedlist>
   <listitem>
    <para>
     您的硬體是否有特殊的注意事項。
    </para>
   </listitem>
   <listitem>
    <para>
     所用的任何軟體套件是否已發生重大變更。
    </para>
   </listitem>
   <listitem>
    <para>
     是否需要對您的安裝施行特殊預防措施。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   版本說明還會提供無法及時編入手冊的資訊。它們還包含有關已知問題的說明。
  </para>

  <para>
   安裝套件 <package>release-notes-ses</package> 之後，本地的 <filename>/usr/share/doc/release-notes</filename> 目錄中或 <link xlink:href="https://www.suse.com/releasenotes/"/> 網頁上會提供版本說明。
  </para>
 </sect1>
 <sect1 xml:id="ceph.upgrade.general">
  <title>一般升級程序</title>

  <para>
   開始升級程序之前，請考慮以下幾項︰
  </para>

  <variablelist>
   <varlistentry>
    <term>升級順序</term>
    <listitem>
     <para>
      在升級 Ceph 叢集之前，需要針對 SCC 或 SMT 正確註冊基礎 SUSE Linux Enterprise Server 和 SUSE Enterprise Storage。當叢集已連線並正在執行時，便可升級叢集中的精靈。某些類型的精靈相依於其他精靈。例如，Ceph Object Gateway 相依於 Ceph Monitor 和 Ceph OSD 精靈。建議您依照以下順序升級︰
     </para>
     <orderedlist spacing="normal">
      <listitem>
       <para>
        Ceph Monitor
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph Manager
       </para>
      </listitem>
      <listitem>
       <para>
        Ceph OSD
       </para>
      </listitem>
      <listitem>
       <para>
        中繼資料伺服器
       </para>
      </listitem>
      <listitem>
       <para>
        物件閘道
       </para>
      </listitem>
      <listitem>
       <para>
        iSCSI 閘道
       </para>
      </listitem>
      <listitem>
       <para>
        NFS Ganesha
       </para>
      </listitem>
     </orderedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>刪除不需要的作業系統快照</term>
    <listitem>
     <para>
      移除節點作業系統分割區上不需要的檔案系統快照。如此可確保升級期間有足夠的可用磁碟空間。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>檢查叢集狀態</term>
    <listitem>
     <para>
      我們建議在開始升級程序之前先檢查叢集狀態。
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>逐一升級</term>
    <listitem>
     <para>
      我們建議逐一升級特定類型的所有精靈 (例如，所有監控程式精靈或所有 OSD 精靈)，以確保它們全部採用相同的版本。另外，還建議您在嘗試體驗某個版本的新功能之前，升級叢集中的所有精靈。
     </para>
     <para>
      升級特定類型的所有精靈之後，請檢查精靈的狀態。
     </para>
     <para>
      確保在升級所有監控程式之後，每個監控程式已重新加入最低核准人數︰
     </para>
<screen><prompt>root # </prompt>ceph mon stat</screen>
     <para>
      確保在升級所有 OSD 之後，每個 Ceph OSD 精靈已重新加入叢集︰
     </para>
<screen><prompt>root # </prompt>ceph osd stat</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     設定 <option>require-osd-release luminous</option> 旗標
    </term>
    <listitem>
     <para>
      將最後一個 OSD 升級至 SUSE Enterprise Storage 5 之後，監控程式節點會偵測所有 OSD 是否正在執行「luminous」版本的 Ceph，並可能會指出未設定 <option>require-osd-release luminous</option> osdmap 旗標。在這種情況下，您需要手動設定此旗標，以確認由於叢集已升級至「luminous」，無法將它降級回 Ceph「jewel」。執行以下指令來設定該旗標︰
     </para>
<screen><prompt>root@minion &gt; </prompt>sudo ceph osd require-osd-release luminous</screen>
     <para>
      該指令完成後，警告將會消失。
     </para>
     <para>
      在全新安裝的 SUSE Enterprise Storage 5 上，當 Ceph Monitor 建立啟始 osdmap 時，會自動設定此旗標，因此最終使用者無需執行任何動作。
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="ds.migrate.osd.encrypted">
  <title>升級期間加密 OSD</title>

  <para>
   從 SUSE Enterprise Storage 5 開始，預設會使用 BlueStore 而非 FileStore 來部署 OSD。雖然 BlueStore 支援加密，但預設以非加密模式部署 Ceph OSD。以下程序介紹了在升級期間加密 OSD 的步驟。我們假設部署 OSD 時使用的資料和 WAL/DB 磁碟都是乾淨的，且沒有分割區。如果先前曾使用過該磁碟，請執行<xref linkend="deploy.wiping.disk"/>中所述的程序進行抹除。
  </para>

  <important>
   <title>一次一個 OSD</title>
   <para>
    您需要逐一部署加密的 OSD，不能同時部署。這是因為 OSD 的資料將會用完，叢集反覆數次執行重新平衡。
   </para>
  </important>

  <procedure>
   <step>
    <para>
     為您的部署確定 <option>bluestore block db size</option> 和 <option>bluestore block wal size</option> 的值，並在 Salt Master 上的 <filename>/srv/salt/ceph/configuration/files/ceph.conf.d/global.conf</filename> 檔案中新增這些值。需要以位元組指定這些值。
    </para>
<screen>
[global]
bluestore block db size = 48318382080
bluestore block wal size = 2147483648
</screen>
    <para>
     如需自訂 <filename>ceph.conf</filename> 檔案的詳細資訊，請參閱<xref linkend="ds.custom.cephconf"/>。
    </para>
   </step>
   <step>
    <para>
     執行 DeepSea 階段 3 以分發變更︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     驗證相關 OSD 節點上是否已更新 <filename>ceph.conf</filename> 檔案︰
    </para>
<screen>
<prompt>root@minion &gt; </prompt>cat /etc/ceph/ceph.conf
</screen>
   </step>
   <step>
    <para>
     對 <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions</filename> 目錄下與您要加密的 OSD 相關的 *.yml 檔案進行編輯。再次檢查它們的路徑是否與 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 檔案中定義的路徑一致，以確保您修改的是正確的 *.yml 檔案。
    </para>
    <important>
     <title>長磁碟識別碼</title>
     <para>
      在 <filename>/srv/pillar/ceph/proposals/profile-default/stack/default/ceph/minions/*.yml</filename> 檔案中識別 OSD 磁碟時，請使用長磁碟識別碼。
     </para>
    </important>
    <para>
     下面顯示了一個 OSD 組態範例。請注意，由於我們需要加密，因此移除了 <option>db_size</option> 和 <option>wal_size</option> 選項︰
    </para>
<screen>
ceph:
 storage:
   osds:
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_007027b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
     /dev/disk/by-id/scsi-SDELL_PERC_H730_Mini_00d146b1065faa972100d34d7aa06d86:
       format: bluestore
       encryption: dmcrypt
       db: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
       wal: /dev/disk/by-id/nvme-INTEL_SSDPEDMD020T4D_HHHL_NVMe_2000GB_PHFT642400HV2P0EGN
</screen>
   </step>
   <step>
    <para>
     執行 DeepSea 階段 2 和 3 以加密模式部署新的區塊儲存 OSD︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
    <para>
     您可以執行 <command>ceph -s</command> 或 <command>ceph osd tree</command> 來監視進度。在下一個 OSD 節點上重複該程序前，請務必使叢集重新平衡。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5">
  <title>從 SUSE Enterprise Storage 4 (DeepSea 部署) 升級至版本 5</title>

  <important xml:id="u4to5.softreq">
   <title>軟體要求</title>
   <para>
    您需要在要升級的所有 Ceph 節點上安裝以下軟體並將其更新到最新的套件版本，然後才能開始升級程序︰
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    此外，在開始升級之前，需要透過執行 <command>zypper migration</command> (或偏好的升級方式) 將 Salt Master 節點升級至 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5。
   </para>
  </important>

  <warning>
   <title>升級前的注意事項</title>
   <itemizedlist>
    <listitem>
     <para>
      檢查 AppArmor 服務是否正在執行，並在每個叢集節點上停用該服務。啟動 YaST AppArmor 模組，選取<guimenu>設定</guimenu>，然後取消選取<guimenu>啟用 Apparmor</guimenu> 核取方塊。按一下<guimenu>完成</guimenu>進行確認。
     </para>
     <para>
      請注意，在啟用 AppArmor 的情況下，SUSE Enterprise Storage 將<emphasis>無法</emphasis>正常運作。
     </para>
    </listitem>
    <listitem>
     <para>
      儘管叢集在升級期間可完全正常運作，但 DeepSea 會設定「noout」旗標，用於阻止 Ceph 在停機期間重新平衡資料，從而避免不必要的資料傳輸。
     </para>
    </listitem>
    <listitem>
     <para>
      為了最佳化升級程序，DeepSea 會依據節點的指定角色並遵循 Ceph 上游的建議，依照以下順序升級節點︰MON、MGR、OSD、MDS、RGW、IGW 和 NFS Ganesha。
     </para>
     <para>
      請注意，如果節點執行多個服務，DeepSea 將無法阻止未依上述順序執行升級。
     </para>
    </listitem>
    <listitem>
     <para>
      儘管 Ceph 叢集在升級期間可正常運作，但可能需要將節點重新開機才能套用新核心版本等設定。為了減少等待中的 I/O 操作，建議您在升級期間拒絕內送申請。
     </para>
    </listitem>
    <listitem>
     <para>
      叢集升級可能要花費很長時間 — 所需時間大約為升級一部機器的時間乘以叢集節點數。
     </para>
    </listitem>
    <listitem>
     <para>
      從 Ceph Luminous 開始，不再支援 <option>osd crush location</option> 組態選項。請在升級前更新您的 DeepSea 組態檔案，以使用 <command>crush location</command>。
     </para>
    </listitem>
   </itemizedlist>
  </warning>

  <para>
   若要將 SUSE Enterprise Storage 4 叢集升級至版本 5，請執行以下步驟︰
  </para>

  <procedure>
   <step>
    <para>
     執行以下指令設定新的內部物件排序順序︰
    </para>
<screen><prompt>root # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      若要驗證指令是否成功，建議您執行以下指令
     </para>
<screen><prompt>root # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     使用 <command>rpm -q deepsea</command> 驗證 Salt Master 節點上的 DeepSea 套件版本是否至少以 <literal>0.7</literal> 開頭。例如︰
    </para>
<screen><prompt>root # </prompt>rpm -q deepsea
deepsea-0.7.27+git.0.274c55d-5.1</screen>
    <para>
     如果 DeepSea 套件版本號碼以 0.6 開頭，請再次確認是否已成功將 Salt Master 節點移轉至 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5 (請參閱本節開頭的<xref linkend="u4to5.softreq"/>)。在開始升級程序之前，必須滿足此先決條件。
    </para>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       如果已將系統註冊到 SUSEConnect 並使用 SCC/SMT，則不需要執行進一步的動作。繼續執行<xref linkend="step.updatepillar"/>。
      </para>
     </step>
     <step>
      <para>
       如果您使用的<emphasis role="bold">不是</emphasis> SCC/SMT 而是媒體 ISO 或其他套件來源，請手動新增以下儲存庫︰SLE12-SP3 基礎、SLE12-SP3 更新、SES5 基礎和 SES5 更新。您可以使用 <command>zypper</command> 指令來執行此操作。首先，移除所有現有的軟體儲存庫，然後新增所需的新儲存庫，最後重新整理儲存庫來源︰
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       之後，變更您的 Pillar 資料以使用另一個策略。編輯
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       並新增下行︰
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        <literal>zypper-dup</literal> 策略要求您手動新增最新的軟體儲存庫，而預設的 <literal>zypper-migration</literal> 則相依於 SCC/SMT 提供的儲存庫。
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step xml:id="step.updatepillar">
    <para>
     更新 Pillar︰
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> saltutil.sync_all</screen>
    <para>
     如需 Salt Minion 定位的詳細資料，請參閱<xref linkend="ds.minion.targeting"/>。
    </para>
   </step>
   <step>
    <para>
     驗證是否已成功寫入 Pillar︰
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get upgrade_init</screen>
    <para>
     該指令的輸出應會鏡像複製您新增的項目。
    </para>
   </step>
   <step>
    <para>
     升級 Salt Minion︰
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates.salt</screen>
   </step>
   <step>
    <para>
     驗證是否已升級所有 Salt Minion︰
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version</screen>
   </step>
   <step>
    <para>
     包含叢集的 Salt Minion。如需更多詳細資料，請參閱<xref linkend="ds.depl.stages"/>的<xref linkend="ds.minion.targeting"/>。
    </para>
   </step>
   <step>
    <para>
     開始升級 SUSE Linux Enterprise Server 和 Ceph︰
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade</screen>
    <tip>
     <title>重新開機後重新執行</title>
     <para>
      如果該程序導致將 Salt Master 重新開機，請重新執行該指令，以再次啟動 Salt Minion 的升級程序。
     </para>
    </tip>
   </step>
   <step>
    <para>
     升級後，檢查所有節點上的 AppArmor 是否已停用並已停止︰
    </para>
<screen><prompt>root # </prompt>systemctl disable apparmor.service
systemctl stop apparmor.service</screen>
   </step>
   <step>
    <para>
     升級後，Ceph Manager 尚未安裝。若要讓叢集保持正常狀態，請執行以下操作︰
    </para>
    <substeps>
     <step>
      <para>
       執行階段 0 以啟用 Salt REST API︰
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
     </step>
     <step>
      <para>
       執行階段 1 以建立 <filename>role-mgr/</filename> 子目錄︰
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
     </step>
     <step>
      <para>
       依據<xref linkend="policy.configuration"/>中所述編輯 <guimenu>policy.cfg</guimenu>，並將一個 Ceph Manager 角色新增到部署了 Ceph Monitor 的節點。此外，請為叢集的某個節點新增 openATTIC 角色。如需更多詳細資料，請參閱 <xref linkend="ceph.oa"/>。
      </para>
     </step>
     <step>
      <para>
       執行階段 2 以更新 Pillar︰
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
     </step>
     <step>
      <para>
       DeepSea 現在用另一種方法來產生 <filename>ceph.conf</filename> 組態檔案，如需更多詳細資料，請參閱<xref linkend="ds.custom.cephconf"/>。
      </para>
     </step>
     <step>
      <para>
       執行階段 3 以部署 Ceph Manager︰
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
     </step>
     <step>
      <para>
       執行階段 4 以正確設定 openATTIC︰
      </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
     </step>
    </substeps>
    <note>
     <title>Ceph 金鑰功能不相符</title>
     <para>
      如果 <literal>ceph.stage.3</literal> 失敗並顯示「錯誤 EINVAL︰實體 client.bootstrap-osd 存在，但功能不相符」，則表示現有叢集的 <literal>client.bootstrap.osd</literal> 金鑰的金鑰功能 (caps) 與 DeepSea 嘗試設定的功能不相符。在紅色錯誤訊息的上方，可以看到失敗指令 <command>ceph auth</command> 的傾印。請查看此指令，檢查所用的金鑰 ID 和檔案。對於 <literal>client.bootstrap-osd</literal>，該指令是
     </para>
<screen><prompt>root # </prompt>ceph auth add client.bootstrap-osd \
 -i /srv/salt/ceph/osd/cache/bootstrap.keyring</screen>
     <para>
      若要解決金鑰功能不相符問題，請檢查 DeepSea 正在嘗試部署的金鑰圈檔案的內容，例如︰
     </para>
<screen><prompt>cephadm &gt; </prompt>cat /srv/salt/ceph/osd/cache/bootstrap.keyring
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mgr = "allow r"
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      將此內容與 <command>ceph auth get client.bootstrap-osd</command> 的輸出進行比較︰
     </para>
<screen><prompt>root # </prompt>ceph auth get client.bootstrap-osd
exported keyring for client.bootstrap-osd
[client.bootstrap-osd]
     key = AQD6BpVZgqVwHBAAQerW3atANeQhia8m5xaigw==
     caps mon = "allow profile bootstrap-osd"</screen>
     <para>
      您會注意到，後一個金鑰缺少 <literal>caps mgr = "allow r"</literal>。若要解決此問題，請執行︰
     </para>
<screen><prompt>root # </prompt>ceph auth caps client.bootstrap-osd mgr \
 "allow r" mon "allow profile bootstrap-osd"</screen>
     <para>
      現在，執行 <literal>ceph.stage.3</literal> 應能成功。
     </para>
     <para>
      執行 <literal>ceph.stage.4</literal> 時，中繼資料伺服器和物件閘道金鑰圈可能會出現相同的問題。您可以套用上述相同的程序︰檢查失敗的指令、要部署的金鑰圈檔案，以及現有金鑰的功能。然後執行 <command>ceph auth caps</command> 更新現有的金鑰功能，使之與 DeepSea 正在部署的功能相符。
     </para>
    </note>
   </step>
  </procedure>

  <important>
   <title>升級失敗</title>
   <para>
    如果叢集處於「HEALTH_ERR」狀態的持續時間超過 300 秒，或者每個指定角色的服務之一處於停機狀態的持續時間超過 900 秒，則表示升級失敗。在這種情況下，請嘗試找出問題並予以解決，然後重新執行升級程序。請注意，在虛擬化環境中，逾時會更短。
   </para>
  </important>

  <important>
   <title>將 OSD 重新開機</title>
   <para>
    升級至 SUSE Enterprise Storage 5 之後，FileStore OSD 啟動所需的時間大約會增加五分鐘，因為 OSD 將對其磁碟中的檔案執行一次性轉換。
   </para>
  </important>

  <tip>
   <title>檢查叢集元件/節點的版本</title>
   <para>
    如果您需要確定個別叢集元件和節點的版本 (例如，確定所有節點在升級後是否真正處於相同的修補程式層級)，可以執行
   </para>
<screen><prompt>root@master # </prompt>salt-run status.report</screen>
   <para>
    該指令將會瀏覽所有連接的 Salt Minion，並掃描 Ceph、Salt 和 SUSE Linux Enterprise Server 的版本號，最後提供一份報告，其中會列出大多數節點的版本，並顯示其版本與大多數節點不相同的節點。
   </para>
  </tip>

  <sect2 xml:id="filestore2bluestore">
   <title>將 OSD 移轉至 BlueStore</title>
   <para>
    OSD BlueStore 是 OSD 精靈的新後端。從 SUSE Enterprise Storage 5 開始，它是預設的選項。與以檔案形式將物件儲存於 XFS 檔案系統中的 FileStore 相比，BlueStore 可提供更高的效能，因為它直接將物件儲存於基礎區塊裝置中。BlueStore 還能提供 FileStore 所不具備的其他功能，例如內建壓縮和 EC 覆寫。
   </para>
   <para>
    具體而言，在 BlueStore 中，OSD 包含一個「wal」(預寫記錄檔) 裝置和一個「db」(RocksDB 資料庫) 裝置。RocksDB 資料庫會儲存 BlueStore OSD 的中繼資料。預設情況下，這兩個裝置存放在 OSD 所在的同一部裝置上，但也可以將它們放置在更快/不同的媒體上。
   </para>
   <para>
    在 SES5 中，FileStore 和 BlueStore 均受支援，FileStore 和 BlueStore OSD 可在單一叢集中共存。在 SUSE Enterprise Storage 升級期間，FileStore OSD 不會自動轉換至 BlueStore。請注意，在尚未移轉至 BlueStore 的 OSD 上，將無法使用特定於 BlueStore 的功能。
   </para>
   <para>
    在轉換至 BlueStore 之前，OSD 需要執行 SUSE Enterprise Storage 5。轉換是個緩慢的過程，因為所有資料都需要重新寫入兩次。儘管移轉程序可能需要很長時間才能完成，但在此期間，叢集的工作不會中斷，所有用戶端都可繼續存取叢集。但是，移轉期間的效能勢必會下降，原因是需要重新平衡和回填叢集資料。
   </para>
   <para>
    請執行以下程序將 FileStore OSD 移轉至 BlueStore︰
   </para>
   <tip>
    <title>關閉安全措施</title>
    <para>
     執行移轉所需的 Salt 指令會被安全措施阻止。若要關閉這些預防措施，請執行以下指令︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run disengage.safety
</screen>
   </tip>
   <procedure>
    <step>
     <para>
      移轉硬體設定檔︰
     </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.policy</screen>
     <para>
      此執行程式會移轉 <filename>policy.cfg</filename> 檔案目前正在使用的所有硬體設定檔。它會處理 <filename>policy.cfg</filename>，尋找使用原始資料結構的任何硬體設定檔，並將其轉換為新的資料結構。這樣會產生名為「migrated-<replaceable>original_name</replaceable>」的新硬體設定檔。<filename>policy.cfg</filename> 也會更新。
     </para>
     <para>
      如果原始組態包含單獨的記錄，BlueStore 組態將對該 OSD 的「wal」和「db」使用相同的裝置。
     </para>
    </step>
    <step>
     <para>
      DeepSea 透過將 OSD 的權數設定為 0 (「抽取」資料，直到 OSD 已空) 來移轉 OSD。您可以逐一移轉 OSD，也可以一次性移轉所有 OSD。在任一情況下，當 OSD 已空時，協調化操作會移除該 OSD，然後使用新組態重新建立 OSD。
     </para>
     <tip>
      <title>建議的方法</title>
      <para>
       如果您有大量的實體儲存節點，或幾乎沒有任何資料，請使用 <command>ceph.migrate.nodes</command>。如果一個節點佔用的容量小於總容量的 10%，則使用 <command>ceph.migrate.nodes</command> 同時移動這些 OSD 中所有資料的速度可能會略快一點。
      </para>
      <para>
       如果您不確定要使用哪種方法，或者網站包含的儲存節點較少 (例如，每個節點包含的資料大於叢集資料的 10%)，請選擇 <command>ceph.migrate.osds</command>。
      </para>
     </tip>
     <substeps>
      <step>
       <para>
        若要逐一移轉 OSD，請執行︰
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.osds</screen>
      </step>
      <step>
       <para>
        若要同時移轉每個節點上的所有 OSD，請執行︰
       </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.migrate.nodes</screen>
      </step>
     </substeps>
     <tip>
      <para>
       協調化不提供有關移轉進度的反饋，您可以使用
      </para>
<screen><prompt>root # </prompt>ceph osd tree</screen>
      <para>
       定期查看哪些 OSD 的權數為 0。
      </para>
     </tip>
    </step>
   </procedure>
   <para>
    移轉至 BlueStore 之後，物件計數將保持不變，磁碟使用率也幾乎相同。
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5cephdeloy">
  <title>從 SUSE Enterprise Storage 4 (<command>ceph-deploy</command> 部署) 升級至版本 5</title>

  <important>
   <title>軟體要求</title>
   <para>
    您需要在要升級的所有 Ceph 節點上安裝以下軟體並將其更新到最新的套件版本，然後才能開始升級程序︰
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
   <para>
    為叢集選擇 Salt Master。如果叢集部署了 Calamari，則 Calamari 節點已經<emphasis>是</emphasis> Salt Master。否則，您用來執行 <command>ceph-deploy</command> 指令的管理節點將成為 Salt Master。
   </para>
   <para>
    在開始以下程序之前，您需要執行 <command>zypper migration</command> (或偏好的升級方式) 將 Salt Master 節點升級至 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5。
   </para>
  </important>

  <para>
   若要將使用 <command>ceph-deploy</command> 部署的 SUSE Enterprise Storage 4 叢集升級至版本 5，請執行以下步驟︰
  </para>

  <procedure xml:id="upgrade4to5cephdeploy.all">
   <title>所有叢集節點都要執行的步驟 (包括 Calamari 節點)</title>
   <step>
    <para>
     安裝 SLE-12-SP2/SES4 中的 <systemitem>salt</systemitem> 套件︰
    </para>
<screen><prompt>root # </prompt>zypper install salt</screen>
   </step>
   <step>
    <para>
     安裝 SLE-12-SP2/SES4 中的 <systemitem>salt-minion</systemitem> 套件，然後啟用並啟動相關服務︰
    </para>
<screen><prompt>root # </prompt>zypper install salt-minion
<prompt>root # </prompt>systemctl enable salt-minion
<prompt>root # </prompt>systemctl start salt-minion</screen>
   </step>
   <step>
    <para>
     確定主機名稱「salt」可解析成 Salt Master 節點的 IP 位址。如果無法透過主機名稱 <literal>salt</literal> 連接 Salt Master，請編輯檔案 <filename>/etc/salt/minion</filename>，或建立包含以下內容的新檔案 <filename>/etc/salt/minion.d/master.conf</filename>︰
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <tip>
     <para>
      現有 Salt Minion 的 <filename>/etc/salt/minion.d/calamari.conf</filename> 中已設定 <option>master:</option> 選項。使用什麼組態檔案名稱無關緊要，但 <filename>/etc/salt/minion.d/</filename> 目錄非常重要。
     </para>
    </tip>
    <para>
     如果對上述組態檔案執行了任何變更，請在所有 Salt Minion 上重新啟動 Salt 服務︰
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       如果已將系統註冊到 SUSEConnect 並使用 SCC/SMT，則不需要執行進一步的動作。
      </para>
     </step>
     <step>
      <para>
       如果您使用的<emphasis role="bold">不是</emphasis> SCC/SMT 而是媒體 ISO 或其他套件來源，請手動新增以下儲存庫︰SLE12-SP3 基礎、SLE12-SP3 更新、SES5 基礎和 SES5 更新。您可以使用 <command>zypper</command> 指令來執行此操作。首先，移除所有現有的軟體儲存庫，然後新增所需的新儲存庫，最後重新整理儲存庫來源︰
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
     </step>
    </substeps>
   </step>
  </procedure>

  <procedure xml:id="upgrade4to5cephdeploy.admin">
   <title>Salt Master 節點執行的步驟</title>
   <step>
    <para>
     執行以下指令設定新的內部物件排序順序︰
    </para>
<screen><prompt>root@master # </prompt>ceph osd set sortbitwise</screen>
    <tip>
     <para>
      若要驗證指令是否成功，建議您執行以下指令
     </para>
<screen><prompt>root@master # </prompt>ceph osd dump --format json-pretty | grep sortbitwise
 "flags": "sortbitwise,recovery_deletes,purged_snapdirs",</screen>
    </tip>
   </step>
   <step>
    <para>
     將 Salt Master 節點升級至 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5。對於在 SCC 中註冊的系統，請使用 <command>zypper migration</command>。如果您是手動提供所需的軟體儲存庫，請使用 <command>zypper dup</command>。升級後，請先確定 Salt Master 節點上只有 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5 的儲存庫處於使用中狀態 (且已重新整理)，然後再繼續。
    </para>
   </step>
   <step>
    <para>
     如果該 Master 節點尚不存在，請安裝 <systemitem>salt-master</systemitem> 套件，然後啟用並啟動相關服務︰
    </para>
<screen><prompt>root@master # </prompt>zypper install salt-master
<prompt>root@master # </prompt>systemctl enable salt-master
<prompt>root@master # </prompt>systemctl start salt-master</screen>
   </step>
   <step>
    <para>
     透過列出所有 Salt Minion 的金鑰來驗證這些 Minion 是否存在︰
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     將所有 Salt Minion 金鑰新增到 Salt Master (包括 Minion Master)︰
    </para>
<screen><prompt>root@master # </prompt>salt-key -A -y</screen>
   </step>
   <step>
    <para>
     確保已接受所有 Salt Minion 的金鑰︰
    </para>
<screen><prompt>root@master # </prompt>salt-key -L</screen>
   </step>
   <step>
    <para>
     確保 Salt Master 節點上的軟體是最新的︰
    </para>
<screen><prompt>root@master # </prompt>zypper migration</screen>
   </step>
   <step>
    <para>
     安裝 <systemitem>deepsea</systemitem> 套件︰
    </para>
<screen><prompt>root@master # </prompt>zypper install deepsea</screen>
   </step>
   <step>
    <para>
     包含叢集的 Salt Minion。如需更多詳細資料，請參閱<xref linkend="ds.depl.stages"/>的<xref linkend="ds.minion.targeting"/>。
    </para>
   </step>
   <step>
    <para>
     輸入 <command>ceph-deploy</command> 安裝的現有叢集︰
    </para>
<screen><prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster</screen>
    <para>
     該指令將執行以下操作︰
    </para>
    <itemizedlist>
     <listitem>
      <para>
       將全部所需的 Salt 和 DeepSea 模組分發到所有 Salt Minion。
      </para>
     </listitem>
     <listitem>
      <para>
       檢查執行中的 Ceph 叢集，並在 <filename>/srv/pillar/ceph/proposals</filename> 中填入叢集的配置。
      </para>
      <para>
       將使用與所有偵測到的執行中 Ceph 服務相符的角色建立 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename>。檢視此檔案以驗證每個現有的 MON、OSD、RGW 和 MDS 節點是否具有適當的角色。OSD 節點將輸入到 <filename>profile-import/</filename> 子目錄，因此您可以檢查 <filename>/srv/pillar/ceph/proposals/profile-import/cluster/</filename> 和 <filename>/srv/pillar/ceph/proposals/profile-import/stack/default/ceph/minions/</filename> 中的檔案，以確認是否正確選取了 OSD。
      </para>
      <note>
       <para>
        對於 Salt Master 節點，產生的 <filename>policy.cfg</filename> 只會套用偵測到的 Ceph 服務的角色「role-mon」、「role-mgr」、「role-mds」、「role-rgw」、「role-admin」和「role-master」。如果需要其他角色，則需手動將其新增到該檔案 (請參閱<xref linkend="policy.role.assignment"/>)。
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       現有叢集的 <filename>ceph.conf</filename> 將儲存到 <filename>/srv/salt/ceph/configuration/files/ceph.conf.import</filename>。
      </para>
     </listitem>
     <listitem>
      <para>
       <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> 將包含叢集的 fsid、叢集網路和公用網路，並指定 <option>configuration_init: default-import</option> 選項，如此 DeepSea 將使用前面所述的 <filename>ceph.conf.import</filename> 組態檔案，而不是使用 DeepSea 的預設範本 <filename>/srv/salt/ceph/configuration/files/ceph.conf.j2</filename>。
      </para>
      <note>
       <title>自訂 <filename>ceph.conf</filename></title>
       <para>
        如果您需要將 <filename>ceph.conf</filename> 檔案與自訂變更整合，請等待輸入/升級程序成功完成。然後，編輯 <filename>/srv/pillar/ceph/proposals/config/stack/default/ceph/cluster.yml</filename> 檔案，備註掉下面一行︰
       </para>
<screen>
configuration_init: default-import
</screen>
       <para>
        儲存檔案，然後依據<xref linkend="ds.custom.cephconf"/>中的資訊操作。
       </para>
      </note>
     </listitem>
     <listitem>
      <para>
       叢集的各個金鑰圈將儲存到以下目錄︰
      </para>
<screen>/srv/salt/ceph/admin/cache/
/srv/salt/ceph/mon/cache/
/srv/salt/ceph/osd/cache/
/srv/salt/ceph/mds/cache/
/srv/salt/ceph/rgw/cache/</screen>
      <para>
       確認這些金鑰圈檔案都存在，並且以下目錄中<emphasis>沒有</emphasis>金鑰圈檔案 (低於 SUSE Enterprise Storage 5 的版本中不存在 Ceph Manager)︰
      </para>
<screen>
/srv/salt/ceph/mgr/cache/
</screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command> 指令不會處理 openATTIC 組態的輸入。您需要手動編輯 <filename>policy.cfg</filename> 檔案，並新增 <literal>role-openattic</literal> 一行。如需更多詳細資料，請參閱 <xref linkend="policy.configuration"/>。
    </para>
   </step>

   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command> 指令不會處理 iSCSI 閘道組態的輸入。如果您的叢集包含 iSCSI 閘道，請手動輸入其組態︰
    </para>
    <substeps>
     <step>
      <para>
       在其中一個 iSCSI 閘道節點上，輸出目前的 <filename>lrbd.conf</filename>，然後將其複製到 Salt Master 節點︰
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt;/tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       在 Salt Master 節點上，將預設 iSCSI 閘道組態新增到 DeepSea 設定中︰
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       在 <filename>policy.cfg</filename> 中新增 iSCSI 閘道角色，然後儲存檔案︰
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     執行階段 1 以建立所有可能的角色︰
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
   </step>
   <step>
    <para>
     在 <filename>/srv/pillar/ceph/stack</filename> 下產生所需的子目錄︰
    </para>
<screen><prompt>root@master # </prompt>salt-run push.proposal</screen>
   </step>
   <step>
    <para>
     確認有正常執行的受 DeepSea 管理叢集且為其正確指定了角色︰
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> pillar.get roles</screen>
    <para>
     將輸出與叢集的實際配置進行比較。
    </para>
   </step>
   <step>
    <para>
     Calamari 會持續執行一個排程的 Salt 工作來檢查叢集狀態，請移除該工作︰
    </para>
<screen>
<prompt>root@minion &gt; </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
   </step>
   <step>
    <para>
     然後，執行<xref linkend="ceph.upgrade.4to5"/>中所述的程序。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.4to5crowbar">
  <title>從 SUSE Enterprise Storage 4 (Crowbar 部署) 升級至版本 5</title>

  <important>
   <title>軟體要求</title>
   <para>
    您需要在要升級的所有 Ceph 節點上安裝以下軟體並將其更新到最新的套件版本，然後才能開始升級程序︰
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP2
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 4
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   若要將使用 Crowbar 部署的 SUSE Enterprise Storage 4 升級至版本 5，請執行以下步驟︰
  </para>

  <procedure>
   <step>
    <para>
     對每個 Ceph 節點 (包括 Calamari 節點) 停止並停用與 Crowbar 相關的所有服務︰
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl stop chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable chef-client
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_join
<prompt>root@minion &gt; </prompt>sudo systemctl disable crowbar_notify_shutdown
</screen>
   </step>
   <step>
    <para>
     對每個 Ceph 節點 (包括 Calamari 節點)，確認軟體儲存庫指向 SUSE Enterprise Storage 5 和 SUSE Linux Enterprise Server 12 SP3 產品。如果仍然存在指向較舊產品版本的儲存庫，請將其停用。
    </para>
   </step>
   <step>
    <para>
     對每個 Ceph 節點 (包括 Calamari 節點)，確認
     <package>salt-minion</package> 已安裝。如果未安裝，請予以安裝︰
    </para>
<screen><prompt>root@minion &gt; </prompt>sudo zypper in salt salt-minion</screen>
   </step>
   <step>
    <para>
     對於未安裝 <package>salt-minion</package>
     套件的 Ceph 節點，請建立 <filename>/etc/salt/minion.d/master.conf</filename> 檔案，並在其中將 <option>master</option> 選項指向 Calamari 節點的完整主機名稱︰
    </para>
<screen>master: <replaceable>full_calamari_hostname</replaceable></screen>
    <tip>
     <para>
      現有 Salt Minion 的 <filename>/etc/salt/minion.d/calamari.conf</filename> 中已設定 <option>master:</option> 選項。使用什麼組態檔案名稱無關緊要，但 <filename>/etc/salt/minion.d/</filename> 目錄非常重要。
     </para>
    </tip>
    <para>
     啟用並啟動 <systemitem class="daemon">salt-minion</systemitem> 服務︰
    </para>
<screen>
<prompt>root@minion &gt; </prompt>sudo systemctl enable salt-minion
<prompt>root@minion &gt; </prompt>sudo systemctl start salt-minion
</screen>
   </step>
   <step>
    <para>
     在 Calamari 節點上接受其他所有 Salt Minion 金鑰︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-key -L
[...]
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
[...]

<prompt>root@master # </prompt>salt-key -A
The following keys are going to be accepted:
Unaccepted Keys:
d52-54-00-16-45-0a.example.com
d52-54-00-70-ac-30.example.com
Proceed? [n/Y] y
Key for minion d52-54-00-16-45-0a.example.com accepted.
Key for minion d52-54-00-70-ac-30.example.com accepted.
</screen>
   </step>
   <step>
    <para>
     如果 Ceph 部署在公用網路上，且沒有 VLAN 介面，請在 Crowbar 公用網路為 Calamari 節點新增一個 VLAN 介面。
    </para>
   </step>
   <step>
    <para>
     使用 <command>zypper migration</command> 或您偏好的方法將 Calamari 節點升級至 SUSE Linux Enterprise Server 12 SP3 和 SUSE Enterprise Storage 5。此後，Calamari 節點便成為 <emphasis>Salt Master</emphasis>。升級後，將 Salt Master 重新開機。
    </para>
   </step>
   <step>
    <para>
     在 Salt Master 上安裝 DeepSea︰
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     指定 <option>deepsea_minions</option> 選項，以將正確的 Salt Minion 群組併入部署階段。如需更多詳細資料，請參閱 <xref linkend="ds.minion.targeting.dsminions"/>。
    </para>
   </step>
   <step>
    <para>
     DeepSea 期望所有 Ceph 節點都具有相同的 <filename>/etc/ceph/ceph.conf</filename>。Crowbar 會為每個節點部署一個稍有差別的 <filename>ceph.conf</filename>，因此您需要進行整合︰
    </para>
    <itemizedlist>
     <listitem>
      <para>
       移除 <option>osd crush location hook</option> 選項，它是由 Calamari 所新增。
      </para>
     </listitem>
     <listitem>
      <para>
       從 <literal>[mon]</literal> 區段中移除 <option>public addr</option> 選項。
      </para>
     </listitem>
     <listitem>
      <para>
       從 <option>mon host</option> 選項中移除連接埠號碼。
      </para>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     之前，如果您執行的是物件閘道，Crowbar 會部署一個單獨的 <filename>/etc/ceph/ceph.conf.radosgw</filename> 檔案，以便將 Keystone 機密與一般 <filename>ceph.conf</filename> 檔案區分開。Crowbar 還會新增一個自訂 <filename>/etc/systemd/system/ceph-radosgw@.service</filename> 檔案。由於 DeepSea 不支援此檔案，您需要將其移除︰
    </para>
    <itemizedlist>
     <listitem>
      <para>
       將所有 <literal>[client.rgw....]</literal> 區段 (<filename>ceph.conf.radosgw</filename> 檔案中) 附加至所有節點上的 <filename>/etc/ceph/ceph.conf</filename> 中。
      </para>
     </listitem>
     <listitem>
      <para>
       在物件閘道節點上執行以下指令︰
      </para>
<screen><prompt>root@minion &gt; </prompt>rm /etc/systemd/system/ceph-radosgw@.service
systemctl reenable ceph-radosgw@rgw.public.$<replaceable>hostname</replaceable></screen>
     </listitem>
    </itemizedlist>
   </step>
   <step>
    <para>
     再次檢查從 Salt Master 執行時 <command>ceph status</command> 是否起作用︰
    </para>
<screen><prompt>root@master # </prompt>ceph status
cluster a705580c-a7ae-4fae-815c-5cb9c1ded6c2
health HEALTH_OK
[...]
</screen>
   </step>
   <step>
    <para>
     輸入現有叢集︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run populate.engulf_existing_cluster
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run push.proposal
</screen>
   </step>

   <step>
    <para>
     <command>salt-run populate.engulf_existing_cluster</command> 指令不會處理 iSCSI 閘道組態的輸入。如果您的叢集包含 iSCSI 閘道，請手動輸入其組態︰
    </para>
    <substeps>
     <step>
      <para>
       在其中一個 iSCSI 閘道節點上，輸出目前的 <filename>lrbd.conf</filename>，然後將其複製到 Salt Master 節點︰
      </para>
<screen>
<prompt>root@minion &gt; </prompt>lrbd -o &gt; /tmp/lrbd.conf
<prompt>root@minion &gt; </prompt>scp /tmp/lrbd.conf admin:/srv/salt/ceph/igw/cache/lrbd.conf
</screen>
     </step>
     <step>
      <para>
       在 Salt Master 節點上，將預設 iSCSI 閘道組態新增到 DeepSea 設定中︰
      </para>
<screen>
<prompt>root@master # </prompt>mkdir -p /srv/pillar/ceph/stack/ceph/
<prompt>root@master # </prompt>echo 'igw_config: default-ui' &gt;&gt; /srv/pillar/ceph/stack/ceph/cluster.yml
<prompt>root@master # </prompt>chown salt:salt /srv/pillar/ceph/stack/ceph/cluster.yml
</screen>
     </step>
     <step>
      <para>
       在 <filename>policy.cfg</filename> 中新增 iSCSI 閘道角色，然後儲存檔案︰
      </para>
<screen>
role-igw/stack/default/ceph/minions/ses-1.ses.suse.yml
role-igw/cluster/ses-1.ses.suse.sls
[...]
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <substeps>
     <step>
      <para>
       如果已將系統註冊到 SUSEConnect 並使用 SCC/SMT，則不需要執行進一步的動作。
      </para>
     </step>
     <step>
      <para>
       如果您使用的<emphasis role="bold">不是</emphasis> SCC/SMT 而是媒體 ISO 或其他套件來源，請手動新增以下儲存庫︰SLE12-SP3 基礎、SLE12-SP3 更新、SES5 基礎和 SES5 更新。您可以使用 <command>zypper</command> 指令來執行此操作。首先，移除所有現有的軟體儲存庫，然後新增所需的新儲存庫，最後重新整理儲存庫來源︰
      </para>
<screen>
<prompt>root # </prompt>zypper sd {0..99}
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/Storage/5/x86_64/product/ SES5-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/Storage/5/x86_64/update/ SES5-UPDATES
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product/ SLES12-SP3-POOL
<prompt>root # </prompt>zypper ar \
 http://172.17.2.210:82/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update/ SLES12-SP3-UPDATES
<prompt>root # </prompt>zypper ref
</screen>
      <para>
       之後，變更您的 Pillar 資料以使用另一個策略。編輯
      </para>
<screen>/srv/pillar/ceph/stack/<replaceable>name_of_cluster</replaceable>/cluster.yml</screen>
      <para>
       並新增下行︰
      </para>
<screen>upgrade_init: zypper-dup</screen>
      <tip>
       <para>
        <literal>zypper-dup</literal> 策略要求您手動新增最新的軟體儲存庫，而預設的 <literal>zypper-migration</literal> 則相依於 SCC/SMT 提供的儲存庫。
       </para>
      </tip>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     修復主機 Grain，以便讓 DeepSea 在公用網路上為 Ceph 精靈例項 ID 使用短主機名稱。對於每個節點，您需要使用新的 (短) 主機名稱執行 <command>grains.set</command>。執行 <command>grains.set</command> 前，請透過執行 <command>ceph status</command> 驗證目前的監控程式例項。下面是設定短主機名稱前後的範例︰
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-16-45-0a.example.com:
    d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    d52-54-00-49-17-2a
d52-54-00-76-21-bc.example.com:
    d52-54-00-76-21-bc
d52-54-00-70-ac-30.example.com:
    d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt d52-54-00-16-45-0a.example.com grains.set \
 host public.d52-54-00-16-45-0a
<prompt>root@master # </prompt>salt d52-54-00-49-17-2a.example.com grains.set \
 host public.d52-54-00-49-17-2a
<prompt>root@master # </prompt>salt d52-54-00-76-21-bc.example.com grains.set \
 host public.d52-54-00-76-21-bc
<prompt>root@master # </prompt>salt d52-54-00-70-ac-30.example.com grains.set \
 host public.d52-54-00-70-ac-30
</screen>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.get host
d52-54-00-76-21-bc.example.com:
    public.d52-54-00-76-21-bc
d52-54-00-16-45-0a.example.com:
    public.d52-54-00-16-45-0a
d52-54-00-49-17-2a.example.com:
    public.d52-54-00-49-17-2a
d52-54-00-70-ac-30.example.com:
    public.d52-54-00-70-ac-30
</screen>
   </step>
   <step>
    <para>
     執行升級︰
    </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> state.apply ceph.updates
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> test.version
<prompt>root@master # </prompt>salt-run state.orch ceph.maintenance.upgrade
</screen>
    <para>
     每個節點都將重新開機。叢集將再次啟動，報告沒有使用中的 Ceph Manager 例項。這是正常的。此時，不應再安裝/執行 Calamari。
    </para>
   </step>
   <step>
    <para>
     執行所有必要的部署階段，以將叢集置於健康狀態︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.0
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.1
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.3
</screen>
   </step>
   <step>
    <para>
     若要部署 openATTIC (請參閱<xref linkend="ceph.oa"/>)，請在 <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 中新增適當的 <literal>role-openattic</literal> 行 (請參閱<xref linkend="policy.role.assignment"/>)，然後執行︰
    </para>
<screen>
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.2
<prompt>root@master # </prompt>salt-run state.orch ceph.stage.4
</screen>
   </step>
   <step>
    <para>
     升級期間，您可能會收到「錯誤 EINVAL︰項目 [...] 存在，但功能不相符」錯誤。若要修復這些錯誤，請參閱<xref linkend="ceph.upgrade.4to5"/>。
    </para>
   </step>
   <step>
    <para>
     執行其餘清理操作︰
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Crowbar 會在每個 OSD 的 <filename>/etc/fstab</filename> 中建立一些項目。這些項目並不需要，因此請將其刪除。
      </para>
     </listitem>
     <listitem>
      <para>
       Calamari 會持續執行一個排程的 Salt 工作來檢查叢集狀態，請移除該工作︰
      </para>
<screen>
<prompt>root@master # </prompt>salt <replaceable>target</replaceable> schedule.delete ceph.heartbeat
</screen>
     </listitem>
     <listitem>
      <para>
       同時還安裝了一些不需要的套件，大多為 ruby gem 以及 chef 相關套件。不需要移除它們，但如果非要刪除，可透過執行 <command>zypper rm <replaceable>pkg_name</replaceable></command> 來完成。
      </para>
     </listitem>
    </itemizedlist>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="ceph.upgrade.3to5">
  <title>從 SUSE Enterprise Storage 3 升級至版本 5</title>

  <important>
   <title>軟體要求</title>
   <para>
    您需要在要升級的所有 Ceph 節點上安裝以下軟體並將其更新到最新的套件版本，然後才能開始升級程序︰
   </para>
   <itemizedlist>
    <listitem>
     <para>
      SUSE Linux Enterprise Server 12 SP1
     </para>
    </listitem>
    <listitem>
     <para>
      SUSE Enterprise Storage 3
     </para>
    </listitem>
   </itemizedlist>
  </important>

  <para>
   若要將 SUSE Enterprise Storage 3 叢集升級至版本 5，請依次執行<xref linkend="upgrade4to5cephdeploy.all"/>和<xref linkend="upgrade4to5cephdeploy.admin"/>中所述的步驟。
  </para>
 </sect1>
</chapter>
