<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="deployment_hwrecommend.xml" version="5.0" xml:id="storage-bp-hwreq">
 <title>硬體要求和建議</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:editurl>https://github.com/SUSE/doc-ses/edit/maintenance/ses6/xml/</dm:editurl>
   <dm:translation>是</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Ceph 的硬體要求在很大程度上取決於 IO 工作負載。在開始進行詳細規劃時，應考慮以下硬體要求和建議。
 </para>
 <para>
  一般情況下，本節所述的建議是按程序提出的。如果同一部機器上有多個程序，則需要提高 CPU、RAM、磁碟和網路要求。
 </para>
 <sect1 xml:id="multi-architecture">
  <title>多架構組態</title>

  <para>
   SUSE Enterprise Storage 支援 x86 和 Arm 架構。考慮每個架構時，請務必注意從每個 OSD 的核心數、頻率和 RAM 的角度進行，不同的 CPU 架構在大小調整方面並無實際差異。
  </para>

  <para>
   與較小的 x86 處理器 (非伺服器) 一樣，效能較低的基於 Arm 的核心可能無法提供最佳體驗，特別是用於糾刪碼池時。
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-minimum-cluster">
  <title>最低叢集組態</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     至少需要四個 OSD 節點，每個節點包含八個 OSD 磁碟。
    </para>
   </listitem>
   <listitem>
    <para>
     三個 Ceph 監控程式節點 (需要使用 SSD 做為專屬 OS 磁碟機).
    </para>
   </listitem>
   <listitem>
    <para>
     iSCSI 閘道、物件閘道和中繼資料伺服器需要遞增的 4 GB RAM 和四個核心。
    </para>
   </listitem>
   <listitem>
    <para>
     Ceph 監控程式、物件閘道和中繼資料伺服器的節點需要備援部署。
    </para>
   </listitem>
   <listitem>
    <para>
     具有 4 GB RAM、四個核心和 1 TB 容量的獨立管理節點，通常是 Salt Master 節點。管理節點上不支援 Ceph 監控程式、Ceph 管理員、中繼資料伺服器、Ceph OSD、物件閘道或 NFS Ganesha 等 Ceph 服務和閘道。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="deployment-osd-recommendation">
  <title>物件儲存節點</title>

  <sect2 xml:id="sysreq-osd">
   <title>最低需求</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      CPU 建議：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        每個旋轉式磁碟 1x 2GHz CPU 線串
       </para>
      </listitem>
      <listitem>
       <para>
        每個 SSD 2x 2GHz CPU 線串
       </para>
      </listitem>
      <listitem>
       <para>
        每個 NVMe 磁碟 4x 2GHz CPU 線串
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
    <listitem>
     <para>
      獨立的 10 GbE 網路 (公用/用戶端和後端)，需要 4x 10 GbE，建議 2x 25 GbE。
     </para>
    </listitem>
    <listitem>
     <para>
      總計所需 RAM = OSD 數量 x (1 GB + <option>osd_memory_target</option>) + 16 GB
     </para>
     <para>
      如需 <option>osd_memory_target</option> 的更多詳細資料，請參閱<xref linkend="config-auto-cache-sizing"/>。
     </para>
    </listitem>
    <listitem>
     <para>
      OSD 磁碟採用 JBOD 組態或個別的 RAID-0 組態。
     </para>
    </listitem>
    <listitem>
     <para>
      OSD 記錄可以存放在 OSD 磁碟上。
     </para>
    </listitem>
    <listitem>
     <para>
      OSD 磁碟應該專門由 SUSE Enterprise Storage 使用。
     </para>
    </listitem>
    <listitem>
     <para>
      作業系統專屬的磁碟/SSD，最好採用 RAID 1 組態。
     </para>
    </listitem>
    <listitem>
     <para>
      如果此 OSD 主機將要代管用於快取分層的一部分快取池，請至少額外配置 4 GB RAM。
     </para>
    </listitem>
    <listitem>
     <para>
      Ceph 監控程式、閘道和中繼資料伺服器可以存放在物件儲存節點上。
     </para>
    </listitem>
    <listitem>
     <para>
      考慮到磁碟效能，我們建議使用裸機做為 OSD 節點，而不要使用虛擬機器。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-mindisk">
   <title>最小磁碟大小</title>
   <para>
    需要在 OSD 上執行以下兩種類型的磁碟空間：磁碟記錄 (針對 FileStore) 或 WAL/DB 裝置 (針對 BlueStore) 的空間以及儲存資料的主要空間。記錄/WAL/DB 的最小 (預設) 值為 6 GB。資料的最小空間為 5 GB，因為系統會自動為小於 5 GB 的分割區指定權數 0。
   </para>
   <para>
    因此，儘管 OSD 的最小磁碟空間為 11 GB，但不建議使用小於 20 GB 的磁碟，即使在測試中也是如此。
   </para>
  </sect2>

  <sect2 xml:id="rec-waldb-size">
   <title>BlueStore 的 WAL 和 DB 裝置的建議大小</title>
   <tip>
    <title>更多資訊</title>
    <para>
     如需 BlueStore 的詳細資訊，請參閱<xref linkend="about-bluestore"/>。
    </para>
   </tip>
   <itemizedlist>
    <listitem>
     <para>
      我們建議為 WAL 裝置保留 4 GB。對於大多數工作負載而言，建議的 DB 大小為 64 GB。
     </para>
    </listitem>
    <listitem>
     <para>
      如果您打算將 WAL 和 DB 裝置置於同一磁碟，建議您為這兩個裝置使用一個分割區，而不是為每個裝置使用單獨的分割區。這樣，Ceph 便可以使用 DB 裝置來執行 WAL 操作。這對於磁碟空間的管理也會更有效，因為 Ceph 只會在需要時才會為 WAL 使用 DB 分割區。另一個好處是，WAL 分割區填滿的可能性很小，當該分割區未完全利用時，其空間並不會浪費，而是用於 DB 操作。
     </para>
     <para>
      若要與 WAL 共用 DB 裝置，請<emphasis>不要</emphasis>指定 WAL 裝置，而是僅指定 DB 裝置。
     </para>
     <para>
      如需指定 OSD 配置的詳細資訊，請參閱<xref linkend="ds-drive-groups"/>。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="ses-bp-share-ssd-journal">
   <title>使用 SSD 儲存 OSD 記錄</title>
   <para>
    固態硬碟 (SSD) 不包含移動部件。這可以減少隨機存取時間和讀取延遲，同時加快資料輸送量。由於 SSD 的每 MB 價格大大高於旋轉型硬碟，SSD 只適用於較小規模的儲存。
   </para>
   <para>
    如果將記錄儲存在 SSD 上，並將物件資料儲存在獨立的硬碟上，OSD 的效能會得到大幅提高。
   </para>
   <tip>
    <title>為多個記錄共用 SSD</title>
    <para>
     由於記錄資料佔用的空間相對較小，因此您可以將多個記錄目錄掛接至一個 SSD 磁碟。請注意，每共用一個記錄，SSD 磁碟的效能就會有所下降。不建議在同一個 SSD 磁碟中共用 6 個以上的記錄，或者在 NVMe 磁碟中共用 12 個以上的記錄。
    </para>
   </tip>
  </sect2>

  <sect2 xml:id="maximum-count-of-disks-osd">
   <title>磁碟的最大建議數量</title>
   <para>
    您可以在一部伺服器上使用所允許的任意數量的磁碟。規劃每部伺服器的磁碟數量時，需要考慮以下幾點：
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <emphasis>網路頻寬：</emphasis>在一部伺服器中使用的磁碟越多，執行磁碟寫入操作時必須透過網路卡傳輸的資料就越多。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>記憶體：</emphasis>系統會將超過 2 GB 的 RAM 用於 BlueStore 快取。當 <option>osd_memory_target</option> 設為預設值 4 GB 時，該起始快取大小對於旋轉式媒體而言是比較合理的。如果使用 SSD 或 NVME，請考慮增加快取大小以及為每個 OSD 配置的 RAM，以便最大限度地提高效能。
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>容錯：</emphasis>如果整部伺服器發生故障，則伺服器包含的磁碟越多，叢集暫時丟失的 OSD 就越多。此外，為了確保複製規則的執行，需要將發生故障伺服器中的所有資料複製到叢集中的其他節點。
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-mon">
  <title>監控程式節點</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     至少需要三個 Ceph 監控程式節點。監控程式數量應永遠為奇數 (1+2n)。
    </para>
   </listitem>
   <listitem>
    <para>
     4 GB RAM。
    </para>
   </listitem>
   <listitem>
    <para>
     具有四個邏輯核心的處理器。
    </para>
   </listitem>
   <listitem>
    <para>
     強烈建議您為監控程式使用 SSD 或其他速度足夠快的儲存類型，特別是針對每個監控程式節點上的 <filename>/var/lib/ceph</filename> 路徑，因為最低核准人數可能不穩定且磁碟延遲較高。建議提供兩個採用 RAID 1 組態的磁碟來進行備援。建議為監控程式程序使用獨立的磁碟，或者至少是獨立的磁碟分割區，以防止記錄檔案增大等問題導致監控程式的可用磁碟空間不足。
    </para>
   </listitem>
   <listitem>
    <para>
     每個節點只能有一個監控程式程序。
    </para>
   </listitem>
   <listitem>
    <para>
     僅當有足夠的硬體資源可用時，才支援混用 OSD、監控程式或物件閘道節點。這意味著，對於所有服務需提高相應要求。
    </para>
   </listitem>
   <listitem>
    <para>
     與多個交換器結合的兩個網路介面。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-rgw">
  <title>物件閘道節點</title>

  <para>
   物件閘道節點應有 6 到 8 個 CPU 核心和 32 GB RAM (建議 64 GB)。如果將其他程序併置在同一部機器上，則需要提高相應要求。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-mds">
  <title>中繼資料伺服器節點</title>

  <para>
   中繼資料伺服器節點的適當大小取決於特定使用案例。一般而言，中繼資料伺服器需要處理的開啟檔案越多，所需要的 CPU 和 RAM 就越多。以下是最低要求：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     為每個中繼資料伺服器精靈指定 3 GB 的 RAM。
    </para>
   </listitem>
   <listitem>
    <para>
     結合網路介面。
    </para>
   </listitem>
   <listitem>
    <para>
     2.5 GHz CPU，至少具有兩個核心。
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sysreq-smaster">
  <title>Salt Master</title>

  <para>
   至少需要 4 GB RAM 和四核 CPU。其中包括在管理節點上執行 Ceph Dashboard。對於包含數百個節點的大型叢集，建議提供 6 GB RAM。
  </para>
 </sect1>
 <sect1 xml:id="sysreq-iscsi">
  <title>iSCSI 節點</title>

  <para>
   iSCSI 節點應有 6 到 8 個 CPU 核心和 16 GB RAM。
  </para>
 </sect1>
 <sect1 xml:id="ceph-install-ceph-deploy-network">
  <title>網路建議</title>

  <para>
   要執行 Ceph 的網路環境最好是至少包含兩個網路介面結合的組合，該組合使用 VLAN 邏輯分割成公共部分和可信的內部部分。如果可能，建議採用 802.3ad 結合模式，以提供最高的頻寬和復原能力。
  </para>

  <para>
   公共 VLAN 用於向客戶提供服務，而內部部分則用於已驗證的 Ceph 網路通訊。建議採用此模式的主要原因在於，儘管 Ceph 可提供驗證並能在建立機密金鑰後防範攻擊，但用於設定這些金鑰的訊息可能會公開傳輸，因而容易受到攻擊。
  </para>

  <tip>
   <title>透過 DHCP 設定的節點</title>
   <para>
    如果儲存節點是透過 DHCP 設定的，則預設逾時可能會不夠長，無法保證能在各個 Ceph 精靈啟動前正確設定網路。如果發生此問題，Ceph MON 和 OSD 將不會正常啟動 (執行 <command>systemctl status ceph\*</command> 會導致「無法結合」錯誤)。為避免此問題發生，建議您在儲存叢集的每個節點上，將 DHCP 用戶端逾時增加到至少 30 秒。為此，可在每個節點上變更以下設定：
   </para>
   <para>
    在 <filename>/etc/sysconfig/network/dhcp</filename> 中，設定
   </para>
<screen>DHCLIENT_WAIT_AT_BOOT="30"</screen>
   <para>
    在 <filename>/etc/sysconfig/network/config</filename> 中，設定
   </para>
<screen>WAIT_FOR_INTERFACES="60"</screen>
  </tip>

  <sect2 xml:id="storage-bp-net-private">
   <title>將私人網路新增到執行中的叢集</title>
   <para>
    如果您在 Ceph 部署期間未指定叢集網路，則系統假設使用的是單一公用網路環境。儘管 Ceph 可在公用網路中正常運作，但如果您設定了另一個私人叢集網路，Ceph 的效能和安全性將會得到提升。若要支援兩個網路，每個 Ceph 節點上至少需有兩個網路卡。
   </para>
   <para>
    需要對每個 Ceph 節點套用以下變更。對小型叢集執行此操作的速度相對較快，但如果叢集包含數百甚至數千個節點，則此程序可能十分耗時。
   </para>
   <procedure>
    <step>
     <para>
      在每個叢集節點上停止 Ceph 相關的服務。
     </para>
     <para>
      在 <filename>/etc/ceph/ceph.conf</filename> 中新增一行以定義叢集網路，例如：
     </para>
<screen>cluster network = 10.0.0.0/24</screen>
     <para>
      如果需要指定具體的靜態 IP 位址或覆寫 <option>cluster network</option> 設定，您可以使用選擇性的 <option>cluster addr</option> 執行此操作。
     </para>
    </step>
    <step>
     <para>
      檢查私人叢集網路在 OS 層級是否如預期般運作。
     </para>
    </step>
    <step>
     <para>
      在每個叢集節點上啟動 Ceph 相關的服務。
     </para>
<screen><prompt>root # </prompt>systemctl start ceph.target</screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="storage-bp-net-subnets">
   <title>不同子網路中的監控程式節點</title>
   <para>
    如果監控程式節點位於多個子網路中，例如，位於不同的房間並由不同的交換器提供服務，則您需要相應地調整 <filename>ceph.conf</filename> 檔案。例如，如果節點的 IP 位址為 192.168.123.12、1.2.3.4 和 242.12.33.12，請將以下幾行新增至 <literal>global</literal> 區段：
   </para>
<screen>[global]
[...]
mon host = 192.168.123.12, 1.2.3.4, 242.12.33.12
mon initial members = MON1, MON2, MON3
[...]</screen>
   <para>
    此外，如果您需要指定每個監控程式的公用位址或網路，則需要為每個監控程式新增 <literal>[mon.<replaceable>X</replaceable>]</literal> 區段：
   </para>
<screen>[mon.MON1]
public network = 192.168.123.0/24

[mon.MON2]
public network = 1.2.3.0/24

[mon.MON3]
public network = 242.12.33.12/0</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sysreq-naming">
  <title>命名限制</title>

  <para>
   一般情況下，Ceph 不支援在組態檔案、池名稱、使用者名稱等內容中使用非 ASCII 字元。設定 Ceph 叢集時，建議您在所有 Ceph 物件/組態名稱中僅使用簡單的英數字元 (A-Z、a-z、0-9) 和最少量的標點符號 (「.」、「-」、「_」)。
  </para>
 </sect1>
 <sect1 xml:id="ses-bp-diskshare">
  <title>共用一部伺服器的 OSD 和監控程式</title>

  <para>
   儘管從技術上講可以在測試環境中的同一部伺服器上執行 Ceph OSD 和監控程式，但強烈建議您在生產環境中為每個監控程式節點使用獨立的伺服器。這樣做的主要原因在於效能 — 叢集包含的 OSD 越多，監控程式節點需要執行的 I/O 操作就越多。另外，當監控程式節點與 OSD 之間共用一部伺服器時，OSD I/O 操作便將成為監控程式節點的限制因素。
  </para>

  <para>
   另一個考慮因素是，是否要在 OSD、監控程式節點與伺服器上的作業系統之間共用磁碟。答案非常簡單：如果可能，請將一個獨立的磁碟專門用於 OSD，並將一部獨立的伺服器用於監控程式節點。
  </para>

  <para>
   儘管 Ceph 支援基於目錄的 OSD，但 OSD 應永遠具有一個專屬磁碟，而不能與作業系統共用一個磁碟。
  </para>

  <tip>
   <para>
    如果<emphasis>確實</emphasis>有必要在同一部伺服器上執行 OSD 和監控程式節點，請將一個獨立磁碟掛接至 <filename>/var/lib/ceph/mon</filename> 目錄以在該磁碟上執行監控程式，這樣可以稍微提升一些效能。
   </para>
  </tip>
 </sect1>
 <sect1 xml:id="ses-bp-production-cluster">
  <title>建議的生產叢集組態</title>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     七個物件儲存節點
    </para>
    <itemizedlist>
     <listitem>
      <para>
       單一節點不超出總容量的 15% 左右
      </para>
     </listitem>
     <listitem>
      <para>
       10 Gb 乙太網路 (與多個交換器結合的四個實體網路)
      </para>
     </listitem>
     <listitem>
      <para>
       每個儲存叢集有 56 個以上的 OSD
      </para>
     </listitem>
     <listitem>
      <para>
       每個 OSD 儲存節點具有 RAID 1 OS 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       依據 6:1 的 SSD 記錄與 OSD 的比率為記錄提供 SSD
      </para>
     </listitem>
     <listitem>
      <para>
       在每個物件儲存節點上，為每 TB 的原始 OSD 容量提供 1.5 GB RAM
      </para>
     </listitem>
     <listitem>
      <para>
       為每個物件儲存節點上的每個 OSD 提供 2 GHz
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     專屬的實體基礎架構節點
    </para>
    <itemizedlist>
     <listitem>
      <para>
       三個 Ceph 監控程式節點：4 GB RAM，四核處理器，RAID 1 SSD 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       一個 SES 管理節點：4 GB RAM，四核處理器，RAID 1 SSD 磁碟
      </para>
     </listitem>
     <listitem>
      <para>
       閘道或中繼資料伺服器節點的備援實體部署：
      </para>
      <itemizedlist>
       <listitem>
        <para>
         物件閘道節點：32 GB RAM，八核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
       <listitem>
        <para>
         iSCSI 閘道節點：16 GB RAM，四核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
       <listitem>
        <para>
         中繼資料伺服器節點 (一個使用中/一個熱待命)：32 GB RAM，八核處理器，RAID 1 SSD 磁碟
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="req-ses-other">
  <title>SUSE Enterprise Storage 6 以及其他 SUSE 產品</title>

  <para>
   本節包含有關將 SUSE Enterprise Storage 6 與其他 SUSE 產品整合的重要資訊。
  </para>

  <sect2 xml:id="req-ses-suma">
   <title>SUSE Manager</title>
   <para>
    SUSE Manager 與 SUSE Enterprise Storage 未整合，因此 SUSE Manager 目前無法管理 SUSE Enterprise Storage 叢集。
   </para>
  </sect2>
 </sect1>
</chapter>
