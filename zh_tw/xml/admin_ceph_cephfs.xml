<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_ceph_cephfs.xml" version="5.0" xml:id="cha-ceph-cephfs">
 <title>叢集檔案系統</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>編輯</dm:status>
   <dm:deadline/>
   <dm:priority/>
   <dm:translation>yes</dm:translation>
   <dm:languages/>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  本章介紹通常應在完成叢集設定與輸出 CephFS 後執行的管理任務。如需有關設定 CephFS 的詳細資訊，請參閱<xref linkend="cha-ceph-as-cephfs"/>。
 </para>
 <sect1 xml:id="ceph-cephfs-cephfs-mount">
  <title>掛接 CephFS</title>

  <para>
   建立檔案系統後，如果 MDS 處於使用中狀態，您便可以從用戶端主機掛接檔案系統。
  </para>

  <sect2 xml:id="cephfs-client-preparation">
   <title>用戶端準備</title>
   <para>
    如果用戶端主機執行的是 SUSE Linux Enterprise 12 SP2 或 SP3，您可以跳過本節，因為系統無需額外設定即可掛接 CephFS。
   </para>
   <para>
    如果用戶端主機執行的是 SUSE Linux Enterprise 12 SP1，您需要套用所有最新的修補程式，才能掛接 CephFS。
   </para>
   <para>
    無論是哪一種情況，SUSE Linux Enterprise 中都包含了掛接 CephFS 需要的所有項目。不需要 SUSE Enterprise Storage 6 產品。
   </para>
   <para>
    為了支援完整的 <command>mount</command> 語法，在嘗試掛接 CephFS 之前，應該先安裝
    <package>ceph-common</package> 套件 (於 SUSE Linux Enterprise 中隨附)。
   </para>
  </sect2>

  <sect2 xml:id="Creating-Secret-File">
   <title>建立機密檔案</title>
   <para>
    Ceph 叢集執行時預設會開啟驗證功能。您應該建立一個可用於儲存您的機密金鑰 (不是金鑰圈自身) 的檔案。請執行以下操作，以獲取特定使用者的機密金鑰並建立該檔案：
   </para>
   <procedure>
    <title>建立機密金鑰</title>
    <step>
     <para>
      在金鑰圈檔案中檢視特定使用者的金鑰：
     </para>
<screen><prompt>cephadm@adm &gt; </prompt>cat /etc/ceph/ceph.client.admin.keyring</screen>
    </step>
    <step>
     <para>
      複製要使用所掛接 Ceph FS 檔案系統的使用者的金鑰。金鑰的格式通常類似下方所示：
     </para>
<screen>AQCj2YpRiAe6CxAA7/ETt7Hcl9IyxyYciVs47w==</screen>
    </step>
    <step>
     <para>
      為使用者 <emphasis>admin</emphasis> 建立一個檔案名稱包含使用者名稱的檔案，例如 <filename>/etc/ceph/admin.secret</filename>。
     </para>
    </step>
    <step>
     <para>
      將金鑰值貼至上一步中建立的檔案中。
     </para>
    </step>
    <step>
     <para>
      設定對該檔案的適當存取權限。該使用者應該是唯一有權讀取該檔案的使用者，其他人不能有任何存取權限。
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph-cephfs-krnldrv">
   <title>掛接 CephFS</title>
   <para>
    可以使用 <command>mount</command> 指令掛接 CephFS。您需要指定監控程式的主機名稱或 IP 位址。由於 SUSE Enterprise Storage 中預設會啟用 <systemitem>cephx</systemitem> 驗證，因此，您還需要指定一個使用者名稱及其相關的機密：
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <para>
    由於上一個指令會保留在外圍程序歷程中，因此更安全的做法是從檔案讀取機密：
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    請注意，機密檔案應該僅包含實際的金鑰圈機密。因此，在本範例中，該檔案僅包含下行：
   </para>
<screen>AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
   <tip>
    <title>指定多個監控程式</title>
    <para>
     最好能在 <command>mount</command> 指令行中指定多個監控程式並以逗號分隔，以避免某個監控程式在掛接時剛好發生停機的情況。每個監控程式的位址採用<literal>主機[:連接埠]</literal> 格式。如果未指定連接埠，預設會使用連接埠 6789。
    </para>
   </tip>
   <para>
    在本地主機上建立掛接點：
   </para>
<screen><prompt>root # </prompt>mkdir /mnt/cephfs</screen>
   <para>
    掛接 CephFS：
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    如果要掛接檔案系統的某個子集，可以指定子目錄 <filename>subdir</filename>：
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <para>
    可在 <command>mount</command> 指令中指定多個監控程式主機：
   </para>
<screen><prompt>root # </prompt>mount -t ceph ceph_mon1,ceph_mon2,ceph_mon3:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
   <important>
    <title>對根目錄的讀取存取權</title>
    <para>
     如果使用實施了路徑限制的用戶端，則 MDS 功能需要包含對根目錄的讀取存取權。例如，金鑰圈的格式可能如下所示：
    </para>
<screen>client.bar
 key: supersecretkey
 caps: [mds] allow rw path=/barjail, allow r path=/
 caps: [mon] allow r
 caps: [osd] allow rwx</screen>
    <para>
     <literal>allow r path=/</literal> 部分表示路徑受限的用戶端能夠查看根磁碟區，但無法寫入資料至其中。在要求完全隔離的使用情況下，這可能會造成問題。
    </para>
   </important>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-unmount">
  <title>卸載 CephFS</title>

  <para>
   若要卸載 CephFS，請使用 <command>umount</command> 指令：
  </para>

<screen><prompt>root # </prompt>umount /mnt/cephfs</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-cephfs-fstab">
  <title><filename>/etc/fstab</filename> 中的 CephFS</title>

  <para>
   若要在用戶端啟動時自動掛接 CephFS，請在其檔案系統表 <filename>/etc/fstab</filename> 中插入相應的行：
  </para>

<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime,_netdev 0 2</screen>
 </sect1>
 <sect1 xml:id="ceph-cephfs-activeactive">
  <title>多個使用中 MDS 精靈 (主動/主動 MDS)</title>

  <para>
   依預設，CephFS 是針對單個使用中 MDS 精靈設定的。若要調整大規模系統的中繼資料效能，您可以啟用多個使用中 MDS 精靈，以便互相分擔中繼資料工作負載。
  </para>

  <sect2>
   <title>何時使用主動/主動 MDS</title>
   <para>
    如果依預設設定使用單個 MDS 時中繼資料效能出現瓶頸，可考慮使用多個使用中 MDS 精靈。
   </para>
   <para>
    增加精靈數量並不會提高所有工作負載類型的效能。例如，增加 MDS 精靈的數量不會讓單個用戶端上執行的單個應用程式受益，除非該應用程式在同時執行大量中繼資料操作。
   </para>
   <para>
    一般而言，能夠因大量使用中 MDS 精靈受益的工作負載是使用許多用戶端的工作負載，也許是在許多獨立目錄中運作的工作負載。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-increase">
   <title>增加 MDS 使用中叢集的大小</title>
   <para>
    每個 CephFS 檔案系統都有一項 <option>max_mds</option> 設定，用於控制將要建立的階層數。僅當某個備用精靈可用於承擔新階層時，檔案系統中的實際階層數才會增加。例如，如果只有執行一個 MDS 精靈，而 <option>max_mds</option> 設定為兩個，則將不會建立另一個階層。
   </para>
   <para>
    在下面的範例中，我們將 <option>max_mds</option> 選項設定為 2，以便在保留預設階層的情況下再建立一個新階層。若要查看變更，請在設定 <option>max_mds</option> 之前和之後執行 <command>ceph status</command>，然後觀察包含 <literal>fsmap</literal> 的行：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 2
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]</screen>
   <para>
    新建立的階層 (1) 會經歷「正在建立」狀態，然後進入「使用中」狀態。
   </para>
   <important>
    <title>待命精靈</title>
    <para>
     即使具有多個使用中 MDS 精靈，當任何在執行使用中精靈的伺服器發生故障時，高可用性系統也仍會要求待命精靈接管工作。
    </para>
    <para>
     因此，高可用性系統的 <option>max_mds</option> 合理最大值比系統中的 MDS 伺服器總數小 1。若要在發生多次伺服器故障時保持可用性，可增加系統中待命精靈的數量，使之與不會導致失去可用性的伺服器故障數相符。
    </para>
   </important>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-decrease">
   <title>減小階層數</title>
   <para>
    所有階層 (包括要移除的階層) 首先必須是使用中的。這表示至少需要有 <option>max_mds</option> 個 MDS 精靈可用。
   </para>
   <para>
    首先，將 <option>max_mds</option> 設為一個較小的數字。例如，我們重新使用單個使用中 MDS：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/2 up  {0=node2=up:active,1=node1=up:active}
    [...]
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds set max_mds 1
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby
    [...]</screen>
   <para>
    請注意，我們仍有兩個使用中 MDS。即使減小 <option>max_mds</option>，階層也仍會存在，因為 <option>max_mds</option> 只會限制新階層的建立。
   </para>
   <para>
    接下來，使用 <command>ceph mds deactivate <replaceable>rank</replaceable></command> 指令移除不需要的階層：
   </para>
<screen><prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:active}
<prompt>cephadm@adm &gt; </prompt><command>ceph</command> mds deactivate 1
telling mds.1:1 192.168.58.101:6805/2799214375 to deactivate

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-2/2/1 up  {0=node2=up:active,1=node1=up:stopping}

<prompt>cephadm@adm &gt; </prompt><command>ceph</command> status
  [...]
  services:
    [...]
    mds: cephfs-1/1/1 up  {0=node2=up:active}, 1 up:standby</screen>
   <para>
    已停用的階層首先會進入「正在停止」狀態並會保持一段時間，期間它會將所分擔的中繼資料負載轉移給其餘使用中精靈。此階段可能需要數秒到數分鐘時間。如果 MDS 看起來像停在「正在停止」狀態，則應該調查原因，以判斷是否有可能存在錯誤。
   </para>
   <para>
    如果一個 MDS 精靈在「正在停止」狀態下當機或終止，待命精靈會接管工作，階層將恢復到「使用中」狀態。當此精靈重新啟動後，您可以嘗試再次將它停用。
   </para>
   <para>
    精靈結束「正在停止」狀態後，將再次啟動，並重新變為待命精靈。
   </para>
  </sect2>

  <sect2 xml:id="cephfs-activeactive-pinning">
   <title>手動將目錄樹關聯到層級</title>
   <para>
    在多個使用中中繼資料伺服器組態中，將會執行一個平衡器，用於在叢集中均衡分配中繼資料負載。這種模式通常足以符合大多數使用者的需求，但有時，使用者需要使用中繼資料到特定階層的明確對應來覆寫動態平衡器。這樣，管理員或使用者便可以在整個叢集上均衡地分攤應用程式負載，或限制使用者的中繼資料要求對整個叢集的影響。
   </para>
   <para>
    針對此目的提供的機制稱為「輸出關聯」。它是目錄的延伸屬性。此延伸屬性名為 <literal>ceph.dir.pin</literal>。使用者可以使用標準指令來設定此屬性：
   </para>
<screen><prompt>root # </prompt>setfattr -n ceph.dir.pin -v 2 <replaceable>/path/to/dir</replaceable></screen>
   <para>
    延伸屬性的值 (<option>-v</option>) 是要將目錄子樹指定到的階層。預設值 -1 表示不關聯該目錄。
   </para>
   <para>
    目錄輸出關聯從設定了輸出關聯的最近的父級繼承。因此，對某個目錄設定輸出關聯會影響該目錄的所有子項。但是，可以透過設定子目錄輸出關聯來覆寫父級的關聯。例如：
   </para>
<screen><prompt>root # </prompt>mkdir -p a/b                      # "a" and "a/b" start with no export pin set.
setfattr -n ceph.dir.pin -v 1 a/  # "a" and "b" are now pinned to rank 1.
setfattr -n ceph.dir.pin -v 0 a/b # "a/b" is now pinned to rank 0
                                  # and "a/" and the rest of its children
                                  # are still pinned to rank 1.</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-cephfs-failover">
  <title>管理容錯移轉</title>

  <para>
   如果 MDS 精靈停止與監控程式通訊，監控程式會等待 <option>mds_beacon_grace</option> 秒 (預設為 15 秒)，然後將精靈標示為 <emphasis>laggy</emphasis>。可以設定一或多個「待命」精靈，用於在 MDS 精靈容錯移轉期間接管工作。
  </para>

  <sect2 xml:id="ceph-cephfs-failover-standby">
   <title>設定待命精靈</title>
   <para>
    有多項組態設定可控制精靈處於待命狀態時的行為。您可以在執行 MDS 精靈的主機上的 <filename>ceph.conf</filename> 中指定這些設定。精靈在啟動時會載入這些設定，然後將其傳送至監控程式。
   </para>
   <para>
    依預設，如果不使用其中的任何設定，則不具備階層的所有 MDS 精靈將會做為任一階層的「待命」精靈。
   </para>
   <para>
    將待命精靈與特定名稱或階層加以關聯的設定不保證該精靈只用於該階層。具體而言，當有多個待命精靈可用時，將使用關聯的待命精靈。如果某個階層發生故障，而此時有某個待命精靈可用，則即使該精靈與其他某個階層或指定的精靈相關聯，也會使用該精靈。
   </para>
   <variablelist>
    <varlistentry>
     <term>mds_standby_replay</term>
     <listitem>
      <para>
       如果設定為 true，則待命精靈將持續讀取某個已啟動階層的中繼資料記錄。這就為此階層提供了一個熱中繼資料快取，當為該階層提供服務的精靈發生故障時，此記錄可加快容錯移轉程序的速度。
      </para>
      <para>
       一個已啟動的階層只能指定一個待命重放精靈。如果將兩個精靈都設定為待命重放，則其中任意一個會贏得控制權，另一個將成為正常的非重放待命精靈。
      </para>
      <para>
       當某個精靈進入待命重放狀態時，它只會做為所跟隨階層的待命精靈。如果另一個階層發生故障，系統不會使用此待命重放精靈來取代前者，即使沒有其他可用的待命精靈也是如此。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_name</term>
     <listitem>
      <para>
       如果指定此設定，則僅當最後一個具備故障階層的精靈與此名稱相符時，待命精靈才會接管該故障階層。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_rank</term>
     <listitem>
      <para>
       如果指定此設定，待命精靈只會接管指定的階層。如果另外的階層發生故障，將不會使用此精靈來取代此階層。
      </para>
      <para>
       與 <option>mds_standby_for_fscid</option> 結合使用時，可以指定在使用多個檔案系統時，具體針對哪個檔案系統的階層。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mds_standby_for_fscid</term>
     <listitem>
      <para>
       如果設定了 <option>mds_standby_for_rank</option>，則 mds_standby_for_fscid 只是一個用於指出所指檔案系統階層的修飾詞。
      </para>
      <para>
       如果未設定 <option>mds_standby_for_rank</option>，則設定 FSCID 會導致此精靈以指定 FSCID 中的任何階層為目標。如果您希望只在特定的檔案系統中將某個精靈用於任何階層，可使用此設定。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>mon_force_standby_active</term>
     <listitem>
      <para>
       在監控程式主機上使用此設定。其預設值為 true。
      </para>
      <para>
       如果值為 false，則 <option>standby_replay 設定為 true</option> 的精靈只有在其已設定要跟隨的階層/名稱發生故障時，才會變為使用中精靈。另一方面，如果此設定為 true，則可將其他某個階層指定給 <option>standby_replay 設定為 true</option> 的精靈。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ceph-cephfs-failover-examples">
   <title>範例</title>
   <para>
    下面顯示了多個範例 <filename>ceph.conf</filename> 組態。您可將包含所有精靈的組態的 <filename>ceph.conf</filename> 複製到所有伺服器，或者在每部伺服器上建立一個不同的檔案，並在其中包含該伺服器的精靈組態。
   </para>
   <sect3>
    <title>簡單配對</title>
    <para>
     「a」和「b」兩個 MDS 精靈充當一對。其中，目前未指定階層的精靈將是另一個精靈的待命重放跟隨者。
    </para>
<screen>[mds.a]
mds standby replay = true
mds standby for rank = 0

[mds.b]
mds standby replay = true
mds standby for rank = 0</screen>
   </sect3>

  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-quotas">
  <title>設定 CephFS 定額</title>

  <para>
   您可以對 Ceph 檔案系統的任何子目錄設定定額。定額可限制目錄階層中指定點下所儲存的<emphasis role="bold">位元組</emphasis>或<emphasis role="bold">檔案</emphasis>數。
  </para>

  <sect2 xml:id="cephfs-quotas-limitation">
   <title>限制</title>
   <para>
    CephFS 的定額設定具有以下限制：
   </para>
   <variablelist>
    <varlistentry>
     <term>定額是合作性而非競爭性的。</term>
     <listitem>
      <para>
       達到限制時，Ceph 定額依賴於掛接檔案系統的用戶端來停止向其寫入資料。伺服器端無法阻止惡意用戶端寫入所需數量的資料。在用戶端完全不受信任的環境中，請勿使用定額來阻止填入檔案系統。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>定額並不精確。</term>
     <listitem>
      <para>
       在達到定額限制不久後，系統便會停止向檔案系統寫入資料的程序。這樣便會不可避免地允許這些程序在超出設定的限制後又寫入一定數量的資料。系統將會在超出所設定限制後的十分之一秒內停止用戶端寫入程序。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>自 4.17 版本起，即在核心用戶端中實作了定額。</term>
     <listitem>
      <para>
       定額受使用者空間用戶端 (libcephfs、ceph-fuse) 支援。4.17 及更新版本的 Linux 核心用戶端支援 SUSE Enterprise Storage 6 叢集上的 CephFS 定額。核心用戶端 (甚至最近的版本) 無法處理較舊叢集的定額，即使它們可以設定定額延伸屬性也是如此。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>與基於路徑的掛接限制配合使用時，需謹慎設定定額。</term>
     <listitem>
      <para>
       用戶端需要有權存取設定了定額的目錄 Inode，才能強制執行這些定額。如果依據 MDS 功能，用戶端對特定路徑 (例如 <filename>/home/user</filename>) 的存取受到限制，並且對其無權存取的上階目錄 (<filename>/home</filename>) 設定了定額，則用戶端將無法強制執行該定額。在使用基於路徑的存取限制時，請務必對用戶端可以存取的目錄 (例如 <filename>/home/user</filename> 或 /home/user/quota_dir) 設定定額。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="cephfs-quotas-config">
   <title>組態</title>
   <para>
    您可以使用虛擬延伸屬性來設定 CephFS 定額：
   </para>
   <variablelist>
    <varlistentry>
     <term><option>ceph.quota.max_files</option></term>
     <listitem>
      <para>
       設定<emphasis>檔案</emphasis>限制。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><option>ceph.quota.max_bytes</option></term>
     <listitem>
      <para>
       設定<emphasis>位元組</emphasis>限制。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    如果某個目錄 Inode 存在這些屬性，即表示對該位置設定了定額。如果不存在，則表示未對該目錄設定定額 (即使可能對父目錄設定了定額)。
   </para>
   <para>
    若要設定 100 MB 的定額，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 100000000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    若要設定 10,000 個檔案的定額，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 10000 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <para>
    若要檢視定額設定，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_bytes <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
<screen>
<prompt>cephadm@mds &gt; </prompt>getfattr -n ceph.quota.max_files <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
   <note>
    <title>未設定定額</title>
    <para>
     如果延伸屬性的值為「0」，則表示未設定定額。
    </para>
   </note>
   <para>
    若要移除定額，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_bytes -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
<prompt>cephadm@mds &gt; </prompt>setfattr -n ceph.quota.max_files -v 0 <replaceable>/SOME/DIRECTORY</replaceable>
</screen>
  </sect2>
 </sect1>
 <sect1 xml:id="cephfs-snapshots">
  <title>管理 CephFS 快照</title>

  <para>
   在建立 CephFS 快照時，快照會建立此時間點檔案系統的唯讀檢視。您可以在任何目錄中建立快照。快照將涵蓋檔案系統中指定目錄下的所有資料。建立快照後，系統會從各用戶端非同步衝洗緩衝區資料。因此，建立快照的速度十分快。
  </para>

  <important>
   <title>多個檔案系統</title>
   <para>
    如果您有多個 CephFS 檔案系統在共用一個池 (透過名稱空間)，則這些檔案系統的快照將會發生衝突，並且刪除其中一個快照將會導致共用同一個池的其他快照遺失檔案資料。
   </para>
  </important>

  <sect2 xml:id="cephfs-snapshots-create">
   <title>建立快照</title>
   <para>
    對於新檔案系統，系統預設會啟用 CephFS 快照功能。若要對現有檔案系統啟用該功能，請執行以下指令：
   </para>
<screen>
<prompt>cephadm@adm &gt; </prompt>ceph fs set <replaceable>CEPHFS_NAME</replaceable> allow_new_snaps true
</screen>
   <para>
    啟用快照後，CephFS 中的所有目錄都將包含一個特殊的 <filename>.snap</filename> 子目錄。
   </para>
   <para>
    CephFS 核心用戶端有一個限制：它們無法處理一個目錄樹狀結構中所含快照數量超過 400 個的情況。快照的數量應永遠低於此上限，無論使用哪個用戶端均如此。如果使用較舊的 CephFS 用戶端 (例如 SLE12-SP3)，請記住，快照數量超過 400 個對運作十分有害，因為這會導致用戶端當機。
   </para>
   <tip>
    <title>自訂快照子目錄名稱</title>
    <para>
     您可以透過進行 <option>client snapdir</option> 設定來為快照子目錄設定其他名稱。
    </para>
   </tip>
   <para>
    若要建立快照，請以自訂名稱在 <filename>.snap</filename> 目錄下建立子目錄。例如，若要建立目錄 <filename>/<replaceable>CEPHFS_MOUNT</replaceable>/2/3/</filename> 的快照，請執行以下指令：
   </para>
<screen>
<prompt>tux &gt; </prompt>mkdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>

  <sect2 xml:id="cephfs-snapshots-delete">
   <title>刪除快照</title>
   <para>
    若要刪除某個快照，請在 <filename>.snap</filename> 目錄中刪除該快照的子目錄：
   </para>
<screen>
<prompt>tux &gt; </prompt>rmdir /<replaceable>CEPHFS_MOUNT</replaceable>/2/3/.snap/<replaceable>CUSTOM_SNAPSHOT_NAME</replaceable>
</screen>
  </sect2>
 </sect1>
</chapter>
