<?xml version="1.0" encoding="UTF-8"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" xml:base="admin_install_salt.xml" version="5.0" xml:id="ceph-install-saltstack">
 <title>使用 DeepSea/Salt 部署</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:translation>是</dm:translation>
   <dm:release>SES 6</dm:release>
  </dm:docmanager>
 </info>
 <para>
  Salt 與 DeepSea 是元件的<emphasis>堆疊</emphasis>，可協助您部署和管理伺服器基礎架構。Salt 具有很高的可延展性，速度快，且相對容易執行。在開始使用 Salt 部署叢集之前，請閱讀以下注意事項：
 </para>
 <itemizedlist>
  <listitem>
   <para>
    <emphasis>Salt Minion</emphasis> 是由一個稱為 Salt Master 的專屬節點控制的節點。Salt Minion 具有角色，例如 Ceph OSD、Ceph 監控程式、Ceph 管理員、物件閘道、iSCSI 閘道或 NFS Ganesha。
   </para>
  </listitem>
  <listitem>
   <para>
    Salt Master 會執行自己的 Salt Minion。執行特權任務 (例如，建立、授權金鑰以及將金鑰複製到 Minion) 需要 Salt Master，這樣，遠端 Minion 就永遠不需要執行特權任務。
   </para>
   <tip>
    <title>每部伺服器共用多個角色</title>
    <para>
     如果將每個角色都部署在一個獨立節點上，則 Ceph 叢集的效能是最佳的。但實際部署有時會要求多個角色共用一個節點。為避免效能欠佳以及升級程序出現問題，請勿向管理節點部署 Ceph OSD、中繼資料伺服器或 Ceph 監控程式角色。
    </para>
   </tip>
  </listitem>
  <listitem>
   <para>
    Salt Minion 需要能透過網路正確解析 Salt Master 的主機名稱。預設情況下，Minion 會尋找 <systemitem>salt</systemitem> 主機名稱，但您可以在 <filename>/etc/salt/minion</filename> 檔案中指定可透過網路連接的其他任何主機名稱，具體請參閱<xref linkend="ceph-install-stack"/>。
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="cha-ceph-install-relnotes">
  <title>閱讀版本說明</title>

  <para>
   在版本說明中，您可以找到其他有關自 SUSE Enterprise Storage 的上一個版本發行後所進行的變更的資訊。檢查版本說明以瞭解：
  </para>

  <itemizedlist>
   <listitem>
    <para>
     您的硬體是否有特殊的注意事項。
    </para>
   </listitem>
   <listitem>
    <para>
     所用的任何軟體套件是否已發生重大變更。
    </para>
   </listitem>
   <listitem>
    <para>
     是否需要對您的安裝施行特殊預防措施。
    </para>
   </listitem>
  </itemizedlist>

  <para>
   版本說明還會提供無法及時編入手冊的資訊。它們還包含有關已知問題的說明。
  </para>

  <para>
   安裝套件 <package>release-notes-ses</package>之後，本地的 <filename>/usr/share/doc/release-notes</filename> 目錄中或 <link xlink:href="https://www.suse.com/releasenotes/"/> 網頁上會提供版本說明。
  </para>
 </sect1>
 <sect1 xml:id="deepsea-description">
  <title>DeepSea 簡介</title>

  <para>
   DeepSea 旨在節省管理員的時間，讓他們能夠自信地對 Ceph 叢集執行複雜操作。
  </para>

  <para>
   Ceph 是一款高度可設定的軟體解決方案。它提高了系統管理員的自由度和職責履行能力。
  </para>

  <para>
   最低的 Ceph 設定能夠很好地滿足展示目的，但無法顯示 Ceph 在處理大量節點時可體現的卓越功能。
  </para>

  <para>
   DeepSea 會收集並儲存個別伺服器的相關資料，例如位址和裝置名稱。對於諸如 Ceph 的分散式儲存系統，可能需要收集並儲存數百個這樣的項目。收集資訊並手動將資料輸入至組態管理工具的程序非常耗費精力，並且容易出錯。
  </para>

  <para>
   準備伺服器、收集組態資訊以及設定和部署 Ceph 所需執行的步驟大致相同。但是，這種做法無法解決管理獨立功能的需求。在日常操作中，必須做到不厭其煩地將硬體新增至給定的功能，以及適當地移除硬體。
  </para>

  <para>
   DeepSea 透過以下策略解決了這些需求：DeepSea 可將管理員的多項決策合併到一個檔案中。這些決策包括叢集指定、角色指定和設定檔指定。此外，DeepSea 還會收集各組任務以組成一個簡單的目標。每個目標就是一個<emphasis>階段</emphasis>：
  </para>

  <itemizedlist xml:id="deepsea-stage-description">
   <title>DeepSea 階段描述</title>
   <listitem>
    <para>
     <emphasis role="bold">階段 0</emphasis> — <emphasis role="bold">準備</emphasis>：在此階段，將套用全部所需的更新，並且可能會將您的系統重新開機。
    </para>
    <important>
     <title>管理節點重新開機後重新執行階段 0</title>
     <para>
      如果在階段 0 執行期間，管理節點進行了重新開機以載入新核心版本，則您需要再次執行階段 0，否則將無法定位 Minion。
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">階段 1</emphasis> 即<emphasis role="bold">探查</emphasis>階段，在此階段會偵測您叢集中的所有硬體，並會收集 Ceph 組態的必要資訊。如需組態的詳細資料，請參閱<xref linkend="deepsea-pillar-salt-configuration"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">階段 2</emphasis> — <emphasis role="bold">組態</emphasis>：您需要以特定的格式準備組態資料。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">階段 3</emphasis> — <emphasis role="bold">部署</emphasis>：建立包含必要 Ceph 服務的基本 Ceph 叢集。如需必要服務的清單，請參閱<xref linkend="storage-intro-core-nodes"/>。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">階段 4</emphasis> — <emphasis role="bold">服務</emphasis>：可在此階段安裝 Ceph 的其他功能，例如 iSCSI、物件閘道和 CephFS。其中每個功能都是選擇性的。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">階段 5</emphasis> — 移除階段：此階段不是必需的，在啟始設定期間，通常不需要此階段。在此階段，將會移除 Minion 的角色以及叢集組態。如果您需要從叢集中移除某個儲存節點，則需要執行此階段。如需詳細資料，請參閱<xref linkend="salt-node-removing"/>。
    </para>
   </listitem>
  </itemizedlist>



  <sect2 xml:id="deepsea-organisation-locations">
   <title>組織和重要位置</title>
   <para>
    Salt 在 Master 節點上使用多個標準位置和多個命名約定：
   </para>
   <variablelist>
    <varlistentry>
     <term><filename>/srv/pillar</filename></term>
     <listitem>
      <para>
       該目錄儲存叢集 Minion 的組態資料。<emphasis>Pillar</emphasis> 是向所有叢集 Minion 提供全域組態值的介面。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/</filename></term>
     <listitem>
      <para>
       該目錄儲存 Salt 狀態檔案 (也稱為 <emphasis>sls</emphasis> 檔案)。狀態檔案是叢集應處於狀態的帶格式說明。

      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/module/runners</filename></term>
     <listitem>
      <para>
       該目錄儲存稱為執行程式的 Python 程序檔。執行程式在 Master 節點上執行。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/_modules</filename></term>
     <listitem>
      <para>
       該目錄儲存稱為模組的 Python 程序檔。這些模組將套用於叢集中的所有 Minion。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/pillar/ceph</filename></term>
     <listitem>
      <para>
       該目錄由 DeepSea 使用。收集的組態資料儲存在此處。
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><filename>/srv/salt/ceph</filename></term>
     <listitem>
      <para>
       DeepSea 使用的目錄。其中儲存了可採用不同格式的 sls 檔案，但 sls 檔案包含在各子目錄中。每個子目錄僅包含一種類型的 sls 檔案。例如，<filename>/srv/salt/ceph/stage</filename> 包含 <command>salt-run state.orchestrate</command> 執行的協調化檔案。
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="ds-minion-targeting">
   <title>定位 Minion</title>
   <para>
    DeepSea 指令透過 Salt 基礎架構執行。使用 <command>salt</command> 指令時，您需要指定一組將受到該指令影響的 Salt Minion。我們將該組 Minion 描述為 <command>salt</command> 指令的<emphasis>目標</emphasis>。以下各節說明定位 Minion 的可行方法。
   </para>
   <sect3 xml:id="ds-minion-targeting-name">
    <title>比對 Minion 名稱</title>
    <para>
     您可以透過名稱比對來定位一個或一組 Minion。Minion 的名稱通常為執行該 Minion 的節點的短主機名稱。這是一種一般的 Salt 定位方法，與 DeepSea 無關。您可以使用萬用字元、一般運算式或清單來限制 Minion 名稱的範圍。遵循的一般語法如下：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> example.module</screen>
    <tip>
     <title>僅限 Ceph 叢集</title>
     <para>
      如果您環境中的所有 Salt Minion 均屬於 Ceph 叢集，則可以安全地使用 <literal>'*'</literal> 取代 <replaceable>target</replaceable>，以包含<emphasis>所有</emphasis>註冊的 Minion。
     </para>
    </tip>
    <para>
     比對 example.net 網域中的所有 Minion (假設 Minion 名稱與其「完整」的主機名稱相同)：
    </para>
<screen><prompt>root@master # </prompt>salt '*.example.net' test.ping</screen>
    <para>
     比對「web1」Minion 與「web5」Minion：
    </para>
<screen><prompt>root@master # </prompt>salt 'web[1-5]' test.ping</screen>
    <para>
     使用一般運算式比對「web1-prod」和「web1-devel」Minion：
    </para>
<screen><prompt>root@master # </prompt>salt -E 'web1-(prod|devel)' test.ping</screen>
    <para>
     比對簡單的 Minion 清單：
    </para>
<screen><prompt>root@master # </prompt>salt -L 'web1,web2,web3' test.ping</screen>
    <para>
     比對叢集中的所有 Minion：
    </para>
<screen><prompt>root@master # </prompt>salt '*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-grain">
    <title>使用 DeepSea 粒紋進行定位</title>
    <para>
     在受 Salt 管理的異質環境中 (SUSE Enterprise Storage 6 與其他叢集解決方案一併部署在一部分節點上)，執行 DeepSea 階段 0 之前，您需要透過對相關 Minion 套用「deepsea」粒紋來標示它們。這樣您便可以在無法透過 Minion 名稱比對的環境中輕鬆定位 DeepSea Minion。
    </para>
    <para>
     若要將「deepse」Grain 套用於一組 Minion，請執行以下指令：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.append deepsea default</screen>
    <para>
     若要從一組 Minion 移除「deepse」Grain，請執行以下指令：
    </para>
<screen><prompt>root@master # </prompt>salt <replaceable>target</replaceable> grains.delval deepsea destructive=True</screen>
    <para>
     將「deepsea」Grain 套用於相關 Minion 後，您可以執行以下指令來進行定位：
    </para>
<screen><prompt>root@master # </prompt>salt -G 'deepsea:*' test.ping</screen>
    <para>
     或執行以下等效指令：
    </para>
<screen><prompt>root@master # </prompt>salt -C 'G@deepsea:*' test.ping</screen>
   </sect3>
   <sect3 xml:id="ds-minion-targeting-dsminions">
    <title>設定 <option>deepsea_minions</option> 選項</title>
    <para>
     設定 <option>deepsea_minions</option> 選項的目標是 DeepSea 部署所需。在階段執行期間，DeepSea 會使用該選項指示 Minion (如需詳細資料，請參閱<xref linkend="deepsea-stage-description"/>)。
    </para>
    <para>
     若要設定或變更 <option>deepsea_minions</option> 選項，請編輯 Salt Master 上的 <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> 檔案，新增或取代下行：
    </para>
<screen>deepsea_minions: <replaceable>target</replaceable></screen>
    <tip>
     <title><option>deepsea_minions</option> 目標</title>
     <para>
      對於 <option>deepsea_minions</option> 選項的 <replaceable>target</replaceable>，您可以使用以下任何定位方法：<xref linkend="ds-minion-targeting-name" xrefstyle="select: title"/>和<xref linkend="ds-minion-targeting-grain" xrefstyle="select: title"/>。
     </para>
     <para>
      比對叢集中的所有 Salt Minion：
     </para>
<screen>deepsea_minions: '*'</screen>
     <para>
      使用「deepsea」Grain 比對所有 Minion：
     </para>
<screen>deepsea_minions: 'G@deepsea:*'</screen>
    </tip>
   </sect3>
   <sect3>
    <title>更多資訊</title>
    <para>
     您可以使用 Salt 基礎架構以更進階的方式來定位 Minion。您可以透過「deepsea-minions」手冊頁瞭解有關 DeepSea 定位的更多詳細資料 (<command>man 7 deepsea_minions</command>)。
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph-install-stack">
  <title>叢集部署</title>

  <para>
   叢集部署程序包括多個階段。首先，需要透過設定 Salt 來準備叢集的所有節點，然後部署並設定 Ceph。
  </para>

  <tip xml:id="dev-env">
   <title>在未定義 OSD 設定檔的情況下部署監控程式節點</title>
   <para>
    如果您需要跳過定義 OSD 的儲存角色 (如<xref linkend="policy-role-assignment"/>所述)，先部署 Ceph 監控程式節點，則可以透過設定 <option>DEV_ENV</option> 變數來實現。
   </para>
   <para>
    此設定允許您在 <filename>role-storage/</filename> 目錄不存在的情況下部署監控程式，以及部署至少包含<emphasis>一個</emphasis>儲存、監控程式和管理員角色的 Ceph 叢集。
   </para>
   <para>
    若要設定環境變數，請將其全域啟用，方法是在 <filename>/srv/pillar/ceph/stack/global.yml</filename> 檔案中進行設定，或者僅針對目前的外圍程序工作階段設定：
   </para>
<screen><prompt>root@master # </prompt>export DEV_ENV=true</screen>
   <para>
    例如，可建立包含以下內容的 <filename>/srv/pillar/ceph/stack/global.yml</filename>：
   </para>
<screen>DEV_ENV: <replaceable>True</replaceable></screen>
  </tip>

  <para>
   下面詳細說明了叢集準備程序。
  </para>

  <procedure>
   <step>
    <para>
     在叢集的每個節點上安裝並註冊 SUSE Linux Enterprise Server 15 SP1 以及 SUSE Enterprise Storage 6 延伸。
    </para>
   </step>
   <step>
    <para>
     列出現有的軟體儲存庫，驗證是否已安裝並註冊正確的產品。執行 <command>zypper lr -E</command>，並將輸出與以下清單進行比較：
    </para>
<screen>
 SLE-Product-SLES15-SP1-Pool
 SLE-Product-SLES15-SP1-Updates
 SLE-Module-Server-Applications15-SP1-Pool
 SLE-Module-Server-Applications15-SP1-Updates
 SLE-Module-Basesystem15-SP1-Pool
 SLE-Module-Basesystem15-SP1-Updates
 SUSE-Enterprise-Storage-6-Pool
 SUSE-Enterprise-Storage-6-Updates
</screen>
   </step>
   <step>
    <para>
     在每個節點上設定網路設定，包括正確的 DNS 名稱解析。Salt Master 和所有 Salt Minion 需要依據各自的主機名稱相互解析。如需設定網路的詳細資訊，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_network_yast.html"/>。如需設定 DNS 伺服器的詳細資訊，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_dns.html"/>。
    </para>
   </step>
   <step>
    <para>
     選取一或多個時間伺服器/池，並將其與本地時間同步。確認每次系統啟動時都會啟用時間同步服務。您可以使用 <command>yast ntp-client</command> 指令 (在 <package>yast2-ntp-client</package> 套件中提供) 來設定時間同步。
    </para>
    <tip>
     <para>
      虛擬機器並非可靠的 NTP 來源。
     </para>
    </tip>
    <para>
     如需設定 NTP 的詳細資訊，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_sle_admin/data/sec_ntp_yast.html"/>。
    </para>
   </step>
   <step>
    <para>
     在 Salt Master 節點上安裝 <literal>salt-master</literal> 和 <literal>salt-minion</literal> 套件：
    </para>
<screen><prompt>root@master # </prompt>zypper in salt-master salt-minion</screen>
    <para>
     檢查 <systemitem>salt-master</systemitem> 服務是否已啟用並啟動，並視需要進行啟用和啟動：
    </para>
<screen><prompt>root@master # </prompt>systemctl enable salt-master.service
<prompt>root@master # </prompt>systemctl start salt-master.service</screen>
   </step>
   <step>
    <para>
     如果您要使用防火牆，請驗證 Salt Master 節點是否為所有 Salt Minion 節點開啟了連接埠 4505 和 4506。如果這些連接埠處於關閉狀態，您可以使用 <command>yast2 firewall</command> 指令並透過允許 <guimenu>SaltStack</guimenu> 服務來開啟這些連接埠。
    </para>
    <warning>
     <title>使用防火牆時，DeepSea 階段失敗</title>
     <para>
      當防火牆處於使用中狀態 (甚至只是設定了防火牆) 時，DeepSea 部署階段會失敗。若要正確完成該階段，需要執行以下指令關閉防火牆
     </para>
<screen>
    <prompt>root # </prompt>systemctl stop firewalld.service
</screen>
     <para>
      或在 <filename>/srv/pillar/ceph/stack/global.yml</filename> 中將 <option>FAIL_ON_WARNING</option> 選項設為「False」：
     </para>
<screen>
FAIL_ON_WARNING: False
</screen>
    </warning>
   </step>
   <step>
    <para>
     在所有 Minion 節點上安裝 <literal>salt-minion</literal> 套件。
    </para>
<screen><prompt>root@minion &gt; </prompt>zypper in salt-minion</screen>
    <para>
     請確定所有其他節點都可將每個節點的<emphasis>完全合格的網域名稱</emphasis>解析為公用網路 IP 位址。
    </para>
   </step>
   <step>
    <para>
     設定所有 Minion (包括 Master Minion) 以連接至 Master。如果無法透過主機名稱 <literal>salt</literal> 連接 Salt Master，請編輯檔案 <filename>/etc/salt/minion</filename>，或建立包含以下內容的新檔案 <filename>/etc/salt/minion.d/master.conf</filename>：
    </para>
<screen>master: <replaceable>host_name_of_salt_master</replaceable></screen>
    <para>
     如果對上述組態檔案執行了任何變更，請在所有 Salt Minion 上重新啟動 Salt 服務：
    </para>
<screen><prompt>root@minion &gt; </prompt>systemctl restart salt-minion.service</screen>
   </step>
   <step>
    <para>
     檢查所有節點上是否已啟用並啟動 <systemitem>salt-minion</systemitem> 服務。依據需要啟用並啟動該服務：
    </para>
<screen><prompt>root # </prompt>systemctl enable salt-minion.service
<prompt>root # </prompt>systemctl start salt-minion.service</screen>
   </step>
   <step>
    <para>
     驗證每個 Salt Minion 的指紋，如果指紋相符，則接受 Salt Master 上的所有 Salt 金鑰。
    </para>
    <note>
     <para>
      如果 Salt Minion 指紋傳回空白，請確定 Salt Minion 具有 Salt Master 組態且可與 Salt Master 通訊。
     </para>
    </note>
    <para>
     檢視每個 Minion 的指紋：
    </para>
<screen><prompt>root@master # </prompt>salt-call --local key.finger
local:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     收集到所有 Salt Minion 的指紋後，將列出 Salt Master 上所有未接受 Minion 金鑰的指紋：
    </para>
<screen><prompt>root@master # </prompt>salt-key -F
[...]
Unaccepted Keys:
minion1:
3f:a3:2f:3f:b4:d3:d9:24:49:ca:6b:2c:e1:6c:3f:c3:83:37:f0:aa:87:42:e8:ff...</screen>
    <para>
     如果 Minion 的指紋相符，則接受這些金鑰：
    </para>
<screen><prompt>root@master # </prompt>salt-key --accept-all</screen>
   </step>
   <step>
    <para>
     驗證是否已接受金鑰：
    </para>
<screen><prompt>root@master # </prompt>salt-key --list-all</screen>
   </step>
   <step xml:id="deploy-wiping-disk">
    <para>
     在部署 SUSE Enterprise Storage 6 前，手動清理所有磁碟。請記得使用正確的磁碟代號取代「X」：
    </para>
    <substeps>
     <step>
      <para>
       停止使用特定磁碟的所有程序。
      </para>
     </step>
     <step>
      <para>
       驗證磁碟上是否掛接有任何分割區，並視需要進行卸載。
      </para>
     </step>
     <step>
      <para>
       如果磁碟由 LVM 管理，請停用整個 LVM 基礎架構並將其刪除。如需更多詳細資料，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/cha_lvm.html"/>。
      </para>
     </step>
     <step>
      <para>
       如果磁碟是 MD RAID 的一部分，請停用 RAID。如需更多詳細資料，請參閱 <link xlink:href="https://www.suse.com/documentation/sles-15/book_storage/data/part_software_raid.html"/>。
      </para>
     </step>
     <step>
      <tip>
       <title>將伺服器重新開機</title>
       <para>
        如果您在執行以下步驟時收到諸如「分割區正在使用」或「無法使用新的分割區表更新核心」之類的錯誤訊息，請將伺服器重新開機。
       </para>
      </tip>
      <para>
       抹除每個分割區的開頭部分 (以 <systemitem class="username">root</systemitem> 身分)：
      </para>
<screen>for partition in /dev/sdX[0-9]*
do
  dd if=/dev/zero of=$partition bs=4096 count=1 oflag=direct
done</screen>
     </step>
     <step>
      <para>
       抹除磁碟機的開頭部分：
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=34 oflag=direct
</screen>
     </step>
     <step>
      <para>
       抹除磁碟機的結尾部分：
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/zero of=/dev/sdX bs=512 count=33 \
  seek=$((`blockdev --getsz /dev/sdX` - 33)) oflag=direct
</screen>
     </step>
     <step>
      <para>
       使用以下指令確認磁碟機是空的 (不含 GPT 結構)：
      </para>
<screen>
<prompt>root # </prompt>parted -s /dev/sdX print free
</screen>
      <para>
       或
      </para>
<screen>
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=34 | hexdump -C
<prompt>root # </prompt>dd if=/dev/sdX bs=512 count=33 \
  skip=$((`blockdev --getsz /dev/sdX` - 33)) | hexdump -C
</screen>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     (選擇性) 如果您需要在安裝 <package>deepsea</package> 套件之前預先組態叢集的網路設定，請手動建立 <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>，並設定 <option>cluster_network:</option> 和 <option>public_network:</option> 選項。請注意，在您安裝 <package>deepsea</package>之後，系統將不會覆寫該檔案。
    </para>
    <tip>
     <title>啟用 IPv6</title>
     <para>
      如果您需要啟用 IPv6 網路定址，請參閱<xref linkend="ds-modify-ipv6"/>。
     </para>
    </tip>
   </step>
   <step>
    <para>
     在 Salt Master 節點上安裝 DeepSea：
    </para>
<screen><prompt>root@master # </prompt>zypper in deepsea</screen>
   </step>
   <step>
    <para>
     <option>master_minion</option> 參數的值是依據 Salt Master 上的 <filename>/etc/salt/minion_id</filename> 檔案動態衍生的。如果您需要覆寫探查的值，請編輯 <filename>/srv/pillar/ceph/stack/global.yml</filename> 檔案並設定相關值：
    </para>
<screen>
master_minion: <replaceable>MASTER_MINION_NAME</replaceable>
</screen>
    <para>
     如果可透過更多主機名稱存取您的 Salt Master，請使用 <command>salt-key -L</command> 指令傳回的名稱做為儲存叢集的 Salt Minion 名稱。如果在 <emphasis>ses</emphasis> 網域中使用 Salt Master 的預設主機名稱 <emphasis>salt</emphasis>，則該檔案如下所示：
    </para>
<screen>master_minion: salt.ses</screen>
   </step>
  </procedure>

  <para>
   現在部署並設定 Ceph。除非另有說明，否則必須執行所有步驟。
  </para>

  <note>
   <title>Salt 指令慣例</title>
   <para>
    可透過兩種方式執行 <command>salt-run state.orch</command>，一種方式是使用「stage.<replaceable>STAGE_NUMBER</replaceable>」，另一種方式是使用階段的名稱。這兩種標記法會產生相同的效果，至於使用哪個指令，完全取決於您的偏好。
   </para>
  </note>

  <procedure xml:id="ds-depl-stages">
   <title>執行部署階段</title>
   <step>
    <para>
     確定可透過 <filename>/srv/pillar/ceph/deepsea_minions.sls</filename> 中的 <option>deepsea_minions</option> 選項正確定位到屬於 Ceph 叢集的 Salt Minion。如需相關資訊，請參閱<xref linkend="ds-minion-targeting-dsminions"/>。
    </para>
   </step>
   <step>
    <para>
     依預設，DeepSea 會使用 Ceph 監控程式、Ceph 管理員和 Ceph OSD 節點上調整後的使用中設定檔來部署 Ceph 叢集。在某些情形中，您可能需要在沒有已調整設定檔的情況下進行部署。若要完成此操作，請在執行 DeepSea 階段前在 <filename>/srv/pillar/ceph/stack/global.yml</filename> 中新增下面幾行。
    </para>
<screen>
alternative_defaults:
 tuned_mgr_init: default-off
 tuned_mon_init: default-off
 tuned_osd_init: default-off
</screen>
   </step>
   <step>
    <para>
     <emphasis>選擇性</emphasis>：為 <filename>/var/lib/ceph/</filename> 建立 Btrfs 子磁碟區。需要在執行 DeepSea stage.0 之前執行此步驟。若要移轉現有目錄或瞭解更多詳細資料，請參閱<xref linkend="storage-tips-ceph-btrfs-subvol"/>。
    </para>
    <para>
     對每個 Salt Minion 套用以下指令：
    </para>
<screen><prompt>root@master # </prompt>salt 'MONITOR_NODES' saltutil.sync_all
<prompt>root@master # </prompt>salt 'MONITOR_NODES' state.apply ceph.subvolume
</screen>
    <note>
     <para>
      Ceph.subvolume 指令會建立 <filename>/var/lib/ceph</filename> 做為 <filename>@/var/lib/ceph</filename> Btrfs 子磁碟區。
     </para>
    </note>
    <para>
     現已掛接新子磁碟區，且更新了 <literal>/etc/fstab</literal>。
    </para>
   </step>
   <step>
    <para>
     準備叢集。如需更多詳細資料，請參閱<xref linkend="deepsea-stage-description"/>。
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.0</screen>
    <para>
     或
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.prep</screen>
    <note>
     <title>使用 DeepSea CLI 執行或監控階段</title>
     <para>
      使用 DeepSea CLI，可透過在監控模式下執行 DeepSea CLI，或者直接透過 DeepSea CLI 執行階段，來即時追蹤階段執行進度。如需詳細資料，請參閱<xref linkend="deepsea-cli"/>。
     </para>
    </note>
   </step>
   <step>
    <para>
     探查階段會從所有 Minion 收集資料並建立組態片段，這些片段儲存在 <filename>/srv/pillar/ceph/proposals</filename> 目錄中。資料以 YAML 格式儲存在 *.sls 或 *.yml 檔案中。
    </para>
    <para>
     執行以下指令以觸發探查階段：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.1</screen>
    <para>
     或
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.discovery</screen>
   </step>
   <step>
    <para>
     成功完成上述指令後，請在 <filename>/srv/pillar/ceph/proposals</filename> 中建立 <filename>policy.cfg</filename> 檔案。如需詳細資料，請參閱<xref linkend="policy-configuration"/>。
    </para>
    <tip>
     <para>
      如果需要變更叢集的網路設定，請編輯 <filename>/srv/pillar/ceph/stack/ceph/cluster.yml</filename>，調整以 <literal>cluster_network:</literal> 和 <literal>public_network:</literal> 開頭的行。
     </para>
    </tip>
   </step>
   <step>
    <para>
     組態階段將會剖析 <filename>policy.cfg</filename> 檔案，並將包含的檔案合併為其最終形式。叢集和角色相關的內容放置於 <filename>/srv/pillar/ceph/cluster</filename> 中，而 Ceph 特定的內容放置於 <filename>/srv/pillar/ceph/stack/default</filename> 中。
    </para>
    <para>
     執行以下指令以觸發組態階段：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.2</screen>
    <para>
     或
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.configure</screen>
    <para>
     組態步驟可能需要數秒完成。指令完成後，您可以透過執行以下指令，檢視指定 Minion (例如，名為 <literal>ceph_minion1</literal>、<literal>ceph_minion2</literal> 等的 Minion) 的 pillar 資料：
    </para>
<screen><prompt>root@master # </prompt>salt 'ceph_minion*' pillar.items</screen>
    <tip>
     <title>修改 OSD 的配置</title>
     <para>
      如果您要修改預設的 OSD 配置並變更 DriveGroups 組態，請執行<xref linkend="ds-drive-groups"/>所述的程序。
     </para>
    </tip>
    <note>
     <title>覆寫預設值</title>
     <para>
      一旦指令完成，您便可檢視預設組態並視需要進行變更。如需詳細資料，請參閱<xref linkend="ceph-deploy-ds-custom"/>。
     </para>
    </note>
   </step>
   <step>
    <para>
     現在執行部署階段。在此階段，將驗證 Pillar 並會啟動 Ceph 監控程式和 Ceph OSD 精靈：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.3</screen>
    <para>
     或
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.deploy</screen>
    <para>
     執行該指令需要幾分鐘時間。如果該指令失敗，則您需要解決問題，然後再次執行前面的階段。該指令成功後，請執行以下指令來檢查狀態：
    </para>
<screen><prompt>cephadm@adm &gt; </prompt>ceph -s</screen>
   </step>
   <step>
    <para>
     Ceph 叢集部署程序的最後一個步驟是<emphasis>服務</emphasis>階段。在此階段，您要例項化目前支援的所有服務：iSCSI 閘道、CephFS、物件閘道、 和 NFS Ganesha。此階段將建立所需的池、授權金鑰圈和啟動服務。若要啟動該階段，請執行以下指令：
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.4</screen>
    <para>
     或
    </para>
<screen><prompt>root@master # </prompt>salt-run state.orch ceph.stage.services</screen>
    <para>
     視具體設定而定，該指令可能會執行幾分鐘時間。
    </para>
   </step>
   <step>
    <para>
     繼續之前，我們強烈建議先啟用 Ceph 遙測模組。如需詳細資訊和說明，請參閱<xref linkend="mgr-modules-telemetry"/>。
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="deepsea-cli">
  <title>DeepSea CLI</title>

  <para>
   DeepSea 還提供了一個指令行介面 (CLI) 工具，供使用者監控或執行階段，同時即時直觀地呈現執行進度。確認是否已安裝 <package>deepsea-cli</package> 套件 (在您執行 <command>deepsea</command> 可執行檔之前)。
  </para>

  <para>
   支援使用以下兩種模式來視覺化階段的執行進度：
  </para>

  <itemizedlist xml:id="deepsea-cli-modes">
   <title>DeepSea CLI 模式</title>
   <listitem>
    <para>
     <emphasis role="bold">監控模式</emphasis>：視覺化另一個終端工作階段中發出的 <command>salt-run</command> 指令所觸發 DeepSea 階段的執行進度。
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis role="bold">獨立模式</emphasis>：執行 DeepSea 階段，並在該階段的組成步驟執行時提供相應的即時視覺化效果。
    </para>
   </listitem>
  </itemizedlist>

  <important>
   <title>DeepSea CLI 指令</title>
   <para>
    在 Salt Master 節點上，必須使用 <systemitem class="username">root</systemitem> 權限才能執行 DeepSea CLI 指令。
   </para>
  </important>

  <sect2 xml:id="deepsea-cli-monitor">
   <title>DeepSea CLI：監控模式</title>
   <para>
    進度監控程式提供詳細的即時視覺化效果，顯示在其他終端機工作階段中使用 <command>salt-run state.orch</command> 指令執行階段期間發生的情況。
   </para>
   <tip>
    <title>在新終端機工作階段中啟動監控程式</title>
    <para>
     在執行任何 <command>salt-run state.orch</command> 指令<emphasis>之前</emphasis>，您都需要在新終端機視窗中啟動監控程式，如此，當階段開始執行時，監控程式便可以偵測到。
    </para>
   </tip>
   <para>
    如果在發出 <command>salt-run state.orch</command> 指令之後再啟動監控程式，將不會顯示執行進度。
   </para>
   <para>
    您可執行以下指令來啟動監控模式：
   </para>
<screen><prompt>root@master # </prompt>deepsea monitor</screen>
   <para>
    如需 <command>deepsea 監控程式</command>指令的可用指令行選項的詳細資訊，請查看該指令的手冊頁：
   </para>
<screen><prompt>root@master # </prompt>man deepsea-monitor</screen>
  </sect2>

  <sect2 xml:id="deepsea-cli-standalone">
   <title>DeepSea CLI：獨立模式</title>
   <para>
    在獨立模式下，可以使用 DeepSea CLI 來執行 DeepSea 階段，並即時顯示其執行進度。
   </para>
   <para>
    從 DeepSea CLI 執行 DeepSea 階段的指令遵循以下格式：
   </para>
<screen><prompt>root@master # </prompt>deepsea stage run <replaceable>stage-name</replaceable></screen>
   <para>
    其中，<replaceable>stage-name</replaceable> 對應於 Salt 協調化狀態檔案的參考方式。例如，對應於 <emphasis role="bold">/srv/salt/ceph/stage/deploy</emphasis> 中目錄的<filename>部署</filename>階段以 <emphasis role="bold">ceph.stage.deploy</emphasis> 的形式參考。
   </para>
   <para>
    此指令可取代用於執行 DeepSea 階段 (或任何 DeepSea 協調化狀態檔案) 的基於 Salt 的指令。
   </para>
   <para>
    指令 <command>deepsea stage run ceph.stage.0</command> 與 <command>salt-run state.orch ceph.stage.0</command> 等效。
   </para>
   <para>
    如需 <command>deepsea stage run</command> 指令接受的可用指令行選項的詳細資訊，請查看該指令的手冊頁：
   </para>
<screen><prompt>root@master # </prompt>man deepsea-stage run</screen>
   <para>
    下圖顯示了執行<emphasis role="underline">階段 2</emphasis> 時，DeepSea CLI 的輸出範例：
   </para>
   <figure>
    <title>DeepSea CLI 階段執行進度輸出</title>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="deepsea-cli-stage2-screenshot.png" width="70%" format="PNG"/>
     </imageobject>
    </mediaobject>
   </figure>
   <sect3 xml:id="deepsea-cli-run-alias">
    <title>DeepSea CLI <command>stage run</command> 別名</title>
    <para>
     針對 Salt 的進階使用者，我們還支援使用別名來執行 DeepSea 階段，採用執行階段所用的 Salt 指令 (例如 <command>salt-run state.orch <replaceable>stage-name</replaceable></command>) 做為 DeepSea CLI 的指令。
    </para>
    <para>
     範例：
    </para>
<screen><prompt>root@master # </prompt>deepsea salt-run state.orch <replaceable>stage-name</replaceable></screen>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="deepsea-pillar-salt-configuration">
  <title>組態和自訂</title>

  <sect2 xml:id="policy-configuration">
   <title><filename>policy.cfg</filename> 檔案</title>
   <para>
    <filename>/srv/pillar/ceph/proposals/policy.cfg</filename> 組態檔案用於確定個別叢集節點的角色。例如，哪些節點充當 Ceph OSD 或 Ceph 監控程式。請編輯 <filename>policy.cfg</filename>，以反映所需的叢集設定。區段採用任意順序，但所包含行的內容將覆寫前面行的內容中相符的金鑰。
   </para>
   <tip>
    <title><filename>policy.cfg</filename> 的範例</title>
    <para>
     您可以在 <filename>/usr/share/doc/packages/deepsea/examples/</filename> 目錄中找到完整規則檔案的多個範例。
    </para>
   </tip>
   <sect3 xml:id="policy-cluster-assignment">
    <title>叢集指定</title>
    <para>
     在 <emphasis role="bold">cluster</emphasis> 區段中選取叢集的 Minion。您可以選取所有 Minion，也可以將 Minion 加入黑名單或白名單。下面顯示了名為 <emphasis role="bold">ceph</emphasis> 的叢集的範例。
    </para>
    <para>
     若要包含<emphasis role="bold">所有</emphasis> Minion，請新增以下幾行：
    </para>
<screen>cluster-ceph/cluster/*.sls</screen>
    <para>
     若要將特定的 Minion 加入<emphasis role="bold">白名單</emphasis>，請執行以下指令：
    </para>
<screen>cluster-ceph/cluster/abc.domain.sls</screen>
    <para>
     若要將一組 Minion 加入白名單，可以使用外圍程序萬用字元：
    </para>
<screen>cluster-ceph/cluster/mon*.sls</screen>
    <para>
     若要將 Minion 加入<emphasis role="bold">黑名單</emphasis>，可將其設定為 <literal>unassigned</literal>：
    </para>
<screen>cluster-unassigned/cluster/client*.sls</screen>
   </sect3>
   <sect3 xml:id="policy-role-assignment">
    <title>角色指定</title>
    <para>
     本節詳細介紹了如何為您的叢集節點指定「角色」。在本文中，「角色」是指您需要在節點上執行的服務，例如 Ceph 監控程式、物件閘道或 iSCSI 閘道。不會自動指定任何角色，只會部署已新增到 <command>policy.cfg</command> 中的角色。
    </para>
    <para>
     指定遵循以下模式：
    </para>
<screen>role-<replaceable>ROLE_NAME</replaceable>/<replaceable>PATH</replaceable>/<replaceable>FILES_TO_INCLUDE</replaceable></screen>
    <para>
     其中的項目具有以下意義和值：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <replaceable>ROLE_NAME</replaceable> 為下列任何一項：「master」、「admin」、「mon」、「mgr」、「storage」、「mds」、「igw」、「rgw」、「ganesha」、「grafana」或「prometheus」。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>PATH</replaceable> 是 .sls 或 .yml 檔案的相對目錄路徑。對於 .sls 檔案，該路徑通常是 <filename>cluster</filename>；而 .yml 檔案則位於 <filename>stack/default/ceph/minions</filename>。
      </para>
     </listitem>
     <listitem>
      <para>
       <replaceable>FILES_TO_INCLUDE</replaceable> 是 Salt 狀態檔案或 YAML 組態檔案。這些檔案通常包含 Salt Minion 的主機名稱，例如 <filename>ses5min2.yml</filename>。您可以使用外圍程序萬用字元進行更具體的相符。
      </para>
     </listitem>
    </itemizedlist>
    <para>
     每個角色的範例如下：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       <emphasis>master</emphasis> - 該節點具有所有 Ceph 叢集的管理金鑰圈。目前僅支援一個 Ceph 叢集。由於 <emphasis>master</emphasis> 角色是必需的，因此，請一律新增類似下方所示的行：
      </para>
<screen>role-master/cluster/master*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>admin</emphasis> - 該 Minion 將取得管理金鑰圈。您可按如下方式定義角色：
      </para>
<screen>role-admin/cluster/abc*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mon</emphasis> - 該 Minion 將向 Ceph 叢集提供監控程式服務。此角色需要已指定 Minion 的位址。從 SUSE Enterprise Storage 5 開始，將以動態方式計算公用位址，並且 Salt Pillar 中不再需要該位址。
      </para>
<screen>role-mon/cluster/mon*.sls</screen>
      <para>
       該範例將監控程式角色指定給一組 Minion。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>mgr</emphasis> - 從整個叢集收集所有狀態資訊的 Ceph 管理員精靈。請將它部署在您想要部署 Ceph 監控程式角色的所有 Minion 上。
      </para>
<screen>role-mgr/cluster/mgr*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>storage</emphasis> - 使用此角色可指定儲存節點。
      </para>
<screen>role-storage/cluster/data*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>mds</emphasis> - 該 Minion 將提供中繼資料服務以支援 CephFS。
      </para>
<screen>role-mds/cluster/mds*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>igw</emphasis> - 該 Minion 將充當 iSCSI 閘道。此角色需要所指定 Minion 的位址，因此，您還需要包含 <filename>stack</filename> 目錄中的檔案：
      </para>
<screen>role-igw/cluster/*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>rgw</emphasis> - 該 Minion 將充當物件閘道：
      </para>
<screen>role-rgw/cluster/rgw*.sls</screen>
     </listitem>
     <listitem>
      <para>
       <emphasis>ganesha</emphasis> - 該 Minion 將充當 NFS Ganesha 伺服器。「ganesha」角色需要叢集中的「rgw」或「mds」角色，否則，驗證將於階段 3 失敗。
      </para>
<screen>role-ganesha/cluster/ganesha*.sls</screen>
      <para>
       若要成功安裝 NFS Ganesha，需要進行額外的設定。如果您要使用 NFS Ganesha，請在執行階段 2 和 4 之前閱讀<xref linkend="cha-as-ganesha"/>。但是，也可於稍後再安裝 NFS Ganesha。
      </para>
      <para>
       在某些情況下，為 NFS Ganesha 節點定義自訂角色可能很有用。如需詳細資料，請參閱<xref linkend="ceph-nfsganesha-customrole"/>。
      </para>
     </listitem>
     <listitem>
      <para>
       <emphasis>grafana</emphasis>、<emphasis>prometheus</emphasis> - 此節點會將基於 Prometheus 警示的 Grafana 圖表新增至 Ceph Dashboard。如需詳細描述，請參閱<xref linkend="ceph-dashboard"/>。
      </para>
<screen>role-grafana/cluster/grafana*.sls</screen>
<screen>role-prometheus/cluster/prometheus*.sls</screen>
     </listitem>
    </itemizedlist>
    <note>
     <title>叢集節點的多個角色</title>
     <para>
      您可將多個角色指定給一個節點。例如，可將「mds」角色指定給監控程式節點：
     </para>
<screen>role-mds/cluster/mon[1,2]*.sls</screen>
    </note>
   </sect3>
   <sect3 xml:id="policy-common-configuration">
    <title>通用組態</title>
    <para>
     通用組態區段包括<emphasis>探查 (階段 1)</emphasis>期間產生的組態檔案。這些組態檔案儲存 <literal>fsid</literal> 或 <literal>public_network</literal> 等參數。若要包含所需的 Ceph 通用組態，請新增以下幾行：
    </para>
<screen>config/stack/default/global.yml
config/stack/default/ceph/cluster.yml</screen>
   </sect3>
   <sect3 xml:id="deepsea-policy-filtering">
    <title>項目過濾</title>
    <para>
     有時，使用 *.sls 萬用字元無法包含給定目錄中的所有檔案。<filename>policy.cfg</filename> 檔案剖析器可識別以下過濾器：
    </para>
    <warning>
     <title>進階方法</title>
     <para>
      本節介紹供進階使用者使用的過濾方法。如果使用不當，過濾可能會導致問題發生，例如，節點編號改變。
     </para>
    </warning>
    <variablelist>
     <varlistentry>
      <term>slice=[start:end]</term>
      <listitem>
       <para>
        使用 slice 過濾器可以僅包含從 <emphasis>start</emphasis> 到 <emphasis>end-1</emphasis> 的項目。請注意，給定目錄中的項目將依英數字元順序排序。下行包含 <filename>role-mon/cluster/</filename> 子目錄中的第三到第五個檔案：
       </para>
<screen>role-mon/cluster/*.sls slice[3:6]</screen>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>re=regexp</term>
      <listitem>
       <para>
        使用一般運算式過濾器可以僅包含與給定運算式相符的項目。例如：
       </para>
<screen>role-mon/cluster/mon*.sls re=.*1[135]\.subdomainX\.sls$</screen>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="deepsea-example-policy-cfg">
    <title>例如 <filename>policy.cfg</filename> 檔案</title>
    <para>
     下面是一個基本 <filename>policy.cfg</filename> 檔案的範例：
    </para>
<screen>## Cluster Assignment
cluster-ceph/cluster/*.sls <co xml:id="co-policy-1"/>

## Roles
# ADMIN
role-master/cluster/examplesesadmin.sls <co xml:id="co-policy-2"/>
role-admin/cluster/sesclient*.sls <co xml:id="co-policy-3"/>

# MON
role-mon/cluster/ses-example-[123].sls <co xml:id="co-policy-5"/>

# MGR
role-mgr/cluster/ses-example-[123].sls <co xml:id="co-policy-mgr"/>

# STORAGE
role-storage/cluster/ses-example-[5,6,7,8].sls <co xml:id="co-policy-storage"/>

# MDS
role-mds/cluster/ses-example-4.sls <co xml:id="co-policy-6"/>

# IGW
role-igw/cluster/ses-example-4.sls <co xml:id="co-policy-10"/>

# RGW
role-rgw/cluster/ses-example-4.sls <co xml:id="co-policy-11"/>

# COMMON
config/stack/default/global.yml <co xml:id="co-policy-8"/>
config/stack/default/ceph/cluster.yml <co xml:id="co-policy-13"/>
</screen>
    <calloutlist>
     <callout arearefs="co-policy-1">
      <para>
       指示在 Ceph 叢集中包含所有 Minion。如果您不想在 Ceph 叢集中包含某些 Minion，請使用：
      </para>
<screen>cluster-unassigned/cluster/*.sls
cluster-ceph/cluster/ses-example-*.sls</screen>
      <para>
       第一行將所有 Minion 標記為未指定。第二行覆寫與「ses-example-*.sls」相符的 Minion，並將其指定給 Ceph 叢集。
      </para>
     </callout>
     <callout arearefs="co-policy-2">
      <para>
       名為「examplesesadmin」的 Minion 具有「master」角色。順便指出，這表示該 Minion 將取得叢集的管理金鑰。
      </para>
     </callout>
     <callout arearefs="co-policy-3">
      <para>
       與「sesclient*」相符的所有 Minion 也將取得管理金鑰。
      </para>
     </callout>
     <callout arearefs="co-policy-5">
      <para>
       與「ses-example-[123]」相符的所有 Minion (應該是 ses-example-1、ses-example-2 和 ses-example-3 這三個 Minion) 將設定為 MON 節點。
      </para>
     </callout>
     <callout arearefs="co-policy-mgr">
      <para>
       與「ses-example-[123]」相符的所有 Minion (範例中的所有 MON 節點) 將設定為 MGR 節點。
      </para>
     </callout>
     <callout arearefs="co-policy-storage">
      <para>
       所有與「ses-example-[5,6,7,8]」相符的 Minion 都將設定為儲存節點。
      </para>
     </callout>
     <callout arearefs="co-policy-6">
      <para>
       Minion「ses-example-4」將具有 MDS 角色。
      </para>
     </callout>
     <callout arearefs="co-policy-10">
      <para>
       Minion「ses-example-4」將具有 IGW 角色。
      </para>
     </callout>
     <callout arearefs="co-policy-11">
      <para>
       Minion「ses-example-4」將具有 RGW 角色。
      </para>
     </callout>
     <callout arearefs="co-policy-8">
      <para>
       表示我們接受 <option>fsid</option> 和 <option>public_network</option> 等通用組態參數的預設值。
      </para>
     </callout>
     <callout arearefs="co-policy-13">
      <para>
       表示我們接受 <option>fsid</option> 和 <option>public_network</option> 等通用組態參數的預設值。
      </para>
     </callout>
    </calloutlist>
   </sect3>
  </sect2>

  <sect2 xml:id="ds-drive-groups">
   <title>DriveGroups</title>
   <para>
    <emphasis>DriveGroups</emphasis> 用於指定 Ceph 叢集中 OSD 的配置。需要在單個檔案 <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> 中定義 DriveGroups。
   </para>
   <para>
    管理員應手動指定一組相關的 OSD (在固態硬碟和旋轉式硬碟上部署的混合式 OSD)，或一組使用相同部署選項的 OSD (例如，物件儲存、加密選項完全相同的各獨立 OSD)。為了避免明確列出裝置，DriveGroups 會使用與 <command>ceph-volume</command> 庫存報告中的幾個所選欄位對應的過濾項目清單。在最簡單的情形中，過濾項目可能就是「rotational」旗標 (所有固態磁碟機為「db_devices」，所有旋轉式磁碟機為「data devices」)，或者是相關性更高的項目 (例如「model」字串或大小)。DeepSea 將提供用於將這些 DriveGroups 轉換為供使用者檢查的實際裝置清單的代碼。
   </para>
   <para>
    下面是一個簡單的程序，演示了設定 DriveGroups 時的基本工作流程：
   </para>
   <procedure>
    <step>
     <para>
      檢查磁碟的內容是否與 <command>ceph-volume</command> 指令的輸出一致。DriveGroups 僅可接受這些內容：
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.details
</screen>
    </step>
    <step>
     <para>
      開啟 <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> YAML 檔案，並依據您的需求進行調整。請參閱 <xref linkend="ds-drive-groups-specs"/>。請記得使用空格而不是定位字元。如需更多進階範例，請參閱<xref linkend="ds-drive-groups-examples"/>。下面的範例包括 Ceph 可用做 OSD 的所有磁碟機：
     </para>
<screen>
default_drive_group_name:
  target: '*'
  data_devices:
    all: true
</screen>
    </step>
    <step>
     <para>
      確認新配置：
     </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.list
</screen>
     <para>
      此執行程式會依據您的 DriveGroups 傳回由相符磁碟組成的結構。如果您對結果不滿意，請重複上一步。
     </para>
     <tip>
      <title>詳細報告</title>
      <para>
       除了 <command>disks.list</command> 執行程式外，您還可以使用 <command>disks.report</command> 執行程式列印接下來的 DeepSea 階段 3 呼叫中將發生事件的詳細報告。
      </para>
<screen>
<prompt>root@master # </prompt>salt-run disks.report
</screen>
     </tip>
    </step>
    <step>
     <para>
      部署 OSD。在接下來的 DeepSea 階段 3 呼叫中，將依據您的磁碟機群組規格部署 OSD 磁碟。
     </para>
    </step>
   </procedure>
   <sect3 xml:id="ds-drive-groups-specs">
    <title>規格</title>
    <para>
     <filename>/srv/salt/ceph/configuration/files/drive_groups.yml</filename> 接受以下選項：
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  db_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  wal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  block_wal_size: '5G'  # (optional, unit suffixes permitted)
  block_db_size: '5G'   # (optional, unit suffixes permitted)
  osds_per_device: 1   # number of osd daemons per device
  format:              # 'bluestore' or 'filestore' (defaults to 'bluestore')
  encryption:           # 'True' or 'False' (defaults to 'False')
</screen>
    <para>
     對於 FileStore 設定，<filename>drive_groups.yml</filename> 可能如下所示：
    </para>
<screen>
drive_group_default_name:
  target: *
  data_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  journal_devices:
    drive_spec: <replaceable>DEVICE_SPECIFICATION</replaceable>
  format: filestore
  encryption: True
</screen>
   </sect3>
   <sect3>
    <title>相符磁碟裝置</title>
    <para>
     您可以使用以下過濾器描述規格：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       依磁碟型號：
      </para>
<screen>
model: <replaceable>DISK_MODEL_STRING</replaceable>
</screen>
     </listitem>
     <listitem>
      <para>
       依磁碟廠商：
      </para>
<screen>
vendor: <replaceable>DISK_VENDOR_STRING</replaceable>
</screen>
      <tip>
       <title>小寫的廠商字串</title>
       <para>
        <replaceable>DISK_VENDOR_STRING</replaceable> 一律採用小寫。
       </para>
      </tip>
     </listitem>
     <listitem>
      <para>
       磁碟是否為旋轉式硬碟。SSD 和 NVME 磁碟機不屬於旋轉式硬碟。
      </para>
<screen>
rotational: 0
</screen>
     </listitem>
     <listitem>
      <para>
       部署將<emphasis>全部</emphasis>可用磁碟機皆用於 OSD 的節點：
      </para>
<screen>
data_devices:
  all: true
</screen>
     </listitem>
     <listitem>
      <para>
       此外，還可透過限制相符磁碟的數量來過濾：
      </para>
<screen>
limit: 10
</screen>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>依大小過濾裝置</title>
    <para>
     您可以依磁碟裝置大小對其過濾，可以依確切大小也可以依大小範圍來過濾。<option>size:</option> 參數接受使用下列格式的引數：
    </para>
    <itemizedlist>
     <listitem>
      <para>
       '10G' - 包括大小為該值的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       '10G:40G' - 包括大小在該範圍內的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       ':10G' - 包括大小小於或等於 10 GB 的磁碟。
      </para>
     </listitem>
     <listitem>
      <para>
       '40G:' - 包括大小等於或大於 40 GB 的磁碟。
      </para>
     </listitem>
    </itemizedlist>
    <example>
     <title>依磁碟大小比對</title>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '40TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <note>
     <title>需要引號</title>
     <para>
      如果使用「:」分隔符，您需要使用引號括住大小，否則系統會將「:」符號解譯為新組態雜湊。
     </para>
    </note>
    <tip>
     <title>單位捷徑</title>
     <para>
      除了 G (十億位元組)，您還可以使用 M (百萬位元組) 或 T (兆位元組) 來指定大小。
     </para>
    </tip>
   </sect3>
   <sect3 xml:id="ds-drive-groups-examples">
    <title>範例</title>
    <para>
     本節包含其他 OSD 設定的範例。
    </para>
    <example>
     <title>簡單設定</title>
     <para>
      下面的範例描述了使用相同設定的兩個節點：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      對應的 <filename>drive_groups.yml</filename> 檔案如下所示：
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: MC-55-44-XZ
   </screen>
     <para>
      這樣的組態簡單有效。但問題是管理員將來可能要新增來自不同廠商的磁碟，而這樣的磁碟不在可新增範圍內。您可以透過減少針對磁碟機核心內容的過濾器來予以改進。
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
   </screen>
     <para>
      在上面的範例中，我們強制將所有旋轉式裝置宣告為「data devices」，將所有非旋轉式裝置當成「shared devices」(wal、db) 使用。
     </para>
     <para>
      如果您知道大小超過 2 TB 的磁碟機將一律做為較慢的資料裝置，則可以依大小過濾：
     </para>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    size: '2TB:'
  db_devices:
    size: ':2TB'
</screen>
    </example>
    <example>
     <title>進階設定</title>
     <para>
      下面的範例描述了兩種不同的設定：20 個 HDD 將共用 2 個 SSD，而 10 個 SSD 將共用 2 個 NVMe。
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        12 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      這樣的設定可使用如下兩個配置定義：
     </para>
<screen>
drive_group:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
</screen>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    vendor: samsung
    size: 256GB
</screen>
    </example>
    <example>
     <title>包含非統一節點的進階設定</title>
     <para>
      上述範例假設所有節點的磁碟機都相同，但情況並非總是如此：
     </para>
     <para>
      節點 1-5：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      節點 6-10：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        5 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        20 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      您可以在配置中使用「target」鍵來定位特定節點。Salt 定位標記可讓事情變得簡單：
     </para>
<screen>
drive_group_node_one_to_five:
  target: 'node[1-5]'
  data_devices:
    rotational: 1
  db_devices:
    rotational: 0
</screen>
     <para>
      後接
     </para>
<screen>
drive_group_the_rest:
  target: 'node[6-10]'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
</screen>
    </example>
    <example>
     <title>專家設定</title>
     <para>
      上述所有案例都假設 WAL 和 DB 使用相同裝置，但也有可能會在專屬裝置上部署 WAL：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        20 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        2 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
<screen>
drive_group_default:
  target: '*'
  data_devices:
    model: MC-55-44-XZ
  db_devices:
    model: SSD-123-foo
  wal_devices:
    model: NVME-QQQQ-987
</screen>
    </example>
    <example>
     <title>複雜 (和不太可能的) 設定</title>
     <para>
      在以下設定中，我們嘗試定義：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        由 1 個 NVMe 支援 20 個 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 個 SSD (db) 和 1 個 NVMe (wal) 支援 2 個 HDD
       </para>
      </listitem>
      <listitem>
       <para>
        由 1 個 NVMe 支援 8 個 SSD
       </para>
      </listitem>
      <listitem>
       <para>
        2 個獨立 SSD (加密)
       </para>
      </listitem>
      <listitem>
       <para>
        1 個 HDD 做為備用，不應部署
       </para>
      </listitem>
     </itemizedlist>
     <para>
      共使用如下數量的磁碟機：
     </para>
     <itemizedlist>
      <listitem>
       <para>
        23 個 HDD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Intel
         </para>
        </listitem>
        <listitem>
         <para>
          型號：SSD-123-foo
         </para>
        </listitem>
        <listitem>
         <para>
          大小：4 TB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        10 個 SSD
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Micron
         </para>
        </listitem>
        <listitem>
         <para>
          型號：MC-55-44-ZX
         </para>
        </listitem>
        <listitem>
         <para>
          大小：512 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
      <listitem>
       <para>
        1 個 NVMe
       </para>
       <itemizedlist>
        <listitem>
         <para>
          廠商：Samsung
         </para>
        </listitem>
        <listitem>
         <para>
          型號：NVME-QQQQ-987
         </para>
        </listitem>
        <listitem>
         <para>
          大小：256 GB
         </para>
        </listitem>
       </itemizedlist>
      </listitem>
     </itemizedlist>
     <para>
      DriveGroups 定義如下所示：
     </para>
<screen>
drive_group_hdd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_hdd_ssd_nvme:
  target: '*'
  data_devices:
    rotational: 0
  db_devices:
    model: MC-55-44-XZ
  wal_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_nvme:
  target: '*'
  data_devices:
    model: SSD-123-foo
  db_devices:
    model: NVME-QQQQ-987
 </screen>
<screen>
drive_group_ssd_standalone_encrypted:
  target: '*'
  data_devices:
    model: SSD-123-foo
  encryption: True
 </screen>
     <para>
      在檔案的整個剖析過程中，一直會保留一個 HDD。
     </para>
    </example>
   </sect3>
  </sect2>

  <sect2>
   <title>使用自訂設定調整 <filename>ceph.conf</filename></title>
   <para>
    如果需要將自訂設定放入 <filename>ceph.conf</filename> 組態檔案，請參閱<xref linkend="ds-custom-cephconf"/>瞭解更多詳細資料。
   </para>
  </sect2>
 </sect1>
</chapter>
