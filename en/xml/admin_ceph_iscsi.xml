<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.iscsi">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>&ceph; iSCSI Gateway</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
 </info>
 <para>
  iSCSI is a storage area network (SAN) protocol that allows clients (called
  <emphasis>initiators</emphasis>) to send SCSI commands to SCSI storage
  devices (<emphasis>targets</emphasis>) on remote servers. &storage;
  includes a facility that opens &ceph; storage management to heterogeneous
  clients, such as Microsoft Windows and VMware vSphere, through the iSCSI
  protocol. Multipath iSCSI access enables availability and scalability for
  these clients, and the standardized iSCSI protocol also provides an
  additional layer of security isolation between clients and the SUSE
  Enterprise Storage 2 cluster. The configuration facility is named
  <systemitem>lrbd</systemitem>. Using <systemitem>lrbd</systemitem>, &ceph;
  storage administrators can define thin-provisioned, replicated,
  highly-available volumes supporting read-only snapshots, read-write
  clones, and automatic resizing with &ceph; RADOS Block Device (RBD).
  Administrators can then export volumes either via a single
  <systemitem>lrbd</systemitem> gateway host, or via multiple gateway hosts
  supporting multipath failover. Linux, Windows, and VMware hosts can
  connect to volumes using the iSCSI protocol, which makes them available
  like any other SCSI block device. This means &storage; 2 customers can
  effectively run a complete block-storage infrastructure subsystem on
  &ceph; that provides all features and benefits of a conventional SAN
  enabling future growth.
 </para>
 <para>
  This chapter introduces detailed information how to set up a &ceph;
  cluster infrastructure together with an iSCSI gateway so that the client
  hosts can utilize remotely stored data as local storage devices using the
  iSCSI protocol.
 </para>
 <sect1 xml:id="ceph.iscsi.iscsi">
  <title>iSCSI Block Storage</title>

  <para>
   iSCSI is an implementation of the Small Computer System Interface (SCSI)
   command set using the Internet Protocol (IP), specified in RFC 3720.
   iSCSI is implemented as a service where a client (the initiator) talks to
   a server (the target) via a session on TCP port 3260. An iSCSI target's
   IP address and port are called an iSCSI portal, where a target can be
   exposed through one or more portals. The combination of a target and one
   or more portals is called the target portal group (TPG).
  </para>

  <para>
   The underlying data link layer protocol for iSCSI is commonly Ethernet.
   More specifically, modern iSCSI infrastructures use 10 Gigabit Ethernet
   or faster networks for optimal throughput. 10 Gigabit Ethernet
   connectivity between the iSCSI gateway and the back-end &ceph; cluster is
   strongly recommended.
  </para>

  <sect2 xml:id="ceph.iscsi.iscsi.target">
   <title>The Linux Kernel iSCSI Target</title>
   <para>
    The Linux kernel iSCSI target was originally named LIO for
    linux-iscsi.org, the project's original domain and website. For some
    time, no fewer than 4 competing iSCSI target implementations were
    available for the Linux platform, but LIO ultimately prevailed as the
    single iSCSI reference target. The mainline kernel code for LIO uses the
    simple, but somewhat ambiguous name "target", distinguishing between
    "target core" and a variety of front-end and back-end target modules.
   </para>
   <para>
    The most commonly used front-end module is arguably iSCSI. However, LIO
    also supports Fibre Channel (FC), Fibre Channel over Ethernet (FCoE) and
    several other front-end protocols. At this time, only the iSCSI protocol
    is supported by &storage;.
   </para>
   <para>
    The most frequently used target back-end module is one that is capable
    of simply re-exporting any available block device on the target host.
    This module is named iblock. However, LIO also has an RBD-specific
    back-end module supporting parallelized multipath I/O access to RBD
    images.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.iscsi.initiators">
   <title>iSCSI Initiators</title>
   <para>
    This section introduces a brief information on iSCSI initiators used on
    Linux, Microsoft Windows, and VMware platforms.
   </para>
   <sect3>
    <title>Linux</title>
    <para>
     The standard initiator for the Linux platform is
     <systemitem>open-iscsi</systemitem>.
     <systemitem>open-iscsi</systemitem> launches a daemon,
     <systemitem>iscsid</systemitem>, which the user can then use to
     discover iSCSI targets on any given portal, log in to targets, and map
     iSCSI volumes. <systemitem>iscsid</systemitem> communicates with the
     SCSI mid layer to create in-kernel block devices that the kernel can
     then treat like any other SCSI block device on the system. The
     <systemitem>open-iscsi</systemitem> initiator can be deploying in
     conjunction with the Device Mapper Multipath
     (<systemitem>dm-multipath</systemitem>) facility to provide a highly
     available iSCSI block device.
    </para>
   </sect3>
   <sect3>
    <title>Microsoft Windows and Hyper-V</title>
    <para>
     The default iSCSI initiator for the Microsoft Windows operating system
     is the Microsoft iSCSI initiator. The iSCSI service can be configured
     via a Graphical User Interface (GUI), and supports multipath I/O for
     high availability.
    </para>
   </sect3>
   <sect3>
    <title>VMware</title>
    <para>
     The default iSCSI initiator for VMware vSphere and ESX is the VMware
     ESX software iSCSI initiator, <systemitem>vmkiscsi</systemitem>. Once
     enabled, it can be configured either from the vSphere client, or using
     the <command>vmkiscsi-tool</command> command. You can then format
     storage volumes connected through the vSphere iSCSI storage adapter
     with VMFS, and use them like any other VM storage device. The VMware
     initiator also supports multipath I/O for high availability.
    </para>
   </sect3>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.lrdb">
  <title>General Information about lrdb</title>

  <para>
   <systemitem>lrbd</systemitem> combines the benefits of RADOS Block
   Devices with the ubiquitous versatility of iSCSI. By employing
   <systemitem>lrbd</systemitem> on an iSCSI target host (known as the
   <systemitem>lrbd</systemitem> gateway), any application that needs to
   make use of block storage can benefit from &ceph;, even if it does not
   speak any &ceph; client protocol. Instead, users can use iSCSI or any
   other target front-end protocol to connect to an LIO target, which
   translates all target I/O to RBD storage operations.
  </para>

  <figure>
   <title>&ceph; Cluster with a Single iSCSI Gateway</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme1.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   <systemitem>lrbd</systemitem> is inherently highly-available and supports
   multipath operations. Thus, downstream initiator hosts can use multiple
   iSCSI gateways for both high availability and scalability. When
   communicating with an iSCSI configuration with more than one gateway,
   initiators may load-balance iSCSI requests across multiple gateways. In
   the event of a gateway failing, being temporarily unreachable, or being
   disabled for maintenance, I/O will transparently continue via another
   gateway.
  </para>

  <figure>
   <title>&ceph; Cluster with Multiple iSCSI Gateways</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="lrbd_scheme2.png" width="75%" format="PNG"/>
    </imageobject>
   </mediaobject>
  </figure>
 </sect1>
 <sect1 xml:id="ceph.iscsi.deploy">
  <title>Deployment Considerations</title>

  <para>
   A minimum configuration of &storage; with <systemitem>lrbd</systemitem>
   consists of the following components:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. The &ceph; cluster consists of a minimum of
     four physical servers hosting at least eight object storage daemons
     (OSDs) each. In such a configuration, three OSD nodes also double as a
     monitor (MON) host.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI target server running the LIO iSCSI target, configured via
     <systemitem>lrbd</systemitem>.
    </para>
   </listitem>
   <listitem>
    <para>
     An iSCSI initiator host, running <systemitem>open-iscsi</systemitem>
     (Linux), the Microsoft iSCSI Initiator (Microsoft Windows), or any
     other compatible iSCSI initiator implementation.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   A recommended production configuration of &storage; with
   <systemitem>lrbd</systemitem> consists of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     A &ceph; storage cluster. A production &ceph; cluster consists of any
     number of (typically more than 10) OSD nodes, each typically running
     10-12 object storage daemons (OSDs), with no fewer than three dedicated
     MON hosts.
    </para>
   </listitem>
   <listitem>
    <para>
     Several iSCSI target servers running the LIO iSCSI target, configured
     via <systemitem>lrbd</systemitem>. For iSCSI fail-over and
     load-balancing, these servers must run a kernel supporting the
     <systemitem>target_core_rbd</systemitem> module. Updates packages are
     available from the &sls; maintenance channel.
    </para>
   </listitem>
   <listitem>
    <para>
     Any number of iSCSI initiator hosts, running
     <systemitem>open-iscsi</systemitem> (Linux), the Microsoft iSCSI
     Initiator (Microsoft Windows), or any other compatible iSCSI initiator
     implementation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="ceph.iscsi.install">
  <title>Installation and Configuration</title>

  <para>
   This section describes steps to install and configure iSCSI gateway on
   top of &storage;.
  </para>

  <sect2>
   <title>Install &storage; and Deploy a &ceph; Cluster</title>
   <para>
    Before you start installing and configuring an iSCSI gateway, you need
    to install &storage; and deploy a &ceph; cluster as described in
    <xref linkend="cha.ceph.install"/>.
   </para>
  </sect2>

  <sect2>
   <title>Installing the <systemitem>ceph_iscsi</systemitem> Pattern</title>
   <para>
    On your designated iSCSI target server nodes, install the
    <systemitem>ceph_iscsi</systemitem> pattern. Doing so will automatically
    install <systemitem>lrbd</systemitem>, the necessary &ceph; binaries and
    libraries, and the <command>targetcli</command> command-line tool:
   </para>
<screen>sudo zypper in -t pattern ceph_iscsi</screen>
   <para>
    Repeat this step on any node that you want to act as a fail-over or
    load-balancing target server node.
   </para>
  </sect2>

  <sect2>
   <title>Create RBD Images</title>
   <para>
    RBD images are created in the &ceph; store and subsequently exported to
    iSCSI. We recommend that you use a dedicated RADOS pool for this
    purpose. You can create a volume from any host that is able to connect
    to your storage cluster using the &ceph; <command>rbd</command>
    command-line utility. This requires the client to have at least a
    minimal ceph.conf configuration file, and appropriate CephX
    authentication credentials.
   </para>
   <para>
    To create a new volume for subsequent export via iSCSI, use the
    <command>rbd create</command> command, specifying the volume size in
    megabytes. For example, in order to create a 100GB volume named
    <literal>testvol</literal> in the pool named <literal>iscsi</literal>,
    run:
   </para>
<screen>rbd --pool iscsi create --size=102400 testvol</screen>
   <para>
    The above command creates an RBD "format 1" volume. In order to enable
    layered snapshots and cloning, you may want to create a "format 2"
    volume instead:
   </para>
<screen>rbd --pool iscsi create --image-format 2 --size=102400 testvol</screen>
   <para>
    To always enable format 2 on all newly-created images, add the following
    configuration options to <filename>/etc/ceph/ceph.conf</filename>:
   </para>
<screen>[client]
   rbd default image format = 2</screen>
   <para>
    Once this option is set, every invocation of <command>rbd
    create</command> becomes functionally equivalent to <command>rbd create
    --image-format 2</command>, and always creates a volume in the new image
    format. Note that this is a client-side option that must be set on any
    host you use to create a new volume. It is not sufficient to set this on
    your &ceph; servers.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.rbd.export">
   <title>Export RBD Images via iSCSI</title>
   <para>
    To export RBD images via iSCSI, use the <systemitem>lrbd</systemitem>
    utility. <systemitem>lrbd</systemitem> allows you to create, review, and
    modify the iSCSI target configuration, which uses a JSON format.
   </para>
   <para>
    In order to edit the configuration, use <command>lrbd -e</command> or
    <command>lrbd --edit</command>. This command will invoke the default
    editor, as defined by the <literal>EDITOR</literal> environment
    variable. You may override this behavior by setting the
    <option>-E</option> option in addition to <option>-e</option>.
   </para>
   <para>
    Below is an example configuration for
   </para>
   <itemizedlist>
    <listitem>
     <para>
      two iSCSI gateway hosts named <literal>iscsi1.example.com</literal>
      and <literal>iscsi2.example.com</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      defining a single iSCSI target with an iSCSI Qualified Name (IQN) of
      <literal>iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      with a single iSCSI Logical Unit (LU),
     </para>
    </listitem>
    <listitem>
     <para>
      backed by an RBD image named <literal>testvol</literal> in the RADOS
      pool <literal>rbd</literal>,
     </para>
    </listitem>
    <listitem>
     <para>
      and exporting the target via two portals named "east" and "west":
     </para>
    </listitem>
   </itemizedlist>
<screen>{
    "auth": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "authentication": "none"
        }
    ], 
    "targets": [
        {
            "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
            "hosts": [
                {
                    "host": "iscsi1.example.com", 
                    "portal": "east"
                }, 
                {
                    "host": "iscsi2.example.com", 
                    "portal": "west"
                }
            ]
        }
    ], 
    "portals": [
        {
            "name": "east", 
            "addresses": [
                "192.168.124.104"
            ]
        }, 
        {
            "name": "west", 
            "addresses": [
                "192.168.124.105"
            ]
        }
    ], 
    "pools": [
        {
            "pool": "rbd", 
            "gateways": [
                {
                    "target": "iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol", 
                    "tpg": [
                        {
                            "image": "testvol"
                        }
                    ]
                }
            ]
        }
    ]
    }</screen>
   <para>
    Note that whenever you refer to a host name in the configuration, this
    host name must match the iSCSI gateway's <command>uname -n</command>
    command output.
   </para>
   <para>
    The edited JSON is stored in the extended attributes (xattrs) of a
    single RADOS object per pool. This object is available to the gateway
    hosts where the JSON is edited, and all gateway hosts connected to the
    same &ceph; cluster. No configuration information is stored locally on
    the <systemitem>lrbd</systemitem> gateway.
   </para>
   <para>
    To activate the configuration, store it in the &ceph; cluster, and do
    one of the following things (as &rootuser;):
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Run the <command>lrbd</command> command (without additional options)
      from the command line,
     </para>
    </listitem>
   </itemizedlist>
   <para>
    or
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Restart the <systemitem>lrbd</systemitem> service with
      <command>service lrbd restart</command>.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    The <systemitem>lrbd</systemitem> "service" does not operate any
    background daemon. Instead, it simply invokes the
    <command>lrbd</command> command. This type of service is known as a
    "one-shot" service.
   </para>
   <para>
    You should also enable <systemitem>lrbd</systemitem> to auto-configure
    on system startup. To do so, run the <command>systemctl enable
    lrbd</command> command.
   </para>
   <para>
    The configuration above reflects a simple, one-gateway setup.
    <systemitem>lrbd</systemitem> configuration can be much more complex and
    powerful. The <systemitem>lrbd</systemitem> RPM package comes with an
    extensive set of configuration examples, which you may refer to by
    checking the contents of the
    <filename>/usr/share/doc/packages/lrbd/samples</filename> directory
    after installation. The samples are also available from
    <link xlink:href="https://github.com/SUSE/lrbd/tree/master/samples"/>.
   </para>
   <!-- 2016-03-24 tbazant: commenting out on eric's advice
   <sect3 xml:id="ceph.iscsi.multiple_images">
    <title>Multiple Images Sharing the Same iSCSI Target</title>
    <para>
     The configuration for <systemitem>lrbd</systemitem> can cause strange
     effects when dealing with multiple images 'behind' the same target.
     These images are presented with LUN numbers starting with 0 and
     counting up.
    </para>
    <para>
     When adding further images and removing existing images, the LUN
     numbering will get mixed up causing problems on clients that are
     depending on static and persistent LUN numbers.
    </para>
    <para>
     Therefore you may need to use multiple targets with one image in each
     as that leads to having only <emphasis>LUN0</emphasis>.
    </para>
   </sect3>
   -->
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.connect">
  <title>Connecting to lrbd-managed targets</title>

  <para>
   This chapter describes how to connect to lrdb-managed targets from
   clients running Linux, Microsoft Windows, or VMware.
  </para>

  <sect2 xml:id="ceph.iscsi.connect.linux">
   <title>Linux (<systemitem>open-iscsi</systemitem>)</title>
   <para>
    Connecting to lrbd-backed iSCSI targets with
    <systemitem>open-iscsi</systemitem> is a two-step process. First the
    initiator must discover the iSCSI targets available on the gateway host,
    then it must log in and map the available Logical Units (LUs).
   </para>
   <para>
    Both steps require that the <systemitem>open-iscsi</systemitem> daemon
    is running. The way you start the <systemitem>open-iscsi</systemitem>
    daemon is dependent on your Linux distribution:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      On &sls; (SLES); and &rhel; (RHEL) hosts, run <command>systemctl start
      iscsid</command> (or <command>service iscsid start</command> if
      <command>systemctl</command> is not available).
     </para>
    </listitem>
    <listitem>
     <para>
      On Debian and Ubuntu hosts, run <command>systemctl start
      open-iscsi</command> (or <command>service open-iscsi start</command>).
     </para>
    </listitem>
   </itemizedlist>
   <para>
    If your initiator host runs &sls;, refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/stor_admin/data/sec_iscsi_initiator.html"/>
    or
    <link xlink:href="https://www.suse.com/documentation/sles11/stor_admin/data/sec_inst_system_iscsi_initiator.html"/>
    for details on how to connect to an iSCSI target.
   </para>
   <para>
    For any other Linux distribution supporting
    <systemitem>open-iscsi</systemitem>, proceed to discover targets on your
    <systemitem>lrbd</systemitem> gateway (this example uses
    iscsi1.example.com as the portal address; for multipath access repeat
    these steps with iscsi2.example.com):
   </para>
<screen>iscsiadm -m discovery -t sendtargets -p iscsi1.example.com
192.168.124.104:3260,1 iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol</screen>
   <para>
    Then, log in to the portal. If the login completes successfully, any
    RBD-backed logical units on the portal will immediately become available
    on the system SCSI bus:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --login
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] (multiple)
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    Repeat this process for other portal IP addresses or hosts.
   </para>
   <para>
    If your system has the <systemitem>lsscsi</systemitem> utility
    installed, you use it to enumerate available SCSI devices on your
    system:
   </para>
<screen>lsscsi
[8:0:0:0]    disk    SUSE     RBD              4.0   /dev/sde 
[9:0:0:0]    disk    SUSE     RBD              4.0   /dev/sdf</screen>
   <para>
    In a multipath configuration (where two connected iSCSI devices
    represent one and the same LU), you can also examine the multipath
    device state with the <systemitem>multipath</systemitem> utility:
   </para>
<screen>multipath -ll
360014050cf9dcfcb2603933ac3298dca dm-9 SUSE,RBD
size=49G features='0' hwhandler='0' wp=rw
|-+- policy='service-time 0' prio=1 status=active
| `- 8:0:0:0 sde 8:64 active ready running
`-+- policy='service-time 0' prio=1 status=enabled
`- 9:0:0:0 sdf 8:80 active ready running</screen>
   <para>
    You can now use this multipath device as you would any block device. For
    example, you can use the device as a Physical Volume for Linux Logical
    Volume Management (LVM), or you can simply create a file system on it.
    The example below demonstrates how to create an XFS file system on the
    newly connected multipath iSCSI volume:
   </para>
<screen>mkfs -t xfs /dev/mapper/360014050cf9dcfcb2603933ac3298dca 
log stripe unit (4194304 bytes) is too large (maximum is 256KiB)
log stripe unit adjusted to 32KiB
meta-data=/dev/mapper/360014050cf9dcfcb2603933ac3298dca isize=256    agcount=17, agsize=799744 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=0        finobt=0
data     =                       bsize=4096   blocks=12800000, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=0
log      =internal log           bsize=4096   blocks=6256, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0</screen>
   <para>
    Note that XFS being a non-clustered file system, you may only ever mount
    it on a single iSCSI initiator node at any given time.
   </para>
   <para>
    If at any time you want to discontinue using the iSCSI LUs associated
    with a particular target, run the following command:
   </para>
<screen>iscsiadm -m node -p iscsi1.example.com --logout
Logging out of session [sid: 18, iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260]
Logout of [sid: 18, target: iqn.2003-01.org.linux-iscsi.iscsi.x86:testvol, portal: 192.168.124.104,3260] successful.</screen>
   <para>
    As with discovery and login, you must repeat the logout steps for all
    portal IP addresses or hostnames.
   </para>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.win">
   <title>Microsoft Windows (Microsoft iSCSI initiator)</title>
   <para>
    To connect to a &storage; iSCSI target from a Windows 2012 server,
    follow these steps:
   </para>
   <procedure>
    <step>
     <para>
      Open Windows Server Manager. From the Dashboard, select
      <menuchoice><guimenu>Tools</guimenu><guimenu>iSCSI
      Initiator</guimenu></menuchoice>. The <guimenu>iSCSI Initiator
      Properties</guimenu> dialog appears. Select the
      <guimenu>Discovery</guimenu> tab:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-initiator-props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>Discover Target Portal</guimenu> dialog, enter the
      target's host name or IP address in the <guimenu>Target</guimenu>
      field and click <guimenu>OK</guimenu>:
     </para>
     <figure>
      <title>Discover Target Portal</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Repeat this process for all other gateway host names or IP addresses.
      Once completed, review the <guimenu>Target Portals</guimenu> list:
     </para>
     <figure>
      <title>Target Portals</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-ip-list.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Next, switch to the <guimenu>Targets</guimenu> tab and review your
      discovered target(s).
     </para>
     <figure>
      <title>Targets</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-targets.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Click <guimenu>Connect</guimenu> in the <guimenu>Targets</guimenu>
      tab. The <guimenu>Connect To Target</guimenu> dialog appears. Select
      the <guimenu>Enable Multi-path</guimenu> check box to enable multipath
      I/O (MPIO), then click <guimenu>OK</guimenu>:
     </para>
    </step>
    <step>
     <para>
      Once the <guimenu>Connect to Target</guimenu> dialog closes, select
      <guimenu>Properties</guimenu> to review the target's properties:
     </para>
     <figure>
      <title>iSCSI Target Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-target-properties.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select <guimenu>Devices</guimenu>, and click <guimenu>MPIO</guimenu>
      to review the multipath I/O configuration:
     </para>
     <figure>
      <title>Device Details</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-device-details.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      The default <guimenu>Load Balance policy</guimenu> is <guimenu>Round
      Robin With Subset</guimenu>. If you prefer a pure fail-over
      configuration, change it to <guimenu>Fail Over Only</guimenu>.
     </para>
    </step>
   </procedure>
   <para>
    This concludes the iSCSI initiator configuration. The iSCSI volumes are
    now available just like any other SCSI devices, and may be initialized
    for use as volumes and drives. Click <guimenu>OK</guimenu> to close the
    <guimenu>iSCSI Initiator Properties</guimenu> dialog, and proceed with
    the<guimenu> File and Storage Services</guimenu> role from the
    <guimenu>Server Manager</guimenu> dashboard.
   </para>
   <para>
    Observe the newly connected volume. It identifies as <emphasis>SUSE RBD
    SCSI Multi-Path Drive</emphasis> on the iSCSI bus, and is initially
    marked with an <emphasis>Offline</emphasis> status and a partition table
    type of <emphasis>Unknown</emphasis>. If the new volume does not appear
    immediately, select <guimenu>Rescan Storage</guimenu> from the
    <guimenu>Tasks</guimenu> drop-down to rescan the iSCSI bus.
   </para>
   <procedure>
    <step>
     <para>
      Right-click on the iSCSI volume and select <guimenu>New
      Volume</guimenu> from the context menu. The <guimenu>New Volume
      Wizard</guimenu> appears. Click <guimenu>Next</guimenu>, highlight the
      newly connected iSCSI volume and click <guimenu>Next</guimenu> to
      begin.
     </para>
     <figure>
      <title>New Volume Wizard</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-wizard.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Initially, the device is empty and does not contain a partition table.
      When prompted, confirm the dialog indicating that the volume will be
      initialized with a GPT partition table:
     </para>
     <figure>
      <title>Offline Disk Prompt</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-win-prompt1.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      Select the volume size. Typically, you would use the device's full
      capacity. Then assign a drive letter or folder name where the newly
      created volume will become available. Then select a file system to
      create on the new volume, and finally confirm your selections with
      <guimenu>Create</guimenu> to finish creating the volume:
     </para>
     <figure>
      <title>Confirm Volume Selections</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-volume-confirm.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      When the process finishes, review the results, then
      <guimenu>Close</guimenu> to conclude the drive initialization. Once
      initialization completes, the volume (and its NTFS file system)
      becomes available just like a newly initialized local drive.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="ceph.iscsi.connect.vmware">
   <title>VMware</title>
   <para></para>
   <procedure>
    <step>
     <para>
      To connect to <systemitem>lrbd</systemitem> managed iSCSI volumes you
      need a configured iSCSI software adapter. If no such adapter is
      available in your vSphere configuration, create one by selecting
      <menuchoice><guimenu>Configuration</guimenu><guimenu>Storage
      Adapters</guimenu> <guimenu>Add</guimenu><guimenu>iSCSI Software
      initiator</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Once available, select the adapter's properties by right-clicking on
      the adapter and selecting <guimenu>Properties</guimenu> from the
      context menu:
     </para>
     <figure>
      <title>iSCSI Initiator Properties</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi_vmware_adapter_props.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      In the <guimenu>iSCSI Software Initiator</guimenu> dialog, click the
      <guimenu>Configure</guimenu> button. Then go to the <guimenu>Dynamic
      Discovery</guimenu> tab and select <guimenu>Add</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Enter the IP address or host name of your
      <systemitem>lrbd</systemitem> iSCSI gateway. If you run multiple iSCSI
      gateways in a failover configuration, repeat this step for as many
      gateways as you operate.
     </para>
     <figure>
      <title>Add Target Server</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-target.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Once you have entered all iSCSI gateways, click Yes in the dialog box
      to initiate a rescan of the iSCSI adapter
     </para>
    </step>
    <step>
     <para>
      Once the rescan completes, the new iSCSI device appears below the
      <guimenu>Storage Adapters</guimenu> list in the
      <guimenu>Details</guimenu> pane. For multipath devices, you can now
      right-click on the adapter and select <guimenu>Manage Paths</guimenu>
      from the context menu:
     </para>
     <figure>
      <title>Manage Multipath Devices</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-multipath.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      You should now see all paths with a green light under
      <guimenu>Status</guimenu>. One of your paths should be marked
      <guimenu>Active (I/O)</guimenu> and all others simply
      <guimenu>Active</guimenu>:
     </para>
     <figure>
      <title>Paths Listing for Multipath</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-paths.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
    <step>
     <para>
      You can now switch from <guimenu>Storage Adapters</guimenu> to the
      item labeled <guimenu>Storage</guimenu>. Select <guimenu>Add
      Storage...</guimenu> in the top-right corner of the pane to bring up
      the <guimenu>Add Storage</guimenu> dialog. Then, select
      <guimenu>Disk/LUN</guimenu> and click <guimenu>Next</guimenu>. The
      newly added iSCSI device appears in the <guimenu>Select
      Disk/LUN</guimenu> list. Select it, then click <guimenu>Next</guimenu>
      to proceed:
     </para>
     <figure>
      <title>Add Storage Dialog</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-add-storage-dialog.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Next</guimenu> to accept the default disk layout.
     </para>
    </step>
    <step>
     <para>
      In the <guimenu>Properties</guimenu> pane, assign a name to the new
      datastore, and click <guimenu>Next</guimenu>. Accept the default
      setting to use the volume's entire space for the datastore, or select
      <guimenu>Custom Space Setting</guimenu> for a smaller datastore:
     </para>
     <figure>
      <title>Custom Space Setting</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-custom-datastore.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
     <para>
      Click <guimenu>Finish</guimenu> to complete the datastore creation.
     </para>
     <para>
      The new datastore now appears in the datastore list and you can select
      it to retrieve details. You are now able to use the
      <systemitem>lrbd</systemitem>-backed iSCSI volume like any other
      vSphere datastore.
     </para>
     <figure>
      <title>iSCSI Datastore Overview</title>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="iscsi-vmware-overview.png" width="70%" format="PNG"/>
       </imageobject>
      </mediaobject>
     </figure>
    </step>
   </procedure>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.iscsi.conclude">
  <title>Conclusion</title>

  <para>
   <systemitem>lrbd</systemitem> is a key component of &storage; 2 that
   enables access to distributed, highly available block storage from any
   server or client capable of speaking the iSCSI protocol. By using
   <systemitem>lrbd</systemitem> on one or more iSCSI gateway hosts, &ceph;
   RBD images become available as Logical Units (LUs) associated with iSCSI
   targets, which can be accessed in an optionally load-balanced, highly
   available fashion.
  </para>

  <para>
   Since all of <systemitem>lrbd</systemitem>'s configuration is stored in
   the &ceph; RADOS object store, <systemitem>lrbd</systemitem> gateway
   hosts are inherently without persistent state and thus can be replaced,
   augmented, or reduced at will. As a result, &storage; 2 enables SUSE
   customers to run a truly distributed, highly-available, resilient, and
   self-healing enterprise storage technology on commodity hardware and an
   entirely open-source platform.
  </para>
 </sect1>
</chapter>
