<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.ceph.cephfs">
<!-- ============================================================== -->
<!-- initially imported from https://github.com/SUSE/lrbd/wiki -->
 <title>Clustered File System</title>
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:maintainer>tbazant@suse.com</dm:maintainer>
   <dm:status>editing</dm:status>
   <dm:deadline></dm:deadline>
   <dm:priority></dm:priority>
   <dm:translation></dm:translation>
   <dm:languages></dm:languages>
   <dm:release>SES2.1</dm:release>
  </dm:docmanager>
</info>

<warning>
  <title>Technical Preview</title>
  <para>
    As of &storage; 3, &cephfs; is considered a technical preview and is not
    supported.
  </para>
</warning>
 
 <para>
  The &ceph; file system (&cephfs;) is a POSIX-compliant file system that
  uses a &ceph; storage cluster to store its data. &cephfs; uses the same
  cluster system as &ceph; block devices, &ceph; object storage with its S3
  and Swift APIs, or native bindings (<systemitem>librados</systemitem>).
 </para>
 <para>
  To use &cephfs;, you need to have a running &ceph; storage cluster, and at
  least one running <emphasis>&ceph; metadata server</emphasis>.
 </para>
 <sect1 xml:id="ceph.cephfs.mds">
  <title>&ceph; Metadata Server</title>

  <para>
   &ceph; metadata server (MDS) stores metadata for the &cephfs;. &ceph;
   block devices and &ceph; object storage <emphasis>do not</emphasis> use
   MDS. MDS's make it possible for POSIX file system users to execute basic
   commands&mdash;such as <command>ls</command> or
   <command>find</command>&mdash;without placing an enormous burden on the
   &ceph; storage cluster.
  </para>

  <sect2 xml:id="ceph.cephfs.mdf.add">
   <title>Adding a Metadata Server</title>
   <para>
    After you deploy OSD's and monitors, you can deploy metadata servers.
    Although MDS service can share a node with an OSD and/or monitor one,
    you are encouraged to deploy it on a separate cluster node for
    performance reasons.
   </para>
<screen>ceph-deploy mds create <replaceable>host-name</replaceable>:<replaceable>daemon-name</replaceable></screen>
   <para>
    You may optionally specify a daemon instance name if you need to run
    multiple daemons on a single server.
   </para>
  </sect2>

  <sect2 xml:id="ceph.cephfs.mdf.config">
   <title>Configuring a Metadata Server</title>
   <para>
    You can fine tune the MDS behavior by inserting relevant options in the
    <filename>ceph.conf</filename> configuration file. For detailed list of
    MDS related configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/mds-config-ref/"/>.
   </para>
   <para>
    For detailed list of MDS journaler configuration options, see
    <link xlink:href="http://docs.ceph.com/docs/master/cephfs/journaler/"/>.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="ceph.cephfs.cephfs">
<!-- http://docs.ceph.com/docs/master/cephfs/createfs/ -->

  <title>&cephfs;</title>

  <para>
   Once you have a healthy &ceph; storage cluster with at least one &ceph;
   metadata server, you may create and mount your &ceph; file system. Ensure
   that your client has network connectivity and a proper authentication
   keyring.
  </para>

  <sect2 xml:id="ceph.cephfs.cephfs.create">
   <title>Creating &cephfs;</title>
   <para>
    A &cephfs; requires at least two RADOS pools: one for
    <emphasis>data</emphasis> and one for <emphasis>metadata</emphasis>.
    When configuring these pools, you might consider:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Using a higher replication level for the metadata pool, as any data
      loss in this pool can render the whole file system inaccessible.
     </para>
    </listitem>
    <listitem>
     <para>
      Using lower-latency storage such as SSDs for the metadata pool, as
      this will directly affect the observed latency of file system
      operations on clients.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For more information on managing pools, see
    <xref linkend="ceph.pools"/>.
   </para>
   <para>
    To create the two required pools&mdash;for example 'cephfs_data' and
    'cephfs_metadata'&mdash; with default settings for use with &cephfs;,
    run the following commands:
   </para>
<screen>ceph osd pool create cephfs_data <replaceable>pg_num</replaceable>
ceph osd pool create cephfs_metadata <replaceable>pg_num</replaceable></screen>
   <para>
    Once the pools are created, you may enable the file system with the
    <command>ceph fs new</command> command:
   </para>
<screen>ceph fs new <replaceable>fs_name</replaceable> <replaceable>metadata_pool_name</replaceable> <replaceable>data_pool_name</replaceable></screen>
   <para>
    For example
   </para>
<screen>ceph fs new cephfs cephfs_metadata cephfs_data</screen>
   <para>
    You can check that the file system was created by listing all available
    &cephfs;'s:
   </para>
<screen>ceph fs ls
 name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</screen>
   <para>
    Once the file system has been created, your MDS will be able to enter an
    <emphasis>active</emphasis> state. For example, in a single MDS system:
   </para>
<screen>ceph mds stat
 e5: 1/1/1 up</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.mount">
   <title>Mounting &cephfs;</title>
   <para>
    Once the file system is created and the MDS is active, you are ready to
    mount the file system from a client host.
   </para>
   <sect3>
    <title>Mount &cephfs; with the Kernel Driver</title>
    <para>
     You can mount &cephfs;, normally with the <command>mount</command>
     command. You need to specify the monitor host name or IP address.
    </para>
    <tip>
     <title>Specify Multiple Monitors</title>
     <para>
      It is a good idea to specify multiple monitors separated by commas on
      the <command>mount</command> command line in case one monitor happens
      to be down at the time of mount. Each monitor address takes the
      form<literal> host[:port]</literal>. If the port is not specified, it
      defaults to 6789.
     </para>
    </tip>
    <para>
     Create the mount point on the local host:
    </para>
<screen>sudo mkdir /mnt/cephfs</screen>
    <para>
     Mount the &cephfs;:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs</screen>
    <para>
     A subdirectory <filename>subdir</filename> may be specified if a subset
     of the file system is to be mounted:
    </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/subdir /mnt/cephfs</screen>
    <tip>
     <title>&cephfs; and <systemitem>cephx</systemitem> Authentication</title>
     <para>
      To mount &cephfs; with <systemitem>cephx</systemitem> authentication
      enabled, you need to specify a user name and a secret:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==</screen>
     <para>
      As the previous command remains in the shell history, a more secure
      approach is to read the secret from a file:
     </para>
<screen>sudo mount -t ceph ceph_mon1:6789:/ /mnt/cephfs \
 -o name=admin,secretfile=/etc/ceph/admin.secret</screen>
    </tip>
   </sect3>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.unmount">
   <title>Unmounting &cephfs;</title>
   <para>
    To unmount the &cephfs;, use the <command>umount</command> command:
   </para>
<screen>sudo umount /mnt/cephfs</screen>
  </sect2>

  <sect2 xml:id="ceph.cephfs.cephfs.fstab">
   <title>&cephfs; in <filename>/etc/fstab</filename></title>
   <para>
    To mount &cephfs; automatically on the client startup, insert the
    corresponding line in its file systems table
    <filename>/etc/fstab</filename>:
   </para>
<screen>mon1:6790,mon2:/subdir /mnt/cephfs ceph name=admin,secretfile=/etc/ceph/secret.key,noatime 0 2</screen>
  </sect2>
 </sect1>
</chapter>
